2020.coling-main.500,D19-1307,0,0.0339871,"Missing"
2020.coling-main.500,N19-1423,0,0.00517476,"y arbitrary document particle (s and d should be of the same lexical form such as bigram); (2) Extract top-k document particles based on similarity to form the anchor set, i.e. As = {ds1 , ds2 , ..., dsk }. Also, we record the similarity as the strength of anchor and denote the strength between s and dsi as qsi (1 ≤ i ≤ k). The embedding vector of the particle in this paper is obtained by averaging the contextualized embeddings of all tokens occurring in the particle. Specifically, in the following experiment, we will sum the last four hidden layers of the pretrained uncased BERT Base model2 (Devlin et al., 2019) to get the embedding for each token (dimension of embedding vector is 768). An example of anchor set can be found in Figure 2. Source document ----------huge success-----excellent result-------------------------------------remarkable achievement Summary (peer/reference) ------------------------great success------------------------- Anchor set (k=3) huge success excellent result remarkable achiev. Figure 2: An example of anchor set for the bigram “great success” when top-3 results are extracted. The anchored version of ROUGE can be defined as follows once all the anchor sets for summary partic"
2020.coling-main.500,2020.acl-main.124,0,0.115179,"off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our knowledge, few of th"
2020.coling-main.500,D15-1013,0,0.0295391,"On both datasets, anchored ROUGE has achieved the highest correlations according to all three correlation coefficients. More specifically, both AncR-1 and AncR-2 have a correlation higher than their original counterparts (i.e. R-1 and R-2) and the gaps are over 2.5 and 1.3 percent, respectively. Even the most recent metric based on advanced contextualized embeddings, i.e. Mover, has fallen behind our metric (by over one percent as compared with AncR-1 on TAC 2008 and AncR-2 on TAC 2009). For a more convincing comparison, we have conducted the pairwise Williams significance test recommended by Graham (2015) between our metric (more precisely AncR-1 on TAC 2008 and AncR-2 on TAC 2009) and other competitors and the result shows that the increases of our metric over 3 are statistically significant (p-value < 0.05). others except the supervised metric Sbest Hyperparameter effect & Robustness. Two extra tests have been performed to further analyze our metric. The effects of anchor set size k on Pearson correlations are illustrated in Figure 3, indicating that an anchor set with the proper size is needed to establish the efficacy of our metric. The correlations deteriorate when k is less than three an"
2020.coling-main.500,N18-1065,0,0.0130312,"with an extremely large k that causes more intensive computation. The effect of the number of reference summaries is shown in Table 2. We have used all available references to compute metrics when n is equal to four and used n randomly selected references with a smaller n (note that the average of n4 results is reported). The observation is that our metric is relatively robust to n and it demonstrates that our metric is less prone to the reference noise observed in Kryscinski et al. (2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015; Grusky et al., 2018). 4 Related Work There are various reference-based automatic evaluation metrics for the task of document summarization. The widely accepted metric is ROUGE (Lin, 2004) that focuses primarily on n-gram co-occurrence statistics. Some strategies are proposed to replace the “hard matching” of ROUGE, such as the adoption of WordNet (ShafieiBavani et al., 2018) and the fusion of ROUGE and word2vec (Ng and Abrecht, 2015). Another promising method of designing metrics is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings s"
2020.coling-main.500,D19-1051,0,0.0604432,"ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here. 1 Introduction Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization. Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summar"
2020.coling-main.500,P11-1052,0,0.0566795,"on results between reference-based automatic metrics and human judgments (k = 5 and n = 4). Best correlations are in bold and our proposed metrics are AncR-1 and AncR-2. P ROUGE-anchored = T (d, summ) = X s∈summ P min(T (d, peer), T (d, ref)) P , ref∈RefSumm d∈Cref T (d, ref) ref∈RefSumm d∈Cref P i=k X δd,dsi · ws · qsi , for summ ∈ {peer, ref}. (1) (2) i=1 dsi ∈As The anchored metric listed above has based the computation on anchor sets residing in the source document. For convenience, we will compare it with the original ROUGE metric, especially the equivalent definition of ROUGE-N given by Lin and Bilmes (2011) (Theorem 3 in the original paper). Function T replaces the count of summary particles, which is adopted in original ROUGE, and for a specific document particle sums the weighted contributions from different summary particles (the weight coefficient is the anchor strength qsi as shown in Eqn. 2). The factor ws is used to assess the effect of multiple occurrences of the same summary particle s. In Eqn. 1, the min function is utilized to compute the weighted matching degree based on document particle d (thus the overall metric will be less than one), which revises the exact count of matching sum"
2020.coling-main.500,W04-1013,0,0.328857,"ce document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here. 1 Introduction Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization. Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source doc"
2020.coling-main.500,J13-2002,0,0.0277847,"sets of topic-focused multi-document summarization (MDS), i.e. TAC 20083 and TAC 20094 , for two main reasons: (1) MDS is more challenging than single document summarization and summarizers tend to behave more differently for evaluation, which fits the purpose to examine various metrics; (2) Multiple reference summaries are offered, which makes it possible to perform robustness test (see Table 2). The two datasets consist of 48 and 44 topics, respectively, each of which has 10 source documents and 4 reference summaries, i.e. n is 4. We only use document set A of official datasets in line with Louis and Nenkova (2013) and Gao et al. (2020). Additionally, TAC 2008 has 57 peer summaries for each topic while TAC 2009 has 55. All summaries are at most 100 words and each peer summary is associated with a Pyramid score (Nenkova and Passonneau, 2004), which serves as the human judgment. For tuning the anchor set size (i.e. k in Section 2), another dataset (DUC 20075 ) will be used. Comparing metrics. These reference-based metrics are involved in the experiment. (1) ROUGE (Lin, 2004): a traditional metric for counting lexical-level overlap. For comparison, two variants are considered based on either unigram (R-1)"
2020.coling-main.500,N04-1019,0,0.224005,"evaluation, which fits the purpose to examine various metrics; (2) Multiple reference summaries are offered, which makes it possible to perform robustness test (see Table 2). The two datasets consist of 48 and 44 topics, respectively, each of which has 10 source documents and 4 reference summaries, i.e. n is 4. We only use document set A of official datasets in line with Louis and Nenkova (2013) and Gao et al. (2020). Additionally, TAC 2008 has 57 peer summaries for each topic while TAC 2009 has 55. All summaries are at most 100 words and each peer summary is associated with a Pyramid score (Nenkova and Passonneau, 2004), which serves as the human judgment. For tuning the anchor set size (i.e. k in Section 2), another dataset (DUC 20075 ) will be used. Comparing metrics. These reference-based metrics are involved in the experiment. (1) ROUGE (Lin, 2004): a traditional metric for counting lexical-level overlap. For comparison, two variants are considered based on either unigram (R-1) or bigram (R-2). (2) ROUGE-WE (Ng and Abrecht, 2015): a metric based on word2vec embeddings (Mikolov et al., 2013) to compute semantic similarity. ROUGE-WE with unigram (R-1-WE) and bigram (R-2-WE) are computed. (3) BERTScore (Zha"
2020.coling-main.500,D15-1222,0,0.348392,"ce-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our knowledge, few of them consider the impact of source document (or documents in multi-document summarization) on the computation. This goes against common sense as source document is the true information source of both summaries and can be utilized to boost the discriminative power of metrics. Therefore, we advance a new protocol of reference-based metrics for the evaluation of document summarization. More specifically, the direct participation of source document is a necessity to compute any reference-based metric for document summa"
2020.coling-main.500,W17-4510,0,0.0132227,"uation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our k"
2020.coling-main.500,E17-2007,0,0.0134001,"n on more solid ground. Empirical results on benchmark datasets validate that source document helps to induce a higher correlation with human judgments for ROUGE metric. Being self-explanatory and easy-to-implement, the protocol can naturally foster various effective designs of reference-based metrics besides the anchored ROUGE introduced here. 1 Introduction Automatic evaluation metric plays a vital role in evaluating system performance for the task of document summarization. Challenges remain in the design of an ideal evaluation metric and the off-the-shelf metrics have their own drawbacks (Schluter, 2017; Kryscinski et al., 2019). The widely adopted metrics, e.g. ROUGE (Lin, 2004), are reference-based in that they compare the output of some summarizer (namely peer summary) with one or multiple human-authored summaries (namely reference/model summary). The reference-free metrics are still not mature enough to be utilized for evaluation in a real-world setting since their correlations with human judgments have been reported to fall far behind reference-based metrics, especially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol"
2020.coling-main.500,D18-1085,0,0.0130258,"ur metric is relatively robust to n and it demonstrates that our metric is less prone to the reference noise observed in Kryscinski et al. (2019) or the reference bias introduced when very few reference summaries are available (Hermann et al., 2015; Grusky et al., 2018). 4 Related Work There are various reference-based automatic evaluation metrics for the task of document summarization. The widely accepted metric is ROUGE (Lin, 2004) that focuses primarily on n-gram co-occurrence statistics. Some strategies are proposed to replace the “hard matching” of ROUGE, such as the adoption of WordNet (ShafieiBavani et al., 2018) and the fusion of ROUGE and word2vec (Ng and Abrecht, 2015). Another promising method of designing metrics is to directly compute the semantic similarity of peer and reference summary, including the metrics utilizing various word embeddings such as ELMo (Sun and Nenkova, 2019) and BERT (Zhang et al., 2019; Zhao et al., 2019). Furthermore, Zhang et 5699 al. (2020) proposes a metric computing factual correctness based on information extraction. However, none of the above metrics fall into the newly-introduced protocol. The anchored ROUGE proposed by us is a refined metric that has followed the"
2020.coling-main.500,D19-1116,0,0.0530917,"cially for multi-document summarization (Peyrard et al., 2017; Gao et al., 2020)1 . In this paper, we consider a new protocol of reference-based summarization metrics by rethinking the role of source document which is indeed a lost treasure neglected by most previous works. Furthermore, a specific implementation of the protocol (i.e. anchored version of ROUGE) will be discussed. The reference-based metrics that already exist typically pursue a kinda computation of overlap between the peer and reference summary either at a lexical level (Lin, 2004) or at a semantic level (Ng and Abrecht, 2015; Sun and Nenkova, 2019; Zhang et al., 2019). However, to our knowledge, few of them consider the impact of source document (or documents in multi-document summarization) on the computation. This goes against common sense as source document is the true information source of both summaries and can be utilized to boost the discriminative power of metrics. Therefore, we advance a new protocol of reference-based metrics for the evaluation of document summarization. More specifically, the direct participation of source document is a necessity to compute any reference-based metric for document summarization. This makes so"
2020.coling-main.500,2020.acl-main.458,0,0.0780863,"Missing"
2020.coling-main.500,D19-1053,0,0.0547772,".nist.gov/duc2007/tasks.html#pilot 4 5698 0.85 TAC 2008 r ρ Pearson Correlation 0.83 0.81 AncR-1 n=4 n=3 n=2 n=1 .772 .770 .769 .764 .690 .685 .686 .679 .837 .836 .832 .831 .730 .726 .724 .721 AncR-2 n=4 n=3 n=2 n=1 .756 .760 .754 .751 .653 .658 .654 .652 .842 .840 .835 .833 .738 .736 .732 .729 AncR-1TAC2008 AncR-2TAC2008 0.79 AncR-1TAC2009 0.77 AncR-2TAC2009 0.75 0.73 1 2 3 4 5 6 7 8 9 10 Anchor set size k Figure 3: Exploring anchor set size k. TAC 2009 r ρ Table 2: Correlations computed with n references. 2017): two learned metrics that combine different sets of existing metrics. (5) Mover (Zhao et al., 2019): a contextualized-embedding-based metric using Word Mover’s Distance (Kusner et al., 2015). We report its best version with the BERT embeddings and the certain methods for fine-tuning and aggregation of embeddings according to the original paper. (6) ROUGE-anchored: our metric proposed under the new protocol as formulated in Section 2. Similar to ROUGE, we consider two variants with different particle granularities, i.e. unigram (AncR-1) and bigram (AncR-2). Tuning on DUC 2007 sets the anchor set size to 5. Following the convention, we compute the average summary-level correlation with human"
2020.conll-1.48,S19-1028,0,0.0335433,"Missing"
2020.conll-1.48,P04-3031,0,0.339572,"Missing"
2020.conll-1.48,D15-1075,0,0.0339651,"ss function for a training batch with k examples is a weighted P sum of instance-level loss li : Lbatch = αi ∗ li /( ki=1 αi ). Bias Product Ensemble: an ensemble method that is a product of experts pˆi = sof tmax(log(pi ) + log(bi )). By doing so, the prime model would be encouraged to learn all the information except the specific bias. An intuitive justification from the probabilistic view can be found in Clark et al. (2019). Note that while training, only the prime model is updated while the bias-only model remains unchanged. 3 3.2 2018) in our testing. Training Resources: apart from SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018), we also incorporate Diverse NLI (DNLI) (Poliak et al., 2018a) and Adversarial NLI (ANLI) (Nie et al., 2020) datasets for training. For DNLI, we merge the subsets to form unified train/valid/test sets. Dataset Statistics are shown in Table 2. 2.3 Model Performance on the Benchmark Mixture of Experts (MoE) Debiasing Word overlap heuristics: To combat the word overlap heuristics (HANS (McCoy et al., 2019), renamed as IS-SD in Sec 2.1.2), Clark et al. (2019) used the following features to train a biasonly model: (1) whether the hypothesis is a subsequence of"
2020.conll-1.48,N19-1423,0,0.031017,"m guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experiments in Table 6. Table 7 shows how we evaluate the test sets with only two labels in 3-way NLI classification. while in the single mode, w"
2020.conll-1.48,P17-2097,0,0.0216575,"e datasets. Our findings suggest model-level MoE ensemble, text swap DA and performance based dataset merging would effectively combat multiple (though not all) distinct biases. Although we haven’t found a debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from ne"
2020.conll-1.48,2020.emnlp-main.657,1,0.834756,"Missing"
2020.conll-1.48,P17-1152,0,0.0280573,"the first 4 layers are close to random guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experiments in Table 6. Table 7 shows how we evaluate the test sets with only two labels in 3-way NLI cla"
2020.conll-1.48,P19-1554,0,0.0110103,"(Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed efficient methods to mitigate a particular known bias in NLI. Benchmark collection in NLI: GLUE (Wang et al., 2019b,a) benchmark contains several NLIrelated benchmark datasets. However it does not include adversarial test sets, domain specific test (Romanov and Shivade, 2018; Ravichander et al., Acknowledgments We would like to thank Sam Wiseman and Kevin Gimpel for very thoughtful discussions, and the an"
2020.conll-1.48,D19-1418,0,0.304731,"g methods could increase the model performance on the paired adversarial dataset, they might hinder the model performance on other adversarial datasets, as well as hurt the model generalization power, i.e. deficient scores on cross-datasets or cross-domain settings. These phenomena motivate us to investigate if it exists a unified model-agnostic debiasing strategy which can mitigate distinct (or even all) known biases while keeping or strengthening the model generalization power. We begin with NLI debiasing models. To make our trials more generic, we adopt a mixture of experts (MoE) strategy (Clark et al., 2019), which is known for being model-agnostic and is adaptable to various kinds of known biases, as backbone. Specifically we treat three known biases, namely word overlap, length mismatch and partial input heuristics as independent experts and train corresponding debiasing models. Our results show that the debiasing methods tied to one particular known bias may not be sufficient to build a generalized, robust model. This motivates us to investigate a better solution to integrate the advantages of distinct debiasing models. We find model-level ensemble is more effective than other MoE ensemble met"
2020.conll-1.48,P18-2103,0,0.206059,"guage Learning, pages 596–608 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Datasets Paper Categories Labels Size PI-CD (a) 1k3k7 (E,N,C) 3.2k PI-SP (b) 1k3k7 (E,N,C) .37k IS-SD (c) 2k5k8 (¬E, E) 30k IS-CS (d) 2k3k7 (E,N,C) .65k (e)(f) 2k4k9 (E,C) 9.9K LI-LI LI-TS (g)(h) 2k6k10 (¬C, C) 9.8K ST-WO (e) 2k4k11 (E,N,C) 9.8K ST-NE (e) 2k4k11 (E,N,C) 9.8K ST-LM (e) 2k4k11 (E,N,C) 9.8K (e) 2k4k12 (E,N,C) 31K ST-SE (a) Gururangan et al. (2018) (b) Liu et al. (2020) (c) McCoy et al. (2019) (d) Nie et al. (2019) (e) Naik et al. (2018) (f) Glockner et al. (2018) (g) Wang et al. (2019c) (h) Minervini and Riedel Category First-level Second-level 1 (I) Partial input heuristics 2 (I) Inter-sentence heuristics 3 (II) Instance selection 4 (II) Single Sentence Modification 5 (II) Sentence Pair Modification 6 (II) Sentence Pair Swapping 7 (III) Lexical Statistical Irregularity 8 (III) Syntactic Statistical Irregularity 9 (III) Lexical Inference 10 (III) First Order Logic 11 (III) Stress Test - Distraction Test (III) Stress Test - Noise Test 12 (I) Where are the heuristics? (II) How did the dataset constructed? (III) Which aspect did the dataset detect? NLI d"
2020.conll-1.48,D17-1070,0,0.0225944,"early all test sets except ANLI get higher scores by using higher layers. On ANLI, the performance of the first 4 layers are close to random guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experim"
2020.conll-1.48,N18-2017,0,0.0324523,"Missing"
2020.conll-1.48,D19-6115,0,0.0249291,"sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed efficient methods to mitigate a particular known bias in NLI. Benchmark collection in NLI: GLUE (Wang et al., 2019b,a) benchmark contains several NLIrelated benchmark datasets. However it does"
2020.conll-1.48,N19-1090,0,0.0327223,"Missing"
2020.conll-1.48,N18-1170,0,0.0238641,"sarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019)"
2020.conll-1.48,P19-1334,0,0.207308,". Finally, we investigate several methods to merge heterogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models. 1 Introduction Natural language inference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2005, 2013). Recent works have found that NLI models are sensitive to the compositional features (Nie et al., 2019), syntactic heuristics (McCoy et al., 2019), stress test (Geiger et al., 2018; Naik et al., 2018) and human artifacts in the data collection phase (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). ∗ 1 In this paper, we use the term ‘bias’ to refer to these known dataset biases in NLI following Clark et al. (2019). In other context, ‘bias’ may refer to systematic mishandling of gender or evidences of racial stereotypes (Rudinger et al., 2017) in NLI datasets or models. Equal contribution. 596 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 596–608 c Online, November 19-20, 2020. 2020 A"
2020.conll-1.48,K18-1007,0,0.0792749,"remise sentences to obtain the ’lexically misleading scores (LMS)’ for each instance in the test sets. We use CS0.7 in their paper which denotes the subsets whose LMS are larger that 0.7. 2.1.3 Logical Inference Ability (LI) Lexical Inference Test (LI-LI): A proper NLI system should recognize hypernyms and hyponyms; synonym and antonyms. We merge the “antonym” category in Naik et al. (2018) and Glockner et al. (2018) to assess the models’ capability to model lexical inference. Text-fragment Swap Test (LI-TS): NLI system should also follow the first-order logic constraints (Wang et al., 2019c; Minervini and Riedel, 2018). For example, if the premise sentence sp entails the hypothesis sentence sh , then sh must not be contradicted by sp . We then swap the two sentences in the original MultiNLI mismatched dev sets. If the gold label is ‘contradiction’, the corresponding label in the swapped instance remains unchanged, otherwise it becomes ‘non-contradicted’. 2.1.4 Insights within Adversarial Tests Stress Test (ST) 2.2 We also include the “word overlap” (ST-WO), “negation” (ST-NE), “length mismatch” (ST-LM) and “spelling errors” (ST-SE) in Naik et al. (2018), in which ST-WO aims at detecting lexical overlap heur"
2020.conll-1.48,C18-1198,0,0.285393,"rogeneous training data (1.35M) and perform model ensembling, which are straightforward but effective to strengthen NLI models. 1 Introduction Natural language inference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2005, 2013). Recent works have found that NLI models are sensitive to the compositional features (Nie et al., 2019), syntactic heuristics (McCoy et al., 2019), stress test (Geiger et al., 2018; Naik et al., 2018) and human artifacts in the data collection phase (Gururangan et al., 2018; Poliak et al., 2018b; Tsuchiya, 2018). ∗ 1 In this paper, we use the term ‘bias’ to refer to these known dataset biases in NLI following Clark et al. (2019). In other context, ‘bias’ may refer to systematic mishandling of gender or evidences of racial stereotypes (Rudinger et al., 2017) in NLI datasets or models. Equal contribution. 596 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 596–608 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.o"
2020.conll-1.48,W19-5333,0,0.0613467,"Missing"
2020.conll-1.48,N15-1098,0,0.0322294,"model ensemble, and benchmark these methods on various adversarial and general purpose datasets. Our findings suggest model-level MoE ensemble, text swap DA and performance based dataset merging would effectively combat multiple (though not all) distinct biases. Although we haven’t found a debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics betwe"
2020.conll-1.48,2020.lrec-1.846,1,0.589335,"ls. Equal contribution. 596 Proceedings of the 24th Conference on Computational Natural Language Learning, pages 596–608 c Online, November 19-20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 Datasets Paper Categories Labels Size PI-CD (a) 1k3k7 (E,N,C) 3.2k PI-SP (b) 1k3k7 (E,N,C) .37k IS-SD (c) 2k5k8 (¬E, E) 30k IS-CS (d) 2k3k7 (E,N,C) .65k (e)(f) 2k4k9 (E,C) 9.9K LI-LI LI-TS (g)(h) 2k6k10 (¬C, C) 9.8K ST-WO (e) 2k4k11 (E,N,C) 9.8K ST-NE (e) 2k4k11 (E,N,C) 9.8K ST-LM (e) 2k4k11 (E,N,C) 9.8K (e) 2k4k12 (E,N,C) 31K ST-SE (a) Gururangan et al. (2018) (b) Liu et al. (2020) (c) McCoy et al. (2019) (d) Nie et al. (2019) (e) Naik et al. (2018) (f) Glockner et al. (2018) (g) Wang et al. (2019c) (h) Minervini and Riedel Category First-level Second-level 1 (I) Partial input heuristics 2 (I) Inter-sentence heuristics 3 (II) Instance selection 4 (II) Single Sentence Modification 5 (II) Sentence Pair Modification 6 (II) Sentence Pair Swapping 7 (III) Lexical Statistical Irregularity 8 (III) Syntactic Statistical Irregularity 9 (III) Lexical Inference 10 (III) First Order Logic 11 (III) Stress Test - Distraction Test (III) Stress Test - Noise Test 12 (I) Where are the he"
2020.conll-1.48,2020.acl-main.441,0,0.171242,"an ensemble method that is a product of experts pˆi = sof tmax(log(pi ) + log(bi )). By doing so, the prime model would be encouraged to learn all the information except the specific bias. An intuitive justification from the probabilistic view can be found in Clark et al. (2019). Note that while training, only the prime model is updated while the bias-only model remains unchanged. 3 3.2 2018) in our testing. Training Resources: apart from SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018), we also incorporate Diverse NLI (DNLI) (Poliak et al., 2018a) and Adversarial NLI (ANLI) (Nie et al., 2020) datasets for training. For DNLI, we merge the subsets to form unified train/valid/test sets. Dataset Statistics are shown in Table 2. 2.3 Model Performance on the Benchmark Mixture of Experts (MoE) Debiasing Word overlap heuristics: To combat the word overlap heuristics (HANS (McCoy et al., 2019), renamed as IS-SD in Sec 2.1.2), Clark et al. (2019) used the following features to train a biasonly model: (1) whether the hypothesis is a subsequence of the premise, (2) whether all words in the hypothesis appear in the premise, (3) the percent of words from the hypothesis that appear in the premis"
2020.conll-1.48,2021.ccl-1.108,0,0.106003,"Missing"
2020.conll-1.48,D16-1244,0,0.094087,"Missing"
2020.conll-1.48,D14-1162,0,0.0845486,"Missing"
2020.conll-1.48,D18-1534,0,0.0152222,"debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed th"
2020.conll-1.48,N18-1202,0,0.031295,"scores by using higher layers. On ANLI, the performance of the first 4 layers are close to random guess while that of higher layers is about 4 point lower than random guess. 54.2 54.8 56.7 27.7 27.4 28.0 80.0 79.9 80.3 83.5 83.4 83.5 Table 8: The performance of BERT base model under different model selection strategies. 6 6.1 Experimental Settings Implementation Details We set up both pretrained and non-pretrained model baselines for the proposed evaluation bechmarks. We rerun their public available codebases (Wolf et al., 2019), including InferSent (Conneau et al., 2017) 6 (w/ and w/o Elmo (Peters et al., 2018)), DAM (Parikh et al., 2016) 7 , ESIM (Chen et al., 2017)8 , BERT (uncased) (Devlin et al., 2019), XLNet (cased) (Yang et al., 2019) and RoBERTa (Liu et al., 2019), 9 . we map the vector at the position of the ‘[CLS]’ token in the pretrained models to three-way NLI classification via linear transformation. We show the per-layer analyses for RoBERTa model in Table 2. We try to reduce the randomness of our experiments by 3 runs using different random seeds. We report the median of the 3 runs for all the tables except the ensemble-related (Sec 5.2) experiments in Table 6. Table 7 shows how we eva"
2020.conll-1.48,K17-1004,0,0.0191586,"ndings suggest model-level MoE ensemble, text swap DA and performance based dataset merging would effectively combat multiple (though not all) distinct biases. Although we haven’t found a debiasing strategy that can guarantee the NLI models to be more robust on every adversarial dataset used in this paper, we leave the question of whether such a debiasing method exists for future research. Related Work Bias in NLI: The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan"
2020.conll-1.48,W18-5441,0,0.0395994,"Missing"
2020.conll-1.48,L18-1239,0,0.0471816,"Missing"
2020.conll-1.48,S18-2023,0,0.0335729,"Missing"
2020.conll-1.48,K19-1033,0,0.0257256,"Missing"
2020.conll-1.48,D18-1187,0,0.0260372,"Missing"
2020.conll-1.48,W17-1609,0,0.0514757,"Missing"
2020.conll-1.48,N18-1179,0,0.020067,"tations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed efficient methods to mitigate a particular known bias in NLI. Benchmark collection in NLI: GLUE (Wang et al., 2019b,a) benchmark contains several NLIrelated benchmark datasets. However it does not include adversarial test sets, domain specific test (Romanov and Shivade, 2018; Ravichander et al., Acknowledgments We w"
2020.conll-1.48,P18-1042,0,0.0373074,"Missing"
2020.conll-1.48,N18-1101,0,0.0188457,"h k examples is a weighted P sum of instance-level loss li : Lbatch = αi ∗ li /( ki=1 αi ). Bias Product Ensemble: an ensemble method that is a product of experts pˆi = sof tmax(log(pi ) + log(bi )). By doing so, the prime model would be encouraged to learn all the information except the specific bias. An intuitive justification from the probabilistic view can be found in Clark et al. (2019). Note that while training, only the prime model is updated while the bias-only model remains unchanged. 3 3.2 2018) in our testing. Training Resources: apart from SNLI (Bowman et al., 2015), and MultiNLI (Williams et al., 2018), we also incorporate Diverse NLI (DNLI) (Poliak et al., 2018a) and Adversarial NLI (ANLI) (Nie et al., 2020) datasets for training. For DNLI, we merge the subsets to form unified train/valid/test sets. Dataset Statistics are shown in Table 2. 2.3 Model Performance on the Benchmark Mixture of Experts (MoE) Debiasing Word overlap heuristics: To combat the word overlap heuristics (HANS (McCoy et al., 2019), renamed as IS-SD in Sec 2.1.2), Clark et al. (2019) used the following features to train a biasonly model: (1) whether the hypothesis is a subsequence of the premise, (2) whether all words in"
2020.conll-1.48,P19-1435,0,0.0193032,"sks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017; Schwartz et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses(Nie et al., 2019; Dasgupta et al., 2018), data permutations (Schluter and Varab, 2018; Wang et al., 2019c) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). Other evidences of artifacts include sentence occurrence (Zhang et al., 2019), syntactic heuristics between hypotheses and premises (McCoy et al., 2019) and black-box clues derived from neural models (Gururangan et al., 2018; Poliak et al., 2018b; He et al., 2019). Rudinger et al. (2017) showed hypotheses in SNLI has the evidence of gender, racial stereotypes, etc. Sanchez et al. (2018) analysed the behaviour of NLI models and the factors to be more robust. Feng et al. (2019) discussed how to use partial-input baseline in future dataset creation. Belinkov et al. (2019); Clark et al. (2019); He et al. (2019); Yaghoobzadeh et al. (2019); Ding et al. (2020) proposed effic"
2020.emnlp-main.127,P19-1279,0,0.0281002,"t and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involved in a relation may appear in different sentences. Therefore a relati"
2020.emnlp-main.127,D19-1498,0,0.744909,"Ve/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Results We show GAIN’s performance on the DocRED dataset in Table 2, in comparison with other baselines. Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9 ∼ 12.82 F1 score on the test set. Among the models using BERT"
2020.emnlp-main.127,N19-1423,0,0.0191511,"GloVe (100d) and BiLSTM (256d) as word embedding and encoder. GAINBERTbase and GAIN-BERTlarge use BERTbase and BERTlarge as encoder respectively and the learning rate is set to 1e−5 . 4.3 Baselines and Evaluation Metrics We use the following models as baselines. Yao et al. (2019) proposed models to encode the document into a sequence of hidden state vector {hi }ni=1 using CNN (Fukushima, 1980), LSTM (Hochreiter and Schmidhuber, 1997), and BiLSTM (Schuster and Paliwal, 1997) as their encoder, and predict relations between entities with their representations. Other pre-trained models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and CorefBERT (Ye et al., 2020) are also used as encoder (Wang et al., 2019a; Ye et al., 2020) to document-level RE task. Context-Aware, also proposed by Yao et al. (2019) on DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe"
2020.emnlp-main.127,P19-1024,0,0.0534732,". (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Results We show GAIN’s performance on the DocRED dataset in Table 2, in comparison with other baselines. Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9 ∼ 12.82 F1 score on the test set. Among the models using BERT or BERT variants, GAINBERTbas"
2020.emnlp-main.127,P82-1020,0,0.80706,"Missing"
2020.emnlp-main.127,N19-1370,0,0.062618,"Missing"
2020.emnlp-main.127,2021.ccl-1.108,0,0.0767001,"Missing"
2020.emnlp-main.127,P16-2022,0,0.0280836,"ion function. With this module, an entity can be represented by fusing information from its mentions, which usually spread in multiple sentences. Moreover, potential reasoning clues are modeled by different paths between entities. Then they can be integrated with the attention mechanism so that we will take into account latent logical reasoning chains to predict relations. 3.4 Classification Module For each entity pair (eh , et ), we concatenate the following representations: (1) the head and tail entity representation eh and et derived in the Entity-level Graph, with the comparing operation (Mou et al., 2016) to strengthen features, i.e., absolute value of subtraction between the representation of two entities, |eh − et |, and element-wise multiplication, eh et ; (2) the representation of document node in Mention-level Graph, mdoc , as it can help aggregate cross-sentence information and provide document-aware representation; (3) the comprehensive inferential path information ph,t . Ih,t = [eh ; et ; |eh − et |; eh et ; mdoc ; ph,t ] (11) Finally, we formulate the task as multi-label classification task and predict relations between entities: P (r|eh , et ) = sigmoid (Wb σ(Wa Ih,t + ba ) + bb ) (1"
2020.emnlp-main.127,2020.acl-main.141,0,0.608448,"utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Resu"
2020.emnlp-main.127,Q17-1008,0,0.141436,"te Performer Part of May 26, 2002 Without Me Publication Date Figure 3: The case study of our proposed GAIN and baseline models. The models take the document as input and predict relations among different entities in different colors. We only show a part of entities within the documents and the according sentences due to the space limitation. inevitable restriction in practice, where many realworld relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction. Several approaches (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019) leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. Recently, many models are proposed to address this problem. Tang et al. (2020) proposed a hierarchical inference network by considering information from entity-level, sentence-level, and documentlevel. However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way. (Christopoulou et al., 2019) is one of the mos"
2020.emnlp-main.127,D14-1162,0,0.0847802,"DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1"
2020.emnlp-main.127,P19-1617,1,0.604441,"Baltimore； Eldersburg Object: Maryland relation: located in the administrative territorial entity Subject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several"
2020.emnlp-main.127,E17-1110,0,0.112741,"on Date Publication Date Performer Part of May 26, 2002 Without Me Publication Date Figure 3: The case study of our proposed GAIN and baseline models. The models take the document as input and predict relations among different entities in different colors. We only show a part of entities within the documents and the according sentences due to the space limitation. inevitable restriction in practice, where many realworld relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction. Several approaches (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019) leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. Recently, many models are proposed to address this problem. Tang et al. (2020) proposed a hierarchical inference network by considering information from entity-level, sentence-level, and documentlevel. However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way. (Christopoulou et al., 2019"
2020.emnlp-main.127,P19-1423,0,0.333231,"target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word embedding. LSR-GloVe/LSR-BERTbase , proposed by Nan et al. (2020) recently. They construct a graph based on the dependency tree and predict relations by latent structure induction and GCN. Nan et al. (2020) also adapted four graph-based state-of-the-art RE models to DocRED, including GAT (Velickovic et al., 2017), GCNN (Sahu et al., 2019), EoG (Christopoulou et al., 2019), and AGGCN (Guo et al., 2019). We also include their results. Following Yao et al. (2019), we use the widely used metrics F1 and AUC in our experiment. We also use Ign F1 and Ign AUC, which calculate F1 and AUC excluding the common relation facts in the training and dev/test sets. 4.4 Results We show GAIN’s performance on the DocRED dataset in Table 2, in comparison with other baselines. Among the models not using BERT or BERT variants, GAIN-GloVe consistently outperforms all sequential-based and graph-based strong baselines by 0.9 ∼ 12.82 F1 score on the tes"
2020.emnlp-main.127,D18-1246,0,0.0345295,"ut Me Publication Date Figure 3: The case study of our proposed GAIN and baseline models. The models take the document as input and predict relations among different entities in different colors. We only show a part of entities within the documents and the according sentences due to the space limitation. inevitable restriction in practice, where many realworld relation facts can only be extracted across sentences. Therefore, many researchers gradually shift their attention into document-level relation extraction. Several approaches (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019) leverage dependency graph to better capture document-specific features, but they ignore ubiquitous relational inference in document. Recently, many models are proposed to address this problem. Tang et al. (2020) proposed a hierarchical inference network by considering information from entity-level, sentence-level, and documentlevel. However, it conducts relational inference implicitly based on a hierarchical network while we adopt the path reasoning mechanism, which is a more explicit way. (Christopoulou et al., 2019) is one of the most powerful systems on document-level RE"
2020.emnlp-main.127,D17-1188,0,0.0828932,"ls as baselines. Yao et al. (2019) proposed models to encode the document into a sequence of hidden state vector {hi }ni=1 using CNN (Fukushima, 1980), LSTM (Hochreiter and Schmidhuber, 1997), and BiLSTM (Schuster and Paliwal, 1997) as their encoder, and predict relations between entities with their representations. Other pre-trained models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and CorefBERT (Ye et al., 2020) are also used as encoder (Wang et al., 2019a; Ye et al., 2020) to document-level RE task. Context-Aware, also proposed by Yao et al. (2019) on DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical Inference Network (HIN) aggregate information from entity-level, sentence-level, and document-level to predict target relations, and use GloVe (Pennington et al., 2014) or BERTbase for word em"
2020.emnlp-main.127,N18-1080,0,0.158726,"se an attention mechanism to selectively fuse all possible path information for the entity pair while without extra overhead. When we were writing this paper, (Nan et al., 2020) make their work public as preprints, which adopt the dependency tree to capture the semantic information in the document. They put mention and entity nodes in the same graph and conduct inference implicitly by using GCN. Unlike their work, our GAIN presents mention node and entity node in different graphs to better conduct inter-sentence information aggregation and infer relations more explicitly. Some other attempts (Verga et al., 2018; Sahu et al., 2019; Christopoulou et al., 2019) study document-level RE in a specific domain like biomedical RE. However, the datasets they use usually contain very limited relation types and entity types. For instance, CDR (Li et al., 2016) only has one type of relation and two types of entities, which may not be the ideal testbed for relational reasoning. 1637 6 Conclusion Extracting inter-sentence relations and conducting relational reasoning are challenging in documentlevel relation extraction. In this paper, we introduce Graph Aggregationand-Inference Network (GAIN) to better cope with d"
2020.emnlp-main.127,P16-1123,0,0.217534,"Missing"
2020.emnlp-main.127,C16-1119,0,0.14292,"dersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involv"
2020.emnlp-main.127,P19-1074,0,0.555886,"h is transformed into Entity-level Graph, where the paths between entities are identified for reasoning. Finally, the classification module predicts target relations based on the above information. Different entities are in different colors. The number i in the mention node denotes that it belongs to the i-th sentence. tion module (Sec. 3.2), entity-level graph inference module (Sec. 3.3), classification module (Sec. 3.4), as is shown in Figure 2. 3.1 Encoding Module In the encoding module, we convert a document D = {wi }ni=1 containing n words into a sequence of vectors {gi }ni=1 . Following Yao et al. (2019), for each word wi in D, we first concatenate its word embedding with entity type embedding and coreference embedding: xi = [Ew (wi ); Et (ti ); Ec (ci )] (1) 3.2 Mention-level Graph Aggregation Module To model the document-level information and interactions between mentions and entities, a heterogeneous Mention-level Graph (MG) is constructed. MG has two different kinds of nodes: mention node and document node. Each mention node denotes one particular mention of an entity. And MG also has one document node that aims to model the overall document information. We argue that this node could serv"
2020.emnlp-main.127,2020.emnlp-main.582,0,0.246993,"NBERTbase and GAIN-BERTlarge use BERTbase and BERTlarge as encoder respectively and the learning rate is set to 1e−5 . 4.3 Baselines and Evaluation Metrics We use the following models as baselines. Yao et al. (2019) proposed models to encode the document into a sequence of hidden state vector {hi }ni=1 using CNN (Fukushima, 1980), LSTM (Hochreiter and Schmidhuber, 1997), and BiLSTM (Schuster and Paliwal, 1997) as their encoder, and predict relations between entities with their representations. Other pre-trained models like BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and CorefBERT (Ye et al., 2020) are also used as encoder (Wang et al., 2019a; Ye et al., 2020) to document-level RE task. Context-Aware, also proposed by Yao et al. (2019) on DocRED adapted from (Sorokin and Gurevych, 2017), uses an LSTM to encode the text, but further utilizes attention mechanism to absorb the context relational information for predicting. BERT-Two-Stepbase , proposed by Wang et al. (2019a) on DocRED. Though similar to BERTREbase , it first predicts whether two entities have a relationship and then predicts the specific target relation. HIN-GloVe/HIN-BERTbase , proposed by Tang et al. (2020). Hierarchical"
2020.emnlp-main.127,P17-1053,0,0.0206069,"country Subject: Baltimore； Eldersburg Object: Maryland relation: located in the administrative territorial entity Subject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author"
2020.emnlp-main.127,D15-1203,0,0.268304,"ject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and ob"
2020.emnlp-main.127,C14-1220,0,0.382319,"ritorial entity Subject: Baltimore； Eldersburg Object: U.S. relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly,"
2020.emnlp-main.127,D18-1244,0,0.100492,"Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involved in a relation may appear in different"
2020.emnlp-main.127,D17-1004,0,0.222047,". relation: country Figure 1: An example document and its desired relations from DocRED (Yao et al., 2019). Entity mentions and relations involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction The task of identifying semantic relations between entities from text, namely relation extraction (RE), plays a crucial role in a variety of knowledge-based applications, such as question answering (Yu et al., 2017, Qiu et al., 2019) and large-scale knowledge graph construction. Previous methods (Zeng et al., 2014; Zeng et al., 2015; Xiao and Liu, 2016; Zhang et al., 2017; Zhang et al., 2018; Baldini Soares et al., 2019) focus on sentence-level RE, which predicts relations among entities in a single sentence. However, sentence-level RE models suffer from an inevitable limitation – they fail to recognize relations between entities across sentences. Hence, extracting relations at the document-level is necessary for a holistic understanding of knowledge in text. ∗ † Equal contribution. Corresponding author. There are several major challenges in effective relation extraction at the document-level. Firstly, the subject and object entities involved in a relation may"
2020.emnlp-main.127,P16-2034,0,0.0257981,"dy of our proposed model GAIN, in comparison with other baselines. As is shown, BiLSTM can only identify two relations within the first sentence. Both BERT-REbase and GAIN-BERTbase can successfully predict Without Me is part of The Eminem Show. But only GAIN-BERTbase is able to deduce the performer and publication date of Without Me are the same as those of The Eminem Show, namely Eminem and May 26, 2002, where it requires logical inference across sentences. 5 Related Work Previous approaches focus on sentence-level relation extraction (Zeng et al., 2014; Zeng et al., 2015; Wang et al., 2016; Zhou et al., 2016; Xiao and Liu, 2016; Zhang et al., 2017; Feng et al., 2018; Zhu et al., 2019). But sentence-level RE models face an 1636 [1] The Eminem Show is the fourth studio album by American rapper Eminem, released on May 26, 2002 by Aftermath Entertainment, Shady Records, and Interscope Records. [2] The Eminem Show includes the commercially successful singles &quot;Without Me&quot;, &quot;Cleanin’ Out My Closet&quot;, &quot;Superman&quot;, and &quot;Sing for the Moment&quot;.… Performer BiLSTM Eminem Performer The Eminem Show Publication Date Without Me BERTRE The Eminem Show Publication Date May 26, 2002 Eminem Performer GAINBERT The Eminem"
2020.emnlp-main.127,P19-1128,0,0.432377,", BiLSTM can only identify two relations within the first sentence. Both BERT-REbase and GAIN-BERTbase can successfully predict Without Me is part of The Eminem Show. But only GAIN-BERTbase is able to deduce the performer and publication date of Without Me are the same as those of The Eminem Show, namely Eminem and May 26, 2002, where it requires logical inference across sentences. 5 Related Work Previous approaches focus on sentence-level relation extraction (Zeng et al., 2014; Zeng et al., 2015; Wang et al., 2016; Zhou et al., 2016; Xiao and Liu, 2016; Zhang et al., 2017; Feng et al., 2018; Zhu et al., 2019). But sentence-level RE models face an 1636 [1] The Eminem Show is the fourth studio album by American rapper Eminem, released on May 26, 2002 by Aftermath Entertainment, Shady Records, and Interscope Records. [2] The Eminem Show includes the commercially successful singles &quot;Without Me&quot;, &quot;Cleanin’ Out My Closet&quot;, &quot;Superman&quot;, and &quot;Sing for the Moment&quot;.… Performer BiLSTM Eminem Performer The Eminem Show Publication Date Without Me BERTRE The Eminem Show Publication Date May 26, 2002 Eminem Performer GAINBERT The Eminem Show Publication Date May 26, 2002 Part of Without Me May 26, 2002 Eminem Per"
2020.emnlp-main.32,N19-1423,0,0.00705487,"comparison purposes, we consider the following three strategies of building sentence embeddings. Tf-isf: the simple tf-idf model with a finer granularity. More details can be found in Wan et al. (2007) and Wang et al. (2017). ESE: the enhanced feature embedding model (Yang et al., 2019). The embedding of each sentence is the concatenation of all components: paragraph vector, positional embedding and three feature embeddings (namely word-part-of-speech, bigram and trigram). BERT: the sentence encoder that learns vector representations by pre-training a deep bi-directional Transformer network (Devlin et al., 2019). The advantage is that BERT is context-sensitive when considering the word embedding. Notice that the leading sentences in each document should have priority in the summary extraction. For injecting this knowledge, aij is multiplied 437 by the average positional weight 1/(oi +oj ). This can differentiate the sentences across documents and preserve the symmetry of A. 2.4 Justifications of Hypothesis We validate our spectral-based hypothesis by the following three complementary perspectives: Semantic scaling: dominant eigenvalue of affinity matrix determines the vector scaling in semantic space"
2020.emnlp-main.32,P19-1102,0,0.477734,"nts about the same news event. In addition, four human-written summaries are offered for each cluster to be the reference (golden) summary. Yelp3 : an all-purpose dataset that can be utilized for MDS. We only use the subset that has the reference summary (the test split offered by Chu and Liu (2019)): 100 businesses (document clusters), each of which includes 8 reviews (documents). One reference summary was collected for each cluster using crowdsourcing. More details of building the dataset can be found in Chu and Liu (2019). Multi-News4 : a large-scale dataset collected from news aggregator (Fabbri et al., 2019). It has 5622 document clusters (in the test split offered by the original paper), and multiple documents are present 2 https://duc.nist.gov/duc2004/tasks.html https://www.yelp.com/dataset 4 https://github.com/Alex-Fabbri/Multi-News 3 1 The eigenvector v can be of arbitrary length, which differs from the normalized vector u in Eq. (3). 439 in each cluster. Furthermore, each cluster is attached with one human-written reference summary. DUC 2004 Domain #Clusters #Docs per cluster #Ref. per cluster #Doc sources News 50 10 4 2 Yelp Business review 100 8 1 1 Multi-News News 5622 2∼10 1 >1500 Table"
2020.emnlp-main.32,hong-etal-2014-repository,0,0.0214492,"ncatenated embedding is 800). For the strategy BERT, we used the uncased BERTBase model6 pre-trained on Wikipedia, through bert-as-service7 to obtain the sentence embedding of 768 dimensions. All the experiments are performed on a machine with two CPUs (3.5GHz) and one GPU (16G memory). The extractive MDS methods need a length limit of summary to terminate the extraction of summary sentences. We adopt 100 words as the length limit in the DUC dataset, instead of 665 bytes specified by the official task. The change has also been made to provide the same setting for evaluating various methods in Hong et al. (2014) and Zheng et al. (2019). For the Yelp dataset, we set the limit to be the 99.5th percentile less than the maximum length of any document; for Multi-News, the limit is set as 300 words. The same settings have been adopted in Chu and Liu (2019) and Fabbri et al. (2019), respectively. 5 3.3 Evaluation Metrics We adopt ROUGE (Lin, 2004) as the automatic metric, which has been observed in a good agreement with human judgment (Owczarzak et al., 2012). It measures the overlap of N -grams (R-N) and skipbigrams with a maximum distance of four words (R-SU4). Also, it can be computed based on the longes"
2020.emnlp-main.32,W04-1013,0,0.0865664,"ry to terminate the extraction of summary sentences. We adopt 100 words as the length limit in the DUC dataset, instead of 665 bytes specified by the official task. The change has also been made to provide the same setting for evaluating various methods in Hong et al. (2014) and Zheng et al. (2019). For the Yelp dataset, we set the limit to be the 99.5th percentile less than the maximum length of any document; for Multi-News, the limit is set as 300 words. The same settings have been adopted in Chu and Liu (2019) and Fabbri et al. (2019), respectively. 5 3.3 Evaluation Metrics We adopt ROUGE (Lin, 2004) as the automatic metric, which has been observed in a good agreement with human judgment (Owczarzak et al., 2012). It measures the overlap of N -grams (R-N) and skipbigrams with a maximum distance of four words (R-SU4). Also, it can be computed based on the longest common subsequence (R-L). Each version of ROUGE has their scores oriented to recall, precision and F1. In the experiments, we report the different combinations of ROUGE scores for each dataset, which have been recommended and adopted by previous works. Specifically, the recall scores of R-1,2,4 will be reported for the DUC 2004 dat"
2020.emnlp-main.32,P11-1052,0,0.0470702,"Another hypothesis in the sparse-codingbased methods (Li et al., 2015b; Liu et al., 2015; Yao et al., 2015) regards the original sentences as a linear combination of summary sentences. This leads to an intuitive reconstruction, whereas linear combination is more a simplification than a necessity. Our proposed hypothesis offers a spectral viewpoint and will be explicitly validated on the real dataset. (2) Optimization objective. Multi-criteria optimization is suitable for MDS as various criteria (goals) exist in the task, such as relevancy criterion and non-redundancy criterion. For instance, Lin and Bilmes (2011) is a bi-criteria case that imposes the submodularity constraint on each criterion. Multi-criteria loss functions in neuralnetwork-based methods (Ma et al., 2016; Chu and Liu, 2019; Zheng et al., 2019) include the reconstruction errors from different spaces. In the above cases, the overall objective functions used include some hyperparameters for gluing singletons. Comparatively, our proposed objective (spectral impact) has a compact form. It avoids the hyperparameter setting and simulates the non-separable processing of multiple MDS criteria by human beings. (3) Model complexity. There is a t"
2020.emnlp-main.32,D18-1446,0,0.0297214,"Missing"
2020.emnlp-main.32,C16-1143,0,0.0433498,"Missing"
2020.emnlp-main.32,D17-1221,0,0.127261,"Comparing Methods We compare our method with both traditional and state-of-the-art MDS methods. Lead: The documents in a cluster are randomly shuffled, and the first sentence of the document is added to the summary until the length limit is reached. LexRank (Erkan and Radev, 2004): It performs the sentence relevancy estimation by the random walk process on the sentence graph. CLASSY04 (Conroy et al., 2004): It ranked first in the official evaluation of DUC 2004. As a supervised method, it uses a Hidden Markov Model to rank sentences and a QR decomposition to produce the summary. C-Attention (Li et al., 2017a): The cascaded attention based auto-encoder is proposed for estimating the relevancy of words and sentences. GRU-GCN (Yasunaga et al., 2017): It is a supervised method that employs a Graph Convolutional Network on sentence graph. The sentence embedding obtained from a Recurrent Neural Network serves as the input node feature. ParaFuse (Nayeem et al., 2018): MDS is formulated as multi-sentence compression. As the statehttps://tartarus.org/martin/PorterStemmer/ https://github.com/google-research/bert 7 https://github.com/hanxiao/bert-as-service 6 8 https://github.com/andersjo/pyrouge/tree/mast"
2020.emnlp-main.32,C18-1102,0,0.457535,"Missing"
2020.emnlp-main.32,W12-2601,0,0.0240031,"C dataset, instead of 665 bytes specified by the official task. The change has also been made to provide the same setting for evaluating various methods in Hong et al. (2014) and Zheng et al. (2019). For the Yelp dataset, we set the limit to be the 99.5th percentile less than the maximum length of any document; for Multi-News, the limit is set as 300 words. The same settings have been adopted in Chu and Liu (2019) and Fabbri et al. (2019), respectively. 5 3.3 Evaluation Metrics We adopt ROUGE (Lin, 2004) as the automatic metric, which has been observed in a good agreement with human judgment (Owczarzak et al., 2012). It measures the overlap of N -grams (R-N) and skipbigrams with a maximum distance of four words (R-SU4). Also, it can be computed based on the longest common subsequence (R-L). Each version of ROUGE has their scores oriented to recall, precision and F1. In the experiments, we report the different combinations of ROUGE scores for each dataset, which have been recommended and adopted by previous works. Specifically, the recall scores of R-1,2,4 will be reported for the DUC 2004 dataset according to Hong et al. (2014), Wang et al. (2017) and Zheng et al. (2019); the F1 scores of R-1,2,L will be"
2020.emnlp-main.32,W17-1003,0,0.0128996,"urrent Neural Network serves as the input node feature. ParaFuse (Nayeem et al., 2018): MDS is formulated as multi-sentence compression. As the statehttps://tartarus.org/martin/PorterStemmer/ https://github.com/google-research/bert 7 https://github.com/hanxiao/bert-as-service 6 8 https://github.com/andersjo/pyrouge/tree/master/tools/ ROUGE-1.5.5 440 of-the-art on DUC 2004, however, it needs some extra resource and toolkit, such as paraphrase bank and keyword extractor. Best Review (Chu and Liu, 2019): A simple baseline selecting the best document to be summary based on word overlap. Centroid (Rossiello et al., 2017): Word embeddings are exploited to boost the performance of centroid-based methods. MeanSum (Chu and Liu, 2019): An end-to-end neural model is put forward to implement the abstractive summarization of business review documents. The summary is decoded from the mean of the representations of input reviews. PG (See et al., 2017): It introduces a pointergenerator (PG) network that motivates the summarizer to copy original words from input via pointing, while preserving the ability to generate new words. Hi-MAP (Fabbri et al., 2019): It proposes the integration of sentence-level MMR scores into the"
2020.emnlp-main.32,P17-1099,0,0.0346665,"s/ ROUGE-1.5.5 440 of-the-art on DUC 2004, however, it needs some extra resource and toolkit, such as paraphrase bank and keyword extractor. Best Review (Chu and Liu, 2019): A simple baseline selecting the best document to be summary based on word overlap. Centroid (Rossiello et al., 2017): Word embeddings are exploited to boost the performance of centroid-based methods. MeanSum (Chu and Liu, 2019): An end-to-end neural model is put forward to implement the abstractive summarization of business review documents. The summary is decoded from the mean of the representations of input reviews. PG (See et al., 2017): It introduces a pointergenerator (PG) network that motivates the summarizer to copy original words from input via pointing, while preserving the ability to generate new words. Hi-MAP (Fabbri et al., 2019): It proposes the integration of sentence-level MMR scores into the PG network in order to adapt the attention weights on a word-level. The MMR score is computed by the Maximal Marginal Relevance algorithm (Carbonell and Goldstein, 1998), which gives the goodness of the available sentence given already selected ones. Our method Spectral: This is our spectral-based method specified in Alg. 2."
2020.emnlp-main.32,D17-1020,1,0.927428,"element in the affinity matrix A is a pairwise affinity of two different sentences. Since our hypothesis depends on A, a better MDS performance can be expected by adjusting the building of A. Sentence embeddings play a vital role in the process of building A, since affinity aij can be set to be the cosine similarity of the embeddings of sentences si and sj (i.e. aij = aji and aii = 0). For comparison purposes, we consider the following three strategies of building sentence embeddings. Tf-isf: the simple tf-idf model with a finer granularity. More details can be found in Wan et al. (2007) and Wang et al. (2017). ESE: the enhanced feature embedding model (Yang et al., 2019). The embedding of each sentence is the concatenation of all components: paragraph vector, positional embedding and three feature embeddings (namely word-part-of-speech, bigram and trigram). BERT: the sentence encoder that learns vector representations by pre-training a deep bi-directional Transformer network (Devlin et al., 2019). The advantage is that BERT is context-sensitive when considering the word embedding. Notice that the leading sentences in each document should have priority in the summary extraction. For injecting this"
2020.emnlp-main.32,K17-1045,0,0.0157188,"andomly shuffled, and the first sentence of the document is added to the summary until the length limit is reached. LexRank (Erkan and Radev, 2004): It performs the sentence relevancy estimation by the random walk process on the sentence graph. CLASSY04 (Conroy et al., 2004): It ranked first in the official evaluation of DUC 2004. As a supervised method, it uses a Hidden Markov Model to rank sentences and a QR decomposition to produce the summary. C-Attention (Li et al., 2017a): The cascaded attention based auto-encoder is proposed for estimating the relevancy of words and sentences. GRU-GCN (Yasunaga et al., 2017): It is a supervised method that employs a Graph Convolutional Network on sentence graph. The sentence embedding obtained from a Recurrent Neural Network serves as the input node feature. ParaFuse (Nayeem et al., 2018): MDS is formulated as multi-sentence compression. As the statehttps://tartarus.org/martin/PorterStemmer/ https://github.com/google-research/bert 7 https://github.com/hanxiao/bert-as-service 6 8 https://github.com/andersjo/pyrouge/tree/master/tools/ ROUGE-1.5.5 440 of-the-art on DUC 2004, however, it needs some extra resource and toolkit, such as paraphrase bank and keyword extra"
2020.emnlp-main.32,D19-1311,0,0.0399039,"Missing"
2020.emnlp-main.657,N10-1112,1,0.874309,"g step after estimating the generative classifier distributions. They used log loss as their discriminative objective. We also consider using a discriminative fine-tuning step when training our model, specifically we compare log loss to four other discriminative losses: • Perceptron loss: the loss function underlying the perceptron algorithm (Rosenblatt, 1958) • Hinge loss: the loss function underlying support vector machines (SVMs) and structured SVMs (Wahba et al., 1999; Taskar et al., 2004) • Softmax-margin: which combines log loss with a cost function as in hinge loss (Povey et al., 2008; Gimpel and Smith, 2010) • Bayes risk: the expectation of the cost function with respect to the model’s conditional distribution (Kaiser et al., 2000; Smith and Eisner, 2006) Table 1 shows these discriminative losses.2 Some losses use a cost function, which can be chosen by the practitioner to penalize different errors differently. In our experiments, we define it as cost(y, y 0 ) = 1 for y 6= y 0 and cost(y, y 0 ) = 0 if y = y 0 , where y is the gold label and y 0 is a candidate label. In addition, we introduce a very simple loss that is inspired by these other discriminative losses while performing quite well overa"
2020.emnlp-main.657,P18-2103,0,0.0686436,"ich first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most existing NLI models are trained in a dis8190 criminative manner by maximizing the conditional log-"
2020.emnlp-main.657,P16-1154,0,0.177104,"simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative classifiers have advantages over their discriminative counterparts in non-ideal conditions (Yogatama et al., 2017; Lewis and Fan, 2019; Ding and Gimpel, 2019). In this paper, we develop generative classifiers for NLI. Our model, which we call GenNLI, defines the conditional probability of the hypothesis given the premise and the label, parameterizing the distribution using a sequence-to-sequence model with attention (Luong et al., 2015) and a copy mechanism (Gu et al., 2016). We explore training objectives for discriminative fine-tuning of our generative classifiers, comparing several classical discriminative criteria. We find that several losses, including hinge loss and softmax-margin, outperform log loss fine-tuning used in prior work (Lewis and Fan, 2019) while similarly retaining the advantages of generative classifiers. We also find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our evaluation focuses on challenging experimental conditions: small training sets, imbalanced label distributions, and label n"
2020.emnlp-main.657,N18-2017,0,0.0373154,"uch methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most exi"
2020.emnlp-main.657,2021.ccl-1.108,0,0.178082,"Missing"
2020.emnlp-main.657,D15-1166,0,0.217154,"ing discriminative models, including both simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative classifiers have advantages over their discriminative counterparts in non-ideal conditions (Yogatama et al., 2017; Lewis and Fan, 2019; Ding and Gimpel, 2019). In this paper, we develop generative classifiers for NLI. Our model, which we call GenNLI, defines the conditional probability of the hypothesis given the premise and the label, parameterizing the distribution using a sequence-to-sequence model with attention (Luong et al., 2015) and a copy mechanism (Gu et al., 2016). We explore training objectives for discriminative fine-tuning of our generative classifiers, comparing several classical discriminative criteria. We find that several losses, including hinge loss and softmax-margin, outperform log loss fine-tuning used in prior work (Lewis and Fan, 2019) while similarly retaining the advantages of generative classifiers. We also find strong results with a simple unbounded modification to log loss, which we call the “infinilog loss”. Our evaluation focuses on challenging experimental conditions: small training sets, imba"
2020.emnlp-main.657,marelli-etal-2014-sick,0,0.421086,"n its discriminative analogue, logistic regression. Yogatama et al. (2017) compared the performance of generative and discriminative classifiers and showed the advantages of neural generative classifiers in terms of sample complexity, data shift, and zero-shot and continual learning settings. Ding and Gimpel (2019) further improved the performance of generative classifiers on document classification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased t"
2020.emnlp-main.657,P19-1334,0,0.0854708,"Missing"
2020.emnlp-main.657,K18-1007,0,0.0945586,"o sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural language texts, known as the premise and the hypothesis, and a label indicating the relation between the two texts. Formally, we denote an instance hx(p) , x(h) , yi as a tuple consisting of a premise (p) (p) (p) x(p) = {x1 , x2 , ..., xN }, a hypothesis x(h) = (h) (h) (h) {x1 , x2 , ..., xT }, and a label y ∈ Y . Most existing NLI models are trained in a dis8190 criminative manner by maximizing the conditional log-likelihood of the label give"
2020.emnlp-main.657,W17-5308,0,0.0232681,"ved the performance of generative classifiers on document classification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a na"
2020.emnlp-main.657,D16-1244,0,0.214468,"Missing"
2020.emnlp-main.657,D14-1162,0,0.0901545,"strong on standard leaderboards.5 3 While MRPC is a binary paraphrase classification task rather than an NLI or entailment task, we treat it as a binary entailment task by choosing one of the sentences arbitrarily as the premise and using the other as the hypothesis. 4 MRPC and RTE have no public test set, so we report their performances on the development sets. 5 GLUE leaderboard: https://gluebenchmark. com/leaderboard/; SNLI leaderboard: https:// nlp.stanford.edu/projects/snli/ Training Details Both generative and discriminative models are initialized with GloVe pretrained word embeddings (Pennington et al., 2014).6 The word embedding dimension and the LSTM hidden state dimension are set to 300. All parameters, including the word embeddings, are updated during training. The label embedding dimensionality for GenNLI is set to 100. All the experiments are conducted 5 times with different random seeds and we report the median scores. GenNLI. The training includes two steps: the model is first trained with the generative objective only (Equation 1) for 20 epochs, followed by the discriminative fine-tuning objective only (one of the objectives in Table 1) for 15 epochs. Unless otherwise specified, we use in"
2020.emnlp-main.657,N18-1202,0,0.0641951,"ent uses a BiLSTM network with max pooling (Collobert and Weston, 2008) to learn generic sentence embeddings that perform well on several NLI tasks. ESIM has a relatively complicated network structure, including a recursive architecture of local inference modeling (MacCartney, 2009; Parikh et al., 2016) and inference composition. The pretrained models we compare to are BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and XLNet (Yang et al., 2019). We select these models as our baselines because (1) they are open-source and are frequently used as baselines for NLI tasks in related work (Peters et al., 2018; Williams et al., 2018), and (2) their performance is strong on standard leaderboards.5 3 While MRPC is a binary paraphrase classification task rather than an NLI or entailment task, we treat it as a binary entailment task by choosing one of the sentences arbitrarily as the premise and using the other as the hypothesis. 4 MRPC and RTE have no public test set, so we report their performances on the development sets. 5 GLUE leaderboard: https://gluebenchmark. com/leaderboard/; SNLI leaderboard: https:// nlp.stanford.edu/projects/snli/ Training Details Both generative and discriminative models a"
2020.emnlp-main.657,S18-2023,0,0.0768613,"Missing"
2020.emnlp-main.657,D16-1264,0,0.0308844,"he training data shows severe label imbalance and when training labels are randomly corrupted. We additionally use GenNLI to generate hypotheses for given premises and labels. While the generations tend to have low diversity due to high lexical overlap with the premise, they are generally fluent and comport with the given labels, even in the small data setting. 2 2.1 Background and Related Work Generative Classifiers into the generative story. Lewis and Fan (2019) developed generative classifiers for question answering and achieved comparable performance to discriminative models on the SQuAD (Rajpurkar et al., 2016) dataset, and much better performance in challenging experimental settings. In this paper, we develop generative models for natural language inference inspired by models for sequence-to-sequence tasks. We additionally contribute an exploration of several discriminative objectives for fine-tuning our generative classifiers, finding multiple choices to outperform log loss used in prior work. We also compare our generative classifiers with fine-tuning of large-scale pretrained models, and characterize performance under other realistic settings such as imbalanced and noisy datasets. 2.2 While disc"
2020.emnlp-main.657,P06-2101,0,0.0910095,"minative fine-tuning step when training our model, specifically we compare log loss to four other discriminative losses: • Perceptron loss: the loss function underlying the perceptron algorithm (Rosenblatt, 1958) • Hinge loss: the loss function underlying support vector machines (SVMs) and structured SVMs (Wahba et al., 1999; Taskar et al., 2004) • Softmax-margin: which combines log loss with a cost function as in hinge loss (Povey et al., 2008; Gimpel and Smith, 2010) • Bayes risk: the expectation of the cost function with respect to the model’s conditional distribution (Kaiser et al., 2000; Smith and Eisner, 2006) Table 1 shows these discriminative losses.2 Some losses use a cost function, which can be chosen by the practitioner to penalize different errors differently. In our experiments, we define it as cost(y, y 0 ) = 1 for y 6= y 0 and cost(y, y 0 ) = 0 if y = y 0 , where y is the gold label and y 0 is a candidate label. In addition, we introduce a very simple loss that is inspired by these other discriminative losses while performing quite well overall in our experiments. We call it the infinilog loss and define it as 2 Again, the label prior p(y) ends up canceling out because it is uniform over l"
2020.emnlp-main.657,W18-5446,0,0.0415137,"Missing"
2020.emnlp-main.657,N18-1101,0,0.554735,"erimental settings, including small training sets, imbalanced label distributions, and label noise. 1 Introduction Natural language inference (NLI) is the task of identifying the relationship between two fragments of text, called the premise and the hypothesis (Dagan et al., 2005; Dagan et al., 2013). The task was originally defined as binary classification, in which the labels are entailment (the premise implies the hypothesis) or not entailment. Subsequent variations added a third contradiction label. Most models for NLI are trained and evaluated on standard benchmarks (Bowman et al., 2015; Williams et al., 2018; Wang et al., 2018) in a discriminative manner (Conneau et al., 2017; Chen et al., 2017a). These benchmarks typically have relatively clean, balanced, and abundant annotated data, and there ∗ † Equal contribution. Contribution during visiting TTIC. is no distribution shift between the training and test sets. However, when data quality and conditions are not ideal, there is a substantial performance decrease for existing discriminative models, including both simple model architectures and more complex ones. Prior work on document classification and question answering has shown that generative"
2020.emnlp-main.657,D18-1408,1,0.849005,"ssification by introducing discrete latent variables Natural Language Inference Early methods for NLI mainly relied on conventional, feature-based methods trained from smallscale datasets (Dagan et al., 2013; Marelli et al., 2014). The release of larger datasets, such as SNLI, made neural network methods feasible. Such methods can be roughly categorized into two classes: sentence embedding bottleneck methods which first encode the two sentences as vectors and then feed them into a classifier for classification (Conneau et al., 2017; Nie and Bansal, 2017; Choi et al., 2018; Chen et al., 2017b; Wu et al., 2018), and more general methods which usually involve interactions while encoding the two sentences in the pair (Chen et al., 2017a; Gong et al., 2018; Parikh et al., 2016). Recently, NLI models are shown to be biased towards spurious surface patterns in the human annotated datasets (Poliak et al., 2018; Gururangan et al., 2018; Liu et al., 2020a), which makes them vulnerable to adversarial attacks (Glockner et al., 2018; Minervini and Riedel, 2018; McCoy et al., 2019; Liu et al., 2020b). 3 A Generative Classifier for NLI Each example in a natural language inference dataset consists of two natural"
2020.lrec-1.846,P19-1084,0,0.0464383,"Missing"
2020.lrec-1.846,P17-2097,0,0.111928,"he easy and hard sets as it dynamically adjusts the debiasing strategies (i.e. the weight of training instances in Eq 5 and 6). 4. 5. Acknowledgments We would like to thank the anonymous reviewers for their valuable suggestions. This work is supported by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) di"
2020.lrec-1.846,P17-1152,0,0.0770692,"train the encoders, discriminator and classifier in Eq 1 together with a gradient reversal layer (Ganin et al., 2016) as shown in Fig 1. We negate the gradients from the discriminator D (red arrow in Fig 1) to push the hypothesis encoder Eh to the opposite direction while update its parameters. The usage of gradient reversal layer makes it easier to optimize the min-max game in Eq 1 (Xie et al., 2017; 10 It would be more challenging to manipulate the gradients in the non-sentence vector-based models, e.g. models which contain interactions between hypothesis and premise sentence encoders like (Chen et al., 2017a). We leave this to the future work. 11 https://nlp.stanford.edu/projects/snli/ 6856 Chen et al., 2018) than training the two adversarial components alternately like Generative Adversarial Nets (GANs) (Goodfellow et al., 2014). We update the model parameters θ by gradient descending (m is the batch size): new old θD = θD − new old θC = θC − new old θE = θE − h h m 1 X ∇θ [log pD (y i |Eh (xih ))] m i=1 D (2) m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] (3) m i=1 C m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] m i=1 Eh m (4) γ X ∇θ [log pD (y i |Eh (xih ))] m i=1 Eh | {z } + gradient reverse"
2020.lrec-1.846,Q18-1039,0,0.066773,"et al., 2016) as shown in Fig 1. We negate the gradients from the discriminator D (red arrow in Fig 1) to push the hypothesis encoder Eh to the opposite direction while update its parameters. The usage of gradient reversal layer makes it easier to optimize the min-max game in Eq 1 (Xie et al., 2017; 10 It would be more challenging to manipulate the gradients in the non-sentence vector-based models, e.g. models which contain interactions between hypothesis and premise sentence encoders like (Chen et al., 2017a). We leave this to the future work. 11 https://nlp.stanford.edu/projects/snli/ 6856 Chen et al., 2018) than training the two adversarial components alternately like Generative Adversarial Nets (GANs) (Goodfellow et al., 2014). We update the model parameters θ by gradient descending (m is the batch size): new old θD = θD − new old θC = θC − new old θE = θE − h h m 1 X ∇θ [log pD (y i |Eh (xih ))] m i=1 D (2) m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] (3) m i=1 C m 1 X ∇θ [log pC (y i |Eh (xih ), Ep (xip ))] m i=1 Eh m (4) γ X ∇θ [log pD (y i |Eh (xih ))] m i=1 Eh | {z } + gradient reverse 3.2.2. Guidance from Artificial Patterns The artificial patterns turns out to be useful guidances for bo"
2020.lrec-1.846,D19-1418,0,0.0638623,"Missing"
2020.lrec-1.846,D17-1070,0,0.0606994,"ence, we wonder if it is possible to get rid of these biases via debiasing the hypothesis sentence vector. More specifically, we focus on the ‘sentence vector-based models’ 10 category as defined on SNLI’s web page11 . Notably the idea of debiasing NLI via adversarial training has been proposed before (Belinkov et al., 2019; Belinkov et al., 2018). We hereby briefly introduce how we implement our adversarial training and how we incorporate instance reweighting method in this framework. In the following experiments, we use the full training sets without any down-sampling. We use the InferSent (Conneau et al., 2017) (biLSTM with max pooling) model as the benchmark sentence encoder. 3.2.1. Adversarial Debiasing Framework As shown in Fig 1, given the outputs sh = Eh (xh ), sp = Es (xs ) of hypothesis and premise encoders Eh , Ep , we are interested in predicting the NLI label y using a classifier C, pC (y|sh = Eh (xh ), sp = Es (xs )). In addition, we train a hypothesis-only discriminator trying to predict the correct label y solely from the hypothesis sentence representation sh by modeling pD (y|sh = Eh (xh )). We formulate the training process in the adversarial setting by a min-max game. Specifically we"
2020.lrec-1.846,P19-1554,0,0.572229,"true (entailment) or false (contradiction) given the premise, or whether the truth value can not be inferred (neutral). A proper NLI decision should apparently rely on both the premise and the hypothesis. However, some recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have shown that it is possible for a trained model to identify the true label by only looking at the hypothesis without observing the premise. The phenomenon is referred to as annotation artifacts (Gururangan et al., 2018), statistical irregularities (Poliak et al., 2018) or partial-input heuristics (Feng et al., 2019). In this paper we use the term hypothesis-only bias (Poliak et al., 2018) to refer to this phenomenon. Such hypothesis-only bias originates from the human annotation process of data collection. In the data collection process of many large-scale NLI datasets such as SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018), human annotators are required to write new sentences (hypotheses) based on the given premise and a specified label among entailment, contradiction and neutral. Some of the human-elicited hypotheses contain patterns that spuriously correlate to some specific labels. Fo"
2020.lrec-1.846,P18-2103,0,0.0545727,"a under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve superb performance on SNL"
2020.lrec-1.846,N18-2017,0,0.15606,"ction Natural language inference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2006; Dagan et al., 2013). NLI models are usually required to determine whether a hypothesis is true (entailment) or false (contradiction) given the premise, or whether the truth value can not be inferred (neutral). A proper NLI decision should apparently rely on both the premise and the hypothesis. However, some recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have shown that it is possible for a trained model to identify the true label by only looking at the hypothesis without observing the premise. The phenomenon is referred to as annotation artifacts (Gururangan et al., 2018), statistical irregularities (Poliak et al., 2018) or partial-input heuristics (Feng et al., 2019). In this paper we use the term hypothesis-only bias (Poliak et al., 2018) to refer to this phenomenon. Such hypothesis-only bias originates from the human annotation process of data collection. In the data collection process of many large-s"
2020.lrec-1.846,D19-6115,0,0.217796,"Missing"
2020.lrec-1.846,N18-1170,0,0.0470107,"orted by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this p"
2020.lrec-1.846,N15-1098,0,0.0960085,"e elaborated in Sec 3.2.2. and Sec 3.2.3. respectively. the performance gap between the easy and hard sets as it dynamically adjusts the debiasing strategies (i.e. the weight of training instances in Eq 5 and 6). 4. 5. Acknowledgments We would like to thank the anonymous reviewers for their valuable suggestions. This work is supported by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed t"
2020.lrec-1.846,D18-1170,1,0.819086,"91.5 90.1 89.9 Several Yes various ... According 54.7 54.4 53.7 53.1 53.1 addition also locals battle dangerous 69.6 68.6 65.7 63.3 63.2 None refused never perfectly Nobody 85.4 80.5 79.0 77.3 77.1 SNLI Table 1: Top 3 artificial patterns sorted by the pattern-label conditional probability p(l|b) (Sec 2.1.). The listed patterns appear at least in 500/200 instances in SNLI/MultiNLI training sets, notably the numbers 500/200 here are chosen only for better visualization. ‘#’ is the placeholder for an arbitrary token. The underlined artificial pattern serves as an example in Sec 2.1.. al., 2018; Luo et al., 2018). The experiments show that the guidance from the derived artificial patterns can be helpful to the success of sentence-level NLI debiasing. 2. Datasets In this section, we identify the artificial patterns from the hypothesis sentences which highly correlate to specific labels in the training sets and then derive hard, easy subsets from the original test sets based on them. 2.1. Artificial Pattern Collection ‘Pattern’ in this work refers to (maybe nonconsecutive) word segments in the hypothesis sentences. We try to identify the ‘artificial patterns’ which spuriously correlate to a specific lab"
2020.lrec-1.846,P19-1334,0,0.0702703,"Missing"
2020.lrec-1.846,K18-1007,0,0.154524,"l Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve s"
2020.lrec-1.846,C18-1198,0,0.146235,"Missing"
2020.lrec-1.846,D16-1244,0,0.23239,"Missing"
2020.lrec-1.846,S18-2023,0,0.0310535,"ference (NLI) (also known as recognizing textual entailment) is a widely studied task which aims to infer the relationship (e.g., entailment, contradiction, neutral) between two fragments of text, known as premise and hypothesis (Dagan et al., 2006; Dagan et al., 2013). NLI models are usually required to determine whether a hypothesis is true (entailment) or false (contradiction) given the premise, or whether the truth value can not be inferred (neutral). A proper NLI decision should apparently rely on both the premise and the hypothesis. However, some recent studies (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018) have shown that it is possible for a trained model to identify the true label by only looking at the hypothesis without observing the premise. The phenomenon is referred to as annotation artifacts (Gururangan et al., 2018), statistical irregularities (Poliak et al., 2018) or partial-input heuristics (Feng et al., 2019). In this paper we use the term hypothesis-only bias (Poliak et al., 2018) to refer to this phenomenon. Such hypothesis-only bias originates from the human annotation process of data collection. In the data collection process of many large-scale NLI datasets suc"
2020.lrec-1.846,W17-1609,0,0.0919498,"paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve superb performance on SNLI by wordlevel heuristics (Dasgupta et al., 2018). (MacCartney and Manning, 2009) first re"
2020.lrec-1.846,N18-1179,0,0.0619198,"al inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate known bias. The InferSent model, which served as an important baseline in this paper, are found to achieve superb performance on SNLI by wordlevel heuristics (Dasgupta et al., 2018). (MacCartney and Manning, 2009) first revealed the difficulties of natural language inference model with bag-of-words models. Different from the artificial"
2020.lrec-1.846,D18-1534,0,0.370256,"ke to thank the anonymous reviewers for their valuable suggestions. This work is supported by the National Science Foundation of China under Grant No. 61751201, No. 61772040, No. 61876004. The corresponding authors of this paper are Baobao Chang and Zhifang Sui. Related Work The bias in the data annotation exists in many tasks, e.g. lexical inference (Levy et al., 2015), visual question answering (Goyal et al., 2017), ROC story cloze (Cai et al., 2017) etc. The NLI models are shown to be sensitive to the compositional features in premises and hypotheses (Nie et al., 2019a), data permutations (Schluter and Varab, 2018; Wang et al., 2018) and vulnerable to adversarial examples (Iyyer et al., 2018; Minervini and Riedel, 2018; Glockner et al., 2018) and crafted stress test (Geiger et al., 2018; Naik et al., 2018). (Rudinger et al., 2017) showed hypothesis in SNLI has the evidence of gender, racial and religious stereotypes, etc. (Sanchez et al., 2018) analysed the behaviour of NLI models and the factors to be more robust. (Feng et al., 2019) discussed how to use partialinput baseline (hypothesis-only classifier in NLI) in future dataset creation. (Clark et al., 2019) uses an ensemblebased method to mitigate k"
2020.lrec-1.846,L18-1239,0,0.126103,"Missing"
2020.lrec-1.846,N16-1174,0,0.100608,"Missing"
2020.lrec-1.846,D18-1009,0,0.0310084,"Missing"
2020.lrec-1.846,Q17-1036,0,0.0307413,") 82.9 90.4 60.0 InferSent+Guidance 84.1 95.5 61.7 dInferSent 81.6 92.5 59.9 +Guidance 82.2 86.9 63.3 +Guidance+Reweight 80.9 78.2 67.3 (a) InferSent trained on SNLI Model Full Easy Hard InferSent 70.4 92.7 54.4 InferSent+DS(λ=0.8) 69.9 91.4 53.6 InferSent+Guidance 70.1 92.1 54.9 dInferSent 68.8 91.1 54.7 +Guidance 68.0 87.9 55.3 +Guidance+Reweight 66.5 79.4 58.8 (b) InferSent trained on MultiNLI ∆Hard Easy (↓) 38.3 30.4 33.8 32.6 23.6 10.9 works (GANs) (Goodfellow et al., 2014). Several works on learning encoders which are invariant to certain properties of text and image (Chen et al., 2018; Zhang et al., 2017; Xie et al., 2017; Moyer et al., 2018; Jaiswal et al., 2018) in the adversarial settings. ∆Hard Easy (↓) In this study, we show that the hypothesis-only bias in trained NLI models mainly comes from unevenly distributed surface patterns, which could be used to identify hard and easy instances for more convincing re-evaluation on currently overestimated NLI models. The attempts to mitigate the bias are meaningful as such bias not only makes NLI models fragile to adversarial examples. We try to mitigate this bias by removing those artificial patterns in the training sets, with experiments showin"
2020.lrec-1.846,P19-1435,0,0.0944433,"Missing"
2020.lrec-1.846,D15-1075,0,0.189211,"Missing"
2020.lrec-1.846,N18-1101,0,0.117673,"Missing"
2021.acl-long.274,D19-1582,0,0.154472,"where we simply use string matching to detect entity coreference following Zheng et al. (2019) , and the entity embedding Ei is computed by the average of its mention node embedding, Ei = Mean({hj }j∈Mention(i) ). In this way, the sentences and entities are interactively represented in a context-aware way. 3.3 Virtual Node Event Types Detection Since a document can express events of different types, we formulate the task as a multi-label classification and leverage sentences feature matrix S to * Traditional methods in sentence-level EE also utilize graph to extract events (Liu et al., 2018; Yan et al., 2019), based on the dependency tree. However, our interaction graph is heterogeneous and have no demands for dependency tree. detect event types: A = MultiHead(Q, S, S) ∈ Rdm ×T R = Sigmoid(A&gt; Wt ) ∈ RT where Q ∈ Rdm ×T and Wt ∈ Rdm are trainable parameters, and T denotes the number of possible event types. MultiHead refers to the standard multi-head attention mechanism with Query/Key/Value. Therefore, we derive the event b ∈ RT : types detection loss with golden label R Ldetect = − T   X bt = 1 log P (Rt |D) I R t=1 (2)   bt = 0 log (1 − P (Rt |D)) +I R 3.4 Event Records Extraction Since a doc"
2021.acl-long.274,N16-1033,0,0.027015,"e-trained language model (Yang et al., 2019), and explicit external knowledge (Liu et al., 2019a; Tong et al., 2020) such as WordNet (Miller, 1995). Du and Cardie (2020b) also try to extract events in a Question-Answer way. These studies usually conduct experiments on sentencelevel event extraction dataset, ACE05 (Walker et al., 2006). However, it is hard for the sentence-level models to extract multiple qualified events spanning across sentences, which is more common in real-world scenarios. 4.6 Document-level Event Extraction. Documentlevel EE has attracted more and more attention recently. Yang and Mitchell (2016) use well-defined features to handle the event-argument relations across sentences, which is, unfortunately, quite nontrivial. Yang et al. (2018) extract events from a central sentence and find other arguments from neighboring sentences separately. Although Zheng et al. (2019) use Transformer to fuse sentences and entities, interdependency among events is neglected. Du and Cardie (2020a) try to encode the sentences in a multi-granularity way and Du et al. (2020) leverage a seq2seq model. They conduct experiments on MUC-4 (Sundheim, 1992) dataset with 1, 700 documents and 5 kinds of entity-base"
2021.acl-long.274,P18-4009,0,0.360185,"n (EE) is one of the key and challenging tasks in Information Extraction (IE), which aims to detect events and extract their arguments from the text. Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence. The sentence-level model, however, fails to extract events whose arguments spread in multiple sentences, which is much more common in real-world scenarios. Hence, extracting events at the document-level is critical. It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020). author. 7.2 million Nov 6, 2014 … Figure 1: An example document from a Chinese dataset proposed by Zheng et al. (2019) in the financial domain, and we translate it into English for illustration. Entity mentions are colored. Due to space limitation, we only show four associated sentences and three argument roles of each event type. The complete original document can be found in Appendix C. EU: Equity Underweight, EO: Equity Overweight. Introduction * Corresponding Xiaoting Wu Though promising, document-level EE still faces two critic"
2021.acl-long.274,P19-1522,0,0.123575,"19) show G IT outperforms the existing best methods by 2.8 F1. Further analysis reveals G IT is effective in extracting multiple correlated events and event arguments that scatter across the document. Our code is available at https: //github.com/RunxinXu/GIT. 1 EventType EquityHolder TradedShares StartDate … Mingting Wu 7.2 million Nov 6, 2014 … EU EO Event Extraction (EE) is one of the key and challenging tasks in Information Extraction (IE), which aims to detect events and extract their arguments from the text. Most previous methods (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Yang et al., 2019; Du and Cardie, 2020b) focus on sentence-level EE, extracting events from a single sentence. The sentence-level model, however, fails to extract events whose arguments spread in multiple sentences, which is much more common in real-world scenarios. Hence, extracting events at the document-level is critical. It has attracted much attention recently (Yang et al., 2018; Zheng et al., 2019; Du and Cardie, 2020a; Du et al., 2020). author. 7.2 million Nov 6, 2014 … Figure 1: An example document from a Chinese dataset proposed by Zheng et al. (2019) in the financial domain, and we translate it into"
2021.acl-long.274,2020.emnlp-main.127,1,0.593902,"ther by M-Minter edges. As in document EE, an entity usually corresponds to multiple mentions across sentences, we thus use M-Minter edge to track all the appearances of a specific entity, which facilitates the long distance event extraction from a global perspective. In Section. 4.5, experiments show that all of these four kinds of edges play an important role in event detection, and the performance would decrease without any of them. After heterogeneous graph construction * , we apply multi-layer Graph Convolution Network (Kipf and Welling, 2017) to model the global interactions inspired by Zeng et al. (2020). Given node u at the l-th layer, the graph convolutional operation is defined as follows:   X X 1 (l) h(l+1) = ReLU  W h(l)  u cu,k k v S k∈K v∈Nk (u) A EquityFreeze B Pledger C E H D F I C G J Tracker Pledgee StartDate A … Virtual Node EquityPledge B E F A C E A D F H I B C G J A A B K Global Memory … K A Completed Uncompleted Figure 3: The decoding module of G IT. Three Equity Freeze records have been extracted completely, and G IT is predicting the StartDate role for the Equity Pledge records (in the dashed frame ), based on the global memory where Tracker tracks the records on-the-fly"
2021.ccl-1.8,P15-1168,0,0.0363435,"Missing"
2021.ccl-1.8,D15-1141,0,0.0470307,"Missing"
2021.ccl-1.8,W04-3236,0,0.216815,"Missing"
2021.ccl-1.8,P14-1028,1,0.845389,"Missing"
2021.ccl-1.8,C04-1081,0,0.0371792,"Missing"
2021.ccl-1.8,I05-3027,0,0.230996,"Missing"
2021.ccl-1.8,P16-2092,0,0.0378584,"Missing"
2021.ccl-1.8,O03-4002,0,0.434603,"Missing"
2021.ccl-1.8,2020.ccl-1.52,1,0.654577,"Missing"
2021.ccl-1.8,D13-1061,0,0.0611631,"Missing"
2021.emnlp-main.749,2020.emnlp-main.634,0,0.269945,"free or task-driven mask is performed on the gradients of the non-child network, resetting them to zero (grey diagonal grids). poor generalization ability in transferring to out-ofdomain data or other related tasks (Mahabadi et al., 2021; Aghajanyan et al., 2021). Preventing the fine-tuned models to deviate too much from the pretrained weights (i.e., with less 1 Introduction knowledge forgetting), is proved to be effective to mitigate the above challenges (Gouk et al., 2020). Pretrained Language Models (PLMs) have had a remarkable effect on the natural language process- For instance, RecAdam (Chen et al., 2020) introing (NLP) landscape recently (Devlin et al., 2019; duces L2 distance penalty between the fine-tuned weights and their pretrained weights. In addition, Liu et al., 2019; Clark et al., 2020). Pretraining and fine-tuning have become a new paradigm of NLP, Mixout (Lee et al., 2020) randomly replaces part of the model parameters with their pretrained weights dominating a large variety of tasks. during fine-tuning. The core idea behind them is Despite its great success, how to adapt such large-scale pretrained language models with mil- to utilize the pretrained weights to regularize the lions"
2021.emnlp-main.749,P07-1033,0,0.323932,"Missing"
2021.emnlp-main.749,N19-1423,0,0.452812,"of the non-child network, resetting them to zero (grey diagonal grids). poor generalization ability in transferring to out-ofdomain data or other related tasks (Mahabadi et al., 2021; Aghajanyan et al., 2021). Preventing the fine-tuned models to deviate too much from the pretrained weights (i.e., with less 1 Introduction knowledge forgetting), is proved to be effective to mitigate the above challenges (Gouk et al., 2020). Pretrained Language Models (PLMs) have had a remarkable effect on the natural language process- For instance, RecAdam (Chen et al., 2020) introing (NLP) landscape recently (Devlin et al., 2019; duces L2 distance penalty between the fine-tuned weights and their pretrained weights. In addition, Liu et al., 2019; Clark et al., 2020). Pretraining and fine-tuning have become a new paradigm of NLP, Mixout (Lee et al., 2020) randomly replaces part of the model parameters with their pretrained weights dominating a large variety of tasks. during fine-tuning. The core idea behind them is Despite its great success, how to adapt such large-scale pretrained language models with mil- to utilize the pretrained weights to regularize the lions to billions of parameters to various scenarios, fine-tu"
2021.emnlp-main.749,2021.acl-long.378,0,0.0719407,"Missing"
2021.emnlp-main.749,2020.acl-main.197,0,0.0430013,"ons. Effective and generalizable fine-tuning. With a mass of parameters, fine-tuning large PLMs tend to achieve degenerated performance due to overfitting and have poor generalization ability, especially on small datasets (Devlin et al., 2019; Phang et al., 2018; Lee et al., 2020). Therefore, different finetuning techniques have been proposed. Some of them utilize the pretrained weights to regularize the deviation of the fine-tuned model (Lee et al., 2020; Daumé III, 2007; Chen et al., 2020), while others compress the output information (Mahabadi et al., 2021) or injects noise into the input (Jiang et al., 2020; Aghajanyan et al., 2021). Moreover, Zhang et al. (2021) and Mosbach et al. (2021) point out that the omission of bias correction in the Adam optimizer used in Devlin et al. (2019) is also responsible for the degenerated results. Orthogonal to these methods, C HILD -T UNING address the problems by detecting the child network within the model in a task-free or task-driven way. It only updates parameters within the child network via a gradient mask, which is proved to be effective in adapting large PLMs to various tasks, along with better generalization ability. as possible parameters to mainta"
2021.emnlp-main.749,2021.acl-long.47,0,0.219485,"r generalization performance by large margins. Vanilla Backward ?! + Forward ?! CHILD-TUNING Backward ?"" ∆?! Task-Free or Task-Driven Gradients Mask Pretrained Weights = ∆?! ?"" Weights at 1-th Iteration Figure 1: The illustration of C HILD -T UNING. Left: It forwards on the whole network while backwarding on a subset of network (i.e., child network). Right: To achieve this, a task-free or task-driven mask is performed on the gradients of the non-child network, resetting them to zero (grey diagonal grids). poor generalization ability in transferring to out-ofdomain data or other related tasks (Mahabadi et al., 2021; Aghajanyan et al., 2021). Preventing the fine-tuned models to deviate too much from the pretrained weights (i.e., with less 1 Introduction knowledge forgetting), is proved to be effective to mitigate the above challenges (Gouk et al., 2020). Pretrained Language Models (PLMs) have had a remarkable effect on the natural language process- For instance, RecAdam (Chen et al., 2020) introing (NLP) landscape recently (Devlin et al., 2019; duces L2 distance penalty between the fine-tuned weights and their pretrained weights. In addition, Liu et al., 2019; Clark et al., 2020). Pretraining and fine-tu"
2021.emnlp-main.749,2020.acl-main.703,0,0.0198146,"so similar. The reason may be that both SST2 and CoLA belongs to a single sentence classification task, while others are in a different format of sentence-pair classification tasks. 5 Related Work Explosion of PLMs. There has been an explosion of studies on Pretrained Language Models (PLMs). Devlin et al. (2019) propose BERT that is pretrained on large quantities of unannotated corpus with self-supervised tasks. Many PLMs also emerged such as GPT-2 (Radford et al., 2018), GPT3 (Brown et al., 2020), ELECTRA (Clark et al., 2020), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), and BART (Lewis et al., 2020). The number of parameters of PLMs also explodes. BERTLARGE has 340 millions of parameters, and the number for GPT-3 is even up to 175 billions. Effective and generalizable fine-tuning. With a mass of parameters, fine-tuning large PLMs tend to achieve degenerated performance due to overfitting and have poor generalization ability, especially on small datasets (Devlin et al., 2019; Phang et al., 2018; Lee et al., 2020). Therefore, different finetuning techniques have been proposed. Some of them utilize the pretrained weights to regularize the deviation of the fine-tuned model (Lee et al., 2020;"
2021.findings-acl.227,P98-1013,0,0.646497,"nt identification which reduces the computational complexity. To our best knowledge, it’s the first practice to introduce the pointer network into frame semantic parsing. The experiments show improvement over state of the art models on FrameNet dataset. 1 Figure 1: A sentence annotated the arguments and roles of frame Text creation. The arrow marks indicate the order of arguments identification and roles classification. Introduction Frame semantic parsing is a fundamental study in Natural Language Processing. It aims to parse sentences into frame-style semantic structures defined in FrameNet (Baker et al., 1998). An example of frame-style semantic structures is shown in Figure 1. The word write.v is a target that evokes the frame called Text creation. The phrases underlined with green lines are called arguments. Author, Text and Form are roles (also called frame elements) the arguments play in this frame. Hence the frame semantic parsing contains three subtasks, namely frame identification, argument identification and role classification. For a sentence with a given target, the frame identification is to disambiguate the frame for the target based on its contextual information, the argument identific"
2021.findings-acl.227,C10-3009,0,0.0953284,"Missing"
2021.findings-acl.227,S10-1059,0,0.0351827,"with partially-annotated exemplar sentences data. Then we train the model on the offcial train set. We evaluate our model on development test and save the best performance model for test. We use Glove (Pennington et al., 2014) to initialize the word embeddings, and average the existing embeddings for out-of-vocabulary words. We randomly initialize embeddings for part-of-speech tags, and token type tags. All the embeddings are learnable during training. Other detail hyper-parameters are shown on Table 2. Model. We compare our model with following previous models: SEMAFOR: A widely known system(Chen et al., 2010) that uses a variety of syntactic features. Framat: An open-source semantic role labeling tool proposed by Bj¨orkelund et al. (2010). Framat+context: An extension version of Framat that adds extra context features by Roth and Lapata (2015). Hermann et al.(2014): A frame identification model uses feature representation based on word embedding and WSABIE algorithm (Weston et al., 2011). FitzGerald et al.(2015): A pipeline model that improves frame identification performance based on Hermann et al. (2014). Open-SESAME: A pipeline model that predicts frame by FitzGerald et al. (2015) and designs a"
2021.findings-acl.227,D16-1001,0,0.0621458,"Missing"
2021.findings-acl.227,P14-1136,0,0.233977,"rlined with green lines are called arguments. Author, Text and Form are roles (also called frame elements) the arguments play in this frame. Hence the frame semantic parsing contains three subtasks, namely frame identification, argument identification and role classification. For a sentence with a given target, the frame identification is to disambiguate the frame for the target based on its contextual information, the argument identification is to identify the boundaries of all the arguments, and the role classification is to assign a semantic role to each argument we have found. Early work (Hermann et al., 2014; FitzGerald et al., 2015; Hartmann et al., 2017) on frame seman∗ Corresponding author tic parsing adopts pipeline strategy. Their models apply independent models to handle different subtasks which ignore the interactions among subtasks. Moreover, the pipeline strategy usually causes error propagation problem. The accuracy of frame identification can become the bottleneck of the overall performance. Later work (Yang and Mitchell, 2017; Peng et al., 2018) processes all the subtasks jointly by optimizing them together during training. Their joint models show improvement over pipeline models, whi"
2021.findings-acl.227,S07-1048,0,0.0295963,"rk into frame semantic parsing task. • We design a target-aware attention mechanism to aggregate the semantic information of other targets in the same sentence. We evaluate our model on FrameNet dataset, and the experiments show that our model outperforms state of the art models, which demonstrates the effectiveness of our model. 2 Related Work Frame semantic parsing task is first proposed by Gildea and Jurafsky (2002) and has drawn attention since the SemEval 2007 shared task (Baker et al., 2007) was released. Early researches on frame semantic parsing focus on the feature-engineered methods(Johansson and Nugues, 2007; Das et al., 2010). Most of the early researches regard the frame semantic parsing as a pipeline of classification tasks and employ machine learning algorithms (such as Support Vector Machines etc.). With the popularity of neural network and representation learning, neural network models are introduced to model frame semantic parsing problem. Hermann et al. (2014) uses distributed representations in frame identification and embedded both frames and the contextual representations of words into a shared low-dimension vector space. FitzGerald et al. (2015) uses a neural network to learn embeddin"
2021.findings-acl.227,D18-1191,0,0.0129392,"reST A ) = ScoreEN D = P (ieτ |S, t, f ) ieτ argmax(P (isτ |S, t, f )) D H > (W5 hEN + W6 hisτ ) τ = softmax(ScoreEN D ) = argmax(P (ieτ |S, t, f )) (14) (15) (16) (17) H is dh × n matrix (h0 , . . . , hn−1 ) that represents the encoder output of sentence S. W4 , W5 and W6 are dh × dh weight matrixes. The hierarchical pointer network achieves arguments identification within linear computational complexity. For an n-tokens and k-arguments sentence, our model can identify the start and end positions of each argument with O(2n) computational complexity, and O(2n · k) for all arguments. 3.4 2016; Ouchi et al., 2018). With the probability distribution P (rτ |S, t, f, aτ ), we can predict the role rτ . Moreover, we add a special role ’None’ at the final decoding step and let rk be ’None’. During the inference stage, the role classification decoder and argument identification decoder will automatically stop when predicting ’None’. 4 We utilize cross-entropy loss to maximize the probability of the oracle frame type, span boundaries (start-end pair) and role types: Lf rame = log(P (fˆ|S, t)) Lrole = yτ = W7 · [hieτ + hisτ ; hieτ − hisτ ; ef ] hrole = LSTMR (hrole τ τ −1 , yτ ) P (rτ |S, t, f, aτ ) = MLP([hrol"
2021.findings-acl.227,N18-1135,0,0.118499,"y the boundaries of all the arguments, and the role classification is to assign a semantic role to each argument we have found. Early work (Hermann et al., 2014; FitzGerald et al., 2015; Hartmann et al., 2017) on frame seman∗ Corresponding author tic parsing adopts pipeline strategy. Their models apply independent models to handle different subtasks which ignore the interactions among subtasks. Moreover, the pipeline strategy usually causes error propagation problem. The accuracy of frame identification can become the bottleneck of the overall performance. Later work (Yang and Mitchell, 2017; Peng et al., 2018) processes all the subtasks jointly by optimizing them together during training. Their joint models show improvement over pipeline models, which demonstrates the benefit of joint training strategy. However, their systems don’t have specific design to model the interactions among the subtasks. To strengthen the interactions of subtasks, we propose a joint framework based on three taskspecific decoders. The interactions in our framework are mainly reflected in two aspects. On one hand, the representations of both the target and its frame that derived from frame identification decoder are applied"
2021.findings-acl.227,D14-1162,0,0.084706,"a sequence of vectors h0 , . . . , hn−1 , where hi is the contextual representation of word wi . For each token, we concatenate its word embedding ewi , lemma embedding eli , POS embedding epi and a binary tag embedding ebi : ei = [ewi ; eli ; epi ; ebi ] (1) The binary tag embedding ebi is to distinguish t from other words in S. Let it be the position index of t in S, then we can calculate ebi :  e1 , i = it ebi = (2) e0 , i 6= it At last, ei is fed to the encoder to get contextual representation hi : hi = Encoder (ei ) (3) The word embedding and lemma embedding are initialized with Glove (Pennington et al., 2014) while POS embedding is randomly initialized. We use Bi-LSTM as the encoder in our experiment, which can be also replaced with any other encoder model such as Bert (Devlin et al., 2018). The dimension of ei is de and the dimension of hi is dh . 3.2 Frame identification module In frame identification module, we build a target representation rt for t and identify the frame f based on rt . As there are likely to be multiple targets t0 , . . . , tm−1 in S evoking multiple frames f0 , . . . , fm−1 and we believe that other targets in S can contribute to identifying current frame f for target t, we"
2021.findings-acl.227,Q15-1032,0,0.0185859,"alize the word embeddings, and average the existing embeddings for out-of-vocabulary words. We randomly initialize embeddings for part-of-speech tags, and token type tags. All the embeddings are learnable during training. Other detail hyper-parameters are shown on Table 2. Model. We compare our model with following previous models: SEMAFOR: A widely known system(Chen et al., 2010) that uses a variety of syntactic features. Framat: An open-source semantic role labeling tool proposed by Bj¨orkelund et al. (2010). Framat+context: An extension version of Framat that adds extra context features by Roth and Lapata (2015). Hermann et al.(2014): A frame identification model uses feature representation based on word embedding and WSABIE algorithm (Weston et al., 2011). FitzGerald et al.(2015): A pipeline model that improves frame identification performance based on Hermann et al. (2014). Open-SESAME: A pipeline model that predicts frame by FitzGerald et al. (2015) and designs a softmax-margin segmental RNN to improve arguHyper-parameters Batch size MLP layers Encoder lstm layers Word/lemma embedding Token type embedding POS embedding Pre-train/train epochs Pre-train/train optimizer Activation Function Encoder/De"
2021.findings-acl.227,P11-1144,0,0.0608418,"Missing"
2021.findings-acl.227,P16-1218,1,0.823038,"a0 , . . . , aτ −1 and their roles r0 , . . . , rτ −1 . Hence we use the same LSTM architecture named LSTMR to record them. Both the contextual information of aτ and the frame embedding ef are used to predict rτ : Loss Function k−1 X log(P (ˆisτ |S, t, f ))+ τ =0 k−1 X (23) log(P (ˆieτ |S, t, f )) τ =0 We optimize the losses of the three subtasks jointly: L = αLf rame + βLspan + γLrole (24) α, β and γ are hyper-parameters that adjust the direction of training optimization. 5 Experiment (20) hie + his represents the boundary feature of aτ and hie − his represents the inner feature of the span (Wang and Chang, 2016; Cross and Huang, Dataset. We train and evaluate our model on FrameNet 1.5 dataset proposed by (Das and Smith, 2011) following previous work (Yang and Mitchell, 2017; Swayamdipta et al., 2017; Peng et al., 2018). 2574 We also follow the same train/development/test split. Meanwhile, previous work adds the partiallyannotated exemplar sentences (each exemplar sentence contains only one target). As is reported in previous work (Das et al., 2014; Yang and Mitchell, 2017; Swayamdipta et al., 2017), the exemplar sentences data can help to improve their models’ performance. We add it as pre-train dat"
2021.findings-acl.227,D15-1112,0,0.0441401,"Missing"
2021.findings-acl.227,D17-1128,0,0.216761,"ntification is to identify the boundaries of all the arguments, and the role classification is to assign a semantic role to each argument we have found. Early work (Hermann et al., 2014; FitzGerald et al., 2015; Hartmann et al., 2017) on frame seman∗ Corresponding author tic parsing adopts pipeline strategy. Their models apply independent models to handle different subtasks which ignore the interactions among subtasks. Moreover, the pipeline strategy usually causes error propagation problem. The accuracy of frame identification can become the bottleneck of the overall performance. Later work (Yang and Mitchell, 2017; Peng et al., 2018) processes all the subtasks jointly by optimizing them together during training. Their joint models show improvement over pipeline models, which demonstrates the benefit of joint training strategy. However, their systems don’t have specific design to model the interactions among the subtasks. To strengthen the interactions of subtasks, we propose a joint framework based on three taskspecific decoders. The interactions in our framework are mainly reflected in two aspects. On one hand, the representations of both the target and its frame that derived from frame identification"
2021.findings-acl.227,J02-3001,0,0.654002,"boundaries of arguments directly. The hierarchical pointer network predicts arguments within linear computational complexity. To our best knowledge, it’s the first practice to introduce the pointer network into frame semantic parsing task. • We design a target-aware attention mechanism to aggregate the semantic information of other targets in the same sentence. We evaluate our model on FrameNet dataset, and the experiments show that our model outperforms state of the art models, which demonstrates the effectiveness of our model. 2 Related Work Frame semantic parsing task is first proposed by Gildea and Jurafsky (2002) and has drawn attention since the SemEval 2007 shared task (Baker et al., 2007) was released. Early researches on frame semantic parsing focus on the feature-engineered methods(Johansson and Nugues, 2007; Das et al., 2010). Most of the early researches regard the frame semantic parsing as a pipeline of classification tasks and employ machine learning algorithms (such as Support Vector Machines etc.). With the popularity of neural network and representation learning, neural network models are introduced to model frame semantic parsing problem. Hermann et al. (2014) uses distributed representat"
2021.findings-acl.227,E17-1045,0,0.0303884,"Missing"
2021.findings-acl.47,D19-1498,0,0.0486266,"vernment, it will help IBM … [6] … IBM has 4 priority areas in Brazil … country [1, 2, 5] São Paulo continent [1, 5] Brazil South America has part [1, 5] continent [1, 2, 5] Figure 1: Two examples from DocRED (Yao et al., 2019b) for illustration of intra- and inter-sentential relations. Sentence numbers, entity mentions, and supporting evidence involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction Relation Extraction (RE) is an important way of obtaining knowledge facts from natural language text. Many recent advancements (Sahu et al., 2019; Christopoulou et al., 2019; Yao et al., 2019b; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020) manage to tackle the document-level relation extraction (doc-level RE) that extracts semantic relations among entities across multiple sentences. Due to its strong correlation with real-world scenarios, doc-level RE has attracted much attention in the field of information extraction. The doc-level RE task is usually formulated as a classification problem that predicts possible rela∗ Head: Polar Music Tail: Swedish relation: country of origin Corresponding author. tions for all entity pairs, using the information from"
2021.findings-acl.47,N19-1423,0,0.0585091,"Missing"
2021.findings-acl.47,N18-2007,0,0.0191528,"teractions among entity mentions and document and use the latter to conduct path-based logical reasoning. However, these works do not explicitly distinguish the intra- and inter-sentential relation instances in the design of the model and use the same way to encode them. So the most significant difference between our model and previous models is that we treat intra-sentential and intersentential relations differently to conform with the relational patterns for their prediction. Reasoning in relation extraction. Reasoning problem has been extensively studied in the field of question answering (Dhingra et al., 2018). However, few works manage to tackle this problem in the document-level relation extraction task. Zeng et al. (2020) is the first to propose the explicit way of relational reasoning on doc-level RE, which mainly focuses on logical reasoning. They use the paths on their entity-level graph to provide clues for logical reasoning. However, since not all entity pairs are connected with a path and have the correct logical reasoning paths in their graph, their methods are somehow limited. In this work, we design a new form of logical reasoning to cover more cases of logical reasoning. Conclusion Int"
2021.findings-acl.47,D14-1162,0,0.0850404,"lti-label GCN, use ReLU as our activation function, and classification task: set the dropout rate to 0.3, learning rate to 0.001. We train SIRE using AdamW (Loshchilov and HutP (r|ei,h , ei,t ) = sigmoid (W1 σ(W2 ri + b1 ) + b2 ) ter, 2019) as optimizer with weight decay 0.0001 (17) and implement SIRE under PyTorch (Paszke et al., where W1 , W2 , b1 , b2 are trainable parameters, σ is 2017) and DGL (Wang et al., 2019b) frameworks. an activation function (e.g., ReLU). We use binary We implement two settings for our SIRE. SIREcross entropy as objective to train our SIRE: GloVe uses GloVe (100d, Pennington et al., 2014) XX X and BiLSTM (512d, Schuster and Paliwal, 1997) as Lrel = − I (ri = 1) log P (ri |ei,h , ei,t ) word embedding and encoder, respectively. SIRED∈C h6=t ri ∈R BERT use BERT-base (Devlin et al., 2019) as en+ I (ri = 0) log (1 − P (ri |ei,h , ei,t )) coder on DocRED, cased BioBERT-Base v1.1 as (18) the encoder on CDR/GDA, and the learning rate −5 and learning where C denotes the whole corpus, R denotes rela- for BERT parameters is set to 1e −3 rate for other parameters remains 1e . Detailed tion type set and I (·) refers to indicator function. hyperparameter settings are in Appendix. 3 3.1 Exp"
2021.findings-acl.47,E17-1110,0,0.0195435,"ctly find the words by using Eq.5 that trigger the relations of these entity pairs. In inter-sentential relations, the supporting evidence that the model finds, i.e., sentences 1 and 2, indeed expresses the relations between S˜ao Paul and South America. We also conduct logical reasoning in terms of the logical reasoning chains: S˜ao Paul→ other-entity → South America. Our SIRE could focus on the correct logical reasoning chains: S˜ao Paul→ Brazil → South America. These cases show the predictions of SIRE are explainable. 4 Related Work 5 Document-level relation extraction. Many recent efforts (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019; Yao et al., 2019b; Wang et al., 2019a; Tang et al., 2020; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020; Dai et al., 2020) manage to tackle the document-level relation extraction. Most of them use graph-based models, such as Graph Convolutional Networks (GCNs, Kipf and Welling, 2017; Schlichtkrull et al., 2017) that has been used in many natural language processing tasks (Marcheggiani and Titov, 2017; Yao et al., 2019a; Liu et al., 2020). They construct a graph structure from the input document. This graph u"
2021.findings-acl.47,N19-1370,0,0.0270531,"Missing"
2021.findings-acl.47,P19-1423,0,0.012097,"on with Brazil’s government, it will help IBM … [6] … IBM has 4 priority areas in Brazil … country [1, 2, 5] São Paulo continent [1, 5] Brazil South America has part [1, 5] continent [1, 2, 5] Figure 1: Two examples from DocRED (Yao et al., 2019b) for illustration of intra- and inter-sentential relations. Sentence numbers, entity mentions, and supporting evidence involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction Relation Extraction (RE) is an important way of obtaining knowledge facts from natural language text. Many recent advancements (Sahu et al., 2019; Christopoulou et al., 2019; Yao et al., 2019b; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020) manage to tackle the document-level relation extraction (doc-level RE) that extracts semantic relations among entities across multiple sentences. Due to its strong correlation with real-world scenarios, doc-level RE has attracted much attention in the field of information extraction. The doc-level RE task is usually formulated as a classification problem that predicts possible rela∗ Head: Polar Music Tail: Swedish relation: country of origin Corresponding author. tions for all entity pairs,"
2021.findings-acl.47,2020.lrec-1.210,1,0.711597,"ated Work 5 Document-level relation extraction. Many recent efforts (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019; Yao et al., 2019b; Wang et al., 2019a; Tang et al., 2020; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020; Dai et al., 2020) manage to tackle the document-level relation extraction. Most of them use graph-based models, such as Graph Convolutional Networks (GCNs, Kipf and Welling, 2017; Schlichtkrull et al., 2017) that has been used in many natural language processing tasks (Marcheggiani and Titov, 2017; Yao et al., 2019a; Liu et al., 2020). They construct a graph structure from the input document. This graph uses the word, mentions or entities as nodes and uses heuristic rules and semantic dependencies as edges. They use this graph to model document information and interactions and to predict possible relations for all entity pairs. Nan et al. (2020) proposed a latent structure induction to induce the dependency tree in the document dynamically. Zeng et al. (2020) proposed a double graph-based graph aggregationand-inference network that constructs two graphs: mention-level graph and entity-level graph. They use the former to ca"
2021.findings-acl.47,D17-1159,0,0.0265773,"ow the predictions of SIRE are explainable. 4 Related Work 5 Document-level relation extraction. Many recent efforts (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019; Yao et al., 2019b; Wang et al., 2019a; Tang et al., 2020; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020; Dai et al., 2020) manage to tackle the document-level relation extraction. Most of them use graph-based models, such as Graph Convolutional Networks (GCNs, Kipf and Welling, 2017; Schlichtkrull et al., 2017) that has been used in many natural language processing tasks (Marcheggiani and Titov, 2017; Yao et al., 2019a; Liu et al., 2020). They construct a graph structure from the input document. This graph uses the word, mentions or entities as nodes and uses heuristic rules and semantic dependencies as edges. They use this graph to model document information and interactions and to predict possible relations for all entity pairs. Nan et al. (2020) proposed a latent structure induction to induce the dependency tree in the document dynamically. Zeng et al. (2020) proposed a double graph-based graph aggregationand-inference network that constructs two graphs: mention-level graph and entity-"
2021.findings-acl.47,2020.acl-main.141,0,0.216458,"iority areas in Brazil … country [1, 2, 5] São Paulo continent [1, 5] Brazil South America has part [1, 5] continent [1, 2, 5] Figure 1: Two examples from DocRED (Yao et al., 2019b) for illustration of intra- and inter-sentential relations. Sentence numbers, entity mentions, and supporting evidence involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction Relation Extraction (RE) is an important way of obtaining knowledge facts from natural language text. Many recent advancements (Sahu et al., 2019; Christopoulou et al., 2019; Yao et al., 2019b; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020) manage to tackle the document-level relation extraction (doc-level RE) that extracts semantic relations among entities across multiple sentences. Due to its strong correlation with real-world scenarios, doc-level RE has attracted much attention in the field of information extraction. The doc-level RE task is usually formulated as a classification problem that predicts possible rela∗ Head: Polar Music Tail: Swedish relation: country of origin Corresponding author. tions for all entity pairs, using the information from the entire document. It has two diffe"
2021.findings-acl.47,Q17-1008,0,0.019252,"using Eq.5 that trigger the relations of these entity pairs. In inter-sentential relations, the supporting evidence that the model finds, i.e., sentences 1 and 2, indeed expresses the relations between S˜ao Paul and South America. We also conduct logical reasoning in terms of the logical reasoning chains: S˜ao Paul→ other-entity → South America. Our SIRE could focus on the correct logical reasoning chains: S˜ao Paul→ Brazil → South America. These cases show the predictions of SIRE are explainable. 4 Related Work 5 Document-level relation extraction. Many recent efforts (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019; Yao et al., 2019b; Wang et al., 2019a; Tang et al., 2020; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020; Dai et al., 2020) manage to tackle the document-level relation extraction. Most of them use graph-based models, such as Graph Convolutional Networks (GCNs, Kipf and Welling, 2017; Schlichtkrull et al., 2017) that has been used in many natural language processing tasks (Marcheggiani and Titov, 2017; Yao et al., 2019a; Liu et al., 2020). They construct a graph structure from the input document. This graph uses the word, menti"
2021.findings-acl.47,D18-1246,0,0.0136038,"of these entity pairs. In inter-sentential relations, the supporting evidence that the model finds, i.e., sentences 1 and 2, indeed expresses the relations between S˜ao Paul and South America. We also conduct logical reasoning in terms of the logical reasoning chains: S˜ao Paul→ other-entity → South America. Our SIRE could focus on the correct logical reasoning chains: S˜ao Paul→ Brazil → South America. These cases show the predictions of SIRE are explainable. 4 Related Work 5 Document-level relation extraction. Many recent efforts (Quirk and Poon, 2017; Peng et al., 2017; Gupta et al., 2019; Song et al., 2018; Jia et al., 2019; Yao et al., 2019b; Wang et al., 2019a; Tang et al., 2020; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020; Dai et al., 2020) manage to tackle the document-level relation extraction. Most of them use graph-based models, such as Graph Convolutional Networks (GCNs, Kipf and Welling, 2017; Schlichtkrull et al., 2017) that has been used in many natural language processing tasks (Marcheggiani and Titov, 2017; Yao et al., 2019a; Liu et al., 2020). They construct a graph structure from the input document. This graph uses the word, mentions or entities as nodes and uses heuri"
2021.findings-acl.47,N18-1080,0,0.0408809,"Missing"
2021.findings-acl.47,2020.emnlp-main.303,0,0.247143,"2, 5] São Paulo continent [1, 5] Brazil South America has part [1, 5] continent [1, 2, 5] Figure 1: Two examples from DocRED (Yao et al., 2019b) for illustration of intra- and inter-sentential relations. Sentence numbers, entity mentions, and supporting evidence involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction Relation Extraction (RE) is an important way of obtaining knowledge facts from natural language text. Many recent advancements (Sahu et al., 2019; Christopoulou et al., 2019; Yao et al., 2019b; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020) manage to tackle the document-level relation extraction (doc-level RE) that extracts semantic relations among entities across multiple sentences. Due to its strong correlation with real-world scenarios, doc-level RE has attracted much attention in the field of information extraction. The doc-level RE task is usually formulated as a classification problem that predicts possible rela∗ Head: Polar Music Tail: Swedish relation: country of origin Corresponding author. tions for all entity pairs, using the information from the entire document. It has two different kinds of relations: intra-sententi"
2021.findings-acl.47,P19-1074,0,0.0312484,"Missing"
2021.findings-acl.47,2020.emnlp-main.582,0,0.0209979,"he encoder on DocRED and use the output from the encoder to represent all entity pairs to predict relations. Wang et al. (2019a) propose BERT to replace the BiLSTM as the encoder on DocRED. Moreover, they also propose BERT-Two-Step, which first predicts whether two entities have a relation and then predicts the specific target relation. Tang et al. (2020) propose the hierarchical inference networks HIN-GloVe and HIN-BERT, which make full use of multi-granularity inference information including entity level, sentence level, and document level to infer relations. Similar to Wang et al. (2019a), Ye et al. (2020) propose a language representation model called CorefBERT as encoder on DocRED that can capture the coreferential relations in context. Nan et al. (2020) propose the LSR-GloVe and LSR-BERT to dynamically induce the latent dependency tree structure to better model the document interactions for prediction. Wang et al. (2020) propose a global-to-local network GLRE, which encodes the document information in terms of entity global and local representations as well as context relation representations. Zeng et al. (2020) propose the graph aggregationand-inference networks GAIN-GloVe and GAIN529 Dev M"
2021.findings-acl.47,2020.emnlp-main.127,1,0.0686725,"azil … country [1, 2, 5] São Paulo continent [1, 5] Brazil South America has part [1, 5] continent [1, 2, 5] Figure 1: Two examples from DocRED (Yao et al., 2019b) for illustration of intra- and inter-sentential relations. Sentence numbers, entity mentions, and supporting evidence involved in these relation instances are colored. Other mentions are underlined for clarity. Introduction Relation Extraction (RE) is an important way of obtaining knowledge facts from natural language text. Many recent advancements (Sahu et al., 2019; Christopoulou et al., 2019; Yao et al., 2019b; Nan et al., 2020; Zeng et al., 2020; Wang et al., 2020) manage to tackle the document-level relation extraction (doc-level RE) that extracts semantic relations among entities across multiple sentences. Due to its strong correlation with real-world scenarios, doc-level RE has attracted much attention in the field of information extraction. The doc-level RE task is usually formulated as a classification problem that predicts possible rela∗ Head: Polar Music Tail: Swedish relation: country of origin Corresponding author. tions for all entity pairs, using the information from the entire document. It has two different kinds of relat"
2021.naacl-main.437,Q17-1010,0,0.0227941,"und truth deﬁnition are Hyper-parameters: We tune hyper-parameters to achieve the best BLEU score on the validation set. covered by the predicted deﬁnition. The overall We use Adam (Kingma and Ba, 2015) with an ini- metric measures the overall quality of the predicted deﬁnition, referencing the ground-truth deﬁnition. tial learning rate of 10−3 as the optimizer. We set We randomly select 100 entries from the test set, hidden size to 300, batch size to 64 and dropout rate to 0.2. Word embeddings are 300-dimensional, and hire three raters to rate the predicted deﬁnitions pretrained by fastText (Bojanowski et al., 2017). on a scale of 1 to 5, where each entry includes (1) the source word, (2) the ground-truth deﬁnition, We train for up to 50 epochs, and early stop the and (3) the predicted deﬁnition to the raters. We training process once the performance does not show in Table 5 the detailed guideline for raters on improve for 10 consecutive epochs. We run our each point. experiments on a single NVIDIA GeForce GTX 2080Ti GPU with 11 GB memory. The inter-rater kappa (Fleiss and Cohen, 1973) is 0.65 for coverage and 0.66 for overall. We average Baselines: We compare with two reproducible baselines that have a"
2021.naacl-main.437,P18-2043,0,0.0608041,"Missing"
2021.naacl-main.437,N19-1350,0,0.0917766,"ically Fuses different features through a gating mechanism, and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word"
2021.naacl-main.437,2020.acl-main.65,0,0.333516,"and generaTes word definitions. Experimental results show that our method is both effective and robust. 1 1 Introduction Deﬁnition Generation (DG) aims at automatically generating an explanatory text for a word. This task is of practical importance to assist dictionary construction, especially in highly productive languages like Chinese (Yang et al., 2020). Most existing methods take the source word as an indecomposable lexico-semantic unit, using features like word embedding (Noraset et al., 2017) and context (Gadetsky et al., 2018; Ishiwatari et al., 2019). Recently, Yang et al. (2020) and Li et al. (2020) achieve improvement by decomposing the word meaning into different semantic components. In decomposing the word meaning, the word formation process is an intuitive and informative way that has not been explored in DG by far. For parataxis languages like Chinese, a word is formed by formation components, i.e., morphemes, and Word 䕾冲 Word Definition 䕾冲䕼共䖃冰ȼ (White flower.) Formation Rule Modifier-Head 䕾冲 䕼䕼☯冰尸ȼ (Vainly spend.) Morphemes: Definitions 䕾1: 䕼共䖃 (white) 冲1: 冰㘴 (flower) Adverb-Verb 䕾2: 䕼䕼☯ (vainly) 冲2: 冰尸 (spend) Figure 1: Word formation process for the polysemous &quot;白花&quot;. With mor"
2021.naacl-main.437,P18-2023,0,0.23046,"Missing"
2021.naacl-main.437,D19-1357,0,0.0366765,"Missing"
2021.naacl-main.437,N19-1097,0,0.152168,"–5531 June 6–11, 2021. ©2021 Association for Computational Linguistics Recent methods attempt to decompose the word meaning by using HowNet sememes (Yang et al., 2020) or modeling latent variables (Li et al., 2020). Semantic Components: To systematically deﬁne words, linguists decompose the word meaning into semantic components (Wierzbicka, 1996). Following this idea, HowNet (Dong and Dong, 2006) uses manually-created sememes to describe the semantic aspects of words. Recent studies also show that leveraging subword information produces better embeddings (Park et al., 2018; Lin and Liu, 2019; Zhu et al., 2019), but these methods lack a clear distinction among different formation rules. 3 Word Formation Process in Chinese It is linguistically motivated to explore the word formation process to better understand words. Instead of combining roots and afﬁxes, Chinese words are formed by characters in a parataxis way (Li et al., 2018). Here, we introduce two formation features and construct a formation-informed dataset. 3.1 Formation components and rules Chinese formation components are morphemes, deﬁned as the smallest meaning-bearing units (Zhu, 1982). Morphemes are unambiguous in representing word mea"
C12-1098,C08-2006,0,0.17358,"about the same topic. Recently, there have been many attempts to explore different approaches to generate update summaries. The predominant approaches are mainly built upon the sentence extraction framework. Update summarization for an evolving topic differs from previous generic summarization for a static topic in that the latter aims to acquire the salient information in one topic, while the former cares for both the salience and the novelty of information. By developing traditional summarization techniques, massive efforts on update summarization have been made to dig out new information (Boudin et al., 2008; Fisher and Boark, 2008; Wan, 2007; Li et al., 2008; Du et al., 2010; Li et al., 2012). The typical examples include the scaled Maximal Marginal Relevance (MMR) algorithm which excludes those sentences similar to the history documents, and some extensions of TextRank such as TimedTextRank (Wan, 2007), PNR2 (Li et al., 2008), MRSP (Du et al., 2010) which re-rank the salience scores of sentences by employing various kinds of reinforcement between sentences. One problem with these approaches is that they tend to regard update summarization more as a redundancy removal problem than a novelty dete"
C12-1098,P11-1050,0,0.0162171,"cy (repeating less the same information). The score is an integer between 1 (very poor) and 5 (very good). We randomly select 28 topics from TAC 2011 data and assign each topic to three different assessors7. In Table 3, the left four columns report the average scores of each criterion for the three systems. The experimental results indicate that h-uHDPSum is significantly better than both Peer 43 and 2LevLDASum (based on paired t-test with p-value &lt; 0.01). Simultaneously, a fairly standard approach for manual evaluation is conducted through pairwise comparison (Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2011). According to the rating scores, each pair of summaries is judged which one is better under each criterion. If two summaries have the same score, they are judged a tie (of the equal quality). We record the times of ‘winning’ (having a higher score) and tie for each system. In Table 3, the right six columns show the evaluation results in frequencies respectively for h-uHDPSum vs. Peer 43, and h-uHDPSum vs. 2LevLDASum. The experimental results also indicate that h-uHDPSum is significantly better than both Peer 43 and 2LevLDASum. We also observe that the winning times of h-uHDPSum under the nove"
C12-1098,E12-1022,0,0.74095,"ome new may appear over time, causing the number of aspects and aspect structures to change at different epochs. 1 Aspect in this article is usually called cluster in evolutionary clustering. 1604 Under the framework of extractive summarization, it is important to acquire the relationship between sentences and aspects for sentence selection. However, in most existing HDP models, the sentence level is disregarded and we cannot directly get the aspect distribution of sentences. Inspired by the progress made in Latent Dirichlet Allocation (LDA) models (Chemudugunta et al., 2007; Li et al., 2010; Delort and Alfonseca, 2012), we newly add the sentence level between the word level and document level in the h-uHDP model. Since neighboring sentences in one document usually talk about one same aspect, we assume that the aspect assignment of each sentence is not conditionally independently. With such assumption, the aspect of each sentence is determined by the aspect distribution of both the document and its neighboring sentences. Our huHDP model is capable of mapping multiple levels of information into the latent aspect space. The rest of this paper is organized as follows. Section 2 discusses the related work on upd"
C12-1098,N09-1041,0,0.139189,"summarization aims to produce an update summary for the documents in the update epoch, assuming that users already read earlier documents in the history epoch. That is, we need to boost sentences in update epoch that can bring out important and novel information. On one hand, the generated summary should extract the main content in DU, and on the other hand, the summary should avoid mentioning too much old information in DH. To care for these two points, we propose a sentence selection strategy based on Kullback-Leibler (KL) divergence, which has been widely used in extractive summarization (Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Delort and Alfonseca, 2012 ). Given the history sentence set SH and the update sentence set SU, we propose a function to score a set of sentences Sum which is a subset SU. Score(Sum )  K L ( p ||p S u m )   K L ( p ||p S u m ) (17) S S H U In the equation, the first term means the prize on the divergence from epoch history and the second term represents the penalty on the divergence from epoch update. The parameter  (called as epoch balance factor) is used to tune the weights of two KL distances. empirical aspect distribution of the candidate summary Sum. the as"
C12-1098,P10-1066,0,0.12074,"ization more as a redundancy removal problem than a novelty detection problem. Another problem is that these approaches are mainly based on the computation of lexical similarities between sentences and fail to consider higher level information to avoid semantic redundancy in update summarization. To solve these two problems, we borrow the techniques of evolutionary clustering which focuses on detecting the dynamics of a given topic. Normally, one topic is described from various specific aspects 1 , accompanied with the background information running the whole topic (Chemudugunta et al., 2007; Li et al., 2010). For example, the topic “Quebec independence” may involve the specific aspects including “leader in independence movement”, “referendum”, “related efforts in independence movement” and so on, while “Quebec” and “independence” are seen as the general background information. The evolving dynamics of a topic is mainly embodied in the birth, splitting, merging and death of the specific aspects (Ren et al., 2008). Then, the commonality and diversity between history documents and update documents can be easily summarized from the aspect level and update summarization is not limited to lexical redun"
C12-1098,C08-1062,0,0.370449,"tempts to explore different approaches to generate update summaries. The predominant approaches are mainly built upon the sentence extraction framework. Update summarization for an evolving topic differs from previous generic summarization for a static topic in that the latter aims to acquire the salient information in one topic, while the former cares for both the salience and the novelty of information. By developing traditional summarization techniques, massive efforts on update summarization have been made to dig out new information (Boudin et al., 2008; Fisher and Boark, 2008; Wan, 2007; Li et al., 2008; Du et al., 2010; Li et al., 2012). The typical examples include the scaled Maximal Marginal Relevance (MMR) algorithm which excludes those sentences similar to the history documents, and some extensions of TextRank such as TimedTextRank (Wan, 2007), PNR2 (Li et al., 2008), MRSP (Du et al., 2010) which re-rank the salience scores of sentences by employing various kinds of reinforcement between sentences. One problem with these approaches is that they tend to regard update summarization more as a redundancy removal problem than a novelty detection problem. Another problem is that these approac"
C12-1098,N03-1020,0,0.315773,"Missing"
C12-1098,W11-0507,0,0.0204407,"n update summary for the documents in the update epoch, assuming that users already read earlier documents in the history epoch. That is, we need to boost sentences in update epoch that can bring out important and novel information. On one hand, the generated summary should extract the main content in DU, and on the other hand, the summary should avoid mentioning too much old information in DH. To care for these two points, we propose a sentence selection strategy based on Kullback-Leibler (KL) divergence, which has been widely used in extractive summarization (Haghighi and Vanderwende, 2009; Mason and Charniak, 2011; Delort and Alfonseca, 2012 ). Given the history sentence set SH and the update sentence set SU, we propose a function to score a set of sentences Sum which is a subset SU. Score(Sum )  K L ( p ||p S u m )   K L ( p ||p S u m ) (17) S S H U In the equation, the first term means the prize on the divergence from epoch history and the second term represents the penalty on the divergence from epoch update. The parameter  (called as epoch balance factor) is used to tune the weights of two KL distances. empirical aspect distribution of the candidate summary Sum. the aspect distribution of SH an"
C12-1098,C10-1111,0,0.0419546,"Missing"
C12-1098,W04-3252,0,\N,Missing
C14-1035,S07-1002,0,0.031164,"han on a word-by-word basis. This is not only simple but also efficient, especially in the case where there are a large number of target words to be concerned. We further extend the parametric model into a non-parametric model, as it allows adaptation of model complexity to data. By extending our model to non-parametric model, the need to preset the numbers of senses and latent concepts are totally removed and, moreover, the model performance is also improved. We evaluate our model on the commonly used benchmark datasets released by both Semeval-2010 (Manandhar et al., 2010) and Semeval-2007 (Agirre and Soroa, 2007). The test results show that our models perform much better than the state-of-the-art systems. 2 2.1 The parametric model Basic Model The main point of our work is that different senses are signaled by contexts with different concept configurations, where different concepts are formally defined as different distributions over context words. Formally, we denote by P (s) the global multinomial distribution over senses of an ambiguous word and by P (w|z) the multinomial distributions over context words w given concept z. Context words are generated by a mixture of different concepts whose mixture"
C14-1035,D09-1056,0,0.0660008,"Missing"
C14-1035,E09-1013,0,0.0424101,"ask of Word Sense Induction. Almost all work relies on the distributional hypothesis, which states that words occurring in similar contexts will have similar meanings. Different work exploits distributional information in different forms, including context clustering models (Sch¨utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010), graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian 2 Formally, the position feature is the context words with its relative position to the target word. 362 models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features such as words, part-of-speech and dependency information. Yao and Van Durme (2011) extended LDA-based model into non-parametric HDP model but removed the feature engineering. Lau et al. (2012) showed improved supervised F-score by including position feature to the HDP model. Choe and Charniak (2013) proposed a re"
C14-1035,D13-1148,0,0.0155607,"yesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features such as words, part-of-speech and dependency information. Yao and Van Durme (2011) extended LDA-based model into non-parametric HDP model but removed the feature engineering. Lau et al. (2012) showed improved supervised F-score by including position feature to the HDP model. Choe and Charniak (2013) proposed a reweighted naive Bayes model by incorporating the idea that words closer to the target word are more relevant in predicting the sense. Our model differs from the context clustering models and graph-based models, as it is a Bayesian probabilistic model. Our work also differs from the LDA-based models. LDA topics were actually re-interpreted as senses of target word as Brody and Lapata (2009) applied the LDA to WSI tasks, so did Yao and Van Durme (2011) and Lau et al. (2012). They induced word senses by firstly tagging (sampling) senses (of target words) to context words and selectin"
C14-1035,S10-1082,0,0.0232259,"rict our attention to the first group in which all models are Bayesian model, our BNP model without feature engineering outperforms the HDP model which is also non-parametric model without feature engineering. 6 Related Work A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies on the distributional hypothesis, which states that words occurring in similar contexts will have similar meanings. Different work exploits distributional information in different forms, including context clustering models (Sch¨utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010), graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian 2 Formally, the position feature is the context words with its relative position to the target word. 362 models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features such as words, part-of-speech and dep"
C14-1035,S10-1078,0,0.0147149,"the first group in which all models are Bayesian model, our BNP model without feature engineering outperforms the HDP model which is also non-parametric model without feature engineering. 6 Related Work A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies on the distributional hypothesis, which states that words occurring in similar contexts will have similar meanings. Different work exploits distributional information in different forms, including context clustering models (Sch¨utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010), graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian 2 Formally, the position feature is the context words with its relative position to the target word. 362 models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features such as words, part-of-speech and dependency information."
C14-1035,D10-1073,0,0.151738,"out feature engineering outperforms the HDP model which is also non-parametric model without feature engineering. 6 Related Work A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies on the distributional hypothesis, which states that words occurring in similar contexts will have similar meanings. Different work exploits distributional information in different forms, including context clustering models (Sch¨utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010), graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian 2 Formally, the position feature is the context words with its relative position to the target word. 362 models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features such as words, part-of-speech and dependency information. Yao and Van Durme (2011) extended LDA-based model into non-parametric HDP model but r"
C14-1035,S10-1079,0,0.500483,"dy and Lapata, 2009; Yao and Van Durme, 2011; Lau et al., 2012) did, we compare with previous work with supervised F-score on 80-20 data split in Semeval2010 and noun data in Semeval-2007. Table 3 (left) compares our models against the state-of-the-art systems tested on 80-20 data split in Semeval-2010. HDP+position (Lau et al., 2012) improved the HDP model (Yao and Van Durme, 2011) by including a position feature. distNB (Choe and Charniak, 2013) extends the naive Bayes model by reweighting the conditional probability of a context word given the sense by its distance to the target word. UoY (Korkontzelos and Manandhar, 2010) is the best performing system in Semeval-2010 competition which used a graph-based model. We re-implemented and tested the HDP model on the Semeval-2010 dataset since Yao and Van Durme (2011) and Lau et al. (2012) did not report their HDP results on this dataset. Different with normal practice in WSI work, there is no feature engineering in our model. However, our BNP model outperformed all the systems on supervised evaluation. Even the Basic Model outperformed the best performing Semeval-2010 system. Especially, our BNP model performs much better than the HDP model. Both Lau et al. (2012) an"
C14-1035,E12-1060,0,0.227701,"sition feature is the context words with its relative position to the target word. 362 models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features such as words, part-of-speech and dependency information. Yao and Van Durme (2011) extended LDA-based model into non-parametric HDP model but removed the feature engineering. Lau et al. (2012) showed improved supervised F-score by including position feature to the HDP model. Choe and Charniak (2013) proposed a reweighted naive Bayes model by incorporating the idea that words closer to the target word are more relevant in predicting the sense. Our model differs from the context clustering models and graph-based models, as it is a Bayesian probabilistic model. Our work also differs from the LDA-based models. LDA topics were actually re-interpreted as senses of target word as Brody and Lapata (2009) applied the LDA to WSI tasks, so did Yao and Van Durme (2011) and Lau et al. (2012). T"
C14-1035,P13-2099,0,0.0172091,"he j-th concept-word distribution P (w|z = j), however, there are now an infinite number of such distributions. So is the number of senses. However, with a fixed number of contexts of the target word, only a finite number of senses and concepts are active and they could be inferred automatically by the inference procedure. 4 Model Inference We use Gibbs sampling (Casella and George, 1992) for inference to both the parametric and nonparametric model. As a particular Markov Chain Monte Carlo (MCMC) method, Gibbs sampling is widely used for inference in various Bayesian models (Teh et al., 2006; Li and Li, 2013; Li and Cardie, 2014). 4.1 The Parametric Model ~ senseFor the parametric model, we use collapsed Gibbs sampling, in which the sense distribution θ, concept distribution ρ ~i and concept-word distribution ϕ ~ j are integrated out. At each iteration, the sense label sm of the target word in context m is sampled from conditional distribution p(sm |~s¬m , ~z, w), ~ and the concept label zm,n for the context word wm,n is sampled from conditional distribution p(zm,n |~s, ~z¬(m,n) , w). ~ Here ~s¬m refers to all current sense assignments other than sm and ~z¬(m,n) refers to all current concept assi"
C14-1035,S10-1011,0,0.480827,"ould be set on an all-word basis, rather than on a word-by-word basis. This is not only simple but also efficient, especially in the case where there are a large number of target words to be concerned. We further extend the parametric model into a non-parametric model, as it allows adaptation of model complexity to data. By extending our model to non-parametric model, the need to preset the numbers of senses and latent concepts are totally removed and, moreover, the model performance is also improved. We evaluate our model on the commonly used benchmark datasets released by both Semeval-2010 (Manandhar et al., 2010) and Semeval-2007 (Agirre and Soroa, 2007). The test results show that our models perform much better than the state-of-the-art systems. 2 2.1 The parametric model Basic Model The main point of our work is that different senses are signaled by contexts with different concept configurations, where different concepts are formally defined as different distributions over context words. Formally, we denote by P (s) the global multinomial distribution over senses of an ambiguous word and by P (w|z) the multinomial distributions over context words w given concept z. Context words are generated by a m"
C14-1035,S07-1037,0,0.0267591,"utperforms all systems. If we restrict our attention to the first group in which all models are Bayesian model, our BNP model without feature engineering outperforms the HDP model which is also non-parametric model without feature engineering. 6 Related Work A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies on the distributional hypothesis, which states that words occurring in similar contexts will have similar meanings. Different work exploits distributional information in different forms, including context clustering models (Sch¨utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010), graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian 2 Formally, the position feature is the context words with its relative position to the target word. 362 models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features"
C14-1035,S10-1081,0,0.0143601,"tems. If we restrict our attention to the first group in which all models are Bayesian model, our BNP model without feature engineering outperforms the HDP model which is also non-parametric model without feature engineering. 6 Related Work A large body of previous work is devoted to the task of Word Sense Induction. Almost all work relies on the distributional hypothesis, which states that words occurring in similar contexts will have similar meanings. Different work exploits distributional information in different forms, including context clustering models (Sch¨utze, 1998; Niu et al., 2007; Pedersen, 2010; Elshamy et al., 2010; Kern et al., 2010), graph-based models (Korkontzelos and Manandhar, 2010; Klapaftis and Manandhar, 2010) and Bayesian 2 Formally, the position feature is the context words with its relative position to the target word. 362 models. For Bayesian models, Brody and Lapata (2009) firstly introduced a Bayesian model to WSI task. They used the LDA-based model in which contexts of target word were viewed as documents as in the LDA model (Blei et al., 2003) and senses as topics. They trained a separate model for each target word and included a variety of features such as words,"
C14-1035,D07-1043,0,0.0466186,"Missing"
C14-1035,J98-1004,0,0.725004,"Missing"
C14-1035,W11-1102,0,0.0448452,"Missing"
C16-1161,S07-1025,0,0.0217758,"a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure of the embedding space and enforces it to be temporally cons"
C16-1161,P14-2082,0,0.0362075,"15; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure of the embedding space and enforces it to be temporally consistent and accurate. Time-aware joint inference with"
C16-1161,P08-1090,0,0.0298628,"Missing"
C16-1161,P07-2044,0,0.0409635,"Missing"
C16-1161,D14-1165,0,0.0722366,"2013): the mean of correct entity ranks (Mean Rank) and the proportion of valid entities ranked in top-10 (Hits@10). As mentioned in (Bordes et al., 2013), the metrics are desirable but flawed when a corrupted triple exists in the KG. As a countermeasure, we may filter out all these corrupted triples which have appeared in KG before ranking. We name the first evaluation set as Raw and the second as Filter. For each test quad (triple), we replace the head/tail entity ei by those entities with compatible types as removing triples with incompatible types during test time leads to better results (Chang et al., 2014; 1720 Wang et al., 2015). Entity type information is easy to obtain for YAGO and Freebase. Then we rank the generated corrupted triples in descending order, according to the plausibility (for baselines and TAE model) or the decision variables (for time-aware ILP model). Then we check whether the original correct triple ranks in top-10. To calculate Hit@10 for ILP model, for each test quad, we add additional P (r ) constraints that at most 10 corrupted are true: i,j xei e1j ≤ 10. Mean Rank is missing for ILP method as we could not rank the binary decision variables. Baseline methods. For compa"
C16-1161,P12-1012,0,0.0169546,"rnal information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Con"
C16-1161,D15-1038,0,0.0323252,"016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin,"
C16-1161,P15-1009,0,0.0161756,"esearch related to our work. Knowledge Graph Completion. Nickel et al. (2016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky an"
C16-1161,D15-1082,0,0.194831,"r1T, r2T(tr &lt; tr ) 1 r1T r1 2 r2T (a)TransE (b)TransE-TAE Figure 1: Simple illustration of Temporal Evolving Matrix T in the time-aware embedding (TAE) space. For example, r1 =wasBornIn happened before r2 =diedIn. After projection by T, we get prior relation’s projection r1 T near subsequent relation r2 in the space, i.e.,r1 T ≈ r2 , but r2 T 6= r1 . Here, x+ ∈ ∆ is the observed (i.e., positive) triple, and x− ∈ ∆0 is the negative triple constructed by replacing entities in x+ . γ is the margin separating positive and negative triples and [z]+ = max(0, z). Please refer to (Wang et al., 2014a; Lin et al., 2015b) for TransH, TransR and other models. After we obtain the embeddings, the plausibility of a missing triple can be predicted by using the scoring function. In general, triples with higher plausibility are more likely to be true. 2.3 Time-Aware KG Embedding Model TransE assumes that each relation is time independent and entity/relation representation is only affected by structural patterns in KGs. To better model knowledge evolution, we assume temporal ordered relations are related to each other and evolve in a time dimension. For example, for the same person, there exists a temporal order amo"
C16-1161,W09-2418,0,0.0301052,"o et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large Web corpora (Talukdar et al., 2012b; Talukdar et al., 2012a). The TempEval task (Pustejovsky and Verhagen, 2009) and systems (Chambers et al., 2007; Bethard and Martin, 2007; Chambers and Jurafsky, 2008; Cassidy et al., 2014) have been successful in extracting temporally related events. Temporal reasoning is also explored to solve temporal conflicts in KG (Dylla et al., 2011; Wang et al., 2010). This paper differs from this line of work as we directly use temporal information from KG to perform KG completion. 1722 6 Conclusion and Future Work In this paper, we propose two novel time-aware KG completion models. Time-aware embedding (TAE) model imposes temporal order constraints on the geometric structure"
C16-1161,N13-1008,0,0.0132003,"08] and a person cannot marry two people at the same time. 5 Related Work There are two lines of research related to our work. Knowledge Graph Completion. Nickel et al. (2016) provide a broad overview of machine learning models for KG completion. These models predict new facts in a given knowledge graph using information from existing entities and relations. The most related work from this line of work is KG embedding models (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013). Aside from fact triples, external information is employed to improve KG embedding such as combining text (Riedel et al., 2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), relation path (Lin et al., 2015a; Gu et al., 2015), and logical rules (Wang et al., 2015; Rockt¨aschel et al., 2015). However, these methods have not utilized temporal information among facts. Temporal Information Extraction. This line of work mainly falls into two categories: methods that extract temporal facts from web (Ling and Weld, 2010; Wang et al., 2011; Artiles et al., 2011; Garrido et al., 2012) and methods that infer temporal scopes from aggregate statistics in large"
C16-1161,N15-1118,0,0.0239417,"Missing"
C16-1161,D14-1167,0,0.148049,"Projection r1 r2 r2 r1T, r2T(tr &lt; tr ) 1 r1T r1 2 r2T (a)TransE (b)TransE-TAE Figure 1: Simple illustration of Temporal Evolving Matrix T in the time-aware embedding (TAE) space. For example, r1 =wasBornIn happened before r2 =diedIn. After projection by T, we get prior relation’s projection r1 T near subsequent relation r2 in the space, i.e.,r1 T ≈ r2 , but r2 T 6= r1 . Here, x+ ∈ ∆ is the observed (i.e., positive) triple, and x− ∈ ∆0 is the negative triple constructed by replacing entities in x+ . γ is the margin separating positive and negative triples and [z]+ = max(0, z). Please refer to (Wang et al., 2014a; Lin et al., 2015b) for TransH, TransR and other models. After we obtain the embeddings, the plausibility of a missing triple can be predicted by using the scoring function. In general, triples with higher plausibility are more likely to be true. 2.3 Time-Aware KG Embedding Model TransE assumes that each relation is time independent and entity/relation representation is only affected by structural patterns in KGs. To better model knowledge evolution, we assume temporal ordered relations are related to each other and evolve in a time dimension. For example, for the same person, there exists a"
C16-1270,D15-1075,0,0.37321,"epresent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) c"
C16-1270,P16-1139,0,0.220145,"(Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) combines parsing and interpretation within a single tree sequence hybrid model by integrating tree structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. 2871 Premise representation ⨁ ~ ~ (a) LSTM Unit (b) rLSTM Unit Figure 1: The inner architecture of the traditional LSTM unit and the re-read LSTM unit. 3 3.1 Model Background The LSTM architecture (Hochreiter and Schmidhuber, 1997) addresses this probl"
C16-1270,D16-1053,0,0.576145,"eep neural network for classification. However, in the sentence encoding process, the premise and the hypothesis cannot affect each other. It is well known that the encoding procedure is just automatically learning useful features. Without the impact between the two sentences, it is difficult for the encoder to extract the sentence-relationship-specific features. Other methods mainly make use of attention mechanism to capture the word-by word alignment information while training (Rockt¨aschel et al., 2015) or just integrate memory network into LSTM to make the model remember more information (Cheng et al., 2016). Among them, only the attention mechanism can make the two sentences contact with each other. However, the word-by-word attention does not represent a better understanding of the sentences. When deciding the entailment relationship between a pair of sentences, what is really matters? Unlike paraphrasing and machine translation, entailment relationship does not force the two sentences have the same meaning. Instead, as long as the premise can cover the meaning of the hypothesis, the entailment stands. Therefore, if the premise entails the hypothesis, that doesn’t really mean that the words in"
C16-1270,E09-1025,0,0.0238895,"ctional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the e"
C16-1270,C08-1043,0,0.0255133,"rage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the atten"
C16-1270,D16-1176,0,0.109637,"Missing"
C16-1270,W07-1407,0,0.0125968,"premise, and use a bidirectional rLSTM to read the hypothesis. The output of the standard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check"
C16-1270,P16-2022,0,0.225039,"can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the performance of LSTM-based recurrent neural network. Then, Yin et al. (2015) applied attention mechanic to convolution neural network, Liu et al. (2016a) proposed coupled-LSTM, Vendrov et al. (2015) proposed ordered embedding, Mou et al. (2016) applied Tree-based CNN, Wang and Jiang (2015) proposed matching LSTM, Liu et al. (2016b) applied inner-attention, Cheng et al. (2016) proposed Long Short-Term Memory-Networks to improve the performance. To free the model from traditional parsing process, Bowman et al. (2016) combines parsing and interpretation within a single tree sequence hybrid model by integrating tree structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Parikh et al. (2016) uses attention to decompose the problem into subproblems that can be solved separately, thus making it tr"
C16-1270,D16-1244,0,0.295878,"Missing"
C16-1270,D14-1162,0,0.0887401,"premise and the hypothesis. 4.1 Datasets and Model Configuration We conduct experiments on the Stanford Natural Language Inference corpus (SNLI) (Bowman et al., 2015). The original data set contains 570,152 sentence pairs, each labeled with one of the following relationships: entailment, contradiction, neutral and −, where − indicates a lack of consensus from the 2874 human annotators. We discard the sentence pairs labeled with − and keep the remaining ones for our experiments. Table 2 summarizes the statistics of the three entailment classes in SNLI. We use 300 dimensional GloVe embeddings (Pennington et al., 2014) to represent words, which is trained on the Wikipedia+Gigaword dataset. The embeddings of unknown tokens are initialized by random vectors. 4.2 Methods for Comparison Although we list all of the approaches designed for recognizing textural entailment task on the SNLI dataset in Table 3, we mainly want to compare our model with the word-by-word attention model by Rockt¨aschel et al. (2015), long short term memory network model by Cheng et al. (2016) and the decomposable attention model by Parikh et al. (2016) since they are either related to our work or achieved the state-of-the-art performanc"
C16-1270,D15-1185,1,0.872603,"ariety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based techniq"
C16-1270,P11-2098,0,0.0237541,", including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the perf"
C16-1270,W11-2402,0,0.0166197,", including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used for recognizing the entailment. However, these work are still based on hand-engineered features which is not easy to generalize. Recently, neural network based methods start to show its effectiveness. Based on (Bowman et al., 2015), Rockt¨aschel et al. (2015) uses the attention-based technique to improve the perf"
C16-1270,U06-1019,0,0.0101027,"utput of the standard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer"
C16-1270,W07-1406,0,0.0319259,"eral input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabilistic methods are used"
C16-1270,P06-1051,0,0.0406042,"ard BiLSTM is taken as the general input of the bidirectional rLSTM. Experiments show that our method has outperformed the state-of-the-art approaches. 2 Related Work Textual Entailment Recognizing (RTE) task has been widely studied by many previous work. Firstly, the methods use statistical classifiers which leverage a wide variety of features, including hand-engineered features derived from complex NLP pipelines and similarity between sentences (T and H) and sentence pairs ((T 0 , H 0 ) and (T 00 , H 00 ))(Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006; Zanzotto and Moschitti, 2006; Wang and Neumann, 2007; Dinu and Wang, 2009; Nielsen et al., 2009; Malakasiotis, 2011). This kind of methods are hard to generalize due to the complexity of feature engineering. Moreover, the hand-engineered features usually cannot represent implicit meanings of sentences. Secondly, (Hickl, 2008; Sha et al., 2015; Shnarch et al., 2011b; Shnarch et al., 2011a; Beltagy et al., 2013; Rios et al., 2014) extract the structured information (discourse commitments or predicate-argument representations) in T -H pair and check if the information in T contains or can infer the information in H. Probabi"
C16-1309,P12-1056,0,0.0309786,"and has been extensively studied for the decades (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; Fung et al., 2005; He et al., 2007; Sayyadi et al., 2009; Zhao et al., 2012; Sayyadi and Raschid, 2013; Ge et al., 2015). They are based on either document- or keyword-based clustering, which usually suffer from either unawareness of time, high expensive computation cost or deviation of cluster centroids. In contrast, our approach is time-aware, centroid-aware and so efficient that it can be run on a large text stream. In addition, there is much work (Sakaki et al., 2010; Lee et al., 2011; Diao et al., 2012; Aggarwal and Subbian, 2012; Wang et al., 2013; Dong et al., 2015) studying event detection problem in social media. They usually use more or less social media features such as spatio-temporal information, which are not in the same setting with our task. 6 Conclusion and Future Work This paper proposes to use a novel text stream representation – Burst Information Networks to address the retrospective event detection challenge. Based on the BINet, we propose two fast centroid-aware event 3284 detection models that can effectively overcome the limitations of the previous event detection models"
C16-1309,P15-1056,1,0.851895,"(320.27s) 304.56s 716s 9.25s 3591.98s (1350.08s) BINet-ADM 2,562.17s (320.27s) 304.56s 716s 27.3s 3610.03s (1368.13s) Table 6: The running time of 4 parts of our BINet-based event detection approaches. The number in the round bracket is the running time of the model when it is run in 8-way parallel. 5 Related Work Event detection is one of the most popular research topics in recent years and has been extensively studied for the decades (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; Fung et al., 2005; He et al., 2007; Sayyadi et al., 2009; Zhao et al., 2012; Sayyadi and Raschid, 2013; Ge et al., 2015). They are based on either document- or keyword-based clustering, which usually suffer from either unawareness of time, high expensive computation cost or deviation of cluster centroids. In contrast, our approach is time-aware, centroid-aware and so efficient that it can be run on a large text stream. In addition, there is much work (Sakaki et al., 2010; Lee et al., 2011; Diao et al., 2012; Aggarwal and Subbian, 2012; Wang et al., 2013; Dong et al., 2015) studying event detection problem in social media. They usually use more or less social media features such as spatio-temporal information, w"
C16-1309,D16-1075,1,0.770827,"it is likely that the clusters obtained by the methods are not eventcentric, which has an adverse effect on the result, as illustrated in Figure 1. Figure 1: Deviation of cluster centroids: If clusters are not constructed around the centroid of the events (e.g., the dashline cluster is constructed around non-centroids such as people, kill and injure instead of earthquake or bombing), the performance will be adversely affected. To offer a better solution to event detection without the above limitations, we propose to use a novel text stream representation: Burst Information Networks (BINets) (Ge et al., 2016a; Ge et al., 2016b). In contrast to the keyword graph which is based on word co-occurrence, a BINet is constructed based on burst co-occurrence. In a BINet (Fig. 2), a node is a burst of one word, which can be represented by the word with one of its burst periods, and an edge between two nodes indicates how strongly they are related (i.e., how frequently they co-occur). Since the nodes in a BINet contains temporal information (e.g., burst period), a BINet is time-aware in which nodes in a community are both topically and temporally coherent. Hence, we can say each community in a BINet corresp"
C16-1309,P14-5010,0,0.00286061,"event communities: C = [E1 , E2 , ..., Ek ] 3: while kLk > 0 do 4: A ← L[0] (the first element in L) 5: E ← {A} ∪ {A0 |f (A, A0 ) > σA } 6: L←L−E 7: C.append(E) 8: end while 3281 4 Experiments and Evaluation We conduct experiments to evaluate the performance of our approach. We first evaluate our approach on the TDT4 dataset to compare other event detection approaches. Then, we apply our approach on a larger corpus (2009 – 2010 news corpus) to test its scalability and performance. For preprocessing, we remove stopwords and conduct lemmatization and name tagging using Stanford CoreNLP toolkit (Manning et al., 2014) before the construction of a BINet. 4.1 Evaluation on TDT4 The TDT4 collection is a well known dataset for comparing methods for event detection. The English part of the dataset includes approximately 29,000 news documents from news agencies such as CNN and BBC from October 2000 to Janurary 2001 (spanning 4 months), while only 1,884 documents1 are annotated to be related to 71 human identified events (topics). As the setting adopted by previous work (Li et al., 2005; Sayyadi and Raschid, 2013), we use the annotated subset as gold standard for evaluating the performance of our models. As most"
C16-1309,P12-2009,0,0.190351,"based on the BINet representation, which not only solve the centroid deviation problem but also are more efficient than traditional approaches. • We construct and release a dataset for evaluating event detection models on a large text stream during a long time span. 2 2.1 Burst Information Networks Burst Detection A word’s burst refers to a sharp increase of word frequency during a period. It usually indicates key information, important events or trending topics in a text stream as Figure 3 shows and is useful for many applications. In this paper, we detect a word’s burst using the method of Zhao et al. (2012) which is a variant of (Kleinberg, 2003) and models burst detection as a burst state sequence decoding problem where a word w’s burst state st (w) at time t could be 1 or 0 to indicate if the word bursts or not at t. Specially, if a word w bursts at every time epoch during a period, we call this period a burst period of w and w has a burst during this period. In Figure 3, earthquake has 2 burst periods (i.e., Jan 12 - Jan 31, and Feb 27 - Mar 7), which correspond to two famous earthquake events (i.e., 2010 Haiti earthquake and 2010 Chile earthquake). 3277 … government police aid donation Haiti"
D08-1034,P98-1013,0,0.112798,"Missing"
D08-1034,boas-2002-bilingual,0,0.0127585,"of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and Màrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary w"
D08-1034,W04-2412,0,0.054297,"tic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and Màrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpu"
D08-1034,W05-0620,0,0.0851451,"Missing"
D08-1034,J08-2001,0,0.0229749,"Missing"
D08-1034,J02-3001,0,0.615386,"Missing"
D08-1034,kingsbury-palmer-2002-treebank,0,0.0864164,"Missing"
D08-1034,P04-1043,0,0.0795691,"Missing"
D08-1034,W05-0630,0,0.0745894,"(2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic 324 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 324–333, c Honolulu, October 2008. 2008 Association for Computational Linguistics role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although"
D08-1034,C04-1100,0,0.0307479,"Introduction Semantic Role labeling (SRL) was first defined in Gildea and Jurafsky (2002). The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and Màrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004),"
D08-1034,N04-1032,0,0.718239,"yanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and Màrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschi"
D08-1034,P03-1002,0,0.0262239,"ined in Gildea and Jurafsky (2002). The purpose of SRL task is to identify and classify the semantic roles of each predicate in a sentence. The semantic roles are marked and each of them is assigned a tag which indicates the type of the semantic relation with the related predicate. Typical tags include Agent, Patient, Source, etc. and some adjuncts such as Temporal, Manner, Extent, etc. Since the arguments can provide useful semantic information, the SRL is crucial to many natural language processing tasks, such as Question and Answering (Narayanan and Harabagiu 2004), Information Extraction (Surdeanu et al. 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and Màrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and J"
D08-1034,W03-1707,0,0.326652,"se SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al. (2005) has made some preliminary attempt on the idea of hierarchical semantic 324 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 324–333, c Honolulu, October 2008. 2008 Association for Computational Linguistics role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process"
D08-1034,W04-3212,0,0.0512111,"ary attempt on the idea of hierarchical semantic 324 Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 324–333, c Honolulu, October 2008. 2008 Association for Computational Linguistics role labeling. However, without considerations on how to utilize the characteristics of linguistically similar semantic roles, the purpose of the hierarchical system is to simplify the classification process to make it less time consuming. So the hierarchical system in their paper performs a little worse than the traditional SRL systems, although it is more efficient. Xue and Palmer (2004) did very encouraging work on the feature calibration of semantic role labeling. They found out that different features suited for different sub tasks of SRL, i.e. semantic role identification and classification. For semantic analysis, developing features that capture the right kind of information is crucial. Experiments on Chinese SRL (Xue and Palmer 2005, Xue 2008) reassured these findings. In this paper, we mainly focus on the semantic role classification (SRC) process. With the findings about the linguistic discrepancy of different semantic role groups, we try to build a 2-step semantic ro"
D08-1034,J08-2004,0,0.547605,"Surdeanu et al. 2003), and Machine Translation(Boas 2002). With the efforts of many researchers (Carreras and Màrquez 2004, 2005, Moschitti 2004, Pradhan et al 2005, Zhang et al 2007), different machine learning methods and linguistics resources are applied in this task, which has made SRL task progress fast. Compared to the research on English, the research on Chinese SRL is still in its infancy stage. Previous work on Chinese SRL mainly focused on how to transplant the machine learning methods which has been successful with English, such as Sun and Jurafsky (2004), Xue and Palmer (2005) and Xue (2008). Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus of Chinese. They just labeled the predicate-argument structures of ten specified verbs to a small collection of Chinese sentences, and used Support Vector Machines to identify and classify the arguments. This paper made the first attempt on Chinese SRL and produced promising results. After the PropBank (Xue and Palmer 2003) was built, Xue and Palmer (2005) and Xue (2008) have produced more complete and systematic research on Chinese SRL. Moschitti et al. (2005) has made some prelim"
D08-1034,N07-1069,0,0.0242181,"ouraging that the hierarchical SRC system outperformed the strong baseline built with traditional methods. And the selected features could be explained, which in turn proves that the linguistic discrepancy of semantic role groups not only exists but also can be captured. Then we integrated the idea of exploiting argument interdependence to further improve the performance of our system and explained linguistically why the results of our system were different from the ones in previous research. 332 Although we make discriminations of arguments and adjuncts, the analysis is still coarse-grained. Yi et al. (2007) has made the first attempt working on the single semantic role level to make further improvement. However, the impact of this idea is limited due to that the amount of the research target, ARG2, is few in PropBank. What if we could extend the idea of hierarchical architecture to the single semantic role level? Would that help the improvement of SRC? Acknowledgements This work was supported by National Natural Science Foundation of China under Grant No. 60303003 and National Social Science Foundation of China under Grant No. 06BYY048. We want to thank Nianwen Xue, for his generous help at the"
D08-1034,C98-1013,0,\N,Missing
D08-1034,P07-1026,0,\N,Missing
D10-1077,J05-4005,0,0.0797743,"Missing"
D10-1077,I05-3027,0,0.0606432,"Missing"
D10-1077,Y06-1012,0,0.386038,"Missing"
D10-1077,O03-4002,0,\N,Missing
D10-1077,I05-3025,0,\N,Missing
D13-1001,D08-1073,0,0.163414,"al analysis, document timestamps are very useful. For instance, temporal information retrieval models take into consideration the document’s creation time for document retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). I"
D13-1001,P12-1011,0,0.0784756,"ocument retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). In Chambers’s work, discriminative classifiers – maximum entropy (MaxEnt) classifiers were used by incorporating linguistic features and temporal con"
D13-1001,de-marneffe-etal-2006-generating,0,0.00993176,"Missing"
D13-1001,D11-1142,0,0.0108213,"allenges and their corresponding solutions are presented. We first discuss the event extraction and processing involving relative temporal relation mining, event coreference resolution and distinguishing specific extractions from generic ones in Section 3.1. Then, we show the confidence boosting algorithm in detail in Section 3.2. 3.1 Event extraction and processing As mentioned in previous sections, events play a key role in the propagation models. We define an event as a Subject-Predicate-Object (SPO) triple. To extract events from raw text, an open information extraction software - ReVerb (Fader et al., 2011) is used. ReVerb is a program that automatically identifies and extracts relationships from English sentences. It takes raw text as input and outputs SPO triples which are called extractions. However, extractions extracted by ReVerb cannot be used directly for our propagation models for three main reasons. First, the relative temporal relations between documents and the extractions are unavailable. Second, the extractions extracted from different documents do not have any connection even if they refer to the same event. Third, propagations from generic events are very likely to lead to propaga"
D13-1001,P09-1046,0,0.0213263,"mps are very useful. For instance, temporal information retrieval models take into consideration the document’s creation time for document retrieval and ranking (Kalczynski and Chou, 2005; Berberich et al., 2007) for better dealing with time-sensitive queries; some infor∗ Corresponding author mation retrieval applications such as Google Scholar can list articles published during the time a user specifies for better satisfying users’ needs. In addition, timeline summarization techniques (Hu et al., 2011; Binh Tran et al., 2013) and some event-event ordering models (Chambers and Jurafsky, 2008; Yoshikawa et al., 2009) also rely on the timestamps. Unfortunately, many documents on the web do not have a credible timestamp, as Chambers (2012) reported. Therefore, it is significant to date documents, that is to predict document creation time. One typical method for dating document is based on temporal language models, which were first used for dating by de Jong et al. (2005). They learned language models (unigram) for specific time periods and scored articles with normalized log-likelihood ratio scores. The other typical approach for the task was proposed by Nathanael Chambers (2012). In Chambers’s work, discri"
D14-1092,I05-3017,0,0.190856,"i to decide whether there is a word boundary between ci and ci+1 . Denote h1 as the hypothesis that it forms a word boundary (the corresponding result is βw1 w2 γ where w1 = ci−2 ci−1 ci and w2 = ci+1 ci+2 ), and h2 as the opposite hypothesis (then the corresponding result is βwγ where w = ci−2 ci−1 ci ci+1 ci+2 ). The posterior probability for these two hypotheses would be: − − − P (h1 |h ) ∝ PD (h1 |h )PM (h1 |h ) (9) − − − P (h2 |h ) ∝ PD (h2 |h )PM (h2 |h ) (10) 4 In this section, we test our joint model on PKU and MSRA datesets provided by the Second Segmentation Bake-off (SIGHAN 2005) (Emerson, 2005). Most previous works reported their results on these two datasets, this will make it convenient to directly compare our joint model with theirs. 4.1 PD (h1 |h− ) = PD (w1 |wl , h− ) × PD (w2 |w1 , h− )PD (wr |w2 , h− ) (11) PD (h2 |h− ) = PD (w|wl , h− ) (12) where wl (wr ) is the first word to the left (right) of w. And the posterior probabilities for the Bayesian HMM model is given as: PM (h1 |h− ) ∝ i+2 Y Pt (tj |tj−1 , h− )Pe (cj |tj , h− ) (13) j=i−2 PM (h2 |h− ) ∝ i+2 Y Setting The second SIGHAN Bakeoff provides several large-scale labeled data for evaluating the performance of Chinese"
D14-1092,J04-1004,0,0.108776,"ation School of Electronics Engineering and Computer Science, Peking University Beijing, P.R.China, 100871 miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn Abstract unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segm"
D14-1092,P06-2056,0,0.171015,"Computer Science, Peking University Beijing, P.R.China, 100871 miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn Abstract unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given t"
D14-1092,W99-0701,0,0.135303,"f Computational Linguistics, Ministry of Education School of Electronics Engineering and Computer Science, Peking University Beijing, P.R.China, 100871 miaohong-chen@foxmail.com,{chbb,peiwenzhe}@pku.edu.cn Abstract unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonp"
D14-1092,P12-2075,0,0.700957,"ail.com,{chbb,peiwenzhe}@pku.edu.cn Abstract unsupervised word segmentation methods are still attractive to researchers due to its independence on domain and manually labeled corpora. Previous unsupervised approaches to word segmentation can be roughly classified into two types. The first type uses carefully designed goodness measure to identify word candidates. Popular goodness measures include description length gain (DLG) (Kit and Wilks, 1999), accessor variety (AV) (Feng et al., 2004), boundary entropy (BE) (Jin and Tanaka-Ishii, 2006) and normalized variation of branching entropy (nVBE) (Magistry and Sagot, 2012) etc. Goodness measure based model is not segmentation model in a very strict meaning and is actually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given the observed character sequences. Typical statistical models includes Hierarchica"
D14-1092,P09-1012,0,0.622591,"ually strong in generating word list without supervision. It inherently lacks capability to deal with ambiguous string, which is one of main sources of segmentation errors and has been extensively explored in supervised Chinese word segmentation. The second type focuses on designing sophisticated statistical model, usually nonparametric Bayesian models, to find the segmentation with highest posterior probability, given the observed character sequences. Typical statistical models includes Hierarchical Dirichlet process (HDP) model (Goldwater et al., 2009), Nested PitmanYor process (NPY) model (Mochihashi et al., 2009) etc, which are actually nonparametric language models and therefor can be categorized as word-based model. Word-based model makes decision on wordhood of a candidate character sequence mainly based on information outside the sequence, namely, the wordhood of character sequences being adjacent to the concerned sequence. Inspired by the success of character-based model in supervised word segmentation, we propose a Bayesian HMM model for unsupervised Chinese word segmentation. With the Bayesian HMM model, we formulate the unsupervised segmentation tasks as procedure of tagging positional In this"
D14-1092,C04-1081,0,0.0497258,"es of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by 3 Joint Model In this section, we will discuss our joint model in detail. 855 3.1 Combining HDP and HMM tag set is {Single, Begin, Middle, End}. Specifically, S means the character forms a single word, B/E means the character is the begining/ending character of the word, and M means the character is in the middle of the word. Existing models are trained on manually annotated data in a supervised way based on discriminative models such as Conditional Random Fields (Peng et al., 2004; Tseng et al., 2005). Supervised character-based methods make full use of character level information and thus have been very successful in the last decade. However, no unsupervised model has utilized character level information in the way as supervised method does. We can also build a character-based model for Chinese word segmentation using hidden Markov model(HMM) as formulated in the following equation: In supervised Chinese word segmentation literature, word-based approaches and characterbased approaches often have complementary advantages (Wang et al., 2010).Since the two types of model"
D14-1092,I05-3027,0,0.0246259,"ures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by 3 Joint Model In this section, we will discuss our joint model in detail. 855 3.1 Combining HDP and HMM tag set is {Single, Begin, Middle, End}. Specifically, S means the character forms a single word, B/E means the character is the begining/ending character of the word, and M means the character is in the middle of the word. Existing models are trained on manually annotated data in a supervised way based on discriminative models such as Conditional Random Fields (Peng et al., 2004; Tseng et al., 2005). Supervised character-based methods make full use of character level information and thus have been very successful in the last decade. However, no unsupervised model has utilized character level information in the way as supervised method does. We can also build a character-based model for Chinese word segmentation using hidden Markov model(HMM) as formulated in the following equation: In supervised Chinese word segmentation literature, word-based approaches and characterbased approaches often have complementary advantages (Wang et al., 2010).Since the two types of model try to solve the pro"
D14-1092,C10-1132,0,0.0731274,"as Conditional Random Fields (Peng et al., 2004; Tseng et al., 2005). Supervised character-based methods make full use of character level information and thus have been very successful in the last decade. However, no unsupervised model has utilized character level information in the way as supervised method does. We can also build a character-based model for Chinese word segmentation using hidden Markov model(HMM) as formulated in the following equation: In supervised Chinese word segmentation literature, word-based approaches and characterbased approaches often have complementary advantages (Wang et al., 2010).Since the two types of model try to solve the problem from different perspectives and by utilizing different levels of information (word level and character level). In unsupervised Chinese word segmentation literature, the HDP-base model can be viewed as a typical word-based method. And we can also build a character-based unsupervised model by using a hidden Markov model. We believe that the HDPbased model and the HMM-based model are also complementary with each other, and a combination of them will take advantage of both and thus capture different levels of information. Now the problem we ar"
D14-1092,J11-3001,0,0.527859,"and machine translation. A great deal of supervised methods have been proposed for Chinese word segmentation. While successful, they require manually labeled resources and often suffer from issues like poor domain adaptability. Thus, 854 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 854–863, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics Feng et al. (2004) and Boundary Entropy (Jin and Tanaka-Ishii, 2006). A notable goodness-based method is ESA: “Evaluation, Selection, Adjustment”, which is proposed by Wang et al. (2011) for unsupervised Mandarin Chinese word segmentation. ESA is an iterative model based on a new goodness algorithm that adopts a local maximum strategy and avoids threshold setting. One disadvantage of ESA is that it needs to iterate the process several times on the corpus to get good performance. Another disadvantage is the requirement for a manually segmented training corpus to find best value for parameters (they called it proper exponent). Another notable work is nVBE: Magistry and Sagot (2012) proposed a model based on the Variation of Branching Entropy. By adding normalization and viterbi"
D14-1092,O03-4002,0,0.0594272,"s way does not involve any extra parameters and Gibbs sampling can be easily used for model inference. 3.2 |C| Y ti |ti−1 = t, pt ∼ M ult(pt ) ci |ti = t, et ∼ M ult(et ) pt |θ ∼ Dirichlet(θ) et |σ ∼ Dirichlet(σ) where pt and et are transition and emission distributions, θ and σ are the symmetric parameters of Dirichlet distributions. Now suppose we have observed tagged text h, then the conditional probability PM (wi |wi−1 = l, h) can be obtained: Bayesian HMM PM (wi |wi−1 = l, h) The dominant method for supervised Chinese word segmentation is character-based model which was first proposed by Xue (2003). This method treats word segmentation as a tagging problem, each tag indicates the position of a character within a word. The most commonly used |wi | = Y Pt (tj |tj−1 , h)Pe (cj |tj , h) (3) j=1 where < wi−1 , wi > is a word bigram, l is the index of word wi−1 , cj is the jth character in word 856 wi and tj is the corresponding tag.Pt (tj |tj−1 , h) and Pe (cj |tj , h) are the posterior probabilities, they are given as: n<tj−1 ,tj > + θ n<tj−1 ,∗> + T θ n<tj ,cj > + σ Pe (cj |tj , h) = n<tj ,∗> + V σ Pt (tj |tj−1 , h) = where twi denotes the number of tables associated with wi in the Chinese"
D14-1092,I08-1002,0,0.0889424,"aper is organized as follows. In Section 2, we will introduce several related systems for unsupervised word segmentation. Then our joint model is presented in Section 3. Section 4 shows our experiment results on the benchmark datasets and Section 5 concludes the paper. 2 Related Work Unsupervised Chinese word segmentation has been explored in a number of previous works and by various methods. Most of these methods can be divided into two categories: goodness measure based methods and nonparametric Bayesian methods. There have been a plenty of work that is based on a specific goodness measure. Zhao and Kit (2008) compared several popular unsupervised models within a unified framework. They tried various types of goodness measures, such as Description Length Gain (DLG) proposed by Kit and Wilks (1999), Accessor Variety (AV) proposed by 3 Joint Model In this section, we will discuss our joint model in detail. 855 3.1 Combining HDP and HMM tag set is {Single, Begin, Middle, End}. Specifically, S means the character forms a single word, B/E means the character is the begining/ending character of the word, and M means the character is in the middle of the word. Existing models are trained on manually annot"
D14-1092,zhang-etal-2004-interpreting,0,0.0732283,"Missing"
D15-1099,D14-1193,1,0.645722,"Missing"
D15-1099,W07-1013,0,0.033082,"els S exactly. The 0/1 loss is defined as follows: Hammingloss = x) 6= y ) 0/1loss = I(h(x (6) Let pj and rj denote the precision and recall for the j-th label. The macro-averaged F score is a harmonic mean between precision and recall, defined as follows: Experiments 3.1 n 3782 978 1702 28596 Datasets We perform experiments on four real world data sets: 1) the first data set is Slashdot (Read et al., 2011). The Slashdot data set is concerned about predicting multiple labels given science and technology news titles and partial blurbs mined from Slashdot.org. 2) the second data set is Medical (Pestian et al., 2007). This data set involves the assignment of ICD-9-CM codes to radiology reports. 3) The third data set is Enron. The enron data set is a subset of the Enron Email Dataset, as labelled by the UC Berkeley Enron Email Analysis Project2 . It is concerned about classifying emails into some categories. 4) the fourth data set m F score = 1 X 2 ∗ pj ∗ rj m pj + rj (7) i=j 3.3 Method Setup In this paper, we focus on the predictions-asfeatures style methods, and use CC and LEAD as the baselines. Our methods are JCC and JLEAD. JCC(JLEAD) is CC(LEAD) trained by our joint algorithm and we compare JCC(JLEAD)"
D15-1185,D11-1142,0,0.0773194,"married to a doctor who lives in Austin, the capital of Texas. T1. bemarriedTo(Ayrton Senna,Doctor) T1. Ayrton Senna was married to a doctor T2. livein(Doctor, Austin) T2. [The] doctor lives in Austin T3. beCapitalof(Austin, Texas) T3. Austin [is] the capital of Texas H1. livein(Ayrton Senna, Texas) Hypothesis: Ayrton Senna lives in Texas. Hypothesis: Ayrton Senna lives in Texas. Figure 2: Text Commitments Example Figure 3: Text Predicates Example ments to predicates. For example, the commitments in Figure 2 can be transformed to the predicates (or triples) shown in Figure 3. We use R E VERB (Fader et al., 2011) to extract the triples (predicate + 2 Arguments). To make the inference process in the next section more convenient, we order that all of the arguments should be NPs. Therefore, we check if the arguments in the triples contain or have overlap with any of the NPs, replace it with that NP, and the predicates are successfully extracted. facts. We use AMIE to mine inference rules from YAGO. AMIE1 (Gal´arraga et al., 2013) is a state-ofthe-art inference rule mining system. The motivation of AMIE is that KBs themselves often already contain enough information to derive and add new facts. If, for ex"
D15-1185,H05-1049,0,0.143446,"n et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likel"
D15-1185,C08-1043,0,0.722221,"atural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments from T which are most likely to support the"
D15-1185,N06-1006,0,0.0918988,"Missing"
D15-1185,W07-1407,0,0.79166,"(entailed) from the other one (T )(Dagan et al., 2006). For many natural language processing applications like question answering, information retrieval which involve the diversity of natural language, recognising textual entailments is a critical step. PASCAL Recognizing Textual Entailment (RTE) Challenges (Dagan et al., 2006) have witnessed a variety of excellent systems which intend to recognize the textual entailment instances. These systems mainly employ “shallow” techniques, including heuristics, term overlap, syntactic dependencies(Vanderwende et al., 2006; Jijkoun and de Rijke, 2005; Malakasiotis and Androutsopoulos, 2007; Haghighi et al., 2005). As Hickl (2008) stated, the shallow approaches do not work well for long sentences for the missing of underlying information which needs to be mined from the surface level expression. Recently, some deep techniques are developed to mine the facts latent in the text. Hickl (2008) proposed the concept of discourse commitments which can be seen as the set of propositions inferred from the text, and used a series of syntaxlevel and semantic-level rules to extract the commitments from the T -H pairs. Then the RTE task is reduced to the identification of the commitments fro"
D15-1185,P09-1113,0,0.0204461,"in Austin, the capital of Texas, in 1998. H: Ayrton Senna lives in Texas. R T: R1(e11,e12) R2(e21,e22) R3(e31,e32) H: RH(eH1,eH2) Discourse commitment extract YAGO R .. R YAGO H Inference rules R R T R AIME .. R R R1(e11,e12)^R2(e21,e22)=&gt;R3(e31,e32) R4(e41,e42)^R5(e51,e52)=&gt;R6(e61,e62) R7(e71,e72)^R8(e81,e82)=&gt;R9(e91,e92) MLN Construct Markov Logic Network .. Rp(ep1,ep2)^Rq(eq1,eq2)=&gt;Rr(er1,er2) facts P(RH) H is True or false? train Figure 1: The Framework of our RTE system pose to use the predicate-argument structure to represent the extracted discourse commitments. Inspired by the work of (Mintz et al., 2009), we make use of the external knowledge YAGO and borrow the distant supervision technique to mine implicit facts for the extracted predicates. For example, Ayrton Senna was married to a doctor who lives in Austin, the capital of Texas, in 1998. We translate this example into the predicateargument structures such as bemarried(Senna, doctor), livein(doctor, Austin), captial(Austin, Texas). Then through distant supervision, we can get some new facts livein(Senna, Austin), livein(Senna, Texas). To judge the confidence of the new facts, we construct a probabilistic network with all the facts and ad"
D15-1185,P06-2105,0,0.216099,"of the commitments from H. From the work of Hickl (2008), we can see that a deep understanding of text is critical to the RTE performance and discourse commitments can serve a good media to understanding text. However, the limitation of Hickl (2008)’s work is, the extracted discourse commitments are still from the original text and do not explore the implicit meaning latent behind the text. Another kind of deep methods involves first transferring natural language to logic representation and then conducting strict logic inference based on the logic representations (de Salvo Braz et al., 2006; Tatu and Moldovan, 2006; Wotzlaw and Coote, 2013). Through logic inference, some implicit knowledge behind the text can be mined. However, it is not easy to translate the natural language text into formal logic expressions and the translation process inevitably suffer from great information loss. Through analysis above, in our work, we pro1620 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1620–1625, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. Align the predicates to the YAGO database T: Ayrton Senna was married to a doctor"
D15-1185,U06-1019,0,0.10598,"ng can be corrected by our framework. For instance, T is “Hughes loved his wife, Gracia, and was absolutely obsessed with his little daughter Elicia.” and H is “Gracia’s daughter is Elicia.” It is not easy for the former baselines to recognize this entailment, but our framework can easily recognize it to be “true”. In this way, our framework has achieved a higher result. 4 Related work Textual Entailment Recognizing (RTE) task has been widely studied by many previous works. Firstly, the method based on similarity and overlap (Malakasiotis and Androutsopoulos, 2007; Jijkoun and de Rijke, 2005; Wan et al., 2006). This kind of methods can help solve the paraphrase recognition problem, which is a subset of RTE. Another important similarity-based method is tree kernel (Zanzotto and Moschitti, 2006), which rely on the cross-pair similarity between two pairs (T 0 , H 0 ) and (T 00 , H 00 ). Secondly, some approaches extract the knowledge in T -H pair and check if the knowledge in T contains the knowledge in H. Hickl (2008) transformed the T -H pair into discourse commitments, reducing the RTE task to the identification of the commitments from a T which support the inference of the H. Other works map the t"
D15-1185,P06-1051,0,0.467153,"ing predicates and mining inference rules. YAGO2 contains more than 940K facts and about 470K entities. We run the AMIE system on YAGO2 for only one time to get all inference rules (about more than 1.8K in total). For each T -H pair, we only choose a portion of related inference rules to construct MLN. The chosen rules must contain at least one predicate which occurred in the predicates of T -H pair. We only use the MLN to infer when the discourse commitment paraphrasing cannot identify a T -H pair as ”Entailment”, which is a back-off method. We compare our result with 5 baseline systems: (1) Zanzotto and Moschitti (2006)’s simple termoverlap measure, (2) MacCartney et al. (2006)’s semantic graph-mapping approach, (3) Hickl et al. (2006)’s classification-based term alignment approach. (4) Hickl (2008)’s discourse commitment based Alignment, (5) Tatu and Moldovan (2006)’s strict logic based method. The comparison of the 5 baselines and our framework is shown in Table 1. Since we only need to judge “Yes” or “No” for the 1600 examples, the precision is equal to the recall, so that we only report the precision. According to the Table 1, the performance of our framework is higher than Hickl (2008)’s baseline, which"
D15-1186,P06-2013,0,0.0308745,"ho presented a a system based on statistical classifiers trained on hand-annotated corpus FrameNet. Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3 1629 https://code.google.com/p/word2vec/ word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can"
D15-1186,D08-1034,1,0.921176,"ssign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent semantic meaning, can"
D15-1186,J02-3001,0,0.485015,"ing. Experimental results on Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Figure 1: A sentence with semantic roles labeled from CPB. Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engine"
D15-1186,P15-1109,0,0.0484622,"mation over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. In our approach, the bidirectional and long-range dependencies in a sentence, which are important for Chinese SRL, can be well modeled. And with the framework of deep neural netwo"
D15-1186,N04-1032,0,0.825387,"n Chinese Proposition Bank (CPB) show a significant improvement over the state-ofthe-art methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Figure 1: A sentence with semantic roles labeled from CPB. Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain"
D15-1186,D09-1153,1,0.93767,"Table 2, compared to standard pre-training, the influence of heterogenous data is more evident. We can explain this difference via the distinction between these two kinds of methods for performance improvement. The information provided by standard pretraining with unlabeled data is more general, while that of heterogenous resource is more relevant to our task, hence is more informative and evident. 3.1 Experimental Setting To facilitate comparison with previous work, we conduct experiments on the standard benchmark dataset CPB 1.0.1 We follow the same data setting as previous work (Xue, 2008; Sun et al., 2009), which divided the dataset into three parts: 648 files (from chtb 081.fid to chtb 899.fid) are used as the training set. The development set includes 40 files, from chtb 041.fid to chtb 080.fid. The test set includes 72 files, which are chtb 001.fid to chtb 040.fid, and chtb 900.fid to chtb 931.fid. We use another annotated corpus2 with distinct semantic role labels and annotation schema, which is designed by ourselves for other projects, as heterogeneous resource. This labeled dataset has 17,308 annotated sentences, and the semantic roles concerned are like “agent” and “patient”, resulting i"
D15-1186,P10-2031,0,0.609207,"of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent semantic meaning, can alleviate this problem. However,"
D15-1186,D14-1003,0,0.0178283,", Hochreiter and Schmidhuber (1997) proposed long-short-term memory (LSTM), which has been shown capable of storing and accessing information over very long time spans. Bidirectional RNN (Schuster and Paliwal, 1997) and bidirectional LSTM RNN (Graves et al., 2005) are the extensions of RNN and LSTM RNN with the capability of capturing contextual information from both directions in the sequence. In recent years, RNN has shown the state-of-theart results in many NLP problems such as language modeling (Mikolov et al., 2010) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2014). Sundermeyer et al. (2014) also used bidirectional LSTM RNN model to improve strong baselines when modeling translation. More recently, Zhou and Xu (2015) proposed LSTM RNN approach for English Semantic Role Labeling, which shared similar idea with our model. However, the features used and the network architecture were different from ours. Moreover, it is delightful that our work can achieve a rather good result with a relatively simpler model architecture. 5 Conclusion In this paper, we formulate Chinese SRL problem with the framework of bidirectional LSTM RNN model. In our approach, the bidirectional and long-range d"
D15-1186,W03-1707,0,0.829677,"rt methods. Moreover, our model makes it convenient to introduce heterogeneous resource, which makes a further improvement on our experimental performance. 1 Figure 1: A sentence with semantic roles labeled from CPB. Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, althoug"
D15-1186,J08-2004,0,0.16159,"icate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Traditional approaches to Chinese SRL often extract a large number of handcrafted features from the sentence, even its parse tree, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). However, these methods suffer from three major problems. Firstly, their performances are heavily dependent on feature engineering, which needs domain knowledge and laborious work of feature extraction and selection. Secondly, although sophisticated features are designed, the long-range dependencies in a sentence can hardly be modeled. Thirdly, a specific annotated dataset is often limited in its scalability, but the existence of heterogenous resource, which has very different semantic role labels and annotation schema but related latent"
D15-1186,D14-1041,0,0.122523,"un and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. After CPB (Xue and Palmer, 2003) was built, Xue and Palmer (2005) and Xue (2008) produced more complete and systematic research on Chinese SRL. Ding and Chang (2009) established a 3 1629 https://code.google.com/p/word2vec/ word based Chinese SRL system, which is quite different from the previous parsing based ones. Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Different from most work relying on a large number of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature engineering. However, by max-pooling operation, the convolution approach only preserved the most evident features in a sentence, thus can only weakly model the dependencies. With our bidirectional LSTM RNN model, this problem can be well all"
D15-1289,P14-1013,0,0.0145563,"epresentations for entities from their binary term vectors with an unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. separatively calculating the similarities and aggregating them later with a combination method; 2) the learned representation can not only express the meaning of the original descriptions of an entity but also captures the interactions among different descriptions. 3.1.1 Creating term vector for entity We first generate a"
D15-1289,P13-2006,0,0.176723,"rn the abstract representations for entities from their binary term vectors with an unsupervised way. The deep neural network (DNN) (Hinton et al., 2006; Bengio et al., 2007) is a multilayer learning model. It is mainly used for learning the high-level abstract representations of original input data. Given the generalization and the abstraction introduced in the representation learning procedure, DNN allows us to better model the interactions among different kinds of input features, and measure the similarity at a more general level. Inspired by the work in (Hinton, 2007; Bengio et al., 2012; He et al., 2013; Cui et al., 2014), we use auto-encoder (Bourlard and Kamp, 1988; Hinton and Zemel, 1994) to learn the representations for classes and properties. The auto-encoder is one of the neural network variants that can automatically discover interesting abstractions to represent an unlabeled dataset. separatively calculating the similarities and aggregating them later with a combination method; 2) the learned representation can not only express the meaning of the original descriptions of an entity but also captures the interactions among different descriptions. 3.1.1 Creating term vector for entity W"
D15-1289,W14-2416,0,0.0234937,"Missing"
D15-1289,P14-1090,0,0.0855721,"Missing"
D16-1035,P14-1048,0,0.246512,"represented by a Discourse Tree (DT). Each leaf of a DT denotes a text unit referred to as an Elementary Discourse Unit (EDU) and an inner node of a DT represents a text span which is constituted by several adjacent EDUs. DTs can be utilized by many NLP tasks including automatic document summarization (Louis et al., 2010; Marcu, 2000), question-answering (Verberne et al., 2007) and sentiment analysis (Somasundaran, 2010) etc. Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art approaches heavily rely on manual feature engineering (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of Li et al. (2014a). Li et al. (2014a) propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function t"
D16-1035,P14-1002,0,0.341944,"ourse Tree (DT). Each leaf of a DT denotes a text unit referred to as an Elementary Discourse Unit (EDU) and an inner node of a DT represents a text span which is constituted by several adjacent EDUs. DTs can be utilized by many NLP tasks including automatic document summarization (Louis et al., 2010; Marcu, 2000), question-answering (Verberne et al., 2007) and sentiment analysis (Somasundaran, 2010) etc. Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art approaches heavily rely on manual feature engineering (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of Li et al. (2014a). Li et al. (2014a) propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function they use is weak at modelin"
D16-1035,P13-1048,0,0.681775,"f a document can be represented by a Discourse Tree (DT). Each leaf of a DT denotes a text unit referred to as an Elementary Discourse Unit (EDU) and an inner node of a DT represents a text span which is constituted by several adjacent EDUs. DTs can be utilized by many NLP tasks including automatic document summarization (Louis et al., 2010; Marcu, 2000), question-answering (Verberne et al., 2007) and sentiment analysis (Somasundaran, 2010) etc. Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art approaches heavily rely on manual feature engineering (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of Li et al. (2014a). Li et al. (2014a) propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal tra"
D16-1035,D13-1176,0,0.0235927,"iants of Conditional Random Fields (CRFs) are mostly used in these models. Li et al. (2014b) distinctively propose to use dependency structure to represent the relations between EDUs. Recursive deep model proposed by Li et al. (2014a) has been the only proposed deep learning model on RST-DT. Incorporating attention mechanism into RNN (e.g., LSTM, GRU) has been shown to learn better representation by attending over the output vectors and picking up important information from relevant positions of a sequence and this approach has been utilized in many tasks including neural machine translation (Kalchbrenner and Blunsom, 2013; Bahdanau et al., 2014; Hermann et al., 2015), text entailment recognition (Rockt¨aschel et al., 2015) etc. Some work also uses tensor-based transformation function to make stronger interaction between features and learn combinatorial features and they get performance boost in their tasks (Sutskever et al., 2009; Socher et al., 2013; Pei et al., 2014). 7 Conclusion In this paper, we propose an attention-based hierarchical neural network for discourse parsing. Our attention-based hierarchical bi-LSTM network produces effective compositional semantic representations of text spans. We adopt tens"
D16-1035,D14-1220,0,0.12194,"al., 2010; Marcu, 2000), question-answering (Verberne et al., 2007) and sentiment analysis (Somasundaran, 2010) etc. Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art approaches heavily rely on manual feature engineering (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of Li et al. (2014a). Li et al. (2014a) propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function they use is weak at modeling complicated interactions which has been stated by Socher et al. (2013). As many documents contain more than a hundred EDUs which form quite a long sequence, those weaknesses may lead to inferior results on this task. In this paper, we propose to use a hierarchical bidirectional Lo"
D16-1035,P14-1003,0,0.366342,"al., 2010; Marcu, 2000), question-answering (Verberne et al., 2007) and sentiment analysis (Somasundaran, 2010) etc. Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art approaches heavily rely on manual feature engineering (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of Li et al. (2014a). Li et al. (2014a) propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function they use is weak at modeling complicated interactions which has been stated by Socher et al. (2013). As many documents contain more than a hundred EDUs which form quite a long sequence, those weaknesses may lead to inferior results on this task. In this paper, we propose to use a hierarchical bidirectional Lo"
D16-1035,W10-4327,0,0.0586741,"o identify the relations between the text units and to determine the structure of the whole document the text units form. Rhetorical Structure Theory (RST) (Mann and Thompson, 1988) is one of the most influential discourse theories. According to RST, the discourse structure of a document can be represented by a Discourse Tree (DT). Each leaf of a DT denotes a text unit referred to as an Elementary Discourse Unit (EDU) and an inner node of a DT represents a text span which is constituted by several adjacent EDUs. DTs can be utilized by many NLP tasks including automatic document summarization (Louis et al., 2010; Marcu, 2000), question-answering (Verberne et al., 2007) and sentiment analysis (Somasundaran, 2010) etc. Much work has been devoted to the task of RSTstyle discourse parsing and most state-of-the-art approaches heavily rely on manual feature engineering (Joty et al., 2013; Feng and Hirst, 2014; Ji and Eisenstein, 2014). While neural network models have been increasingly focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of Li et al."
D16-1035,P14-5010,0,0.00261096,"h those words using a matrix W ∈ R50×50 that maps word embeddings from the pre-trained word embedding space to the fine-tuned word embedding space. The objective function for training the matrix W is as follows: min ||Vtuned − Vpretrained W − b||22 W,b (29) where Vtuned , Vpretrained ∈ R|V |×50 contain finetuned and pre-trained embeddings of words appearing in training set respectively, |V |is the size of RST-DT training set vocabulary and b is the bias term also to be trained. We lemmatize all the words appeared and represent all numbers with a special token. We use Stanford CoreNLP toolkit (Manning et al., 2014) to preprocess the text including lemmatization, POS tagging etc. We use Theano library (Bergstra et al., 2010) to implement our parsing model. We randomly initialize all parameters within (-0.012, 0.012) except word embeddings. We adopt dropout strategy (Hinton et al., 2012) to avoid overfitting and we set the dropout rate to be 0.3. 5.2 Results and Analysis To show the effectiveness of the components incorporated into our model, we firstly test the performance of the basic hierarchical bidirectional LSTM network without attention mechanism (ATT), tensor-based transformation (TE) and handcraf"
D16-1035,P14-1028,1,0.933923,"utput layer: ysp = σ(ws vsp + bs ) (16) ynu = sof tmax(Wn vnu + bn ) (17) yrel = sof tmax(Wr vrel + br ) (18) where ws ∈ Rh , bs ∈ R, Wn ∈ R3×h , Wn ∈ R3×h , bn ∈ R3 , Wr ∈ Rnr ×h , bn ∈ Rnr are parameters and nr is the number of different discourse relations. The first classifier is a binary classifier which outputs the probability the two spans should be combined. The second classifier is a multiclass classifier 365 Tensor-based transformation function has been successfully utilized in many tasks to allow complicated interaction between features (Sutskever et al., 2009; Socher et al., 2013; Pei et al., 2014). Based on the intuition that allowing complicated interaction between the features of the two spans may help to identify how they are related, we adopt tensor-based transformation function to strengthen our model. A tensor-based transformation function on x ∈ Rd1 is as follows: y = W x + xT T [1:d2 ] x + b (19) X X [i] yi = Wij xj + Tj,k xj xk + bi (20) j j,k where y ∈ Rd2 is the output vector, yi ∈ R is the ith element of y, W ∈ Rd2 ×d1 is the transformation matrix, T [1:d2 ] ∈ Rd1 ×d1 ×d2 is a 3rd-order transformation tensor. A normal transformation function in neural network models only ha"
D16-1035,D14-1162,0,0.117881,"Missing"
D16-1035,prasad-etal-2008-penn,0,0.0217162,"ss weight to Span1 (EDU30∼EDU32) That means that if the offense deals with one part of the business, you don’t attempt to seize the whole business; Span2 (EDU33) you attempt to seize assets related to the crime, W 0.13 0.38 0.49 W 1.0 Table 6: An example of the weights derived from our attention model. The relation between span1 and span2 is Contrast. 369 EDU30 and focuses more on EDU32 which is reasonable according to our analysis above. 6 Related Work Two most prevalent discourse parsing treebanks are RST Discourse Treebank (RST-DT) (Carlson et al., 2003) and Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). We evaluate our system on RST-DT which is annotated in the framework of Rhetorical Structure Theory (Mann and Thompson, 1988). It consists of 385 Wall Street Journal articles and is partitioned into a set of 347 documents for training and a set of 38 documents for test. 110 fine-grained and 18 coarse-grained relations are defined on RST-DT. Parsing algorithms published on RST-DT can mainly be categorized as shift-reduce parsers and probabilistic CKY-like parsers. Shiftreduce parsers are widely used for their efficiency and effectiveness and probabilistic CKY-like parsers lead to the global o"
D16-1035,D13-1170,0,0.0946186,"focused on for their ability to automatically extract efficient features which reduces the burden of feature engineering, there is little neural network based work for RST-style discourse parsing except the work of Li et al. (2014a). Li et al. (2014a) propose a recursive neural network model to compute the representation for each text span based on the representations of its subtrees. However, vanilla recursive neural networks suffer from gradient vanishing for long sequences and the normal transformation function they use is weak at modeling complicated interactions which has been stated by Socher et al. (2013). As many documents contain more than a hundred EDUs which form quite a long sequence, those weaknesses may lead to inferior results on this task. In this paper, we propose to use a hierarchical bidirectional Long Short-Term Memory (bi-LSTM) network to learn representations of text spans. Comparing with vanilla recursive/recurrent neural networks, LSTM-based networks can store information for a long period of time and don’t suffer from gradient vanishing problem. We apply a hierarchical bi-LSTM network because the way words form an EDU and EDUs form a text span is different and thus they shoul"
D16-1075,P15-1056,1,0.182234,"challenges for summarizing a text stream. First, a stream summarization model should be able to be aware of redundant information in the stream for avoiding generating redundant content in the summary; second, a stream summarization algorithm should be capable of analyzing text content on the stream level for identifying the most important information in the stream; third, a stream summarization model should be efficient, scalable and able to run in an online fashion because data size of a text stream is usually huge, and it is dynamic and updated every second. The previous approaches (e.g., (Ge et al., 2015b)) tend to cluster similar documents as event detection to avoid redundancy, rank the clusters based on their sizes and topical relevance to the reference summaries, and select one document from each cluster as representative documents. Due to the high time complexity of clustering models, their approaches usually run slowly and are not scalable. 785 To overcome the limitations, we propose Burst Information Networks (BINet) as a novel representation of a text stream. In a BINet (Figure 2), a node is a burst word (including entities) with the time span of one of its burst periods, and an edge"
D16-1075,P15-1155,0,0.0215125,"xt stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem in the big data age a"
D16-1075,P13-2099,1,0.841386,"on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select important sentences, the focus of stream summarization is to select representative documents referring to important news events. The novel paradigm focuses on the summarization problem"
D16-1075,W04-1013,0,0.022973,"Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated by various approaches, annotated each entry based on the reference summary and the manually edited event chronicles on the web, and used precision@K to evaluate the quality of top K event entries in a stream summary instead of using ROUGE (Lin, 2004) because news stream summaries are eventcentric. In this paper, we adopt the same evaluation setting and use the same reference summaries and the annotations with our previous work (Ge et al., 2015b) to evaluate our summaries’ quality. For the event entries that are not in Ge et al. (2015b)’s annotations, we have 3 human judges annotate them according to the previous annotation guideline and consider an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseli"
D16-1075,P14-5010,0,0.00387579,"APW and XIN news stories in English Gigaword (Graff et al., 2003)) as a news stream. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which models word burst detection as a burst state decoding problem. In total, there are 140,557 documents in the dataset. Topic Disaster Sports Politics Military Comprehensive #Entry 35 19 8 14 85 #Entry in corpus 28 12 5 13 64 Table 2: The number of event entries in the reference summaries. The third column is the number of event entries excluding those events that do not appear in the corpus. We removed stopwords and used Stanford CoreNLP (Manning et al., 2014) to do lemmatization and named tagging, and built BINets on the news stream during 2009 and 2010 separately. On the 2009 news stream, there are 31,888 nodes and 833,313 edges while there are 32,997 nodes and 825,976 edges on the 2010 stream. Ge et al. (2015b) used manually edited event chronicles of various topics on the web3 during 2009 3 http://www.mapreport.com; http://www.infoplease.com; 789 as reference summaries for summarizing the news stream during 2010. The information of the reference summaries is summarized in Table 2. In evaluation, they pooled entries in stream sumamries generated"
D16-1075,Q14-1015,0,0.0274957,"marization challenge. However, they studied the problem on a static timestamped corpus instead of on a dynamic text stream and their proposed pipeline-style approach cannot be applied on a real-time text stream due to high complexity in time and space. Other previous work dealing with stream data is mainly focused on topic and event detection (Yang et al., 1998; Swan and Allan, 2000; Allan, 2002; He et al., 2007; Sayyadi et al., 2009; Sakaki et al., 2010; Zhao et al., 2012; Ge et al., 2015a), dynamic language and topic modelling (Blei and Lafferty, 2006; Iwata et al., 2010; Wang et al., 2012; Yogatama et al., 2014), incremental (temporal) summarization and timeline generation for one major news event (Allan et al., 2001; Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013; Kedzie et al., 2015; Tran et al., 2015; Yao et al., 2016), a sports match (Takamura et al., 2011) or users on the social network (Li and Cardie, 2014). Different from traditional single and multi792 document summarization (Carbonell and Goldstein, 1998; Lin, 2004; Erkan and Radev, 2004; Conroy et al., 2004; Li et al., 2007; Wan and Yang, 2008; Chen and Chen, 2012; Wan and Zhang, 2014) whose focus is to select importan"
D16-1075,P12-2009,0,0.164471,"r an entry correct if it is annotated as correct by at least 2 judges. We evaluate our approaches by comparing to Ge et al. (2015b)’s approach and the baselines in their work: • R ANDOM: this baseline randomly selects documents in the dataset as event entries. • N B: this baseline uses Naive Bayes to cluster documents for event detection and ranks the clusters based on the combination score of topical relevance and the event impact (i.e., event cluster size). The earliest documents in the topranked clusters are selected as entries. • B-H AC: similar to N B except that BurstVSM representation (Zhao et al., 2012) is used for event detection using Hierarchical Agglomerative Clustering algorithm. • TA HBM: similar to N B except that the stateof-the-art event detection model (TaHBM) proposed by Ge et al. (2015b) is used for event detection. • Ge et al. (2015b): the state-of-the-art stream summarization approach which used TaHBM to detect events and L2R model to rank events. Note that we did not compare with previous multidocument summarization models because the goal and setting of stream summarization are different from multi-document summarization, as Section 1 https://en.wikipedia.org/wiki/2009 Random"
D16-1075,N16-3015,0,\N,Missing
D16-1212,P06-2013,0,0.553957,") task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible ar"
D16-1212,D08-1034,1,0.920352,"Missing"
D16-1212,J02-3001,0,0.822677,"revious works did not explicitly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predica"
D16-1212,P16-1116,1,0.675445,"ial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible arguments and incompatible arguments into account. Inspired by Sha et al. (2016), our approach model the two argument relationships explicitly to achieve a better performance on Chinese SRL. 3 Capturing the Relationship Between Arguments We found that there are two typical relationships between candidate arguments: (1) Compatible arguments: if one candidate argument belongs to one 2012 event, then the other is more likely to belong to the same event; (2) incompatible arguments: if one candidate argument belongs to one event, then the other is less likely to belong to the same event. We trained a maximum entropy classifier to predict the relationship between two candidate"
D16-1212,N04-1032,0,0.812915,"citly model argument relationships. We use a simple maximum entropy classifier to capture the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is mor"
D16-1212,D09-1153,1,0.914347,"elated Work Semantic Role Labeling (SRL) task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned a"
D16-1212,P10-2031,0,0.246301,"Relationships for Chinese Semantic Role Labeling Lei Sha, Tingsong Jiang, Sujian Li, Baobao Chang, Zhifang Sui Key Laboratory of Computational Linguistics, Ministry of Education School of Electronics Engineering and Computer Science, Peking University Collaborative Innovation Center for Language Ability, Xuzhou 221009 China shalei, tingsong, lisujian, chbb, szf@pku.edu.cn Abstract large number of handcrafted features from the sentence, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). Neural network based approaches usually take Chinese SRL as sequence labeling task and use bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem (Wang et al., 2015). In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task’s performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same pr"
D16-1212,D15-1186,1,0.468647,"neering and Computer Science, Peking University Collaborative Innovation Center for Language Ability, Xuzhou 221009 China shalei, tingsong, lisujian, chbb, szf@pku.edu.cn Abstract large number of handcrafted features from the sentence, and feed these features to statistical classifiers such as CRF, MaxEnt and SVM (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008; Ding and Chang, 2009; Sun, 2010). Neural network based approaches usually take Chinese SRL as sequence labeling task and use bidirectional recurrent neural network (RNN) with long-short-term memory (LSTM) to solve the problem (Wang et al., 2015). In this paper, we capture the argument relationships for Chinese semantic role labeling task, and improve the task’s performance with the help of argument relationships. We split the relationship between two candidate arguments into two categories: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candidate argument belongs to a given predicate, then the other is less likely to belong to the same predicate. However, previous works did not explicitly model argument"
D16-1212,W03-1707,0,0.841245,"ure the two categories of argument relationships and test its performance on the Chinese Proposition Bank (CPB). The experiments show that argument relationships is effective in Chinese semantic role labeling task. 1 Introduction Semantic Role Labeling (SRL) is defined as the task to recognize arguments for a given predicate and assign semantic role labels to them. Because of its ability to encode semantic information, there has been an increasing interest in SRL on many languages (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004). Figure 1 shows an example in Chinese Proposition Bank (CPB) (Xue and Palmer, 2003), which is a Chinese corpus annotated with semantic role labels. Previous works of Chinese SRL include featurebased approaches and neural network based approaches. Feature-based approaches often extract a However, both of the above two kinds of approaches identify each candidate argument separately without considering the relationship between arguments. We define two categories of argument relationships here: (1) Compatible arguments: if one candidate argument belongs to a given predicate, then the other is more likely to belong to the same predicate; (2) Incompatible arguments: if one candida"
D16-1212,J08-2004,0,0.877101,"when investing entrepreneurs” 2 Related Work Semantic Role Labeling (SRL) task was first proposed by Gildea and Jurafsky (2002). Previous approaches on Chinese SRL can be classified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese"
D16-1212,D14-1041,0,0.334716,"assified into two categories: (1) feature-based approaches (2) neural network based approaches. Among feature-based approaches, Sun and Jurafsky (2004) did the preliminary work on Chinese SRL without any large semantically annotated corpus and produced promising results. Xue and Palmer (2003) proposed Chinese Proposition Bank (CPB), which leads to more complete and systematic research on Chinese SRL (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009). Sun et al. (2009) extended the work of Chen et al. (2006), performed Chinese SRL with shallow parsing, which took partial parses as inputs. Yang and Zong (2014) proposed multi-predicate SRL, which showed improvements both on English and Chinese Proposition Bank. Neural network based approaches are free of handcrafted features, Collobert and Weston (2008) proposed a convolutional neural network for SRL. Their approach achieved competitive performance on English SRL without requiring task specific feature. Wang et al. (2015) proposed a bidirectional LSTM-RNN for Chinese SRL. However, most of the aforementioned approaches did not take the compatible arguments and incompatible arguments into account. Inspired by Sha et al. (2016), our approach model the"
D16-1260,S13-2002,0,0.0328848,"Missing"
D16-1260,P14-2082,0,0.0158495,"Missing"
D16-1260,P08-1090,0,0.0168089,"Missing"
D16-1260,P07-2044,0,0.0828666,"Missing"
D16-1260,Q14-1022,0,0.0213226,"Missing"
D16-1260,D14-1165,0,0.0225719,"Missing"
D16-1260,D15-1038,0,0.0453683,"Missing"
D16-1260,P15-1009,0,0.0235187,"Missing"
D16-1260,D15-1082,0,0.00895825,"Missing"
D16-1260,N13-1008,0,0.0236909,"Missing"
D16-1260,D14-1167,0,0.0386669,"Missing"
D17-1020,N03-1020,0,0.594216,"Missing"
D17-1020,P11-1052,0,0.21185,"recall of ability to identify the better summary in a pair, and ROUGE-4 recall which has the highest precision of ability to identify the better summary in a pair (Owczarzak et al., 2012). 5.3 (Conroy et al., 2004) was the participant of the official DUC 2004 evaluation with the best evaluation score. It employs a Hidden Markov Model using topic signature feature and requires a linguistic preprocessing component. CLASSY 11 (Conroy et al., 2011) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization problem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a global diversity model encouraging non-overlapping information. ICSISumm (Gillick and Favre, 2009) aims at finding the globally optimal summary by formulating the summarization task in Integer Linear Programming. WFS-NMF (Wang et al., 2010) extends the non-negative matrix factorization algorithm and provides a good framework for weighting different terms and documents. GRASSHOPPER, DivRank and GCD are the three graph-based ranking"
D17-1020,W12-2601,0,0.0598294,"the transition matrix. To get the final multi-document summary, we use the same producingSummary function. 5 Task DUC 2003 Task 2 2 http://www-nlpir.nist.gov/projects/duc/intro.html 216 https://tartarus.org/martin/PorterStemmer/ 2012; Hong et al., 2014)3 . We compute ROUGE-2 recall with stemming and stopwords not removed, which provides the best agreement with manual evaluations. We also compute ROUGE-1 recall which has the highest recall of ability to identify the better summary in a pair, and ROUGE-4 recall which has the highest precision of ability to identify the better summary in a pair (Owczarzak et al., 2012). 5.3 (Conroy et al., 2004) was the participant of the official DUC 2004 evaluation with the best evaluation score. It employs a Hidden Markov Model using topic signature feature and requires a linguistic preprocessing component. CLASSY 11 (Conroy et al., 2011) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization problem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a g"
D17-1020,W09-1802,0,0.0356409,"evaluation with the best evaluation score. It employs a Hidden Markov Model using topic signature feature and requires a linguistic preprocessing component. CLASSY 11 (Conroy et al., 2011) is the successor of CLASSY 04 and selects the non-redundant sentences using the non-negative matrix factorization algorithm. In the Submodular system (Lin and Bilmes, 2011), multi-document summarization is formulated as a submodular set function maximization problem. DPP (Lin and Bilmes, 2011) combines a sentence saliency model with a global diversity model encouraging non-overlapping information. ICSISumm (Gillick and Favre, 2009) aims at finding the globally optimal summary by formulating the summarization task in Integer Linear Programming. WFS-NMF (Wang et al., 2010) extends the non-negative matrix factorization algorithm and provides a good framework for weighting different terms and documents. GRASSHOPPER, DivRank and GCD are the three graph-based ranking models mentioned in Section 2. APRW and AAPRW are our methods. APRW is the method of affinitypreserving random walk described in Section 4.2 and AAPRW is the method of adjustable affinitypreserving random walk described in Section 4.3. Experimental Results In the"
D17-1020,hong-etal-2014-repository,0,0.423113,"x= 215 µ(D−1 W) x + (1 − µ)y T kµ(D−1 W) x + (1 − µ)yk1 (10) ? ? (?) rameter tuning of our method. We preprocess the document data sets by removing stopwords from each sentence and stemming the remaining words using the Porter’s stemmer2 . Also, the sentences containing the said clause (if a said, says, told, tells word and quotation marks appear simultaneously) are filtered out. For evaluation, four reference summaries generated by human judges for each document cluster are provided by DUC as the ground truth. A brief summary over the evaluation datasets is shown in Table 5.1. According to (Hong et al., 2014), we adjust the length limit of summary in DUC 2004 from 665 bytes to 100 words as it provides the same setting for system evaluations. ? ? (? + ?) Figure 4.2: Sentence augmented graphs for summarization in two successive iterations. GA (K): augmented graph in the iteration K. Virtual summary V = {s1 , s3 , s5 }, which is constructed from x in the iteration (K−1) by producingSummary. D = diag([C(s1 ), Cmax , C(s3 ), Cmax , C(s5 )]). GA (K + 1): augmented graph in the iteration (K+1). V = {s1 , s4 , s5 }, which is constructed from x in the iteration K by producingSummary. D = diag([C(s1 ), Cmax"
D17-1020,N07-1013,0,0.0978706,"Missing"
D17-1189,P11-1055,0,0.950309,"ver, the automatic labeling inevitably accompanies with wrong labels because the relations of entity pairs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to combat the noise. The method divides the training set into multiple bags of entity pairs (shown in Fig 1) and labels the bags with the relations of entity pairs in the KB. Each bag consists of sentences mentioning both head and tail entities. Much effort has been made in reducing the influence of noisy sentences within the bag, including methods based on at-least-one assumption (Hoffmann et al., 2011; Ritter et al., 2013; Zeng et al., 2015) and attention mechanisms over instances (Lin et al., 2016; Ji et al., 2017). However, the sentence level denoise methods can’t fully address the wrong labeling problem largely because they use a hard-label method in which the labels of entity pairs are immutable dur1790 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1790–1795 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing training, no matter whether they are correct or not. As shown in Fig 1, due to the abse"
D17-1189,C14-1220,0,0.187956,", hhn , tn i}. Each bag hhi , ti i contains sentences {x1 , x2 , · · · , xc } which mention both head entity hi and tail entity ti . The representation si of bag hhi , ti i is a weighted combination of related sentence vectors {x1 , x2 , · · · , xc } which are encoded by CNN. Finally, we use soft-label score function to correct wrong labels of bags of entity pairs while computing probabilities for each relation type. 2.1 Sentence Encoder We get the representation of certain sentence xi = {w1 , w2 , · · · , wm } by concatenating word embeddings {w1 , w2 , · · · , wm } and position embeddings (Zeng et al., 2014) {p1 , p2 , · · · , pm }, where wi ∈ Rd , wi ∈ Rdw , pi ∈ Rdp (d = dw + dp ). Convolution layer utilizes a sliding window of size l. We define qi ∈ Rl×d as the concatenation of words within the i-th window. qi = wi−l+1:i (1 ≤ i ≤ m + l − 1) (1) The convolution matrix is denoted by Wc ∈ Rdc ×(l×d) , where dc is the sentence embedding size. The i-th filter of the convolutional layer is computed as: fi = [Wc q + b]i (2) Afterwards, Piecewise max-pooling (Zeng et al., 2015) is used to divide convolutional filter fi into three parts fi1 , fi2 , fi3 by head and tail entities. For example, the sente"
D17-1189,P16-1200,0,0.505909,"rs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to combat the noise. The method divides the training set into multiple bags of entity pairs (shown in Fig 1) and labels the bags with the relations of entity pairs in the KB. Each bag consists of sentences mentioning both head and tail entities. Much effort has been made in reducing the influence of noisy sentences within the bag, including methods based on at-least-one assumption (Hoffmann et al., 2011; Ritter et al., 2013; Zeng et al., 2015) and attention mechanisms over instances (Lin et al., 2016; Ji et al., 2017). However, the sentence level denoise methods can’t fully address the wrong labeling problem largely because they use a hard-label method in which the labels of entity pairs are immutable dur1790 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1790–1795 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing training, no matter whether they are correct or not. As shown in Fig 1, due to the absence of (Jan Eliasson1 , Sweden) from Nationality relation in the KB, the entity pair is mislabeled"
D17-1189,P09-1113,0,0.946423,"(:) 78(:&/ !@=( ""#(:) P+&:=&/ >? 7,& 89&#41BC:;""#,"" =(9=C(:C2&Q=( &$&G(& ? E? D &B.9$C.89C=(K.&.1$ 7,& 89&#41BC<;""#,"" 9R-J 9669= .89#C&:=C1.89$ 9.8:(KC&2(&:2 B$1#C.89 K1Q:.$,? 0$Q9 T12(.(A9 Figure 1: An example of soft-label correction on Nationality relation. We intend to use syntactic/ semantic information of correctly labeled entity pairs (blue) to correct the false positive and false negative instances (orange) during training. Introduction Relation Extraction (RE) aims to obtain relational facts from plain text. Traditional supervised RE systems suffer from lack of manually labeled data. Mintz et al. (2009) proposes distant supervision, which exploits relational facts in knowledge bases (KBs). Distant supervision automatically generates training examples by aligning entity mentions in plain text with those in KB and labeling entity pairs with their relations in KB. If there’s no relation link between certain entity pair in KB, it will be labeled as negative instance (NA). However, the automatic labeling inevitably accompanies with wrong labels because the relations of entity pairs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to comba"
D17-1189,Q13-1030,0,0.009397,"ling inevitably accompanies with wrong labels because the relations of entity pairs might be missing from KBs or mislabeled. Multi-instances learning (MIL) is proposed by Riedel et al. (2010) to combat the noise. The method divides the training set into multiple bags of entity pairs (shown in Fig 1) and labels the bags with the relations of entity pairs in the KB. Each bag consists of sentences mentioning both head and tail entities. Much effort has been made in reducing the influence of noisy sentences within the bag, including methods based on at-least-one assumption (Hoffmann et al., 2011; Ritter et al., 2013; Zeng et al., 2015) and attention mechanisms over instances (Lin et al., 2016; Ji et al., 2017). However, the sentence level denoise methods can’t fully address the wrong labeling problem largely because they use a hard-label method in which the labels of entity pairs are immutable dur1790 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1790–1795 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ing training, no matter whether they are correct or not. As shown in Fig 1, due to the absence of (Jan Eliasson1"
D17-1189,D12-1042,0,0.872878,"of the DS label. The score of the t-th relation type ot is calculated based on the trained relation matrice M and bias b: exp (Mst + b) ot = P (6) k exp (Msk + b) We use entity-pair level cross-entropy loss function using soft labels as gold labels while training: J(θ) = n X log p(ri |si ; θ) (7) i=1 In the testing stage, we still use the DS label li of certain entity pair hhi , ti i as the gold label: G(θ) = n X i=1 log p(li |si ; θ) (8) Figure 2: Precision/Recall curves of our model and previous state-of-the-art systems. Mintz (Mintz et al., 2009), MultiR (Hoffmann et al., 2011) and MIMLRE (Surdeanu et al., 2012) are feature-based models. ONE (Zeng et al., 2015) and ATT (Lin et al., 2016) are neural network models based on at-least-one assumption and selective attention, respectively. 3 Experiments In this section, we first introduce the dataset and evaluation metrics in our experiments. Then, we demonstrate the parameter settings in our experiments. Besides, we compare the performance of our method with state-of-the-art feature-based and neural network baselines. Case study shows our soft-label corrections are of high accuracy. 3.1 Dataset and Evaluation Metrics We evaluate our model on the benchmark"
D17-1189,P13-2117,0,0.0383401,"Missing"
D17-1189,D15-1203,0,\N,Missing
D18-1170,C14-1151,0,0.462113,"s “games/sports” in the gloss g1 can help to highlight the important words “football” in the context and ignore the words “know each other” which are useless for distinguishing the sense of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorpora"
D18-1170,P15-2003,0,0.0161208,"Language Modeling (Ahn et al., 2016), and LSTMs (Xu et al., 2016; Yang and Mitchell, 2017) show that integrating knowledge and labeled data into a unified system can achieve better performance than other methods which only learn from large scale labeled data. Therefore, it’s a promising and 1 Play in the sentence means behave in a certain way. challenging study to integrate labeled data and lexical knowledge into a unified system. A few recent studies of WSD have exploited several ways to incorporate lexical resources into supervised systems. In the field of traditional feature-based methods (Chen et al., 2015; Rothe and Sch¨utze, 2015), they usually utilize knowledge (to train word sense embeddings) as features of the classifier like the support vector machine (SVM). In the field of neural-based methods, Raganato et al. (2017a) regard lexical resource LEX which is extracted from the WordNet as an auxiliary classification task, and propose a multi-task learning framework for WSD and LEX. Luo et al. (2018) integrate the context and glosses of the target word into a unified framework via a memory network. It encodes the context and glosses of the target word separately, and then models the semantic r"
D18-1170,S01-1001,0,0.642799,"Raganato et al. (2017b) map all the sense annotations in the training and test datasets to WordNet 3.0 via a semi-automatic method. Therefore, We choose WordNet 3.0 as the sense inventory for extracting the gloss. Experiments and Evaluation 4.1 Data SE2 SE3 SE7 SE13 SE15 SemCor Datasets Validation and Evaluation Datasets: We evaluate our model on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al. (2017b) which include five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions: • Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It cons"
D18-1170,P17-1021,0,0.027302,"dNet. All studies listed above show that integrating lexical resources especially gloss into supervised systems of WSD can significantly improve the performance. Therefore, we follow this direction and seek a new way of better integrating gloss knowledge. Instead of building representations for context and gloss separately, we use the inner connection between the gloss and the context to promote the representation of each other. The interaction process can be modeled by a co-attention mechanism which has made great progress in the question answering task (Xiong et al., 2016; Seo et al., 2016; Hao et al., 2017; Lu et al., 2016). We are enlightened by this iterative procedure and introduce it into WSD. We then make some adaptations to the output of the original co-attention model to get the score of each word sense. 3 The Co-Attention Model for WSD In this section, we first give an overview of the CAN: co-attention neural network for WSD (Figure 1). And then, we extend it into a hierarchical architecture HCAN (Figure 2). 3.1 Overview The overall architecture of the proposed nonhierarchical co-attention model is shown in Figure 1. It consists of three parts: 1403 3. The output layer merges the output"
D18-1170,P16-1085,0,0.452294,"of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and la"
D18-1170,W16-5307,0,0.22808,"Missing"
D18-1170,P18-1230,1,0.685697,"(Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and later calculates their similarity in a memory network. However, we find that the learning of the representations of the context and gloss can contribute to each other. We use an example to illustrate our ideas. Table 1 shows that the red words are more important than the blue words when distinguishing the sense of the target word. In other words, we should pay more attention"
D18-1170,S15-2049,0,0.324327,"and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli, 2015, SE15): It’s the latest WSD dataset, which consists of 1022 sense annotations from three heterogeneous domains. Noun 1066 900 159 1644 531 87002 Verb 517 588 296 0 251 88334 Adj 445 350 0 0 160 31753 Adv 254 12 0 0 80 18947 Table 2: Statistics of the different parts of speech annotations in English all-words WSD train and test datasets. . 4.2 Settings We use the validation set (SE7) to find the optimal hyper parameters of our models: the word embedding size dw , the hidden state size ds of LSTM, the optimizer, etc. However, since there are no adverbs and adjectives in SE7, we randomly sample"
D18-1170,Q14-1019,0,0.708724,"shows that the words “games/sports” in the gloss g1 can help to highlight the important words “football” in the context and ignore the words “know each other” which are useless for distinguishing the sense of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first"
D18-1170,S13-2040,0,0.303724,"Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli, 2015, SE15): It’s the latest WSD dataset, which consists of 1022 sense annotations from three heterogeneous domains. Noun 1066 900 159 1644 531 87002 Verb 517 588 296 0 251 88334 Adj 445 350 0 0 160 31753 Adv 254 12 0 0 80 18947 Table 2: Statistics of the different parts of speech annotations in English all-words WSD train and test datasets. . 4.2 Settings We use the validation set (SE7) to find the optimal hyper parameters of our models:"
D18-1170,S07-1016,0,0.165733,"on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al. (2017b) which include five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions: • Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli, 2015, SE15): It’s the latest WSD dataset, which consists of 1022 sense annotations from three heterogeneous domains. Noun 1066 900 159 1644 531 87002 Verb 517 588 296 0 251 88334 Adj 445 350 0 0 160 3"
D18-1170,D17-1120,0,0.485246,"“games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and later calculates their similarity in a memory network. However, we find that the lea"
D18-1170,E17-1010,0,0.641246,"“games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as distributed vectors and later calculates their similarity in a memory network. However, we find that the lea"
D18-1170,P15-1173,0,0.0774322,"Missing"
D18-1170,W04-0811,0,0.29984,"e, We choose WordNet 3.0 as the sense inventory for extracting the gloss. Experiments and Evaluation 4.1 Data SE2 SE3 SE7 SE13 SE15 SemCor Datasets Validation and Evaluation Datasets: We evaluate our model on several English all-words WSD datasets. For a fair comparison, we use the benchmark datasets proposed by Raganato et al. (2017b) which include five standard all-words fine-grained WSD datasets from the Senseval and SemEval competitions: • Senseval-2 (Edmonds and Cotton, 2001, SE2): It consists of 2282 sense annotations, including nouns, verbs, adverbs and adjectives. • Senseval-3 task 1 (Snyder and Palmer, 2004, SE3): It consists of 1850 sense annotations from three different domains (editorial, news story and fiction), including nouns, verbs, adverbs and adjectives. • SemEval-07 task 17 (Pradhan et al., 2007, SE7): It consists of 455 sense annotations of nouns and verbs, which is the smallest among the five datasets. Like Luo et al. (2018) and Raganato et al. (2017a), we choose SE7 as the validation set. • SemEval-13 task 12 (Navigli et al., 2013, SE13): It consists of 1644 sense annotations from thirteen documents of various domains. SE13 contains nouns only. • SemEval-15 task 13 (Moro and Navigli"
D18-1170,P17-1132,0,0.0278757,"ased methods have shown the effectiveness of textual knowledge such as gloss (Lesk, 1986; Basile et al., 2014) and the structural knowledge (Moro et al., 2014; Agirre et al., 2014) of the lexical databases. However, the prime shortcoming of knowledge-based methods is that they have worse performance than supervised methods, but they have wider coverage for the polysemous words, thanks to the use of large-scale knowledge resources (Navigli, 2009). There are many other tasks such as Chinese Word Segmentation (Zhang et al., 2018), Language Modeling (Ahn et al., 2016), and LSTMs (Xu et al., 2016; Yang and Mitchell, 2017) show that integrating knowledge and labeled data into a unified system can achieve better performance than other methods which only learn from large scale labeled data. Therefore, it’s a promising and 1 Play in the sentence means behave in a certain way. challenging study to integrate labeled data and lexical knowledge into a unified system. A few recent studies of WSD have exploited several ways to incorporate lexical resources into supervised systems. In the field of traditional feature-based methods (Chen et al., 2015; Rothe and Sch¨utze, 2015), they usually utilize knowledge (to train wor"
D18-1170,P10-4014,0,0.785886,"guishing the sense of word “play”. Meanwhile, the context can potentially help to stress on the words “games/sports” of the gloss g1 which is actually the correct sense for the target word. Introduction Word Sense Disambiguation (WSD) is a crucial task and long-standing problem in Natural Language Processing (NLP). Previous researches mainly exploit two kinds of resources. Knowledge-based methods (Lesk, 1986; Moro et al., 2014; Basile et al., 2014) exploit the lexical knowledge like gloss to infer the correct senses of ambiguous words in the context. However, supervised feature-based methods (Zhi and Ng, 2010; Iacobacci et al., 2016) and neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) usually use labeled data to train one or more classifiers. Although both lexical knowledge (especially gloss) and labeled data are of great help for WSD, previous supervised methods rarely take the integration of knowledge into consideration. To the best of our knowledge, Luo et al. (2018) are the first to directly incorporate the gloss knowledge from WordNet into a unified neural network for WSD. This model separately builds the context representation and the gloss representation as d"
D18-1271,C16-1171,0,0.0159595,"nu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates,"
D18-1271,D14-1092,1,0.890876,"Missing"
D18-1271,D12-1025,1,0.84361,"s languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsuperv"
D18-1271,D14-1061,1,0.825032,"ast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that our approach is unsupervised, effective, in"
D18-1271,N07-2008,0,0.0207658,"0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and"
D18-1271,P98-1069,0,0.239043,"slation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel co"
D18-1271,D16-1075,1,0.854091,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,C16-1309,1,0.941152,"nowledge (D2K), we adopt a new Data-to-Network-toKnowledge (D2N2K) paradigm, based on the following observations: (i) most information units are not independent, instead they are interconnected or interacting, forming massive networks; (ii) if information networks can be constructed across multiple languages, they may bring tremendous power to make knowledge mining algorithms more scalable and effective because we can employ the graph structures to acquire and propagate knowledge. Based on the motivations, we employ a promising text stream representation – Burst Information Networks (BINets) (Ge et al., 2016a), which can be easily constructed without rich language resources, as media to display the most important information units and illustrate their connections in the text streams. With the BINet representation, we propose a simple yet effective network decipherment algorithm for aligning cross-lingual text streams, which can take advantage of the coburst characteristic of cross-lingual text streams and easily incorporate prior knowledge and rich clues for fast and accurate network decipherment. 2496 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2"
D18-1271,N13-1056,0,0.0349868,"Missing"
D18-1271,W09-3107,1,0.81454,"ed alignment, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of doc"
D18-1271,D15-1015,0,0.0135832,"10; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods,"
D18-1271,P06-1103,0,0.0425456,"i and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast"
D18-1271,W02-0902,0,0.0611221,"tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are"
D18-1271,W11-2125,0,0.0196081,"guage knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Ro"
D18-1271,N16-1132,0,0.031503,"Missing"
D18-1271,P08-1088,0,0.0159272,"d (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of t"
D18-1271,D14-1198,1,0.826094,"nt, exploring a paradigm for language knowledge acquisition. • We propose a network decipherment approach for text stream alignment, which can work in both low and rich resource settings and outperform previous approaches. • We release our data (annotations) and systems to guarantee the reproducibility and help future work improve on this task. 2 Burst Information Network A Burst Information Network (BINet) is a graphbased text stream representation and has proven effective for multiple text stream mining tasks (Ge et al., 2016a,b,c). In contrast to many information networks (e.g., (Ji, 2009; Li et al., 2014)), BINets are specially for text streams. They focus on the burst information units which are usually related to important events or trending topics in text streams and illustrate their connections. A BINet is originally defined as G = hV, E, ωi in (Ge et al., 2016a). Each node v ∈ V is a burst element defined as a burst word1 during one of its burst periods hw, Pi where w denotes a word and P denotes one consecutive burst period of w, as Figure 2 shows. Each edge  ∈ E indicates the connection between two burst elements with the weight ω which is defined as the number of documents where these"
D18-1271,W11-2206,1,0.807265,"y for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and"
D18-1271,D11-1006,0,0.0252557,", 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By"
D18-1271,J05-4003,0,0.0814633,"ments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016;"
D18-1271,D14-1162,0,0.0810082,"ge. Given that our approach is unsupervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been suppo"
D18-1271,E09-1091,0,0.0212751,"lignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the"
D18-1271,C10-1124,0,0.0211684,"eans it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining"
D18-1271,P15-2118,0,0.0147857,"s and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narro"
D18-1271,N18-1202,0,0.0218069,"pervised, effective, intuitive, interpretable, and easily implementable, it is promising to use it as a framework for never-ending language knowledge mining from big data, which might benefit NLP applications such as machine translation and cross-lingual information access. For future work, we plan to 1) conduct more experiments and analyses following this preliminary study to verify our approach’s effectiveness for more languages and domains (e.g., social stream VS news stream); 2) attempt to use word embedding (e.g., word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014) and ELMo (Peters et al., 2018)) for local context encoding and use it as a clue for decipherment; 3) apply our approach to real-time coordinated text streams for never-ending knowledge mining and use the mined knowledge to improve the downstream applications. Acknowledgments We thank the anonymous reviewers for their valuable comments. We also want to thank Xiaoman Pan, Dr. Taylor Cassidy, Dr. Clare R. Voss, Prof. Jiawei Han, Prof. Sujian Li and Prof. Yu Hong for their helpful comments and discussions. This work is supported by NSFC project 61772040 and 61751201. Heng Ji’s work has been supported by the U.S. DARPA AIDA Pro"
D18-1271,P99-1067,0,0.288414,"ing and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. Howev"
D18-1271,N04-1033,0,0.054922,"37 documents. We removed stopwords, conducted lemmatization and name tagging for the English stream, and did word segmentation and name tagging for the Chinese stream using the Stanford CoreNLP toolkit (Manning et al., 2014). 4 Due to the upper bound of Conf (Gc , Ge ), the algorithm must terminate after several iterations. We detected bursts and constructed the BINets5 for the Chinese and English stream based on (Ge et al., 2016a). The constructed Chinese BINet has 7,360 nodes and 33,892 edges while the English one has 8,852 nodes and 85,125 edges. Our seed bi-lingual lexicon is released by (Zens and Ney, 2004), containing 81,990 Chinese word entries, each of which has an English translation. Among the 7,360 nodes in the Chinese BINet, 2,281 nodes need to be deciphered since their words are not in the bi-lingual lexicon. 4.1.2 Evaluation Setting We evaluate our approach in an end-to-end fashion. For a node c in the Chinese BINet, we choose the node e∗ which has the highest score as c’s counterpart in the English BINet: e∗ = arg max Score(c, e) e∈Cand(c) We rank the aligned node pairs by the score and manually evaluate the quality of the top K pairs. A pair hc,ei is annotated as correct if e is a cor"
D18-1271,P11-1002,0,0.0330427,"n (e.g., co-burst across languages). In contrast to the word-level alignment methods, we attempt to mine burst-level alignment to largely narrow down candidates, and introduce powerful clues for improving accuracy and discovering various language knowledge. In contrast to previous cross-lingual projection work like data transfer (Pado and Lapata, 2009) and model transfer (McDonald et al., 2011), we do not require any parallel data. Moreover, our BINets are cheap to construct, which can be easily extended to other languages. This is also the first attempt to apply the decipherment idea (e.g., (Ravi and Knight, 2011; Dou and Knight, 2012; Dou et al., 2014)) to graph structures instead of sequence data. 6 Conclusions and Future Work This paper proposes an approach to deciphering the Burst Information Network constructed from foreign languages as a novel way to align crosslingual text streams. For the first time we propose to model stream alignment as a network decipherment problem. By leveraging the network structures with stream-level burst features as well as various clues, our approach can accurately align the important information units across languages and derive a variety of knowledge. Given that ou"
D18-1271,P10-1115,0,0.0274153,"rger than that used in our experiment and they are endlessly updated. 2503 10 The alignments with a low score (&lt; 0.05) are discarded. That means it is promising to endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Ki"
D18-1271,W02-2026,0,0.132146,"Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for genera"
D18-1271,P17-1179,0,0.0253021,"Missing"
D18-1271,D17-1207,0,0.0444246,"Missing"
D18-1271,C04-1089,0,0.0807467,"ingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpor"
D18-1271,N10-1063,0,0.0216071,"endlessly derive language knowledge by applying our approach to the huge size of endless cross-lingual text streams, which may benefit NLP applications like machine translation, entity linking and name tagging. 5 Related Work Previous studies on cross-lingual text stream alignment tend to focus on coarse-grained (i.e., topic-level) alignment for finding common patterns (Wang et al., 2007; De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et a"
D18-1271,P06-1010,0,0.229533,"Chinese BINet, its candidate nodes in the English BINet can be derived as: Cand(c) = {e|P(e) ∩ P(c) 6= ∅} where e ∈ Ve , and P(c) and P(e) are the burst periods of c and e respectively. 3.3 Candidate Verification For the candidate list for c (i.e., Cand(c)), we need to verify each node e ∈ Cand(c) and choose the most probable one as c’s counterpart. Formally, we define Score(c, e) as the credibility score of e being the correct counterpart of c and propose the following novel clues for verification. Pronunciation Inspired by previous work on name translation mining (e.g., (Schafer III, 2006; Sproat et al., 2006; Ji, 2009)), for a node e ∈ Cand(c), if its pronunciation is similar to c, then e is likely to be the translation of c. For a Chinese node c and an English node e, we define Sp as its scaled pronunciation score to measure their pronunciation similarity whose range is [0, 1]: 1 Sp ∈ [0, 1] ∝ LD where LD is the normalized (by e’s length) Levenshtein edit distance between c’s pinyin3 string and e’s word string. Translation For a node e ∈ Cand(c), it is possible that e’s word exists or partially exists in the bi-lingual lexicon. We can exploit the translation clue to verify if e is c’s counterpar"
D18-1271,D12-1003,0,0.0225396,"De Smet and Moens, 2009; Wang et al., 2009; Zhang et al., 2010; Hu et al., 2012) and discovering parallel sentences and documents (Munteanu and Marcu, 2005; Enright and Kondrak, 2007; Uszkoreit et al., 2010; Smith et al., 2010; Krstovski and Smith, 2011, 2016) across languages. Studies on fine-grained crosslingual alignment are mainly for bilingual lexicon induction (e.g., (Fung and Yee, 1998; Rapp, 1999; Koehn and Knight, 2002; Schafer and Yarowsky, 2002; Shao and Ng, 2004; Schafer III, 2006; Hassan et al., 2007; Haghighi et al., 2008; Udupa et al., 2009; Klementiev and Callison-Burch, 2010; Tamura et al., 2012; Irvine and CallisonBurch, 2013, 2015b; Kiela et al., 2015; Irvine and Callison-Burch, 2015a; Vulic and Moens, 2015; Cao et al., 2016; Zhang et al., 2017b,a)) and name translation mining (e.g., (Sproat et al., 2006; Klementiev and Roth, 2006; Udupa et al., 2008; Ji, 2009; won You et al., 2010; Kotov et al., 2011; Lin et al., 2011; Sellami et al., 2014)) from nonparallel corpora. However, these approaches are mainly developed for general comparable corpora, not specially for cross-lingual text streams; thus many of them did not use the powerful streamlevel information (e.g., co-burst across la"
D18-1311,D15-1159,0,0.0274406,"Missing"
D18-1311,P16-1231,0,0.0505975,"Missing"
D18-1311,D12-1133,0,0.0607716,"Missing"
D18-1311,D07-1101,0,0.113592,"Missing"
D18-1311,D14-1082,0,0.163357,"Missing"
D18-1311,D16-1257,0,0.069825,"neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and Charniak, 2016) also show promising parsing performance. Different from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsing unlabeled data with two existing parsers and selecting only the sentences for which the two parsers produced the same trees. However, the trees produced this way have noise1 and tend to be short sentences, si"
D18-1311,P16-2006,0,0.0120493,"ased models (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) and transition-based models (Nivre, 2008; Zhang and Nivre, 2011) are the most successful solutions to the challenge. Recently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and Charniak, 2016) also show promising parsing performance. Different from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of"
D18-1311,P81-1022,0,0.780315,"Missing"
D18-1311,P15-1033,0,0.0128331,"a sentence. Graphbased models (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) and transition-based models (Nivre, 2008; Zhang and Nivre, 2011) are the most successful solutions to the challenge. Recently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and Charniak, 2016) also show promising parsing performance. Different from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They gener"
D18-1311,N16-1024,0,0.0356407,"l solutions to the challenge. Recently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and Charniak, 2016) also show promising parsing performance. Different from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsing unlabeled data with two existing parsers and selecting only the sentences for which the two parsers produced the same trees. However, the trees produced this way"
D18-1311,Q16-1023,0,0.151034,"and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) and transition-based models (Nivre, 2008; Zhang and Nivre, 2011) are the most successful solutions to the challenge. Recently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and Charniak, 2016) also show promising parsing performance. Different from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsing unlabeled data with two existi"
D18-1311,P10-1001,0,0.0443392,"Missing"
D18-1311,N15-1012,0,0.0661447,"m of negative log probabilities of the ground truth words on the unlabeled data set. Different from language model, the choice for each decoder step is limited in the bag of words. Moreover, selfattention can be introduced into the word ordering model since we have known the bag of words, which could capture the dependency connections between words. We also pre-train a backward word ordering model to generate sentences in reverse order. The forward and backward models share character-level BiLSTM embeddings, selfattention layer, and Softmax layer. Different from previous word ordering models (Liu et al., 2015; Schmaltz et al., 2016), selfattention mechanism is introduced into our model to capture word connections. Moreover, our more important goal is to implicitly utilize large-scale unlabeled data to help dependency parsing. 3 Neural Graph-based Parsing Model We implement an LSTM-based neural network model as our graph-based dependency parsing baseline, which is similar to (Kiperwasser and Goldberg, 2016; Wang and Chang, 2016). As shown in the Figure 2, it consists of three layers: x1:n in which each xi is a concatenation of its word embedding (ewi ), POS tag embedding (epi ), character-level BiL"
D18-1311,J93-2004,0,0.0613117,"Missing"
D18-1311,de-marneffe-etal-2006-generating,0,0.123153,"Missing"
D18-1311,P05-1012,0,0.103346,"Missing"
D18-1311,E06-1011,0,0.202827,"Missing"
D18-1311,J08-4003,0,0.040524,"Missing"
D18-1311,P15-1031,1,0.877214,"Missing"
D18-1311,P17-1161,0,0.0290498,"and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsing unlabeled data with two existing parsers and selecting only the sentences for which the two parsers produced the same trees. However, the trees produced this way have noise1 and tend to be short sentences, since it is easier for different parsers to get consistent results. Pre-trained neural networks are another methods to take advantage of unlabeled data. Pretrained word embeddings (Mikolov et al., 2013) and language model (J´ozefowicz et al., 2016; Peters et al., 2017, 2018) have been shown useful in modelling NLP tasks since word embeddings could capture word semantic information and language model could capture contextual information at the sentence level. However, connections between words in the sentence cannot be directly captured by word embeddings or language model, which are crucial for dependency parsing given its goal is to establish dependency relations between words. In this paper, we propose to implicitly model word connections by a word ordering model. The purpose of word ordering model is to generate a well-formed sentence given a bag of wor"
D18-1311,D16-1255,0,0.108299,"Missing"
D18-1311,E17-1117,0,0.0758263,"challenge. Recently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and Charniak, 2016) also show promising parsing performance. Different from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsing unlabeled data with two existing parsers and selecting only the sentences for which the two parsers produced the same trees. However, the trees produced this way have noise1 and tend"
D18-1311,N03-1033,0,0.40224,"ter-level BiLSTM layer number = 1, wordlevel LSTM hidden vector size = 1024, word-level LSTM layer number = 2, output character-level LSTM hidden vector size = 512, output characterlevel BiLSTM layer number = 1, output lowdimensional word embedding size = 64, batch size = 32, dropout for the input and each layer = 0.5. 4.3 PTB dataset, we follow the standard splits. Using section 2-21 for training, section 22 as development set and 23 as test set. The treebank is converted to Stanford Basic Dependencies (Marneffe et al., 2006) by version 3.3.03 of the Stanford parser. The Stanford POS Tagger (Toutanova et al., 2003) is used for assigning POS tags. Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation. For the CoNLL 09 English dataset, we follow the standard practice and include all punctuation in the evaluation. We pre-train our word ordering model on the 1 billion word benchmark (Chelba et al., 2014). 4.2 Implementation Details The graph-based dependency parsing model and word ordering model are optimized with Adam with an initial learning rate of 2e−3 . The β1 and β2 used in Adam are 0.9 and 0.999 respectively. The followi"
D18-1311,P16-1218,1,0.881046,"t al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010) and transition-based models (Nivre, 2008; Zhang and Nivre, 2011) are the most successful solutions to the challenge. Recently, neural network methods have been successfully introduced into dependency parsing. Deep feed-forward neural network models (Chen and Manning, 2014; Pei et al., 2015; Weiss et al., 2015) are proposed firstly. It alleviates the heavy burden of feature engineering. LSTM networks (Hochreiter and Schmidhuber, 1997) are then applied to dependency parsing (Dyer et al., 2015; Cross and Huang, 2016; Wang and Chang, 2016; Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016) due to its ability to capture contextual information. Generative neural network models (Dyer et al., 2016; Smith et al., 2017; Choe and Charniak, 2016) also show promising parsing performance. Different from Machine Translation task where massive sets of labeled data could be easily obtained, parsing performance is limited by the relatively small size of available treebank. Vinyals et al. (2015) and Weiss et al. (2015) adopt an approach of tri-training to augment the labeled data. They generate large quantities of parse trees by parsin"
D18-1311,P15-1032,0,0.0305894,"Missing"
D18-1311,P11-2033,0,0.0776837,"Missing"
D19-1336,N19-1172,0,0.538622,"t is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs CLM+JD Pun-GAN vs Hum"
D19-1336,W09-2004,0,0.092266,"Missing"
D19-1336,W16-5307,0,0.055248,"Missing"
D19-1336,D18-1170,1,0.925653,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P18-1230,1,0.925353,"abulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function, and x&lt;t is the preceding t − 1 words. Therefore, the generation probability of the whole sentence x is formulated as Gθ (x|s1 , s2 ) = Y Gθ (xt |x&lt;t ) (2) t To give a warm start to the generator, we pretrain it using the same general training corpus in the original paper. 2.1.2 Discriminator The discriminator is extended from the word sense disambiguation models (K˚ageb¨ack and Salomonsson, 2016; Luo et al., 2018a,b). Assuming the pun word w in sentence x has k word senses, we add a new “generated” class. Then, the discriminator is designed to produce a probability distribution over k + 1 classes, which is computed as Dφ (y|x) = softmax(Uw c + b0 ) (3) where c is the context vector from a bi-directional LSTM when taking x as input, Uw is a wordspecific parameter and y is the target label.  Therefore, Dφ y = i|x, i ∈ {1, ..., k} denotes the probability that it belongs to the real i-th word sense, while Dφ (y = k + 1|x) denotes the probability that it is produced by a pun generator. 2.2 Training We fol"
D19-1336,P15-1070,0,0.0609047,"Missing"
D19-1336,S17-2005,0,0.244865,"s – English Wikipedia to train Pun-GAN. For generator, we first tag each word in the English Wikipedia corpus with one word sense using an unsupervised WSD tool2 . Then we use the 2,595K tagged corpus to pre-train our generator. For discriminator, we use several types of data for training: 1) SemCor (Luo et al., 2018a,b) which is a manually annotated corpus for WSD, consisting of 226K sense annotations3 (first part in Eq.4); 2) Wikipedia corpus as unlabeled corpus (second part in Eq.4); 3) Generated puns (third part in Eq.4). Evaluation Dataset: We use the pun dataset from SemEval 2017 task7 (Miller et al., 2017) for evaluation. The dataset consists of 1274 humanwritten puns where target pun words are annotated with two word senses. During testing, we extract the word sense pair as the input of our model. 3.2 Experimental Setting The generator is the same as Yu et al. (2018). The discriminator is a single-layer bi-directional LSTM with hidden size 128. We randomly initialize word embeddings with the dimension size of 300. The sample size K is set as 32. Batch size is 32 and learning rate is 0.001. The optimization algorithm is SGD. Before adversarial training, we pre-train the generator for 5 epochs a"
D19-1336,P12-1101,0,0.0181233,"M (Mikolov et al., 2010): It is a normal recurrent neural language model which takes the target pun word as input. CLM (Mou et al., 2015): It is a constrained language model which guarantees that a pre-given word will appear in the generated sequence. CLM+JD (Yu et al., 2018): It is a state-of-theart model for pun generation which extends a constrained language model by jointly decoding conditioned on two word senses. 3.4 Evaluation Metrics Automatic evaluation: We use two metrics to automatically evaluate the creativeness of the generated puns in terms of unusualness and diversity. Following Pauls and Klein (2012) and He et al. (2019)4 , the unusualness is measured by subtracting the log-probability of training sentences from the log-probability of generated pun sentences. Following Yu et al. (2018), the diversity is measured by the ratio of distinct unigrams (Dist-1) and bigrams (Dist-2) in generated sentences. Human evaluation: Three annotators score the randomly sampled 100 outputs of different systems from 1 to 5 in terms of three criteria. Ambiguity evaluates how likely the sentence is a pun. Fluency measures whether the sentence is fluent. Overall is a comprehensive metric. 3390 Model Pun-GAN vs"
D19-1336,P13-2041,0,0.0766619,"Missing"
D19-1336,D17-1120,0,0.046399,"Missing"
D19-1336,P13-2044,0,0.365409,"Missing"
D19-1336,P18-1153,0,0.436043,"in ambiguity and diversity. 2 Model The sketch of the proposed Pun-GAN is depicted in Figure 1. It consists of a pun generator Gθ and a word sense discriminator Dφ . The following sections will elaborate on the architecture of Pun-GAN and its training algorithm. 2.1 Model Structure 2.1.1 Generator Given two senses (s1 , s2 ) of a target word w, the generator Gθ aims to output a sentence x which not only contains the target word w but also express the two corresponding meanings. Considering the simplicity of the model and the ease of training, we adopt the neural constrained language model of Yu et al. (2018) as the generator. Due to space constraints, we strongly recommend that readers refer to the original paper for details. Compared with traditional neural language model, the main difference is that the generated words at each timestep should have the maximum sum of two probabilities which are calculated with s1 and s2 as input, respectively. Formally, the generation probability over the entire vocabulary at t-th timestep is calculated as Gθ (xt |x&lt;t ) = f (W h1t + b) + f (W h2t + b) (1) where h1t (h2t ) is the hidden state of t-th step when taking s1 (s2 ) as input, f is the softmax function,"
I05-1088,W03-1704,0,0.0625549,"xtraction. In order to extract multiword terms from the domain corpus, three main strategies have been proposed in the literature. First, linguistic rule-based systems propose to extract relevant terms by making use of parts of speech, lexicons, syntax or other linguistic structure [2, 4]. This methodology is language dependent rather than language independent, and the system requires highly specialized linguistic techniques to identify the possible candidate terms. Second, purely statistical systems extract discriminating multiword terms from the text corpora by means of association measures [5, 6, 7]. As they use plain text corpora and only require the information appearing in texts, such systems are highly flexible and extract relevant units independently from the domain and the language of the input text. Finally, hybrid methodologies define co-occurrences of interest in terms of syntactical patterns and statistical regularities [1, 3, 9]. There is no question that the term extraction work comes into play when the tools are parameterized in such a way as to provide as much relevant material (maximizing recall and precision), and as little “noise” as possible. As seen in the literature,"
I05-1088,J93-1007,0,0.247041,"xtraction. In order to extract multiword terms from the domain corpus, three main strategies have been proposed in the literature. First, linguistic rule-based systems propose to extract relevant terms by making use of parts of speech, lexicons, syntax or other linguistic structure [2, 4]. This methodology is language dependent rather than language independent, and the system requires highly specialized linguistic techniques to identify the possible candidate terms. Second, purely statistical systems extract discriminating multiword terms from the text corpora by means of association measures [5, 6, 7]. As they use plain text corpora and only require the information appearing in texts, such systems are highly flexible and extract relevant units independently from the domain and the language of the input text. Finally, hybrid methodologies define co-occurrences of interest in terms of syntactical patterns and statistical regularities [1, 3, 9]. There is no question that the term extraction work comes into play when the tools are parameterized in such a way as to provide as much relevant material (maximizing recall and precision), and as little “noise” as possible. As seen in the literature,"
I05-1088,W01-0513,0,0.0227352,". is very small, and the occurrence of which is also rare. Therefore, the problems of bigrams, tri-grams and 4-grams are primarily taken into considerations in our work. Now let us consider the correlation between two neighboring words A and B. Assuming that these two words are terminologically relevant units, we can intuitively expect that they occur more often than random chance. From a statistical point of view, this probability can be measured by several statistical methods, such as “cooccurrence frequency”, “Mutual Information”, “Dice coefficient”, “Chi-square test”, “log-likelihood”, etc[1, 6, 15]. Table 1 lists several statistical measures which have been widely used in extracting collocations. In table 1: XY represents any two word item； X stands for all words except X； N is the size of corpus； f X and PX are frequency and probability of X respectively； f XY and PXY are frequency and probability of XY respectively。 And assuming that two words X and Y are independent of each other, the formulas are represented as follows: Table 1. Statistical methods used in multi word extraction Method Formula Frequency(Freq) f XY PXY PX PY 2 f XY f X + fY Mutual Information (MI) log 2 Dice Formula ("
I05-1088,A97-1050,0,0.0622677,"Missing"
I05-1088,W02-1801,1,\N,Missing
I05-1088,W97-0115,0,\N,Missing
I13-1181,I05-3025,0,0.0308352,"trongly depends on the predefined lexicon. Xue (2003) proposed a novel way of segmenting Chinese texts, and views the Chinese word segmentation task as a character tagging task. According to Xue’s approach, a tagging model is learned from manually segmented training texts, and then used to assign each character a tag indicating the position of this character within a word it belongs to. Xue’s approach, which did not require any predefined lexicon and have a high performance, became the most popular appoach to Chinese word segmentation in recent years. In Sighan Bakeoff-2005, two participants (Low, 2005) and (Tseng, 2005) have given the best results in almost all word segmentation tracks. Both of their systems use sequence tagging methods based on Conditional Random Field (CRF). CRF is a statistical sequence modeling framework first introduced into natural language processing in (Lafferty et al., 2001). Peng et al. (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. In this paper, we use a CRF-based segmentation system to do a series of compara"
I13-1181,C04-1081,0,0.543193,"ag indicating the position of this character within a word it belongs to. Xue’s approach, which did not require any predefined lexicon and have a high performance, became the most popular appoach to Chinese word segmentation in recent years. In Sighan Bakeoff-2005, two participants (Low, 2005) and (Tseng, 2005) have given the best results in almost all word segmentation tracks. Both of their systems use sequence tagging methods based on Conditional Random Field (CRF). CRF is a statistical sequence modeling framework first introduced into natural language processing in (Lafferty et al., 2001). Peng et al. (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. In this paper, we use a CRF-based segmentation system to do a series of comparative experiments. As in most other work did on segmentation, we use a 4-tag schema, such that each Chinese character is labeled by a tag in the tag set “B,M,E,S”. In which, “B”, “E” and “M” stands for the character in the beginning, ending or middle of a word, “S” means that the character is a word by itself. We use the fo"
I13-1181,W12-2702,0,0.0962049,"Missing"
I13-1181,I05-3027,0,0.341161,"on the predefined lexicon. Xue (2003) proposed a novel way of segmenting Chinese texts, and views the Chinese word segmentation task as a character tagging task. According to Xue’s approach, a tagging model is learned from manually segmented training texts, and then used to assign each character a tag indicating the position of this character within a word it belongs to. Xue’s approach, which did not require any predefined lexicon and have a high performance, became the most popular appoach to Chinese word segmentation in recent years. In Sighan Bakeoff-2005, two participants (Low, 2005) and (Tseng, 2005) have given the best results in almost all word segmentation tracks. Both of their systems use sequence tagging methods based on Conditional Random Field (CRF). CRF is a statistical sequence modeling framework first introduced into natural language processing in (Lafferty et al., 2001). Peng et al. (2004) first used this framework for Chinese word segmentation by treating it as a binary decision task, such that each character is labeled either as the beginning of a word or the continuation of one. In this paper, we use a CRF-based segmentation system to do a series of comparative experiments."
I13-1181,O03-4002,0,0.658154,"al architecture used for segmentation is described in Section 4. Section 5 gives the experimental result. The last section concludes this paper. 2 Chinese Word Segmentation Unlike English and other western languages, Chinese do not delimit words by white-space. Therefore word segmentation is a very basic and important pre-process for Chinese language processing. Traditional word segmentation approaches are lexicon-driven (Liang, 1987). Lexicon-driven methods assume that predefined Chinese word lexicon is available, hence the segmentation performance strongly depends on the predefined lexicon. Xue (2003) proposed a novel way of segmenting Chinese texts, and views the Chinese word segmentation task as a character tagging task. According to Xue’s approach, a tagging model is learned from manually segmented training texts, and then used to assign each character a tag indicating the position of this character within a word it belongs to. Xue’s approach, which did not require any predefined lexicon and have a high performance, became the most popular appoach to Chinese word segmentation in recent years. In Sighan Bakeoff-2005, two participants (Low, 2005) and (Tseng, 2005) have given the best resu"
L18-1079,reschke-etal-2014-event,0,0.0969942,"n 2011 Tohoku earthquake and 869 Sanriku earthquake is expressed by the following text in 2011 Tohoku earthquake: infobox generation problem can be divided into three subtasks: event classification, event schema extraction and slot filling. As mentioned before, EventWiki can provide rich information for event classification and event schema extraction. Moreover, it is extremely useful for training a slot filling model for event extraction. Slot and value pairs in the infobox (intra-event information) in EventWiki can be used as weak (distant) supervision for training a slot filling model, as (Reschke et al., 2014) did. For example, for the slot value pair “magnitude: 9.0” in 2011 Tohoku earthquake, we first find out the sentences which “9.0” appears in. The context information of “9.0” can be used as features and “magnitude” is used as the label of “9.0” for training a slot filling model. 2.3.2. Event-event relation extraction and inference As slot filling for event extraction, we can also use interevent information in EventWiki to train an event-event relation extraction model using distant supervision strategy. For an event relation triple, we can find out the sentences that mention both events in th"
N16-1049,P98-1013,0,0.153293,"es has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atom"
N16-1049,D13-1178,0,0.129215,"t al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion This paper presented a joint entity-driven model to induct e"
N16-1049,P04-1056,0,0.16463,"rates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jen"
N16-1049,P08-1090,0,0.311784,"traction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion"
N16-1049,P09-1068,0,0.11024,"rlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and get good results. 6 Conclusion This paper presented a joint"
N16-1049,P11-1098,0,0.513984,"vent. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as follows: • To better model the inner connect"
N16-1049,D13-1185,0,0.722137,"of bombing event in MUC-4, it has a bombing template and four main slots Introduction Event schema is a high-level representation of a bunch of similar events. It is very useful for the traditional information extraction (IE)(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events,"
N16-1049,P11-1054,0,0.0179381,"ates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chamber"
N16-1049,N13-1104,0,0.728221,"in MUC-4, it has a bombing template and four main slots Introduction Event schema is a high-level representation of a bunch of similar events. It is very useful for the traditional information extraction (IE)(Sagayam et al., 2012) task. An example of event schema is shown in Table 1. Given the bombing schema, we only need to find proper words to fill the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learn"
N16-1049,P03-1028,0,0.0639924,"ts. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can"
N16-1049,J93-3001,0,0.333736,"t templates in the two methods while only 108 entities has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information"
N16-1049,D11-1142,0,0.015167,"and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary"
N16-1049,P06-2027,0,0.810867,"the slots when extracting a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as"
N16-1049,C92-2082,0,0.0452342,"chmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of s"
N16-1049,P10-1029,0,0.0133174,"ent-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captur"
N16-1049,P08-1030,0,0.0579146,"strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal"
N16-1049,W10-0905,0,0.0276645,"animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. However, their structures are limited to frequent topics in a large corpus. Chambers and Jurafsky (2011) uses their idea, and their goal is to characterize a specific domain with limited data using a three-stage clustering algorithm. Also, there are some state-of-the-art works using probabilistic graphic model (Chambers, 2013; Cheung, 2013; Nguyen et al., 2015). They use the Gibbs sampling and"
N16-1049,P15-1019,0,0.389029,"Missing"
N16-1049,D07-1075,0,0.0822568,"and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates w"
N16-1049,D09-1016,0,0.0220445,"nstraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with un"
N16-1049,M92-1008,0,0.221381,"assigned different templates in the two methods while only 108 entities has different slots. This fact illustrates that the sentence constraint can affect the assignment of templates much more than the slots. Therefore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other"
N16-1049,W98-1106,0,0.159823,"t filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432"
N16-1049,P06-2094,0,0.3539,"ng a bombing event. There are two main approaches for AESI task. Both of them use the idea of clustering the potential event arguments to find the event schema. One of them is probabilistic graphical model (Chambers, 2013; Cheung, 2013). By incorporating templates and slots as latent topics, probabilistic graphical models learns those templates and slots that best explains the text. However, the graphical models considers the entities independently and do not take the interrelationship between entities into account. Another method relies on ad-hoc clustering algorithms (Filatova et al., 2006; Sekine, 2006; Chambers and Jurafsky, 2011). (Chambers and Jurafsky, 2011) is a pipelined approach. In the first step, it uses pointwise mutual information(PMI) between any two clauses in the same document to learn events, and then learns syntactic patterns as fillers. However, the pipelined approach suffers from the error propagation problem, which means the errors in the template clustering can lead to more errors in the slot clustering. This paper proposes an entity-driven model which jointly learns templates and slots for event schema induction. The main contribution of this paper are as follows: • To"
N16-1049,N06-1039,0,0.06114,"Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011) also extract binary relations using generative model. Kasch and Oates (2010), Chambers and Jurafsky (2008), Chambers and Jurafsky (2009), Balasubramanian et al. (2013) captures template-like knowledge from unlabeled text by large-scale learning of scripts and narrative schemas. Ho"
N16-1049,P03-1029,0,0.0731297,"e full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekin"
N16-1049,H91-1059,0,0.102863,"slot constraint. Generally, we have the following optimization objective: ( ) tr XTT JJ T XT ) ε3 (XT , XS ) = ( T T tr XS JJ XS (8) The whole joint model is shown in Eq 9. The detailed derivation1 is shown in the supplement file. Bombing XT , XS = argmax ε1 (XT ) + ε2 (XS ) + ε3 (XT , XS ) Perpetrator XT ,XS s.t. XT ∈ {0, 1}|E|×|T |XT 1|T |= 1|E| XS ∈ {0, 1}|E|×|S |XS 1|S |= 1|E| Victim The police chief El salvador Students The guerrillas The Peruvian embassy The drag mafia The diplomat Drug traffickers soldiers The Atlacatl battalion (9) 4 Experiment 4.1 Dataset In this paper, we use MUC-4(Sundheim, 1991) as our dataset, which is the same as previous works (Chambers and Jurafsky, 2011; Chambers, 2013). MUC-4 corpus contains 1300 documents in the training set, 200 in development set (TS1, TS2) and 200 in testing set (TS3, TS4) about Latin American news of terrorism events. We ran several times on the 1500 documents (training/dev set) and choose the best |T |and |S |as |T |= 6, |S |= 4. Then we report the performance of test set. For each document, it provides a series of hand-constructed event schemas, which are called gold schemas. With these gold schemas we can evaluate our results. The MUC-4"
N16-1049,C00-2136,0,0.180339,"ore, the sentence constraint leads largely improvement to the strict mapping performance and very little increase to the slot-only performance. 5 Related Works The traditional information extraction task is to fill the event schema slots. Many slot filling algorithms requires the full information of the event schemas and the labeled corpus. Among them, there are rule-based method (Rau et al., 1992; Chinchor et al., 1993), supervised learning method (Baker et al., 1998; Chieu et al., 2003; Bunescu and Mooney, 2004; Patwardhan and Riloff, 2009; Maslennikov and Chua, 2007), bootstrapping method (Yangarber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007;"
N16-1049,N07-4013,0,0.0431629,"garber et al., 2000) and cross-document inference method (Ji and Grishman, 2008). Also there are many semisupervised solutions, which begin with unlabeled, but clustered event-specific documents, and extract common word patterns as extractors (Riloff and Schmelzenbach, 1998; Sudo et al., 2003; Riloff et al., 2005; Patwardhan and Riloff, 2007; Filatova et al., 2006; Surdeanu et al., 2006) Other traditional information extraction task learns binary relations and atomic facts. Models can learn relations like “Jenny is married to Bob” with unlabeled data (Banko et al., 2007; Etzioni et al., 2008; Yates et al., 2007; Fader et al., 2011), or ontology induction (dog is an animal) and attribute extraction (dogs have tails) (Carlson et al., 2010a; Carlson et al., 2010b; Huang and Riloff, 2010; Van Durme and Pasca, 2008), or rely on predefined patterns (Hearst, 1992). 432 Shinyama and Sekine (2006) proposed an approach to learn templates with unlabeled corpus. They use unrestricted relation discovery to discover relations in unlabeled corpus as well as extract their fillers. Their constraints are that they need redundant documents and their relations are binary over repeated named entities. (Chen et al., 2011"
P14-1028,D13-1167,0,0.0191083,"Missing"
P14-1028,I05-3019,0,0.0797821,"Missing"
P14-1028,N13-1052,0,0.017118,"Missing"
P14-1028,W02-1001,0,0.0164232,"okup Table layer are then concatenated into a single vector a ∈ RH1 , where H1 = w · d is the size of Layer 1. Then a is fed into the next layer which performs linear transformation followed by an element-wise activation function g such as tanh, which is used in our experiments: h = g(W1 a + b1 ) s(c[1:n] , t[1:n] , θ) = (4) where fθ (ti |c[i−2:i+2] ) indicates the score output for tag ti at the i-th character by the network with parameters θ = (M, A, W1 , b1 , W2 , b2 ). Given the sentence-level score, Zheng et al. (2013) proposed a perceptron-style training algorithm inspired by the work of Collins (2002). Compared with Mansur et al. (2013), their model is a global one where the training and inference is performed at sentence-level. Workable as these methods seem, one of the limitations of them is that the tag-tag interaction and the neural network are modeled seperately. The simple tag-tag transition neglects the impact of context characters and thus limits the ability to capture flexible interactions between tags and context characters. Moreover, the simple nonlinear transformation in equation (2) is also poor to model the complex interactional effects in Chinese word segmentation. (2) (3) w"
P14-1028,N13-1090,0,0.0226237,"k-specific features. The main contributions of our work are as follows: Figure 1: Conventional Neural Network describes the details of our model. Experiment results are reported in Section 4. Section 5 reviews the related work. The conclusions are given in Section 6. 2 2.1 Conventional Neural Network Lookup Table The idea of distributed representation for symbolic data is one of the most important reasons why the neural network works. It was proposed by Hinton (1986) and has been a research hot spot for more than twenty years (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013a). Formally, in the Chinese word segmentation task, we have a character dictionary D of size |D|. Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c ∈ D is represented as a real-valued vector (character embedding) Embed(c) ∈ Rd where d is the dimensionality of the vector space. The character embeddings are then stacked into a embedding matrix M ∈ Rd×|D |. For a character c ∈ D that has an associated index k, the corresponding character embedding Embed(c) ∈ Rd"
P14-1028,W12-2702,0,0.0144134,"y use more complex task-specific features. The main contributions of our work are as follows: Figure 1: Conventional Neural Network describes the details of our model. Experiment results are reported in Section 4. Section 5 reviews the related work. The conclusions are given in Section 6. 2 2.1 Conventional Neural Network Lookup Table The idea of distributed representation for symbolic data is one of the most important reasons why the neural network works. It was proposed by Hinton (1986) and has been a research hot spot for more than twenty years (Bengio et al., 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013a). Formally, in the Chinese word segmentation task, we have a character dictionary D of size |D|. Unless otherwise specified, the character dictionary is extracted from the training set and unknown characters are mapped to a special symbol that is not used elsewhere. Each character c ∈ D is represented as a real-valued vector (character embedding) Embed(c) ∈ Rd where d is the dimensionality of the vector space. The character embeddings are then stacked into a embedding matrix M ∈ Rd×|D |. For a character c ∈ D that has an associated index k, the corresponding character e"
P14-1028,I05-3017,0,0.838412,"ding author 293 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 293–303, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics between tags and context characters by exploiting tag embeddings and tensor-based transformation. Moreover, we propose a tensor factorization approach that effectively improves the model efficiency and prevents from overfitting. We evaluate the performance of Chinese word segmentation on the PKU and MSRA benchmark datasets in the second International Chinese Word Segmentation Bakeoff (Emerson, 2005) which are commonly used for evaluation of Chinese word segmentation. Experiment results show that our model outperforms other neural network models. Although we focus on the question that how far we can go without using feature engineering in this paper, the study of deep learning for NLP tasks is still a new area in which it is currently challenging to surpass the state-of-the-art without additional features. Following Mansur et al. (2013), we wonder how well our model can perform with minimal feature engineering. Therefore, we integrate additional simple character bigram features into our m"
P14-1028,P13-1045,0,0.29026,"model, because, by incorporating tag embedding into the neural network, the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model. Now 3.2 Tensor Neural Network A tensor is a geometric object that describes relations between vectors, scalars, and other tensors. It can be represented as a multi-dimensional array of numerical values. An advantage of the tensor is that it can explicitly model multiple interactions in data. As a result, tensor-based model have been widely used in a variety of tasks (Salakhutdinov et al., 2007; Krizhevsky et al., 2010; Socher et al., 2013b). In Chinese word segmentation, a proper modeling of the tag-tag interaction, tag-character interaction and character-character interaction is very important. In linear models, these kinds of interactions are usually modeled as features. In conventional neural network models, however, the input embeddings only implicitly interact through the non-linear function which can hardly model the complexity of the interactions. Given the advantage of tensors, we apply a tensor-based transformation to the input vector. Formally, we use a 3-way tensor V [1:H2 ] ∈ RH2 ×H1 ×H1 to directly model the inter"
P14-1028,D13-1170,0,0.00720392,"model, because, by incorporating tag embedding into the neural network, the effect of tag-tag interaction and tag-character interaction are covered uniformly in one same model. Now 3.2 Tensor Neural Network A tensor is a geometric object that describes relations between vectors, scalars, and other tensors. It can be represented as a multi-dimensional array of numerical values. An advantage of the tensor is that it can explicitly model multiple interactions in data. As a result, tensor-based model have been widely used in a variety of tasks (Salakhutdinov et al., 2007; Krizhevsky et al., 2010; Socher et al., 2013b). In Chinese word segmentation, a proper modeling of the tag-tag interaction, tag-character interaction and character-character interaction is very important. In linear models, these kinds of interactions are usually modeled as features. In conventional neural network models, however, the input embeddings only implicitly interact through the non-linear function which can hardly model the complexity of the interactions. Given the advantage of tensors, we apply a tensor-based transformation to the input vector. Formally, we use a 3-way tensor V [1:H2 ] ∈ RH2 ×H1 ×H1 to directly model the inter"
P14-1028,D11-1090,0,0.376818,"our model can achieve a competitive result with minimal feature engineering. In the future, we plan to further extend our model and apply it to other structure prediction problems. Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular approach treats word segmentation as a sequence labeling problem which was first proposed in Xue (2003). Most previous systems address this task by using linear statistical models with carefully designed features such as bigram features, punctuation information (Li and Sun, 2009) and statistical information (Sun and Xu, 2011). Recently, researchers have tended to explore new approaches for word segmentation which circumvent the feature engineering by automatically learning features with neural network models (Mansur et al., 2013; Zheng et al., 2013). Our study is consistent with this line of research, however, our model explicitly models the interactions between tags and context characters and accordingly captures more semantic information. Tensor-based transformation was also used in other neural network models for its ability to capture multiple interactions in data. For example, Socher et al. (2013b) exploited"
P14-1028,N09-1007,0,0.817796,"Missing"
P14-1028,J09-4006,0,0.0798237,"ts than previous neural network models and that our model can achieve a competitive result with minimal feature engineering. In the future, we plan to further extend our model and apply it to other structure prediction problems. Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular approach treats word segmentation as a sequence labeling problem which was first proposed in Xue (2003). Most previous systems address this task by using linear statistical models with carefully designed features such as bigram features, punctuation information (Li and Sun, 2009) and statistical information (Sun and Xu, 2011). Recently, researchers have tended to explore new approaches for word segmentation which circumvent the feature engineering by automatically learning features with neural network models (Mansur et al., 2013; Zheng et al., 2013). Our study is consistent with this line of research, however, our model explicitly models the interactions between tags and context characters and accordingly captures more semantic information. Tensor-based transformation was also used in other neural network models for its ability to capture multiple interactions in data"
P14-1028,P12-1027,0,0.515214,"Missing"
P14-1028,I05-3027,0,0.154691,"Missing"
P14-1028,N13-1134,0,0.0183477,"Missing"
P14-1028,I13-1183,0,0.0236794,"training time inCRF NN NN+Tag Embed MMTNN P 87.8 92.4 93.0 93.7 R 85.7 92.2 92.7 93.4 F 86.7 92.3 92.9 93.5 OOV 57.1 60.0 61.0 64.2 一(one) 二(two) 三(three) 四(four) 五(five) 六(six) Table 3: Test results with different configurations. NN stands for the conventional neural network. NN+Tag Embed stands for the neural network with tag embeddings. 。(period) ，(comma) ：(colon) ？(question mark) “(quotation mark) 、(Chinese comma) Table 4: Examples of character embeddings is used to discover higher level representation. In fact, CRF can be regarded as a special neural network without non-linear function (Wang and Manning, 2013). Wang and Manning (2013) conduct an empirical study on the effect of non-linearity and the results suggest that non-linear models are highly effective only when distributed representation is used. To explain why distributed representation captures more information than discrete features, we show in Table 4 the effect of character embeddings which are obtained from the lookup table of MMTNN after training. The first row lists three characters we are interested in. In each column, we list the top 5 characters that are nearest (measured by Euclidean distance) to the corresponding character in th"
P14-1028,O03-4002,0,0.906874,"approach that effectively improves the model efficiency and avoids the risk of overfitting. Experiments on the benchmark datasets show that our model achieve better results than previous neural network models and that our model can achieve a competitive result with minimal feature engineering. In the future, we plan to further extend our model and apply it to other structure prediction problems. Chinese word segmentation has been studied with considerable efforts in the NLP community. The most popular approach treats word segmentation as a sequence labeling problem which was first proposed in Xue (2003). Most previous systems address this task by using linear statistical models with carefully designed features such as bigram features, punctuation information (Li and Sun, 2009) and statistical information (Sun and Xu, 2011). Recently, researchers have tended to explore new approaches for word segmentation which circumvent the feature engineering by automatically learning features with neural network models (Mansur et al., 2013; Zheng et al., 2013). Our study is consistent with this line of research, however, our model explicitly models the interactions between tags and context characters and"
P14-1028,P07-1106,0,0.921433,"Missing"
P14-1028,N06-2049,0,0.34648,"Missing"
P14-1028,D13-1031,0,0.730257,"Missing"
P14-1028,D13-1061,0,0.310466,"ol of Electronics Engineering and Computer Science, Peking University Beijing, P.R.China, 100871 {peiwenzhe,getao,chbb}@pku.edu.cn Abstract by the design of features and the number of features could be so large that the result models are too large for practical use and prone to overfit on training corpus. Recently, neural network models have been increasingly focused on for their ability to minimize the effort in feature engineering. Collobert et al. (2011) developed the SENNA system that approaches or surpasses the state-of-the-art systems on a variety of sequence labeling tasks for English. Zheng et al. (2013) applied the architecture of Collobert et al. (2011) to Chinese word segmentation and POS tagging and proposed a perceptronstyle algorithm to speed up the training process with negligible loss in performance. Workable as previous neural network models seem, a limitation of them to be pointed out is that the tag-tag interaction, tag-character interaction and character-character interaction are not well modeled. In conventional feature-based linear (log-linear) models, these interactions are explicitly modeled as features. Take phrase “打篮 球(play basketball)” as an example, assuming we are labeli"
P14-1028,I13-1181,1,\N,Missing
P15-1031,C96-1058,0,0.199687,"(Graff et al., 2003) using word2vec4 and all other parameters are still initialized randomly. In all experiments, we report unlabeled attachment scores (UAS) and labeled attachment scores (LAS) and punctuation5 is excluded in all evaluation metrics. The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM. As we can see, even with random initialization, 1-order-atomic-rand performs as well as conventional first-order model and both 1-order-phraseSecond-order model Our model can be easily extended to a secondorder model using the second-order decoding algorithm (Eisner, 1996; McDonald and Pereira, 2006). The third row of Table 1 shows the additional features we use in our second-order model. Sibling node and its local context are used as additional atomic features. We also used the infix embedding for the infix between sibling pair (s, m), which we call sm infix. It is calculated in the same way as infix between head-modifier pair (h, m) (i.e., hm infix) in Section 2.2 except that the word pair is now s and m. For cases where no sibling information is available, the corresponding sibling-related embeddings are set to zero vector. 4 4.1 Experiment Results Experime"
P15-1031,P10-1001,0,0.107574,"fective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors ∗ • Instead of using large number of hand-craf"
P15-1031,D14-1081,0,0.317463,"exponential large, which makes it impractical to solve equation (1) directly. Previous work (McDonald et al., 2005; McDonald and Pereira, 2006; Koo and Collins, 2010) assumes that the score of yˆ(x) factors through the scores of subgraphs c of yˆ(x) so that efficient algorithms can be designed for decoding: X Score(x, yˆ(x); θ) = ScoreF (x, c; θ) (2) ables our model to exploit richer context information that previous work did not consider due to the curse of dimension but also captures inherent correlations between phrases. • Unlike other neural network based models (Chen et al., 2014; Le and Zuidema, 2014) where an additional parser is needed for either extracting features (Chen et al., 2014) or generating k-best list for reranking (Le and Zuidema, 2014), both training and decoding in our model are performed based on our neural network architecture in an effective way. c∈ˆ y (x) Figure 1 gives two examples of commonly used factorization strategy proposed by Mcdonald et.al (2005) and Mcdonald and Pereira (2006). The simplest subgraph uses a first-order factorization (McDonald et al., 2005) which decomposes a dependency tree into single dependency arcs (Figure 1(a)). Based on the first-order mode"
P15-1031,E06-1011,0,0.0692074,"nhcube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors ∗ • Ins"
P15-1031,P05-1012,0,0.323984,"activation function tanhcube. Moreover, we propose a simple yet effective way to utilize phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensi"
P15-1031,P14-1028,1,0.547997,"crafted features were used to capture context and structure information in the subgraph which not only limits the model’s ability to generalize well but only slows down the parsing speed. We evaluate our models on the English Penn Treebank. Experiment results show that both our first-order and second-order models outperform the corresponding conventional models. 314 Figure 3: Illustration for phrase embeddings. h, m and x0 to x6 are words in the sentence. Figure 2: Architecture of the Neural Network 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f ∈ D is represented as a realvalued vector (feature embedding) Embed(f ) ∈ Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M ∈ Rd×|D |. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). In our work, we propose a neural network model for scoring subgraph c in the tree: ScoreF (x, c; θ) = N N (x, c) (4) where N N is our scoring function based on neural network (Figure 2). As we will show in the following sections, it"
P15-1031,W12-2702,0,0.0162269,"nald et al., 2005; McDonald and Pereira, 2006), millions of hand-crafted features were used to capture context and structure information in the subgraph which not only limits the model’s ability to generalize well but only slows down the parsing speed. We evaluate our models on the English Penn Treebank. Experiment results show that both our first-order and second-order models outperform the corresponding conventional models. 314 Figure 3: Illustration for phrase embeddings. h, m and x0 to x6 are words in the sentence. Figure 2: Architecture of the Neural Network 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f ∈ D is represented as a realvalued vector (feature embedding) Embed(f ) ∈ Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M ∈ Rd×|D |. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). In our work, we propose a neural network model for scoring subgraph c in the tree: ScoreF (x, c; θ) = N N (x, c) (4) where N N is our scoring function based on neural netw"
P15-1031,D13-1170,0,0.021066,"6), millions of hand-crafted features were used to capture context and structure information in the subgraph which not only limits the model’s ability to generalize well but only slows down the parsing speed. We evaluate our models on the English Penn Treebank. Experiment results show that both our first-order and second-order models outperform the corresponding conventional models. 314 Figure 3: Illustration for phrase embeddings. h, m and x0 to x6 are words in the sentence. Figure 2: Architecture of the Neural Network 2003; Collobert et al., 2011; Schwenk et al., 2012; Mikolov et al., 2013; Socher et al., 2013; Pei et al., 2014). Formally, we have a feature dictionary D of size |D|. Each feature f ∈ D is represented as a realvalued vector (feature embedding) Embed(f ) ∈ Rd where d is the dimensionality of the vector space. All feature embeddings stacking together forms the embedding matrix M ∈ Rd×|D |. The embedding matrix M is initialized randomly and trained by our model (Section 2.6). In our work, we propose a neural network model for scoring subgraph c in the tree: ScoreF (x, c; θ) = N N (x, c) (4) where N N is our scoring function based on neural network (Figure 2). As we will show in the foll"
P15-1031,N03-1033,0,0.276899,"Missing"
P15-1031,W03-3023,0,0.0667107,"xcept that the word pair is now s and m. For cases where no sibling information is available, the corresponding sibling-related embeddings are set to zero vector. 4 4.1 Experiment Results Experiments Experiment Setup 2 http://sourceforge.net/projects/ mstparser 3 Note that Koo and Collins (2010)’s third-order model and our models are not strict comparable since their model is an unlabeled model. 4 https://code.google.com/p/word2vec/ 5 Following previous work, a token is a punctuation if its POS tag is {“ ” : , .} We use the English Penn Treebank (PTB) to evaluate our model implementations and Yamada and Matsumoto (2003) head rules are used to extract dependency trees. We follow the standard splits of PTB3, using section 2-21 for training, section 22 as development set and 23 as test set. The Stanford 318 Feature Type Words (word2vec) Instance in his which Words (Our model) in his which POS-tags NN JJ Neighboors the, of, and, for, from himself, her, he, him, father its, essentially, similar, that, also on, at, behind, among, during her, my, their, its, he where, who, whom, whose, though NNPS, NNS, EX, NNP, POS JJR, JJS, PDT, RBR, RBS Figure 4: Convergence curve for tanh-cube and cube activation function. Tabl"
P15-1031,P11-2033,0,0.116993,"phrase-level information that is expensive to use in conventional graph-based parsers. Experiments on the English Penn Treebank show that parsers based on our model perform better than conventional graph-based parsers. 1 Introduction Dependency parsing is essential for computers to understand natural languages, whose performance may have a direct effect on many NLP application. Due to its importance, dependency parsing, has been studied for tens of years. Among a variety of dependency parsing approaches (McDonald et al., 2005; McDonald and Pereira, 2006; Carreras, 2007; Koo and Collins, 2010; Zhang and Nivre, 2011), graph-based models seem to be one of the most successful solutions to the challenge due to its ability of scoring the parsing decisions on whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, ranging from the smallest edge (first-order) to a controllable bigger subgraph consisting of more than one single edge (second-order and third order), and score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as a high-dimensional feature vectors ∗ • Instead of using large number of hand-crafted features, our model"
P15-1031,C14-1078,0,\N,Missing
P15-1031,D14-1082,0,\N,Missing
P15-1031,D07-1101,0,\N,Missing
P15-1056,W06-0901,0,0.0467242,"relevant event chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is re"
P15-1056,S13-2002,0,0.0329879,"Missing"
P15-1056,P08-1030,1,0.800799,"vent chronicle generation work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most"
P15-1056,R09-1032,1,0.842039,"er from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the per"
P15-1056,D08-1073,0,0.0878802,"Missing"
P15-1056,Q14-1022,0,0.0297756,"Missing"
P15-1056,S13-2012,0,0.0607878,"Missing"
P15-1056,P13-2099,1,0.719355,"n aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time unit is a day. Table 3 shows the performance of our approach in labeling event time. For disaster, sports and war, the accurac"
P15-1056,D12-1092,0,0.0143416,"times called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymou"
P15-1056,N09-2053,1,0.822412,"on work but there are some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focu"
P15-1056,P13-1008,1,0.798212,"2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for their thought-provoki"
P15-1056,C12-1033,0,0.0200147,"c detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We thank the anonymous reviewers for the"
P15-1056,P10-1081,0,0.0232743,"re some related tasks. Event detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeA"
P15-1056,D12-1062,0,0.0241202,"blems. Disaster event chronicles suffer from the indirect description problem since there are many responses (e.g., humanitarian aids) to a disaster. These responses are topically relevant and contain many documents, and where E and E∗ are our chronicle and the manually edited event chronicle respectively. te is e’s 582 time labeled by our method and t∗e is e’s correct time. Note that for multiple entries referring the same event in event chronicles, the earliest entry’s time is used as the event’s time to compute diff. sports 0.800 politics 3.363 disaster 1.042 war 1.610 line for a document (Do et al., 2012), a centroid entity (Ji et al., 2009) or one major event (Hu et al., 2011; Yan et al., 2011; Lin et al., 2012; Li and Li, 2013). In addition, Li and Cardie (2014) generated timelines for users in microblogs. The most related work to ours is Swan and Allan (2000). They used a timeline to show bursty events along the time, which can be seen as an early form of event chronicles. Different from their work, we generate a topically relevant event chronicle based on a reference event chronicle. comprehensive 2.467 Table 3: Difference between an event’s actual time and the time in our chronicles. Time"
P15-1056,D13-1001,1,0.844429,"in sports chronicles but it is not a good entry in comprehensive chronicles. Compared with comprehensive event chronicles, events in other chronicles tend to describe more details. For example, a sports chronicle may regard each match in the World Cup as an event while comprehensive chronicles consider the World Cup as one event, which requires us to adapt event granularity for different chronicles. Also, we evaluate the time of event entries in these five event chronicles because event’s happening time is not always equal to the timestamp of the document creation time (UzZaman et al., 2012; Ge et al., 2013). We collect existing manually edited 2010 chronicles on the web and use their event time as gold standard. We define a metric to evaluate if the event entry’s time in our chronicle is accurate: P diff = e∈E∩E∗ |(te − t∗e )|/|E ∩ E∗ | not show significant improvement. A possible reason is that a comprehensive event chronicle does not care the topical relevance of a event. In other words, its ranking problem is simpler so that the learning-to-rank does not improve the basic ranking criterion much. Moreover, we analyze the incorrect entries in the chronicles generated by our approaches. In gener"
P15-1056,P14-5010,0,0.00567471,"ifically, we collected disaster, sports, war, politics and comprehensive chronicles during 2009 from mapreport7 , infoplease and Wikipedia8 . To generate chronicles during 2010, we use 2009-2010 APW and Xinhua news in English Gigaword (Graff et al., 2003) and remove documents whose titles and first paragraphs do not include any burst words. We detect burst words using Kleinberg algorithm (Kleinberg, 2003), which is a 2-state finite automaton model and widely used to detect bursts. In total, there are 140,557 documents in the corpus. Preprocessing: We remove stopwords and use Stanford CoreNLP (Manning et al., 2014) to do lemmatization. Parameter setting: For TaHBM, we empirically set α = 0.05, βz = 0.005, βe = 0.0001, γs = 0.05, γx = 0.5, ε = 0.01, the number of topics K = 50, and the number of events E = 5000. We run Gibbs sampler for 2000 iterations with burn-in period of 500 for inference. For event ranking, we set regularization parameter of SVMRank c = 0.1. Chronicle display: We use a heuristic way to generate the description of each event. Since the first paragraph of a news article is usually a good summary of the article and the earliest document in a cluster usually explicitly describes the eve"
P15-1056,P11-1113,0,0.0152544,"ent detection, sometimes called topic detection (Allan, 2002), is an important part of our approach. Yang et al. (1998) used clustering techniques for event detection on news. He et al. (2007) and Zhao et al. (2012) designed burst feature representations for detecting bursty events. Compared with our TaHBM, these methods lack the ability of distinguishing similar events. Similar to event detection, event extraction focuses on finding events from documents. Most work regarding event extraction (Grishman et al., 2005; Ahn, 2006; Ji and Grishman, 2008; Chen and Ji, 2009; Liao and Grishman, 2010; Hong et al., 2011; Li et al., 2012; Chen and Ng, 2012; Li et al., 2013) was developed under Automatic Content Extraction (ACE) program. The task only defines 33 event types and events are in much finer grain than those in our task. Moreover, there was work (Verhagen et al., 2005; Chambers and Jurafsky, 2008; Bethard, 2013; Chambers, 2013; Chambers et al., 2014) about temporal event extraction and tracking. Like ACE, the granularity of events in this task is too fine to be suitable for our task. Also, timeline generation is related to our work. Most previous work focused on generating a timeAcknowledgments We t"
P15-1056,P05-3021,0,0.114846,"Missing"
P15-1056,P12-2009,0,0.252735,"to the event chronicle) in Section 3.3 are labeled as high rank priority while those without positive documents are labeled as low priority. 4.2 |De | σe : Features We use the following features to train the ranking model, all of which can be provided by TaHBM. • P (s = 1|e): the probability that an event e is topically relevant to the reference chronicle. • P (e|z): the probability reflects an event’s impact given its topic. • σe : the parameter of an event e’s Gaussian distribution. It determines the ‘bandwidth’ 7 8 580 http://www.mapreport.com http://en.wikipedia.org/wiki/2009 5.2 schema (Zhao et al., 2012) to detect events, which is the state-of-the-art event detection method for general domains. Evaluation Methods and Baselines Since there is no existing evaluation metric for the new task, we design a method for evaluation. Although there are manually edited event chronicles on the web, which may serve as references for evaluation, they are often incomplete. For example, the 2010 politics event chronicle on Wikipedia has only two event entries. Hence, we first pool all event entries of existing chronicles on the web and chronicles generated by approaches evaluated in this paper and then have 3"
P15-2110,D14-1082,0,0.0136346,"words. Method 2.1 Local Predictor Conversation-specific features: As mentioned in Section 1, different person roles being the subject or the object of a predicate may have an effect on the tense in a conversation. We analyze the person roles of the subject and the object of the main predicate and encode them as features, which helps our model understand effects of interactions on tense. We develop a Maximum Entropy (MaxEnt) classifier (Zhang, 2004) as the local predictor. Basic features: The unigrams, bigrams and trigrams of a sentence. Dependency parsing features: We use the Stanford parser (Chen and Manning, 2014) to conduct dependency parsing3 on the target sentences and use dependency paths associated with the main predicate of a sentence as well as their dependency types as features. By using the parsing features, 2 3 2.2 Global Predictor As we discussed before, tense ambiguity in a sentence arises from the omissions of sentence components. According to the principle of efficient information transmission (Jaeger and Levy, 2006; http://nlp.cs.rpi.edu/data/chinesetense.zip We use CCProcessed dependencies. 669 Jaeger, 2010) and Gricean Maxims (Grice et al., 1975) in cooperative theory, the omitted elem"
P15-2110,I11-1125,0,0.210627,"l) evidence to enhance the performance. Experimental results demonstrate the power of this hybrid approach, which can serve as a new and promising benchmark. 1 Introduction In natural languages, tense is important to indicate the time at which an action or event takes place. In some languages such as Chinese, verbs do not have explicit morphological or grammatical forms to indicate their tense information. Therefore, automatic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd"
P15-2110,xue-zhang-2014-buy,0,0.659972,"tion is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m telling you: the flight"
P15-2110,D08-1074,0,0.0792966,"Missing"
P15-2110,W06-0107,0,0.033577,"atic tense prediction is important for both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m tel"
P15-2110,D14-1204,0,0.557711,"both human’s deep understanding of these languages as well as downstream natural language processing tasks (e.g., machine translation (Liu et al., 2011)). In this paper, we concern “semantic” tense (time of the event relative to speech time) as opposed to morphosyntactic tense systems found in many languages. Our goal is to predict the tense (past, present or future) of the main predicate1 of each sentence in a Chinese conversation, which has never been thoroughly studied before but is extremely important for conversation understanding. Some recent work (Ye et al., 2006; Xue and Zhang, 2014; Zhang and Xue, 2014) on Chinese 1 The main predicate of a sentence can be considered equal to the root of a dependency parse 668 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 668–673, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics a: [如果(if)]你(you)动(touch)我(my)儿子(son)一下(once)，我(I)先(first)废(destroy)了你(you)。 (If you touch my son, I’ll destroy you.) b: 我(I)告诉(tell)你(you)一声，航班(flight)取消(cancel)了。(I’m telling you: the flight is canceled.) c:你 (you"
P15-2110,W03-1730,0,0.0363715,"−λ) n−1 X i=1 p p p p p local prediction p p c p p p global prediction p p p p p p sentences Figure 1: Global tense prediction for the conversation 5 in Table 1. 3 Experiments 3.1 Data and Scoring Metric To the best of our knowledge, tense prediction in Chinese conversations has never been studied before and there is no existing benchmark for evaluation. We collected 294 conversations (including 1,857 sentences) from 25 popular Chinese movies, dramas and TV shows. Each conversation contains 2-18 sentences. We manually annotate the main predicate and its tense in each sentence. We use ICTCLAS (Zhang et al., 2003) to do word segmentation as preprocessing. Since tense prediction can be seen as a multiclass classification problem, we use accuracy as the metric to evaluate the performance. We randomly split our dataset into three sets: training set (244 conversations), development set (25 conversations) and test set (25 conversations) for evaluation. In evaluation, we ignore imperative sentences and sentences without predicates. Global tense prediction Inspired by the burst detection algorithm proposed by Kleinberg (2003), we use a 3-state automaton sequence model to globally predict tense based on the ab"
P16-1116,P11-1098,0,0.0369488,"from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one eve"
P16-1116,P15-1017,0,0.13171,"o and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-only or features-only. Moreover, all of these methods consider arguments sepa1225 rately while ignoring the relationship between arguments, which is also important for argument identification. Even the joint method (Li et al., 2013) does not model argument relations directly. We use trigger embedding, sentencelevel embedding, and pattern features tog"
P16-1116,P98-1067,0,0.0819726,", sentence-level embedding, and pattern features together as the our features for balancing. • We proposed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and N"
P16-1116,P06-1061,0,0.041959,"ootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et"
P16-1116,P11-1113,0,0.84565,"aining (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate mod"
P16-1116,P11-1114,0,0.19134,"ed and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type,"
P16-1116,E12-1029,0,0.213239,"how that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et"
P16-1116,P08-1030,0,0.738218,"ents. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explore"
P16-1116,W05-0610,0,0.0178924,"f and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into conside"
P16-1116,P13-1008,0,0.390565,"2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works ar"
P16-1116,P10-1081,0,0.591789,"n the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff e"
P16-1116,E12-1030,0,0.0160451,"tern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other met"
P16-1116,P12-1088,0,0.0652043,"and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based method (Lu and Roth, 2012) trains separate models for each event type, which requires a lot of training data. The dynamic multi-pooling convolutional neural network (DMCNN) (Chen et al., 2015) is currently the only widely used deep neural network based approach. DMCNN is mainly used to model contextual features. However, DMCNN still does not consider argument-argument interactions. In summary, most of the above works are either pattern-onl"
P16-1116,P07-1075,0,0.0284876,"). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012) are also considered an effective solution. (Li et al., 2013) make full use of the lexical and contextual features to get better results. The semi-CRF based me"
P16-1116,D07-1075,0,0.0301605,"ethods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for exam"
P16-1116,C00-2136,0,0.154785,"ed a regularization-based method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly super"
P16-1116,P05-1062,0,0.0399324,"3; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (M"
P16-1116,D09-1016,0,0.293776,"re patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Patwardhan and Riloff, 2007; Chambers and Jurafsky, 2011). In some of these systems, human work is needed to delete some nonsense patterns or rules. Other methods (Gu and Cercone, 2006; Patwardhan and Riloff, 2009) consider broader context when deciding on role fillers. Other systems take the whole discourse feature into consideration, such as (Maslennikov and Chua, 2007; Liao and Grishman, 2010; Hong et al., 2011; Huang and Riloff, 2011). Ji and Grishman (2008) even consider topic-related documents, proposing a cross-document method. (Liao and Grishman, 2010; Hong et al., 2011) use a series of global features (for example, the occurrence of one event type lead to the occurrence of another) to improve role assignment and event classification performance. Joint models (Li et al., 2013; Lu and Roth, 2012)"
P16-1116,P06-2094,0,0.0388605,"improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,N06-1039,0,0.0146725,"n method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of event extraction systems have also been explored (Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 200"
P16-1116,P05-1047,0,0.0304587,"o make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pattern-based and rule-based) of e"
P16-1116,P03-1029,0,0.0553078,"d method in order to make use of the relationship between candidate arguments. Our experiments on the ACE 2005 data set show that the regularization method does improve the performance of argument identification. 2 Related Work There is a large body of previous work devoted to event extraction. Many traditional works focus on using pattern based methods for identifying event type (Kim and Moldovan, 1993; Riloff and others, 1993; Soderland et al., 1995; Huffman, 1996; Freitag, 1998b; Ciravegna and others, 2001; Califf and Mooney, 2003; Riloff, 1996; Riloff et al., 1999; Yangarber et al., 2000; Sudo et al., 2003; Stevenson and Greenwood, 2005; Grishman et al., 2005; Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012). (Shinyama and Sekine, 2006; Sekine, 2006) are unsupervised methods of extracting patterns from open domain texts. Pattern is not always enough, although some methods (Huang and Riloff, 2012; Liu and Strzalkowski, 2012) use bootstrapping to get more patterns. There are also feature-based classification methods (Freitag, 1998a; Chieu and Ng, 2002; Finn and Kushmerick, 2004; Li et al., 2005; Yu et al., 2005). Apart from the above methods, weakly supervised training (pat"
P16-1116,C98-1064,0,\N,Missing
P16-1218,D15-1041,0,0.0357672,"f hand-crafted features. Different from previous work, which is sensitive to local state and accesses to larger context by higher-order factorization. Our model makes parsing decisions on a global perspective with firstorder factorization, avoiding the expensive computational cost introduced by high-order factorization. LSTM network is heavily utilized in our model. LSTM network has already been explored in transition-based dependency parsing. Dyer et al. (2015) presented stack LSTMs with push and pop operations and used them to implement a state-of-the-art transition-based dependency parser. Ballesteros et al. (2015) replaced lookup-based word representations with characterbased representations obtained by Bidirectional LSTM in the continuous-state parser of Dyer et al. (2015), which was proved experimentally to be useful for morphologically rich languages. 7 Conclusion In this paper, we propose an LSTM-based neural network model for graph-based dependency parsing. Utilizing Bidirectional LSTM and segment embeddings learned by LSTM-Minus allows our model access to sentence-level information, making our model more accurate in recovering longdistance dependencies with only first-order factorization. Experim"
P16-1218,D07-1101,0,0.514079,"arsing models and state-of-the-art models. 1 Introduction Dependency parsing is a fundamental task for language processing which has been investigated for decades. It has been applied in a wide range of applications such as information extraction and machine translation. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012) and then score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights. However, conventional graph-based models heavily rely on feature engineering and their performance is restricted by the design of features. In addition, standard decoding algorithm (Eisner, 2000) only works for the first-order model which limits the scope of feature selection. To incorporate high-order"
P16-1218,D14-1082,0,0.465746,"Missing"
P16-1218,P15-1033,0,0.386707,"ing rate (α = 0.2 in our experiment) and gτ ∈ R|θi |is the subgradient at time step τ for parameter θi . To mitigate overfitting, dropout (Hinton et al., 2012) is used to regularize our model. we apply dropout on the hidden layer with 0.2 rate. 4.3 Model Initialization&Hyperparameters The following hyper-parameters are used in all experiments: word embedding size = 100, POS tag embedding size = 100, hidden layer size = 200, LSTM hidden vector size = 100, Bidirectional LSTM layers = 2, regularization parameter λ = 10−4 . We initialized the parameters using pretrained word embeddings. Following Dyer et al. (2015), we use a variant of the skip n-gram model introduced by Ling et al. (2015) on Gigaword corpus (Graff et al., 2003). We also experimented with randomly initialized embeddings, where embeddings are uniformly sampled from range [−0.3, 0.3]. All other parameters are uniformly sampled from range [−0.05, 0.05]. 2310 First-order Second-order Third-order Fourth-order Unlimited-order Models MSTParser 1st-order atomic (Pei et al., 2015) 1st-order phrase (Pei et al., 2015) Our basic model Our basic model + segment MSTParser 2nd-order phrase (Pei et al., 2015) (Koo and Collins, 2010) (Ma and Zhao, 2012)"
P16-1218,P10-1001,0,0.061188,"troduction Dependency parsing is a fundamental task for language processing which has been investigated for decades. It has been applied in a wide range of applications such as information extraction and machine translation. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012) and then score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights. However, conventional graph-based models heavily rely on feature engineering and their performance is restricted by the design of features. In addition, standard decoding algorithm (Eisner, 2000) only works for the first-order model which limits the scope of feature selection. To incorporate high-order features, Eisner algorithm must be somehow extended o"
P16-1218,N15-1142,0,0.0111723,"me step τ for parameter θi . To mitigate overfitting, dropout (Hinton et al., 2012) is used to regularize our model. we apply dropout on the hidden layer with 0.2 rate. 4.3 Model Initialization&Hyperparameters The following hyper-parameters are used in all experiments: word embedding size = 100, POS tag embedding size = 100, hidden layer size = 200, LSTM hidden vector size = 100, Bidirectional LSTM layers = 2, regularization parameter λ = 10−4 . We initialized the parameters using pretrained word embeddings. Following Dyer et al. (2015), we use a variant of the skip n-gram model introduced by Ling et al. (2015) on Gigaword corpus (Graff et al., 2003). We also experimented with randomly initialized embeddings, where embeddings are uniformly sampled from range [−0.3, 0.3]. All other parameters are uniformly sampled from range [−0.05, 0.05]. 2310 First-order Second-order Third-order Fourth-order Unlimited-order Models MSTParser 1st-order atomic (Pei et al., 2015) 1st-order phrase (Pei et al., 2015) Our basic model Our basic model + segment MSTParser 2nd-order phrase (Pei et al., 2015) (Koo and Collins, 2010) (Ma and Zhao, 2012) (Zhang and McDonald, 2012) (Zhang et al., 2013) (Zhang and McDonald, 2014)"
P16-1218,C12-2077,0,0.109483,"arsing is a fundamental task for language processing which has been investigated for decades. It has been applied in a wide range of applications such as information extraction and machine translation. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012) and then score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights. However, conventional graph-based models heavily rely on feature engineering and their performance is restricted by the design of features. In addition, standard decoding algorithm (Eisner, 2000) only works for the first-order model which limits the scope of feature selection. To incorporate high-order features, Eisner algorithm must be somehow extended or modified, which is"
P16-1218,de-marneffe-etal-2006-generating,0,0.0391472,"Missing"
P16-1218,E06-1011,0,0.206881,"der graph-based dependency parsing models and state-of-the-art models. 1 Introduction Dependency parsing is a fundamental task for language processing which has been investigated for decades. It has been applied in a wide range of applications such as information extraction and machine translation. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012) and then score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights. However, conventional graph-based models heavily rely on feature engineering and their performance is restricted by the design of features. In addition, standard decoding algorithm (Eisner, 2000) only works for the first-order model which limits the scope of feature selection. To incorp"
P16-1218,P05-1012,0,0.931054,"ur model could be competitive with previous higher-order graph-based dependency parsing models and state-of-the-art models. 1 Introduction Dependency parsing is a fundamental task for language processing which has been investigated for decades. It has been applied in a wide range of applications such as information extraction and machine translation. Among a variety of dependency parsing models, graph-based models are attractive for their ability of scoring the parsing decisions on a whole-tree basis. Typical graph-based models factor the dependency tree into subgraphs, including single arcs (McDonald et al., 2005), sibling or grandparent arcs (McDonald and Pereira, 2006; Carreras, 2007) or higher-order substructures (Koo and Collins, 2010; Ma and Zhao, 2012) and then score the whole tree by summing scores of the subgraphs. In these models, subgraphs are usually represented as high-dimensional feature vectors which are then fed into a linear model to learn the feature weights. However, conventional graph-based models heavily rely on feature engineering and their performance is restricted by the design of features. In addition, standard decoding algorithm (Eisner, 2000) only works for the first-order mod"
P16-1218,P14-1028,1,0.628714,"onality of hidden layer vector, bdh ∈ Rdh is bias term. Whdi and bdh are bound with index d ∈ {0, 1} which indicates the direction between head and modifier. A output layer is finally added on the top of the hidden layer for scoring dependency arcs: ScoreC(x, c) = Wod h + bdo (6) Where Wod ∈ RL×dh is weight matrices, bdo ∈ RL is bias term, ScoreC(x, c) ∈ RL is the output vector, L is the number of dependency types. Each dimension of the output vector is the score for each kind of dependency type of head-modifier pair. 3.5 Features in our model Previous neural network models (Pei et al., 2015; Pei et al., 2014; Zheng et al., 2013) normally set context window around a word and extract atomic features within the window to represent the contextual information. However, context window limits their ability in detecting long-distance information. Simply increasing the context window size to get more contextual information puts their model in the risk of overfitting and heavily slows down the speed. Unlike previous work, we apply Bidirectional LSTM to capture long range contextual information and eliminate the need for context windows, avoiding the limit of the window-based feature selection approach. Com"
P16-1218,P15-1031,1,0.136238,"er model which limits the scope of feature selection. To incorporate high-order features, Eisner algorithm must be somehow extended or modified, which is usually done at high cost in terms of efficiency. The fourth-order graph-based model (Ma and Zhao, 2012), which seems the highest-order model so far to our knowledge, requires O(n5 ) time and O(n4 ) space. Due to the high computational cost, highorder models are normally restricted to producing only unlabeled parses to avoid extra cost introduced by inclusion of arc-labels into the parse trees. To alleviate the burden of feature engineering, Pei et al. (2015) presented an effective neural network model for graph-based dependency parsing. They only use atomic features such as word unigrams and POS tag unigrams and leave the model to automatically learn the feature combinations. However, their model requires many atomic features and still relies on high-order factorization strategy to further improve the accuracy. Different from previous work, we propose an LSTM-based dependency parsing model in this paper and aim to use LSTM network to capture richer contextual information to support parsing decisions, instead of adopting a high-order factorization"
P16-1218,P14-2107,0,0.0671826,"Missing"
P16-1218,P11-2033,0,0.0849117,"and McDonald, 2014). Following previous work, UAS (unlabeled attachment scores) and LAS (labeled attachment scores) are calculated by excluding punctuation5 . The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM which is same to Pei et al. (2015). We measure the parsing speeds of Pei et al. (2015) according to their codes6 and parameters. On accuracy, as shown in table 2, our 4 http://sourceforge.net/projects/ mstparser 5 Following previous work, a token is a punctuation if its POS tag is {“ ” : , .} 6 https://github.com/Williammed/ DeepParser 2311 Method (Zhang and Nivre, 2011) (Bernd Bohnet, 2012) (Zhang and McDonald, 2014) (Dyer et al., 2015) (Weiss et al., 2015) Our basic model + segment Penn-YM UAS LAS 92.9 91.8 93.39 92.38 93.57 92.48 93.51 92.45 Penn-SD UAS LAS 93.01 90.64 93.1 90.9 93.99 92.05 94.08 91.82 CTB5 UAS LAS 86.0 84.4 87.5 85.9 87.96 86.34 87.2 85.7 87.55 86.23 Table 3: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5. basic model outperforms previous first-order graph-based models by a substantial margin, even outperforms Zhang and McDonald (2012)’s unlimited-order model. Moreover, incorporating segment information furt"
P16-1218,D13-1093,0,0.152553,"p n-gram model introduced by Ling et al. (2015) on Gigaword corpus (Graff et al., 2003). We also experimented with randomly initialized embeddings, where embeddings are uniformly sampled from range [−0.3, 0.3]. All other parameters are uniformly sampled from range [−0.05, 0.05]. 2310 First-order Second-order Third-order Fourth-order Unlimited-order Models MSTParser 1st-order atomic (Pei et al., 2015) 1st-order phrase (Pei et al., 2015) Our basic model Our basic model + segment MSTParser 2nd-order phrase (Pei et al., 2015) (Koo and Collins, 2010) (Ma and Zhao, 2012) (Zhang and McDonald, 2012) (Zhang et al., 2013) (Zhang and McDonald, 2014) UAS 91.60 92.14 92.59 93.09 93.51 92.30 93.29 93.04 93.4 93.06 93.50 93.57 LAS 90.39 90.92 91.37 92.03 92.45 91.06 92.13 N/A N/A 91.86 92.41 92.48 Speed(sent/s) 20 55 26 61 26 14 10 N/A N/A N/A N/A N/A Table 2: Comparison with previous graph-based models on Penn-YM. 5 Experiments In this section, we present our experimental setup and the main result of our work. 5.1 Experiments Setup We conduct our experiments on the English Penn Treebank (PTB) and the Chinese Penn Treebank (CTB) datasets. For English, we follow the standard splits of PTB3. Using section 2-21 for tr"
P16-1218,D13-1061,0,0.031699,"layer vector, bdh ∈ Rdh is bias term. Whdi and bdh are bound with index d ∈ {0, 1} which indicates the direction between head and modifier. A output layer is finally added on the top of the hidden layer for scoring dependency arcs: ScoreC(x, c) = Wod h + bdo (6) Where Wod ∈ RL×dh is weight matrices, bdo ∈ RL is bias term, ScoreC(x, c) ∈ RL is the output vector, L is the number of dependency types. Each dimension of the output vector is the score for each kind of dependency type of head-modifier pair. 3.5 Features in our model Previous neural network models (Pei et al., 2015; Pei et al., 2014; Zheng et al., 2013) normally set context window around a word and extract atomic features within the window to represent the contextual information. However, context window limits their ability in detecting long-distance information. Simply increasing the context window size to get more contextual information puts their model in the risk of overfitting and heavily slows down the speed. Unlike previous work, we apply Bidirectional LSTM to capture long range contextual information and eliminate the need for context windows, avoiding the limit of the window-based feature selection approach. Compared with Pei et al."
P16-1218,N03-1033,0,0.366597,"Missing"
P16-1218,P15-1032,0,0.106269,"beled attachment scores) are calculated by excluding punctuation5 . The parsing speeds are measured on a workstation with Intel Xeon 3.4GHz CPU and 32GB RAM which is same to Pei et al. (2015). We measure the parsing speeds of Pei et al. (2015) according to their codes6 and parameters. On accuracy, as shown in table 2, our 4 http://sourceforge.net/projects/ mstparser 5 Following previous work, a token is a punctuation if its POS tag is {“ ” : , .} 6 https://github.com/Williammed/ DeepParser 2311 Method (Zhang and Nivre, 2011) (Bernd Bohnet, 2012) (Zhang and McDonald, 2014) (Dyer et al., 2015) (Weiss et al., 2015) Our basic model + segment Penn-YM UAS LAS 92.9 91.8 93.39 92.38 93.57 92.48 93.51 92.45 Penn-SD UAS LAS 93.01 90.64 93.1 90.9 93.99 92.05 94.08 91.82 CTB5 UAS LAS 86.0 84.4 87.5 85.9 87.96 86.34 87.2 85.7 87.55 86.23 Table 3: Comparison with previous state-of-the-art models on Penn-YM, Penn-SD and CTB5. basic model outperforms previous first-order graph-based models by a substantial margin, even outperforms Zhang and McDonald (2012)’s unlimited-order model. Moreover, incorporating segment information further improves our model’s accuracy, which shows that segment embeddings do capture richer"
P16-1218,W03-3023,0,0.257701,"Missing"
P16-1218,D08-1059,0,0.0281657,"Missing"
P16-1218,D12-1030,0,0.153232,"we use a variant of the skip n-gram model introduced by Ling et al. (2015) on Gigaword corpus (Graff et al., 2003). We also experimented with randomly initialized embeddings, where embeddings are uniformly sampled from range [−0.3, 0.3]. All other parameters are uniformly sampled from range [−0.05, 0.05]. 2310 First-order Second-order Third-order Fourth-order Unlimited-order Models MSTParser 1st-order atomic (Pei et al., 2015) 1st-order phrase (Pei et al., 2015) Our basic model Our basic model + segment MSTParser 2nd-order phrase (Pei et al., 2015) (Koo and Collins, 2010) (Ma and Zhao, 2012) (Zhang and McDonald, 2012) (Zhang et al., 2013) (Zhang and McDonald, 2014) UAS 91.60 92.14 92.59 93.09 93.51 92.30 93.29 93.04 93.4 93.06 93.50 93.57 LAS 90.39 90.92 91.37 92.03 92.45 91.06 92.13 N/A N/A 91.86 92.41 92.48 Speed(sent/s) 20 55 26 61 26 14 10 N/A N/A N/A N/A N/A Table 2: Comparison with previous graph-based models on Penn-YM. 5 Experiments In this section, we present our experimental setup and the main result of our work. 5.1 Experiments Setup We conduct our experiments on the English Penn Treebank (PTB) and the Chinese Penn Treebank (CTB) datasets. For English, we follow the standard splits of PTB3. Usin"
P16-1218,E12-1009,0,\N,Missing
P17-1018,P16-1086,0,0.0145181,"t that passage parts are of different importance to the particular question for reading comprehension and question answering. are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage. End-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models (Hermann et al., 2015; Hill et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Sordoni et al., 2016; Cui et al., 2016; Trischler et al., 2016; Dhingra et al., 2016; Shen et al., 2016) have been introduced to solve this problem. Hermann et al. (2015) first introduce attention mechanism into reading comprehension. Hill et al. (2016) propose a windowbased memory network for CBT dataset. Kadlec et al. (2016) introduce pointer networks with one attention step to predict the blanking out entities. Sordoni et al. (2016) propose an iterative alternating attention mechanism to better model the links between question and passage. Trischler et al. (2016) solve cloze-style questio"
P17-1018,D14-1159,0,0.00946933,"odel for encoding evidence from the passage, we draw the align194 Result Analysis (a) (b) (c) (d) Figure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated. model and its ablati"
P17-1018,D15-1161,0,0.0171807,"yen et al., 2016) is also a large-scale dataset. The questions in the dataset Related Work Reading Comprehension and Question Answering Dataset Benchmark datasets play an important role in recent progress in reading comprehension and question answering research. Exist195 self-matching attention in our model. It dynamically refines the passage representation by looking over the whole passage and aggregating evidence relevant to the current passage word and question, allowing our model make full use of passage information. Weightedly attending to word context has been proposed in several works. Ling et al. (2015) propose considering window-based contextual words differently depending on the word and its relative position. Cheng et al. (2016) propose a novel LSTM network to encode words in a sentence which considers the relation between the current token being processed and its past tokens in the memory. Parikh et al. (2016) apply this method to encode words in a sentence according to word form and its distance. Since passage information relevant to question is more helpful to infer the answer in reading comprehension, we apply self-matching based on question-aware representation and gated attention-ba"
P17-1018,P16-1223,0,0.37859,"t. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated. model and its ablation models. As we can see, both four models show the same trend. The questions are split into different groups based on a set of question words we have defined, including “what”, “how”, “who”, “when”, “which”, “where”, and “why”. As we can see, our model is better at “when” and “who” questions, but poorly on “why” questions. This is mainly because the answers to why questions can be very diverse, and they are"
P17-1018,D16-1053,0,0.675629,"ive Innovation Center for Language Ability, Xuzhou, 221009, China {wangwenhui,chbb}@pku.edu.cn {nanya,fuwei,mingzhou}@microsoft.com Abstract 2015; Hill et al., 2016) in which answers are single words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the answer (Rajpurkar et al., 2016). Rapid progress has been made since the release of the SQuAD dataset. Wang and Jiang (2016b) build question-aware passage representation with match-LSTM (Wang and Jiang, 2016a), and predict answer boundaries in the passage with pointer networks (Vinyals et al., 2015). Seo et al. (2016) introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. Xiong et al. (2016) propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. Lee et al. (2016) and Yu et al. (2016) predict answers by ranking continuous text spans within passages. Inspired by Wang and Jiang (2016b), we introduce a gated self-matching network, illustrated in Figure 1, an end-to-end neural network model for reading comprehension and question answering. Our model consists of four parts:"
P17-1018,P14-5010,0,0.00543631,"Missing"
P17-1018,D14-1179,0,0.0126271,"Missing"
P17-1018,D16-1244,0,0.102298,"Missing"
P17-1018,N16-1170,0,0.257789,"d Question Answering Wenhui Wang†§∗ Nan Yang‡§ Furu Wei‡ Baobao Chang† Ming Zhou‡ † Key Laboratory of Computational Linguistics, Peking University, MOE, China ‡ Microsoft Research, Beijing, China  Collaborative Innovation Center for Language Ability, Xuzhou, 221009, China {wangwenhui,chbb}@pku.edu.cn {nanya,fuwei,mingzhou}@microsoft.com Abstract 2015; Hill et al., 2016) in which answers are single words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the answer (Rajpurkar et al., 2016). Rapid progress has been made since the release of the SQuAD dataset. Wang and Jiang (2016b) build question-aware passage representation with match-LSTM (Wang and Jiang, 2016a), and predict answer boundaries in the passage with pointer networks (Vinyals et al., 2015). Seo et al. (2016) introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. Xiong et al. (2016) propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. Lee et al. (2016) and Yu et al. (2016) predict answers by ranking continuous text spans within passages. Inspired by Wang and Jiang ("
P17-1018,D14-1162,0,0.122829,"Missing"
P17-1018,D16-1264,0,0.76541,"tation is that it has very limited knowledge of context. One answer candidate is often oblivious to important (5) 191 When predicting the start position, hat−1 represents the initial hidden state of the answer recurrent network. We utilize the question vector rQ as the initial state of the answer recurrent network. rQ = att(uQ , VrQ ) is an attention-pooling vector of the question based on the parameter VrQ : cues in the passage outside its surrounding window. Moreover, there exists some sort of lexical or syntactic divergence between the question and passage in the majority of SQuAD dataset (Rajpurkar et al., 2016). Passage context is necessary to infer the answer. To address this problem, we propose directly matching the question-aware passage representation against itself. It dynamically collects evidence from the whole passage for words in passage and encodes the evidence relevant to the current passage word and its matching question information into the passage representation hPt : hPt = BiRNN(hPt−1 , [vtP , ct ]) Q Q sj = vT tanh(WuQ uQ j + Wv V r ) ai = exp(si )/Σm j=1 exp(sj ) Q rQ = Σm i=1 ai ui To train the network, we minimize the sum of the negative log probabilities of the ground truth start"
P17-1018,D13-1020,0,0.104396,"show the ability of the model for encoding evidence from the passage, we draw the align194 Result Analysis (a) (b) (c) (d) Figure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point. ing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that are automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated."
P17-1018,D15-1237,0,0.122175,"Missing"
P17-1018,D16-1013,0,0.0821462,"cular question for reading comprehension and question answering. are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage. End-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models (Hermann et al., 2015; Hill et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Sordoni et al., 2016; Cui et al., 2016; Trischler et al., 2016; Dhingra et al., 2016; Shen et al., 2016) have been introduced to solve this problem. Hermann et al. (2015) first introduce attention mechanism into reading comprehension. Hill et al. (2016) propose a windowbased memory network for CBT dataset. Kadlec et al. (2016) introduce pointer networks with one attention step to predict the blanking out entities. Sordoni et al. (2016) propose an iterative alternating attention mechanism to better model the links between question and passage. Trischler et al. (2016) solve cloze-style question answering task by combining an attentive model with a rerankin"
P17-1189,D16-1212,1,0.768908,"Missing"
P17-1189,N04-1032,0,0.0950101,"Missing"
P17-1189,W13-3820,0,0.176999,"B 1.0 show that our model outperforms state-of-the-art methods. 1 Meanwhile widely solicit opinions, for many times [Rel 修改] [Arg1 *pro*] 。 revise (omitted) . They to system made revise . Figure 1: Sentences from (a) CPB and (b) our heterogeneous dataset. In CPB, each predicate (e.g., 修改) has a specific set of core roles given with numbers (e.g., Arg0). While our dataset uses a different semantic role set, and all roles are nonpredicate-specific. Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBan"
P17-1189,P06-2013,0,0.111481,"Missing"
P17-1189,J02-3001,0,0.152943,"Missing"
P17-1189,C16-1120,0,0.317942,"which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al. (2015) introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings. Specifically, they learn an LSTM RNN model based on NetBank first, then initi"
P17-1189,D09-1153,1,0.955645,"NLP tasks, like event extraction and relation classification, etc. 2070 CPB only annotate common verbs as predicates. • In terms of semantic roles, CSB has a more fine-grained semantic role set. There are 31 roles defined in five types (as Table. 1 shows). Whereas in CPB, there are totally 23 roles, including core roles and non-core roles. • CSB does not have any pre-defined frames for predicates because all roles are set to be non-predicate-specific. The reason for not defining frames is that frames may lead inconsistencies in labels. For example, according to Chinese verb formation theory (Sun et al., 2009), in CPB, an agent of a verb is often marked as its Arg0, but not all Arg0 are agents. Therefore, roles are defined for predicates with similar syntactic and semantic regularities, rather than single predicate. Two direct benefits of using stand-alone nonpredicate-specific roles are: First, meanings of all semantic roles can be directly inferred from their labels. For instance, roles of things that people are telling (谈 ) or looking (看) are labeled as 内 容/content, because verbs like 谈 and 看 are often followed by an object. Second, we can easily annotate sentences with new predicates without de"
P17-1189,D15-1186,1,0.409335,"s been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al. (2015) introduced a heterogeneous dataset, Chinese NetBank, by pretraining word embeddings. Specifically, they learn an LSTM RNN model based on NetBank"
P17-1189,P12-1095,0,0.0160469,"ely solicit opinions, for many times [Rel 修改] [Arg1 *pro*] 。 revise (omitted) . They to system made revise . Figure 1: Sentences from (a) CPB and (b) our heterogeneous dataset. In CPB, each predicate (e.g., 修改) has a specific set of core roles given with numbers (e.g., Arg0). While our dataset uses a different semantic role set, and all roles are nonpredicate-specific. Semantic role labeling (SRL) is one of the fundamental tasks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the"
P17-1189,xue-2006-annotating,0,0.0762818,"Missing"
P17-1189,J08-2004,0,0.894447,", 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced resources are heterogeneous in nature, without sharing the same tagging schema, semantic role set, syntactic tag set and domain. For example, Wang et al."
P17-1189,W03-1707,0,0.232,"sks in natural language processing because of its important role in information extraction (Bastianelli et al., 2013), statistical machine translation (Aziz et al., 2016; Xiong et al., 2012), and so on. However, state-of-the-art performance of Chinese SRL is still far from satisfactory. And data sparsity has been a bottleneck which can not be http://www.klcl.pku.edu.cn/ShowNews.aspx?id=156 repeatedly (b) [agent 他们] 对 [patient 系统]进行了[Rel 修改] 。 Introduction 1 the NPC Standing Committee 广泛 征求 意见, [ArgM-ADV 多次] [ArgM-ADV 反复] ignored. For English, the most commonly used benchmark dataset PropBank (Xue and Palmer, 2003) has about 54,900 sentences. But for Chinese, there are only 10,364 sentences in Chinese PropBank 1.0 (CPB) (with about 35,700 propositions) (Xue, 2008). To mitigate the data sparsity, models incorporating heterogeneous resources have been introduced to improve Chinese SRL performance (Wang et al., 2015; Guo et al., 2016; Li et al., 2016). The heterogeneous resources introduced by these models include other semantically annotated corpora with annotation schema different to that used in PropBank, and even of a different language. The challenge here lies in the fact that those newly introduced r"
P17-1189,D14-1041,0,0.0386753,"Missing"
P17-1189,W11-2136,0,\N,Missing
P18-1230,D14-1110,0,0.114658,"Missing"
P18-1230,D17-1167,0,0.0278569,"d semantic labels in WordNet can help to WSD in a multi-task learning framework. As far as we know, there is no study directly integrates glosses or semantic relations of the WordNet into an end-to-end model. In this paper, we focus on how to integrate glosses into a unified neural WSD system. Memory network (Sukhbaatar et al., 2015b; Kumar et al., 2016; Xiong et al., 2016) is initially proposed to solve question answering problems. Recent researches show that memory network obtains the state-of-the-art results in many NLP tasks such as sentiment classification (Li et al., 2017) and analysis (Gui et al., 2017), poetry generation (Zhang et al., 2017), spoken language understanding (Chen et al., 2016), etc. Inspired by the success of memory network used in many NLP tasks, we introduce it into WSD. We make some adaptations to the initial memory network in order to incorporate glosses and capture the inner relationship between the context and glosses. 3 Incorporating Glosses into Neural Word Sense Disambiguation In this section, we first give an overview of the proposed model GAS: a gloss-augmented WSD neural network which integrates the context and the glosses of the target word into a unified framewo"
P18-1230,P16-1085,0,0.188836,"on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet (Miller, 1995) which are w"
P18-1230,W16-5307,0,0.165559,"Missing"
P18-1230,C14-1151,0,0.437299,"Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet (Miller, 1995) which are widely used in the knowledge-based methods. The gloss, which extensionally defines a word sense meaning, plays a key role in the well-known Lesk algorithm (Lesk, 1986). Recent studies (Banerjee and Pedersen, 2002; Basile et al., 2014) have shown that enriching gloss information through its semantic relations can greatly improve the accuracy of Lesk algorithm. To this end, our goal is to incorporate the gloss information into a unified neural network for all of the polysemous words. We further consider extending the original gloss through its semantic relations in our framework. As shown in Figure 1, the glosses of hypernyms and hyponyms can enrich the original gloss information as well as help to build better a sense representation. Therefore, we integrate not only the original gloss but also the related glosses of hyperny"
P18-1230,P15-1072,0,0.0598781,"Missing"
P18-1230,P15-2003,0,0.0608941,"Missing"
P18-1230,J81-4005,0,0.734025,"Missing"
P18-1230,Q14-1019,0,0.40617,"external knowledge. The fives blocks list the MFS baseline, two knowledge-based systems, two supervised systems (feature-based), three neuralbased systems and our models, respectively. . • Bi-LSTM+att.+LEX and its variant BiLSTM+att.+LEX+P OS : Raganato et al. (2017a) transfers WSD into a sequence learning task and propose a multi-task learning framework for WSD, POS tagging and coarse-grained semantic labels (LEX). These two models have used the external knowledge, for the LEX is based on lexicographer files in WordNet. gloss information via its semantic relations can help to WSD. • Babelfy: Moro et al. (2014) exploits the semantic network structure from BabelNet and builds a unified graph-based architecture for WSD and Entity Linking. 4.3.2 Supervised Systems The supervised systems mentioned in this paper refers to traditional feature-based systems which train a dedicated classifier for every word individually (word expert). • IMS: Zhi and Ng (2010) selects a linear Support Vector Machine (SVM) as its classifier and makes use of a set of features surrounding the target word within a limited window, such as POS tags, local words and local collocations. • IMS+emb : Iacobacci et al. (2016) selects IM"
P18-1230,D17-1120,0,0.266526,"rich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only con"
P18-1230,E17-1010,0,0.3374,"rich the gloss information. The experimental results show that our model outperforms the state-of-theart systems on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only con"
P18-1230,P15-1173,0,0.101594,"Missing"
P18-1230,P17-1125,0,0.0318585,"to WSD in a multi-task learning framework. As far as we know, there is no study directly integrates glosses or semantic relations of the WordNet into an end-to-end model. In this paper, we focus on how to integrate glosses into a unified neural WSD system. Memory network (Sukhbaatar et al., 2015b; Kumar et al., 2016; Xiong et al., 2016) is initially proposed to solve question answering problems. Recent researches show that memory network obtains the state-of-the-art results in many NLP tasks such as sentiment classification (Li et al., 2017) and analysis (Gui et al., 2017), poetry generation (Zhang et al., 2017), spoken language understanding (Chen et al., 2016), etc. Inspired by the success of memory network used in many NLP tasks, we introduce it into WSD. We make some adaptations to the initial memory network in order to incorporate glosses and capture the inner relationship between the context and glosses. 3 Incorporating Glosses into Neural Word Sense Disambiguation In this section, we first give an overview of the proposed model GAS: a gloss-augmented WSD neural network which integrates the context and the glosses of the target word into a unified framework. After that, each individual module i"
P18-1230,P10-4014,0,0.319401,"-of-theart systems on several English all-words WSD datasets. 1 Introduction Word Sense Disambiguation (WSD) is a fundamental task and long-standing challenge in Natural Language Processing (NLP). There are several lines of research on WSD. Knowledge-based methods focus on exploiting lexical resources to infer the senses of word in the context. Supervised methods usually train multiple classifiers with manual designed features. Although supervised methods can achieve the state-of-the-art performance (Raganato et al., 2017b,a), there are still two major challenges. Firstly, supervised methods (Zhi and Ng, 2010; Iacobacci et al., 2016) usually train a dedicated classifier for each word individually (often called word expert). So it can not easily scale up to all-words WSD task which requires to disambiguate all the polysemous word in texts 1 . Recent neural-based methods (K˚ageb¨ack and Salomonsson, 2016; Raganato et al., 2017a) solve this problem by building a unified model for all the polysemous words, but they still can’t beat the best word expert system. Secondly, all the neural-based methods always only consider the local context of the target word, ignoring the lexical resources like WordNet ("
P19-1194,P18-1139,0,0.0593011,"ed on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from them, we dynamically update the pseudo-parallel data via on-the-fly back-translation (Lample et al., 2018b) during training (Eq. 12). There are some other tasks of NLP also show interest in controlling the fine-grained attribute of text generation. For example, Zhang et al. (2018a) and Ke et al. (2018) propose to control the specificity and diversity in dialogue generation. We borrow ideas from these works but the motivation and proposed models of our work are a far cry from them. The main differences are: (1) Since sentiment is dependent on local context while specificity is independent of local context, there is a series of design in our model to take the local context (or previous generated words) st into consideration (e.g., Eq. 1, Eq. 3). (2) Due to the lack of parallel data, we propose a cycle reinforcement learning algorithm to train the proposed model (Section 2.3). 6 Conclusion In"
P19-1194,D14-1181,0,0.00290278,"ut is also important. Inspired by the Mean Reciprocal Rank metric which is widely used in the Information Retrieval area, we design a Mean Relative Reciprocal Rank (MRRR) metric to measure the relative ranking MRRR = N 1 X 1 N i=1 |rank(vi ) − rank(ˆ vi ) |+ 1 (13) In addition, we also compare our model with the coarse-grained sentiment transfer systems. In order to make the results comparable, we define the generated test samples of all baselines for reproducibility. sentiment intensity larger/smaller than 0.5 as positive/negative results. Then we use a pre-trained binary TextCNN classifier (Kim, 2014) to compute the classification accuracy. 3.4.2 Human Evaluation We also perform human evaluation to assess the quality of generated sentences more accurately. Each item contains the source input, the sampled target sentiment intensity value, and the output of different systems. Then 500 items are distributed to 3 evaluators, who are required to score the generated sentences from 1 to 5 based on the input and target sentiment intensity value in terms of three criteria: content, sentiment, fluency. Content evaluates the content preservation degree. Sentiment refers to how much the output matches"
P19-1194,D18-1549,0,0.0242572,"Missing"
P19-1194,D17-1230,0,0.0454095,"in Figure 2. By means of policy gradient method (Williams, 1992), for each training example, the expected gradient of Eq. 10 can be approximated as: K   1 X (k) ∇θ L(θ) &apos; − r − b ∇θ log pθ (yˆ(k) ) K k=1 (11) where K is the sample size and b is the greedy search decoding baseline that aims to reduce the variance of gradient estimate which is implemented in the same way as Paulus et al. (2017). Nevertheless, RL training strives to optimize a specific metric which may not guarantee the fluency of the generated text (Paulus et al., 2017), and 2016 usually faces the unstable training problems (Li et al., 2017). The most direct way is to expose the sentences which are from the training corpus to the decoder and trained via MLE (also called teacher-forcing). In order to expose the decoder to the original sentence from the training corpus, we borrow ideas from back-translation (Lample et al., 2018a,b). Specifically, the model first generates a sequence yˆ based on the input text x and the target sentiment intensity value vy , and then reconstructs the source input x based on yˆ and the source sentiment intensity value vx . Therefore, the gradient of the cycle reconstruction loss is defined as:   ∇θ"
P19-1194,N18-1169,0,0.0483598,"Missing"
P19-1194,D18-1420,0,0.19687,"ntences whose intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,D15-1166,0,0.0327,"0, 1]. Intuitively, in order to achieve fine-grained control of sentiment, words whose sentiment intensities are closer to the target sentiment intensity value vy should be assigned a higher probability. Take Figure 2 as an example, at the 5-th time-step, word “good” should be assigned a higher probability than word “bad”, thus the predicted intensity value g(“good”, s4 ) is closer to the target sentiment intensity than g(“bad”, s4 ). To favor words whose sentiment intensity is near vy , we introduce a Gaussian kernel layer which places a Gaussian distribution centered around vy , inspired by Luong et al. (2015) and Zhang et al. (2018a). Specifically, the sentiment probability is formulated as: 2 ! g(Es , st ) − vy 1 s ot = √ exp − (4) 2σ 2 2πσ pst = softmax(ost ) (5) where σ is the standard deviation. To balance both sentiment transformation and content preservation, the final probability distribution pt over the entire vocabulary is defined as a mixture of two probability distributions: pt = γpst + (1 − γ)pct (6) where γ is the hyper-parameter that controls the trade-off between two generation probabilities. 2015 &&apos; Encoder Algorithm 1 The cycle reinforcement learning algorithm for training Seq2Se"
P19-1194,S18-1001,0,0.0359134,"Missing"
P19-1194,E17-1096,0,0.0651517,"Missing"
P19-1194,P02-1040,0,0.107261,"2 2.64 2.54 2.37 2.52 3.84 3.85 2.13 2.14 3.41 2.43 2.84 3.21 Seq2SentiSeq 32.5 10.3 0.13 0.78 35.1 3.62 4.09 4.17 3.96 Human Reference 100.0 100.0 0.07 0.83 31.2 4.51 4.36 4.75 4.54 Table 1: Automatic evaluation and human evaluation in three aspects: Content (BLUE-1, BLUE-2), Sentiment (MAE, MRRR) and Fluency (PPL). Avg shows the average human scores. ↑ denotes larger is better, and vice versa. Bold denotes the best results. review in the test dataset, crowd-workers are required to write five references with sentiment intensity value from V 0 = [0.1, 0.3, 0.5, 0.7, 0.9]. Therefore, the BLEU (Papineni et al., 2002) score between the human reference and the corresponding generated text of the same sentiment intensity can evaluate the content preservation performance. Fluency: To measure the fluency, we calculate the perplexity (PPL) of each generated sequence via a pre-trained bi-directional LSTM language model (Mousa and Schuller, 2017). Sentiment: In order to measure how close the sentiment intensity of outputs to the target intensity values, we define three metrics. Given an input sentence x and a list of target intensity values V = [v1 , v2 , ..., vN ], the corresponding outputs of the model are [yˆ1"
P19-1194,P18-2031,0,0.0321807,"results show that our approach can outperform existing methods by a large margin in both automatic evaluation and human evaluation. Our code and data, including outputs of all baselines and our model are available at https://github.com/luofuli/ Fine-grained-Sentiment-Transfer. 1 1 Target Sentiment Text sentiment transfer aims to rephrase the input to satisfy a given sentiment label (value) while preserving its original semantic content. It facilitates various NLP applications, such as automatically converting the attitude of review and fighting against offensive language in social media (dos Santos et al., 2018). Previous work (Shen et al., 2017; Li et al., 2018; Luo et al., 2019) on text sentiment transfer mainly focuses on the coarse-grained level: the reversal of Joint work between WeChat AI and Peking University. 0.1 Horrible food and terrible service! 0.3 Plain food, slow service. 0.5 Food and service need improvement. 0.7 Good food and service. 0.9 Amazing food and perfect service!! Figure 1: An example of the input and output of the fine-grained text sentiment transfer task. The output reviews describe the same content (e.g. food/service) as the input while expressing different sentiment inten"
P19-1194,P18-1080,0,0.0258986,"tly different. In the semantic embedding space, most of the positive words and negative words lie closely. On the contrary, in the sentiment embedding space, positive words are far from negative words. In conclusion, neighbors on semantic embedding space are semantically related, while neighbors on sentiment embedding space express a similar sentiment intensity. Related Work Recently, there is a growing literature on the task of unsupervised sentiment transfer. This task aims to reverse the sentiment polarity of a sentence but keep its content unchanged without parallel data (Fu et al., 2018; Tsvetkov et al., 2018; Li et al., 2018; Xu et al., 2018; Lample et al., 2019). However, there are few researches focus on the fine-grained control of sentiment. Liao et al. (2018) exploits pseudo-parallel data via heuristic rules, thus turns this task to a supervised setting. They then propose a model based on Variational Autoencoder (VAE) to first disentangle the content factor and source sentiment factor, and then combine the content with target sentiment factor. However, the quality of the pseudo-parallel data is not quite satisfactory, which seriously affects the performance of the VAE model. Different from th"
P19-1194,P18-1090,1,0.78267,"e intensity is from interval [0, 1] with a step of 0.05 to guide the model training (Step 6 in Algorithm 1). 3.2 Experiment Settings We tune hyper-parameters on the validation set. The size of vocabulary is set to 10K. Both the semantic and sentiment embeddings are 300dimensional and are learned from scratch. We 5 implement both encoder and decoder as a 1-layer LSTM with a hidden size of 256, and the former is bidirectional. The batch size is 64. We pre-train our model for 10 epochs with the MLE loss using pseudo-parallel sentences conducted by Jaccard Similarity, which is same as Liao et al. (2018). Harmonic weight β in Eq. 9 is 1 and γ in Eq. 6 is 0.5. The standard deviation σ is set to 0.01 for yielding suitable peaked distributions. The sample size K in Eq. 11 is set to 16. The optimizer is Adam (Kingma and Ba, 2014) with 10−3 initial learning rate for pre-training and 10−5 for cycleRL training. Dropout (Srivastava et al., 2014) is used to avoid overfitting. 3.3 Baselines We compare our proposed method with the following two series of state-of-the-art systems. Fine-grained systems aim to modify an input sentence to satisfy a given sentiment intensity. Liao et al. (2018) construct pse"
P19-1194,P18-1102,0,0.162234,"t intensity2 , while keeping the semantic content unchanged. Taking Figure 1 as an example, given the same input and five sentiment intensity values ranging from 0 (most negative) to 1 (most positive), the system generates five different outputs that satisfy the corresponding sentiment intensity in a relative order. There are two main challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. t"
P19-1194,W17-5227,0,0.0201437,"challenges of FTST task. First, it is tough to achieve fine-grained control of the sentiment intensity when generating sentence. Previous work about coarse-grained text sentiment transfer usually uses a separate decoder for each sentiment label (Xu et al., 2018; Zhang et al., 2018b) or embeds each sentiment label into a separate vector (Fu et al., 2018; Li et al., 2018). However, these methods are not feasible for fine-grained text sentiment transfer since the 2 The sentiment intensity is a real-valued score between 0 and 1, following sentiment intensity prediction task in sentiment analysis (Zhang et al., 2017; Mohammad et al., 2018). 2013 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2013–2022 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Input Se Tar Senti 0. 0. 0. 0. 0. target sentiment intensity value is a real value, other than discrete labels. Second, parallel data3 is unavailable in practice. In other words, we can only access the corpora which are labeled with fine-grained sentiment ratings or intensity values. Therefore, in the FTST task, we can not train a generative model via ground truth outpu"
P19-1600,D10-1049,0,0.0349147,"xt belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al."
P19-1600,H05-1042,0,0.206311,", although our model is able to cover much more comprehensive information than the previous models (Table 2 and 3). Some implicitly expressed (like if a person is retired or not) or rarely covered (like ‘spouse’ or ‘high school’) attributes in the source tables might still be ignored in the descriptions generated by our model. Furthermore, those pieces of information which need some form of inference across Related Work Data-to-text a language generation task to generate text for structured data. Table-to-text belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning t"
P19-1600,N06-1046,0,0.14018,"Missing"
P19-1600,E06-1032,0,0.0188045,"er the key points in the infoboxes, we also use information richness (Eq 5) as one of our automatic evaluation. ‘Hit at least 1 word’ for an attribute means that a biography has at least one overlapping word with the words (or their synonyms) in that attribute, which are lemmatized and filtered by a stop-words list like the way we get WB-filter in Sec 4.1. ‘HIT-1 coverage’ for an attribute is the ratio of the instances involving that attribute whose biographies ‘Hit at least 1 word’ in that attribute. Human Evaluation: Since automatic evaluations like BLEU may not be reliable for NLG systems (Callison-Burch et al., 2006; Reiter and Belz, 2009; Reiter, 2018). We use human evaluation which involves the generation fluency, coverage (how much given information in the infobox is mentioned in the related biography) and correctness (how much false or irrelevant information is mentioned in the biography). We firstly sampled 300 generated biographies from the generators for human evaluation. After that, we hired 3 thirdparty crowd-workers who are equipped with sufficient background knowledge to rank the given biographies. We present the generated descriptions to the annotators in a randomized order and ask them to be"
P19-1600,D18-2003,0,0.0145712,"ootball records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work some"
P19-1600,W02-2112,0,0.0655139,"vered (like ‘spouse’ or ‘high school’) attributes in the source tables might still be ignored in the descriptions generated by our model. Furthermore, those pieces of information which need some form of inference across Related Work Data-to-text a language generation task to generate text for structured data. Table-to-text belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Onto"
P19-1600,W13-0108,0,0.032971,"and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey"
P19-1600,D18-1426,0,0.0208961,"led the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descripti"
P19-1600,E17-1060,0,0.201101,"(2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (2018) in the sense that they emphasis easily ignored (usually less frequent) features or bits of information in the training procedure by smoothing or regularization. The greatest difference between our work and theirs is"
P19-1600,P13-2121,0,0.0111481,"r datasets (with or without force-attention module), respectively. We replace UNK tokens with the most relevant token in the source table according to the attention matrix (Jean et al., 2015). 4.4 Table 2: BLEU and ROUGE scores on the WIKIBIO and WB-filter datasets. The baselines with * are based on our implementation while the others are reported by their authors. Models with † are trained using the RL criterion specified in Sec 3.2.2 while the remaining models are trained using the maximum likelihood estimate (MLE). Baselines KN & Template KN: A template-based KneserNey (KN) language model (Heafield et al., 2013) The extracted template for Table 1 is “name 1 name 2 (born birthdate 1 · · · ”. During inference, the decoder is constrained to emit words from the vocabulary or the special tokens in the tables. Table NLM: Lebret et al. (2016) proposed a neural language model Table NLM taking the attribute information into consideration. Order-planning: Sha et al. (2018) proposed a link matrix to model the order for the attributevalue tuples while generating biographies. Struct-aware: Liu et al. (2018) proposed a structure-aware model using a modified LSTM unit and a specific attention mechanism to incorpora"
P19-1600,P82-1020,0,0.797622,"Missing"
P19-1600,N18-2098,0,0.0182192,"e verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al."
P19-1600,P15-1001,0,0.0141721,"a, 2014), respectively. We use Xavier initialization (Glorot and Bengio, 2010) for all the parameters in our model. The global constraint of force-attention (Eq 4) is adapted after 4 and 1.5 epochs of training to avoid hurting the primary loss for the WIKIBIO and WB-filter datasets, respectively. Before the richness-oriented reinforced training, the neural generator is pre-trained 8 and 4 epochs for the WIKIBIO and WB-filter datasets (with or without force-attention module), respectively. We replace UNK tokens with the most relevant token in the source table according to the attention matrix (Jean et al., 2015). 4.4 Table 2: BLEU and ROUGE scores on the WIKIBIO and WB-filter datasets. The baselines with * are based on our implementation while the others are reported by their authors. Models with † are trained using the RL criterion specified in Sec 3.2.2 while the remaining models are trained using the maximum likelihood estimate (MLE). Baselines KN & Template KN: A template-based KneserNey (KN) language model (Heafield et al., 2013) The extracted template for Table 1 is “name 1 name 2 (born birthdate 1 · · · ”. During inference, the decoder is constrained to emit words from the vocabulary or the sp"
P19-1600,P18-1154,0,0.0122378,"its of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al"
P19-1600,N18-2101,0,0.101198,"nd Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate"
P19-1600,D16-1032,0,0.0320047,"et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (2018) in the sense that they emphasis easily ignored (usually less frequent) features or bits of information in the training procedure by smoothing or regularization. The greatest difference between our work and theirs is that our method is tailored for covering the key information embedded in the attributes (entries) of the key-value tables rather than single words or labels. Although the deficient score of Tu et al. (2016) in Table 2 has demonstrated that word-level coverage oriented methods may not still be suitable to the structured tabl"
P19-1600,D16-1128,0,0.0981335,"Missing"
P19-1600,C18-1089,0,0.0165885,"ey Attri.: A soccer player who plays as forward Groundless info: A Utah forward in the national team Less Informative: An American forward Table 1: An example for comprehensive generation. Suppose we only have two attribute-value tuples, the underlined content is groundless information not mentioned in source tables. Introduction Generating descriptions for the factual attributevalue tables has attracted widely interests among NLP researchers especially in a neural end-to-end fashion (e.g. Lebret et al. (2016); Liu et al. (2018); Sha et al. (2018); Bao et al. (2018); Puduppully et al. (2018); Li and Wan (2018); Nema et al. (2018)) as shown in Fig 1a. For broader potential applications in this field, we also simulate useroriented generation, whose goal is to provide comprehensive generation for the selected attributes according to particular user interests like Fig 1b. However, we find that previous models might miss key information and generate less informative and groundless content in its generated descriptions towards source tables. For example, in Table 1, the ‘missing key attribute’ case doesn’t mention where the player comes from (birthplace) while the ‘less informative’ one chooses American"
P19-1600,P09-1011,0,0.138946,"over much more comprehensive information than the previous models (Table 2 and 3). Some implicitly expressed (like if a person is retired or not) or rarely covered (like ‘spouse’ or ‘high school’) attributes in the source tables might still be ignored in the descriptions generated by our model. Furthermore, those pieces of information which need some form of inference across Related Work Data-to-text a language generation task to generate text for structured data. Table-to-text belongs to the data-to-text generation (Reiter and Dale, 2000). Many previous work (Barzilay and Lapata, 2005, 2006; Liang et al., 2009) treated the task as a pipelined systems, which viewed content selection and surface realization as two separate tasks. Duboue and McKeown (2002) proposed a clustering approach in the biography domain by scoring the semantic relevance of the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data a"
P19-1600,N19-1263,0,0.0153354,"al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (20"
P19-1600,D17-1189,1,0.842031,"et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al"
P19-1600,W16-6624,0,0.0268522,"the text and paired knowledge base. In a similar vein, Barzilay and Lapata (2005) modeled the dependencies between the American football records and identified the bits of information to be verbalized. Liang et al. (2009); Angeli et al. (2010) extended the work of Barzilay and Lapata (2005) to soccer and weather domains by learning the alignment between data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repeti"
P19-1600,N16-1086,0,0.0618734,"ween data and text using hidden variable models. Androutsopoulos et al. (2013) and Duma and Klein (2013) focused on generating descriptive language for Ontologies and RDF triples. Most recent work utilize neural networks on data-to-text generation (Mahapatra et al., 2016; Wiseman et al., 2017; Laha et al., 2018; Kaffee et al., 2018; Freitag and Roy, 2018; Qader et al., 2018; Dou et al., 2018; Yeh et al., 2018; Jhamtani et al., 2018; Jain et al., 2018; Liu et al., 2017b, 2019; Peng et al., 2019; 5992 Duˇsek et al., 2019). Some closely relevant work also focused on the table-to-text generation. Mei et al. (2016) proposed an encoder-aligner-decoder framework for generating weather broadcast. Hachey et al. (2017) used a table-text and text-table autoencoder framework for table-to-text generation. Nema et al. (2018) proposed gated orthogonalization to avoid repetitions. Wiseman et al. (2018) used neural semi-HMM to generate template-like descriptions for structured data. Our work somewhat shares similar goals as Kiddon et al. (2016); Tu et al. (2016); Liu et al. (2017a); Gong et al. (2018) in the sense that they emphasis easily ignored (usually less frequent) features or bits of information in the train"
P19-1600,N18-1139,0,0.545422,"player who plays as forward Groundless info: A Utah forward in the national team Less Informative: An American forward Table 1: An example for comprehensive generation. Suppose we only have two attribute-value tuples, the underlined content is groundless information not mentioned in source tables. Introduction Generating descriptions for the factual attributevalue tables has attracted widely interests among NLP researchers especially in a neural end-to-end fashion (e.g. Lebret et al. (2016); Liu et al. (2018); Sha et al. (2018); Bao et al. (2018); Puduppully et al. (2018); Li and Wan (2018); Nema et al. (2018)) as shown in Fig 1a. For broader potential applications in this field, we also simulate useroriented generation, whose goal is to provide comprehensive generation for the selected attributes according to particular user interests like Fig 1b. However, we find that previous models might miss key information and generate less informative and groundless content in its generated descriptions towards source tables. For example, in Table 1, the ‘missing key attribute’ case doesn’t mention where the player comes from (birthplace) while the ‘less informative’ one chooses American rather than Utah. Th"
P19-1600,P02-1040,0,0.105184,"Missing"
P19-1600,N18-1137,0,0.347035,"ayer) Comprehensive: A Utah soccer player who plays as forward Missing Key Attri.: A soccer player who plays as forward Groundless info: A Utah forward in the national team Less Informative: An American forward Table 1: An example for comprehensive generation. Suppose we only have two attribute-value tuples, the underlined content is groundless information not mentioned in source tables. Introduction Generating descriptions for the factual attributevalue tables has attracted widely interests among NLP researchers especially in a neural end-to-end fashion (e.g. Lebret et al. (2016); Liu et al. (2018); Sha et al. (2018); Bao et al. (2018); Puduppully et al. (2018); Li and Wan (2018); Nema et al. (2018)) as shown in Fig 1a. For broader potential applications in this field, we also simulate useroriented generation, whose goal is to provide comprehensive generation for the selected attributes according to particular user interests like Fig 1b. However, we find that previous models might miss key information and generate less informative and groundless content in its generated descriptions towards source tables. For example, in Table 1, the ‘missing key attribute’ case doesn’t mention where th"
P19-1600,W18-6532,0,0.0313192,"Missing"
P19-1600,J18-3002,0,0.0261587,"rmation richness (Eq 5) as one of our automatic evaluation. ‘Hit at least 1 word’ for an attribute means that a biography has at least one overlapping word with the words (or their synonyms) in that attribute, which are lemmatized and filtered by a stop-words list like the way we get WB-filter in Sec 4.1. ‘HIT-1 coverage’ for an attribute is the ratio of the instances involving that attribute whose biographies ‘Hit at least 1 word’ in that attribute. Human Evaluation: Since automatic evaluations like BLEU may not be reliable for NLG systems (Callison-Burch et al., 2006; Reiter and Belz, 2009; Reiter, 2018). We use human evaluation which involves the generation fluency, coverage (how much given information in the infobox is mentioned in the related biography) and correctness (how much false or irrelevant information is mentioned in the biography). We firstly sampled 300 generated biographies from the generators for human evaluation. After that, we hired 3 thirdparty crowd-workers who are equipped with sufficient background knowledge to rank the given biographies. We present the generated descriptions to the annotators in a randomized order and ask them to be objective and not to guess which syst"
P19-1600,J09-4008,0,0.0604034,"boxes, we also use information richness (Eq 5) as one of our automatic evaluation. ‘Hit at least 1 word’ for an attribute means that a biography has at least one overlapping word with the words (or their synonyms) in that attribute, which are lemmatized and filtered by a stop-words list like the way we get WB-filter in Sec 4.1. ‘HIT-1 coverage’ for an attribute is the ratio of the instances involving that attribute whose biographies ‘Hit at least 1 word’ in that attribute. Human Evaluation: Since automatic evaluations like BLEU may not be reliable for NLG systems (Callison-Burch et al., 2006; Reiter and Belz, 2009; Reiter, 2018). We use human evaluation which involves the generation fluency, coverage (how much given information in the infobox is mentioned in the related biography) and correctness (how much false or irrelevant information is mentioned in the biography). We firstly sampled 300 generated biographies from the generators for human evaluation. After that, we hired 3 thirdparty crowd-workers who are equipped with sufficient background knowledge to rank the given biographies. We present the generated descriptions to the annotators in a randomized order and ask them to be objective and not to g"
P19-1600,P16-1008,0,0.275522,"nted reinforcement learning to produce accurate, informative and loyal descriptions. 3.1 Force-Attention Module For ‘missing key attributes’ problem (Table 1), we find that the generator usually focuses on particular attributes while the other attributes have relatively low attention values in the entire decoding procedure. So force attention method is proposed to guide the decoder to pay more attention to the previous uncovered attributes with low attention values to avoid potential key attribute missing. Note that FA method focuses on attributelevel coverage rather than word-level coverage (Tu et al., 2016) as our goal is to reduce the ‘missing key attributes’ phenomenons instead of building rigid word-by-word alignment between tables and descriptions. Stepwise Forcing Attention: We define attributeP level attention βtak = avg( xi ∈ak αti ) at the t-th step for attribute ak as the average value of the word-level attention values for the words in that attribute. The word-level coverage is defined as the sum of attention vector before the t-th step i θti = θt−1 + αti (Tu et al., 2016). In the similar way, we define the attribute-level coverage ak γtak = γt−k + βtak as the overall attention for att"
P19-1600,D17-1239,0,0.0943602,"Missing"
P19-1600,D18-1356,0,0.0863405,"Missing"
P19-1603,S18-1032,0,0.0289074,"Missing"
P19-1603,N18-2008,0,0.0522299,"ces a policy gradient learning approach to ensure that the model ends with a specific type of event given in advance. Yao et al. (2018b) uses manually annotated story data to control the ending valence and storyline of story generation. Different from them, our proposed framework can acquire distant sentiment labels without the dependence on the human annotations. Sentimental Text Generation Generating sentimental and emotional texts is a key step towards building intelligent and controllable natural language generation systems. To date several works of dialogue generation (Zhou et al., 2018; Huang et al., 2018; Zhou and Wang, 2018) and text sentiment transfer task (Li et al.; Luo et al., 2019) have studied on generating emotional or sentimental text. They always pre-define a binary sentiment label (positive/negative) or a small limited set of emotions, such as “anger”, “love”. Different from them, controlling the fine-grained sentiment (a numeric value) for story ending generation is not limited to several emotional labels, thus we can not embed each sentiment label into a separate vector as usual. Therefore, we propose to introduce the numeric sentiment value via a Gaussian Kernel Layer. 6 Conclus"
P19-1603,N18-1169,0,0.078025,"control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at htt"
P19-1603,C18-1088,0,0.0894506,"control the sentiment of the output. To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at htt"
P19-1603,D18-1420,0,0.0224517,"oid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding into a real value, the target sentiment intensity s is the mean of the Gaussian distribution, WU and bU are trainable parameters. 3.2 Baselines Since there is no direct related work of this task, we design an intuitive pipeline (generate-andmodify) as baseline. It first generates a story ending using a general sequence-to-sequence model with attention (Luong et al., 2015), and then modifies the sentiment of the story ending towards the target sentiment intensity via a fine-grained sentiment modification method (Liao et al., 2018). We call this baseline Seq2Seq + SentiMod. 3.3 Experiment Settings We tune hyper-parameters on the validation set. For the RM and DA sentiment analyzer, we implement the encoder as a 3-layer bidirectional LSTM with a hidden size of 512. We implement the regression module as a MLP with 1 hidden layer of size 32. For domain adaption, we implement a domain discriminator as a MLP with 1 hidden layer of size 32. A Gradient Reversal Layer is added into the domain discriminator. For the sentimental generator, both the semantic and sentiment embeddings are 256 dimensions and randomly initialized. We"
P19-1603,D15-1166,0,0.0213571,"th the target sentiment intensity s. As shown in Figure 3, the probability of generating a target word P is composed of two probabilities: P (yt ) = ↵PR (yt ) + PS (yt ) (1) where PR (yt ) denotes the semantic generation probability, PS (yt ) denotes the sentiment generation probability, ↵ and are trainable coefficients. Specifically, PR (yt ) is defined as follow: PR (yt = w) = wT (WR · hyt + bR ), ht = LSTM(yt 1 , h t 1 , ct ) (2) (3) where w is a one-hot indicator vector of word w, WR and bR are trainable parameters, ht is the t-th hidden state of the LSTM decoder with attention mechanism (Luong et al., 2015). PS (yt ) measures the generation probability of the target word given the target sentiment intensity s. For all words, beyond their semantic embeddings, they also have sentiment embeddings U. The sentiment embeddings of words reflect their sentiment properties. A Gaussian Kernel Layer (Luong et al., 2015; Zhang et al., 2018) is used to encourage words with sentiment intensity near to target sentiment s, and PS (yt ) is defined as follow: ✓ ◆ 1 ( S (Uw) s)2 PS (yt = w) = p exp 2 2 2⇡ (4) S (U, w) Dataset = sigmoid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding"
P19-1603,N16-1098,0,0.0491088,"timent intensities s. It consists of an encoder and a decoder equipped with a Gaussian Kernel Layer. The encoder is to map the input story context x into a compact vector that can capture its essential context features. Specifically, we use a normal bi-directional LSTM as the encoder. All context words xi are represented by their semantic embeddings E as the input and we use the concatenation of final forward and backward hidden states as the initial hidden state of the decoder. 6021 3 Decoder ?? y?? = ?????? ???? + ?????? ???? Experiment 3.1 ℎ??−1 We choose the widely-used ROCStories corpus (Mostafazadeh et al., 2016) which consists of 100k five-sentence stories. We split the data into a training set with 93,126 stories, a validation set with 5,173 stories and a test set with 5,175 stories. Gaussian Kernel Layer ???? Semantic Embeddings Sentiment Embeddings Target Sentiment Intensity Figure 3: The decoder of the sentimental generator. A Gaussian Kernel Layer is introduced to make use of the target sentiment intensity. The decoder aims to generate a story ending which accords with the target sentiment intensity s. As shown in Figure 3, the probability of generating a target word P is composed of two probabi"
P19-1603,P02-1040,0,0.104057,"story endings in the test set (H-M SentiCons). To evaluate the performance of sentimental generator, for each story context in the test set, we generate five story endings with five target sentiment intensity ranging from [0, 1]. Then we calculate SentiCons of input target sentiment intensities and sentiment intensities of the outputs predicted by the best sentiment analyzer (I-O SentiCons). BLEU: For each story in the test set, we take the context x and the human-annotated sentiment intensity s of the gold story ending y as input. The ˆ Then we calculate the corresponding output is y. BLEU (Papineni et al., 2002) score of y and yˆ as the overall quality of the generated story endings. 3.4.2 Human Evaluation We hire two evaluators who are skilled in English to evaluate the generated story endings. For each story in the test set, we distribute the story context, five target sentiment intensities and corresponding generated story endings to the evaluators. Evaluators are required to score the generated endings from 1 to 5 in terms of three criteria: Coherency, Fluency and Sentiment. Coherency measures whether the endings are coherent with the context. Fluency measures whether the endings are fluent. Sent"
P19-1603,D13-1170,0,0.00606287,"detailed configurations in each module. 2.2 Sentiment Analyzer The sentiment analyzer S aims to predicting the sentiment intensity s of the gold story ending y to construct paired data (x, s; y). As the first attempt to solve the proposed task, we explore three kinds of sentiment analyzers as follows. Rule-based (RB): VADER (Hutto and Gilbert, 2014) is an rule-based unsupervised model for sentiment analysis. We use it to extract the sentiment intensity s of y and then scale s to [0, 1]. Regression Model (RM): We first train a linear regression model R on the Stanford Sentiment Treebank (SST) (Socher et al., 2013) dataset, which is widely-used for sentiment analysis. Then we use R to acquire the sentiment intensity of y. Domain-Adversarial (DA): In the absence of sentiment annotations for the story dataset, domain adaptation can provide an effective solution since there exists some labeled datasets of a similar task but from a different domain. We use adversarial learning (Ganin and Lempitsky, 2015) to extract a domain-independent feature which not only performs well in the SST sentiment regression task but also misleads the domain discriminator. Finally, we use the adapted regression model to acquire"
P19-1603,D18-1462,1,0.934018,"To the best of our knowledge, this is the first endeavor to control the fine-grained sentiment for story ending generation without manually annotating sentiment labels. Experiments show that our proposed framework can generate story endings which are not only more coherent and fluent but also able to meet the given sentiment intensity better.1 1 Target Sentiment Story ending generation aims at completing the plot and concluding a story given a story context. Previous works mainly study on how to generate a coherent, reasonable and diversified story ending (Li et al., 2018; Guan et al., 2018; Xu et al., 2018). However, few of them focus on controllable story ending generation, especially ⇤ 0.1 She still lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at https://github. com/luofuli/sentimental-"
P19-1603,W18-1505,0,0.0845105,"ll lost the game and was very upset. 0.3 She almost won the game, but eventually lost. 0.5 The game ended with a draw. 0.7 She eventually won the game. 0.9 She won the game and was very proud of her team. Figure 1: An example of the input story context and output story endings for this task. All of the story endings are coherent with the story context but express different sentiment intensities. Introduction Equal Contribution. Our code and data can be found at https://github. com/luofuli/sentimental-story-ending 1 Generated Story Endings controlling the sentiment for story ending generation. Yao et al. (2018b) is the only work on controlling the sentiment for story ending generation. However, their work needs manually label the story dataset with sentiment labels (happy, sad, unknown), which is time-consuming and laborintensive. What’s more, they only focus on coarsegrained sentiment. Different from previous work, we propose the task of controlling the sentiment for story ending generation at a fine-grained level, without any human annotation of story dataset2 . Take Figure 1 as an example, given the same story context, our goal is to generate a story ending that satisfies the given sentiment int"
P19-1603,P18-1102,0,0.0569065,"fically, PR (yt ) is defined as follow: PR (yt = w) = wT (WR · hyt + bR ), ht = LSTM(yt 1 , h t 1 , ct ) (2) (3) where w is a one-hot indicator vector of word w, WR and bR are trainable parameters, ht is the t-th hidden state of the LSTM decoder with attention mechanism (Luong et al., 2015). PS (yt ) measures the generation probability of the target word given the target sentiment intensity s. For all words, beyond their semantic embeddings, they also have sentiment embeddings U. The sentiment embeddings of words reflect their sentiment properties. A Gaussian Kernel Layer (Luong et al., 2015; Zhang et al., 2018) is used to encourage words with sentiment intensity near to target sentiment s, and PS (yt ) is defined as follow: ✓ ◆ 1 ( S (Uw) s)2 PS (yt = w) = p exp 2 2 2⇡ (4) S (U, w) Dataset = sigmoid(wT (U · WU + bU )) (5) where 2 is the variance, S maps the sentiment embedding into a real value, the target sentiment intensity s is the mean of the Gaussian distribution, WU and bU are trainable parameters. 3.2 Baselines Since there is no direct related work of this task, we design an intuitive pipeline (generate-andmodify) as baseline. It first generates a story ending using a general sequence-to-sequ"
P19-1603,P18-1104,0,0.0337842,"t learning approach to ensure that the model ends with a specific type of event given in advance. Yao et al. (2018b) uses manually annotated story data to control the ending valence and storyline of story generation. Different from them, our proposed framework can acquire distant sentiment labels without the dependence on the human annotations. Sentimental Text Generation Generating sentimental and emotional texts is a key step towards building intelligent and controllable natural language generation systems. To date several works of dialogue generation (Zhou et al., 2018; Huang et al., 2018; Zhou and Wang, 2018) and text sentiment transfer task (Li et al.; Luo et al., 2019) have studied on generating emotional or sentimental text. They always pre-define a binary sentiment label (positive/negative) or a small limited set of emotions, such as “anger”, “love”. Different from them, controlling the fine-grained sentiment (a numeric value) for story ending generation is not limited to several emotional labels, thus we can not embed each sentiment label into a separate vector as usual. Therefore, we propose to introduce the numeric sentiment value via a Gaussian Kernel Layer. 6 Conclusion and Future Work In"
S14-2102,I05-5003,0,0.0520931,"Missing"
S14-2102,W05-0909,0,0.108881,"ons to measure the hypotheses against references. It is a corpus-based and precision-based metric, and uses “brevity penalty” as a replacement for recall. Yet this penalty is meaningless on sentence level. Therefore we considers only the precision factors in BLEU: Fskip2 = skip2(ref, hyo) C(m, 2) skip2(ref, hyo) C(n, 2) (1 + β 2 )Pskip2 Rskip2 Rskip2 + β 2 Pskip2 Where C is combination, and skip2(ref, hyo) is the number of common skip-bigrams. We also set β = 1 here, and call these three indicators ROUGE-S-derived features. 3.4 N gramref ∩ N gramhyo = N gramref METEOR-Derived Features METEOR (Banerjee and Lavie, 2005) evaluates a hypothesis by aligning it to a reference translation and gives sentence-level similarity scores. It uses a generalized concept of unigram mapping that matches words in the following types: exact match on words surface forms , stem match on words stems, synonym match according to the synonym sets in WordNet, and paraphrase match (Denkowski and Lavie, 2010). METEOR also makes distinction between content words and function words. Each type of match mi is weighted by wi , let (mi (hc ), mi (hf )) be the number of content and function words covered by this type in the hypothesis, and W"
S14-2102,S12-1092,0,0.0312846,"Missing"
S14-2102,P02-1040,0,0.0933809,"yntactic level to semantic level. We consider only lexical ones to avoid complicated steps like parsing or semantic role labelling, which are computational expensive and may bring extra noise. But instead of directly using the MT evaluation metrics, we use the factors in them as features, the idea is that the overall score of the original metric is highly related to the length of both of the compared pair, but its factors are often related to the length of just one side yet still carry useful similarity information. 3.1 3.3 Pskip2 = Rskip2 = As the most wildly used MT evaluation metric, BLEU (Papineni et al., 2002) uses the geometric mean of n-gram precisions to measure the hypotheses against references. It is a corpus-based and precision-based metric, and uses “brevity penalty” as a replacement for recall. Yet this penalty is meaningless on sentence level. Therefore we considers only the precision factors in BLEU: Fskip2 = skip2(ref, hyo) C(m, 2) skip2(ref, hyo) C(n, 2) (1 + β 2 )Pskip2 Rskip2 Rskip2 + β 2 Pskip2 Where C is combination, and skip2(ref, hyo) is the number of common skip-bigrams. We also set β = 1 here, and call these three indicators ROUGE-S-derived features. 3.4 N gramref ∩ N gramhyo ="
S14-2102,N10-1031,0,0.253989,"re C is combination, and skip2(ref, hyo) is the number of common skip-bigrams. We also set β = 1 here, and call these three indicators ROUGE-S-derived features. 3.4 N gramref ∩ N gramhyo = N gramref METEOR-Derived Features METEOR (Banerjee and Lavie, 2005) evaluates a hypothesis by aligning it to a reference translation and gives sentence-level similarity scores. It uses a generalized concept of unigram mapping that matches words in the following types: exact match on words surface forms , stem match on words stems, synonym match according to the synonym sets in WordNet, and paraphrase match (Denkowski and Lavie, 2010). METEOR also makes distinction between content words and function words. Each type of match mi is weighted by wi , let (mi (hc ), mi (hf )) be the number of content and function words covered by this type in the hypothesis, and We use the modified n-gram precision here and regard “paragraph” as “reference”, and “sentence” as the “hypothesis”. N = 1,2,3,4. We call these four features BLEU-derived features. 3.2 ROUGE-S-Derived Features ROUGE-S (Lin and Och, 2004) uses skip-bigram co-occurrence statistics for similarity measurement. One advantage of skip-bigram over BLEU is that it does not requ"
S14-2102,W14-3348,0,0.0187047,"n model with the same features as Run1. Run3 is a simple linear regression model, which is free from the “0-trap”, thus we use all the 14 features without smoothing. We use Matlab for regression. The baseline is officially given using LCS. P ·R αP + (1 − α)R To account for word order difference, the fragmentation penalty is calculated using the total number of matched words(m) and the number of chunks1 (ch) in the hypothesis:  P en = γ · ch m β And the final METEOR score is: Score = (1 − P en) · Fmean Parameters α, β, γ, δand wi ...wn are tuned to maximize correlation with human judgements (Denkowski and Lavie, 2014). We use Meteor1.5 system2 for scoring. Parameters are tuned on WMT12, and the paraphrase table is extracted on the WMT data. We use the p, r, f rag(f rag = ch/m) and score as features and call them METEOR-derived features. 4 Run Baseline run1 run2 run3 Data Set 4.4 The SemEval2014 task3 subtask gives a training set of 500 paragraph-sentence pairs, with human annotated continuous score of 0 − 4. These pairs are labelled with genres of “Newswire/ cqa3 / metaphoric/ scientific/ travel/ review”. Systems are asked to predict the similarity scores for 500 pairs in the test set. Performance is evalu"
S14-2102,S12-1059,0,\N,Missing
S14-2102,P04-1077,0,\N,Missing
S14-2102,S14-2003,0,\N,Missing
W02-1801,1994.amta-1.26,0,\N,Missing
W02-1801,H91-1026,0,\N,Missing
W10-4136,Y06-1012,0,0.0459749,"Missing"
W10-4136,I05-3025,0,0.0355425,"Missing"
W13-4413,P00-1032,0,0.140215,"Missing"
W17-4305,W11-2136,0,0.0190009,"o the model in an architecture engineering way. Also, to take dependency relation type into account, we introduce trainable weights for different types of dependency relation. The weights can be trained to indicate importance of a dependency type. SA-LSTM is able to directly model the whole tree structure of dependency relation in an architecture engineering way. Experiments show that Introduction The task of Semantic Role Labeling (SRL) is to recognize arguments of a given predicate in a sentence and assign semantic role labels. Many NLP works such as machine translation (Xiong et al., 2012; Aziz et al., 2011) benefit from SRL because of the semantic structure it provides. Figure 1 shows a sentence with semantic role label. Dependency relation is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005), since it can provide rich structure and syntax information for SRL. At the bottom of Figure 1 shows dependency of the sentence. ∗ P olice ROLE Corresponding Author 27 Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 27–32 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Depen"
W17-4305,D14-1082,0,0.0435427,"where T is the current training pair, x denotes current training sentence, and y is the corresponding correct answer path. yt = k means that the t-th word has the k-th semantic role label. The score of ot is calculated as: s(x, y, θ) = otyt Experiment setting In order to compare with previous Chinese SRL works, we choose to do experiment on CPB 1.0. We also follow the same data setting as previous Chinese SRL work (Xue, 2008; Sun et al., 2009) did. Pre-trained1 word embeddings are tested on SA-LSTM and shows improvement. For English SRL, we test on CoNLL 2005 dataset. We use Stanford Parser (Chen and Manning, 2014) to get dependency relation. The training set of Chinese parser overlaps a part of CPB 1.0 test set, so we retrained the parser. Dimension of hyper parameters are tuned according to development set. n1 = 200, nh = 100, n2 = 200, n3 = 100, learning rate = 0.001. Optimization Ni X Experiment 3.2 Syntax Aware LSTM Performance To prove that SA-LSTM models dependency relation better than simple feature engineering (11) 1 t=1 Trained by word2vec on Chinese Gigaword Corpus All experiment code and related files are available on request 3 We test the model on CPB 1.0 2 where Ni is the word number of th"
W17-4305,P06-2013,0,0.0387213,"1.0. Both “bi-LSTM + feature engineering dependency” and Path-LSTM only model dependency parsing information for each single word, which can not model the whole dependency tree struc4 Related works Semantic role labeling (SRL) was first defined by (Gildea and Jurafsky, 2002). Early works (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL got promising result without large annotated SRL corpus. Xue and Palmer (2003) built the Chinese Proposition Bank to standardize Chinese SRL research. Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) 1 We add syntax-aware connections to every bi-LSTM layer in the 8-layer model of (He et al., 2017) 30 the National Basic Research Program (973 Program No. 2014CB340405). use feature engineering methods. Their methods can take dependency relation into account in feature engineering way, such as syntactic path feature. It is obvious that feature engineering method can not fully capture the tree structure of dependency relation. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a Convolutional Neural Network (CNN) method for SR"
W17-4305,D08-1034,1,0.857298,"Missing"
W17-4305,J02-3001,0,0.68195,"Missing"
W17-4305,P17-1044,0,0.421699,"-art model. 1 now investigate A0 AM –T M P REL IBOES S–A0 DEP EN DEN CY P ARSIN G S–AM –T M P REL advmod nsubj cause accident A1 B–A1 E–A1 compound–nn ROOT dobj Figure 1: A sentence from Chinese Proposition Bank 1.0 (CPB 1.0) (Xue and Palmer, 2003) with semantic role labels and dependency. Traditional methods (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) do classification according to manually designed features. Feature engineering requires expertise and is labor intensive. Recent works based on Recurrent Neural Network (RNN) (Zhou and Xu, 2015; Wang et al., 2015; He et al., 2017) extract features automatically, and significantly outperform traditional methods. However, because RNN methods treat language as sequential data, they fail to integrate the tree structured dependency into RNN. We propose Syntax Aware Long Short Time Memory (SA-LSTM) to directly model complex tree structure of dependency relation in an architecture engineering way. Architecture of SALSTM is shown in Figure 2. SA-LSTM is based on bidirectional LSTM (bi-LSTM). In order to model the whole dependency tree, we add additional directed connections between dependency related words in bi-LSTM. SA-LSTM"
W17-4305,W05-0634,0,0.0391418,"a dependency type. SA-LSTM is able to directly model the whole tree structure of dependency relation in an architecture engineering way. Experiments show that Introduction The task of Semantic Role Labeling (SRL) is to recognize arguments of a given predicate in a sentence and assign semantic role labels. Many NLP works such as machine translation (Xiong et al., 2012; Aziz et al., 2011) benefit from SRL because of the semantic structure it provides. Figure 1 shows a sentence with semantic role label. Dependency relation is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005), since it can provide rich structure and syntax information for SRL. At the bottom of Figure 1 shows dependency of the sentence. ∗ P olice ROLE Corresponding Author 27 Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 27–32 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Dependency T ree Structure SA-LSTM can model dependency relation better than traditional feature engineering way. SALSTM gives state of the art F1 on CPB 1.0 and also shows improvement on English CoNLL 2005 dataset. 2 ROOT nsubj dobj com"
W17-4305,J08-2005,0,0.0937417,"o indicate importance of a dependency type. SA-LSTM is able to directly model the whole tree structure of dependency relation in an architecture engineering way. Experiments show that Introduction The task of Semantic Role Labeling (SRL) is to recognize arguments of a given predicate in a sentence and assign semantic role labels. Many NLP works such as machine translation (Xiong et al., 2012; Aziz et al., 2011) benefit from SRL because of the semantic structure it provides. Figure 1 shows a sentence with semantic role label. Dependency relation is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005), since it can provide rich structure and syntax information for SRL. At the bottom of Figure 1 shows dependency of the sentence. ∗ P olice ROLE Corresponding Author 27 Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 27–32 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Dependency T ree Structure SA-LSTM can model dependency relation better than traditional feature engineering way. SALSTM gives state of the art F1 on CPB 1.0 and also shows improvement on English CoNLL 2005 dataset"
W17-4305,D16-1212,1,0.79247,"Missing"
W17-4305,N04-1032,0,0.0653133,"Missing"
W17-4305,P10-2031,0,0.0457617,"Missing"
W17-4305,D09-1153,0,0.107113,"r SA-LSTM. We use maximum likelihood criterion to train SA-LSTM. We choose stochastic gradient descent algorithm to optimize parameters. Given a training pair T = (x, y) where T is the current training pair, x denotes current training sentence, and y is the corresponding correct answer path. yt = k means that the t-th word has the k-th semantic role label. The score of ot is calculated as: s(x, y, θ) = otyt Experiment setting In order to compare with previous Chinese SRL works, we choose to do experiment on CPB 1.0. We also follow the same data setting as previous Chinese SRL work (Xue, 2008; Sun et al., 2009) did. Pre-trained1 word embeddings are tested on SA-LSTM and shows improvement. For English SRL, we test on CoNLL 2005 dataset. We use Stanford Parser (Chen and Manning, 2014) to get dependency relation. The training set of Chinese parser overlaps a part of CPB 1.0 test set, so we retrained the parser. Dimension of hyper parameters are tuned according to development set. n1 = 200, nh = 100, n2 = 200, n3 = 100, learning rate = 0.001. Optimization Ni X Experiment 3.2 Syntax Aware LSTM Performance To prove that SA-LSTM models dependency relation better than simple feature engineering (11) 1 t=1 T"
W17-4305,D15-1186,1,0.759755,"to the stateof-the-art model. 1 now investigate A0 AM –T M P REL IBOES S–A0 DEP EN DEN CY P ARSIN G S–AM –T M P REL advmod nsubj cause accident A1 B–A1 E–A1 compound–nn ROOT dobj Figure 1: A sentence from Chinese Proposition Bank 1.0 (CPB 1.0) (Xue and Palmer, 2003) with semantic role labels and dependency. Traditional methods (Sun and Jurafsky, 2004; Xue, 2008; Ding and Chang, 2008, 2009; Sun, 2010) do classification according to manually designed features. Feature engineering requires expertise and is labor intensive. Recent works based on Recurrent Neural Network (RNN) (Zhou and Xu, 2015; Wang et al., 2015; He et al., 2017) extract features automatically, and significantly outperform traditional methods. However, because RNN methods treat language as sequential data, they fail to integrate the tree structured dependency into RNN. We propose Syntax Aware Long Short Time Memory (SA-LSTM) to directly model complex tree structure of dependency relation in an architecture engineering way. Architecture of SALSTM is shown in Figure 2. SA-LSTM is based on bidirectional LSTM (bi-LSTM). In order to model the whole dependency tree, we add additional directed connections between dependency related words in"
W17-4305,C16-1020,0,0.0404158,"Missing"
W17-4305,P12-1095,0,0.0175771,"cy tree directly into the model in an architecture engineering way. Also, to take dependency relation type into account, we introduce trainable weights for different types of dependency relation. The weights can be trained to indicate importance of a dependency type. SA-LSTM is able to directly model the whole tree structure of dependency relation in an architecture engineering way. Experiments show that Introduction The task of Semantic Role Labeling (SRL) is to recognize arguments of a given predicate in a sentence and assign semantic role labels. Many NLP works such as machine translation (Xiong et al., 2012; Aziz et al., 2011) benefit from SRL because of the semantic structure it provides. Figure 1 shows a sentence with semantic role label. Dependency relation is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005), since it can provide rich structure and syntax information for SRL. At the bottom of Figure 1 shows dependency of the sentence. ∗ P olice ROLE Corresponding Author 27 Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 27–32 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computation"
W17-4305,J08-2004,0,0.522541,"e trained to indicate importance of a dependency type. SA-LSTM is able to directly model the whole tree structure of dependency relation in an architecture engineering way. Experiments show that Introduction The task of Semantic Role Labeling (SRL) is to recognize arguments of a given predicate in a sentence and assign semantic role labels. Many NLP works such as machine translation (Xiong et al., 2012; Aziz et al., 2011) benefit from SRL because of the semantic structure it provides. Figure 1 shows a sentence with semantic role label. Dependency relation is considered important for SRL task (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005), since it can provide rich structure and syntax information for SRL. At the bottom of Figure 1 shows dependency of the sentence. ∗ P olice ROLE Corresponding Author 27 Proceedings of the 2nd Workshop on Structured Prediction for Natural Language Processing, pages 27–32 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Dependency T ree Structure SA-LSTM can model dependency relation better than traditional feature engineering way. SALSTM gives state of the art F1 on CPB 1.0 and also shows improvement on E"
W17-4305,W03-1707,0,0.0888156,"n predicate and argument for each word using LSTM, then does classification according to such path embedding and some other features. SA-LSTM (79.81%F1 ) outperforms Path-LSTM (79.01%F1 ) on CPB 1.0. Both “bi-LSTM + feature engineering dependency” and Path-LSTM only model dependency parsing information for each single word, which can not model the whole dependency tree struc4 Related works Semantic role labeling (SRL) was first defined by (Gildea and Jurafsky, 2002). Early works (Gildea and Jurafsky, 2002; Sun and Jurafsky, 2004) on SRL got promising result without large annotated SRL corpus. Xue and Palmer (2003) built the Chinese Proposition Bank to standardize Chinese SRL research. Traditional works such as (Xue and Palmer, 2005; Xue, 2008; Ding and Chang, 2009; Sun et al., 2009; Chen et al., 2006; Yang et al., 2014) 1 We add syntax-aware connections to every bi-LSTM layer in the 8-layer model of (He et al., 2017) 30 the National Basic Research Program (973 Program No. 2014CB340405). use feature engineering methods. Their methods can take dependency relation into account in feature engineering way, such as syntactic path feature. It is obvious that feature engineering method can not fully capture th"
W17-4305,D14-1041,0,0.030224,"Missing"
W17-4305,P15-1109,0,0.122343,"ang et al., 2014) 1 We add syntax-aware connections to every bi-LSTM layer in the 8-layer model of (He et al., 2017) 30 the National Basic Research Program (973 Program No. 2014CB340405). use feature engineering methods. Their methods can take dependency relation into account in feature engineering way, such as syntactic path feature. It is obvious that feature engineering method can not fully capture the tree structure of dependency relation. More recent SRL works often use neural network based methods. Collobert and Weston (2008) proposed a Convolutional Neural Network (CNN) method for SRL. Zhou and Xu (2015) proposed bidirectional RNN-LSTM method for English SRL, and Wang et al. (2015) proposed a biRNN-LSTM method for Chinese SRL on which SA-LSTM is based. He et al. (2017) further extends the work of Zhou and Xu (2015). NN based methods extract features automatically and significantly outperforms traditional methods. However, most NN based methods can not utilize dependency relation which is considered important for semantic related NLP tasks (Xue, 2008; Punyakanok et al., 2008; Pradhan et al., 2005). The work of Roth and Lapata (2016) and Sha et al. (2016) have the same motivation as SALSTM, but"
W19-1302,D18-1136,0,0.0526585,"AM (Chen et al., 2017), TRMN (Wang et al., 2018) and IARM (Majumder et al., 2018) introduce deep memory network and multi-hop attention model over sentence-level memories to incorporate target information into sentence representations. Speciﬁcally, TRMN and IARM attach importance to the interaction between targets and contexts, and inter-target relations, which contain the information of relationship between multiple targets in one sentence, respectively. Different from them, our model adopts a novel and effective soft label approach in an intuitive way. There are few works (Xue and Li, 2018; Huang and Carley, 2018) applying CNN, which is considered to be good at text classiﬁcation, on targetlevel sentiment classiﬁcation. GCAE (Xue and Li, 2018) and PG-CNN (Huang and Carley, 2018) are both CNN-based models and adopt gate mechanism to make interaction between target and context tokens. To improve the effectiveness of convolution layers, our model further adopts positional weights, which take relative distance information into account. • Our model uses a soft label approach to evaluating the likelihood of a context word as an opinion word based on the history information. • Our model leverages convolution"
W19-1302,P11-1016,0,0.0571731,"r parts: (1) a Bi-LSTM (Schuster and Paliwal, 1997) layer to get context-aware representations, (2) a convolution based feature extractor, (3) computation of soft labels, and (4) sentiment classiﬁcation using the soft labels and positional weights. We introduce the following notations: s = [w1 , w2 , ..., wn ] denotes a sentence which consists of n words. wi ∈ Rd0 is the embedding of the i-th word. t = [t, t + 1, ..., t + m − 1] denotes the posiEarly methods mainly apply supervised learning approach with large quantities of handcrafted features (Blair-Goldensohn et al., 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014), but ignore context information and deep relations between target and context tokens. Neural network models have achieved high accuracy on this task. AE-LSTM and ATAE-LSTM (Wang et al., 2016) simply concatenate target embeddings to context word embeddings to make connection between targets and contexts. However, both models described above do not obtain target representations based on context-aware information. Inspired by the TNet (Li et al., 2018), 7 Positive Sentiment Classification Max-Pooling Computation of Soft Labels SLC Ċ SLC SLC SLC TR TR Average-Pooling SL"
W19-1302,D17-1047,0,0.159857,"our approach signiﬁcantly outperforms previous models and gives state-of-the-art results on these datasets. 1 Introduction Target-level sentiment classiﬁcation aims to identify the sentiment polarities towards given targets by analyzing sentence context. For example, in the sentence “The food is good but service is bad.”, there are two targets “food” and “service” mentioned. The sentiment towards “food” and “service” are positive and negative respectively. Neural network models (Tang et al., 2016a; Wang et al., 2016; Tang et al., 2016b; Liu and Zhang, 2017; Ma et al., 2017; Tay et al., 2017; Chen et al., 2017; Huang et al., 2018; Gu et al., 2018) have achieved high accuracy on this task. Most of the neural network models introduce attention mechanism to ﬁnd the correlation between target and context tokens. However, the combination of word-level features computed by attention weights may introduce noise into model. For instance, in “The dish tastes bad but its vegetable is delicious though it looks ugly.”, these attentionbased models tend to highlight some involve some other words such as “bad” and “ugly”. Instead of using the attention mechanism, we propose a soft label approach for the target-le"
W19-1302,C18-1066,0,0.0133332,"previous models and gives state-of-the-art results on these datasets. 1 Introduction Target-level sentiment classiﬁcation aims to identify the sentiment polarities towards given targets by analyzing sentence context. For example, in the sentence “The food is good but service is bad.”, there are two targets “food” and “service” mentioned. The sentiment towards “food” and “service” are positive and negative respectively. Neural network models (Tang et al., 2016a; Wang et al., 2016; Tang et al., 2016b; Liu and Zhang, 2017; Ma et al., 2017; Tay et al., 2017; Chen et al., 2017; Huang et al., 2018; Gu et al., 2018) have achieved high accuracy on this task. Most of the neural network models introduce attention mechanism to ﬁnd the correlation between target and context tokens. However, the combination of word-level features computed by attention weights may introduce noise into model. For instance, in “The dish tastes bad but its vegetable is delicious though it looks ugly.”, these attentionbased models tend to highlight some involve some other words such as “bad” and “ugly”. Instead of using the attention mechanism, we propose a soft label approach for the target-level 6 Proceedings of the 10th Workshop"
W19-1302,D14-1181,0,0.00299851,"herefore, we implement an LSTM-based (Hochreiter and Schmidhuber, 1997) soft labeling model by a history-based approach, which utilizes history information (previous soft labels and cell states) together with representation of the current word, to decide how to pay attention to history information or current word representation based on their correlation with target representation. Moreover, since the convolution layer (LeCun et al., 1989) does better in capturing local active features than other neural networks do and these extracted features are proved to be beneﬁcial to text classiﬁcation (Kim, 2014; Johnson and Zhang, 2015), we apply a convolution based encoder to extract these features. The distance of the features to target is also essential as texts may be long and contain several targets. The closer tokens In this paper, we propose a soft label approach to target-level sentiment classiﬁcation task, in which a history-based soft labeling model is proposed to measure the possibility of a context word as an opinion word. We also apply a convolution layer to extract local active features, and introduce positional weights to take relative distance information into consideration. In addit"
W19-1302,S14-2076,0,0.0208967,"TM (Schuster and Paliwal, 1997) layer to get context-aware representations, (2) a convolution based feature extractor, (3) computation of soft labels, and (4) sentiment classiﬁcation using the soft labels and positional weights. We introduce the following notations: s = [w1 , w2 , ..., wn ] denotes a sentence which consists of n words. wi ∈ Rd0 is the embedding of the i-th word. t = [t, t + 1, ..., t + m − 1] denotes the posiEarly methods mainly apply supervised learning approach with large quantities of handcrafted features (Blair-Goldensohn et al., 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014), but ignore context information and deep relations between target and context tokens. Neural network models have achieved high accuracy on this task. AE-LSTM and ATAE-LSTM (Wang et al., 2016) simply concatenate target embeddings to context word embeddings to make connection between targets and contexts. However, both models described above do not obtain target representations based on context-aware information. Inspired by the TNet (Li et al., 2018), 7 Positive Sentiment Classification Max-Pooling Computation of Soft Labels SLC Ċ SLC SLC SLC TR TR Average-Pooling SLC + TR G + G CR Convolution"
W19-1302,D16-1021,0,0.200284,"n target and context tokens. We conduct experiments on SemEval 2014 datasets and the experimental results show that our approach signiﬁcantly outperforms previous models and gives state-of-the-art results on these datasets. 1 Introduction Target-level sentiment classiﬁcation aims to identify the sentiment polarities towards given targets by analyzing sentence context. For example, in the sentence “The food is good but service is bad.”, there are two targets “food” and “service” mentioned. The sentiment towards “food” and “service” are positive and negative respectively. Neural network models (Tang et al., 2016a; Wang et al., 2016; Tang et al., 2016b; Liu and Zhang, 2017; Ma et al., 2017; Tay et al., 2017; Chen et al., 2017; Huang et al., 2018; Gu et al., 2018) have achieved high accuracy on this task. Most of the neural network models introduce attention mechanism to ﬁnd the correlation between target and context tokens. However, the combination of word-level features computed by attention weights may introduce noise into model. For instance, in “The dish tastes bad but its vegetable is delicious though it looks ugly.”, these attentionbased models tend to highlight some involve some other words suc"
W19-1302,P18-1087,0,0.0482824,"sed learning approach with large quantities of handcrafted features (Blair-Goldensohn et al., 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014), but ignore context information and deep relations between target and context tokens. Neural network models have achieved high accuracy on this task. AE-LSTM and ATAE-LSTM (Wang et al., 2016) simply concatenate target embeddings to context word embeddings to make connection between targets and contexts. However, both models described above do not obtain target representations based on context-aware information. Inspired by the TNet (Li et al., 2018), 7 Positive Sentiment Classification Max-Pooling Computation of Soft Labels SLC Ċ SLC SLC SLC TR TR Average-Pooling SLC + TR G + G CR Convolution Based Feature Extractor CR Context-Aware Representations Ċ Delicious Ċ Sake and CR Ikura roll well Figure 1: Overall architecture of the proposed method. We use the sentence “Delicious and good-looking Sake Ikura roll, and sashimi tastes good as well.” as an example. The term “SLC” indicates soft label computation. “TR” indicates target representation and “CR” represents context representation. tion of the target tokens, where t ≥ 1, t + m − 1 ≤ n."
W19-1302,P18-1088,0,0.0564998,"s deep representations for targets, we propose a model which could strengthen the interaction between target and context tokens. Recently, most of the previous state-of-the-art models leverage attention mechanism to evaluate the correlation between the tokens in one sentence. IAN (Ma et al., 2017) adopts two separate LSTM layers and an interactive attention mechanism. Hazarika et al. (2018) classiﬁes the sentiment polarities of all the targets in one sentence simultaneously with attention mechanism to model inter-target dependencies. MemNet (Tang et al., 2016b), RAM (Chen et al., 2017), TRMN (Wang et al., 2018) and IARM (Majumder et al., 2018) introduce deep memory network and multi-hop attention model over sentence-level memories to incorporate target information into sentence representations. Speciﬁcally, TRMN and IARM attach importance to the interaction between targets and contexts, and inter-target relations, which contain the information of relationship between multiple targets in one sentence, respectively. Different from them, our model adopts a novel and effective soft label approach in an intuitive way. There are few works (Xue and Li, 2018; Huang and Carley, 2018) applying CNN, which is c"
W19-1302,E17-2091,0,0.0183646,"val 2014 datasets and the experimental results show that our approach signiﬁcantly outperforms previous models and gives state-of-the-art results on these datasets. 1 Introduction Target-level sentiment classiﬁcation aims to identify the sentiment polarities towards given targets by analyzing sentence context. For example, in the sentence “The food is good but service is bad.”, there are two targets “food” and “service” mentioned. The sentiment towards “food” and “service” are positive and negative respectively. Neural network models (Tang et al., 2016a; Wang et al., 2016; Tang et al., 2016b; Liu and Zhang, 2017; Ma et al., 2017; Tay et al., 2017; Chen et al., 2017; Huang et al., 2018; Gu et al., 2018) have achieved high accuracy on this task. Most of the neural network models introduce attention mechanism to ﬁnd the correlation between target and context tokens. However, the combination of word-level features computed by attention weights may introduce noise into model. For instance, in “The dish tastes bad but its vegetable is delicious though it looks ugly.”, these attentionbased models tend to highlight some involve some other words such as “bad” and “ugly”. Instead of using the attention mechani"
W19-1302,D16-1058,0,0.523566,"tokens. We conduct experiments on SemEval 2014 datasets and the experimental results show that our approach signiﬁcantly outperforms previous models and gives state-of-the-art results on these datasets. 1 Introduction Target-level sentiment classiﬁcation aims to identify the sentiment polarities towards given targets by analyzing sentence context. For example, in the sentence “The food is good but service is bad.”, there are two targets “food” and “service” mentioned. The sentiment towards “food” and “service” are positive and negative respectively. Neural network models (Tang et al., 2016a; Wang et al., 2016; Tang et al., 2016b; Liu and Zhang, 2017; Ma et al., 2017; Tay et al., 2017; Chen et al., 2017; Huang et al., 2018; Gu et al., 2018) have achieved high accuracy on this task. Most of the neural network models introduce attention mechanism to ﬁnd the correlation between target and context tokens. However, the combination of word-level features computed by attention weights may introduce noise into model. For instance, in “The dish tastes bad but its vegetable is delicious though it looks ugly.”, these attentionbased models tend to highlight some involve some other words such as “bad” and “ugly"
W19-1302,P18-1234,0,0.0588645,"et al., 2016b), RAM (Chen et al., 2017), TRMN (Wang et al., 2018) and IARM (Majumder et al., 2018) introduce deep memory network and multi-hop attention model over sentence-level memories to incorporate target information into sentence representations. Speciﬁcally, TRMN and IARM attach importance to the interaction between targets and contexts, and inter-target relations, which contain the information of relationship between multiple targets in one sentence, respectively. Different from them, our model adopts a novel and effective soft label approach in an intuitive way. There are few works (Xue and Li, 2018; Huang and Carley, 2018) applying CNN, which is considered to be good at text classiﬁcation, on targetlevel sentiment classiﬁcation. GCAE (Xue and Li, 2018) and PG-CNN (Huang and Carley, 2018) are both CNN-based models and adopt gate mechanism to make interaction between target and context tokens. To improve the effectiveness of convolution layers, our model further adopts positional weights, which take relative distance information into account. • Our model uses a soft label approach to evaluating the likelihood of a context word as an opinion word based on the history information. • Our mod"
W19-1302,P11-1150,0,0.0238041,"divided into four parts: (1) a Bi-LSTM (Schuster and Paliwal, 1997) layer to get context-aware representations, (2) a convolution based feature extractor, (3) computation of soft labels, and (4) sentiment classiﬁcation using the soft labels and positional weights. We introduce the following notations: s = [w1 , w2 , ..., wn ] denotes a sentence which consists of n words. wi ∈ Rd0 is the embedding of the i-th word. t = [t, t + 1, ..., t + m − 1] denotes the posiEarly methods mainly apply supervised learning approach with large quantities of handcrafted features (Blair-Goldensohn et al., 2008; Yu et al., 2011; Jiang et al., 2011; Kiritchenko et al., 2014), but ignore context information and deep relations between target and context tokens. Neural network models have achieved high accuracy on this task. AE-LSTM and ATAE-LSTM (Wang et al., 2016) simply concatenate target embeddings to context word embeddings to make connection between targets and contexts. However, both models described above do not obtain target representations based on context-aware information. Inspired by the TNet (Li et al., 2018), 7 Positive Sentiment Classification Max-Pooling Computation of Soft Labels SLC Ċ SLC SLC SLC TR T"
W19-1302,D14-1162,0,0.0820435,"architecture is similar to MemNet. • IARM (Majumder et al., 2018) also leverages recurrent memory networks with attention mechanism. The memory is built by the sentence representation based on target information as ATAE-LSTM does. In addition, the model conTable 1: Statistics of benchmark datasets. 2014)1 , which contain reviews about laptop and restaurant respectively and are used by previous works. The statistics of two benchmark datasets are shown in Table 1. There are three kinds of sentiment polarity: positive, negative and neutral. In our experiments, we use GloVe.840B.300d embeddings (Pennington et al., 2014)2 as previous works do. Each word embedding has 300 dimensions. Out-of-vocabulary (OOV) words are randomly sampled from the uniform distribution U (−0.02, 0.02). Weight matrices are initialized by sampling from uniform distribution U (−0.1, 0.1). The kernel sizes of convolution based feature extractors s1 , s2 , s3 are 3, 4, 5. Each kernel consists of 128 ﬁlters. The dimension of outputs of LSTM 2d0 and the convolution layer d1 are 400 and 384 respectively. We use Adam optimizer (Kingma and Ba, 2014) with learning rate 0.003. The batch size is set to 128. In order to alleviate overﬁtting, we"
W19-1302,S14-2004,0,0.0830757,"Missing"
W19-1302,C16-1311,0,0.0492581,"Missing"
