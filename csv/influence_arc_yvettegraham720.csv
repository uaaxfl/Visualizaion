2007.mtsummit-ucnlg.2,W05-0909,0,0.00921405,"sfer-based MT systems where the transfer rules have been automatically acquired from parsed sentence-aligned bitext corpora. The method provides a means of quantifying the upper bound imposed on the MT system by the quality of the parsing and generation technologies for the target language. We include experiments to calculate this upper bound for both handcrafted and automatically induced parsing and generation technologies currently in use by transfer-based MT systems. 1 Introduction Automatic methods of evaluation for MT include BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al., 2003), TER (Snover et al., 2006) and dependency-based evaluation (Owczarzak et al., 2007). Each of these evaluation methods gives an overall result for the entire MT system, based on a comparison of the sentence output by the MT system with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation compo"
2007.mtsummit-ucnlg.2,P04-1041,1,0.898456,"Missing"
2007.mtsummit-ucnlg.2,P06-1130,1,0.893763,"Missing"
2007.mtsummit-ucnlg.2,P05-1022,0,0.0417968,"only evaluation as results are not necessarily for the same set of sentences. 4.1 Evaluation of Automatically Induced Resources Using the methodology described in detail in Section 3, we evaluated the history-based statistical generator of Hogan et al. (2007). This generator, an extension of the work presented in (Cahill and van Genabith, 2006), generates sentences from LFG fstructures and achieves state-of-the-art results when tested on input generated from Penn Treebank gold standard trees. In order to generate f-structure inputs in a completely automated fashion for evaluation, we used the Charniak and Johnson (2005) reranking parser to parse the original test sentences into Penn Treebank style trees 4 . The f-structure annotation algorithm of Cahill et al. (2004) was then applied to the parser-generated trees to create a set of f-structures for testing. Section 23 (2416 sentences) Input NIST BLEU From Gold-standard Trees 13.29 0.6680 From Parser Trees 13.01 0.6511 Table 1: Entire Test Set Results on Section 23 of the Penn Treebank for the generator of (Hogan et al., 2007) on input automatically generated from gold standard and from parser generated trees. Section 23 (2416 sentences) Input NIST BLEU Cover"
2007.mtsummit-ucnlg.2,W07-2204,1,0.890256,"Missing"
2007.mtsummit-ucnlg.2,P06-1043,0,0.0223686,"using automatic string comparison metrics such as NIST and BLEU. Testing on previously unseen sections of the Penn Treebank demonstrates to what degree a generator has achieved broad coverage and high accuracy (according to BLEU and NIST scores). However, if we wish to take into account how the generator might fare as 6 Figure 1: Translation of Source Language Sentence to Target Language a component of a machine translation system, this evaluation methodology is unrealistic in two main respects. Firstly, there is the problem of domain adaptation (well documented for parsing, see for example (McClosky et al., 2006; Foster et al., 2007)). Domain adaptation is particularly relevant for the systems of (Langkilde-Geary, 2002; Nakanishi et al., 2005; Cahill and van Genabith, 2006) which are trained on sections of the Wall Street Journal Penn treebank. It is highly likely that were these generators tested in an MT setting, the testing domain would change1 and, as in statistical parsing, testing on a domain which differs to the domain of the training data would lead to a deterioration in generation results. In addition, the inputs to the generators are constructed from gold standard (hence near perfect) trees"
2007.mtsummit-ucnlg.2,W01-1406,0,0.0215501,"pproaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together. In addition, they give no indication of how well th"
2007.mtsummit-ucnlg.2,P98-2139,0,0.0226045,"tence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together. In addition, they giv"
2007.mtsummit-ucnlg.2,W07-0411,1,0.870141,"Missing"
2007.mtsummit-ucnlg.2,P02-1035,0,0.0984101,"would not be the intended use of our evaluation method, the abstract structure that is given to the generator in our evaluation method should only contain information that is possible to be produced by the transfer component. Such information, like the actual surface form and word order in the target language would not be possible to have in the target language structure if it adheres to the transfer-based architecture that we describe in Section 3.1. 8 4 Experimental Results We conducted two sets of experiments to evaluate the quality of hand-crafted LFG parsing and generation technologies (Riezler et al., 2002) and the treebank-induced LFG parsing (Cahill et al., 2004) and generation (Cahill and van Genabith, 2006; Hogan et al., 2007) technologies using our new method. Both the automatically-induced technologies and the hand-crafted technologies are currently being used as part of transfer-based MT systems. Overall evaluation results for one of these MT systems are available and we include these results in Section 4.2 (Riezler and Maxwell, 2006). We evaluate the parsing/generation technologies on the English language components of three different MT bitext test sets: the 1755 English sentences of le"
2007.mtsummit-ucnlg.2,1992.tmi-1.12,0,0.178277,"ystem with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together."
2007.mtsummit-ucnlg.2,N06-1032,0,0.200528,"stical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 results of such a system, the quality of the parsing and generation components should also be tested in isolation to the MT system. However, previous work in the area of transfer-based MT, for example (Furuse and Hitoshi, 1992; Meyers et al., 1998; Menezes and Richardson, 2001; Riezler and Maxwell, 2006), have relied solely on mainstream MT evaluation methods and have not included any breakdown of results for the parsing and generation components of the system. Existing methods of evaluating parsing and generation technologies as stand-alone systems, however, are insufficient for evaluating how well such technologies will perform as part of a transfer-based MT system, as they do not take into account the fact that the MT system relies on the degree to which the parsing and generation technologies perform together. In addition, they give no indication of how well the generation and parsing tec"
2007.mtsummit-ucnlg.2,2006.amta-papers.25,0,0.0133449,"tomatically acquired from parsed sentence-aligned bitext corpora. The method provides a means of quantifying the upper bound imposed on the MT system by the quality of the parsing and generation technologies for the target language. We include experiments to calculate this upper bound for both handcrafted and automatically induced parsing and generation technologies currently in use by transfer-based MT systems. 1 Introduction Automatic methods of evaluation for MT include BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al., 2003), TER (Snover et al., 2006) and dependency-based evaluation (Owczarzak et al., 2007). Each of these evaluation methods gives an overall result for the entire MT system, based on a comparison of the sentence output by the MT system with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understand fully the overall 5 resu"
2007.mtsummit-ucnlg.2,W02-1506,0,0.0621942,"Missing"
2007.mtsummit-ucnlg.2,2003.mtsummit-papers.51,0,0.040552,"transfer rules have been automatically acquired from parsed sentence-aligned bitext corpora. The method provides a means of quantifying the upper bound imposed on the MT system by the quality of the parsing and generation technologies for the target language. We include experiments to calculate this upper bound for both handcrafted and automatically induced parsing and generation technologies currently in use by transfer-based MT systems. 1 Introduction Automatic methods of evaluation for MT include BLEU (Papineni et al., 2002), NIST (Doddington, 2002), METEOR (Banerjee and Lavie, 2005), GTM (Turian et al., 2003), TER (Snover et al., 2006) and dependency-based evaluation (Owczarzak et al., 2007). Each of these evaluation methods gives an overall result for the entire MT system, based on a comparison of the sentence output by the MT system with a reference sentence. Unlike other approaches to MT, such as Statistical Machine Translation, transfer-based MT involves three main components: parsing, transfer and generation and each of these contributes to the errors produced by the MT system. Transfer-based MT systems rely heavily on the quality of the parsing and generation components. In order to understa"
2007.mtsummit-ucnlg.2,N03-1017,0,0.00563435,"Missing"
2007.mtsummit-ucnlg.2,W02-2103,0,0.116365,"sults and, finally, section 6 gives some conclusions of the work presented in this paper. 2 Existing Generation and Parsing Evaluation Methods There is a considerable body of work on sentence realisation from abstract linguistic representation where evaluation has been carried out on standalone generators and independent of any MT system. However, an oft-cited future application for such generators is as the generation component of an MT system. Recently there has been an increasing amount of work in the area of robust, broad coverage sentence generation, tested on newswire text, for example (Langkilde-Geary, 2002; Callaway, 2003; Nakanishi et al., 2005; Cahill and van Genabith, 2006). Abstract semantic/syntactic inputs to these generators were automatically constructed from sections of the hand-crafted Penn Treebank. Sentences were then generated for these inputs and compared to the original sentences, using automatic string comparison metrics such as NIST and BLEU. Testing on previously unseen sections of the Penn Treebank demonstrates to what degree a generator has achieved broad coverage and high accuracy (according to BLEU and NIST scores). However, if we wish to take into account how the generato"
2007.mtsummit-ucnlg.2,D07-1028,1,\N,Missing
2007.mtsummit-ucnlg.2,2001.mtsummit-ebmt.4,0,\N,Missing
2007.mtsummit-ucnlg.2,W05-1510,0,\N,Missing
2007.mtsummit-ucnlg.2,P02-1040,0,\N,Missing
2007.mtsummit-ucnlg.2,C98-2134,0,\N,Missing
2008.eamt-1.10,W08-0319,0,0.0225558,"Missing"
2008.eamt-1.10,W02-1503,0,0.0208418,"e set of solutions of the algorithm retrieves all unpacked rules from the packed representation. 5 Experimental Evaluation In order to evaluate the effects of using the packed rule representation on the space required to store transfer rules, we ran an automatic transfer rule induction algorithm on sentences of the Europarl corpus. We restricted the test corpus to German-English sentences within the length range of 5 to 15 words. This resulted in 219,666 sentence pairs. We reserved 2000 of these sentences as a development set. Each side of the corpus was parsed with a monolingual LFG grammar (Butt et al., 2002; Riezler et al., 63 2002). The automatic rule induction algorithm used a bilingual dictionary (Richter, 2007) and Giza++ word alignments (Och and Ney, 2000) to align local f-structures. A packed transfer representation for each input f-structure pair was induced. All of the rules were then unpacked and counted. Our rule induction algorithm induced 5,148,874 transfer rules from the training data f-structure pairs. This resulted in an average of 23.65 rules being induced from each aligned f-structure pair 11 . The total time taken for the rule extraction algorithm was approximately 3.5 hours ru"
2008.eamt-1.10,2003.mtsummit-papers.6,0,0.0120417,"o load large numbers of transfer rules to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of transfer rules can now be produced. Riezler and Max"
2008.eamt-1.10,D07-1079,0,0.0252786,"Missing"
2008.eamt-1.10,P03-2041,0,0.0133095,"ransfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of transfer rules can now be produced. Riezler and Maxwell (2006) use feature structures of the Lexical Functional Grammar (LFG) formalism (Kaplan and Bresnan, 1982; Bresnan, 2001; Dalrymple, 2001) for deep transfer. They impose a limit of a maximum of three primitive rules to construct a complex rule 1"
2008.eamt-1.10,W02-1506,0,0.0372394,"Missing"
2008.eamt-1.10,N03-1017,0,0.0131535,"Missing"
2008.eamt-1.10,D07-1091,0,0.100734,"as a high impact on the amount of time taken to load large numbers of transfer rules to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of trans"
2008.eamt-1.10,P00-1056,0,0.00655862,"of using the packed rule representation on the space required to store transfer rules, we ran an automatic transfer rule induction algorithm on sentences of the Europarl corpus. We restricted the test corpus to German-English sentences within the length range of 5 to 15 words. This resulted in 219,666 sentence pairs. We reserved 2000 of these sentences as a development set. Each side of the corpus was parsed with a monolingual LFG grammar (Butt et al., 2002; Riezler et al., 63 2002). The automatic rule induction algorithm used a bilingual dictionary (Richter, 2007) and Giza++ word alignments (Och and Ney, 2000) to align local f-structures. A packed transfer representation for each input f-structure pair was induced. All of the rules were then unpacked and counted. Our rule induction algorithm induced 5,148,874 transfer rules from the training data f-structure pairs. This resulted in an average of 23.65 rules being induced from each aligned f-structure pair 11 . The total time taken for the rule extraction algorithm was approximately 3.5 hours running the algorithm on 8 parallel processors (28 CPU hours). In order to determine the effect of the packed representation we randomly selected 10 sets of 10"
2008.eamt-1.10,P05-1034,0,0.0149854,"epresentation also has a high impact on the amount of time taken to load large numbers of transfer rules to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much l"
2008.eamt-1.10,P02-1035,0,0.0339934,"Missing"
2008.eamt-1.10,N06-1032,0,0.0956483,"es to memory with very little trade-off in time needed to unpack the rules. We include an experimental evaluation which shows a considerable reduction in space and time requirements for a large set of automatically induced transfer rules by storing the rules in the packed representation. 1 Introduction Probabilistic Transfer-Based Machine Translation is one of several current approaches to machine translation that combine data-driven statistical methods with the use of linguistic information (Quirk et al., 2005; Koehn and Hoang, 2007; Ding and Palmer, 2005; Charniak et al., 2003; Lavie, 2008; Riezler and Maxwell, 2006; Bojar and Hajiˇc, 2008). Traditionally, transfer rules were manually developed. Recently, methods of automatically inducing transfer rules from bilingual corpora have emerged (Hajiˇc et al., 2002; Eisner, 2003; Bojar and Hajiˇc, 2008; Riezler and Maxwell, 2006). Acquiring transfer rules automatically from bilingual corpora 57 has several advantages. One obvious advantage is that automatic methods of rule induction are much quicker than manual rule development. This means that a much larger quantity of transfer rules can now be produced. Riezler and Maxwell (2006) use feature structures of th"
2008.eamt-1.10,W99-0604,0,\N,Missing
2008.eamt-1.10,P05-1067,0,\N,Missing
2020.coling-main.590,P18-1236,0,0.0543152,"nt to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantastic or excellent with correspondingly high ratings but anot"
2020.coling-main.590,D19-1562,0,0.0437692,"show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantastic or excellent with correspondingl"
2020.coling-main.590,D16-1171,0,0.155509,"ther. Experiment results on IMDB, Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user migh"
2020.coling-main.590,N19-1423,0,0.0888363,"that product. Secondly, learning meaningful user and product embeddings that are only updated by back propagation is difficult when a user or product only has a small number of reviews, whereas one may still be able to glean something useful from the text of even a small number of reviews. A naive approach might compute representations of all the reviews of a given user or product each time we have a new training sample but this would be too expensive, and we instead propose the following incremental approach: With each new training sample, we obtain the review text representation, with BERT (Devlin et al., 2019) as our encoder, before using the representation together with user and product vectors to obtain a user-biased document representation and a product-biased document representation, which are then employed to obtain sentiment polarity. We then add the user-biased and product-biased This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 6724 Proceedings of the 28th International Conference on Computational Linguistics, pages 6724–6729 Barcelona, Spain (Online), December 8-13, 2020 document representati"
2020.coling-main.590,D17-1054,0,0.0180653,"lp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantasti"
2020.coling-main.590,W18-6220,0,0.0931193,"Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such as fantastic or excellent with"
2020.coling-main.590,I17-1064,0,0.27945,"sults on IMDB, Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using the text of all the reviews belonging to a single user or product – see Fig. 1 (left). There are two reasons to incorporate review text into user/product modelling. Firstly, the reviews from a given user will reflect their word choices when conveying sentiment. For example, a typical user might use words such"
2020.coling-main.590,S14-2004,0,0.0170246,"t a better idea of where there is room for improvement for IUPC, we examine the 43 Yelp13 dev set cases, where the predicted label differs from the gold label by more than two points. There are a handful of cases of sarcasm, e.g. that lovely tempe waste/tap water taste in the food, but the most noteworthy phenomenon is mixed sentiment, e.g. tacos were good the soup was not tasty, or the more subtle brave the scary parking and lack of ambiance. It is not always clear from the reviews which aspect of the service the rating is directed towards. This suggests that aspect-based sentiment analysis (Pontiki et al., 2014) might be useful here, and training an IUPC model for this task is a possible avenue for future work. 4 Conclusion In this paper, we propose a neural sentiment analysis architecture that explicitly utilizes all past reviews from a given user or product to improve sentiment polarity classification on the document level. Our experimental results on the IMDB, Yelp-13 and Yelp-14 datasets demonstrate that incorporating this additional context is effective, particularly for the Yelp datasets. The code used to run the experiments is available for use by the research community.4 Acknowledgements This"
2020.coling-main.590,P15-1098,0,0.0245957,"oring representations of reviews written by the same user and about the same product and force the model to memorize all reviews for one particular user and product. Additionally, we drop the hierarchical architecture used in previous work to enable words in the text to directly attend to each other. Experiment results on IMDB, Yelp 2013 and Yelp 2014 datasets show improvement to state-of-the-art of more than 2 percentage points in the best case. 1 Introduction Document-level sentiment analysis aims to predict sentiment polarity of text that often takes the form of product or service reviews. Tang et al. (2015) demonstrated that modelling the individual who has written the review, as well as the product being reviewed, is worthwhile for polarity prediction, and this has led to exploratory work on how best to combine review text with user/product information in a neural architecture (Chen et al., 2016; Ma et al., 2017; Dou, 2017; Long et al., 2018; Amplayo, 2019; Amplayo et al., 2018). A feature common amongst past studies is that user and product IDs are modelled as embedding vectors whose parameters are learned during training. We take this idea a step further and represent users and products using"
2020.emnlp-main.6,W07-0718,1,0.764038,". The use of reverse-created test sets was not the only concern raised by Läubli et al. (2018) and Toral et al. (2018). Both used more context than the original sentence-level evaluation in Hassan et al. (2018), Läubli et al. (2018) now asking human judges to assess entire documents, and Toral et al. (2018) involving assessment of MT output sentences in the order that they appeared in original documents. Furthermore, in contrast to the use of Direct Assessment (Graham et al., 2016) by Hassan et al. (2018), both reassessments used relative ranking, a method formerly used in WMT for evaluation (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016), but now abandoned, partly due to low inter-annotator agreement. Therefore, although both re-evaluations improved the methodology employed in two respects, by eliminating reverse-created test data and including more context, both potentially include other sources of inaccuracy, such as lack of reliability of human judges when human evaluation takes the form of relative ranking. Furthermore, Toral et al. (2018) employ Trueskill to reach the conclusion that the MT system in question has not achieved human performance, and alth"
2020.emnlp-main.6,W19-5301,1,0.910427,"Missing"
2020.emnlp-main.6,W08-0309,1,0.862244,"Missing"
2020.emnlp-main.6,W10-1703,1,0.865007,"Missing"
2020.emnlp-main.6,W12-3102,1,0.905186,"Missing"
2020.emnlp-main.6,W11-2103,1,0.87397,"Missing"
2020.emnlp-main.6,W19-5204,0,0.0608855,"18) and Toral et al. (2018) used only test data that originated in the source language. Inspired by this work, other authors considered the effect of the 50/50 set-up on evaluation using WMT data. Edunov et al. (2019) questioned whether improvements in performance due to backtranslation were just an artifact of the test set construction. They found that, whilst back-translation had a disproportionately large positive effect on BLEU for reverse-created test sets, human evaluation showed that back-translation did indeed provide robust improvements to MT for forwardcreated text. Related to this, Freitag et al. (2019) also showed BLEU to be misleading on the reversecreated part of the test sets, when analysing why their automatic post-editing (APE) method produced improved translations according to human evaluation, but not according to BLEU. Given the concern in the community about using reversecreated test sets, the organisers of the WMT19 news translation task used only forward-created sentences in all their test sets (Barrault et al., 2019). In this current paper we provide detailed evidence to justify this decision. We note that Zhang and Toral (2019) also provide analysis of the effect of reverse-cre"
2020.emnlp-main.6,J12-4004,0,0.0612623,"Missing"
2020.emnlp-main.6,D18-1512,0,0.115616,"Missing"
2020.emnlp-main.6,P02-1040,0,0.115043,"ems participating in WMT-15 to WMT-18, as well as differences in human DA scores for systems participating in WMT-17 to WMT-18. The absence of systems in the upper-left and lower-right quadrants reassuringly shows that although extreme changes in BLEU and human scores do occur when test set creation direction is altered, the changes are at least somewhat systematic in the sense that when a difference in scores occurs (a drop or increase BLEU Besides human evaluation, the performance of MT systems is often measured using automatic metrics, the most common of which remains to be the BLEU score (Papineni et al., 2002). Figure 2 shows a box plot of absolute differences in BLEU scores for systems (reverse BLEU − forward BLEU) participating in WMT news translation tasks from 2015 to 2018. Counter expectation there is a clear mix of positive and negative BLEU score differences for several language pairs. Comparison of BLEU scores is not as straightforward as human evaluation however, and there are further consideration to be made before drawing conclusions from the mix of positive and negative absolute BLEU score differences described above. For example, the fact that splitting the test 76 Kendall 1.0 1.0 1.0"
2020.emnlp-main.6,W18-6312,0,0.155882,"of a human evaluation previously criticized for including reverse-created test data that claimed human parity of Chinese to English MT. We reveal insights into additional potential sources of inaccuracy of conclusions beyond the presence of translationese with the aim of preventing future inaccuracies. 2 spect to human evaluation, without considering its differing effect on automatic evaluation. Also, they do not consider the problem of statistical power in human evaluation, which we raise below. The use of reverse-created test sets was not the only concern raised by Läubli et al. (2018) and Toral et al. (2018). Both used more context than the original sentence-level evaluation in Hassan et al. (2018), Läubli et al. (2018) now asking human judges to assess entire documents, and Toral et al. (2018) involving assessment of MT output sentences in the order that they appeared in original documents. Furthermore, in contrast to the use of Direct Assessment (Graham et al., 2016) by Hassan et al. (2018), both reassessments used relative ranking, a method formerly used in WMT for evaluation (Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016), but now abandoned, p"
2020.emnlp-main.6,W19-5208,0,0.182034,"nts to MT for forwardcreated text. Related to this, Freitag et al. (2019) also showed BLEU to be misleading on the reversecreated part of the test sets, when analysing why their automatic post-editing (APE) method produced improved translations according to human evaluation, but not according to BLEU. Given the concern in the community about using reversecreated test sets, the organisers of the WMT19 news translation task used only forward-created sentences in all their test sets (Barrault et al., 2019). In this current paper we provide detailed evidence to justify this decision. We note that Zhang and Toral (2019) also provide analysis of the effect of reverse-created test sets on WMT evaluation campaigns. However they focus only on the effect of translationese with re73 REV FWD tion of statistical power. In most MT human evaluations, it is not feasible to evaluate the full test set of sentences for all systems and it is common to instead evaluate a sample of translations, usually drawn at random from the test data. In current WMT evaluations, for example, translations of all test sentences produced by all participating systems are pooled and a random sample is humanevaluated. This method ensures that"
2020.findings-emnlp.375,N15-1124,1,0.853486,"verage ratings attributed to translations sampled from large test sets, and although such methodology does allow application of statistical significance testing to identify potentially meaningful differences in system performance, they do not provide any insight into the reasons behind a significantly higher score or the degree to which systems perform better when translating individual segments. Furthermore, DA score distributions produced in the human evaluation of the news task are based on individual DA scores that alone cannot be relied upon to reflect the quality of individual segments (Graham et al., 2015). Past work, has however provided a means of running a DA human evaluation in such a way that DA scores accurately reflect the performance of a system on a given individual segment (Graham 1 DA is also used in other task evaluations such as Video Captioning and Multilingual Surface Realisation (Awad et al., 2019; Graham et al., 2018; Mille et al., 2019). 4199 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4199–4207 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2015). This method comes with the trade-off of requiring substantially"
2020.findings-emnlp.375,W19-5301,1,0.889077,"Missing"
2020.findings-emnlp.375,D18-1512,0,0.0479597,"Missing"
2020.findings-emnlp.375,W19-5302,1,0.82035,"Missing"
2020.findings-emnlp.375,D19-6301,1,0.787469,"etter when translating individual segments. Furthermore, DA score distributions produced in the human evaluation of the news task are based on individual DA scores that alone cannot be relied upon to reflect the quality of individual segments (Graham et al., 2015). Past work, has however provided a means of running a DA human evaluation in such a way that DA scores accurately reflect the performance of a system on a given individual segment (Graham 1 DA is also used in other task evaluations such as Video Captioning and Multilingual Surface Realisation (Awad et al., 2019; Graham et al., 2018; Mille et al., 2019). 4199 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4199–4207 c November 16 - 20, 2020. 2020 Association for Computational Linguistics et al., 2015). This method comes with the trade-off of requiring substantially more repeat assessments per segment than the test set level evaluation generally run, for example, to evaluate all primary submissions in the WMT news task. In this work we demonstrate how this method has the potential to be employed as a secondary method of evaluation in WMT tasks for a smaller subset of systems to provide segment-level insight into w"
2020.findings-emnlp.375,W19-5333,0,0.0132682,"++ + ++ + + ++ + + ++ + + + + ++ + + ++ + + + 0.000 0 25 50 adequacy 75 100 20 40 60 80 100 Facebook-FAIR (src) Figure 3: Density plot of sample of 540 accurate segment-level DA scores for German to English translation new translation for the top-performing system, FACEBOOK -FAIR, in WMT-19 versus the human translator where in the official results the system beat human performance; Human denotes evaluation of segments translated by the creator of the standard WMT reference translations Figures 1, 2 and 3 include density plots for human translation and the top-performing FACEBOOK -FAIR system (Ng et al., 2019) for the same 540 translated segments from WMT-19 for the three language pairs we investigate. For German to English and English to German translation in Figures 2 and 3 a similar pattern emerges in terms of comparison of human and machine-translated segments, as for both a slightly larger proportion of FACEBOOK -FAIR translations are scored high compared to the human translator – as can be seen from the higher red peak close to the extreme right of both plots indicating that the machine produces a marginally higher number of translations with higher levels of adequacy. For English to Russian"
2020.findings-emnlp.375,W18-6312,0,0.016085,"ments. 2 Related Work Over the past number of years, machine translation has been biting at the heels of human translation for a small number of language pairs. Beginning with the first claims that machines have surpassed human quality of translation for Chinese to English news text, conclusions received with some skepticism and even controversy (Hassan et al., 2018), as claims of human performance resulted in re-evaluations that scrutinized the methodology applied, highlighting the influence of reverse-created test data and lack of wider document context in evaluations (L¨aubli et al., 2018; Toral et al., 2018). Despite re-evaluations taking somewhat more care to eliminate such sources of inaccuracies, they additionally included some potential issues of their own, such as employing somewhat outdated human evaluation methodologies, non-standard methods of statistical significance testing and lack of planning evaluations in terms of statistical power. Graham et al. (2019, 2020), on the other hand re-run the evaluation, identify and fix remaining causes of error, and subsequently confirm that, on the overall level of the test set, with increased scrutiny on evaluation procedures, conclusions of human p"
2020.findings-emnlp.375,2020.emnlp-main.6,1,0.816952,"Missing"
2020.msr-1.1,W13-3520,0,0.0265768,". Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track are UD structures as in T1 from"
2020.msr-1.1,P11-2040,0,0.0129219,"R’18 (Mille et al., 2018) and SR’19 (Mille et al., 2019). The evaluation method is Direct Assessment (DA) (Graham et al., 2016), as used by the WMT competitions to produce the official ranking of machine translation systems (Barrault et al., 2020) and video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019). We ran the evaluation on Mechanical Turk,6 assessing two quality criteria, in separate evaluation experiments, but using the same method: Readability and Meaning Similarity. We used continuous sliders as rating tools, the evidence being that raters tend to prefer them (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the gray text’), was:7,8 The meaning of the gray text is adequately expressed by t"
2020.msr-1.1,W11-2832,0,0.0178935,"of the SR’20 tracks, data and evaluation methods, as well as brief summaries of the participating systems. Full descriptions of the participating systems can be found in separate system reports elsewhere in this volume. 1 Introduction SR’20 is the fourth in a line of shared tasks focused on surface realisation, the name originally given to the last stage in the first-generation (pre-statistical and pre-neural) Natural Language Generation (NLG) pipeline, mapping from semantic representations to fully realised surface word strings. When we ran the first Surface Realisation Shared Task in 2011 (Belz et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al.,"
2020.msr-1.1,K17-3005,0,0.0265528,"tense, verbal finiteness, etc.). The test data sets can be grouped into three types: (i) in-domain test data, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some histori"
2020.msr-1.1,W19-8652,0,0.0338279,"map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back o"
2020.msr-1.1,W18-3604,1,0.862544,"redicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1, and 13 for T2 (for a summary overview, see Table 2, top 3 sections of the table). The datasets were selected from the available collection of 4"
2020.msr-1.1,D19-1055,0,0.0142412,"ear, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system"
2020.msr-1.1,D19-6304,0,0.216682,"er proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs word order restoration by learning Interpreted Regular Tree Grammar (IRTG) rules encoding the correspondence between UD-subgraphs and word orderings. The grammars build strings and UD graphs simultaneously, using pairs of operations each connecting a set of dependents to their common head while concatenating the corresponding words. Rule 3 Data type Dataset Track train dev test In-domain arabic_padt (ar) chinese_gsd (zh) english_ewt (en) english_gum (en) english_lines (en) english_partut (en) french_gsd (fr) french_partut (fr) french_sequoia (fr) hindi_hdtb (hi) indonesian_gsd (id) j"
2020.msr-1.1,2020.acl-main.703,0,0.0144601,"mmon head while concatenating the corresponding words. The approach extends the team’s 2019 system by allowing rules to reference lemmas in addition to POS-tags and by giving preference to derivations that use a smaller number of more specific rules to construct a particular UD graph. Word order restoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augm"
2020.msr-1.1,K18-2014,0,0.0243276,"ta, in the same domains as the training and development data; (ii) Out-of-domain, which are test sets of parallel sentences in different languages in domains not covered by the training and development data; and (iii) silver standard data, which consists of automatically parsed sentences. The in-domain and out-of-domain data is provided in the UD release V2.3.1 The silver standard data was processed using the best CoNLL’18 parsers for the chosen datasets: the Harbin HIT-SCIR (HIT) parser (Che et al., 2017) for English_ewt, Hindi_hdtb, Korean_kaist and Spanish_ancora; the LATTICE (LAT) parser (Lim et al., 2018) for English_pud and the Stanford (STF) parser (Qi et al., 2019) for Portuguese_bosque.2 A detailed description of all SR’19 datasets and how they were processed can be found in the SR’19 report paper (Mille et al., 2019). 4.2 SR’20 new test sets To obtain new test sets,3 we selected sentences from Wikipedia in six out of the eleven SR’19 languages for which it was possible to get a good quantity of clean texts on the same topics. The used articles contain mostly landmarks and some historical figures. On the extracted sentences, we applied extensive filtering to achieve reasonably good text qu"
2020.msr-1.1,D17-1262,1,0.833898,"look at improvements this year compared to 2019, we see for instance, on the English_ewt test set, last year’s top BLEU score in T1 (the Shallow Track) was 82.98 (IMS); in 2020, it goes up to 86.16 in the restricted track (IMS), and 87.5 in the open track (ADAPT). In T2 (the Deep Track), top BLEU scores also increased, from 54.75 (IMS) to 58.84 in the restricted track, and 58.66 in the unrestricted track (both IMS). We next look at overall improvements of team submissions across all test sets they submitted outputs bias into the evaluation revealing no significant evidence of reference-bias (Ma et al., 2017). 8 ADAPT 20a 20b –BLEU-4– T1_ar_padt T1_en_ewt T2_en_ewt T1_en_gum T2_en_gum T1_en_lines T2_en_lines T1_en_partut T2_en_partut T1_es_ancora T2_es_ancora T1_es_gsd T2_es_gsd T1_fr_gsd T2_fr_gsd T1_fr_partut T2_fr_partut T1_fr_sequoia T2_fr_sequoia T1_hi_hdtb T1_id_gsd T1_ja_gsd T1_ko_gsd T1_ko_kaist T1_pt_bosque T1_pt_gsd T1_ru_gsd T1_ru_syntagrus T1_zh_gsd BME 20a 19 26 57.25 26.4 59.22 60.77 57.57 55.98 48.78 57.96 61.37 59.32 61.09 54.6 53.74 43.21 43.8 52.46 49.17 45.25 46.72 57.2 59.16 50.89 58.37 57.05 39.89 30.68 54.28 54.79 50.58 63.63 54.22 49.53 46.08 47.23 39.53 30.39 54.58 50.91 58"
2020.msr-1.1,W17-3517,1,0.829222,"et al., 2011), it was to address a situation where there were many different approaches to SR but none of them were comparable. We developed a commonground input representation that different approaches could map their normal inputs to, making results directly comparable for the first time. Most SR’11 systems (and all top performing ones) were statistical dependency realisers that did not make use of an explicit, pre-existing grammar. However, the question of how inputs to the realisers were going to be provided in an embedding system was left open. By the time we proposed the second SR Task (Mille et al., 2017), Universal Dependencies (UDs) had emerged as a convenient standard in parsing, with many associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisatio"
2020.msr-1.1,W18-3601,1,0.732438,"ny associated data sets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Mult"
2020.msr-1.1,D19-6301,1,0.741899,"ets, that we were able to pick up and use as the common-ground representation. By now, the third, neural generation of NLG methods was beginning to dominate the field, and systems participating in SR’18 were all trained to map directly from the UD inputs to the surface strings by some form of neural method. The question of how inputs to the realisers were going to be supplied remained open; moreover, most current approaches to NLG no longer even distinguished a separate surface realisation stage. Nevertheless, the community enthusiastically participated in SR’18 and SR’19 (Mille et al., 2018; Mille et al., 2019) as we expanded tracks to 11 languages. This year, things look different again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are"
2020.msr-1.1,W15-4719,0,0.0195724,"rojective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group words to be contracted, and then generates the contracted word form of each group with a seq2seq model. The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises texts by first preprocessing the dependency tree into a preordered linearized form, which is then converted into its textual counterpart using a rule-based approach together with a statistical machine translation (SMT) model. A singular version of the model was trained for each language considered in the experiment. 4 4.1 Data Sets T1 and T2 training and test sets (same as in SR’19) There are 42 datasets in 11 languages, 29 datasets for T1,"
2020.msr-1.1,P02-1040,0,0.108533,"-20-multilingual 2 5 Figure 1: Sample UD structure (without the last two columns). Figure 2: Sample T1 input structure (without the last two columns). Figure 3: Sample T2 input structure (without the last two columns). and Deep Track inputs is available on GitLab.4 Figures 1, 2 and 3 shown sample UD, Track 1 and Track 2 structures respectively, taken from the parsed Wikipedia English dataset. 5 Evaluation Methods 5.1 Automatic methods We used BLEU, NIST, BERT, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST5 is a related n-gram similarity metric weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single) reference text. The resulting number is then divided by the number of characters"
2020.msr-1.1,N18-1202,0,0.0268272,"rd order and inflecting words. a. Restricted-resources subtrack (same as SR’19 Track 1): Teams built models trained on the provided T1 dataset(s), but use of external task-specific data was not permitted. However, teams were allowed to use external generic resources. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material. Also permitted was the use of generic publicly available off-the-shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013). Alternatively, BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). b. Open subtrack: In this track, teams built models trained on the provided T1 dataset(s), also using any additional resources, without restrictions. Teams could even use the SR conversion tool to produce data with the exact same specifications as the data provided in the track, by applying the converter to a parsed output (see Section 4.2). T2 Deep Track: Inputs in this track"
2020.msr-1.1,2020.acl-demos.14,0,0.0252606,"xt quality. We skipped sentences that include special characters, contain unusual tokens (e.g. ISBN), or have unbalanced quotation marks or brackets. Furthermore, we took only sentences with more than 5 tokens and shorter than 50 tokens. After the initial filtering, quite a few malformed sentences remained. In order to remove those, we scored the sentences with BERT and kept only the best scored half. Finally, via manual inspection we identified patterns and expressions to reduce the number of malformed sentences still further. We parsed the cleaned Wikipedia sentences with the Stanza parser (Qi et al., 2020), using the trained models provided for the respective languages; the Stanza parser gets very competitive results on a large set of languages (see Table 3). For each language, we executed the parser with the processors for Tokenisation and Sentence Split, Multi-word Token Expansion, Part-of-Speech and Morphological Tagging, Lemmatisation and Dependency Parsing. The performance of the parser for all six languages in terms of Labelled Attachment Score and lemmatisation, two of the crucial aspects for our task, is provided in Table 3; for reference, we also provide the LAS and lemma scores of the"
2020.msr-1.1,K18-2011,1,0.865135,"Missing"
2020.msr-1.1,D19-6306,0,0.147951,", T2 T1, T2 T1 T1 T1 T1, T2 - - 1,795 1,032 1,675 2,287 471 1,723 Automatically parsed Wikipedia english_wikiST Z (en) french_wikiST Z (fr) korean_wikiST Z (ko) portuguese_wikiST Z (pt) russian_wikiST Z (ru) spanish_wikiST Z (es) T1, T2 T1, T2 T1 T1 T1 T1, T2 - - 1,313 1,313 530 1,135 1,291 1,280 Table 2: SR’20 dataset sizes for training, development and test sets (number of sentences). weights are proportional to the observed frequency of each pattern in the training data. The inflection step uses a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search, then combining the trees into a full projective tree; the completion model generates absent function words sequentially given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert lemmas to word forms character by character; the contraction model predicts BIO tags to group w"
2020.msr-1.1,2020.acl-main.134,0,0.342113,"estoration is performed separately for each clause. For the inflection step, a standard sequence-to-sequence model with biLSTM encoder and LSTM decoder with attention is used. Concordia uses a text-to-text model to tackle graph-to-text surface realisation. The approach finetunes the pre-trained BART (Lewis et al., 2020) language model on the task of surface realisation where the model receives the linearised representation of the dependency tree and generates the surface text. The IMS system builds on their system from the previous year with a substantial change in the lineariser proposed in (Yu et al., 2020), which models the task of word ordering as a Traveling Salesman Problem, and uses a biaffine attention model to calculate the bigram scores for the output sequence. To remedy the restriction of projectivity, it uses a transition system to reorder the sentence. Furthermore, model ensembling and data augmentation is applied to push the performance. The NILC submission explores different ways to represent a UD structure linearly, and models the generation task by using the small version of GPT-2. 3.2 SR’19 systems run on the SR’20 new test sets The BME-UW system (Kovács et al., 2019) performs wo"
2020.msr-1.1,W19-3404,0,0.023759,"ferent again. There is much discussion in the field of how to control the vexed tendencies of neural generators to ‘hallucinate’ content (Dušek et al., 2019), and how to instil some order This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1 Proceedings of the Third Workshop on Multilingual Surface Realisation (MSR’20), pages 1–20 Barcelona, Spain (Online), December 12, 2020. and coherence over longer texts. Multi-hop approaches are increasingly proposed to address such issues (Hua and Wang, 2019; Zhai et al., 2019; Zhao et al., 2020), and are beginning to look somewhat like the old NLG pipeline. In this context, surface realisation is very much back on the agenda, and the term is coming back into frequent use (Zhai et al., 2019; Zhao et al., 2020). Our aim for future editions of the SR Shared Task is to test whether multi-hop gives better results overall than single-hop, but also to link up with content selection modules capable of supplying the inputs required by SR systems. For this year, our main objective is to explore the impact of restricted vs. unrestricted resources in system training, and cros"
2020.wmt-1.1,2020.nlpcovid19-2.5,1,0.796341,"tails of the evaluation. 4.1.1 Covid Test Suite TICO-19 The TICO-19 test suite was developed to evaluate how well can MT systems handle the newlyemerged topic of COVID-19. Accurate automatic translation can play an important role in facilitating communication in order to protect at-risk populations and combat the infodemic of misinformation, as described by the World Health Organization. The test suite has no corresponding paper so its authors provided an analysis of the outcomes directly here. The submitted systems were evaluated using the test set from the recently-released TICO-19 dataset (Anastasopoulos et al., 2020). The dataset provides manually created translations of COVID19 related data. The test set consists of PubMed articles (678 sentences from 5 scientific articles), patient-medical professional conversations (104 sentences), as well as related Wikipedia articles (411 sentences), announcements (98 sentences from Wikisource), and news items (67 sentences from Wikinews), for a total of 2100 sentences. Table 15 outlines the BLEU scores by each submitted system in the English-to-X directions, also breaking down the results per domain. The analysis shows that some systems are significantly more prepar"
2020.wmt-1.1,2020.wmt-1.6,0,0.0647231,"AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua Universit"
2020.wmt-1.1,2020.wmt-1.38,0,0.0746945,"Missing"
2020.wmt-1.1,2020.wmt-1.54,1,0.802974,"Missing"
2020.wmt-1.1,W07-0718,1,0.671054,"Missing"
2020.wmt-1.1,W08-0309,1,0.762341,"Missing"
2020.wmt-1.1,W12-3102,1,0.500805,"Missing"
2020.wmt-1.1,2020.lrec-1.461,0,0.0795779,"Missing"
2020.wmt-1.1,2012.eamt-1.60,0,0.124643,"tted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS"
2020.wmt-1.1,2020.wmt-1.3,0,0.0731913,"on Machine Translation (WMT20)1 was held online with EMNLP 2020 and hosted a number of shared tasks on various aspects of machine translation. This conference built on 14 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; CallisonBurch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018; Barrault et al., 2019). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • automatic post-editing (Chatterjee et al., 2020) • biomedical translation (Bawden et al., 2020b) • chat translation (Farajian et al., 2020) • lifelong learning (Barrault et al., 2020) 1 Makoto Morishita NTT Santanu Pal WIPRO AI Abstract 1 Philipp Koehn JHU http://www.statmt.org/wmt20/ 1 Proceedings of the 5th Conference on Machine Translation (WMT), pages 1–55 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics as “direct assessment”) that we explored in the previous years with convincing results in terms of the trade-off between annotation effort and reliable distinctions between systems. The primary objectives o"
2020.wmt-1.1,2020.wmt-1.8,0,0.0898111,"D D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Know"
2020.wmt-1.1,2009.freeopmt-1.3,0,0.088081,"ve (CONTRASTIVE) or primary (PRIMARY), and the BLEU, RIBES and TER results. The scores are sorted by BLEU. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. This year we recived major number of participants for the case of Indo-Aryan language group NUST-FJWU NUST-FJWU system is an extension of state-of-the-art Transformer model with hierarchical attention networks to incorporate contextual information. During training the model used back-translation. Prompsit This team is participating with a rulebased system based on Apertium (Forcada et al., 2009-11). Apertium is a free/open-source platform for developing rule-based machine translation systems and language technology that was first released in 2005. Apertium is hosted in Github where both language data and code are licensed under the GNU GPL. It is a research and business platform with a very active community that loves small languages. Language pairs are at a very different level of development and output quality in the platform, depending on two main variables: how much funded or in-kind effort has 32 5.4 i.e. Hindi–Marathi (in both directions). We received 22 submissions from 14 te"
2020.wmt-1.1,2020.wmt-1.80,0,0.0933589,"airs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new for this year. Furthermore, English to and from Khmer and Pashto were included, using the same test sets as in the corpus filtering task. Th"
2020.wmt-1.1,W19-5204,0,0.0543621,"Missing"
2020.wmt-1.1,2020.emnlp-main.5,0,0.0410594,"luation of out-ofEnglish translations, HITs were generated using the same method as described for the SR+DC evaluation of into-English translations in Section 3.2.1 with minor modifications. Source-based DA allows to include human references in the evaluation as another system to provide an estimate of human performance. Human references were added to the pull of system outputs prior to sampling documents for tasks generation. If multiple references are available, which is the case for English→German (3 alternative reference translations, including 1 generated using the paraphrasing method of Freitag et al. (2020)) and English→Chinese (2 translations), each reference is assessed individually. Since the annotations are made by researchers and professional translators who ensure a betTable 11: Amount of data collected in the WMT20 manual document- and segment-level evaluation campaigns for bilingual/source-based evaluation out of English and nonEnglish pairs. et al., 2020; Laubli et al., 2020). It differs from SR+DC DA introduced in WMT19 (Bojar et al., 2019), and still used in into-English human evaluation this year, where a single segment from a document is provided on a screen at a time, followed by s"
2020.wmt-1.1,W18-3931,1,0.874637,"ese improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art translation systems on trans"
2020.wmt-1.1,2020.wmt-1.18,0,0.0913945,"2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al., 2020) Baseline System from Biomedical Task (Bawden et al., 2020b) American University of Beirut (no associated paper) Zoho Corporation (no associated paper) Table 6: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion"
2020.wmt-1.1,2020.wmt-1.43,0,0.0835067,"Missing"
2020.wmt-1.1,2020.wmt-1.9,0,0.0939415,"Missing"
2020.wmt-1.1,2020.wmt-1.19,0,0.0674131,"Missing"
2020.wmt-1.1,2009.mtsummit-btm.6,0,0.103443,"Missing"
2020.wmt-1.1,W13-2305,1,0.929934,"work which can be well applied to different translation. directions. Techniques used in the submitted systems include optional multilingual pre-training (mRASP) for low resource languages, very deep Transformer or dynamic convolution models up to 50 encoder layers, iterative backtranslation, knowledge distillation, model ensemble and development set fine-tuning. The key ingredient of the process seems the strong focus on diversification of the (synthetic) training data, using multiple scalings of the Transformer model 3.1 Direct Assessment Since running a comparison of direct assessments (DA, Graham et al., 2013, 2014, 2016) and relative ranking in 2016 (Bojar et al., 2016) and verifying a high correlation of system rankings for the two methods, as well as the advantages of DA, such as quality controlled crowd-sourcing and linear growth relative to numbers of submissions, we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100"
2020.wmt-1.1,E14-1047,1,0.888167,"Missing"
2020.wmt-1.1,2020.lrec-1.312,1,0.804196,"A screenshot of OCELoT is shown in Figure 5. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, Engli"
2020.wmt-1.1,2020.emnlp-main.6,1,0.839606,"shown in Table 3, where the first and second are simple merges or splits, whereas the third is a rare case of more complex reordering. We leave a detailed analysis of the translators’ treatment of paragraph-split data for future work. development set is provided, it is a mixture of both “source-original” and “target-original” texts, in order to maximise its size, although the original language is always marked in the sgm file, except for Inuktitut↔English. The consequences of directionality in test sets has been discussed recently in the literature (Freitag et al., 2019; Laubli et al., 2020; Graham et al., 2020), and the conclusion is that it can have an effect on detrimental effect on the accuracy of system evaluation. We use “source-original” parallel sentences wherever possible, on the basis that it is the more realistic scenario for practical MT usage. Exception: the test sets for the two Inuktitut↔English translation directions contain the same data, without regard to original direction. For most news text in the test and development sets, English was the original language and Inuktitut the translation, while the parliamentary data mixes the two directions. The origins of the news test documents"
2020.wmt-1.1,2020.wmt-1.11,0,0.0940191,"N GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) S"
2020.wmt-1.1,D19-1632,1,0.881933,"ent and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained the document boundaries and text ordering of the originals. Training, development, and test data for Pashto↔English and Khmer↔English are shared with the Parallel Corpus Filtering Shared Task (Koehn et al., 2020). The training data mostly comes from OPUS (software localization, Tatoeba, Global Voices), the Bible, and specialprepared corpora from TED Talks and the Jehova Witness web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics ab"
2020.wmt-1.1,2020.wmt-1.12,1,0.754946,"Missing"
2020.wmt-1.1,2020.wmt-1.13,0,0.0737827,"2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2020) Université du Québec à Montréal (no associated paper) ByteDance AI Lab (Wu et al., 2020a) WeChat (Meng et al"
2020.wmt-1.1,2020.wmt-1.20,0,0.057602,"Missing"
2020.wmt-1.1,2020.wmt-1.14,1,0.820019,"set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Mari"
2020.wmt-1.1,2020.wmt-1.39,1,0.812433,"kables was collected in the first phase of the annotation, which amounted to 4k assessments across the systems. The second annotation phase with 6.5k assessments compared markable translations, always checking outputs of all the 13 competing MT systems but still considering the document-level context of each of them. Among other things, the observations indicate that the better the system, the lower the variance in manual scores. Markables annotation then confirms that frequent errors like bad translation of a term need not be the most severe and conversely, 4.1.3 Gender Coreference and Bias (Kocmi et al., 2020) The test suite by Kocmi et al. (2020) focuses on the gender bias in professions (e.g. physician, teacher, secretary) for the translation from English into Czech, German, Polish and Russian. These nouns are ambiguous with respect to gender in English but exhibit gender in the examined target languages. The test suite is based on the fact that a pronoun referring to the ambiguous noun can reveal the gender of the noun in the English source sentence. Once disambiguated, the gender needs to be preserved in translation. To correctly translate the given noun, the translation system thus has to corr"
2020.wmt-1.1,2020.wmt-1.53,0,0.089538,"Missing"
2020.wmt-1.1,2020.wmt-1.78,1,0.815194,"rence on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inuktitut, Japanese, Polish and Tamil being new fo"
2020.wmt-1.1,W17-1208,0,0.0524248,"ttribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate the performance of state-of-the-art tr"
2020.wmt-1.1,2020.wmt-1.21,0,0.0791885,"Missing"
2020.wmt-1.1,2020.wmt-1.23,0,0.0607352,"020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ubiqus (Hernandez and Nguyen, 2020) University of Edinburgh (Bawden et al., 2020a; Germann, 2020) University of Edinburgh and Charles University (Germann et al., 2"
2020.wmt-1.1,2020.wmt-1.77,1,0.84299,"slation task, both organised alongside the Conference on Machine Translation (WMT) 2020. In the news task, participants were asked to build machine translation systems for any of 11 language pairs, to be evaluated on test sets consisting mainly of news stories. The task was also opened up to additional test suites to probe specific aspects of translation. In the similar language translation task, participants built machine translation systems for translating between closely related pairs of languages. Chi-kiu Lo NRC Masaaki Nagata NTT Marcos Zampieri Rochester Institute of Technology metrics (Mathur et al., 2020) parallel corpus filtering (Koehn et al., 2020) quality estimation (Specia et al., 2020a) robustness (Specia et al., 2020b) unsupervised and very low-resource translation (Fraser, 2020) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We included 22 translation directions this year, with translation between English and each of Chinese, Czech, German and Russian, as well as French to and from German being repeated from last year, and English to and from Inu"
2020.wmt-1.1,2020.wmt-1.24,0,0.0435945,"Missing"
2020.wmt-1.1,D18-1512,0,0.05415,"Missing"
2020.wmt-1.1,2020.wmt-1.47,1,0.740067,"Missing"
2020.wmt-1.1,W18-3601,1,0.891679,", we have employed DA as the primary mechanism for evaluating systems. With DA human evaluation, 15 human assessors are asked to rate a given translation by how adequately it expresses the meaning of the corresponding reference translation or source language input on an analogue scale, which corresponds to an underlying absolute 0–100 rating scale.5 No sentence or document length restriction is applied during manual evaluation. Direct Assessment is also employed for evaluation of video captioning systems at TRECvid (Graham et al., 2018; Awad et al., 2019) and multilingual surface realisation (Mille et al., 2018, 2019). 3.1.1 tion 2, most of our test sets do not include reversecreated sentence pairs, except when there were resource constraints on the creation of the test sets. 3.1.3 Prior to WMT19, the issue of including document context was raised within the community (Läubli et al., 2018; Toral et al., 2018) and at WMT19 a range of DA styles were subsequently tested that included document context. In WMT19, two options were run, firstly, an evaluation that included the document context “+DC” (with document context), and secondly, a variation that omitted document context “−DC” (without document con"
2020.wmt-1.1,2020.wmt-1.27,0,0.247738,"he original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans"
2020.wmt-1.1,D19-6301,1,0.888512,"Missing"
2020.wmt-1.1,W18-6424,0,0.0431841,"(Kocmi, 2020) combines transfer learning from a high-resource language pair Czech–English into the low-resource Inuktitut-English with an additional backtranslation step. Surprising behaviour is noticed when using synthetic data, which can be possibly attributed to a narrow domain of training and test data. The system is the Transformer model in a constrained submission. 2.3.3 Charles University (CUNI) CUNI-D OC T RANSFORMER (Popel, 2020) is similar to the sentence-level version (CUNI-T2T2018, CUBBITT), but trained on sequences with multiple sentences of up to 3000 characters. CUNI-T2T-2018 (Popel, 2018), also called CUBBITT, is exactly the same system as in WMT2018. It is the Transformer model trained according to Popel and Bojar (2018) plus a novel concat-regime backtranslation with checkpoint averaging (Popel et al., 2020), tuned separately for CZ-domain and non CZ-domain articles, possibly handling also translation-direction (“translationese”) issues. For cs→en also a coreference preprocessing was used adding the female-gender CUNI-T RANSFORMER (Popel, 2020) is similar to the WMT2018 version of CUBBITT, but with 12 encoder layers instead of 6 and trained on CzEng 2.0 instead of CzEng 1.7."
2020.wmt-1.1,2020.wmt-1.25,0,0.094349,"Missing"
2020.wmt-1.1,2020.wmt-1.28,0,0.0792624,"cument in the test set, and the origlang tag indicates the original source language. 9 Team AFRL ARIEL XV CUNI DCU D EEP M IND D I D I -NLP DONG - NMT ENMT E T RANSLATION FACEBOOK AI G RONINGEN GTCOM H ELSINKI NLP H UAWEI TSC IIE M ICROSOFT STC I NDIA NICT-K YOTO NICT-RUI N IU T RANS NRC OPPO PROMT SJTU-NICT SRPOL TALP UPC T ENCENT T RANSLATION THUNLP T ILDE T OHOKU -AIP-NTT U BIQUS UEDIN UEDIN-CUNI UQAM_TAN L E VOLC T RANS W E C HAT WMTB IOMED BASELINE YOLO ZLABS - NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 20"
2020.wmt-1.1,2020.lrec-1.443,1,0.79707,"er their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human evaluations. In the rest of this section, we provide brief details of the submitted systems, for those where the authors provided such details. The training corpus for Inuktitut↔English is the recently released Nunavut Hansard Inuktitut– English Parallel Corpus 3.0 (Joanis et al., 2020). For the Japanese↔English tasks, we added several freely available parallel corpora to the training data. It includes JParaCrawl v2.0 (Morishita et al., 2020), a large web-based parallel corpus, Japanese-English Subtitle Corpus (JESC) (Pryzant et al., 2017), the Kyoto Free Translation Task (KFTT) corpus (Neubig, 2011), constructed from the Kyoto-related Wikipedia articles, and TED Talks (Cettolo et al., 2012). The monolingual data we provided was similar to last year’s, with a 2019 news crawl added to all the news corpora. In addition, we provided versions of the news corpora for Czech, English and German, with both the document and paragraph structure retained. In other words, we did not apply sentence splitting to these corpora, and we retained t"
2020.wmt-1.1,2020.wmt-1.48,0,0.090424,"Missing"
2020.wmt-1.1,2020.wmt-1.49,0,0.0519886,"Missing"
2020.wmt-1.1,W19-6712,0,0.0573476,"tly crawled multilingual parallel corpora from Indian government websites (Haddow and Kirefu, 2020; Siripragada et al., 2020), the Tanzil corpus (Tiedemann, 2009), the Pavlick dicParagraph-split Test Sets For the language pairs English↔Czech, English↔German and English→Chinese, we provided the translators with paragraph-split texts, instead of sentence-split texts. We did this in order to provide the translators with greater freedom and, hopefully, to improve the quality of the translation. Allowing translators to merge and split sentences removes one of the “translation shifts” identified by Popovic (2019), which can make translations create solely for MT evaluation different from translations produced for other purposes. We first show some descriptive statistics of the source texts, for Czech, English and German, in 3 Europarl Parallel Corpus Czech ↔ English German ↔ English Polish↔ English German ↔ French Sentences 645,241 1,825,745 632,435 1,801,076 Words 14,948,900 17,380,340 48,125,573 50,506,059 14,691,199 16,995,232 47,517,102 55,366,136 Distinct words 172,452 63,289 371,748 113,960 170,271 62,694 368,585 134,762 News Commentary Parallel Corpus Czech ↔ English 248,927 5,570,734 6,156,063"
2020.wmt-1.1,2020.wmt-1.26,0,0.0845151,"Missing"
2020.wmt-1.1,2020.vardial-1.10,0,0.0933804,"Missing"
2020.wmt-1.1,W18-6301,0,0.038239,"Missing"
2020.wmt-1.1,2020.wmt-1.51,0,0.0917045,"Missing"
2020.wmt-1.1,2020.wmt-1.50,1,0.78567,"Missing"
2020.wmt-1.1,2020.wmt-1.52,0,0.0485419,"Missing"
2020.wmt-1.1,P19-1164,0,0.0581517,"26 26.37 25.51 24.82 28.33 23.33 21.13 21.96 20.43 22.90 22.58 21.90 22.17 22.17 20.53 19.40 20.01 40.44 32.39 30.39 37.04 32.27 27.54 25.97 26.09 46.38 37.30 36.05 35.96 33.76 33.07 27.20 27.07 Table 15: TICO-19 test suite results on the English-to-X WMT20 translation directions. 26 4.1.5 antecedent (a less common direction of information flow), and then correctly express the noun in the target language. The success of the MT system in this test can be established automatically, whenever the gender of the target word can be automatically identified. Kocmi et al. (2020) build upon the WinoMT (Stanovsky et al., 2019) test set, which provides exactly the necessary type of sentences containing an ambiguous profession noun and a personal pronoun which unambiguously (for the human eye) refers to it based the situation described. When extending WinMT with Czech and Polish, Stanovsky et al. have to disregard some test patterns but the principle remains. The results indicate that all MT systems fail in this test, following gender bias (stereotypical patterns attributing the masculine gender to some professions and feminine gender to others) rather than the coreference link. Word Sense Disambiguation (Scherrer et"
2020.wmt-1.1,2020.wmt-1.31,0,0.0881792,"morphological segmentation of the polysynthetic Inuktitut, testing rule-based, supervised, semi-supervised as well as unsupervised word segmentation methods, (2) whether or not adding data from a related language (Greenlandic) helps, and (3) whether contextual word embeddings (XLM) improve translation. G RONINGEN - ENIU use Transformer implemented in Marian with the default setting, improving the performance also with tagged backtranslation, domain-specific data, ensembling and finetuning. 2.3.7 DONG - NMT (no associated paper) No description provided. 2.3.8 ENMT (Kim et al., 2020) Kim et al. (2020) base their approach on transferring knowledge of domain and linguistic characteristics by pre-training the encoder-decoder model with large amount of in-domain monolingual data through unsupervised and supervised prediction task. The model is then fine-tuned with parallel data and in-domain synthetic data, generated with iterative back-translation. For additional gain, final results are generated with an ensemble model and re-ranked with averaged models and language models. G RONINGEN - ENTAM (Dhar et al., 2020) study the effects of various techniques such as linguistically motivated segmenta"
2020.wmt-1.1,2020.wmt-1.32,0,0.0839317,"- NLP Institution Air Force Research Laboratory (Gwinnup and Anderson, 2020) Independent submission (Xv, 2020) Charles University (Popel, 2020, 2018; Kocmi, 2020) Dublin City University (Parthasarathy et al., 2020) DeepMind (Yu et al., 2020) DiDi AI Labs (Chen et al., 2020b) (no associated paper) Indepdendent Submission (Kim et al., 2020) eTranslation (Oravecz et al., 2020) Facebook AI (Chen et al., 2020a) University of Groningen (Roest et al., 2020; Dhar et al., 2020) Global Tone Communication (Bei et al., 2020) University of Helsinki and Aalto University (Scherrer et al., 2020a) Huawei TSC (Wei et al., 2020a) Institute of Information Engineering, Chinese Academy of Sciences (Wei et al., 2020b) Microsoft STC India (Goyal et al., 2020) NICT-Kyoto (Marie et al., 2020) NICT-Rui (Li et al., 2020) NiuTrans (Zhang et al., 2020) National Research Council Canada (Knowles et al., 2020) OPPO (Shi et al., 2020) PROMT (Molchanov, 2020) SJTU-NICT (Li et al., 2020) Samsung Research Poland (Krubi´nski et al., 2020) TALP UPC (Escolano et al., 2020) Tencent Translation (Wu et al., 2020b) NLP Lab at Tsinghua University (no associated paper) Tilde (Krišlauks and Pinnis, 2020) Tohoku-AIP-NTT (Kiyono et al., 2020) Ub"
2020.wmt-1.1,2020.wmt-1.33,0,0.080166,"Missing"
2020.wmt-1.1,2020.wmt-1.34,0,0.0803867,"Missing"
2020.wmt-1.1,2020.wmt-1.35,0,0.0951745,"ss web site (JW300). The development and test sets were created as part of the Flores initiative (Guzmán et al., 2019) by professional translation of Wikipedia content with careful vetting of the translations. Please refer the to the Parallel Corpus Filtering Shared Task overview paper for details on these corpora. 2.3.1 AFRL (Gwinnup and Anderson, 2020) AFRL - SYSCOMB 20 is a system combination consisting of two Marian transformer ensembles, one OpenNMT transformer system and a Moses phrase-based system. AFRL - FINETUNE is an OpenNMT transformer system fine-tuned on newstest2014-2017. 2.3.2 (Xv, 2020) ARIEL XV is a Transformer base model trained with the Sockeye sequence modeling toolkit usSome statistics about the training and test materials are given in Figures 1, 2, 3 and 4. 4 8 ARIEL XV https://github.com/AppraiseDev/OCELoT English I English II English III Chinese Czech German Inuktitut Japanese Polish Russian Tamil ABC News (2), All Africa (5), Brisbane Times (1), CBS LA (1), CBS News (1), CNBC (3), CNN (2), Daily Express (1), Daily Mail (2), Fox News (1), Gateway (1), Guardian (3), Huffington Post (2), London Evening Standard (2), Metro (2), NDTV (7), RTE (7), Reuters (4), STV (2), S"
2020.wmt-1.1,2020.wmt-1.55,0,0.0877526,"Missing"
2020.wmt-1.1,P17-4012,0,0.0273892,"o SJTU-NICT using large XLM model to improve NMT but the exact relation is unclear. 2.3.14 H UAWEI TSC (Wei et al., 2020a) H UAWEI TSC use Transformer-big with a further increased model size, focussing on standard techniques of careful pre-processing and filtering, back-translation and forward translation, including self-training, i.e. translating one of the sides of the original parallel data. Ensembling of individual training runs is used in the forward as well as backward translation, and single models are created from the ensembles using knowledge distillation. The submission uses THUNMT (Zhang et al., 2017) open-source engine. 2.3.19 N IU T RANS (Zhang et al., 2020) N IU T RANS gain their performance from focussed attention to six areas: (1) careful data preprocessing and filtering, (2) iterative back-translation to generate additional training data, (3) using different model architectures, such as wider and/or deeper models, relative position representation and relative length, to enhance the diversity of translations, (4) iterative knowledge distillation by in-domain monolingual data, (5) iterative finetuning for domain adaptation using small training batches, (6) rule-based post-processing of"
2020.wmt-1.1,P98-2238,0,0.590812,"and punctuation, and we tend to attribute all these improvements to increased capacity (which allows increased sensitivity to long-range relations) of the models. 5 Similar Language Translation Most shared tasks at WMT (e.g. News, Biomedical) have historically dealt with translating texts from and to English. In recent years, we observed a growing interest in training systems to translate between languages other than English. This includes a number of papers applying MT to translate between pairs of closely-related languages, national language varieties, and dialects 27 of the same language (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018; Popovi´c et al., 2020). To address this topic, the first Similar Language Translation (SLT) shared task at WMT 2019 has been organized. It featured data from three pairs of closely-related languages from different language families: Spanish - Portuguese (Romance languages), Czech - Polish (Slavic languages), and Hindi - Nepali (IndoAryan languages). Following the success of the first SLT shared task at WMT 2019 and the interest of the community in this topic, we organize, for the second time at WMT, this shared task to evaluate th"
2020.wmt-1.1,2020.wmt-1.41,1,0.814291,"Missing"
2021.emnlp-main.340,P19-1620,0,0.018061,"ound-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we create from summary data using a number of heuristics, to train a QG model. We provide an overview of the proposed method is shown in Figure 2. We then employ the trained QG model to generate synthetic QA data that is further employed to train an unsupervi"
2021.emnlp-main.340,D18-1549,0,0.0258651,"). Document length and stride length are 364 and 128 respectively, the learning rate is set to 1 × 10−5 . Evaluation metrics for unsupervised QA are Exact Match (EM) and F-1 score. 4.2 Results We use the 20k generated synthetic QA pairs to train a BERT QA model and first validate its performance on the development sets of three benchmark QA datasets based on Wikipedia – SQuAD1.1, Natural Questions and TriviaQA. The results of our method are shown in Tables 1 and 2. The unsupervised baselines we compare with are as follows: 1. Lewis et al. (2019) employ unsupervised neural machine translation (Artetxe et al., 2018) to train a QG model; 4M synthetic QA examples were generated to train a QA model; 2. Li et al. (2020) employ dependency trees to generate questions and employed cited documents as passages. 5 For comparison, we also show the results of some supervised models fine-tuned on the correspond4138 Models S UPERVISED M ODELS BERT-base BERT-large U NSUPERVISED M ODELS Lewis et al. (2019) Li et al. (2020) Our Method NQ TriviaQA NewsQA BioASQ DuoRC EM F-1 EM F-1 EM F-1 EM F-1 EM F-1 Lewis et al. (2019) 19.6 28.5 18.9 27.0 26.0 32.6 Li et al. (2020) 33.6 46.3 30.3 38.7 32.7 41.1 Our Method 37.5 50.1 32.0"
2021.emnlp-main.340,2021.emnlp-main.693,1,0.844914,"Missing"
2021.emnlp-main.340,2020.acl-main.413,0,0.0353091,"2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we"
2021.emnlp-main.340,D19-5801,0,0.0143966,"rain the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraphs sampled from all paragraphs of Wikidumps. We employ the NER toolkits of Spacy5 (Honnibal et al., 2020) and AllenNLP6 (Gardner et al., 2017) to extract entity mentions in the paragraphs. We then remove paragraph, answer pairs that meet one or more of the following three conditions: 1) paragraphs with less than 20 words and more than 480 words; 2) par"
2021.emnlp-main.340,E06-1032,0,0.0223319,"Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach"
2021.emnlp-main.340,N10-1086,0,0.0648963,"Missing"
2021.emnlp-main.340,2020.coling-main.306,0,0.0346789,"tically correct questions, the resulting questions often lack variety and incur high lexical overlap with corresponding declarative sentences. For example, the question generated from the sentence Stephen Hawking announced the party in the morning, with Stephen Hawking as the candidate answer span, could be Who announced the party in the morning?, with a high level of lexi1 Introduction cal overlap between the generated question and the The aim of Question Generation (QG) is the pro- declarative sentence. This is undesirable in a QA duction of meaningful questions given a set of input system (Hong et al., 2020) since the strong lexical passages and corresponding answers, a task with clues in the question would make it a poor test of many applications including dialogue systems as real comprehension. 4134 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4134–4148 c November 7–11, 2021. 2021 Association for Computational Linguistics Neural seq2seq models (Sutskever et al., 2014) have come to dominate QG (Du et al., 2017), and are commonly trained with <passage, answer, question> triples taken from human-created QA datasets (Dzendzik et al., 2021) and this l"
2021.emnlp-main.340,P17-1147,0,0.175328,"ish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Quest"
2021.emnlp-main.340,Q19-1026,0,0.0572483,"generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach wh"
2021.emnlp-main.340,2020.acl-main.703,0,0.140325,"n and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG mod"
2021.emnlp-main.340,P19-1484,0,0.320563,"A) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach. Figure 1: Example questions generated via heuristics informed by semantic role labeling of summary sentences using different candidate answer spans well as education (Graesser et al., 2005). Additionally, QG can be applied to Question Answering (QA) for the purpose of data augmentation (Puri et al., 2020) where labeled <passage, answer, question> triples are combined with synthetic <passage, answer, question> triples produced by a QG system to train a QA system, and unsupervised QA (Lewis et al., 2019), in which only the QG system output is used to train the QA system. Early work on QG focused on template or rulebased approaches, employing syntactic knowledge to manipulate constituents in declarative sentences to form interrogatives (Heilman and Smith, 2009, 2010). Although template-based methods are capable of generating linguistically correct questions, the resulting questions often lack variety and incur high lexical overlap with corresponding declarative sentences. For example, the question generated from the sentence Stephen Hawking announced the party in the morning, with Stephen Hawk"
2021.emnlp-main.340,2020.acl-main.600,0,0.369141,"overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri et al. (2020) and Li et al. (2020) propose template/rule-based methods for generating questions and employ retrieved paragraphs and cited passages as source passages to alleviate the problems of lexical similarities between passages and questions. Alberti et al. (2019); Puri et al. (2020); Shakeri et al. (2020) additionally employ existing QA datasets to train a QG model. Although related, this work falls outside the scope of unsupervised QA. 3 Methodology Diverging from supervised neural question generation models trained on existing QA datasets, the approach we propose employs synthetic QG data, that we create from summary d"
2021.emnlp-main.340,W04-1013,0,0.0213914,"lly borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Na"
2021.emnlp-main.340,D18-1206,0,0.173048,"(who undergoes the action), and a set of modifier arguments such as a temporal ARGTMP or locative argument ARG-LOC. Questions are then generated from the arguments according to argument type and NER tags, which means that wh-words can be determined jointly. Returning to the example in Figure 1: given the SRL analysis [U2’s lead singer Bono ARG-0] has [had VERB] [emergency spinal surgery ARG-1] [after suffering an injury while preparing for tour Question Generation 1 Data we employ in experiments is news summary data In order to avoid generating trivial questions that originally from BBC News (Narayan et al., 2018) and the are highly similar to corresponding declarative news articles are typically a few hundred words in length. 4136 dates ARG-TMP]., the three questions shown in Figure 1 can be generated based on these three arguments. The pseudocode for our algorithm to generate questions is shown in Algorithm 1. We first obAlgorithm 1: Question Generation Heuristics S = summary srl_f rames = SRL(S) ners = N ER(S) dps = DP (S) examples = [] for frame in srl_frames do root_verb = dpsroot verb = f rameverb if root_verb equal to verb then for arg in frame do wh∗ = identif y_wh_word(arg, ners) base_verb, au"
2021.emnlp-main.340,P02-1040,0,0.110129,"o interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalglish QG model using news summary data. We uate QG. Even with respect to original text g"
2021.emnlp-main.340,2020.emnlp-main.468,0,0.0807813,"trained with only 20k English Wikipedia-based synthetic QA pairs, the QA model substantially outperforms previous unsupervised models on three in-domain datasets (SQuAD1.1, Natural Questions, TriviaQA) and three out-of-domain datasets (NewsQA, BioASQ, DuoRC), demonstrating the transferability of the approach. Figure 1: Example questions generated via heuristics informed by semantic role labeling of summary sentences using different candidate answer spans well as education (Graesser et al., 2005). Additionally, QG can be applied to Question Answering (QA) for the purpose of data augmentation (Puri et al., 2020) where labeled <passage, answer, question> triples are combined with synthetic <passage, answer, question> triples produced by a QG system to train a QA system, and unsupervised QA (Lewis et al., 2019), in which only the QG system output is used to train the QA system. Early work on QG focused on template or rulebased approaches, employing syntactic knowledge to manipulate constituents in declarative sentences to form interrogatives (Heilman and Smith, 2009, 2010). Although template-based methods are capable of generating linguistically correct questions, the resulting questions often lack var"
2021.emnlp-main.340,D16-1264,0,0.0511569,"ter than 5 tokens (very short questions are likely to have removed too much information) For the dataset in question, this process resulted in a total of 14,830 <passage-answer-question> triples. For training the QG model, we employ implementations of BART (Lewis et al., 2020) from Huggingface (Wolf et al., 2019). The QG model we employ is BART-base. We train the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraph"
2021.emnlp-main.340,J18-3002,0,0.0226063,"Missing"
2021.emnlp-main.340,P18-1156,0,0.0202575,"stion> triples. For training the QG model, we employ implementations of BART (Lewis et al., 2020) from Huggingface (Wolf et al., 2019). The QG model we employ is BART-base. We train the QG model on the QG data for 3 epochs with a learning rate of 3 × 10−5 , using the AdamW optimizer (Loshchilov and Hutter, 2019). 4.1.2 Unsupervised QA Datasets We carry out experiments on six extractive QA datasets, namely, SQuAD1.1 (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015) and DuoRC (Saha et al., 2018). We employ the official data of SQuAD1.1, NewsQA and TriviaQA and for Natural Questions, BioASQ and DuoRC, we employ the pre-processed data released by MRQA (Fisch et al., 2019). Unsupervised QA Training Details To generate synthetic QA training data, we make use of Wikidumps 4 by firstly removing all HTML tags and reference links, then extracting paragraphs that are longer than 500 characters, resulting in 60k paragraphs sampled from all paragraphs of Wikidumps. We employ the NER toolkits of Spacy5 (Honnibal et al., 2020) and AllenNLP6 (Gardner et al., 2017) to extract entity mentions in the"
2021.emnlp-main.340,2020.emnlp-main.439,0,0.0339364,"exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics, the Answer is combined with the Article to form the input to the Encoder, the Question is employed as the ground-truth label for the outputs of the Decoder. Unsupervised QA In unsupervised QA, the QA model is trained using synthetic data based on a QG model instead of an existing QA dataset. Instead of resorting to existing QA datasets, unsupervised QG methods have been employed, such as Unsupervised Neural Machine Translation (Lewis et al., 2019). Fabbri"
2021.emnlp-main.340,D19-1253,0,0.0207924,"on Generation Traditional approaches to QG mostly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrow"
2021.emnlp-main.340,D18-1427,0,0.0182907,"-of-the-art performance even at low volumes of synthetic training data. 2 Related Work Question Generation Traditional approaches to QG mostly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Ban"
2021.emnlp-main.340,W17-2623,0,0.158077,"o original text generemploy our QG model to generate synthetic QA ation tasks, however, the use of such metrics has data to train a QA model in an unsupervised set- been questioned (Callison-Burch et al., 2006; Reting and test the approach with six English QA iter, 2018). Such metrics are particularly probdatasets: SQuAD1.1, Natural Questions, TriviaQA, lematic for QG evaluation since multiple plausiNewsQA, BioASQ and DuoRC (Rajpurkar et al., ble questions exist for a given passage and answer. 2016; Kwiatkowski et al., 2019; Joshi et al., 2017; Consequently, there has been a shift in focus to Trischler et al., 2017; Tsatsaronis et al., 2015; Saha evaluating QG using an extrinsic evaluation that et al., 2018). Experiment results show that our generates synthetic QA pairs for the purpose of approach substantially improves over previous un- evaluating their effectiveness as a data augmentasupervised QA models even when trained on sub- tion or unsupervised QA approach (Alberti et al., stantially fewer synthetic QA examples. 2019; Puri et al., 2020; Shakeri et al., 2020). 4135 Figure 2: An overview of our approach where Answer and Question are generated based on Summary by the Question Generation Heuristics,"
2021.emnlp-main.340,2020.coling-main.228,0,0.0416481,"stly employ linguistic templates and rules to transform declarative sentences into interrogatives (Heilman and Smith, 2009). Recently, Dhole and Manning (2020) showed that, with the help of advanced neural syntactic parsers, template-based methods are capable of generating high-quality questions from texts. Neural seq2seq generation models have additionally been widely employed in QG, with QG data usually borrowed from existing QA datasets (Du et al., 2017; Sun et al., 2018; Ma et al., 2020). Furthermore, reinforcement learning has been employed by Zhang and Bansal (2019); Chen et al. (2019); Xie et al. (2020) to directly optimize discrete evaluation metrics such as BLEU (Papineni et al., 2002). Lewis et al. (2020) and Song et al. (2019) show that a large-scale pre-trained model can achieve state-of-the-art performance for supervised QG (Dong et al., 2019; Narayan et al., 2020). Question Generation Evaluation BLEU (PapIn order to explore the effectiveness of our ineni et al., 2002), ROUGE (Lin, 2004) and Memethod, we carry out extensive experiments. We teor (Banerjee and Lavie, 2005) metrics are comprovide an extrinsic evaluation, and train an En- monly borrowed from text generation tasks to evalgl"
C16-1294,P16-2013,0,0.0124843,"of statistical significance of a difference in dependent correlations with human assessment for competing out-of-English human-targeted metrics; a green cell denotes a significant increase in correlation with human assessment for the metric in a given row over the metric in a given column at p &lt; 0.05. the decision to include fluency should be weighed against the degree to which reference bias is actually present in the adequacy evaluation. If there is strong reference bias, for example, it would be highly advisable to include both fluency and adequacy but for a smaller number of translations. Fomicheva and Specia (2016) investigate bias in monolingual evaluation of MT and conclude reference bias to be a serious issue, with human annotators strongly biased by the reference translation provided. Their conclusion was based on estimation of confidence intervals for Kappa coefficients by means of an unconventional resampling technique, where samples used to estimate confidence intervals were smaller than the original sample size, drawn without replacement, and averaged; this diverges from standard methods of confidence interval estimation, such as bootstrap resampling. As a result, the reported confidence limits"
C16-1294,W13-2305,1,0.859772,"H−BLEU H.TER H−CDER H.CDER H−BLEU H.BLEU H−PER H.WER H.CDER H.TER H.BLEU H.PER CS-EN Figure 1: Significance test results (Williams test) of statistical significance of a difference in dependent correlations with human assessment for competing to-English human-targeted metrics; a green cell denotes a significant increase in correlation with human assessment for the metric in a given row over the metric in a given column at p &lt; 0.05. reference translation. Although certainly not to the same extreme, human assessment of MT that employs a reference translation could incur similar reference bias. Graham et al. (2013) provide discussion on how reference bias could exist for direct assessment of translation adequacy, and, as a solution, propose inclusion of a separate reference-free fluency assessment, to provide a component of the evaluation that cannot be biased in any way by a reference translation. However, it is inevitably the case that resources available to conduct human evaluation are limited, and therefore a trade-off exists between adding fluency assessments of translations and numbers of translations we can feasibly include. For example, in our earlier human evaluation of HTER, it would have been"
C16-1294,N15-1124,1,0.925892,"Translation Adequacy Overall in this work, human assessment was carried out for two distinct data sets, firstly a nine language pair data set to re-evaluate HTER and subsequently an English to Spanish data set (see Section 4.2). Below we describe human assessment of the nine language pair data set. Human direct assessment (DA) of the adequacy of translations was collected by means of a 100-point continuous rating scale via crowd-sourcing. Large numbers of repeat human judgments for translations were collected on Amazon Mechanical Turk, by way of re-implementation and minor adaptation of 3126 Graham et al. (2015).2 Translations were sampled at random from all systems competing in WMT-13 translation task for Czech-to-English (CS-EN), German-to-English (DE-EN), Spanish-to-English (ESEN), French-to-English (FR-EN), Russian-to-English (RU-EN), English-to-German (EN-DE), Englishto-Spanish (EN-ES), English-to-French (EN-FR) and English-to-Russian (EN-RU). Human assessors rated the adequacy of translations in a monolingual human evaluation by comparing the meaning of the original generic human reference translation (rendered in gray) with a given MT output translation (rendered in black) by stating the degre"
C16-1294,P15-1174,1,0.940182,"gth of correlation with human assessment. For example, when the most widely applied human-targeted metric, human-targeted translation edit rate (HTER) was first proposed, Snover et al. (2006) reported that “HTER is more highly correlated to human judgments than BLEU or HBLEU” (p. 230), and hence, since 2006, it is HTER, as opposed to HBLEU, that is employed in MT as a human assessment substitute. 2 Relevant Work HTER is generally regarded as a valid substitute for human assessment. In MT QE, for example, HTER scores are used to evaluate systems in large-scale shared tasks (Bojar et al., 2015; Graham, 2015). If a metric such as HTER is to be relied upon as a substitute for human assessment, it is important that trust in the metric is well-placed to avoid inaccuracies in empirical evaluations. For instance, Bojar et al. (2013) and Graham (2015) make the assumption that HTER provides a valid representation of translation quality and subsequently employ HTER scores as a gold standard representation when evaluating QE systems, ultimately leading to rankings for competing systems. If HTER scores do not in fact provide a valid representation of translation quality, however, system rankings are likely"
C16-1294,P02-1040,0,0.115521,"in the target language of the MT output in question. All human post-editors were unaware of the purpose of the post-editing work and experiments. Annotators were shown the reference and MT output with post-editing instructions as follows:4 ∗∗ Making as few changes as possible ∗∗ , correct the hypothesis segment to make it (a) have the same meaning as the reference segment; (b) grammatically correct. 3.3 HTER Re-evaluation Results Table 2 shows correlations achieved by a range of human-targeted metric formulations with human assessment, including human-targeted versions of segment-level BLEU (Papineni et al., 2002; Koehn et al., 2007), TER (Snover et al., 2006), WER, PER and CDER (Leusch and Ney, 2008). Correlations with human assessment achieved by HTER are, for three of the nine language pairs we evaluate, below a Pearson correlation of 0.6. In addition, comparison of correlations achieved by HTER and HBLEU reveal a higher correlation achieved by HBLEU for five of the nine language pairs we evaluate. As recommended by Graham et al. (2015), we test for significance of difference in dependent correlations using Williams test, as shown in Figures 1 and 2. For two language pairs, DE-EN and FR-EN, correla"
C16-1294,2006.amta-papers.25,0,0.757095,"terparts, the scores they produce are nonetheless still partly automatic. Given the vast number of possible methods of comparing a given MT output with its (human-targeted) reference translation, it is necessary to provide evidence that any given choice of human-targeted metric provides the best formulaic substitute for human assessment. As with fully automatic metrics, human-targeted metrics are themselves evaluated by strength of correlation with human assessment. For example, when the most widely applied human-targeted metric, human-targeted translation edit rate (HTER) was first proposed, Snover et al. (2006) reported that “HTER is more highly correlated to human judgments than BLEU or HBLEU” (p. 230), and hence, since 2006, it is HTER, as opposed to HBLEU, that is employed in MT as a human assessment substitute. 2 Relevant Work HTER is generally regarded as a valid substitute for human assessment. In MT QE, for example, HTER scores are used to evaluate systems in large-scale shared tasks (Bojar et al., 2015; Graham, 2015). If a metric such as HTER is to be relied upon as a substitute for human assessment, it is important that trust in the metric is well-placed to avoid inaccuracies in empirical e"
C16-1294,P07-2045,0,\N,Missing
D14-1020,2003.mtsummit-papers.32,0,0.160669,"metrics is Spearman’s rank correlation with human judgments (Melamed et al., 2003), which measures the relative degree of monotonicity between the metric and human scores in the range [−1, 1]. The standard justification for calculating correlations over ranks rather than raw scores is to: (a) reduce anomalies due to absolute score differences; and (b) focus evaluation on what is generally the primary area of interest, namely the ranking of systems/translations. An alternative means of evaluation is Pearson’s correlation, which measures the linear correlation between a metric and human scores (Leusch et al., 2003). Debate on the relative merits of Spearman’s and Pearson’s correlation for the evaluation of automatic metrics is ongoing, but there is an increasing trend towards Pearson’s correlation, e.g. in the recent W MT-14 shared metrics task. Figure 1 presents the system-level results for two evaluation metrics – A MBER (Chen et al., 2012) and T ERRORCAT (Fishel et al., 2012) – over the W MT-12 Spanish-to-English metrics task. These two metrics achieved the joint-highest rank correlation (ρ = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The l"
D14-1020,W11-2108,0,0.0308069,"Missing"
D14-1020,J82-2005,0,0.753631,"Missing"
D14-1020,W12-3103,0,0.0750178,"Missing"
D14-1020,P03-1021,0,0.0374185,"contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson:"
D14-1020,W12-3104,0,0.0139142,"ferences; and (b) focus evaluation on what is generally the primary area of interest, namely the ranking of systems/translations. An alternative means of evaluation is Pearson’s correlation, which measures the linear correlation between a metric and human scores (Leusch et al., 2003). Debate on the relative merits of Spearman’s and Pearson’s correlation for the evaluation of automatic metrics is ongoing, but there is an increasing trend towards Pearson’s correlation, e.g. in the recent W MT-14 shared metrics task. Figure 1 presents the system-level results for two evaluation metrics – A MBER (Chen et al., 2012) and T ERRORCAT (Fishel et al., 2012) – over the W MT-12 Spanish-to-English metrics task. These two metrics achieved the joint-highest rank correlation (ρ = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The largest contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric invol"
D14-1020,2001.mtsummit-papers.68,0,0.0757134,"trics achieved the joint-highest rank correlation (ρ = 0.965) for the task, but differ greatly in terms of Pearson’s correlation (r = 0.881 vs. 0.971, resp.). The largest contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha,"
D14-1020,W11-2107,0,0.0185794,"Missing"
D14-1020,W12-3106,0,0.0555225,"Missing"
D14-1020,W12-3105,0,0.0303018,"Missing"
D14-1020,W05-0908,0,0.0192495,"the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0.881 Pearson: ● −2 −2 ● −3 −3 ● −3 −2 −1"
D14-1020,N03-1010,0,0.0412204,"). The largest contributor to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.97"
D14-1020,W14-3333,1,0.762543,"presented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0.881 Pearson: ● −2 −2 ● −3 −3 ● −3 −2 −1 0 1 2 3 −3 Human −2 −"
D14-1020,W04-3250,0,0.168479,"e system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0.881 Pearson:"
D14-1020,N04-1022,0,0.0167647,"to this artifact is the system with the lowest human score, represented by the leftmost point in both plots. Introduction Within machine translation (MT), efforts are ongoing to improve evaluation metrics and find better ways to automatically assess translation quality. The process of validating a new metric involves demonstration that it correlates better with human judgment than a standard metric such as B LEU (Papineni et al., 2001). However, although it is standard practice in MT evaluation to measure increases in automatic metric scores with significance tests (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004; Riezler and Maxwell, 2005; Graham et al., 2014), this has not been the case in papers proposing new metrics. Thus it is possible that some reported improvements in correlation with human judgment are attributable to chance rather than a systematic improvement. 172 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 172–176, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics 3 1 0 ● ● ● ● ●● ● ● −1 ● ● ● ● ● ● 0 TerrorCat 1 ● ● ●● ● ● ● −1 AMBER Spearman: 0.965 0.971 Pearson: 2 2 3 Spearman: 0.965 0"
D14-1020,N03-2021,0,\N,Missing
D14-1020,P02-1040,0,\N,Missing
D15-1013,N03-1020,0,0.69976,"riants significantly outperform others, optimal metric variants distinct from current recommended best variants, as well as machine translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems. 1 Introduction Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (Lin and Hovy, 2003), the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score (Papineni et al., 2002). Automatic evaluation in 128 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128–137, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2 obvious disadvantage that superior variants may exist but remain unidentified due to their omission. Related Work When ROUGE (Lin and Hovy, 2003) was first proposed, the methodology applied to its evaluation, in one respect, was similar to"
D15-1013,W08-0309,0,0.0100363,"ts may exist but remain unidentified due to their omission. Related Work When ROUGE (Lin and Hovy, 2003) was first proposed, the methodology applied to its evaluation, in one respect, was similar to that applied to metrics in MT, as ROUGE variants were evaluated by correlation with a form of human assessment. Where the evaluation methodology diverged from MT, however, was with respect to the precise representation of human assessment that was employed. In MT evaluation of metrics, although experimentation has taken place with regards to methods of elicitation of assessments from human judges (Callison-Burch et al., 2008), human assessment is always aimed to encapsulate the overall quality of translations. In contrast in summarization, metrics are evaluated by the degree to which metric scores correlate with human coverage scores for summaries, a recall-based formulation of the number of peer summary units that a human assessor believed had the same meaning as model summaries. Substitution of overall quality assessments with a recall-based manual metric, unfortunately has the potential to introduce bias into the evaluation of metrics in favor of recallbased formulations. One dimension of summary quality omitte"
D15-1013,W13-1721,0,0.0480178,"Missing"
D15-1013,W12-2601,0,0.0852489,"evaluation of automatic metrics in MT has been by correlation with human assessment. In contrast in summarization, over the years since the introduction of ROUGE , summarization evaluation has seen a variety of different methodologies applied to evaluation of its metrics. Evaluation of summarization metrics has included, for example, the ability of a metric/significance test combination to distinguish between sets of human and system-generated summaries (Rankel et al., 2011), or by accuracy of conclusions drawn from metrics when combined with a particular significance test, Wilcoxon ranksum (Owczarzak et al., 2012). Besides moving away from well-established methods such as correlation with human judgment, previous summarization metric evaluations have been additionally limited by inclusion of only a small proportion of possible metrics and variants. For example, although the most commonly used metric ROUGE has a very large number of possible variants, it is common to include only a small range of those in evaluations. This has the We provide an analysis of current evaluation methodologies applied to summarization metrics and identify the following areas of concern: (1) movement away from evaluation by c"
D15-1013,D14-1020,1,0.672029,"surprisingly, BLEU MT evaluation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU , to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew , by Pearson correlation takes the form of quantifying the correlation, r(Mnew , H), that exists between metric scores for systems an"
D15-1013,P02-1040,0,0.105516,"translation metric BLEU to have performance on-par with ROUGE for the purpose of evaluation of summarization systems. We subsequently replicate a recent large-scale evaluation that relied on, what we now know to be, suboptimal ROUGE variants revealing distinct conclusions about the relative performance of state-of-the-art summarization systems. 1 Introduction Automatic metrics of summarization evaluation have their origins in machine translation (MT), with ROUGE (Lin and Hovy, 2003), the first and still most widely used automatic summarization metric, comprising an adaption of the BLEU score (Papineni et al., 2002). Automatic evaluation in 128 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 128–137, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. 2 obvious disadvantage that superior variants may exist but remain unidentified due to their omission. Related Work When ROUGE (Lin and Hovy, 2003) was first proposed, the methodology applied to its evaluation, in one respect, was similar to that applied to metrics in MT, as ROUGE variants were evaluated by correlation with a form of human assessment. Where the evaluation me"
D15-1013,W14-3333,1,0.84316,"OUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel. However, in this respect, ROUGE has a distinct advantage over BLEU , as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE CS+M LQ 2 131 scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems. 3.3 separately to the correlation of each metric with human assessment, with the hope that the new metric will achieve a significant correlation where the baseline metric does not. The reasoning here is flawed however: the fact that one correlation is significantly higher than zero (r(Mnew , H)) and that of another is not, does not necessarily mean that th"
D15-1013,P10-1056,0,0.0870949,"ary appear, and evaluation by human coverage scores alone means that a summary with its units scrambled or even reversed in theory receives precisely the same metric score as the original. Given current evaluation methodologies for assessment of metrics, a metric that scores two such summaries differently would be unfairly penalized for it. Furthermore, when the linguistic quality of summaries has been assessed in parallel with annotations used to compute human coverage scores, it has been shown that the two dimensions of quality do not correlate with one another (no significant correlation) (Pitler et al., 2010), providing evidence that coverage scores alone do not fully represent human judgment of the overall quality of summaries. Subsequent summarization metric evaluations depart from correlation with human judgment furDespite such limitations, however, subsequent evaluations of state-of-the-art summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems (Hong et al., 2014). This forces us to raise some important questions. Firstly, to what degree was the divergence away from"
D15-1013,N15-1124,1,0.106891,"uation metric achieves strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU , to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew , by Pearson correlation takes the form of quantifying the correlation, r(Mnew , H), that exists between metric scores for systems and corresponding human"
D15-1013,D11-1043,0,0.17206,"Missing"
D15-1013,P15-1174,1,0.451339,"s strongest correlation with human assessment overall, r = 0.797, with performance of ROUGE variants ranging from r = 0.786, just below that of BLEU , to as low as r = 0.293. For many pairs of metrics, differences in correlation with human judgment are small, however, and prior to concluding superiority in performance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew , by Pearson correlation takes the form of quantifying the correlation, r(Mnew , H), that exists between metric scores for systems and corresponding human assessment sco"
D15-1013,hong-etal-2014-repository,0,0.523248,"wn that the two dimensions of quality do not correlate with one another (no significant correlation) (Pitler et al., 2010), providing evidence that coverage scores alone do not fully represent human judgment of the overall quality of summaries. Subsequent summarization metric evaluations depart from correlation with human judgment furDespite such limitations, however, subsequent evaluations of state-of-the-art summarization systems operate under the assumption that recommended ROUGE variants are optimal and rely on this assumption to draw conclusions about the relative performance of systems (Hong et al., 2014). This forces us to raise some important questions. Firstly, to what degree was the divergence away from evaluation methodologies still applied to MT metrics today well-founded? For example, were the original methodology, by correlation with human assessment, to be applied, would a distinct variant of ROUGE emerge as superior and subsequently lead to distinct system rankings? Secondly, were all variants of ROUGE to be included in evaluations, would a variant originally omitted from the evaluation emerge as superior and lead to further differences in summarization system rankings? Furthermore,"
D15-1013,W12-2004,0,0.0066656,"another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew , by Pearson correlation takes the form of quantifying the correlation, r(Mnew , H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(Mbase , H). One approach to testing for significance that may seem reasonable is to apply a significance test where rij is the correlation between Xi and Xj , n is the size of the population, and: K = 1 − r12 2 − r13 2 − r23 2 + 2r12 r13 r23 Since the power of Williams"
D15-1013,P11-1019,0,0.00479164,"formance of one metric over another, significance tests should be applied. 4 Metric Significance Testing In MT, recent work has identified the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015; Graham, 2015), and, for similar reasons, Williams test is suited to significance testing differences in performance of competing summarization metrics which we detail further below. Williams test has additionally been used in evaluation of systems that automatically assess spoken and written language quality (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012; Evanini et al., 2013). Evaluation of a given summarization metric, Mnew , by Pearson correlation takes the form of quantifying the correlation, r(Mnew , H), that exists between metric scores for systems and corresponding human assessment scores, and contrasting this correlation with the correlation for some baseline metric, r(Mbase , H). One approach to testing for significance that may seem reasonable is to apply a significance test where rij is the correlation between Xi and Xj , n is the size of the population, and: K = 1 − r12 2 − r13 2 − r23 2 + 2r12 r13"
D15-1013,W04-3250,0,0.0384104,"nal overall ROUGE scores for systems are comprised of the mean or median of ROUGE scores of individual summaries, is, again, a divergence from MT evaluation, as ngram counts used to compute BLEU scores are computed at the document as opposed to sentencelevel. However, in this respect, ROUGE has a distinct advantage over BLEU , as the fact that ROUGE comprises the mean or median score of individual summary scores makes possible the application of standard methods of significance testing differences in system-level ROUGE scores, while BLEU is restricted to the application of randomized methods (Koehn, 2004; Graham et al., 2014). For this purpose, differences in median ROUGE CS+M LQ 2 131 scores can be tested for statistical significance using, for example, Wilcoxon signed-rank test, while paired t-test can be applied to difference of mean ROUGE scores for systems. 3.3 separately to the correlation of each metric with human assessment, with the hope that the new metric will achieve a significant correlation where the baseline metric does not. The reasoning here is flawed however: the fact that one correlation is significantly higher than zero (r(Mnew , H)) and that of another is not, does not ne"
D15-1013,P07-2045,0,\N,Missing
D17-1262,W07-0718,0,0.0556468,"ors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of competing translations from best to worst. Within this set-up, when presented with the same pair of MT output translations, human assessors often disagree with one another’s preference, and even their own previous judgment about which translation is better (Callison-Burch et al., 2007; Bojar et al., 2016). Low levels of inter-annotator agreement in human evaluation of MT not only cause problems with respect to the reliability of MT system evaluations, but unfortunately have an additional knock-on effect with respect to the meta-evaluation of metrics, in providing an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability"
D17-1262,E06-1032,0,0.0800317,"s of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT. 1 Introduction Despite it being known for some time now that automatic metrics, such as BLEU (Papineni et al., 2002), provide a less than perfect substitute for human assessment (Callison-Burch et al., 2006), evaluation in MT more often than not still comprises BLEU scores. Besides increased time and resources required by the alternative, human evaluation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranking (RR) of the output of five alternate MT systems, where they must rank the quality of com"
D17-1262,1993.eamt-1.1,0,0.575525,"Missing"
D17-1262,P16-2013,0,0.447493,"y legitimate variations in scoring strategies. Despite efforts to avoid bias in Graham et al. (2013), since DA is a monolingual evaluation of MT that operates via comparison of MT output with a reference translation, it is therefore still possible, while avoiding other sources of bias, that DA incurs reference bias where the level of superficial similarity of translations with reference translations results in an unfair gain, or indeed an unfair disadvantage for systems that yield translations that legitimately deviate from the surface form of reference translations. Following this intuition, Fomicheva and Specia (2016) carry out an investigation into bias in monolingual evaluation of MT and conclude that in a monolingual setting, human assessors of MT are strongly biased by the reference translation. In this paper, we provide further analysis of experiments originally provided in Fomicheva and Specia (2016), in addition to further investigation into the degree to which the intuition about reference bias can be supported. 2 Background Fomicheva and Specia (2016) provide an investigation into reference bias in monolingual evaluation of MT. 100 Chinese to English MT output translations are assessed by 25 human"
D17-1262,W13-2305,1,0.950422,"ors often disagree with one another’s preference, and even their own previous judgment about which translation is better (Callison-Burch et al., 2007; Bojar et al., 2016). Low levels of inter-annotator agreement in human evaluation of MT not only cause problems with respect to the reliability of MT system evaluations, but unfortunately have an additional knock-on effect with respect to the meta-evaluation of metrics, in providing an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges. DA collects assessments of translations separately in the form of both fluency and adequacy on a 0–100 rating scale, and, by combination of repeat judgments for translations, produces scores that have been shown to be highly reliable in self-replication experiments (Graham et al., 2015). The main component of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessme"
D17-1262,E14-1047,1,0.88998,"Missing"
D17-1262,N15-1124,1,0.852927,"an unstable gold standard. As such, provision of a fair and reliable human evaluation of MT remains a high priority for empirical evaluation. Direct assessment (DA) (Graham et al., 2013, 2014, 2016) is a relatively new human evaluation approach that overcomes previous challenges with respect to lack of reliability of human judges. DA collects assessments of translations separately in the form of both fluency and adequacy on a 0–100 rating scale, and, by combination of repeat judgments for translations, produces scores that have been shown to be highly reliable in self-replication experiments (Graham et al., 2015). The main component of DA used to provide a primary ranking of systems is adequacy, where the MT output is assessed via a monolingual similarity of meaning assessment. A reference translation is displayed to the human assessor (rendered in gray) and below it the MT output (in black), with the human judge asked to state the degree to which they agree that The black text adequately expresses the meaning of the gray text in English.1 The motivation behind 2476 1 Instructions are translated into a given target language. Proceedings of the 2017 Conference on Empirical Methods in Natural Language P"
D17-1262,P02-1040,0,0.111277,"ongly biased in this respect. On re-examination of past analyses, we identify a series of potential analytical errors that force some important questions to be raised about the reliability of past conclusions, however. We subsequently carry out further investigation into reference bias via direct human assessment of MT adequacy via quality controlled crowd-sourcing. Contrary to both intuition and past conclusions, results show no significant evidence of reference bias in monolingual evaluation of MT. 1 Introduction Despite it being known for some time now that automatic metrics, such as BLEU (Papineni et al., 2002), provide a less than perfect substitute for human assessment (Callison-Burch et al., 2006), evaluation in MT more often than not still comprises BLEU scores. Besides increased time and resources required by the alternative, human evaluation of systems, human assessment of MT faces additional challenges, in particular the fact that human assessors of translation quality tend to be highly inconsistent. In recent Conference on Ma‡ Qun Liu† Computing and Info Systems University of Melbourne tb@ldwin.net chine Translation (WMT) shared tasks, for example, manual evaluators complete a relative ranki"
D19-6301,W13-3520,0,0.0329649,"e, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track structures using a series of gra"
D19-6301,P11-2040,1,0.829216,"availability of evaluators: four Shallow Track in-domain datasets (Chinese-GSD, English-EWT, RussianSynTagRus, Spanish-AnCora), one Shallow Track dataset coming from parsed data (SpanishAnCoraHIT ) and one (in-domain) Deep Track dataset (English-EWT). As in SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), we assessed two quality criteria in the human evaluations, in separate evaluation experiments, Readability and Meaning Similarity, and used continuous sliders as rating tools, the evidence being that raters tend to prefer them 14 Thank you to Yevgeniy Puzikov for pointing this out. (Belz and Kow, 2011). Slider positions were mapped to values from 0 to 100 (best). Raters were first given brief instructions, including the direction to ignore formatting errors, superfluous whitespace, capitalisation issues, and poor hyphenation. The statement to be assessed in the Readability evaluation was: The text reads well and is free from grammatical errors and awkward constructions. The corresponding statement in the Meaning Similarity evaluation, in which system outputs (‘the black text’) were compared to reference sentences (‘the grey text’), was as follows: The meaning of the grey text is adequately"
D19-6301,W11-2832,1,0.65823,"ncies.org/ Bernd Bohnet Google Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from whic"
D19-6301,K17-3005,0,0.0303018,"bed by syntactic structure or agreement (such as verbal finiteness or verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the tra"
D19-6301,D19-6302,0,0.0736613,"tion is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions app"
D19-6301,D19-6303,0,0.0332354,"ighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ordering, and a character RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised sta"
D19-6301,W18-3606,0,0.194017,"andard scores (or z-scores) computed on the set of all raw scores by the given evaluator using each evaluator’s mean and standard deviation. For both raw and standard scores, we compute the mean of sentence-level scores. Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.15 4 Overview of Submitted Systems ADAPT is a sequence to sequence model with dependency features attached to word embeddings. A BERT sentence classifier was used as a reranker to choose between different hypotheses. The implementation is very similar to ADAPT’s SR’18 submission (Elder and Hokamp, 2018). The BME-UW system (Kov´acs et al., 2019) learns weighted rules of an Interpreted Regular Tree Grammar (IRTG) to encode the correspondence between word sequences and UDsubgraphs. For the inflection step, a standard sequence-to-sequence model with a biLSTM encoder and an LSTM decoder with attention is used. CLaC (Farahnak et al., 2019) is a pointer network trained to find the best order of the input. A slightly modified version of the transformer model was used as the encoder and decoder for the pointer network. The CMU (Du and Black, 2019) system uses a graph neural network for end-to-end ord"
D19-6301,W18-3604,0,0.0728895,"nto a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. Baseline: In order to set a lower boundary for the automatic and human evaluations, a simple English baseline consisting of 7 lines of python code was implemented16 . It generates from a UD file with an in-order traversal of the tree read by pyconll and outputting the form of each node. 5 Evaluation results There were 14 submissions to the task, of which two were withdrawn; 9 teams participa"
D19-6301,D19-6310,0,0.021759,"tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, which is then realised using a rule-based and a statistical machine translation (SMT) model. B"
D19-6301,D19-6304,0,0.339584,"Missing"
D19-6301,K18-2014,0,0.041076,"verbal number) was removed, whereas semanticlevel information such as nominal number and verbal tense was retained. UD2.3 version of the dataset, whereas CoNLL’18 used UD2.2; we selected treebanks that had not undergone major updates from one version to the next according to their README files on the UD site, and for which the best available parse reached a Labeled Attachment Score of 85 and over.11 There were datasets meeting these criteria for English (2), Hindi, Korean, Portuguese and Spanish; the Harbin HIT-SCIR parser (Che et al., 2017) had best scores on four of these datasets; LATTICE (Lim et al., 2018) and Stanford (Qi et al., 2019) had the best scores for the remaining two;12 see Table 3 for an overview. As is the case for all test data, in the additional automatically parsed test data alignments with surface tokens and with Shallow Track tokens are not provided; however, in the cases described in 4 above, the relative order is provided. 10. Fine-grained PoS labels found in some treebanks (see e.g. column 5 in Figure 2) were removed, and only coarse-grained ones were retained (column 4 in Figures 2 and 3). 11. In the training data, the alignments with the tokens of the Shallow Track struct"
D19-6301,D19-6311,0,0.0501754,"ter RNN for morphology. DepDist (Dyer, 2019) uses syntactic embeddings and a graph neural network with message passing to learn the tolerances for how far a dependent tends to be from its head. These directed 15 https://github.com/ygraham/ segment-mteval dependency distance tolerances form an edgeweighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projec"
D19-6301,W04-2705,0,0.268698,"Missing"
D19-6301,W18-3601,1,0.502146,"Inc. bohnetbd@google.com Leo Wanner ICREA and UPF, Barcelona leo.wanner@upf.edu growing, as is their size (and thus the volume of available training material).2 The SR tasks require participating systems to generate sentences from structures at the level of abstraction of outputs produced by state-of-the-art parsing. In order to promote linkage with parsing and earlier stages of generation, participants are encouraged to explore the extent to which neural network parsing algorithms can be reversed for generation. As was the case with its predecessor tasks SR’11 (Belz et al., 2011) and SR’18 (Mille et al., 2018), SR’19 comprises two tracks distinguished by the level of specificity of the inputs: Shallow Track (T1): This track starts from UD structures in which most of the word order information has been removed and tokens have been lemmatised. In other words, it starts from unordered dependency trees with lemmatised nodes that hold PoS tags and morphological information as found in the original treebank annotations. The task in this track therefore amounts to determining the word order and inflecting words. Deep Track (T2): This track starts from UD structures from which functional words (in particul"
D19-6301,W15-4719,0,0.125325,"The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity). The RALI system (Lapalme, 2019) uses a symbolic approach to transform the dependency tree into a tree of constituents that is transformed into an English sentence by an existing English realiser, JSrealB (Molins and Lapalme, 2015). This realiser was then slightly modified for the two tracks. Surfers (Hong et al., 2019) first performs delexicalisation to obtain a dictionary for proper names and numbers. A GCN is then used to encode the tree inputs, and an LSTM encoder-decoder with copy attention to generate delexicalised outputs. No part-of-speech tags, universal features or pretrained embeddings / language models are used. The Tilburg approach (Ferreira and Krahmer, 2019), based on Ferreira et al. (2018), realises multilingual texts by first preprocessing an input dependency tree into an ordered linearised string, whic"
D19-6301,J05-1004,0,0.0811396,"n the English-gum dataset);8 3. The lines corresponding to combined lexical units (e.g. Spanish “del” &lt;de+el&gt; lit. ’of.the’) and the contents of columns [9] and [10] were removed; 4. Information about the relative order of components of named entities, multiple coordinations and punctuation signs was added in the FEATS column (dependency relations compound, compound:prt, compound:svc, flat, flat:foreign, flat:name, fixed, conj, punct); For the Deep Track, the following steps were additionally carried out: 5. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the 8 Thank you to Guy Lapalme for spotting this. syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation. See also the inventory of relations in Table 2. 6. Functional prepositions"
D19-6301,P02-1040,0,0.108135,"a que los nuevos miembros del CNE deben tener experiencia para “dirigir procesos complejos”. In the original UD files, the reference sentences are by default detokenised. In order to carry out the evaluations of the tokenised outputs, we built a tokenised version of the reference sentences by concatenating the words of the second column of the UD structures (see Figure 1) separated by a whitespace. 3 Evaluation Methods 3.1 Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST13 is a related n-gram similarity metric 13 http://www.itl.nist.gov/iad/mig/ tests/mt/doc/ngram-study.pdf; http:// www.itl.nist.gov/iad/mig/tests/mt/2009/ weighted in favor of less frequent n-grams which are taken to be more informative. DIST starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn t"
D19-6301,N18-1202,0,0.019743,"however, permissible. For example, available parsers such as UUParser (Smith et al., 2018) could be run to create a silver standard versions of provided datasets and use them as additional or alternative training material, and publicly available off-the3 In the case of one team, we agreed to move the two week window between test data release and submission to one week earlier. 4 At SR’18, there were ten languages from five families. 5 https://www.aclweb.org/portal/ content/sigmorphon-shared-task-2019 6 universaldependencies.org shelf language models such as GPT-2 (Radford et al., 2019), ELMo (Peters et al., 2018), polyglot (Al-Rfou et al., 2013) or BERT (Devlin et al., 2018) could be fine-tuned with publicly available datasets such as WikiText (Merity et al., 2016) or the DeepMind Q&A Dataset (Hermann et al., 2015). Datasets were created for 11 languages in the Shallow Track, and for three of those languages, namely English, French and Spanish, in the Deep Track. As in 2018, Shallow Track inputs were generated with the aid of Python scripts from the original UD structures, this time using all available input sentences. Deep Track inputs were then generated by automatically processing the Shallow Track"
D19-6301,D19-6312,0,0.0504468,"els use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to group the words to be contracted, and then generate the contracted word form of each group with a seq2seq model. The LORIA submission (Shimorina and Gardent, 2019) presents a modular approach to surface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. Th"
D19-6301,K18-2011,1,0.820368,"Missing"
D19-6301,D19-6309,0,0.0269112,"rface realisation with three subsequent steps: word ordering, morphological inflection, and contraction generation (for some languages). For word ordering, the data is delexicalised, the input tree is linearised, and the mapping between an input tree and output lemma sequence is learned using a factored sequence-to-sequence model. Morphological inflection makes use of a neural characterbased model, which produces word forms based on lemmas coupled with morphological features; finally, a rule-based contraction generation module is applied for some languages. The OSU-FB pipeline for generation (Upasani et al., 2019) starts by generating inflected word forms in the tree using character seq2seq models. These inflected syntactic trees are then linearised as constituent trees by converting the relations to non-terminals. The linearised constituent trees are fed to seq2seq models (including models with copy and with tree-LSTM encoders) whose outputs also contain tokens marking the tree structure. N-best outputs are obtained for orderings and the highest confidence output sequence with a valid tree is chosen (i.e, one where the input and output trees are isomorphic up to sibling order, ensuring projectivity)."
D19-6301,D19-6306,0,0.0857639,"weighted directed acyclic graph (DAG) (equivalent to a partially ordered set, or poset) for each sentence, the topological sort of which generates a surface order. Inflection is addressed with regex patterns and substitutions approximating productive inflectional paradigms. The DipInfoUnito realiser (Mazzei and Basile, 2019) is a supervised statistical system for surface realisation, in which two neural network-based models run in parallel on the same input structure, namely a list-wise learning to rank network for linearisation and a seq2seq network for morphology inflection prediction. IMS (Yu et al., 2019) uses a pipeline approach for both tracks, consisting of linearisation, completion (for T2 only), inflection, and contraction. All models use the same bidirectional Tree-LSTM encoder architecture. The linearisation model orders each subtree separately with beam search and then combines them into a full projective tree; the completion model generates absent function words in a sequential way given the linearised tree of content words; the inflection model predicts a sequence of edit operations to convert the lemma to word form character by character; the contraction model predicts BIO tags to g"
E14-1047,N12-1017,0,0.0150144,"if that system would have ranked lower if the severity of its inferior translation outputs were taken into account. Rather than directly aiming to increase human judge consistency, some methods instead increase the number of reference translations available to automatic metrics. HTER (Snover et al., 2006) employs humans to post-edit each system output, creating individual human-targeted reference translations which are then used as the basis for computing the translation error rate. HyTER, on the other hand, is a tool that facilitates creation of very large numbers of reference translations (Dreyer and Marcu, 2012). Although both approaches increase fairness compared to automatic metrics that use a single generic reference translation, even human post-editors will inevitably vary in the way they post-edit translations, and the process of creating even a single new reference transRecent human evaluation of machine translation has focused on relative preference judgments of translation quality, making it difficult to track longitudinal improvements over time. We carry out a large-scale crowd-sourcing experiment to estimate the degree to which state-of-theart performance in machine translation has increase"
E14-1047,J08-4004,0,0.0485306,"human assessor to specify how strongly they agree or disagree with that statement. The scale and labels can then be held constant across experimental set-ups for all attributes evaluated – meaning that if the scale is still biased in some way it will be equally so across all set-ups. 3 Assessor Consistency One way of estimating the quality of a human evaluation regime is to measure its consistency: whether or not the same outcome is achieved if the same question is asked a second time. In MT, annotator consistency is commonly measured using Cohen’s kappa coefficient, or some variant thereof (Artstein and Poesio, 2008). Originally developed as a means of establishing assessor independence, it is now commonly used in the reverse sense, with high numeric values being used as evidence of agreement. Two different measurements can be made – whether a judge is consistent with other judgments performed by themselves (intraannotator agreement), and whether a judge is consistent with other judges (inter-annotator agreement). Cohen’s kappa is intended for use with categor2 This dimension of the assessment is similar but not identical to the monolingual adequacy assessment in early NIST evaluation campaigns (NIST, 200"
E14-1047,W13-2305,1,0.581936,"is presented with a set of judgments for translations from two systems, one of which is known to produce better translations than the other, the mean score for the better system will be significantly higher than that of the inferior system. Assumption B is the basis of our quality-control mechanism, and allows us to distinguish between Turkers who are working carefully and those who are merely going through the motions. We use a 100-judgment HIT structure to control same-judge repeat items and deliberately-degraded system outputs (bad reference items) used for workerintrinsic quality control (Graham et al., 2013). bad reference translations for fluency judgments are created as follows: two words in the translation are randomly selected and randomly re-inserted elsewhere in the sentence (but not as the initial or final words of the sentence). Since adding duplicate words will not degrade adequacy in the same way, we use an alternate method to create bad reference items for adequacy judgments: we randomly delete a short sub-string of length proportional to the length of the original translation to emulate a missing phrase. Since 446 Si 70 randomly selected system outputs made up of roughly equal proport"
E14-1047,W05-0909,0,0.0473555,"r each language pair, a 100-translation HIT was constructed by randomly selecting translations from the pool of (3003 + 2007) × 2 that were available, and this results in apparently fewer assessments for the 2007 test set. In fact, numbers of evaluated translations are relative to the size of each test set. Average z scores for each system are also presented, based on the mean and standard deviation of all assessments provided by an individual worker, with positive values representing deviations above the mean of workers. In addition, we include mean B LEU (Papineni et al., 2001) and M ETEOR (Banerjee and Lavie, 2005) automatic scores for the same system outputs. The C URR benchmark shows fluency scores that are 5.9 points higher on the 2007 data set than they are on the 2012 test data, with a larger difference in adequacy of 8.3 points. As such, the 2012 test data is more challenging than the 2007 test data. Despite this, both fluency and adequacy scores for the best system in 2012 have increased by 4.5 and 2.0 points respectively, amounting to estimated average gains of 10.4 points in fluency and 10.3 points in adequacy for state-of-the-art MT across the seven language pairs. Looking at the standardized"
E14-1047,P11-1023,0,0.0255157,"Missing"
E14-1047,W13-2203,0,0.0479048,"m for human evaluation is WMT shared tasks, where assessments have (since 2007) taken the form of ranking five alternate system outputs from best to worst (Bojar et al., 2013). This method has been shown to produce more consistent judgments compared to fluency and adequacy judgments on a five-point scale (CallisonBurch et al., 2007). However, relative preference judgments have been criticized for being a simplification of the real differences between translations, not sufficiently taking into account the large number of different types of errors of varying severity that occur in translations (Birch et al., 2013). Relative preference judgments do not take into account the degree to which one translation is better than another – there is no way of knowing if a winning system produces far better translations than all other systems, or if that system would have ranked lower if the severity of its inferior translation outputs were taken into account. Rather than directly aiming to increase human judge consistency, some methods instead increase the number of reference translations available to automatic metrics. HTER (Snover et al., 2006) employs humans to post-edit each system output, creating individual"
E14-1047,W11-2101,0,0.0426779,"Missing"
E14-1047,2001.mtsummit-papers.68,0,0.0585449,"o test sets (C URR07 , C URR12 ). For each language pair, a 100-translation HIT was constructed by randomly selecting translations from the pool of (3003 + 2007) × 2 that were available, and this results in apparently fewer assessments for the 2007 test set. In fact, numbers of evaluated translations are relative to the size of each test set. Average z scores for each system are also presented, based on the mean and standard deviation of all assessments provided by an individual worker, with positive values representing deviations above the mean of workers. In addition, we include mean B LEU (Papineni et al., 2001) and M ETEOR (Banerjee and Lavie, 2005) automatic scores for the same system outputs. The C URR benchmark shows fluency scores that are 5.9 points higher on the 2007 data set than they are on the 2012 test data, with a larger difference in adequacy of 8.3 points. As such, the 2012 test data is more challenging than the 2007 test data. Despite this, both fluency and adequacy scores for the best system in 2012 have increased by 4.5 and 2.0 points respectively, amounting to estimated average gains of 10.4 points in fluency and 10.3 points in adequacy for state-of-the-art MT across the seven langu"
E14-1047,W07-0718,0,0.365461,"ly qualified workers. With this set-up in place for adequacy, we also re-introduce a fluency assessment. Fluency ratings can be carried out without the presence of a reference translation, reducing any remnant bias towards reference translations in the evaluation setup. That is, we propose a judgment regime in which each task is presented as a two-item fluency and adequacy judgment, evaluated separately, and with adequacy restructured into a monolingual “similarity of meaning” task. When fluency and adequacy were originally used for human evaluation, each rating used a 5point adjective scale (Callison-Burch et al., 2007). However, adjectival scale labels are problematic and ratings have been shown to be highly dependent on the exact wording of descriptors (Seymour et al., 1985). Alexandrov (2010) provides a summary of the extensive problems associated with the use of adjectival scale labels, including bias resulting from positively- and negatively-worded items not being true opposites of one another, and items intended to have neutral intensity in fact proving to have specific conceptual meanings. It is often the case, however, that the question could be restructured so that the rating scale no longer require"
E14-1047,2006.amta-papers.25,0,0.208879,"erent types of errors of varying severity that occur in translations (Birch et al., 2013). Relative preference judgments do not take into account the degree to which one translation is better than another – there is no way of knowing if a winning system produces far better translations than all other systems, or if that system would have ranked lower if the severity of its inferior translation outputs were taken into account. Rather than directly aiming to increase human judge consistency, some methods instead increase the number of reference translations available to automatic metrics. HTER (Snover et al., 2006) employs humans to post-edit each system output, creating individual human-targeted reference translations which are then used as the basis for computing the translation error rate. HyTER, on the other hand, is a tool that facilitates creation of very large numbers of reference translations (Dreyer and Marcu, 2012). Although both approaches increase fairness compared to automatic metrics that use a single generic reference translation, even human post-editors will inevitably vary in the way they post-edit translations, and the process of creating even a single new reference transRecent human e"
E14-1047,W12-3102,0,\N,Missing
E14-1047,P02-1040,0,\N,Missing
E14-1047,W13-2201,0,\N,Missing
E17-2057,N15-1124,1,0.941061,"slation quality and the actual quality of translated documents. Including such weights in the construction of a gold standard potentially invalidates the human evaluation, and is unfortunately very likely to exaggerate the apparent performance of some systems while under-rewarding others. 357 0.2 0.1 3 0.0 −0.2 0 10 20 30 40 Alternate Human Gold Standard A recent development in human evaluation of MT is direct assessment (“DA”), a human assessment shown to yield highly replicable segment-level scores, by combination of a minimum of 15 repeat human assessments per translation into mean scores (Graham et al., 2015). Human adequacy assessments are collected via a 0–100 rating scale that facilitates reliable quality control of crowd-sourcing. Document-level DA scores are computed by repeat assessment of the individual segments within a given document, computation of the mean score for each segment (micro-average), and finally, combination of the mean segment scores into an overall mean document score (macro-average).2 DA assessments are carried out by comparison of a given MT output segment (rendered in black) with a human-generated reference translation (in gray), and human annotators rate the degree to"
E17-2057,C16-1294,1,0.865252,"hree participating sys5 Post-editing cost estimates are based on 0.06 and 0.12 Euro per source document word converted to USD$. Further details provided by the post-editor in relation to estimates can be found at https://github.com/ygraham/ eacl2017 4 Variance in numbers of repeat assessments per document is due to sentences of all documents being sampled without preference for documents made up of larger numbers of sentences. 359 RTM-FS+PLS-TREE GRAPH-DISC BASE-EMB-GP BASELINE RTM-FS-SVR DA WMT-16 0.38 0.32 0.31 0.26 0.23 0.36 0.26 0.39 0.29 0.29 it in document-level QE evaluation therefore. Graham et al. (2016a) provide an investigation into reference bias in monolingual evaluation of MT and despite the risk of reference bias that DA adequacy could potentially encounter, experiment results show no evidence of reference bias. Human assessors of MT appear to genuinely read and compare the meaning of the reference translation and the MT output, as requested with DA, applying their human intelligence to the task in a reliable way, and are not overly influenced by the generic reference. Although DA fluency could still have its own applications, for the purpose of evaluating MT or MT QE, this additional"
E17-2057,P15-1174,1,0.837171,"ments (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison"
E17-2057,E06-1032,0,0.076271,"m (2015). When evaluated with the Pearson correlation, such a set of predictions would not be a reasonable entry to the shared task since the prediction distribution would effectively be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiabl"
E17-2057,W07-0718,0,0.0579243,"proving Evaluation of Document-level Machine Translation Quality Estimation Yvette Graham Dublin City University Qingsong Ma Chinese Academy of Sciences Timothy Baldwin University of Melbourne yvette.graham@dcu.ie maqingsong@ict.ac.cn tb@ldwin.net Qun Liu Dublin City University Carla Parra Dublin City University Carolina Scarton University of Sheffield qun.liu@dcu.ie carla.parra@adaptcentre.ie c.scarton@sheffield.ac.uk Abstract subjective, making high IAA difficult to achieve. For example, in past large-scale human evaluations of MT, low IAA levels have been highlighted as a cause of concern (Callison-Burch et al., 2007; Bojar et al., 2016). Such problems cause challenges not only for evaluation of MT systems, but also for MT quality estimation (QE), where the ideal gold standard comprises human assessment. Meaningful conclusions about the relative performance of NLP systems are only possible if the gold standard employed in a given evaluation is both valid and reliable. In this paper, we explore the validity of human annotations currently employed in the evaluation of document-level quality estimation for machine translation (MT). We demonstrate the degree to which MT system rankings are dependent on weight"
E17-2057,2006.amta-papers.25,0,0.11857,"ely be a constant and its correlation with anything is therefore undefined. Regardless of the predictability of automatic metric scores when evaluated with MAE, they unfortunately do not provide a suitable gold standard, simply because they are known to provide an insufficient substitute for human assessment, often unfairly penalizing translations that happen to be superficially dissimilar to reference translations (Callison-Burch et al., 2006). Consequently, for WMT-16, the gold standard was modified to take the form of a linear combination of two human-targeted translation edit rate (HTER) (Snover et al., 2006) scores assigned to a given document. Scores were produced via two human post-editing steps: firstly, sentences within a given MT-output document were post-edited independent of other sentences in that document, producing post-edition 1 (P E1 ). Secondly, P E1 sentences were concatenated to form a documentlevel translation, and post-edited a second time by the same annotator, with the aim of isolating errors only identifiable when more context is available, to produce post-edition 2 (P E2 ). Next, two translation edit rate (TER) scores were computed by: (1) comparing the document-level MT outp"
E17-2057,W11-2107,0,0.0349009,"machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system as good as the baseline system when evaluated with MAE. The fact that average scores are good predictors is more likely a consequence of the applied evaluation measure, MAE, however, as outlined in Graham (2015). When evaluated with the Pearson correlation, s"
E17-2057,P10-1063,0,0.0362197,"(IAA) enable the likelihood of replicability to be taken into account, were an evaluation to be repeated with a distinct set of human annotators. One approach to achieving high IAA is through the development of a strict set of annotation guidelines, while for machine translation (MT), human assessment is more 356 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 356–361, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics 2 Background label, G, as follows: Document-level QE (Soricut and Echihabi, 2010) is a relatively new area, with only two shared tasks taking place to date (Bojar et al., 2015; Bojar et al., 2016). In WMT-15, gold standard labels took the form of automatic metric scores for documents (specifically Meteor scores (Denkowski and Lavie, 2011)), and system predictions were compared to gold labels via MAE. A conclusion that emerged from the initial shared task was that automatic metric scores were not adequate, based on the following observation: if the average of the training set scores is used as a prediction value for all data points in the test set, this results in a system"
E17-2057,D14-1020,1,0.877386,"ct assessment (DA) and original gold standard (WMT-16 QE English to Spanish) tems, while under-rewarding two other systems. Notably, system GRAPH-DISC, which includes discourse features learned from document-level features, achieves a higher correlation when evaluated with DA compared to the original gold standard. Differences in correlations are small, however, and can’t be interpreted as differences in performance without significance testing. Differences in dependent correlations showed no significant difference for all pairs of competing systems according to Williams test (Williams, 1959; Graham and Baldwin, 2014). 3.3 4 Conclusion Methodological concerns were raised with respect to optimization of weights employed in construction of document-level QE gold standards in WMT-16. We demonstrated the degree to which MT system rankings are dependent on weights employed in the construction of the gold standard. Experiments showed with respect to the alternate gold standard we propose, direct assessment (DA), scores for documents are highly reliable, achieving a correlation of above 0.9 in a self-replication experiment. Finally, DA resulted in a substantial estimated cost reduction, with the original post-edi"
E17-2057,W13-2305,1,0.877135,"by qualitycontrolled crowd-sourcing in two separate data collection runs (Runs A and B) on Mechanical Turk, and compare scores for individual documents collected in each run. Quality control is carried out by inclusion of pairs of genuine MT outputs and automatically degraded versions of them (bad references) within 100-translation HITs, before a difference of means significance test is applied to the ratings belonging to a given worker. The resulting p-value is employed as an estimate of the reliability of a given human assessor to accurately distinguish between the quality of translations (Graham et al., 2013; Graham et al., 2014). Table 1 shows numbers of judgments collected in total for each data collection run on Mechanical Turk, including numbers of assessments before and after quality control filtering, where only data belonging to workers with a p-value below 0.05 were retained. Figure 2 shows the correlation between document-level DA scores collected in Run A with scores produced in Run B, where, for Run B, repeat assessments are down-sampled to show the increasing correspondence between scores as ever-increasing numbers of repeat assessments are collected for a given document. Correlation"
E17-2057,E14-1047,1,\N,Missing
E17-2057,W16-2301,1,\N,Missing
N15-1124,W07-0718,0,0.144209,"ood that an individual metric’s correlation with human judgment is not equal to zero. In addition, data sets for evaluation in both document and segment-level metrics are not independent and the correlation that exists between pairs of metrics should also be taken into account by significance tests. 3 Segment-Level Human Evaluation Many human evaluation methodologies attempt to elicit precisely the same quality judgment for individual translations from all assessors, and inevitably produce large numbers of conflicting assessments in the process, including from the same individual human judge (Callison-Burch et al., 2007; CallisonBurch et al., 2008; Callison-Burch et al., 2009). An alternative approach is to take into account the fact that different judges may genuinely disagree, and allow assessments provided by individuals to each contribute to an overall estimate of the quality of a given translation. In an ideal world in which we had access to assessments provided by the entire population of qualified human assessors, for example, the mean of those assessments would provide a statistic that, in theory at least, would provide a meaningful segment-level human score for translations. If it were possible to c"
N15-1124,W08-0309,0,0.101169,"Missing"
N15-1124,W11-2107,0,0.0326716,"evaluation data. Figure 5 displays the outcome of the Williams significance test as applied to each pairing of competing metrics. Since the power of Williams test increases with the strength of correlation between a pair of metrics, it is important not to conclude the best system by the number of other metrics it outperforms. Instead, the best choice of metric for that language pair is any metric that is not signicifantly outperformed by any other metric. Three metrics prove not to be significantly outperformed by any other metric for Spanish-to-English, and tie for best performance: METEOR (Denkowski and Lavie, 2011), NLEPOR (Han et al., 2013) and SENT BLEU- MOSES (sBLEU-moses). 1189 0.05 0.1 Figure 5: Evaluation of significance of increase in correlation with human judgment between every pair of segment-level metrics competing in the Spanish-toEnglish W MT-13 metrics task. A colored cell (i,j) indicates that system named in row i significantly outperforms system named in column j at p &lt; 0.1, and green cells at p &lt; 0.05. Metric METEOR NLEPOR SENT BLEU- MOSES SIMP BLEU P SIMP BLEU R LEPOR r 0.441 0.416 0.422 0.418 0.404 0.326 Table 3: Pearson’s correlation between each W MT-13 segment-level metric and huma"
N15-1124,D14-1020,1,0.427255,"nor indeed did the evaluation ever directly compare translations for different source language inputs (as relative preference judgments were always relative to other translations for the same input). Pearson’s correlation, on the other hand, compares scores across the entire test set. 4.1 Significance Testing of Segment-level Metrics With the move to Pearson’s correlation, we can also test statistical significance in differences between metrics, based on the Williams test (Williams, 1959),3 which evaluates significance in a difference in dependent correlations (Steiger, 1980). As suggested by Graham and Baldwin (2014), the test is appropriate for evaluation of document-level MT metrics since the data is not independent, and for similar reasons, the test can also be used for evaluation of segment-level metrics. 4.2 Spanish-to-English Segment-level Metrics We first carry out tests for Spanish-to-English segment-level metrics from W MT-13. In our experiments in Section 3.1, we used only a sub-sample 3 Also sometimes referred to as the Hotelling–Williams test. 1188 METEOR NLEPOR SENT BLEU- MOSES DEP - REF - EX DEP - REF - A SIMP BLEU P SIMP BLEU R LEPOR UMEANT MEANT TERRORCAT r τ 0.484 0.483 0.465 0.453 0.453"
N15-1124,W13-2305,1,0.776539,"ents must be collected for a given segment to obtain mean segment scores that truly reflects translation quality? Scores are sampled according to annotation time to simulate a realistic setting. 3.1 Translation Assessment Sample Size MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance). The HITS were structured to include degraded translations and repeat translations, and rated on a continuous Likert scale with a single translation assessment displayed to the assessor at one time (Graham et al., 2014a; Graham et al., 2013). This supports accurate quality-control as well as normalisation of translation scores for each assessor. The assessment 3 3 −3 −2 −1 0 1 2 3 1 0 −1 −2 −3 −2 −1 0 1 2 3 0 N = 10 −1 1 2 3 0 1 2 3 1 2 3 3 3 −1 0 1 2 r = 0.97 −3 −2 −1 0 1 2 r = 0.91 −2 −1 −2 N=5 −3 −3 −2 −1 0 1 2 r = 0.86 −2 −3 N=2 3 N=1 −3 r = 0.76 −3 −3 −2 −1 0 1 2 r = 0.6 2 3 −3 −2 −1 0 1 2 r = 0.41 −3 −2 −1 0 1 N = 15 2 3 −3 −2 −1 0 N = 40 Figure 2: Plots and correlation (r) of translation quality assessments in the initial (x-axis) and replicate experiments (yaxis) for Spanish-to-English over W MT-13, where each point repre"
N15-1124,E14-1047,1,0.149696,"tion: how many assessments must be collected for a given segment to obtain mean segment scores that truly reflects translation quality? Scores are sampled according to annotation time to simulate a realistic setting. 3.1 Translation Assessment Sample Size MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance). The HITS were structured to include degraded translations and repeat translations, and rated on a continuous Likert scale with a single translation assessment displayed to the assessor at one time (Graham et al., 2014a; Graham et al., 2013). This supports accurate quality-control as well as normalisation of translation scores for each assessor. The assessment 3 3 −3 −2 −1 0 1 2 3 1 0 −1 −2 −3 −2 −1 0 1 2 3 0 N = 10 −1 1 2 3 0 1 2 3 1 2 3 3 3 −1 0 1 2 r = 0.97 −3 −2 −1 0 1 2 r = 0.91 −2 −1 −2 N=5 −3 −3 −2 −1 0 1 2 r = 0.86 −2 −3 N=2 3 N=1 −3 r = 0.76 −3 −3 −2 −1 0 1 2 r = 0.6 2 3 −3 −2 −1 0 1 2 r = 0.41 −3 −2 −1 0 1 N = 15 2 3 −3 −2 −1 0 N = 40 Figure 2: Plots and correlation (r) of translation quality assessments in the initial (x-axis) and replicate experiments (yaxis) for Spanish-to-English over W MT-13,"
N15-1124,W14-3333,1,0.814581,"tion: how many assessments must be collected for a given segment to obtain mean segment scores that truly reflects translation quality? Scores are sampled according to annotation time to simulate a realistic setting. 3.1 Translation Assessment Sample Size MTurk was used to collect large numbers of translation assessments, in sets of 100 translations per assessment task (or “HIT” in MTurk parlance). The HITS were structured to include degraded translations and repeat translations, and rated on a continuous Likert scale with a single translation assessment displayed to the assessor at one time (Graham et al., 2014a; Graham et al., 2013). This supports accurate quality-control as well as normalisation of translation scores for each assessor. The assessment 3 3 −3 −2 −1 0 1 2 3 1 0 −1 −2 −3 −2 −1 0 1 2 3 0 N = 10 −1 1 2 3 0 1 2 3 1 2 3 3 3 −1 0 1 2 r = 0.97 −3 −2 −1 0 1 2 r = 0.91 −2 −1 −2 N=5 −3 −3 −2 −1 0 1 2 r = 0.86 −2 −3 N=2 3 N=1 −3 r = 0.76 −3 −3 −2 −1 0 1 2 r = 0.6 2 3 −3 −2 −1 0 1 2 r = 0.41 −3 −2 −1 0 1 N = 15 2 3 −3 −2 −1 0 N = 40 Figure 2: Plots and correlation (r) of translation quality assessments in the initial (x-axis) and replicate experiments (yaxis) for Spanish-to-English over W MT-13,"
N15-1124,W13-2253,0,0.0188496,"the outcome of the Williams significance test as applied to each pairing of competing metrics. Since the power of Williams test increases with the strength of correlation between a pair of metrics, it is important not to conclude the best system by the number of other metrics it outperforms. Instead, the best choice of metric for that language pair is any metric that is not signicifantly outperformed by any other metric. Three metrics prove not to be significantly outperformed by any other metric for Spanish-to-English, and tie for best performance: METEOR (Denkowski and Lavie, 2011), NLEPOR (Han et al., 2013) and SENT BLEU- MOSES (sBLEU-moses). 1189 0.05 0.1 Figure 5: Evaluation of significance of increase in correlation with human judgment between every pair of segment-level metrics competing in the Spanish-toEnglish W MT-13 metrics task. A colored cell (i,j) indicates that system named in row i significantly outperforms system named in column j at p &lt; 0.1, and green cells at p &lt; 0.05. Metric METEOR NLEPOR SENT BLEU- MOSES SIMP BLEU P SIMP BLEU R LEPOR r 0.441 0.416 0.422 0.418 0.404 0.326 Table 3: Pearson’s correlation between each W MT-13 segment-level metric and human assessment for the combin"
N15-1124,W04-3250,0,0.28857,"Missing"
N15-1124,W14-3336,0,0.0917597,"Missing"
N15-1124,W09-0401,0,\N,Missing
N15-1124,aziz-etal-2012-pet,0,\N,Missing
N15-1124,2012.eamt-1.31,0,\N,Missing
N15-1124,W14-3302,0,\N,Missing
N15-1124,W13-2201,0,\N,Missing
N15-1124,2012.tc-1.5,0,\N,Missing
N16-1001,D14-1020,1,0.72989,"Missing"
N16-1001,W13-2305,1,0.802481,"rank order in the original sample and hybrid super-sample evaluations are marked with • and confidence intervals that do 0.50 TerrorCat 0.45 25 20 BLEU 30 0.55 not include zero are in bold. −0.4 −0.2 0.0 0.2 −0.4 Standardized Human −0.2 0.0 0.2 Standardized Human Figure 4: Human, T ERROR C AT and B LEU scores for 10k super-sampled hybrid MT systems for WMT-12 Spanish-to-English. 7 more challenging, but the method of human evaluation we employ facilitates the straight-forward computation of human scores for vast numbers of systems directly from the original human evaluation of only n systems. Graham et al. (2013) provide a human evaluation of MT that elicits adequacy assessments of translations, independent of other translations on a fine-grained 100-point rating scale. After score standardization to iron-out differences in individual human assessor scoring strategies, the overall human score for a MT system is simply computed as the mean of the ratings attributed to its translations, and this facilitates the straight-forward computation of a human score for any hybrid system from the original human evaluation of n systems. To demonstrate, we replicate a previous year’s WMT metrics shared task, constr"
N16-1001,W13-2202,0,0.266957,"Missing"
N16-1001,W14-3336,0,0.178616,"University qliu@computing.dcu.ie Abstract scores for a sample of MT systems correlate with human assessment of that same set of systems. A main venue for evaluation of MT metrics is the annual Workshop for Statistical Machine Translation (WMT) (Bojar et al., 2015) where large-scale human evaluation takes place, primarily for the purpose of ranking systems competing in the translation shared task, but additionally to use the resulting system rankings for evaluation of automatic metrics. Since 2014, WMT has used the Pearson correlation as the official measure for evaluation of metrics (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT systems, therefore. Table 1 shows correlations with human assessment of each of the metrics participating in the Czech-to-English component of WMT14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, R ED S YS S ENT (Wu et al., 2014), with the popular metric B LEU (Papineni et al., 2001), this involves comparison of the correlation point estimate of R ED S YS S ENT, r"
N16-1001,2001.mtsummit-papers.68,0,0.342751,"d the Pearson correlation as the official measure for evaluation of metrics (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT systems, therefore. Table 1 shows correlations with human assessment of each of the metrics participating in the Czech-to-English component of WMT14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, R ED S YS S ENT (Wu et al., 2014), with the popular metric B LEU (Papineni et al., 2001), this involves comparison of the correlation point estimate of R ED S YS S ENT, r = 0.993, with the weaker correlation point estimate of B LEU, r = 0.909, with both computed with reference to human assessment of a sample of 5 MT systems. Automatic Machine Translation metrics, such as B LEU, are widely used in empirical evaluation as a substitute for human assessment. Subsequently, the performance of a given metric is measured by its strength of correlation with human judgment. When a newly proposed metric achieves a stronger correlation over that of a baseline, it is important to take into ac"
N16-1001,W15-3031,0,0.0698975,"Missing"
N16-1001,W14-3355,1,0.825922,"ion of automatic metrics. Since 2014, WMT has used the Pearson correlation as the official measure for evaluation of metrics (Mach´acˇ ek and Bojar, 2014; Stanojevi´c et al., 2015). Comparison of the performance of any two metrics involves the comparison of two Pearson correlation point estimates computed over a sample of MT systems, therefore. Table 1 shows correlations with human assessment of each of the metrics participating in the Czech-to-English component of WMT14 metrics shared task, and, for example, if we wish to compare the performance of the top-performing metric, R ED S YS S ENT (Wu et al., 2014), with the popular metric B LEU (Papineni et al., 2001), this involves comparison of the correlation point estimate of R ED S YS S ENT, r = 0.993, with the weaker correlation point estimate of B LEU, r = 0.909, with both computed with reference to human assessment of a sample of 5 MT systems. Automatic Machine Translation metrics, such as B LEU, are widely used in empirical evaluation as a substitute for human assessment. Subsequently, the performance of a given metric is measured by its strength of correlation with human judgment. When a newly proposed metric achieves a stronger correlation o"
N16-1001,P02-1040,0,\N,Missing
P15-1174,C04-1046,0,0.183448,"servative variance in prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Components of W MT-13 and W MT-14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks. 1 Introduction Machine Translation (MT) Quality Estimation (QE) is the automatic prediction of machine translation quality without the use of reference translations (Blatz et al., 2004; Specia et al., 2009). Human assessment of translation quality in theory provides the most meaningful evaluation of systems, but human assessors are known to be inconsistent and this causes challenges for quality estimation evaluation. For instance, there is a general lack of consensus both with respect to what provides the most meaningful gold standard representation, as well as best method of comparison of gold labels and system predictions. For example, in the 2014 Workshop on Statistical Machine Translation (WMT), which since 2012 has provided a main venue for evaluation of systems, sente"
P15-1174,D14-1020,1,0.684683,"e shift in location and scale that can be used to boost apparent performance of systems when measures like MAE and RMSE, that are not unit-free, are employed. Increasing the distance between system predictions and gold labels for anything less than the entire distribution, a more realistic scenario, or by something other than a constant across the entire distribution, will result in an appropriately weaker Pearson correlation. 4 Quality Estimation Significance Testing Previous work has shown the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015), and, for similar reasons, Williams test is appropriate for significance testing differences in performance of competing quality estimation systems which we detail further below. Evaluation of a given quality estimation system, Pnew , by Pearson correlation takes the form of quantifying the correlation, r(Pnew , G), that exists between system prediction scores and corresponding gold standard labels, and contrasting this correlation with the correlation for some baseline system, r(Pbase , G). At first it might seem reasonable to perform significance testing in the followi"
P15-1174,W14-3333,1,0.814452,"-rtm-tree DFKI-svr FBK-UPV-UEDIN-nowp SHEFF-lite-sparse Multilizer baseline DFKI-svr-xdata SHEFF-lite Original MAE Rescaled MAE 0.129 0.134 0.136 0.140 0.143 0.144 0.150 0.150 0.152 0.161 0.182 0.125 0.127 0.133 0.129 0.132 0.137 0.141 0.135 0.149 0.146 0.168 Table 2: MAE of W MT-14 Task 1.2 systems for original HTER prediction distributions and when distributions are shifted and rescaled to the mean and half the standard deviation of the gold label distribution. cance testing such as bootstrap resampling in combination with BLEU and other metrics have been empirically evaluated (Koehn, 2004; Graham et al., 2014), to the best of our knowledge no research has been carried out to assess the accuracy of similar methods specifically for quality estimation evaluation. In addition, since data used for evaluation of quality estimation systems are not independent, methods of significance testing differences in performance will be inaccurate unless the dependent nature of the data is taken into account. 3 Quality Estimation Evaluation by Pearson Correlation The Pearson correlation is a measure of the linear correlation between two variables, and in the case of quality estimation evaluation this amounts to the"
P15-1174,N15-1124,1,0.395006,"ale that can be used to boost apparent performance of systems when measures like MAE and RMSE, that are not unit-free, are employed. Increasing the distance between system predictions and gold labels for anything less than the entire distribution, a more realistic scenario, or by something other than a constant across the entire distribution, will result in an appropriately weaker Pearson correlation. 4 Quality Estimation Significance Testing Previous work has shown the suitability of Williams significance test (Williams, 1959) for evaluation of automatic MT metrics (Graham and Baldwin, 2014; Graham et al., 2015), and, for similar reasons, Williams test is appropriate for significance testing differences in performance of competing quality estimation systems which we detail further below. Evaluation of a given quality estimation system, Pnew , by Pearson correlation takes the form of quantifying the correlation, r(Pnew , G), that exists between system prediction scores and corresponding gold standard labels, and contrasting this correlation with the correlation for some baseline system, r(Pbase , G). At first it might seem reasonable to perform significance testing in the following manner when an incr"
P15-1174,W04-3250,0,0.142277,"vr USHEFF DCU-rtm-tree DFKI-svr FBK-UPV-UEDIN-nowp SHEFF-lite-sparse Multilizer baseline DFKI-svr-xdata SHEFF-lite Original MAE Rescaled MAE 0.129 0.134 0.136 0.140 0.143 0.144 0.150 0.150 0.152 0.161 0.182 0.125 0.127 0.133 0.129 0.132 0.137 0.141 0.135 0.149 0.146 0.168 Table 2: MAE of W MT-14 Task 1.2 systems for original HTER prediction distributions and when distributions are shifted and rescaled to the mean and half the standard deviation of the gold label distribution. cance testing such as bootstrap resampling in combination with BLEU and other metrics have been empirically evaluated (Koehn, 2004; Graham et al., 2014), to the best of our knowledge no research has been carried out to assess the accuracy of similar methods specifically for quality estimation evaluation. In addition, since data used for evaluation of quality estimation systems are not independent, methods of significance testing differences in performance will be inaccurate unless the dependent nature of the data is taken into account. 3 Quality Estimation Evaluation by Pearson Correlation The Pearson correlation is a measure of the linear correlation between two variables, and in the case of quality estimation evaluatio"
P15-1174,C14-1208,0,0.0286973,"prediction of aggregates is, in general, far easier than individual predictions. In addition, inclusion of confounding test set aggregates such as these in evaluations will likely lead to both an overestimate of the ability of some systems to predict the quality of unseen translations and an underestimate of the accuracy of systems that courageously attempt to predict the quality of translations in the tails of gold distributions, and it follows that systems optimized for MAE can be expected to perform badly when predicting the quality of translations in the tails of gold label distributions (Moreau and Vogel, 2014). Table 2 shows how MAEs of original predicted score distributions for all systems participating in Task 1.2 W MT-14 can be reduced by shifting and rescaling the prediction score distribution according to gold label aggregates. Table 3 shows that for similar reasons other measures commonly applied to evaluation of quality estimation systems, such as root mean squared error (RMSE), that are also not unit-free, encounter the same problem. 2.3 Significance Testing In quality estimation, it is common to apply bootstrap resampling to assess the likelihood that a decrease in MAE (an improvement) has"
P15-1174,W09-0441,0,0.0310935,"than PETs, we compute correlations of each with HTER gold labels of translations from Task 1.2. Table 6 reveals a significantly higher correlation that exists between PER and HTER compared to PET and HTER (p &lt; 0.001) , and we conclude therefore that the PER of a translation provides a more faithful representation of translation quality than PET, and convert PETs for both predictions and gold labels to PERs (in seconds per word) in our later replication of Task 1.3. In Task 1.2 of W MT-14, gold standard labels used to evaluate systems were in the form of human translation error rates (HTERs) (Snover et al., 2009). HTER scores provide an effective representation for evaluation of quality estima1805 1.0 1.0 1 2 (a) Gold Labels 3 0.8 0.0 0.0 0.2 0.2 0.4 0.4 0.6 0.6 0.8 1.0 0.8 0.6 0.4 0.2 0.0 MAE: 0.603 MAE: 0.687 1 2 3 (b) System A Predictions 0.5 1.0 1.5 2.0 2.5 3.0 (c) System B Predictions 3.5 Figure 1: W MT-14 English-to-German quality estimation Task 1.1 where mismatched prediction/gold labels achieves apparent better performance, where (a) gold label distribution; (b) example system disadvantaged by its discrete predictions; (c) example system gaining advantage by its continuous predictions. tion s"
P15-1174,2009.eamt-1.5,0,0.112804,"n prediction score distributions. As an alternative, we propose the use of the unit-free Pearson correlation, in addition to providing an appropriate method of significance testing improvements over a baseline. Components of W MT-13 and W MT-14 quality estimation shared tasks are replicated to reveal substantially increased conclusivity in system rankings, including identification of outright winners of tasks. 1 Introduction Machine Translation (MT) Quality Estimation (QE) is the automatic prediction of machine translation quality without the use of reference translations (Blatz et al., 2004; Specia et al., 2009). Human assessment of translation quality in theory provides the most meaningful evaluation of systems, but human assessors are known to be inconsistent and this causes challenges for quality estimation evaluation. For instance, there is a general lack of consensus both with respect to what provides the most meaningful gold standard representation, as well as best method of comparison of gold labels and system predictions. For example, in the 2014 Workshop on Statistical Machine Translation (WMT), which since 2012 has provided a main venue for evaluation of systems, sentence-level systems were"
P15-1174,W14-3302,0,\N,Missing
S13-2024,C10-3010,1,0.864322,"Missing"
S13-2024,S13-1030,1,0.621199,"Missing"
S13-2024,2005.mtsummit-papers.11,0,0.0241262,"directional, forward, backward, no entailment. We take the compositional approach and separately train a forward, as well as a backward binary classifier. Each classifier is run separately on the set of text fragment pairs to produce two binary labels for forward and backward entailment. The two sets of labels are logically combined to produce a final classification for each test pair of forward, backward, bidirectional or no entailment. 3 Word Alignment Features The test set of topically-related text fragments, T1 (German) and T2 (English) were added to Europarl German–English parallel text (Koehn, 2005) and Giza++ was used for automatic word alignment in both language directions. Moses (Koehn et al., 2007) was then used for symmetrization with the grow diag final and algorithm. This produces a many-to-many alignment between the words of the 133 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 133–137, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics The number of words in T2 (English) that are not aligned with anything in T1 (German) should provide a"
S13-2024,N10-1045,0,0.0617989,"orward and backward before a compositional combination into the final four classes, as well as experiments with additional string similarity features. 1 2 Introduction Cross-lingual Textual Entailment (CLTE) (Negri et al., 2012) proposes the task of automatically identifying the kind of relation that exists between pairs of semantically-related text fragments written in two distinct languages, a variant of the traditional Recognizing Textual Entailment (RTE) task (Bentivogli et al., 2009; Bentivogli et al., 2010). The task targets the cross-lingual content synchronization scenario proposed in Mehdad et al. (2010, 2011). Compositional classification can be used by training two distinct binary classifiers for forward and backward entailment classification, before combining labels into the four final entailment categories that now include bidirectional and no entailment labels. The most similar previous work to this work is the crosslingual approach of the FBK system (Mehdad et al., 2012) from Semeval 2012 (Negri et al., 2012), in which the entailment classification is obtained Compositional Classification Given a pair of topically related fragments, T1 (German) and T2 (English), we automatically annota"
S13-2024,P11-1134,0,0.127608,"Missing"
S13-2024,S12-1105,0,0.0246471,"Missing"
S13-2024,D11-1062,0,0.159966,"Missing"
S13-2024,S12-1053,0,0.16135,"Missing"
S13-2024,W99-0604,0,0.290489,"Missing"
S13-2024,S13-1039,1,0.868654,"Missing"
S13-2024,P07-2045,0,\N,Missing
S13-2024,W07-1401,0,\N,Missing
U12-1010,2010.amta-papers.20,0,0.785958,"endeavoring to ﬁnd better ways of collecting and assessing translation quality. Considering just how important human assessment of translation quality is to empirical machine translation, although there is a signiﬁcant amount of research into developing metrics that correlate with human judgments of translation quality, the underlying topic of ﬁnding ways of increasing the reliability of those judgments to date has received a limited amount of attention (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Denkowski and Lavie, 2010). 4 “translation quality” is not a psychological construct as such, we believe these methods of measurement and validation could be used to develop more reliable and valid measures of translation quality. Psychological constructs are measured indirectly, with the task of deﬁning and measuring a construct known as operationalizing the construct. The task requires examination of the mutual or commonsense understanding of the construct to come up with a set of items that together can be used to indirectly measure it. In psychology, the term construct validity refers to the degree to which inferen"
U12-1010,W06-3114,0,0.156869,"rence set of translation pairs (Ikehara et al., 1994). While this methodology is capable of capturing longitudinal progress for a given MT system, it is prohibitively expensive and doesn’t scale well to multisystem comparison. The annual workshop for statistical machine translation (WMT) has, over recent years, been the main forum for collection of human assessment of translation quality, despite this not being the main focus of the workshop (which is to provide a regular cross-system comparison over standardized datasets for a variety of language pairs by means of a shared translation task) (Koehn and Monz, 2006; CallisonBurch et al., 2007; Callison-Burch et al., 2008; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011; Callison-Burch et al., 2012). Figure 2 shows the approaches used for human judgments of translation quality at the annual workshops. To summarize, across the ﬁeld of machine translation human judges have been asked to assess translation quality in a variety of ways: adequacy being a two-item assessment); • Using different labels (for example, asking which translation is better or asking which is more adequate); • Ordinal level scales (ranking a numbe"
U12-1010,P02-1040,0,0.0942272,"ations. To alleviate these concerns, direct human judgments of translation quality are also collected when possible. During the evaluation of MT shared tasks, for example, human judgments of MT outputs have been used to determine the ranking of participating systems. The same human judgments can also be used in the evaluation of automatic measures, by comparing the degree to which automatic scores (or ranks) of translations correlate with them. This aspect of MT measurement is discussed shortly. One well-known example of an automatic metric is the BLEU (bilingual evaluation understudy) score (Papineni et al., 2002). Computation of a BLEU score for a system, based on a set of candidate translations it has generated, requires only that sets of corresponding reference translations be made available, one per candidate. The ease – and repeatability – of such testing has meant that BLEU is popular as a translation effectiveness measure. But that popularity does not bestow any particular superiority, and, BLEU suffers from drawbacks (Callison-Burch et al., 2006). (As an aside, we note that in all such repeatable scoring arrangements, every subsequent experiment must be designed so that there is clear separatio"
U12-1010,2003.mtsummit-papers.44,0,0.0172878,"scratch (White et al., 1994). Adequacy is the degree to which the information in the source language string is preserved in the translation,1 while ﬂuency is the determination of whether the translation is a well-formed utterance in the target language and ﬂuent in context. Subsequently, many of the large corporate machine translation systems used regression testing to establish whether changes or new modules had a positive impact on machine translation quality. Annotators were asked to select which of two randomly-ordered translations (one from each system) they preferred (Bond et al., 1995; Schwartz et al., 2003), and this was often performed over a reference set of translation pairs (Ikehara et al., 1994). While this methodology is capable of capturing longitudinal progress for a given MT system, it is prohibitively expensive and doesn’t scale well to multisystem comparison. The annual workshop for statistical machine translation (WMT) has, over recent years, been the main forum for collection of human assessment of translation quality, despite this not being the main focus of the workshop (which is to provide a regular cross-system comparison over standardized datasets for a variety of language pair"
U12-1010,1994.amta-1.25,0,0.679144,"Missing"
U12-1010,W11-2101,0,0.0624707,"Missing"
U12-1010,E06-1032,0,0.0414097,"them. This aspect of MT measurement is discussed shortly. One well-known example of an automatic metric is the BLEU (bilingual evaluation understudy) score (Papineni et al., 2002). Computation of a BLEU score for a system, based on a set of candidate translations it has generated, requires only that sets of corresponding reference translations be made available, one per candidate. The ease – and repeatability – of such testing has meant that BLEU is popular as a translation effectiveness measure. But that popularity does not bestow any particular superiority, and, BLEU suffers from drawbacks (Callison-Burch et al., 2006). (As an aside, we note that in all such repeatable scoring arrangements, every subsequent experiment must be designed so that there is clear separation between training and test data, to avoid any risk of hill-climbing and hence over-ﬁtting.) Automatic Measurement of MT The automatic evaluation of MT system output has long been an objective of MT research, with several of the recommendations of the early ALPAC Report (ALPAC, 1966), for example, relating to evaluation: 1. Practical methods for evaluation of translations; . . . 3. Evaluation of quality and cost of various sources of translation"
U12-1010,W07-0718,0,0.47099,"a more robust approach is to ﬁnd ways of increasing the reliability of the human judgments we use as the yard-stick for automatic metrics by endeavoring to ﬁnd better ways of collecting and assessing translation quality. Considering just how important human assessment of translation quality is to empirical machine translation, although there is a signiﬁcant amount of research into developing metrics that correlate with human judgments of translation quality, the underlying topic of ﬁnding ways of increasing the reliability of those judgments to date has received a limited amount of attention (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Denkowski and Lavie, 2010). 4 “translation quality” is not a psychological construct as such, we believe these methods of measurement and validation could be used to develop more reliable and valid measures of translation quality. Psychological constructs are measured indirectly, with the task of deﬁning and measuring a construct known as operationalizing the construct. The task requires examination of the mutual or commonsense understanding of the construct to come up with a set of"
U12-1010,W08-0309,0,0.291115,"ﬁnd ways of increasing the reliability of the human judgments we use as the yard-stick for automatic metrics by endeavoring to ﬁnd better ways of collecting and assessing translation quality. Considering just how important human assessment of translation quality is to empirical machine translation, although there is a signiﬁcant amount of research into developing metrics that correlate with human judgments of translation quality, the underlying topic of ﬁnding ways of increasing the reliability of those judgments to date has received a limited amount of attention (Callison-Burch et al., 2007; Callison-Burch et al., 2008; Przybocki et al., 2009; Callison-Burch et al., 2009; Callison-Burch et al., 2010; Denkowski and Lavie, 2010). 4 “translation quality” is not a psychological construct as such, we believe these methods of measurement and validation could be used to develop more reliable and valid measures of translation quality. Psychological constructs are measured indirectly, with the task of deﬁning and measuring a construct known as operationalizing the construct. The task requires examination of the mutual or commonsense understanding of the construct to come up with a set of items that together can be u"
U12-1010,W12-3102,0,\N,Missing
U12-1010,W09-0401,0,\N,Missing
U12-1010,W10-1703,0,\N,Missing
U13-1004,2001.mtsummit-papers.68,0,0.0462489,"between systems, and to be an accurate proxy for the properties of the systems being studied. For machine translation (MT), measurement has been a combination of human judgments and automated measurements. With the aim of removing system biases and creating robust comparisons, there has been extensive use of workshops and shared tasks such as the ongoing Workshops on Statistical Machine Translation (WMT) and the NIST Open Machine Translation (OpenMT) evaluations. The basis of system evaluation is generally human judgments, which have also been used to evaluate automatic metrics such as BLEU (Papineni et al., 2001), under the assumption that a metric that correlates strongly with human judgments is more valid than a metric with weak correlation. Human evaluation of MT thus forms the foundation of evaluation in empirical MT, regardless of whether a particular evaluation makes use of human judges or automatic metrics. The current methodology used for the task of human evaluation in MT is problematic, however, as assessments carried out by expert judges are highly inconsistent. Even when a single expert judge is asked to assess the same pair of translations in two separate sittings, the second judgment is"
U13-1004,P11-2040,0,0.0303281,"Figure 1: Screen shot for base configuration for fluency assessments, including 100 point visual analog scale (VAS), marked but not labeled at 25-50-75. ments for Spanish and French. The reverse occurs for translation into German, however, where less than one third of completed Human Intelligence Tasks (HITs) were carried out by workers that reached the quality control threshold. clude seven language pairs across all participating systems from WMT 2012 (Callison-Burch et al., 2012). Previous work has shown the advantages of collecting judgments on a continuous rating scale for NLP evaluation (Belz and Kow, 2011) in general, as well as for MT evaluation specifically (Graham et al., 2013), as shown in Figure 1. This approach allows judge-intrinsic quality control to be introduced, so that non-experts can be used, as well as permitting standardization of scores and longitudinal evaluation. We adopt this approach and ask AMT workers to assess the fluency of translations on a continuous rating scale. Since we are primarily concerned with design of the assessment configuration so as to improve the consistency of human judgments, and not with ranking of systems, we limit our assessment to evaluating fluency"
U13-1004,D08-1027,0,0.0785006,"Missing"
U13-1004,W10-0701,0,0.0139675,"ference of means test undertaken, and the resulting p-value then used as a reliability estimate. A threshold (for example, p < 0.05) can then be applied to select the reliable workers. Careless judges have a high p-value, while judges who are both skilled and conscientious have a low p-value. This relationship can be validated by direct inspection of the judgments performed. 4 (1) 5 AMT Lessons Amazon’s Mechanical Turk and other crowdsourcing services are widely used in NLP to collect data (Snow et al., 2008), with guides available that provide advice on how best to make use of such services (Callison-Burch and Dredze, 2010). Whether engaging with crowd-sourcing services such as AMT as a requester or worker, however, there is some degree of risk, primarily because of the anonymity that is assured by the services. The requester, in providing payment for potentially large volumes of work, is vulnerable to substandard or even robotically completed HITs. In this regard there is a clear sense of “buyer beware” that is part and parcel of using crowd-sourcing services. The worker, on the other hand, earns a relatively low hourly rate, and faces an ongoing risk of having completed HITs declined and of not being reimburse"
U13-1004,W07-0718,0,0.0382128,"nt” with a phrase more commonly used to refer to language, that is, whether the text is clearly written, denoted in Table 1 as the written approach. The third dimension is whether or not to include a reference translation (the “south” variant in Figure 2). An assessment of fluency independent of adequacy and without a reference translation provides at least one part of an overall evaluation that will not be biased in favor of systems that happen to produce reference-like translations. However, in the past, fluency judgments have generally been carried out with a reference translation present (Callison-Burch et al., 2007). In this part of the evaluation the instructions described the task as assessing automatic translations as opposed to a simple rating of the fluency of the text, since without this context it would be difficult to explain what a reference in fact was. With each translation that was presented a note was displayed on screen to the users as follows: An equivalent piece of fluent text is provided in gray for your reference. The final dimension explored (the “north” variant in Figure 2) is the effect of the presence of source language words in translations. Many of 18 Configuration Anchor labels Q"
U13-1004,J11-2010,0,0.0141478,"part and parcel of using crowd-sourcing services. The worker, on the other hand, earns a relatively low hourly rate, and faces an ongoing risk of having completed HITs declined and of not being reimbursed for diligently completed work. Recently developed online tools provide slightly more power to workers, by enabling requester reviewing and hence allowing workers to identify requesters who too readily reject completed HITs (Irani and Silberman, 2013). And even when workers are paid, rather than volunteers, payment rates are well below the minimum wages that apply in most developed countries (Fort et al., 2011). Judge Consistency Table 2 shows consistency of human judges for judgments of translations repeated by the same and distinct judges. Mean scores for same judge ask again repeat items show no significant difference. At the same time, mean scores for degraded 20 Human Ethics Posting HITs on a service such as AMT amounts to research involving humans, and human ethics potentially becomes a concern (Gilles et al., 2011; Fort et al., 2011). Research institutes tend to evolve their own specific human ethics policies for crowd-sourcing tasks. In our particular institution, a two-stage procedure for h"
U13-1004,U12-1010,1,0.848717,"n general, as well as for MT evaluation specifically (Graham et al., 2013), as shown in Figure 1. This approach allows judge-intrinsic quality control to be introduced, so that non-experts can be used, as well as permitting standardization of scores and longitudinal evaluation. We adopt this approach and ask AMT workers to assess the fluency of translations on a continuous rating scale. Since we are primarily concerned with design of the assessment configuration so as to improve the consistency of human judgments, and not with ranking of systems, we limit our assessment to evaluating fluency. Graham et al. (2012) suggest translation quality should be measured as a hypothetical construct, where measurements that employ more items (dimensions of measurement) as opposed to fewer are considered more valid. Under this criterion, a two-item (fluency and adequacy) scale is more valid than a single-item translation quality measure, further motivating the inclusion of fluency as an assessment item for measurement of translation quality. Overall, just under half of the Turkers carried out the human evaluation to a standard that met our quality control threshold. In addition, proportions of good quality workers"
U13-1004,W13-2305,1,0.798038,"ding 100 point visual analog scale (VAS), marked but not labeled at 25-50-75. ments for Spanish and French. The reverse occurs for translation into German, however, where less than one third of completed Human Intelligence Tasks (HITs) were carried out by workers that reached the quality control threshold. clude seven language pairs across all participating systems from WMT 2012 (Callison-Burch et al., 2012). Previous work has shown the advantages of collecting judgments on a continuous rating scale for NLP evaluation (Belz and Kow, 2011) in general, as well as for MT evaluation specifically (Graham et al., 2013), as shown in Figure 1. This approach allows judge-intrinsic quality control to be introduced, so that non-experts can be used, as well as permitting standardization of scores and longitudinal evaluation. We adopt this approach and ask AMT workers to assess the fluency of translations on a continuous rating scale. Since we are primarily concerned with design of the assessment configuration so as to improve the consistency of human judgments, and not with ranking of systems, we limit our assessment to evaluating fluency. Graham et al. (2012) suggest translation quality should be measured as a h"
U13-1004,W12-3102,0,\N,Missing
U13-1004,P02-1040,0,\N,Missing
U13-1004,W13-2201,0,\N,Missing
W09-3823,W02-1503,0,0.0308938,"Missing"
W09-3823,W02-1506,0,0.0407156,"Missing"
W09-3823,2005.mtsummit-papers.11,0,0.0122338,"Missing"
W09-3823,P02-1040,0,0.0798233,"Missing"
W09-3823,P02-1035,0,\N,Missing
W10-3815,W08-0319,0,0.037095,"Missing"
W10-3815,D07-1091,0,0.0605128,"Missing"
W10-3815,2005.mtsummit-papers.11,0,0.017517,"Missing"
W10-3815,N03-1017,0,0.006063,"t in the string, as in traditional language models, but instead using the deeper context of a word. In this paper, we explore the potential of deep syntax language models providing an interesting comparison with the traditional string-based language model. We include an experimental evaluation that compares the two kinds of models independently of any MT system to investigate the possible potential of integrating a deep syntax language model into Hierarchical SMT systems. 1 Introduction In Phrase-Based Models of Machine Translation all phrases consistent with the word alignment are extracted (Koehn et al., 2003), with shorter phrases needed for high coverage of unseen data and longer phrases providing improved fluency in Leaving aside heuristic language modeling for a moment, the difficulty of integrating a traditional string-based language model into the decoding process in a hierarchical system, highlights a slight incongruity between the translation model and language model in Hierarchical Models. According to the translation model, the best way to build a fluent TL translation is via discontiguous phrases, while the language model can only provide information about the fluency of contiguous seque"
W10-3815,J07-2003,0,0.181076,"iang, 2005) build on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase. Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discontiguous sequences of TL words increases the difficulty of computing language model probabilities during decoding and require sophisticated heuristic language modeling techniques (Chiang, 2007; Chiang, 2005). Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local"
W10-3815,N06-1032,0,0.0172942,"ency tree SMT system for ChineseEnglish translation, using information from the deeper structure about dependency relations between words, in addition to the position of the words in the string, including information about whether context words were positioned on the left or right of a word. Bojar and Hajiˇc (2008) use a deep syntax language model in an English-Czech dependency tree-to-tree transfer system, and include three separate bigram language models: a reverse, direct and joint model. The model in our evaluation is similar to their direct bigram model, but is not restricted to bigrams. Riezler and Maxwell (2006) use a trigram deep syntax language model in German-English dependency tree-to-tree transfer to re-rank decoder output. The language model of Riezler and Maxwell (2006) is similar to the model in our evaluation, but differs in that it is restricted to a trigram model trained on LFG f-structures. In addition, as language modeling is not the main focus of their work, they provide little detail on the language model they use, except to say that it is based on “log-probability of strings of predicates from root to frontier of target f-structure, estimated from predicate trigrams in English f-struc"
W10-3815,P05-1033,0,0.0582532,"ild on PhraseBased Models by relaxing the constraint that phrases must be contiguous sequences of words and allow a short phrase (or phrases) nested within a longer phrase to be replaced by a non-terminal symbol forming a new hierarchical phrase. Traditional language models use the local context of words to estimate the probability of the sentence and introducing hierarchical phrases that generate discontiguous sequences of TL words increases the difficulty of computing language model probabilities during decoding and require sophisticated heuristic language modeling techniques (Chiang, 2007; Chiang, 2005). Hierarchical Models increase the reordering capabilities of MT systems by introducing non-terminal symbols to phrases that map source language (SL) words/phrases to the correct position in the target language (TL) translation. Building translations via discontiguous TL phrases increases the difficulty of language modeling, however, introducing the need for heuristic techniques such as cube pruning (Chiang, 2005), for example. An additional possibility to aid language modeling in hierarchical systems is to use a language model that models fluency of words not using their local context in the"
W10-3815,P02-1035,0,0.0171643,"the parser, whereas a string-based language model can be trained on any available data of the appropriate language. Since parser coverage is not the focus of our work, we eliminate its effects from the evaluation by selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens strings LFG lemmas/predicates 138.6M 118.4M Ave. Tokens per Sent. 19 16 Vocab 345K 28"
W10-3815,N04-1013,0,0.0160487,"st be in-coverage of the parser, whereas a string-based language model can be trained on any available data of the appropriate language. Since parser coverage is not the focus of our work, we eliminate its effects from the evaluation by selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens strings LFG lemmas/predicates 138.6M 118.4M Ave. Tokens per Se"
W10-3815,W02-1506,0,0.0132038,"y selecting the training and test data for both the stringbased and deep syntax language models on the basis that they are in fact in-coverage of the parser. 8.1 Language Model Training Our training data consists of English sentences from the WMT09 monolingual training corpus with sentence length range of 5-20 words that are in coverage of the parsing resources (Kaplan et al., 2004; Riezler et al., 2002) resulting in approximately 7M sentences. Preparation of training and test data for the traditional language model consisted of tokenization and lower casing. Parsing was carried out with XLE (Kaplan et al., 2002) and an English LFG grammar (Kaplan et al., 2004; Riezler et al., 2002). The parser produces a packed representation of all possible parses according to the LFG grammar and we select only the single best parse for language model training by means of a disambiguation model (Kaplan et 123 Corpus Tokens strings LFG lemmas/predicates 138.6M 118.4M Ave. Tokens per Sent. 19 16 Vocab 345K 280K Table 1: Language model statistics for string-based and deep syntax language models, statistics are for string tokens and LFG lemmas for the same set of 7.29M English sentences al., 2004; Riezler et al., 2002)."
W10-3815,P08-1066,0,0.0287187,"nt and the deeper context of a word, if available, may provide more meaningful information and result in better lexical choice. Integrating such a model into a Hierarchical SMT system is not straightforward, however, and we believe before embarking on this its worthwhile to evaluate the model independently of any MT system. We therefore provide an experimental evaluation of the model and in order to provide an interesting comparison, we evaluate a traditional string-based language model on the same data. 2 Related Work The idea of using a language model based on deep syntax is not new to SMT. Shen et al. (2008) use a dependency-based language model in a string to dependency tree SMT system for ChineseEnglish translation, using information from the deeper structure about dependency relations between words, in addition to the position of the words in the string, including information about whether context words were positioned on the left or right of a word. Bojar and Hajiˇc (2008) use a deep syntax language model in an English-Czech dependency tree-to-tree transfer system, and include three separate bigram language models: a reverse, direct and joint model. The model in our evaluation is similar to t"
W10-3815,P07-2045,0,\N,Missing
W10-3815,P98-2230,0,\N,Missing
W10-3815,C98-2225,0,\N,Missing
W10-4006,P04-1023,0,0.0433803,"sed combined with IBM Models 3 and 4. Although theoretically it is possible to incorporate partial annotation with a small change in its code, Graca et al. do not mention it. Secondly, Talbot (2005) introduces a constrained EM method which constrains the E-step to incorporate partial alignment into word alignment,1 which is in a similar manner to Graca et al. (2007). He conducted experiments using partial alignment annotation based on cognate relations, a bilingual dictionary, domain-specific bilingual semantic annotation, and numerical pattern matching. He did not incorporate BMWEs. Thirdly, Callison-Burch et al. (2004) replace the likelihood maximization in the M-step with mixed likelihood maximization, which is a convex combination of negative log likelihood of known links and unknown links. The remainder of this paper is organized as follows: in Section 2 we define the anchor word alignment problem. In Section 3 we include a review of the EM algorithm with IBM Models 1-5, and the HMM Model. Section 4 describes our own algorithm based on the combination of BMWE extraction and the modified word alignment which incorporates the groupings of BMWEs and enforces their alignment links; we explain the EM algorith"
W10-4006,P91-1023,0,0.194889,"on semantic space that incorporate a notion of order in the bag-of-words model (e.g. co-occurences). Some aligned corpora include implicit partial alignment annotation, while for other corpora a partial alignment can be extracted by state-ofthe-art techniques. For example, implicit tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporat"
W10-4006,N03-1017,0,0.00316695,"Word Alignment Tsuyoshi Okita1 , Alfredo Maldonado Guerra2 , Yvette Graham3 , Andy Way1 {CNGL1 , NCLT3 } / School of Computing / Dublin City University, CNGL / School of Computer Science and Statistics / Trinity College Dublin2 {tokita,ygraham,away}@computing.dcu.ie, maldonaa@scss.tcd.ie Abstract 2007) remains key to providing high-quality translations as all subsequent training stages rely on its performance. It alone does not effectively capture many-to-many word correspondences, but instead relies on the ability of subsequent heuristic phrase extraction algorithms, such as grow-diagfinal (Koehn et al., 2003), to resolve them. This paper presents a new word alignment method which incorporates knowledge about Bilingual Multi-Word Expressions (BMWEs). Our method of word alignment first extracts such BMWEs in a bidirectional way for a given corpus and then starts conventional word alignment, considering the properties of BMWEs in their grouping as well as their alignment links. We give partial annotation of alignment links as prior knowledge to the word alignment process; by replacing the maximum likelihood estimate in the M-step of the IBM Models with the Maximum A Posteriori (MAP) estimate, prior k"
W10-4006,2005.mtsummit-papers.11,0,0.00743896,"We then perform MERT while a 5-gram language model is trained with SRILM. Our implementation is based on a modified version of GIZA++ (Och and Ney, 2003a). This modification is on the function that reads a bilingual terminology file, the function that calculates priors, the M-step in IBM Models 1-5, and the forward-backward algorithm in the HMM Model. Other related software tools are written in Python and Perl: terminology concatenation, terminology numbering, and so forth. 6 Experimental Results We conduct an experimental evaluation on the NTCIR-8 corpus (Fujii et al., 2010) and on Europarl (Koehn, 2005). Firstly, MWEs are extracted from both corpora, as shown in Table 3. In the second step, we apply our modified version of GIZA++ in which we incorporate the results of 6 This is because it needs to maintain potentially an ℓ × m matrix, where ℓ denotes the number of English tokens in the corpus and m denotes the number of foreign tokens, even if the matrix is sparse. Prior Model I only requires an ℓˆ × m ˆ matrix where ℓˆ is the number of English tokens in a sentence and m ˆ is the number of foreign tokens in a sentence, which is only needed until this information is incorporated in a posterio"
W10-4006,P07-2045,0,0.00825313,"Missing"
W10-4006,J10-4005,0,0.00742961,"apping objects. Table 1 shows two example phrase pairs for French to English c’est la vie and that is life, and la vie en rose and rosy life with the initial value for the EM algorithm, the prior value and the fi1 Although the code may be similar in practice to our Prior Model I, his explanation to modify the E-step will not be applied to IBM Models 3 and 4. Our view is to modify the M-step due to the same reason above, i.e. GIZA++ searches only over the alignment space around the Viterbi alignment. 27 bilities t(ei |fj ). It is noted that we use e|f rather than f |e following the notation of Koehn (2010). One important remark is that the Viterbi alignment of the sentence pair (˘ e, f˘) = (eJ1 , f1I ), which is obtained as in (1): Statistical MWE extraction method 97 groupe socialiste socialist group 26 26 101 monsieur poettering mr poettering 1 4 103 monsieur poettering mr poettering 1 11 110 monsieur poettering mr poettering 1 9 117 explication de vote explanation of vote 28 26 Eviterbi : aˆJ1 = arg max pθˆ(f, a|e) (1) Heuristic-based MWE extraction method aJ 1 28 the wheel 2 車輪 ２ 25 5 28 the primary-side fixed armature 13 １ 次 側 固 定 電機 子 １ ３ 13 9 28 the secondary-side rotary magnet 7 ２ 次 側 回"
W10-4006,P93-1003,0,0.929921,"ld ) + log p(t) t 3.2 HMM A first-order Hidden Markov Model (Vogel et al., 1996) uses the sentence length probability p(J|I), the mixture alignment probability p(i|j, I), and the translation probability, as in (4): p(f |e) = p(J|I) J Y j=1 p(fj |ei ) 4.1 (4) PI r(i − j JI ) ′ i′ =1 r(i − j JI ) (5) The HMM alignment probabilities p(i|i′ , I) depend only on the jump width (i − i′ ). Using a set of non-negative parameters s(i − i′ ), we have (6): p(i|i′ , I) = s(i − i′ ) PI l=1 s(l − i′ ) MWE Extraction Our algorithm of extracting MWEs is a statistical method which is a bidirectional version of Kupiec (1993). Firstly, Kupiec presents a method to extract bilingual MWE pairs in a unidirectional manner based on the knowledge about typical POS patterns of noun phrases, which is languagedependent but can be written down with some ease by a linguistic expert. For example in French they are N N, N prep N, and N Adj. Secondly, we take the intersection (or union) of extracted bilingual MWE pairs.2 Suppose we have a training set of R observation sequences Xr , where r = 1, · · · , R, each of which is labelled according to its class m, where m = 1, · · · , M , as in (5): p(i|j, I) = Our Approach 2 In word a"
W10-4006,W06-2402,0,0.714233,"it tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate (McLach1 Introduction Word alignment (Brown et al., 1993; Vogel et al., 1996; Och"
W10-4006,E03-1035,0,0.0189154,"extracted by state-ofthe-art techniques. For example, implicit tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate (McLach1"
W10-4006,W04-3243,0,0.0276884,"Missing"
W10-4006,J03-1002,0,0.0111181,"006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate or the Maximum Penalized Likelihood (MPL) estimate (McLach1 Introduction Word alignment (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2003a; Graca et al., 26 Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 26–34, Beijing, August 2010 pair EN-FR is NULL rosy en that . life la . c‘ that c‘ is est life vie rosy rose lan and Krishnan, 1997; Bishop, 2006). Then, the MAP estimate allows us to incorporate the prior, a probability used to reflect the degree of prior belief about the occurrences of the events. A small number of studies have been carried out that use partial alignment annotation for word alignment. Firstly, Graca et al. (2007) introduce a posterior regularization to"
W10-4006,A97-1050,0,0.0505851,"lignment annotation, while for other corpora a partial alignment can be extracted by state-ofthe-art techniques. For example, implicit tags such as reference number within the patent corpus of Fujii et al. (2010) provide (often many-tomany) correspondences between source and target words, while statistical methods for extracting a partial annotation, like Kupiec et al. (1993), extract terminology pairs using linguistically predefined POS patterns. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Resnik and Melamed (1997) automatically extract domainspecific lexica. Moore (2003) extracts namedentities. In Machine Translation, Lambert and Banchs (2006) extract BMWEs from a phrase table, which is an outcome of word alignment followed by phrase extraction; this method does not alter the word alignment process. This paper introduces a new method of incorporating previously known many-to-many word correspondences into word alignment. A well-known method of incorporating such prior knowledge in Machine Learning is to replace the likelihood maximization in the M-step of the EM algorithm with either the MAP estimate o"
W10-4006,P03-1010,0,0.0224125,"cuments corpora,3 we can use heuristics to extract the “noun phrase” + “reference number” from both sides. This is due to the fact that terminology is often labelled with a unique reference number, which is labelled on both the SL and TL sides. 4.2 Prior Model I Prior for Exhaustive Alignment Space IBM Models 1 and 2 implement a prior for all possible 3 Unlike other language pairs, the availability of Japanese–English parallel corpora is quite limited: the NTCIR patent corpus (Fujii et al., 2010) of 3 million sentence pairs (the latest NTCIR-8 version) for the patent domain and JENAAD corpus (Utiyama and Isahara, 2003) of 150k sentence pairs for the news domain. In this regard, the patent domain is particularly important for this particular language pair. 30 Algorithm 3 Prior Model I for IBM Model 1 Given: parallel corpus e˘, f˘, anchor words biT erm initialize t(e|f ) uniformly do until convergence set count(e|f ) to 0 for all e,f set total(f) to 0 for all f for all sentence pairs (˘ es ,f˘s ) prior(e|f )s = getPriorModelI(˘ e, f˘, biT erm) for all words e in e˘s totals (e) = 0 for all words f in f˘s totals (e) += t(e|f ) for all words e in e˘s for all words f in f˘s count(e|f )+=t(e|f )/totals (e)× prior("
W10-4006,J93-1004,0,\N,Missing
W10-4006,J93-2003,0,\N,Missing
W10-4006,C96-2141,0,\N,Missing
W13-2259,D11-1079,0,0.0182733,"e translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et al. (2010) apply target-side dependency constraints for the same language pair and direction in addition to Arabic to English, Peter et al. (2011) also apply dependency constraints on the target side, but rather soft constraints that can be relaxed in the case that an ill-formed structure does in fact yield a better translation. Gao et al. (2011) similarly apply soft dependency constraints but to the source side for Chinese to English translation, and Galley and Manning (2009) show several advantages to using maximum spanning tree non-projective dependency parsing decoding for Chinese to English translation. Li et al. (2012), although not constraining with dependency structure, instead create non-terminals with part-of-speech tag combinations for Chinese words identified as heads for translation into English. did  the  crisis  begin  ? Figure 3: Non-projective Dependency Structure for many languages, increasingly so for languages"
W13-2259,W10-1749,0,0.04961,"Missing"
W13-2259,N03-1017,0,0.0326267,"), (2) and (3), with hierarchical rules (4) and (5) rules shown in Figure 2. Given the existence of initial rules (1), (2) and (3), hierarchical rules (4) and (5) can be created. Rule (4) specifies the linear position of the translation of the English phrase that precedes house with reference to lexicalised casa. Introduction For hierarchical machine translation models (Chiang, 2005), there is no requirement for a syntactic relationship to exist between the lexicalised words of a rule and the words replaced by nonterminals, the only requirement being that substituted words form an SMT phrase (Koehn et al., 2003). The dependency structure of either the source or target (or indeed both) can, however, be used to constrain rule extraction as to only allow hierarchical rules in which the linear position of dependents are specified with reference to the position of their lexicalised heads. For example, in the case of the hierarchical rules in Figure 2, rule (4) satisfies such a constraint according to both the source and target language dependency structures (since white is the dependent of house and blanca is the dependent of casa, and it is both white and blanca that are replaced by non-terminals while t"
W13-2259,J10-4005,0,0.0212379,"(2008) and Huang and Xiang (2010), in which rule table size is vastly reduced by means of filtering low frequency rules, while Tomeh et al. (2009), Johnson et al. (2007) and Yang and Zheng (2009) take the approach of applying statistical significance tests to rule filtering, with Lee et al. (2012) defining filtering methods that estimate translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et al. (2010) apply target-side dependency constraints for the same language pair and direction in addition to Arabic to English, Peter et al. (2011) also apply dependency constraints on the target side, but rather soft constraints that can be relaxed in the case that an ill-formed structure does in fact yield a better translation. Gao et al. (2011) similarly apply soft dependency constraints but to the source side for Chinese to English translation, and Galley and Manning (2009) show several advantages to using maximum spanning tree non-projective dependency parsing decoding for Chinese to English transla"
W13-2259,W04-3250,0,0.0711091,"tion, a minor decrease of -0.06 mean BLEU occurs (not statistically significant). Similarly for German to English, when reordering is applied to the source side, only a minor decrease (-0.05) results. Non-projective reordering causes the most significant reduction in performance for English to German when the English source is reordered, with a decrease of -0.52 mean BLEU. +0.07 mean BLEU for English to German, with the increase being statistically significant for German to English for the newstest2010 test set, but not statistically significant for newstest2011 test set or English to German (Koehn, 2004). Overall the best performing dependencyconstrained models are those that retain the highest numbers of hierarchical rules in the SCFG. This indicates that although the dependencyconstrained models produce a refined ruleset, they nevertheless discard some SCFG rules that would be useful to translate the unseen test data. One possible reason is that although the nonprojective dependency structures are significantly better, these high-quality linguistic structures may still not be optimal for translation. Another possibility is that a the GHKM rule extraction constraints combined with the depend"
W13-2259,2009.mtsummit-papers.17,0,0.0386487,"to D does not include at least one word positioned linearly in the surface form between H and D. Figure 3 shows an example non-projective dependency structure arising from English Wh-fronting. Non-projective dependencies occur frequently The increased computational complexity introduced by hierarchical machine translation models (Chiang, 2005), has motivated techniques of constraining model size as well as decoder search. Among such include the work of Zollmann et al. (2008) and Huang and Xiang (2010), in which rule table size is vastly reduced by means of filtering low frequency rules, while Tomeh et al. (2009), Johnson et al. (2007) and Yang and Zheng (2009) take the approach of applying statistical significance tests to rule filtering, with Lee et al. (2012) defining filtering methods that estimate translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et al. (2010) apply target-side dependency constraints for the same language pair and direction in addition to Arabic to English, Peter et al."
W13-2259,P12-2057,0,0.0210796,"ture arising from English Wh-fronting. Non-projective dependencies occur frequently The increased computational complexity introduced by hierarchical machine translation models (Chiang, 2005), has motivated techniques of constraining model size as well as decoder search. Among such include the work of Zollmann et al. (2008) and Huang and Xiang (2010), in which rule table size is vastly reduced by means of filtering low frequency rules, while Tomeh et al. (2009), Johnson et al. (2007) and Yang and Zheng (2009) take the approach of applying statistical significance tests to rule filtering, with Lee et al. (2012) defining filtering methods that estimate translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et al. (2010) apply target-side dependency constraints for the same language pair and direction in addition to Arabic to English, Peter et al. (2011) also apply dependency constraints on the target side, but rather soft constraints that can be relaxed in the case that an ill-formed structure d"
W13-2259,W12-3150,0,0.0467071,"ontinuous string, making possible the substitution of this constituent with a non-terminal. The fact that one side of the training data from which hierarchical rules are extracted, however, is no longer guaranteed to be fluent, raises the question as to what effect this disfluency might have when the constraint is applied on the target side. We therefore include in our evaluation for both language directions (and for the case where the constraints are applied to the source) the effects of word reorder cause by the transformation. The Implementation with Moses For rule extraction we use Moses (Williams and Koehn, 2012) implementation of GHKM (Galley et al., 2004; Galley et al., 2006), which although is conventionally used to extract syntax-augmented SCFGs from phrase-structure parses (Zollmann and Venugopal, 2006), we apply the same rule extraction tool to dependency parses. Rule extraction is implemented in such a way as not to be restricted to any particular set of node labels. The conventional input format is for example: <tree label=&quot;NP&quot;> <tree label=&quot;DET&quot;> the </tree> <tree label=&quot;NN&quot;> cat </tree> </tree> The dependency-constrained ruleset can be extracted with this implementation by arranging dependen"
W13-2259,D11-1020,0,0.0209008,"ues of constraining model size as well as decoder search. Among such include the work of Zollmann et al. (2008) and Huang and Xiang (2010), in which rule table size is vastly reduced by means of filtering low frequency rules, while Tomeh et al. (2009), Johnson et al. (2007) and Yang and Zheng (2009) take the approach of applying statistical significance tests to rule filtering, with Lee et al. (2012) defining filtering methods that estimate translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et al. (2010) apply target-side dependency constraints for the same language pair and direction in addition to Arabic to English, Peter et al. (2011) also apply dependency constraints on the target side, but rather soft constraints that can be relaxed in the case that an ill-formed structure does in fact yield a better translation. Gao et al. (2011) similarly apply soft dependency constraints but to the source side for Chinese to English translation, and Galley and Manning (2009) show several advantages"
W13-2259,W12-3128,0,0.0251422,"Missing"
W13-2259,P09-2060,0,0.0170352,"ned linearly in the surface form between H and D. Figure 3 shows an example non-projective dependency structure arising from English Wh-fronting. Non-projective dependencies occur frequently The increased computational complexity introduced by hierarchical machine translation models (Chiang, 2005), has motivated techniques of constraining model size as well as decoder search. Among such include the work of Zollmann et al. (2008) and Huang and Xiang (2010), in which rule table size is vastly reduced by means of filtering low frequency rules, while Tomeh et al. (2009), Johnson et al. (2007) and Yang and Zheng (2009) take the approach of applying statistical significance tests to rule filtering, with Lee et al. (2012) defining filtering methods that estimate translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et al. (2010) apply target-side dependency constraints for the same language pair and direction in addition to Arabic to English, Peter et al. (2011) also apply dependency constraints on the"
W13-2259,N07-1050,0,0.0166409,"ough not constraining with dependency structure, instead create non-terminals with part-of-speech tag combinations for Chinese words identified as heads for translation into English. did  the  crisis  begin  ? Figure 3: Non-projective Dependency Structure for many languages, increasingly so for languages with high levels of free words order. An examination of Chinese treebanks, for example, reports that Chinese displays nine different kinds of nonprojective phenomena (Yuelong, 2012) with reports of as many as one in four sentences in tree banks having non-projective dependency structures (Nivre, 2007). Even for a language with relatively rigid word order such as English nonprojectivity is still common, due to Wh-fronting, topicalisation, scrambling and extraposition. Table 1 shows the frequency of non-projective dependency structures in WMT parallel data sets for German and English when parsed with a state-ofthe-art non-projective dependency parser (Bohnet, 2010). 4 Constrained Model We define the dependency constraint as follows: to create a hierarchical rule by replacing a word or phrase with a non-terminal, all the words of that phrase must belong to a single complete dependency tree an"
W13-2259,W99-0604,0,0.242104,"Missing"
W13-2259,W06-3119,0,0.0275226,"longer guaranteed to be fluent, raises the question as to what effect this disfluency might have when the constraint is applied on the target side. We therefore include in our evaluation for both language directions (and for the case where the constraints are applied to the source) the effects of word reorder cause by the transformation. The Implementation with Moses For rule extraction we use Moses (Williams and Koehn, 2012) implementation of GHKM (Galley et al., 2004; Galley et al., 2006), which although is conventionally used to extract syntax-augmented SCFGs from phrase-structure parses (Zollmann and Venugopal, 2006), we apply the same rule extraction tool to dependency parses. Rule extraction is implemented in such a way as not to be restricted to any particular set of node labels. The conventional input format is for example: <tree label=&quot;NP&quot;> <tree label=&quot;DET&quot;> the </tree> <tree label=&quot;NN&quot;> cat </tree> </tree> The dependency-constrained ruleset can be extracted with this implementation by arranging dependency structures into tree structures as follows:1 Figure 6: Non-Projectivity Transformation <tree label=&quot;X&quot;> <tree label=&quot;X&quot;> <tree label=&quot;X&quot;> the </tree> <tree label=&quot;X&quot;> black </tree> cat </tree> ate"
W13-2259,P03-1021,0,0.0231305,"Missing"
W13-2259,C08-1144,0,0.0193515,"pendency structure in which at least one dependency relation exists between a head, H, and its dependent, D, in which the directed path from H to D does not include at least one word positioned linearly in the surface form between H and D. Figure 3 shows an example non-projective dependency structure arising from English Wh-fronting. Non-projective dependencies occur frequently The increased computational complexity introduced by hierarchical machine translation models (Chiang, 2005), has motivated techniques of constraining model size as well as decoder search. Among such include the work of Zollmann et al. (2008) and Huang and Xiang (2010), in which rule table size is vastly reduced by means of filtering low frequency rules, while Tomeh et al. (2009), Johnson et al. (2007) and Yang and Zheng (2009) take the approach of applying statistical significance tests to rule filtering, with Lee et al. (2012) defining filtering methods that estimate translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et"
W13-2259,2001.mtsummit-papers.68,0,0.112928,"Missing"
W13-2259,2011.iwslt-papers.8,0,0.0204516,"t al. (2009), Johnson et al. (2007) and Yang and Zheng (2009) take the approach of applying statistical significance tests to rule filtering, with Lee et al. (2012) defining filtering methods that estimate translational effectiveness of rules.  When Dependency-based constraints have also been applied in a variety of settings to combat complexity challenges. Xie et al. (2011) use source side dependency constraints for translation from Chinese to English, while Shen et al. (2010) apply target-side dependency constraints for the same language pair and direction in addition to Arabic to English, Peter et al. (2011) also apply dependency constraints on the target side, but rather soft constraints that can be relaxed in the case that an ill-formed structure does in fact yield a better translation. Gao et al. (2011) similarly apply soft dependency constraints but to the source side for Chinese to English translation, and Galley and Manning (2009) show several advantages to using maximum spanning tree non-projective dependency parsing decoding for Chinese to English translation. Li et al. (2012), although not constraining with dependency structure, instead create non-terminals with part-of-speech tag combin"
W13-2259,D07-1103,0,\N,Missing
W13-2259,N04-1035,0,\N,Missing
W13-2259,P02-1040,0,\N,Missing
W13-2259,P09-1087,0,\N,Missing
W13-2259,P07-2045,0,\N,Missing
W13-2259,P05-1033,0,\N,Missing
W13-2259,P06-1121,0,\N,Missing
W13-2259,C10-1056,0,\N,Missing
W13-2305,U12-1010,1,0.77545,"gree to which that output was better than the other. In addition, direct estimation of quality within the context of machine translation extends the usefulness of the annotated data to other tasks such as quality-estimation (CallisonBurch et al., 2012). 33 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 33–41, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics reference translation is needed, and the bias is removed. In earlier work, we consider the possibility that translation quality is a hypothetical construct (Graham et al., 2012), and suggest applying methods of validating measurement of psychological constructs to the validation of measurements of translation quality. In psychology, a scale that employs more items as opposed to fewer is considered more valid. Under this criteria, a two-item (fluency and adequacy) scale is more valid than a single-item translation quality measure. the conventional 5-point interval-level scale and a continuous visual analog scale (VAS) are used for human evaluation. We collected data via Amazon’s Mechanical Turk, where the quality of annotations is known to vary considerably (CallisonB"
W13-2305,W07-0718,0,0.214127,"llison-Burch et al., 2012). Inconsistency in human evaluation of machine translation calls into question conclusions drawn from those assessments, and is the target of this paper: by revising the annotation process, can we improve annotator agreement, and hence the quality of human annotations? Direct estimates of quality are intrinsically continuous in nature, but are often collected using an interval-level scale with a relatively low number of categories, perhaps to make the task cognitively easier for human assessors. In MT evaluation, five and seven-point interval-level scales are common (Callison-Burch et al., 2007; Denkowski and Lavie, 2010). However, the interval-level scale commonly used for direct estimation of translation quality (and other NLP annotation tasks) forces human judges to discretize their assessments into a fixed number of categories, and this process could be a cause of inconsistency in human judgments. In particular, an assessor may be repeatedly forced to choose between two categories, neither of which really fits their judgment. The continuous nature of translation quality assessment, as well as the fact that many statistical methods exist that can be applied to continuous data but"
W13-2305,W08-0309,0,0.0214353,"fat Justin Zobel Department of Computing and Information Systems, The University of Melbourne {ygraham,tbaldwin,ammoffat,jzobel}@unimelb.edu.au Abstract For an evaluation to be credible, the annotations must be credible. The simplest way of establishing this is to have the same data point annotated by multiple annotators, and measure the agreement between them. There has been a worrying trend in recent MT shared tasks – whether the evaluation was structured as ranking translations from best-to-worst, or by direct estimation of fluency and adequacy – of agreement between annotators decreasing (Callison-Burch et al., 2008; CallisonBurch et al., 2009; Callison-Burch et al., 2010; Callison-Burch et al., 2011; Callison-Burch et al., 2012). Inconsistency in human evaluation of machine translation calls into question conclusions drawn from those assessments, and is the target of this paper: by revising the annotation process, can we improve annotator agreement, and hence the quality of human annotations? Direct estimates of quality are intrinsically continuous in nature, but are often collected using an interval-level scale with a relatively low number of categories, perhaps to make the task cognitively easier for"
W13-2305,D08-1027,0,0.0288369,"Missing"
W13-2305,D09-1030,0,0.0229152,"T output, and Figure 2 shows an equivalent VAS using the two most extreme anchor labels, strongly disagree and strongly agree. 4 Crowd-sourcing Judgments The volume of judgments required for evaluation of NLP tasks can be large, and employing experts to undertake those judgments may not always be feasible. Crowd-sourcing services via the Web offer an attractive alternative, and have been used in conjunction with a range of NLP evaluation and annotation tasks. Several guides exist for instructing researchers from various backgrounds on using Amazon’s Mechanical Turk (AMT) (Gibson et al., 2011; Callison-Burch, 2009), and allowance for the use of AMT is increasingly being made in research grant applications, as a cost-effective way of gathering data. Issues remain in connection with low payment levels (Fort et al., 2011); nevertheless, Ethics Approval Boards are typically disinterested in projects that make use of AMT, regarding AMT as being a purchased service rather than a part of the experimentation that may affect human subjects. The use of crowd-sourced judgments does, however, introduce the possibility of increased inconsistency, with service requesters typically havFor example, if the task at hand"
W13-2305,2010.amta-papers.20,0,0.207951,"nconsistency in human evaluation of machine translation calls into question conclusions drawn from those assessments, and is the target of this paper: by revising the annotation process, can we improve annotator agreement, and hence the quality of human annotations? Direct estimates of quality are intrinsically continuous in nature, but are often collected using an interval-level scale with a relatively low number of categories, perhaps to make the task cognitively easier for human assessors. In MT evaluation, five and seven-point interval-level scales are common (Callison-Burch et al., 2007; Denkowski and Lavie, 2010). However, the interval-level scale commonly used for direct estimation of translation quality (and other NLP annotation tasks) forces human judges to discretize their assessments into a fixed number of categories, and this process could be a cause of inconsistency in human judgments. In particular, an assessor may be repeatedly forced to choose between two categories, neither of which really fits their judgment. The continuous nature of translation quality assessment, as well as the fact that many statistical methods exist that can be applied to continuous data but not interval-level data, mo"
W13-2305,W12-3102,0,\N,Missing
W13-2305,W09-0401,0,\N,Missing
W13-2305,J11-2010,0,\N,Missing
W13-2305,W10-1703,0,\N,Missing
W14-3333,1993.eamt-1.1,0,0.361232,"would first require doubling the values of the one-sided bootstrap, leaving those of the two-sided approximate randomization algorithm as-is. The results of the two tests on this basis are extremely close, and in fact, in two out of the five comparisons, those of the bootstrap would have marginally higher pvalues than those of approximate randomization. As such, it is conceivable to conclude that the ex3 3.1 Randomized Significance Tests Bootstrap Resampling Bootstrap resampling provides a way of estimating the population distribution by sampling with replacement from a representative sample (Efron and Tibshirani, 1993). The test statistic is taken as the difference in scores of the two systems, SX − SY , which has an expected value of 0 under the null hypothesis that the two systems perform equally well. A bootstrap pseudo-sample consists of the translations by the two systems (Xb , Yb ) of a bootstrapped test set (Koehn, 2004), constructed by sampling with replacement from the original test set translations. The bootstrap distribution Sboot of the test statistic is estimated by calculating the value of the pseudo-statistic SXb − SYb for each pseudo-sample. 267 Set c = 0 Set c = 0 Compute actual statistic o"
W14-3333,D08-1089,0,0.029392,"ct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shar"
W14-3333,N03-1010,0,0.0799423,"rison of MT output text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with"
W14-3333,W13-2305,1,0.847589,"l., 2002), N IST (NIST, 2002), M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006). In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessments and 62,400 human fluency assessments were collected. After the removal of quality control items and filtering of judgments from low-quality workers, this resulted in an average of 1,280 adequacy and 1,013 fluency assessments per system for Spanishto-English (12 systems), and 1,483 adequacy and 1,534 fluency assessments per system for Englishto-Spanish (11 systems). To remove bias with respect to individual human judge preference scoring severity/leniency, scores provided by each human assessor were standardized according to the mean and standard d"
W14-3333,N10-1129,0,0.0132479,"esampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for"
W14-3333,W06-3114,0,0.0385282,"rap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scal"
W14-3333,W04-3250,0,0.911376,"ference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply presentational g"
W14-3333,N04-1022,0,0.0532519,"th one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with the original paper were simply pr"
W14-3333,W05-0909,0,0.0290276,"han those of the three other systems. The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: B LEU (Papineni et al., 2002), N IST (NIST, 2002), M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006). In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessment"
W14-3333,D11-1080,0,0.0138168,"lude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for two language"
W14-3333,P03-1021,0,0.0852537,"put text with one or more human reference translations. Small differences in automatic metric scores can be difficult to interpret, however, and statistical significance testing provides a way of estimating the likelihood that a score difference has occurred simply by chance. For several metrics, such as B LEU , standard significance tests cannot be applied due to scores not comprising the mean of individual sentence scores, justifying the use of randomized methods. Bootstrap resampling was one of the early randomized methods proposed for statistical significance testing of MT (Germann, 2003; Och, 2003; Kumar and Byrne, 2004; Koehn, 2004), to assess 266 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 266–274, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics periments actually show no substantial difference in Type I error between the two tests, which is consistent with results published in other fields of research (Smucker et al., 2007). We also note that the pseudo-code contains an unconventional computation of mean pseudo-statistics, τB , for shiftto-zero. Rather than speculate over whether these issues with the origin"
W14-3333,P11-2031,0,0.0238496,"nce approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We carry out a large-scale human evaluation of shared task systems for two language pairs to provide a g"
W14-3333,W05-0908,0,0.476407,"lation Yvette Graham Nitika Mathur Timothy Baldwin Department of Computing and Information Systems The University of Melbourne ygraham@unimelb.edu.au, nmathur@student.unimelb.edu.au, tb@ldwin.net Abstract for a pair of systems how likely a difference in B LEU scores occurred by chance. Empirical tests detailed in Koehn (2004) show that even for test sets as small as 300 translations, B LEU confidence intervals can be computed as accurately as if they had been computed on a test set 100 times as large. Approximate randomization was subsequently proposed as an alternate to bootstrap resampling (Riezler and Maxwell, 2005). Theoretically speaking, approximate randomization has an advantage over bootstrap resampling, in that it does not make the assumption that samples are representative of the populations from which they are drawn. Both methods require some adaptation in order to be used for the purpose of MT evaluation, such as combination with an automatic metric, and therefore it cannot be taken for granted that approximate randomization will be more accurate in practice. Within MT, approximate randomization for the purpose of statistical testing is also less common. Riezler and Maxwell (2005) provide a comp"
W14-3333,N06-1032,0,0.0153741,"e randomization with bootstrap resampling (distinct from paired bootstrap resampling), and conclude that since approximate randomization produces higher p-values for a set of apparently equally-performing systems, it more conservatively concludes statistically significant differences, and recommend preference of approximate randomization over bootstrap resampling for MT evaluation. Conclusions drawn from experiments provided in Riezler and Maxwell (2005) are oft-cited, with experiments interpreted as evidence that bootstrap resampling is overly optimistic in reporting significant differences (Riezler and Maxwell, 2006; Koehn and Monz, 2006; Galley and Manning, 2008; Green et al., 2010; Monz, 2011; Clark et al., 2011). Our contribution in this paper is to revisit statistical significance tests in MT — namely, bootstrap resampling, paired bootstrap resampling and Randomized methods of significance testing enable estimation of the probability that an increase in score has occurred simply by chance. In this paper, we examine the accuracy of three randomized methods of significance testing in the context of machine translation: paired bootstrap resampling, bootstrap resampling and approximate randomization. We"
W14-3333,2006.amta-papers.25,0,0.034494,". The combined result for each pair of systems is therefore taken as the p-value from the corresponding fluency significance test. We use the combined human evaluation pairwise significant tests as a gold standard against which to evaluate the randomized methods of statistical significance testing. We evaluate paired bootstrap resampling (Koehn, 2004) and bootstrap resampling as shown in Figure 3 and approximate randomization as shown in Figure 2, each in combination with four automatic MT metrics: B LEU (Papineni et al., 2002), N IST (NIST, 2002), M ETEOR (Banerjee and Lavie, 2005) and T ER (Snover et al., 2006). In order to evaluate the accuracy of the three randomized significance significance tests, we compare conclusions reached in a human evaluation of shared task participant systems. We carry out a large-scale human evaluation of all participating systems from WMT 2012 (Callison-Burch et al., 2012) for the Spanish-to-English and English-toSpanish translation tasks. Large numbers of human assessments of translations were collected using Amazon’s Mechanical Turk, with strict quality control filtering (Graham et al., 2013). A total of 82,100 human adequacy assessments and 62,400 human fluency asse"
W14-3333,W12-3102,0,\N,Missing
W14-3333,P02-1040,0,\N,Missing
W16-2301,W13-3520,0,0.0148252,"Missing"
W16-2301,2011.mtsummit-papers.35,0,0.440323,"Missing"
W16-2301,W15-3001,1,0.419439,"Missing"
W16-2301,W16-2305,0,0.0273073,"Missing"
W16-2301,L16-1662,0,0.00889007,"introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model."
W16-2301,W11-2101,1,0.851508,"Missing"
W16-2301,W16-2306,0,0.0263259,"Missing"
W16-2301,W16-2382,0,0.0451767,"Missing"
W16-2301,W16-2302,1,0.346609,"Missing"
W16-2301,W16-2307,1,0.756182,"Missing"
W16-2301,W16-2308,0,0.0373021,"Missing"
W16-2301,buck-etal-2014-n,0,0.0150538,"Missing"
W16-2301,W13-2201,1,0.287085,"Missing"
W16-2301,W14-3302,1,0.399147,"Missing"
W16-2301,W14-3340,1,0.419045,"Missing"
W16-2301,W07-0718,1,0.759293,"oth automatically and manually. The human evaluation (§3) involves asking human judges to rank sentences output by anonymized systems. We obtained large numbers of rankings from researchers who contributed The quality estimation task had three subtasks, with a total of 14 teams, submitting 39 entries. The automatic post-editing task had a total of 6 teams, submitting 11 entries. 1 Introduction We present the results of the shared tasks of the First Conference on Statistical Machine Translation (WMT) held at ACL 2016. This conference builds on nine previous WMT workshops (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015). 131 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 131–198, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics to refine evaluation and estimation methodologies for machine translation. As before, all of the data, translations, and collected human judgments are publicly available.1 We hope these datasets serve as a valuable resource for research into statistical machine translation and automatic evaluation or prediction of translation quality. New"
W16-2301,W15-3025,1,0.888663,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,P15-2026,1,0.895785,"used for the Task 2p. The submissions to the word-level task are modified in order to comply with the phrase-level task. sequences. Nevertheless, it is worth noticing the phraselevel QE systems introduced a number of interesting strategies that allowed them to outperform a strong baseline phrase-level model. Finally, we recall that the evaluation metric – word-level F1 mult – has difficulties to distinguish phrase-level systems. This suggests that we may need to find a different metric for evaluation of the phrase-level task, with phrase-level F1 -mult one of the candidates. 7 pointed out by Chatterjee et al. (2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; Automatic Post-editing Task This year WMT hosted the second round of the shared task on MT automatic post-editing (APE), which consists in automatically correcting"
W16-2301,W08-0309,1,0.809107,"Missing"
W16-2301,W16-2309,0,0.0305938,"Missing"
W16-2301,W10-1703,1,0.312318,"Missing"
W16-2301,W16-2336,0,0.0356983,"Missing"
W16-2301,W12-3102,1,0.235434,"ir native language. Consequently, health professionals may use the translated information to make clinical decisions impacting patients care. It is vital that translation systems do not contribute to the dissemination of incorrect clinical information. Therefore, the evaluation of biomedical translation systems should include an assessment at the document level indicating whether a translation conveyed erroneous clinical information. 6 Quality Estimation The fifth edition of the WMT shared task on quality estimation (QE) of machine translation (MT) builds on the previous editions of the task (Callison-Burch et al., 2012; Bojar et al., 2013, 2014, 2015), with “traditional” tasks at sentence and word levels, a new task for entire documents quality prediction, and a variant of the word-level task: phrase-level estimation. The goals of this year’s shared task were: • To advance work on sentence and wordlevel quality estimation by providing domainspecific, larger and professionally annotated datasets. • To analyse the effectiveness of different types of quality labels provided by humans for longer texts in document-level prediction. • To investigate quality estimation at a new level of granularity: phrases. Plan"
W16-2301,W16-2330,0,0.0328047,"Missing"
W16-2301,W16-2310,1,0.835279,"Missing"
W16-2301,W11-2103,1,0.65163,"Missing"
W16-2301,W16-2331,0,0.0203904,"cance test results for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W15-3009,1,0.788321,"Missing"
W16-2301,P15-1174,1,0.28154,"endently inserted in another part of this sentence, i.e. to correct an unrelated error. The statistics of the datasets are outlined in Table 20. Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: • Scoring: Pearson’s r correlation score (primary metric, official score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical significance on Pearson r and Spearman rho was computed using the William’s test, following the approach suggested in (Graham, 2015). Results Table 19 summarises the results for Task 1, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems according to the ranking variant. We note that three systems have not submitted results ranking evaluation variant. 6.4 Task 2: Predicting word-level quality The goal of this task is to evaluate the extent to which we can detect word-level errors in MT output. Various classes of errors can be found in translations, but for this task we consider all error types together, aiming at making a b"
W16-2301,W16-2311,0,0.0122367,"016) PROMT Automated Translation Solutions (Molchanov and Bykov, 2016) QT21 System Combination (Peter et al., 2016b) RWTH Aachen (Peter et al., 2016a) ¨ TUBITAK (Bektas¸ et al., 2016) University of Edinburgh (Sennrich et al., 2016) University of Edinburgh (Williams et al., 2016) UEDIN - SYNTAX UEDIN - LMU UH -* USFD - RESCORING UUT YSDA ONLINE -[ A , B , F, G ] University of Edinburgh / University of Munich (Huck et al., 2016) University of Helsinki (Tiedemann et al., 2016) University of Sheffield (Blain et al., 2016) Uppsala University (Tiedemann et al., 2016) Yandex School of Data Analysis (Dvorkovich et al., 2016) Four online statistical machine translation systems Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 136 Figure 2: Screenshot of the Appraise interface used in the human evaluation campaign. The annotator is presented with a source segment, a reference translation, and up to five outputs from competing systems (anonymized"
W16-2301,W13-2305,1,0.509145,"Missing"
W16-2301,W15-3036,0,0.0603245,"Missing"
W16-2301,E14-1047,1,0.831614,"Missing"
W16-2301,W16-2378,0,0.168663,"ts for pairs of systems competing in the news domain translation task (en-ru), where a green cell denotes a significantly higher DA adequacy score for the system in a given row over the system in a given column. cs-en fi-en tr-en de-en ru-en ro-en 0.997 0.996 0.988 0.964 0.961 0.920 en-ru 0.975 DA Correlation with RR Table 11: Correlation between overall DA standardized mean adequacy scores and RR Trueskill scores. 150 4 IT Translation Task from the QTLeap project: HF&FCUL for Portuguese, UPV/EHU for Spanish and Basque, IICT-BAS for Bulgarian, CUNI for Czech and UG for Dutch). Duma and Menzel (2016) describe UHDS- DOC 2 VEC and UHBS- LMI (University of Hamburg). Pahari et al. (2016) describe JU-USAAR (Jadavpur University & Saarland University). Cuong et al. (2016) describe ILLC-U VA-S CORPIO (University of Amsterdam). IILC-U VA-DS is based on Hoang and Sima’an (2014). PROMT-RULE - BASED and PROMT-H YBRID systems were submitted by the PROMT LLC company and they are not described in any paper. QTL -M OSES is the standard Moses setup (MERT-tuned on the in-domain training data, but otherwise without any domain-adaptation) and serves as a baseline. The IT-domain translation task introduced th"
W16-2301,W11-2123,0,0.0116622,"s were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii) the loglinear combination of monolingual and bilingual models in an ensemble-like manner, iii) the addition of task-specific features in the log-linear model to control the final output quality. Concerning the data, in addition"
W16-2301,W16-2384,0,0.0416841,"Missing"
W16-2301,W13-0805,0,0.0110515,"of words in the reference. Lower TER values indicate lower distance from the reference as a proxy for higher MT quality. 33 http://www.cs.umd.edu/˜snover/tercom/ 34 https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 27 The source sentences (together with their reference translations which were not used for the task) were provided by TAUS (https://www.taus.net/) and originally come from a unique IT vendor. 28 It consists of a phrase-based machine translation system leveraging generic and in-domain parallel training data and using a pre-reordering technique (Herrmann et al., 2013). It takes also advantages of POS and word class-based language models. 29 German native speakers working at Text&Form https: //www.textform.com/. 176 Train (12,000) Dev (1,000) Test (2,000) SRC 201,505 17,827 31,477 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, pro"
W16-2301,W16-2315,1,0.824148,"Missing"
W16-2301,P07-2045,1,0.017773,"gets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual translation as another term of comparison. To get some insights about the progress with respect to the first pilot task, participating systems were also evaluated against a reimplementation of the approach firstly proposed by Simard et al. (2007).36 Last year, in fact, this statistical post-editing approach represented the common backbone of all submissions (this is also reflected by the close results achieved by participants in the pilot task). For this purpose, a phrasebased SMT system based on Moses (Koehn et al., 2007) was used. Translation and reordering models were estimated following the Moses protocol with default setup using MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heafield, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the APE system was tuned on Adam Mickiewicz University. This system is among the very first ones exploring the application of neural translation models to the APE task. In particular, it investigates the following aspects: i) the use of artificially-created postedited data to train the neural models, ii)"
W16-2301,W16-2337,0,0.0376798,"Missing"
W16-2301,W16-2303,1,0.821661,"Missing"
W16-2301,L16-1582,1,0.790825,"Missing"
W16-2301,W16-2385,0,0.0304494,"Missing"
W16-2301,P16-2095,1,0.885979,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,W15-3037,0,0.00849978,"stems were submitted for the word-level task. UNBABEL 2 linear is a feature-based linear sequential model. It uses the baseline features provided by the shared task organisers (with slight changes) conjoined with individual labels and pairs of consecutive labels. It also uses various syntactic dependency-based features (dependency relations, heads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produ"
W16-2301,W14-3342,0,0.0237077,"Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Task 1, QuEst++13 (Specia et al., 2015) was used to extract 17 features from the SMT source/target language training corpus: • Number of tokens in source & target sentences. • Target token, its left and right contexts of one word."
W16-2301,W16-2318,0,0.0145349,"Missing"
W16-2301,N06-1014,0,0.0215966,"d on n previous translation and reordering decisions. This technique is able to model both local and long-range reorderings that are quite useful when dealing with the German language. To improve the capability of choosing the correct edit to process, eight new features are added to the loglinear model. These features capture the cost of deleting a phrase and different information on possible gaps in reordering operations. The monolingual alignments between the MT outputs and their post-edits are computed using different methods based on TER, METEOR (Snover et al., 2006) and Berkeley Aligner (Liang et al., 2006). Only the task data is used for these submissions. 7.3 179 TER/BLEU results ID AMU Primary AMU Contrastive FBK Contrastive FBK Primary USAAR Primary USAAR Constrastive CUNI Primary (Simard et al., 2007) Baseline DCU Contrastive JUSAAR Primary JUSAAR Contrastive DCU Primary Avg. TER 21.52 23.06 23.92 23.94 24.14 24.14 24.31 24.64 24.76 26.79 26.92 26.97 28.97 BLEU 67.65 66.09 64.75 64.75 64.10 64.00 63.32 63.47 62.11 58.60 59.44 59.18 55.19 tained this year by the top runs can only be reached by moving from the basic statistical MT backbone shared by all last year’s participants to new and mor"
W16-2301,P13-2109,0,0.0179517,"eads, and second-order structures like siblings and grandparents). The syntactic dependencies are predicted with TurboParser trained on the TIGER German treebank. UNBABEL 2 ensemble uses a stacked architecture, inspired by the last year’s QUETCH+ system (Kreutzer et al., 2015), which combines three neural systems: one feedforward and two recurrent ones. The predictions of these systems are added as additional features in the linear system above. The following external resources were used: part-of-speech tags and extra syntactic dependency information obtained with TurboTagger and TurboParser (Martins et al., 2013), trained on the Penn Treebank (for English) and on the version of the German TIGER corpus used in the SPMRL shared task (Seddah et al., 2014) for German. For the neural models, pretrained word embeddings from Polyglot (AlRfou et al., 2013) and those produced with a neural MT system (Bahdanau et al., 2014) were used. UGENT-LT3 (Task 1, Task 2): The submissions for the word-level task use 41 features in combination with the baseline feature set to train binary classifiers. The 41 additional features attempt to capture accuracy errors (concerned with the meaning transfer from the source to targe"
W16-2301,W16-2387,0,0.0253511,"Missing"
W16-2301,W16-2317,0,0.0414065,"Missing"
W16-2301,N13-1090,0,0.00833788,"Missing"
W16-2301,2015.iwslt-papers.4,1,0.737239,"Missing"
W16-2301,W16-2319,0,0.0339318,"Missing"
W16-2301,W16-2386,1,0.814188,"e package to extract these and other quality estimation features and perform model learning, with suggested methods for all levels of prediction. Participants, described in Section 6.2, could submit up to two systems for each task. Data used to build MT systems or internal system information (such as model scores or n-best lists) were made available on request for Tasks 1 and 2. 6.1 The γ,  and C parameters were optimised via grid search with 5-fold cross validation on the training set. Word-level baseline system: For Tasks 2 and 2p, the baseline features were extracted with the Marmot tool (Logacheva et al., 2016b). For the baseline system we used a number of features that have been found the most informative in previous research on word-level QE. Our baseline set of features is loosely based on the one described in (Luong et al., 2014). It contains the following 22 features: • Word count in the source and target sentences, source and target token count ratio. Although these features are sentence-level (i.e. their values will be the same for all words in a sentence), the length of a sentence might influence the probability of a word being wrong. Baseline systems Sentence-level baseline system: For Tas"
W16-2301,L16-1470,1,0.826657,"Missing"
W16-2301,P03-1021,0,0.0340784,"7 Tokens TGT 210,573 19,355 34,332 PE 214,720 19,763 35,276 Table 31: Types SRC TGT 9,328 14,185 2,931 3,333 3,908 4,695 Data statistics. APE@WMT15 APE@WMT16 (EN-ES, news, crowd) (EN-DE, IT, prof.) SRC 2.905 6.616 TGT 3.312 8.845 PE 3.085 8.245 Table 32: Repetition Rate (RR) of the WMT15 (EnglishSpanish, news domain, crowdsourced post-edits) and WMT16 (English-German, IT domain, professional posteditors) APE Task data. 7.1.3 PE 16,388 3,506 5,047 SRC 5,628 1,922 2,479 Lemmas TGT PE 11,418 13,244 2,686 2,806 3,753 4,050 the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Table 34. For each submitted run, the statistical significance of performance differences with respect to the baselines and the re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). Baseline The official baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a system that leaves all the test targets unmodified.35 Baseline results are reported in Table 34. 7.2 Participants Monolingual"
W16-2301,W16-2338,0,0.0374669,"Missing"
W16-2301,J03-1002,0,0.0268639,"word alignments and bilingual distributed representations to introduce a new set of features for the sentence-Level QE task. The features extracted include three alignment-based features, three bilingual embedding-based features, two embeddingbased features constrained on alignment links, as well as a set of 74 bigrams used as boolean features. The set of bigrams represents the most frequent bigrams in translations that have changed after the post-edition, and they are compiled by aligning translations to their post-editions provided in the WMT QE datasets. To produce these features, GIZA++ (Och and Ney, 2003) was used for word alignment and Multivec (Berard et al., 2016) was used for the bilingual model, which jointly learns distributed representations for source and target languages using a parallel corpus. To build the bilingual model, domain-specific data compiled from the resources made available for the WMT 16 IT-Domain shared task was used. As prediction model, a Linear Regression model using scikit-learn was built using a combination of QuEst++ baseline features and the new features proposed. tional features for both tasks. UNBABEL (Task 2): Two systems were submitted for the word-level tas"
W16-2301,W16-2321,0,0.0372138,"Missing"
W16-2301,W16-2388,1,0.748184,"Missing"
W16-2301,W16-2333,0,0.0259256,"Missing"
W16-2301,W15-3026,0,0.0339859,"Missing"
W16-2301,W16-2379,1,0.819672,"Missing"
W16-2301,W16-2334,1,0.811656,"Missing"
W16-2301,P02-1040,0,0.105557,", eventually, make the APE task more feasible by automatic systems. Other changes concern the language combination and the evaluation mode. As regards the languages, we moved from English-Spanish to English-German, which is one of the language pairs covered by the QT21 Project26 that supported data collection and post-editing. Concerning the evaluation, we changed from TER scores computed both in case-sensitive and caseinsensitive mode to a single ranking based on case sensitive measurements. Besides these changes the new round of the APE task included some extensions in the evaluation. BLEU (Papineni et al., 2002) has been introduced as a secondary evaluation metric to measure the improvements over the rough MT output. In addition, to gain further insights on final output quality, a subset of the outputs of the submitted systems has also been manually evaluated. Based on these changes and extensions, the goals of this year’s shared task were to: i) improve and stabilize the evaluation framework in view of future rounds, ii) analyze the effect on task 26 feasibility of data coming from a narrow domain, iii) analyze the effect of post-edits collected from professional translators, iv) analyze how humans"
W16-2301,W16-2389,0,0.0210658,"Missing"
W16-2301,W16-2390,0,0.0283246,"Missing"
W16-2301,W16-2322,0,0.0139204,"ng and evaluating the manual translations has settled into the following pattern. We ask human annotators to rank the outputs of five systems. From these rankings, we produce pairwise translation comparisons, and then evaluate them with a version of the TrueSkill algorithm adapted to our task. We refer to this approach (described in Section 3.4) as the relative ranking approach (RR), so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer their absolute quality. These results are used to produce the official ranking for the WMT 2016 tasks. However, work in evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality. In this setting, annotators are asked to provide an assessment of the direct quality of the output of a system relative to a reference translation. In order to evaluate the potential of this approach for future WMT evaluations, we conducted a direct assessment evaluation in parallel. This evaluation, together with a comparison of the official results, is described in Section 3.5. 2.3 3.1 2.2 Training data As in past years we provide"
W16-2301,W16-2392,1,0.772695,"Missing"
W16-2301,L16-1649,1,0.704564,"): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence is the more difficult it is to translate. For this purpose, it uses information provided by syntactic parsing (information from parsing trees,"
W16-2301,W16-2391,1,0.796337,"Missing"
W16-2301,2014.eamt-1.21,1,0.643048,"e extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1): The YSDA submission is based on a simple idea that the more complex the sentence"
W16-2301,W15-3040,1,0.909092,"on-word corpus,18 with a vocabulary size of 527K words. Document embeddings are extracted by averaging word embeddings in the document. The GP model was trained with two Rational Quadratic kernels (Rasmussen and Williams, 2006): one for the 17 baseline features and another for the 500 features from the embeddings. Since each kernel has its own set of hyperparameters, the full model can leverage the contributions from the two different sets. The second system (GRAPH-DISC) combines the baseline features with discourse-aware features. The discourse aware features are the same as the ones used by Scarton et al. (2015a) plus Latent Semantic Analysis (LSA) cohesion features (Scarton and Specia, 2014), number of subtrees and height of the Rhetorical Structure Theory (RST) tree and entity graph-based coherence scores (Sim Smith et al., 2016). Discourseaware and RST tree features were extracted only for English (tools are only available for this language), LSA features were extracted for both languages, and entity graph-based coherence scores were extracted for the target language only (Spanish), as the source documents are expected to be coherent. This QE model was trained with an SVR algorithm. YSDA (Task 1)"
W16-2301,2006.amta-papers.25,0,0.850121,"ombinations of several features. A regression model was training to predict BLEU as target metric instead HTER. The machine learning pipeline uses an SVR with RBF kernel to predict BLEU scores, followed by a linear SVR to predict HTER scores from BLEU scores. As external resources, the system uses a syntactic parser, pseudo-references and back-translation from web-scale MT system, and a web-scale language model. 6.3 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the percentage of their words that need to be fixed. HTER (Snover et al., 2006) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version in [0,1]. As in previous years, two variants of the results could be submitted: • Scoring: An absolute HTER score for each sentence translation, to be interpreted as an error metric: lower scores mean better translations. • Ranking: A ranking of sentence translations for all source sentences from best to worst. For this variant, it does not matter how the ranking is produced (from HTER predictions or by other means). The reference ranking is defined based on the true H"
W16-2301,W15-4916,1,0.824025,"Missing"
W16-2301,W16-2346,1,0.0602876,"Missing"
W16-2301,W16-2325,1,0.816227,"Missing"
W16-2301,W16-2393,0,0.033041,"Missing"
W16-2301,W16-2326,0,0.0221449,"Missing"
W16-2301,W16-2327,1,0.737603,"Missing"
W16-2301,W16-2328,0,0.0403946,"Missing"
W16-2301,C00-2137,0,0.0384283,"Missing"
W16-2301,D07-1091,1,\N,Missing
W16-2301,W09-0401,1,\N,Missing
W16-2301,P11-1105,0,\N,Missing
W16-2301,N07-1064,0,\N,Missing
W16-2301,W04-3250,1,\N,Missing
W16-2301,P15-4020,1,\N,Missing
W16-2301,W16-2377,1,\N,Missing
W16-2301,aziz-etal-2012-pet,1,\N,Missing
W16-2301,2012.eamt-1.31,0,\N,Missing
W16-2301,C14-1182,0,\N,Missing
W16-2301,W16-2316,0,\N,Missing
W16-2301,W16-2332,1,\N,Missing
W16-2301,W16-2320,1,\N,Missing
W16-2301,W16-2335,0,\N,Missing
W16-2301,W16-2329,0,\N,Missing
W16-2301,W16-2383,0,\N,Missing
W16-2301,W16-2381,1,\N,Missing
W16-2301,W16-2312,0,\N,Missing
W16-2301,P16-1162,1,\N,Missing
W16-2301,W13-2243,1,\N,Missing
W16-2301,W08-0509,0,\N,Missing
W16-2301,2015.eamt-1.17,1,\N,Missing
W16-2301,W14-6111,0,\N,Missing
W16-2301,2012.tc-1.5,1,\N,Missing
W16-2301,W16-2347,1,\N,Missing
W16-2301,W16-2314,0,\N,Missing
W16-2302,P13-1023,0,0.0135154,"ng algorithm is improved (linear SVM instead of logistic regression) and some features that are relatively slow to compute are removed (paraphrasing, syntax and permutation trees) which resulted in a very large speed-up. BEER is usually trained for ranking but in this case there was a compromise: the initial model is trained for ranking (RR) with ranking SVM and then the output from SVM is scaled using trained regression model to approximate absolute judgment (DA). HUME The HUME metric (Birch et al., 2016) is a novel human evaluation measure that decomposes over the UCCA semantic units. UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its crosslinguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their interrelations. HUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence. We thus avoid the semantic annotation of machinegenerated text, which is often garbled or seman2.4.2 C HARAC T ER C HARAC T ER (Wang et al., 2016) is a novel character-level metric inspired by the commonly applied"
W16-2302,D16-1134,1,0.845309,"and permutation trees. BEER has participated in previous years of the evaluation task. This year the learning algorithm is improved (linear SVM instead of logistic regression) and some features that are relatively slow to compute are removed (paraphrasing, syntax and permutation trees) which resulted in a very large speed-up. BEER is usually trained for ranking but in this case there was a compromise: the initial model is trained for ranking (RR) with ranking SVM and then the output from SVM is scaled using trained regression model to approximate absolute judgment (DA). HUME The HUME metric (Birch et al., 2016) is a novel human evaluation measure that decomposes over the UCCA semantic units. UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its crosslinguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their interrelations. HUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence. We thus avoid the semantic annotation of machinegenerated text, which is often garbled or seman2.4"
W16-2302,W11-2101,1,0.910889,"Missing"
W16-2302,1999.mtsummit-1.31,0,0.193921,"Missing"
W16-2302,W15-3047,0,0.172925,"Missing"
W16-2302,D15-1124,0,0.0419862,"Missing"
W16-2302,W16-2339,0,0.102063,"T10 T10 T10 T9,F8 T9,F8 T9,F8 T9,F8 T9,F8 T9,F8 T10,F9 T11,F10 T11,F10 T11,F10 T11,F10 cs de T5,F4,T6 T5,F5 T8,F4 T8,F5 Table 3: Overview of tables (T) and figures (F) reporting results of the individual “tracks” and language pairs. 2.4.9 UPF-C OBALT, C OBALT F and M ETRICS F optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. Since Moses scorer is versioned on Github, we strongly encourage authors of high-performing metrics to add them to Moses scorer, as this will ensure that their metric can be included in future tasks. UPF-C OBALT (Fomicheva et al., 2016) is an alignment-based metric that examines the syntactic contexts of lexically similar candidate and reference words in order to distinguish meaningpreserving variations from the differences indicative of MT errors. This year the metric was improved by explicitly addressing MT fluency. The new version of the metric, C OBALT F, combines various components of UPF-C OBALT with a number of fine-grained features intended to capture the number and scale of disfluent fragments contained in MT sentences. M ETRICS F is a combination of three evaluation systems, BLEU, M ETEOR and UPF-C OBALT, with the"
W16-2302,W16-2303,1,0.878639,"Missing"
W16-2302,W06-3114,0,0.0357039,"rovision of more conclusive system-level metric rankings. 1 • Relative Ranking (RR) of up to 5 different translation candidates at a time, as collected in WMT in the past, Introduction • Direct Assessment (DA) evaluating the adequacy of a translation candidate on an absolute scale in isolation from other translations, Automatic evaluation of machine translation quality is essential in the development and selection of machine translation systems. Many different automatic MT quality metrics are available and the Metrics Shared Task1 is held annually at WMT to assess their quality, starting with Koehn and Monz (2006) and following up to Stanojevi´c et al. (2015). • HUME, a composite segment-level score aggregated over manual judgments of translation quality of semantic units of the source sentence. Additional changes to the task evaluation include a change in the way we compute confidence 1 http://www.statmt.org/wmt16/ metrics-task/ 199 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 199–231, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics Track RRsysNews RRsysIT DAsysNews RRsegNews DAsegNews HUMEseg Test set newstest201"
W16-2302,D14-1020,1,0.744969,"ores with human assessment variant RR + TT, i.e. standard WMT relative ranking including tuning task systems. In previous years, we reported empirical confidence intervals of system-level correlations obtained by bootstrap resampling human assessments data and computing confidence intervals for individual correlations with human assessment. Such confidence intervals reflect the variance due to particular sentences and assessors involved in the evaluation but lead to over-estimation of significant differences if employed to conclude which metrics outperform others. This year, as recommended by Graham and Baldwin (2014), instead we employ Williams significance test (Williams, 1959). Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other are highlighted in bold in Tables 4 and 5. Since RR is the official method of evaluation for this year’s metrics task, bolded correlations under RR comprise official winners of the news domain portion of the system-level metrics task. DA results are included for comparison and are investigatory only. a significant increase in correlation with h"
W16-2302,E06-1031,0,0.00991093,"MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced by the Moses scorer which is used in Moses model Pn − H)(Mi − M ) qP n 2 2 (H − H) i i=1 i=1 (Mi − M ) r = qP n i=1 (Hi (1) where H are human assessment scores of all systems in a given translation direction, M are corresponding scores as predicted by a given metric. H and M are their means respectively. 7 http://www.itl.nist.gov/iad/mig/ tools/ 205 Since some metrics, such as BLEU, for example, aim to achieve a strong positive correlation with human assessment, while error metrics, such as TER aim for a strong negative correlation, after computation of r for metrics, we compare"
W16-2302,N16-1001,1,0.834545,". Correlations not significantly outperformed by any other are highlighted in bold in Tables 4 and 5. Since RR is the official method of evaluation for this year’s metrics task, bolded correlations under RR comprise official winners of the news domain portion of the system-level metrics task. DA results are included for comparison and are investigatory only. a significant increase in correlation with human assessment over that of BLEU, where a green cell in the column denotes outperformance of BLEU by the metric in that row. For investigatory purposes only, we also include hybrid-supersample (Graham and Liu, 2016) results for system-level metrics. 10K hybrid systems were created per language pair, with corresponding DA human assessment scores, by sampling pairs of systems from WMT16 translation task and creating a hybrid system by combining translations from each system to create new hybrid output test set documents, each with a corresponding DA human assessment score. Not all metrics participating in the system-level metrics shared task submitted metric scores for the large set of hybrid systems, possibly due to the increased time required to run metrics on the large set of 10K systems. In this respec"
W16-2302,W14-3336,1,0.563594,"oncordant |+ |Discordant| (2) where Concordant is the set of all human comparisons for which a given metric suggests the same order and Discordant is the set of all human comparisons for which a given metric disagrees. The formula is not specific with respect to ties, i.e. cases where the annotation says that the two outputs are equally good. The way in which ties (both in human and metric judgment) were incorporated in computing Kendall τ has changed across the years of WMT metrics tasks. Here we adopt the version from WMT14 and WMT15. For a detailed discussion on other options, see Mach´acˇ ek and Bojar (2014). The method is formally described using the following matrix: Given such a matrix Ch,m where h, m ∈ {&lt;, = , &gt;}9 and a metric, we compute the Kendall’s τ for the metric the following way: Segment-level HUME evaluation The evaluation of segment-level metrics with reference to HUME scores operates in a similar way to DA, by computing the Pearson correlation of HUME evaluation scores for individual translations with metric scores. Williams test is also applied to test for significant differences in metric performance. Kendall’s Tau-like Formulation We measure the quality of metrics’ segment-level"
W16-2302,W13-2305,1,0.696366,"rporate DA fluency into future metric evaluations, however. Finally, although it is common to apply a sentence length restriction in WMT human evaluation, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT16. Direct Assessment (DA) In addition to the standard relative ranking (RR) manual evaluation employed to yield official system rankings in WMT16 translation task, this year the translation task also trialed a new method of human evaluation, monolingual direct assessment (DA) of translation fluency (Graham et al., 2013) and adequacy (Graham et al., 2014; Graham et al., 2016). For investigatory purposes, therefore, we also include evaluation of metrics with reference to the newly trialed human assessment method. Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment for both fluency and adequacy. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by simultaneous assessment of several alternate"
W16-2302,W13-2202,1,0.663833,"st of the paper. We discuss systemlevel results for news task systems (including tuning task systems) in Section 3.1. The system-level results for the IT domain are discussed in Section 3.2. The segment-level results are in Section 3.3. We end with discussion in Section 3.4. • Mteval. The metrics MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced by the Moses scorer which is used in Moses model Pn − H)(Mi − M ) qP n 2 2 (H − H) i i=1 i=1 (Mi − M ) r = qP n i=1 (Hi (1) where H are human assessment scores of all systems in a given translation direction, M are corresponding scores as predicted by a given metric. H and M are their"
W16-2302,W16-2340,0,0.0785443,"Metric BEER C HARAC T ER CHR F1,2,3, WORD F1,2,3 D EP C HECK DPMF COMB - WITHOUT-RED MPEDA U OW.R E VAL UPF - COBALT, C OBALT F, M ETRICS F DTED Participant ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) RWTH Aachen University (Wang et al., 2016) Humboldt University of Berlin (Popovi´c, 2016) Charles University, no corresponding paper Chinese Academy of Sciences and Dublin City University (Yu et al., 2015) Jiangxi Normal University (Zhang et al., 2016) University of Wolverhampton (Gupta et al., 2015b) Universitat Pompeu Fabra (Fomicheva et al., 2016) University of St Andrews, (McCaffery and Nederhof, 2016) Table 2: Participants of WMT16 Metrics Shared Task tically unclear. This also allows the re-use of the source semantic annotation for measuring the quality of different translations of the same source sentence, and avoids reliance on possibly suboptimal reference translations. HUME shows good inter-annotator agreement, and reasonable correlation with Direct Assessment (Graham et al., 2015). Segment-level DA adequacy scores were collected as in system-level DA, described in Section 2.3.1, again with strict quality control and score standardization applied. To achieve accurate segment-level sco"
W16-2302,E14-1047,1,0.869993,"ic evaluations, however. Finally, although it is common to apply a sentence length restriction in WMT human evaluation, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT16. Direct Assessment (DA) In addition to the standard relative ranking (RR) manual evaluation employed to yield official system rankings in WMT16 translation task, this year the translation task also trialed a new method of human evaluation, monolingual direct assessment (DA) of translation fluency (Graham et al., 2013) and adequacy (Graham et al., 2014; Graham et al., 2016). For investigatory purposes, therefore, we also include evaluation of metrics with reference to the newly trialed human assessment method. Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment for both fluency and adequacy. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by simultaneous assessment of several alternate translations of a given single so"
W16-2302,P02-1040,0,0.11901,"al. (2016a), metrics task occasionally suffers from “loss of knowledge” when successful metrics participate only in one year. We attempt to avoid this by regularly evaluating also a range of “baseline metrics”: 3 Results Table 3 provides an overview of all the tables and figures in the rest of the paper. We discuss systemlevel results for news task systems (including tuning task systems) in Section 3.1. The system-level results for the IT domain are discussed in Section 3.2. The segment-level results are in Section 3.3. We end with discussion in Section 3.4. • Mteval. The metrics MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced"
W16-2302,N15-1124,1,0.922262,"rsity (Yu et al., 2015) Jiangxi Normal University (Zhang et al., 2016) University of Wolverhampton (Gupta et al., 2015b) Universitat Pompeu Fabra (Fomicheva et al., 2016) University of St Andrews, (McCaffery and Nederhof, 2016) Table 2: Participants of WMT16 Metrics Shared Task tically unclear. This also allows the re-use of the source semantic annotation for measuring the quality of different translations of the same source sentence, and avoids reliance on possibly suboptimal reference translations. HUME shows good inter-annotator agreement, and reasonable correlation with Direct Assessment (Graham et al., 2015). Segment-level DA adequacy scores were collected as in system-level DA, described in Section 2.3.1, again with strict quality control and score standardization applied. To achieve accurate segment-level scores for translations, a human assessment of each translation was collected from 15 distinct human assessors before combination into a mean adequacy score for each individual translation. Although in general agreement in human assessment of MT has been difficult to achieve, segment-level DA scores employing a minimum of 15 repeat assessments have been shown to be almost perfectly replicable."
W16-2302,W16-2341,0,0.054072,"Missing"
W16-2302,2006.amta-papers.25,0,0.140134,"end with discussion in Section 3.4. • Mteval. The metrics MTEVAL BLEU (Papineni et al., 2002) and MTEVAL NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 which is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Mach´acˇ ek and Bojar, 2013). 3.1 System-Level Results for News Task As in previous years, we employ the Pearson correlation (r) as the main evaluation measure for system-level metrics, as follows: • Moses Scorer. The metrics MOSES BLEU, MOSES TER (Snover et al., 2006), MOSES WER, MOSES PER and MOSE CDER (Leusch et al., 2006) were produced by the Moses scorer which is used in Moses model Pn − H)(Mi − M ) qP n 2 2 (H − H) i i=1 i=1 (Mi − M ) r = qP n i=1 (Hi (1) where H are human assessment scores of all systems in a given translation direction, M are corresponding scores as predicted by a given metric. H and M are their means respectively. 7 http://www.itl.nist.gov/iad/mig/ tools/ 205 Since some metrics, such as BLEU, for example, aim to achieve a strong positive correlation with human assessment, while error metrics, such as TER aim for a strong negative c"
W16-2302,W15-3050,1,0.8645,"Missing"
W16-2302,W15-3031,1,0.543137,"Missing"
W16-2302,W16-2342,0,0.0636751,"easure that decomposes over the UCCA semantic units. UCCA (Abend and Rappoport, 2013) is an appealing candidate for semantic analysis, due to its crosslinguistic applicability, support for rapid annotation, and coverage of many fundamental semantic phenomena, such as verbal, nominal and adjectival argument structures and their interrelations. HUME operates by aggregating human assessments of the translation quality of individual semantic units in the source sentence. We thus avoid the semantic annotation of machinegenerated text, which is often garbled or seman2.4.2 C HARAC T ER C HARAC T ER (Wang et al., 2016) is a novel character-level metric inspired by the commonly applied translation edit rate (TER). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches 203 rank, using training data from the English-targeted language pairs from WMT12 to WMT14. In the results DPMF COMB - WITHOUT-RED is represented as DPMF COMB for brevity. the reference, normalized by the length of the hypothesis sentence. C HARAC T ER calculates the character-level edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TE"
W16-2302,W15-3053,0,0.0759192,"W.R E VAL U OW.R E VAL (Gupta et al., 2015b) uses dependency-tree Long Short Term Memory (LSTM) network to represent both the hypothesis and the reference with a dense vector. Training is performed using the judgements from WMT13 (Bojar et al., 2013) converted to similarity scores. The final score at the system level is obtained by averaging the segment level scores obtained from a neural network which takes into account both distance and Hadamard product of the two representations. DPMF COMB - WITHOUT-RED The authors of DPMF COMB - WITHOUT-RED follow the work on last year’s metric DPMF COMB (Yu et al., 2015), but modify it with two main differences. Firstly, they use the ‘case insensitive’ instead of ‘case sensitive’ option when using Asiya. Secondly, RED P are not used. Thus, DPMF COMB - WITHOUT-RED is a combined metric including 57 single metrics. Weights of the individual metrics are trained with SVMU OW.R E VAL is the same as U OW LSTM (Gupta et al., 2015a) that participated in the WMT15 task except that LSTM vector dimension is 150 for UoW.ReVal instead of 300. 204 Track RRsysNews RRsysIT DAsysNews RRsegNews DAsegNews HUMEseg cs de ro fi into-English T4,F1 T4,F1 ru tr English into ro fi ru t"
W16-2302,W16-2343,0,0.0217744,"TED evaluates only the word order. and WORD F WORD F1,2,3 (Popovi´c, 2016) calculate a simple F-score combination of the precision and recall of word n-grams of maximal length 4 with different setting for the β parameter (β = 1, 2, or 3). Precision and recall that are used in computation of the F-score are arithmetic averages of precisions and recalls, respectively, for the different n-gram orders. CHR F1,2,3 calculate the F-score of character n-grams of maximal length 6. β parameter gives β times weight to recall: β = 1 implies equal weights for precision and recall. 2.4.4 2.4.7 MPEDA MPEDA (Zhang et al., 2016) is developed on the basis of the METEOR metric. In order to accurately match words or phrases with the same or similar meaning, it extracts a domain-specific paraphrase table from the monolingual corpus and applies that paraphrase table to the METEOR metric to replace the general one. Unlike traditional paraphrase extraction approaches, it first filters out a domain-specific sub-corpus from a large general monolingual corpus and then extracts domain-specific paraphrase table from the sub-corpus by Markov Network model. Since the proposed paraphrase extraction approach can be used in all langu"
W16-2302,W13-2201,1,\N,Missing
W17-4717,W17-4758,0,0.0835228,"ese submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negative) predicted values prior to the calculation of the HTER score. The two submissions use the baseline features and the EnglishGerman submission also uses features from (Avramidis, 2017a). JXNU (T1): The JXNU submissions use features extracted from a neural network, including embedding features and cross-entropy features of the source sentences and their machine translations. The sentence embedding features are extracted through global average pooling from word embedding, which are trained using the WORD 2 VEC toolkit. The sentence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus."
W17-4717,W17-4772,0,0.0310859,"Missing"
W17-4717,W17-4759,0,0.0327025,"Missing"
W17-4717,L16-1356,1,0.770826,"or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method. They report results considering the word and its context versus the word in isolation, as well as variants with and without the gold labels at training time. Finally, for the phrase-level task, SHEF made use of predictions generated by BMAPS for task 2 and the phrase labelling approaches in (Blain et al., 2016). These approaches use the number of BAD word-level predictions in a phrase: an optimistic version labels the phrase as OK if at least half of the words in it are predicted to be OK, and a superpessimistic version labels the phrase as BAD if any word is in is predicted to be BAD. UHH (T1): The UHH-STK submission is based on sequence and tree kernels applied on the source and target input data for predicting the HTER score. The kernels use a backtranslation of the MT output into the source language as an additional input data representation. Further hand-crafted features were deﬁned in the form"
W17-4717,W17-4760,1,0.841464,"Missing"
W17-4717,W17-4755,1,0.0393895,"nd Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair"
W17-4717,W07-0718,1,0.696979,") • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), part"
W17-4717,W08-0309,1,0.537902,"Missing"
W17-4717,W10-1703,1,0.603181,"Missing"
W17-4717,W12-3102,1,0.508067,"Missing"
W17-4717,W17-4761,0,0.0349831,"Missing"
W17-4717,W17-4723,0,0.0362034,"Missing"
W17-4717,W17-4724,1,0.839009,"Missing"
W17-4717,W11-2103,1,0.744276,"Missing"
W17-4717,W17-4773,1,0.839713,"Missing"
W17-4717,W15-3025,1,0.905105,"Instead of predicting the HTER score, the systems attempted to predict the number of each of the four postediting operations (add, replace, shift, delete) at the sentence level. However, this did not lead to positive results. In future editions of the task, we plan to make this detailed post-editing information available again and suggest clear ways of using it. 5 Automatic Post-editing Task The WMT shared task on MT automatic postediting (APE), this year at its third round at WMT, aims to evaluate systems for the automatic correction of errors in a machine translated text. As pointed out by (Chatterjee et al., 2015b), from the application point of view the task is motivated by its possible uses to: • Improve MT output by exploiting information unavailable to the decoder, or by performing deeper text analysis that is too expensive at the decoding stage; • Cope with systematic errors of an MT system whose decoding process is not accessible; • Provide professional translators with improved MT output quality to reduce (human) post-editing effort; • Adapt the output of a general-purpose MT system to the lexicon/style requested in a speciﬁc application domain. The third round of the APE task proposed to parti"
W17-4717,W17-4718,1,0.0662673,"builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (constraint condition). We held 14 translation tasks this year, between English and each of Chinese, Czech, German, Finnish, Latvian, Russian, and Turkish. The Latvian and Chinese translation tasks were new this year. Latvian is a lesser resourced data condition on challenging language pair. Chinese allowed us to co-operate with an ongoing evaluation campaign on Asian languages org"
W17-4717,W17-4725,0,0.0391077,"Missing"
W17-4717,W08-0509,0,0.00859402,"l post-editors) and WMT17 DE-EN (German-English, pharmacological domain, professional post-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁci"
W17-4717,W17-4726,0,0.0387381,"Missing"
W17-4717,W13-2305,1,0.899702,"(RR) approach, so named because the pairwise comparisons denote only relative ability between a pair of systems, and cannot be used to infer absolute quality. For example, RR can be used to discover which systems perform better than others, but RR does not provide any information about the absolute quality of system translations, i.e. it provides no information about how far a given system is from producing perfect output according to a human user. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and last year’s evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with RR and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established last year (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for English-to-Russian (th"
W17-4717,E14-1047,1,0.508986,"Missing"
W17-4717,N15-1124,1,0.658439,"Missing"
W17-4717,W13-0805,0,0.0140095,"ely contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets released in the three rounds 24 We used phrase-based MT systems trained with generic and in-domain parallel training data, leveraging prereordering techniques (Herrmann et al., 2013), and taking advantage of POS and word class-based language models. 25 For both language directions, the source sentences and reference translations were provided by TAUS (https://www.taus.net/). 197 EN-DE Train (23,000) Dev (1,000) Test (2,000) DE-EN Train (25,000) Dev (1,000) Test (2,000) SRC Tokens TGT PE SRC Types TGT PE SRC Lemmas TGT PE 384448 17827 65120 403306 19355 69812 411246 19763 71483 18220 2931 8061 27382 3333 9765 31652 3506 10502 10946 1922 2626 21959 2686 3976 25550 2806 4282 437833 17578 35087 453096 18130 36082 456163 18313 36480 29745 4426 6987 19866 3583 5391 19172 3642 5"
W17-4717,W17-4775,0,0.0513454,"Missing"
W17-4717,W17-4730,1,0.829861,"Missing"
W17-4717,W17-4731,0,0.029579,"Missing"
W17-4717,W17-4727,0,0.046917,"Missing"
W17-4717,W16-2378,0,0.0869978,"a manuallyrevised version of the target, done by professional translators. Test data consists of (source, target) pairs having similar characteristics of those in the training set. Human post-edits of the test target instances were left apart to measure system performance. English-German data were drawn from the Information Technology (IT) domain. Training and test sets respectively contain 11,000 and 2,000 triplets. The data released for the 2016 round of the task (15,000 instances) and the artiﬁcially generated post-editing triplets (4 million instances) used by last year’s winning system (Junczys-Dowmunt and Grundkiewicz, 2016) were also provided as additional training material. German-English data were drawn from the Pharmacological domain. Training and development sets respectively contain 25,000 and 1,000 triplets, while the test set consists of 2,000 instances. Table 24 provides some basic statistics about the data (the same used for the sentence-level quality estimation task), which has been released by the European Project QT21 (Specia et al., 2017b).25 In addition, Tables 25 and 26 provide a view of the data from a task difﬁculty standpoint. Table 25 shows the repetition rate (RR) values of the data sets rele"
W17-4717,W11-2123,0,0.00943973,"-editors) APE task data. TER BLEU APE15 23.84 n/a APE16 24.76 62.11 APE17 EN-DE 24.48 62.49 APE17 DE-EN 15.55 79.54 Table 26: Translation quality (TER/BLEU of TGT and proportion of TGTs with TER=0) of the WMT15, WMT16, WMT17 EN-DE and WMT17 DE-EN data. Figure 7: TER distribution over the EN-DE test set Figure 8: TER distribution over the DE-EN test set TERcom27 software: lower average TER scores correspond to higher ranks. BLEU was computed using the multi-bleu.perl package28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT o"
W17-4717,I17-1013,0,0.0334345,"Missing"
W17-4717,W16-2384,0,0.0137431,"features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma a"
W17-4717,W17-4763,0,0.0300648,"ence cross-entropy features are calculated by a recurrent neural network language model. They experimented with different sentence embedding dimensions of the source sentences and translation outputs, as well as different sizes of the training corpus. The experimental results show that the neural network features lead to signiﬁcant improvements over the baseline, and that combining the neural network features with baseline features leads to further improvement. POSTECH (T1, T2, T3): POSTECH’s submissions to the sentence/word/phrase-level QE tasks are based on predictor-estimator architecture (Kim et al., 2017; Kim and Lee, 2016), which is the two-stage end-to-end ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamb"
W17-4717,W04-3250,1,0.380171,"age28 available in MOSES. MGIZA++ (Gao and Vogel, 2008) for word alignment. For language modeling we used the KenLM toolkit (Heaﬁeld, 2011) for standard n-gram modeling with an n-gram length of 5. Finally, the system was tuned on the development set, optimizing TER/BLEU with Minimum Error Rate Training (Och, 2003). The results of this additional term of comparison are also reported in Tables 28-29. For each submitted run, the statistical signiﬁcance of performance differences with respect to the baseline and our re-implementation of Simard et al. (2007) was calculated with the bootstrap test (Koehn, 2004). 5.1.3 Baselines Also this year, the ofﬁcial baseline results are the TER and BLEU scores calculated by comparing the raw MT output with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to t"
W17-4717,P07-2045,1,0.0149362,"t with the human post-edits. In practice, the baseline APE system is a “donothing” system that leaves all the test targets unmodiﬁed. Baseline results, the same shown in Table 26, are also reported in Tables 28-29 for comparison with participants’ submissions. In continuity with the previous rounds, we used as additional term of comparison a reimplementation of the method ﬁrstly proposed by Simard et al. (2007). It relies on a phrasebased post-editing approach to the task, which represented the common backbone of APE systems before the spread of neural solutions. The system is based on Moses (Koehn et al., 2007); translation and reordering models were estimated following the Moses protocol with default setup using 27 http://www.cs.umd.edu/˜snover/tercom/ https://github.com/moses-smt/mosesdecoder/ blob/master/scripts/generic/multi-bleu.perl 28 5.2 Participants Seven teams participated in the English-German task by submitting a total of ﬁfteen runs. Two of them also participated in the German-English task with ﬁve submitted runs. Participants are listed in Table 27, and a short description of their systems is provided in the following. Adam Mickiewicz University. AMU’s (ENDE) participation explores and"
W17-4717,C14-1017,0,0.0141812,"ord embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained from three matrices corresponding to the training data, the development set and a “truth” matrix between them, which is built from the word alignments and the gold labels to indicate which lexical items form a pair, and whether or not their lexical relation is OK or BAD. The ﬁrst two matrices are built from 300 dimension word vectors computed with pretrained in-domain word embeddings. They train their model over 100 iterations with the l2 norm as regulariser and using the forward-backward splitting algorithm (FOBOS) (Duchi and Singer, 2009) as optimisation method."
W17-4717,Q17-1015,0,0.0112193,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W16-2387,0,0.0126259,"the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisation algorithm. We note th"
W17-4717,W17-4764,0,0.0178514,"ID CDACM DCU DFKI JXNU POSTECH RTM SHEF UHH Unbabel Participating team Centre for Development of Advanced Computing, India (Patel and M, 2016) Dublin City University (Hokamp, 2017) German Research Centre for Artiﬁcial Intelligence, Germany (Avramidis, 2017b) Jiangxi Normal University, China (Chen et al., 2017) Pohang University of Science and Technology, Republic of Korea (Kim et al., 2017) Referential Translation Machines, Turkey (Bic¸ici, 2017) University of Shefﬁeld, UK (Blain et al., 2017; Paetzold and Specia, 2017) University of Hamburg, Germany (Duma and Menzel, 2017) Unbabel, Portugal (Martins et al., 2017b) Table 10: Participants in the WMT17 quality estimation shared task. neural QE model. The predictor-estimator architecture consists of two types of stacked neural network models: 1) a word prediction model based on bidirectional and bilingual recurrent neural network language model trained on additional large-scale parallel corpora and 2) a neural quality estimation model trained on quality-annotated noisy parallel corpora. To jointly learn the two-stage model, a stack propagation method was applied (Zhang and Weiss, 2016). In addition, a “multilevel model” was developed where a task-speciﬁc"
W17-4717,W06-3114,1,0.104699,"g (Bojar et al., 2017b) • bandit learning (Sokolov et al., 2017) This paper presents the results of the WMT17 shared tasks, which included three machine translation (MT) tasks (news, biomedical, and multimodal), two evaluation tasks (metrics and run-time estimation of MT quality), an automatic post-editing task, a neural MT training task, and a bandit learning task. 1 Introduction We present the results of the shared tasks of the Second Conference on Statistical Machine Translation (WMT) held at EMNLP 2017. This conference builds on eleven previous editions of WMT as workshops and conference (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a). This year we conducted several ofﬁcial tasks. We report in this paper on three tasks: • news translation (Section 2, Section 3) • quality estimation (Section 4) • automatic post-editing (Section 5) The conference featured additional shared tasks that are described in separate papers in these proceedings: • metrics (Bojar et al., 2017a) • multimodal machine translation and multilingual image description (Elliott et al., 2017) • biomedical translation (Jimeno Yepes et al., 2017) In the news trans"
W17-4717,P03-1021,0,0.144477,"ed better than monolingual models. The code for these models is freely available.15 DCU (T2): DCU’s submission is an ensemble of neural MT systems with different input factors, designed to jointly tackle both the automatic post-editing and word-level QE. 15 https://github.com/patelrajnath/rnn4nlp 186 Word-level features which have proven effective for QE, such as part-of-speech tags and dependency labels are included as input factors to NMT systems. NMT systems using different input representations are ensembled together in a log-linear model which is tuned for the F1 -mult metric using MERT (Och, 2003). The output of the ensemble is a pseudo-reference that is then TER aligned with the original MT to obtain OK/BAD tags for each word in the MT hypothesis. DFKI (T1): These submissions investigate alternative machine learning models for the prediction of the HTER score on the sentencelevel task. Instead of directly predicting the HTER score, the systems use a single-layer perceptron with four outputs that jointly predict the number of each of the four distinct post-editing operations that are then used to calculate the HTER score. This also gives the possibility to correct invalid (e.g. negativ"
W17-4717,W15-3037,0,0.0138062,"n in the source side of the SMT parallel corps. These features were used to train a Support Vector Regression (SVR) algorithm using a Radial Basis Function (RBF) kernel within the SCIKITLEARN toolkit.13 The γ, � and C parameters were optimised via grid search with 5-fold cross validation on the training set, resulting in γ=0.01, � = 0.0825, C = 20. This baseline system has proved robust across a range of language pairs, MT systems, and text domains for predicting various In addition to that, 6 new features were included which contain combinations of other features, and which proved useful in (Kreutzer et al., 2015; Martins et al., 2016): 12 https://github.com/ghpaetzold/ questplusplus 13 http://scikit-learn.org/ 184 • • • • Target word + left context. Target word + right context. Target word + aligned source word. POS of target word + POS of aligned source word. • Target word + left context + source word. • Target word + right context + source word. • Punctuation features: The baseline system models the task as a sequence prediction problem using the LinearChain Conditional Random Fields (CRF) algorithm within the CRFSuite tool (Okazaki, 2007). The model was trained using passive-aggressive optimisatio"
W17-4717,P16-1160,0,0.0330991,"t attention (looking at information anywhere in the source sequence during decoding) and hard monotonic attention (looking at one encoder state at a time from left to right, thus being more conservative and faithful to the original input), which are combined in different ways in the case of multi-source models. The artiﬁcial data provided by JunczysDowmunt and Grundkiewicz (2016) are used to boost performance by increasing the size of the corpus used for training. Univerzita Karlova v Praze. CUNI’s (EN-DE) system is based on the character-to-character neural network architecture described in (Lee et al., 2016). This architecture was compared with the standard neural network architecture proposed by Bahdanau et al. (2014) which uses byte-pair encoding (Sennrich et al., 2015) for generating translation tokens. During the experiments, two setups have been compared for each architecture: i) a single encoder with SRC and MT sentences concatenated, and ii) a two-encoder system, where each SRC and MT sentence is fed to a separate encoder. The submitted system uses the two-encoder architecture with a character-level encoder and decoder. The initial state of the decoder is a weighted combination of the ﬁnal"
W17-4717,W17-4733,0,0.0368107,"Missing"
W17-4717,W17-4765,1,0.786172,"Missing"
W17-4717,L16-1582,1,0.768758,"T 183 translations labelled with task-speciﬁc labels. Participants were also provided with a baseline set of features for each task, and a software package to extract these and other quality estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same fo"
W17-4717,C16-1241,0,0.0353857,"Missing"
W17-4717,W14-3342,0,0.0181229,"lity estimation features and perform model learning (Section 4.1). Participants (Section 4.2) could submit up to two systems for each task. A discussion on the main goals and ﬁndings from this year’s task is given in Section 4.7. 4.1 Baseline systems forms of post-editing effort (2012; 2013; 2014; 2015; 2016a). Word-level baseline system: For Task 2, the baseline features were extracted with the M AR MOT tool (Logacheva et al., 2016). These are 28 features that have been deemed the most informative in previous research on word-level QE. 22 of them were taken from the feature set described in (Luong et al., 2014), and had also been used as a baseline feature set at WMT16: Sentence-level baseline system: For Task 1, Q U E ST ++12 (2015) was used to extract 17 MT system-independent features from the source and translation (target) ﬁles and parallel corpora: • Word count in the source and target sentences, and source and target token count ratio. Although these features are sentencelevel (i.e. their values will be the same for all words in a sentence), the length of a sentence might inﬂuence the probability of a word being wrong. • Target token, its left and right contexts of 1 word. • Source word aligne"
W17-4717,P16-2046,0,0.0171489,"Missing"
W17-4717,W16-2379,0,0.0228966,"Missing"
W17-4717,P02-1040,0,0.119222,"Missing"
W17-4717,W16-2389,0,0.038207,"Missing"
W17-4717,W17-4736,0,0.0268357,"Missing"
W17-4717,W17-4737,0,0.0383776,"Missing"
W17-4717,W17-4738,0,0.0427074,"Missing"
W17-4717,W14-3301,1,0.655758,"Missing"
W17-4717,W16-2391,1,0.761669,"target sentences into sequences of character embeddings, and then passes them through a series of deep parallel stacked convolution/max pooling layers. The baseline features are provided through a multi-layer perceptron, 187 and then concatenated with the characterlevel information. Finally, the concatenation is passed onto another multi-layer perceptron and the very last layer outputs HTER values. The two submissions differ in the the use of standard (CNN+BASE-Single) and multi-task learning (CNN+BASE-Multi) for training. The QUEST-EMB submission follows the word embeddings approach used by (Scarton et al., 2016) for document-level QE. Here in-domain word embeddings are used instead of embeddings obtained general purpose data (same as in task 2, below). Word embeddings were averaged to generate a single vector for each sentence. Source and target word embeddings were then concatenated with the baseline features and given to an SVM regressor for model building. For the word-level task SHEF investigated a new approach based on predicting the strength of the lexical relationships between the source and target sentences (BMAPS). Following the work by (Madhyastha et al., 2014), a bilinear model is trained"
W17-4717,W16-2323,1,0.349233,"theses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target information in order to increase robustness and precision of the automatic corrections. The n-best hypotheses produced by 200 this ensemble are further re-ranked using features based on the edit distance between the original MT output and each APE hypothesis, as well as other statistical models (n-gram language model and operation sequence mod"
W17-4717,P16-1159,0,0.0156896,"different input factors, designed to jointly tackle both the APE task and the Word-Level QE task. Word-Level features which have proven effective for QE, such as word-alignments, partof-speech tags, and dependency labels, are included as input factors to neural machine translation systems, which are trained to output PostEdited MT hypotheses. Concatenated source + MT hypothesis are also used as an input representation for some models. The system makes extensive use of the synthetic training data provided by Junczys-Dowmunt and Grundkiewicz (2016), as well as min-risk training for ﬁne-tuning (Shen et al., 2016). The neural systems, which use different input representations but share the same output vocabulary, are then ensembled together in a loglinear model which is tuned for the TER metric using MERT. Fondazione Bruno Kessler. FBK’s (EN-DE & DE-EN) submission extends the existing NMT implementation in the Nematus toolkit (Sennrich et al., 2016) to train an ensemble of multi-source neural APE systems. Building on previous participations based on the phrase-based paradigm (Chatterjee et al., 2015a, 2016), and similar to (Libovick´y et al., 2016), such systems jointly learn from source and target inf"
W17-4717,2006.amta-papers.25,0,0.817721,"trast to last year, we also provide datasets for two language pairs. The structure used for the data have been the same since WMT15. Each data instance consists of (i) a source sentence, (ii) its automatic translation into the target language, (iii) the manually post-edited version of the automatic translation, (iv) a free reference translation of the source sentence. Post-edits are used to extract labels for the 4.4 Task 1: Predicting sentence-level quality This task consists in scoring (and ranking) translation sentences according to the proportion of their words that need to be ﬁxed. HTER (Snover et al., 2006b) is used as quality score, i.e. the minimum edit distance between the machine translation and its manually post-edited version. Labels HTER labels were computed using the TERCOM tool16 with default settings (tokenised, case insensitive, exact matching only), with scores capped to 1. 188 16 http://www.cs.umd.edu/˜snover/tercom/ Evaluation Evaluation was performed against the true HTER label and/or ranking, using the following metrics: with an edit operation: insertion, deletion, substitution or no edit (correct word). We mark each edited word as BAD, and the remainingn as OK. • Scoring: Pears"
W17-4717,W17-4756,0,0.0529328,"Missing"
W17-4717,P15-4020,1,0.738308,"Missing"
W17-4717,P13-4014,1,0.634274,"Missing"
W17-4717,W17-4720,1,0.830567,"Missing"
W17-4717,W17-4776,0,0.0596806,"Missing"
W17-4717,W17-4777,1,0.832756,"Missing"
W17-4717,W17-4742,0,0.0342495,"Missing"
W17-4717,W17-4744,0,0.0607458,"Missing"
W17-4717,C00-2137,0,0.0420443,"ch edited word as BAD, and the remainingn as OK. • Scoring: Pearson’s r correlation score (primary metric, ofﬁcial score for ranking submissions), Mean Average Error (MAE) and Root Mean Squared Error (RMSE). Evaluation Analogously to the last year’s task, the primary evaluation metric is the multiplication of F1 -scores for the OK and BAD classes, denoted as F1 -mult. Unlike previously used F1 BAD score this metric is not biased towards “pessimistic” labellings. We also report F1 -scores for individual classes for completeness. We test the signiﬁcance of the results using randomisation tests (Yeh, 2000) with Bonferroni correction (Abdi, 2007). • Ranking: Spearman’s ρ rank correlation and DeltaAvg. Statistical signiﬁcance on Pearson r was computed using the William’s test.17 Results Tables 13 and 14 summarise the results for Task 1 on German–English and English– German datasets, respectively, ranking participating systems best to worst using Pearson’s r correlation as primary key. Spearman’s ρ correlation scores should be used to rank systems for the ranking variant. The top three systems are the same for both datasets, and the ranking of systems according to their performance is similar for"
W17-4717,W17-4745,1,0.825998,"Missing"
W17-4717,P16-1147,0,0.0102821,"Missing"
W17-4755,W17-4757,1,0.870414,"Missing"
W17-4755,P13-1023,0,0.00853037,"Missing"
W17-4755,W17-4766,0,0.0543842,"L NGRAM 2 VEC T REE AGGREG UHH TSKM Seg-level Sys-level Hybrids • • • • • − • • • • • • • • ⊘ ⊘ ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ − • ⊘ ⊘ ⊘ ⊘ ⊘ − ⊘ ⊘ Participant Charles University (Mareˇcek et al., 2017) Charles University (Mareˇcek et al., 2017) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) ICTCAS-DCU (Ma et al., 2017) University of Tartu (T¨attar and Fishel, 2017) RWTH Aachen University (Wang et al., 2016) (Popovi´c, 2015) (Popovi´c, 2017) (Popovi´c, 2017) NRC (Lo, 2017) NRC (Lo, 2017) University of Tartu (T¨attar and Fishel, 2017) Charles University (Mareˇcek et al., 2017) (Duma and Menzel, 2017) Table 2: Participants of WMT17 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. AUTO DA incl. TECTO BEER B LEND BLEU 2 VEC SEP C HARAC T ER CHR F, incl. + and ++ MEANT 2.0 incl. NOSRL NGRAM 2 VEC T REE AGGREG http://github.com/ufal/auto-hume Baselines: BLEU, NIST CDER, PER, TER, WER SENT BLEU http://github.com/moses-smt/mosesdecoder http://gith"
W17-4755,D14-1020,1,0.396051,"Missing"
W17-4755,D16-1134,1,0.213117,"ximately 300 sentences for each of the four language pairs (from English into Czech, German, Polish and Romanian) coming from both WMT16 news translation task as well as from HimL test sets 2015,3 which are sentences from health-related texts by Cochrane and NHS 24. The reference translations are the standard WMT16 references for the news domain and post-edits of phrase-based MT for the Cochrane and NHS 24 sentences. No document structure has been preserved in this dataset. evaluations are now a little closer: both humans and metrics compare the MT output with the reference. • The HUME score (Birch et al., 2016) is a segment-level score aggregated over manual judgements of translation quality of semantic units of the source sentence. In contrast to previous years, the official method of evaluation changes, moving from “relative ranking” (RR, evaluating up to five system outputs on an annotation screen relative to each other) to DA and employing the Pearson correlation r in most cases. Due to difficulties in obtaining sufficient number of judgements for segment-level evaluation of some language pairs, we re-interpret DA judgements for these language pairs as relative comparisons and use Kendall’s τ as"
W17-4755,N16-1001,1,0.858563,"training task organizers on newstest2017, see Bojar et al. (2017b) for more details. All training task systems can be thus seen as regular submissions to the news translation task, with additional constraints in place. While one would expect these systems to produce outputs more similar to each other than the remaining news task systems, this is not the case, see Table 3 in Findings 2017. Based on the manual evaluation, training task systems however perform similarly, occupying the lower half of the ranking. the aim of providing a larger set of systems against which to evaluate metrics, as in Graham and Liu (2016). Hybrid systems were created separately for newstest2017 and himltest2017 by randomly alternating sentences from the outputs of pairs of systems of the given dataset. In short, we create 10K hybrid MT systems for each language pair. Excluding the hybrid systems, we ended up with 166 system outputs across 16 language pairs and 3 test sets. 2.3 Manual MT Quality Judgments HUME Test Set Round 2 Systems are the MT systems translating himltest2017. For each language pair, three different MT systems are provided. The translations were run by the EU project HimL and the systems cover major MT system"
W17-4755,W11-2101,1,0.897453,"Missing"
W17-4755,W13-2305,1,0.850645,"stem were to a large part included in the HUME track last year and thus leaked to the training data we provided to metrics task participants this year. The affected test set file is himltest2017a.Year1.en-pl with 324 sentences out of 340 included in the training data. The file himltest2017a.PBMT.en-pl also contains 16 known sentences, probably due to identical translation. The performance of trained metrics for en-pl evaluation have the potential to be inflated therefore. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on one screen, where Hybrid Systems are created automatically with 4 http://www.himl.eu/files/D5.4_Second_ Evaluation_Report.pdf 5 491 https://www.mturk.com scores for translations"
W17-4755,1999.mtsummit-1.31,0,0.441529,"Missing"
W17-4755,E14-1047,1,0.39054,"part included in the HUME track last year and thus leaked to the training data we provided to metrics task participants this year. The affected test set file is himltest2017a.Year1.en-pl with 324 sentences out of 340 included in the training data. The file himltest2017a.PBMT.en-pl also contains 16 known sentences, probably due to identical translation. The performance of trained metrics for en-pl evaluation have the potential to be inflated therefore. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. Furthermore, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on one screen, where Hybrid Systems are created automatically with 4 http://www.himl.eu/files/D5.4_Second_ Evaluation_Report.pdf 5 491 https://www.mturk.com scores for translations were unfairly penali"
W17-4755,N15-1124,1,0.70619,"nts. 1 Introduction Evaluating the quality of machine translation (MT) is critical for developers of MT systems to monitor progress as well as for MT users to select among available MT engines for their language pair of interest. Manual evaluation is however costly and difficult to reproduce. Automatic MT evaluation can resolve these issues, if it matches manual evaluation. The Metrics Shared Task1 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality. • In Direct Assessment (DA) (Graham et al., 2015), humans assess the quality of a given MT output translation by comparison with a reference translation (but not the source). DA is the new standard used in WMT news translation task evaluation, requiring only monolingual evaluators. The added benefit for the metrics task is that the manual and automatic 1 http://www.statmt.org/wmt17/ metrics-task.html, starting with Koehn and Monz (2006) up to Bojar et al. (2016b) 489 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 489–513 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computat"
W17-4755,W17-4769,1,0.897756,"Missing"
W17-4755,L16-1262,0,0.0253718,"Missing"
W17-4755,P02-1040,0,0.115662,"Missing"
W17-4755,W06-3114,0,0.0825673,"hared Task1 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality. • In Direct Assessment (DA) (Graham et al., 2015), humans assess the quality of a given MT output translation by comparison with a reference translation (but not the source). DA is the new standard used in WMT news translation task evaluation, requiring only monolingual evaluators. The added benefit for the metrics task is that the manual and automatic 1 http://www.statmt.org/wmt17/ metrics-task.html, starting with Koehn and Monz (2006) up to Bojar et al. (2016b) 489 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 489–513 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics available in Deliverable D5.4 of the project.2 Out selection contains approximately 300 sentences for each of the four language pairs (from English into Czech, German, Polish and Romanian) coming from both WMT16 news translation task as well as from HimL test sets 2015,3 which are sentences from health-related texts by Cochrane and NHS 24. The reference translations are"
W17-4755,W15-3049,0,0.0885115,"Missing"
W17-4755,E06-1031,0,0.16166,"Missing"
W17-4755,W17-4770,0,0.0439896,"Missing"
W17-4755,W15-3056,0,0.0246205,"Missing"
W17-4755,2006.amta-papers.25,0,0.29098,"Missing"
W17-4755,W17-4767,0,0.310193,"Missing"
W17-4755,W15-3050,0,0.209272,"Missing"
W17-4755,W17-4768,1,0.563357,"Missing"
W17-4755,W17-4771,0,0.145209,"Missing"
W17-4755,W14-3336,1,0.627786,"Missing"
W17-4755,W16-2342,0,0.124039,"han“ even for metrics where the better system receives a higher score. 494 Metric AUTO DA AUTO DA. TECTO BEER B LEND BLEU 2 VEC SEP C HARAC T ER CHR F CHR F+ CHR F++ MEANT 2.0 MEANT 2.0- NOSRL NGRAM 2 VEC T REE AGGREG UHH TSKM Seg-level Sys-level Hybrids • • • • • − • • • • • • • • ⊘ ⊘ ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ − • ⊘ ⊘ ⊘ ⊘ ⊘ − ⊘ ⊘ Participant Charles University (Mareˇcek et al., 2017) Charles University (Mareˇcek et al., 2017) ILLC – University of Amsterdam (Stanojevi´c and Sima’an, 2015) ICTCAS-DCU (Ma et al., 2017) University of Tartu (T¨attar and Fishel, 2017) RWTH Aachen University (Wang et al., 2016) (Popovi´c, 2015) (Popovi´c, 2017) (Popovi´c, 2017) NRC (Lo, 2017) NRC (Lo, 2017) University of Tartu (T¨attar and Fishel, 2017) Charles University (Mareˇcek et al., 2017) (Duma and Menzel, 2017) Table 2: Participants of WMT17 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. AUTO DA incl. TECTO BEER B LEND BLEU 2 VEC SEP C HARAC T ER CHR F, incl"
W17-4755,W15-3053,0,0.10151,"Missing"
W17-4768,W16-2302,1,0.654182,"Missing"
W17-4768,W07-0718,0,0.0185925,"h correlation with human assessment. DPMFcomb includes default metrics provided by Asiya MT evaluation toolkit (Gim´enez and M`arquez, 2010), as well as three other metrics, namely ENTF (Yu et al., 2015c), REDp (Yu et al., 2014) and DPMF (Yu et al., 2015b). Over the past two years of WMT metrics tasks, DPMFcomb has achieved the best performance for evaluation of MT of to-English language pairs. Therefore, human RR only provides relative differences in quality of a given 5 hypotheses rather than the overall absolute quality of hypotheses. Besides, the low inter-annotator agreement level in RR (Callison-Burch et al., 2007) has been a longlasting issue in MT human evaluation. The ability and the reliability of RR raise our concern whether the capability of the model trained with RR as the golden standard may be limited. Fortunately, a new emerged evaluation approach, direct assessment (DA) (Graham et al., 2013), has been proven more reliable for evaluation of metrics and was recently adopted as the official human evaluation in WMT17. DA produces absolute quality scores of hypotheses, by measuring to what extend the hypothesis adequately expresses the meaning of the reference translation, through a 1-100 continuo"
W17-4768,P08-1007,0,0.0233044,"Technology, University of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metr"
W17-4768,N15-1124,1,0.323772,"usions in section 4. 2 2.2 Although RR reflects the quality of hypotheses to some extent, it has two obvious defects. Firstly, RR provides relative ranks of the given competing MT hypotheses, which only reflects relative differences in quality rather than the absolute quality of hypotheses. On the other hand, RR suffers from low inter-annotator agreement levels. As a result, the capability of the model trained with RR as the golden standard could be limited. However, DA with carefully design of criteria (Graham et al., 2013) produces highly reliable overall quality scores for each hypothesis (Graham et al., 2015). In addition, since DA has replaced RR as the official human evaluation in the news domain in WMT17, more DA data would become available in the coming years. These motivate our new combined metric, specially designed based on DA, rather than RR, named as Blend, which means it is a metric that can blend advantages of arbitrary metrics in a combined metric that has a high correlation with human assessment. Our metric follows the basic formulation of DPMFcomb. However, since DA is an absolute quality judgment, which is different from RR, the Metrics 2.1 Review of DPMFcomb DPMFcomb utilizes human"
W17-4768,W13-2305,1,0.922921,"s, DPMFcomb has achieved the best performance for evaluation of MT of to-English language pairs. Therefore, human RR only provides relative differences in quality of a given 5 hypotheses rather than the overall absolute quality of hypotheses. Besides, the low inter-annotator agreement level in RR (Callison-Burch et al., 2007) has been a longlasting issue in MT human evaluation. The ability and the reliability of RR raise our concern whether the capability of the model trained with RR as the golden standard may be limited. Fortunately, a new emerged evaluation approach, direct assessment (DA) (Graham et al., 2013), has been proven more reliable for evaluation of metrics and was recently adopted as the official human evaluation in WMT17. DA produces absolute quality scores of hypotheses, by measuring to what extend the hypothesis adequately expresses the meaning of the reference translation, through a 1-100 continuous rating scale that facilitates reliable quality control of crowd-sourcing. Large numbers of repeat human assessments per translation are standardized and then combined into a mean score as the final quality score of the MT hypothesis. The recent development in human evaluation of MT motivat"
W17-4768,P04-1077,0,0.0500749,"Missing"
W17-4768,W16-2342,0,0.100343,"6 .637 .640 Table 5: Segment-level Pearson correlation of Blend.lex incorporating 4 other metrics for to-English language pairs on WMT16, where “avg” denotes the average Pearson correlation of all language pairs. 3.3 Trade-off between Performance and Efficiency Blend.all, and even that of Blend.lex. Since syntactic and semantic based metrics are usually complex, and the performance of Blend.lex is comparable with that of Blend.all, Blend can operate effectively with only incorporating the default lexical based metrics from Asiya toolkit. We further add 4 other metrics to Blend.lex., CharacTer(Wang et al., 2016), a novel characterbased metric; BEER(Stanojevi´c and Sima’an, 2015), a metric combining different kinds of features; DPMF and ENTF, which proved to be effective. All of these 4 metrics are convenient to use. Table 5 shows Blend.lex+4 (.640) achieves better performance than that of Blend.lex (.632), and is very close to that of Blend.all (.641) as shown in Table 3. Hence, we submit Blend.lex+4 to WMT17 Metrics task for to-English language pairs, since it provides a good trade-off between performance and efficiency for Blend. It is convenient for Blend to combine arbitrary metrics in order to a"
W17-4768,W05-0904,0,0.0373706,"ong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb was the best metric on average for toEnglish language pairs (Stanojevi´c et al., 2015; Bojar et al., 2016"
W17-4768,W15-3053,1,0.944823,"ow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb was the best metric on average for toEnglish language pairs (Stanojevi´c et al., 2015; Bojar et al., 2016). DPMFcomb incorporates lexical, syntactic and semantic based metrics, using ranking SVM1 to train parameters of each metric score and achieves a high correlation with human evaluation. Human evaluations in terms of relative ranking (RR) accumulated in WMT Metrics tasks are adopted to generate training data and to guide the training process. Human relative ranking is carried out by ranking the quality of 5 MT hypotheses of the same source segm"
W17-4768,W12-3129,0,0.0494463,"gshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb was the best metric on average for toEnglish language pairs (Stanojevi´c et al., 2015; Bojar et al., 2016). DPMFcomb incorporates lexical, syn"
W17-4768,N03-2021,0,0.0245431,"Missing"
W17-4768,C14-1193,1,0.846426,"porated as features in the form of metric scores attributed to the same hypotheses, with relative ranks as the gold standard to guide SVM-rank to learn parameters for features. When testing, the predicted ranking scores produced by DPMFcomb reflect the quality of hypotheses. DPMFcomb allows the combination of the advantages of a set of arbitrary metrics resulting in a metric with a high correlation with human assessment. DPMFcomb includes default metrics provided by Asiya MT evaluation toolkit (Gim´enez and M`arquez, 2010), as well as three other metrics, namely ENTF (Yu et al., 2015c), REDp (Yu et al., 2014) and DPMF (Yu et al., 2015b). Over the past two years of WMT metrics tasks, DPMFcomb has achieved the best performance for evaluation of MT of to-English language pairs. Therefore, human RR only provides relative differences in quality of a given 5 hypotheses rather than the overall absolute quality of hypotheses. Besides, the low inter-annotator agreement level in RR (Callison-Burch et al., 2007) has been a longlasting issue in MT human evaluation. The ability and the reliability of RR raise our concern whether the capability of the model trained with RR as the golden standard may be limited."
W17-4768,niessen-etal-2000-evaluation,0,0.0405302,": a Novel Combined MT Metric Based on Direct Assessment — CASICT-DCU submission to WMT17 Metrics Task Qingsong Ma1 Yvette Graham2 Shugen Wang1 Qun Liu2,1 1 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, University of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of w"
W17-4768,C10-2175,0,0.0203279,"ity of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective method to take advantage of the merits of existing metrics is to combine quality scores assigned by these metrics, like DPMFcomb (Yu et al., 2015a). In WMT15 and WMT16 Metrics tasks, DPMFcomb"
W17-4768,P02-1040,0,0.116198,"on of metrics incorporated in Blend, in order to find a trade-off between performance and efficiency. 1 Introduction Automatic machine translation evaluation (AMTE) has received much attention in recent years, with the aim of providing quick and stable measurements of the performance of machine translation (MT) systems. Various metrics for AMTE have been proposed and most operate via computation of the similarity between the MT hypothesis and the reference translation. However, different metrics focus on different perspectives in terms of measuring similarity. For lexical based metrics, BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) count n-gram co-occurrence, 1 598 http://www.cs.cornell.edu/People/tj/svm light/svm rank.html Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 598–603 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics WMT15 WMT16 cs-en 500 560 de-en 500 560 fi-en 500 560 ro-en − 560 ru-en 500 560 tr-en − 560 en-ru 500 560 Table 1: The number of sampled DA data for each language pair in WMT15 and WMT16. cess, metrics are incorporated as features in the form of metric scores attributed to the same"
W17-4768,W09-0441,0,0.023715,"ASICT-DCU submission to WMT17 Metrics Task Qingsong Ma1 Yvette Graham2 Shugen Wang1 Qun Liu2,1 1 Key Laboratory of Intelligent Information Processing, Institute of Computing Technology, University of Chinese Academy of Sciences 2 ADAPT Centre, School of Computing, Dublin City University maqingsong@ict.ac.cn, graham.yvette@gmail.com wangshugen@ict.ac.cn, qun.liu@dcu.ie Abstract Meteor (Denkowski and Lavie, 2014) and GTM (Melamed et al., 2003) catch different kinds of matches, ROUGE (Lin and Och, 2004) captures common subsequences, WER (Nießen et al., 2000), PER (Tillmann et al., 1997) and TER (Snover et al., 2009) compute the post-editing distance between the hypothesis and the reference translation. Syntactic based metrics mainly use shallow syntactic structures (Chan and Ng, 2008; Zhu et al., 2010), dependency tree structures or constituent tree structures (Owczarzak et al., 2007; Liu and Gildea, 2005). Semantic measures (Lo et al., 2012) and discourse similarity based metrics (Guzm´an et al., 2014) have also been proposed. Different metrics evaluate similarity between hypotheses and reference translations from various perspectives, each of which has pros and cons. One straightforward and effective m"
W17-4768,W15-3031,0,0.0524792,"Missing"
W17-4768,W15-3050,0,0.454035,"Missing"
W17-4768,W14-3348,0,\N,Missing
W18-3601,P11-2040,1,0.888439,"Missing"
W18-3601,W11-2832,1,0.629164,"e languages may also work for others. The SR’18 task is to generate sentences from structures at the level of abstraction of outputs in state-of-the-art parsing, encouraging participants to explore the extent to which neural network parsing algorithms can be reversed for generation. SR’18 also addresses questions about just how suitable and useful the notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc"
W18-3601,W17-4755,1,0.838761,"gy) of 100 outputs, of which 20 are used solely for quality assurance (QA) (i.e. do not count towards system scores): (i) some are repeated as are, (ii) some are repeated in a ‘damaged’ version and (iii) some are replaced by their corresponding reference texts. In each case, a minimum threshold has to be reached for the HIT to be accepted: for (i), scores must be similar enough, for (ii) the score for the damaged version must be worse, and for (iii) the score for the reference text must be high. For full details of how these additional texts are created and thresholds applied, please refer to Bojar et al. (2017a). Below we report QA figures for the MTurk evaluations (Section 3.2.1). Code: We were able to reuse, with minor adaptations, the code produced for the WMT’17 evaluations.10 3.2.2 Google Data Compute Evaluation In order to cover more languages, and to enable comparison between crowdsourced and expert evaluation, we also conducted human evaluations using Google’s internal ‘Data Compute’ system evaluation service, where experienced evaluators carefully assess each system output. We used an interface that matches the WMT’17 interface above, as closely as was possible within the constraints of th"
W18-3601,P17-1017,0,0.0630018,"he notion of universal dependencies—which is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD str"
W18-3601,D14-1020,1,0.831205,"Missing"
W18-3601,S17-2090,0,0.0582517,"h is in the process of becoming the dominant linguistic formalism across a wide range of NLP applications, parsing in particular—is for NLG. SR’18 follows the SR’11 pilot surface realisation task for English (Belz et al., 2011) which was part of Generation Challenges 2011 (GenChal’11), the fifth round of shared-task evaluation competitions (STECs) involving the language generation tasks. Outside of the SR tasks, just three ‘deep’ NLG shared tasks focusing on language generation from abstract semantic representations have been organised to date: WebNLG3 (Gardent et al., 2017), SemEval Task 94 (May and Priyadarshi, 2017), and E2E5 (Novikova et al., 2017). What is more, these 2 See the recent parsing shared task based on UDs (Nivre and de Marneffe et al., 2016): http:// universaldependencies.org/conll17/. 3 http://talc1.loria.fr/webnlg/stories/ challenge.html 4 http://alt.qcri.org/semeval2017/ task9/ 5 http://www.macs.hw.ac.uk/ InteractionLab/E2E/ tasks have only been offered for English. As in SR’11, the Multilingual Surface Realisation shared task (SR’18) comprises two tracks with different levels of difficulty: Shallow Track: This track starts from genuine UD structures in which word order information has b"
W18-3601,W04-2705,0,0.458005,"Missing"
W18-3601,L16-1262,0,0.0728681,"Missing"
W18-3601,W17-5525,0,0.0981398,"Missing"
W18-3601,J05-1004,0,0.102132,"in CoNLL-U format, with no meta-information.7 Figures 1, 2 and 3 show 6 universaldependencies.org 7 http://universaldependencies.org/ a sample original UD annotation for English, and the corresponding shallow and deep input structures derived from it. To create inputs to the Shallow Track, the UD structures were processed as follows: 1. Word order information was removed by randomised scrambling; 2. Words were replaced by their lemmas. For the Deep Track, the following steps were additionally carried out: 3. Edge labels were generalised into predicate/argument labels, in the PropBank/NomBank (Palmer et al., 2005; Meyers et al., 2004) fashion. That is, the syntactic relations were mapped to core (A1, A2, etc.) and non-core (AM) labels, applying the following rules: (i) the first argument is always labeled A1 (i.e. there is no external argument A0); (ii) in order to maintain the tree structure and account for some cases of shared arguments, there can be inverted argument relations; (iii) all modifier edges are assigned the same generic label AM; (iv) there is a coordinating relation; see the inventory of relations in Table 1. 4. Functional prepositions and conjunctions in argument position (i.e. prepos"
W18-3601,P02-1040,0,0.102984,"nt Example fall→ the ball the ball→ fall fall→ last night fall→ [and] bounce Tower→ Eiffel N/A Table 1: Deep labels. train dev test ar 6,016 897 676 cs 66,485 9,016 9,876 en 12,375 1,978 2,061 es 14,289 1,651 1,719 fi 12,030 1,336 1,525 fr 14,529 1,473 416 it 12,796 562 480 nl 12,318 720 685 pt 8,325 559 476 ru 48,119 6,441 6,366 Table 2: SR’18 dataset sizes for training, development and test sets. 3 3.1 Evaluation Methods Automatic methods We used BLEU, NIST, and inverse normalised character-based string-edit distance (referred to as DIST, for short, below) to assess submitted systems. BLEU (Papineni et al., 2002) is a precision metric that computes the geometric mean of the n-gram precisions between generated text and reference texts and adds a brevity penalty for shorter sentences. We use the smoothed version and report results for n = 4. NIST9 is a related n-gram similarity metric weighted in favour of less frequent n-grams which are taken to be more informative. Inverse, normalised, character-based string-edit distance (DIST in the tables below) starts by computing the minimum number of character inserts, deletes and substitutions (all at cost 1) required to turn the system output into the (single)"
W18-6401,W18-6432,1,0.779979,"Missing"
W18-6401,W07-0718,1,0.492999,"Participants were asked to build machine translation systems for any of 7 language pairs in both directions, to be evaluated on a test set of news stories. The main metric for this task is human judgment of translation quality. This year, we also opened up the task to additional test sets to probe specific aspects of translation. 1 Introduction The Third Conference on Machine Translation (WMT) held at EMNLP 20181 host a number of shared tasks on various aspects of machine translation. This conference builds on twelve previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants we"
W18-6401,W11-2101,1,0.731598,"Missing"
W18-6401,W08-0309,1,0.64499,"Missing"
W18-6401,W10-1703,1,0.498736,"Missing"
W18-6401,W12-3102,1,0.647494,"Missing"
W18-6401,E14-2008,0,0.0242419,"n the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated data. At inference time, translations which are copies of the source are filtered out, replacing them with the output of a very small news-commentary only"
W18-6401,E17-2058,0,0.0576994,"Missing"
W18-6401,W18-6406,0,0.107817,"t (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous y"
W18-6401,W13-2305,1,0.876146,"017 (English) and news 2011 (Chinese). Subwords (BPE) are used for both English and Chinese sentences. 3 Human Evaluation A human evaluation campaign is run each year to assess translation quality and to determine the final ranking of systems taking part in the competition. This section describes how preparation of evaluation data, collection of human assessments, and computation of the official results of the shared task was carried out this year. Work on evaluation over the past few years has provided fresh insight into ways to collect direct assessments (DA) of machine translation quality (Graham et al., 2013, 2014, 2016), and two years ago the evaluation campaign included parallel assessment of a subset of News task language pairs evaluated with relative ranking (RR) and DA. DA has some clear advantages over RR, namely the evaluation of absolute translation quality and the ability to carry out evaluations through quality controlled crowd-sourcing. As established in 2016 (Bojar et al., 2016a), DA results (via crowd-sourcing) and RR results (produced by researchers) correlate strongly, with Pearson correlation ranging from 0.920 to 0.997 across several source languages into English and at 0.975 for"
W18-6401,E14-1047,1,0.89975,"Missing"
W18-6401,W18-6407,1,0.888041,"ns Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered pa"
W18-6401,W18-6410,0,0.0609282,"ions, organized into 35 teams are listed in Table 2 and detailed in the rest of this section. Each system did not necessarily appear in all translation tasks. We also included 39 online MT systems (originating from 5 services), which we anonymized as ONLINE -A,B,F,G. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, these online and commercial systems are treated as unconstrained during the automatic and human evaluations. 2.3.1 A ALTO (Grönroos et al., 2018) Aalto participated in the constrained condition of the multi-lingual subtrack, with a single system trained to translate from English to both Finnish 3 http://www.yandex.com/ Estonian Research Council institutional research grant IUT20-56: “Computational models of the Estonian Language” 4 5 As of Fall 2011, the proceedings of the European Parliament are no longer translated into all official languages. 273 Europarl Parallel Corpus German ↔ English Czech ↔ English Finnish ↔ English Estonian ↔ English Sentences 1,920,209 646,605 1,926,114 652,944 Words 50,486,398 53,008,851 14,946,399 17,376,43"
W18-6401,D18-1045,0,0.0609466,"Missing"
W18-6401,W11-2123,0,0.0087119,"t sets. The second is a Marian (Junczys-Dowmunt et al., 2018) system ensembling 5 Univ. Edinburgh “bi-deep” and 6 transformer models all trained on the WMT18 bitexts provided, including ParaCrawl. Some models employed pretrained word embeddings built on BPE’d corpora (Sennrich et al., 2016). A Marian transformer model performed right-to-left rescoring for this system. The third system is trained with Moses (Koehn et al., 2007), using the same data as the Marian system. Hierarchical reordering and Operation Sequence Model were employed. The 5-gram English language model was trained with KenLM (Heafield, 2011) on the same corpus as the AFRL WMT15 system with the same BPE used in the Marian systems. Lastly, RWTH Jane’s system combination (Freitag et al., 2014) was applied yielding approximately a +0.5 gain in BLEU. 2.3.3 CUNI-KOCMI (Kocmi et al., 2018) 2.3.6 FACEBOOK -FAIR ? (Edunov et al., 2018) FACEBOOK -FAIR is an ensemble of six selfattentional models with back-translation data according to Edunov et al. (2018). Synthetic sources are sampled instead of beam search, oversampling the real bitext at a rate of 16, i.e., each bitext is sampled 16 times more often per epoch than the back-translated da"
W18-6401,E17-3017,1,0.773822,"eriment, right-toleft reranking does not help. Another focus is 277 (SMT) submission for the Finnish morphology test suite (Burlot et al., 2018). given to data filtering through rules, translation model and language model including parallel data and monolingual data. The language model is based the Transformer architecture as well. The final system is trained with four different seeds and mixed data. 2.3.8 2.3.9 JHU (Koehn et al., 2018a) The JHU systems are the result of two relatively independent efforts on German–English language directions and Russian–English, using the Marian and Sockeye (Hieber et al., 2017) neural machine translation toolkits, respectively. The novel contributions are iterative back-translation (for German) and fine-tuning on test sets from prior years (for both languages). HY (Raganato et al., 2018; Hurskainen and Tiedemann, 2017) The University of Helsinki (HY) submitted four systems: HY-AH, HY-NMT, HY-NMT-2 STEP and HY-SMT. 2.3.10 JUCBNMT (Mahata et al., 2018) JUCBNMT is an encoder-decoder sequence-tosequence NMT model with character level encoding. The submission uses preprocessing like tokenization, truecasing and corpus cleaning. Both encoder and decoder use a single LSTM"
W18-6401,P17-4012,0,0.0266435,"ted paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implementation in OpenNMT-py (Klein et al., 2017). It is trained on filtered parallel and filtered back-translated monolingual data. The main contribution is a novel cross-lingual Morfessor (Virpioja et al., 2013) segmentation using cognates extracted from the parallel data. The aim is to improve the consistency of the morphological segmentation. Aalto decode using an ensemble of 3 (et) or 8 (fi) models. 2.3.4 2.3.2 2.3.5 AFRL The CUNI-KOCMI submission focuses on the low-resource language neural machine translation (NMT). The final submission uses a method of transfer learning: the model is pretrained on a related high-resource language (her"
W18-6401,W18-6413,0,0.01703,"he constrained systems, however, the data, taking into account its relatively large size, was not factored. T ENCENT (Wang et al., 2018a) T ENCENT- ENSEMBLE (called TenTrans) is an improved NMT system on Transformer based on self-attention mechanism. In addition to the basic settings of Transformer training, T ENCENTENSEMBLE uses multi-model fusion techniques, multiple features reranking, different segmentation models and joint learning. Additionally, data selection strategies were adopted to fine-tune the trained system, achieving a stable performance improvement. An additional system paper (Hu et al., 2018) describes a non-primary submission. 2.3.29 TILDE 2.3.30 U BIQUS The U BIQUS -NMT system is probably developed by the Ubiqus company (www.ubiqus.com). No further information is available. 2.3.31 UCAM (Stahlberg et al., 2018) UCAM is a generalization of previous work (de Gispert et al., 2017) to multiple architectures. It is a system combination of two Transformer-like models, a recurrent model, a convolutional model, and a phrase-based SMT system. The output is probably dominated by the Transformer, and to some extend by the SMT system. (Pinnis et al., 2018) submitted four systems: TILDE - C -"
W18-6401,W18-6416,1,0.791378,"Missing"
W18-6401,W17-4730,0,0.0118952,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6417,1,0.821949,"as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016a, 2017). This year we conducted several official tasks. We report in this paper on the news translation task. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Neves et al., 2018), • multimodal machine translation (Barrault et al., 2018), • metrics (Ma et al., 2018), • quality estimation (Specia et al., 2018), • automatic post-editing (Chatterjee et al., 2018), and • parallel corpus filtering (Koehn et al., 2018b). In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We held 1 2 http://www.statmt.org/wmt18/ 272 http://statmt.org/wmt18/results.html Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 272–303 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64028 tions are also available for interactive visualization and comparison of diff"
W18-6401,W17-4706,0,0.0132191,". HY-SMT (Tiedemann et al., 2016) is the Helsinki SMT system submitted at WMT 2016 (the constrained-basic+back-translated version). The system was not retrained and it may thus suffer from poor lexical coverage on recent test data. The main motivation for including this baseline was to have a statistical machine translation 2.3.13 LMU- NMT (Huck et al., 2018) For the WMT18 news translation shared task, LMU Munich (Huck et al., 2018) has trained ba278 2.3.15 sic shallow attentional encoder-decoder systems (Bahdanau et al., 2014) with the Nematus toolkit (Sennrich et al., 2017), like last year (Huck et al., 2017a). LMU has participated with these NMT systems for the English–German language pair in both translation directions. The training data is a concatenation of Europarl, News Commentary, Common Crawl, and some synthetic data in the form of backtranslated monolingual news texts. The 2017 monolingual News Crawl is not employed, nor are the parallel Rapid and ParaCrawl corpora. The German data is preprocessed with a linguistically informed word segmentation technique (Huck et al., 2017b). By using a linguistically more sound word segmentation, advantages over plain BPE segmentation are expected in t"
W18-6401,W18-6430,0,0.299738,"of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Part"
W18-6401,W18-6427,0,0.0533796,"Missing"
W18-6401,W18-6428,0,0.0966755,"D U NISOUND U NSUP TARTU Institution Aalto University (Grönroos et al., 2018) Air Force Research Laboratory (Gwinnup et al., 2018) Alibaba Group (Deng et al., 2018) Charles University (Kocmi et al., 2018) Charles University (Popel, 2018) Facebook AI Research (Edunov et al., 2018) Global Tone Communication Technology (Bei et al., 2018) University of Helsinki (Raganato et al., 2018) Johns Hopkins University (Koehn et al., 2018a) Jadavpur University (Mahata et al., 2018) Karlsruhe Institute of Technology (Pham et al., 2018) Li Muze (no associated paper) LMU Munich (Huck et al., 2018) LMU Munich (Stojanovski et al., 2018) Microsoft (Junczys-Dowmunt, 2018) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2018) ModernMT, MMT s.r.l. (no associated paper) University of Tartu (Tars and Fishel, 2018) National Institute of Information and Communications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Ca"
W18-6401,W18-6431,0,0.0938702,"nications Technology (Marie et al., 2018) Northeastern University / NiuTrans Co., Ltd. (Wang et al., 2018b) NLP Group, Nanjing University (no associated paper) NTT Corporation (Morishita et al., 2018) Bo˘gaziçi University (Biçici, 2018) PROMT LLC (Molchanov, 2018) RWTH Aachen (Schamper et al., 2018) RWTH Aachen (Graça et al., 2018) TALP, Technical University of Catalonia (Casas et al., 2018) Tencent (Wang et al., 2018a) Tilde (Pinnis et al., 2018) Ubiqus (no associated paper) University of Cambridge (Stahlberg et al., 2018) University of Edinburgh (Haddow et al., 2018) University of Maryland (Xu and Carpuat, 2018) Unisound (no associated paper) University of Tartu (Del et al., 2018) Table 2: Participants in the shared translation task. Not all teams participated in all language pairs. The translations from the commercial and online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. “?” indicates invited participation with a late submission, where the team is not considered a regular participant. 276 and Estonian. The system is based on the Transformer (Vaswani et al., 2017) implement"
W18-6401,W16-2326,0,0.0278975,"Missing"
W18-6450,W11-2101,1,0.77912,"ent (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment of individual translations in isolation from other outputs. Translation adequacy is structured as a monolingual assessment of similarity of meaning where the target language reference translation and the MT output are displayed to the human assessor. Assessors rate a given translation by how adequately it expresses the meaning of the reference translation on an analogue scale corresponding to an underlying 0-100 rating scale.5 Large numbers of DA human assessments of translations for all 14 language pairs included in the News Translation Task were collected fr"
W18-6450,W17-4755,1,0.502389,"metric carries out a comparison of MT system output translations and human-produced reference translations to produce a single overall • In Direct Assessment (DA) (Graham et al., 2016), humans assess the quality of a given MT output translation by comparison with a reference translation (as opposed to the source and reference). DA is the new standard used in WMT 1 The availability of a reference translation is the key difference between our task and MT quality estimation, where no reference is assumed. 2 http://www.statmt.org/wmt18/metrics-task. html, starting with Koehn and Monz (2006) up to Bojar et al. (2017) 671 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 671–688 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64077 News Translation Task evaluation, requiring only monolingual evaluators. translations. If the task includes a wide range of systems of varying quality, however, or systems are quite different in nature, this could in some way make the task easier for metrics, with metrics that are more sensitive to certain aspects of MT output performing better."
W18-6450,W14-3333,1,0.881849,"riction in WMT human evaluation prior to the introduction of DA, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT18. In the system-level evaluation, the goal is to assess the quality of translation of an MT system for the whole test set. Our manual scoring method, DA, nevertheless proceeds sentence by sentence, aggregating the final score as described below. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment of individual transla"
W18-6450,W18-6432,1,0.880138,"uality. The Metrics Shared Task2 of WMT annually evaluates the performance of automatic machine translation metrics in their ability to provide a substitute for human assessment of translation quality. Again, we keep the two main types of metric evaluation unchanged from the previous years. In system-level evaluation, each metric provides a quality score for the whole translated test set (usually a set of documents, in fact). In segment-level evaluation, a score is assigned by a given metric to every individual sentence. The underlying texts and MT systems come from the News Translation Task (Bojar et al., 2018, denoted as Findings 2018 in the following). The texts were drawn from the news domain and involve translations to/from Chinese (zh), Czech (cs), German (de), Estonian (et), Finnish (fi), Russian (ru), and Turkish (tr), each paired with English, making a total of 14 language pairs. A single form of golden truth of translation quality judgement is used this year: Abstract This paper presents the results of the WMT18 Metrics Shared Task. We asked participants of this task to score the outputs of the MT systems involved in the WMT18 News Translation Task with automatic metrics. We collected scor"
W18-6450,N15-1124,1,0.936055,"Missing"
W18-6450,W14-3348,0,0.0229664,"task. Some details are provided in the system description paper (?). YiSi-1 measures the relative lexical semantic similarity (weighted word embeddings cosine similarity aggregated into n-grams similarity) of the candidate and reference translations, optionally taking the shallow semantic structure (“srl”) into account. YiSi-0 is a degenerate resource-free version using the longest common character substring, instead of word embeddings cosine similarity, to measure the word similarity of the candidate and reference translations. meteor++ meteor++ (Guo et al., 2018) is metric based on Meteor (Denkowski and Lavie, 2014), adding explicing treatment of “copy-words”, i.e. words that are likely to be preserved across all paraphrases of a sentence in a given language. 2.4.6 2.4.9 Baseline Metrics As mentioned by Bojar et al. (2016), Metrics Task occasionally suffers from “loss of knowledge” when successful metrics participate only in one year. We attempt to avoid this by regularly evaluating also a range of “baseline metrics”: RUSE RUSE (Shimanaka et al., 2018) is a perceptron regressor based on three types of sentence embeddings: Infersent, Quick-Thought and Universal Sentence Encoder, designed with the aim to u"
W18-6450,W18-6454,0,0.0421465,"as having statistically significant difference in performance. 6 Due to an error in the write-up for WMT17 (errata to follow), this second change was not properly reflected in the paper, only in the evaluation scripts. 675 Metric Seg-level Sys-level Hybrids • • • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • ⋆ ⊘ ⊘ ⊘ ⊘ BEER BLEND CharacTer ITER meteor++ RUSE UHH_TSKM YiSi-* Participant ILLC – University of Amsterdam (Stanojević and Sima’an, 2015) Tencent-MIG-AI Evaluation & Test Lab (Ma et al., 2017) RWTH Aachen University (Wang et al., 2016a) Jadavpur University (Panja and Naskar, 2018) Peking University (Guo et al., 2018) Tokyo Metropolitan University (Shimanaka et al., 2018) (Duma and Menzel, 2017) NRC (Lo, 2018) Table 2: Participants of WMT18 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. “⋆” indicates that the original ITER system-level scores should be calculated as the micro-average of segment-level scores but we calculate them as simple macro-averaged fo"
W18-6450,W17-4766,0,0.167842,"n error in the write-up for WMT17 (errata to follow), this second change was not properly reflected in the paper, only in the evaluation scripts. 675 Metric Seg-level Sys-level Hybrids • • • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • ⋆ ⊘ ⊘ ⊘ ⊘ BEER BLEND CharacTer ITER meteor++ RUSE UHH_TSKM YiSi-* Participant ILLC – University of Amsterdam (Stanojević and Sima’an, 2015) Tencent-MIG-AI Evaluation & Test Lab (Ma et al., 2017) RWTH Aachen University (Wang et al., 2016a) Jadavpur University (Panja and Naskar, 2018) Peking University (Guo et al., 2018) Tokyo Metropolitan University (Shimanaka et al., 2018) (Duma and Menzel, 2017) NRC (Lo, 2018) Table 2: Participants of WMT18 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. “⋆” indicates that the original ITER system-level scores should be calculated as the micro-average of segment-level scores but we calculate them as simple macro-averaged for the hybrid systems. See the ITER paper for more details. BEER BLEND CharacTer"
W18-6450,W06-3114,0,0.125502,"usual set-up, an automatic metric carries out a comparison of MT system output translations and human-produced reference translations to produce a single overall • In Direct Assessment (DA) (Graham et al., 2016), humans assess the quality of a given MT output translation by comparison with a reference translation (as opposed to the source and reference). DA is the new standard used in WMT 1 The availability of a reference translation is the key difference between our task and MT quality estimation, where no reference is assumed. 2 http://www.statmt.org/wmt18/metrics-task. html, starting with Koehn and Monz (2006) up to Bojar et al. (2017) 671 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 671–688 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64077 News Translation Task evaluation, requiring only monolingual evaluators. translations. If the task includes a wide range of systems of varying quality, however, or systems are quite different in nature, this could in some way make the task easier for metrics, with metrics that are more sensitive to certain aspects of MT"
W18-6450,D14-1020,1,0.8977,"r computation of r for metrics, we compare metrics via the absolute value of a given metric’s correlation with human assessment. Table 4 provides the system-level correlations of metrics evaluating translation of newstest2018 into English while Table 5 provides the same for out-of-English language pairs. The underlying texts are part of the WMT18 News Translation test set (newstest2018) and the underlying MT systems are all MT systems participating in the WMT18 News Translation Task with the exception of a single tr-en system not included in the initial human evaluation run. As recommended by Graham and Baldwin (2014), we employ Williams significance test (Williams, 1959) to identify differences in correlation that are statistically significant. Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in Tables 4 and 5. Since pairwise comparisons of metrics may be also of interest, e.g. to learn which metrics We run chrF++.py with the parameters -nw 0 -b 3 to obtain the chrF score and with -nw 0 -b 1 to obtain the chrF"
W18-6450,W04-3250,0,0.366364,"Missing"
W18-6450,N16-1001,1,0.948707,", see below for details and references. Section 2 describes our datasets, i.e. the sets of underlying sentences, system outputs, human judgements of translation quality and also participating metrics. Sections 3.1 and 3.2 then provide the results of system and segment-level metric evaluation, respectively. We discuss the results in Section 4. 2 News Task Systems are machine translation systems participating in the WMT18 News Translation Task (see Findings 2018).3 Hybrid Systems are created automatically with the aim of providing a larger set of systems against which to evaluate metrics, as in Graham and Liu (2016). Hybrid systems were created for newstest2018 by randomly selecting a pair of MT systems from all systems taking part in that language pair and producing a single output document by randomly selecting sentences from either of the two systems. In short, we create 10K hybrid MT systems for each language pair. Data This year, we provided the task participants with one test set along with reference translations and outputs of MT systems. Participants were free to choose which language pairs they wanted to participate and whether they reported system-level, segment-level scores or both. 2.1 Exclud"
W18-6450,E06-1031,0,0.123079,"UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that exploit the syntactic structure of the sentences. Optionally, the match can also be performed for the candidate and a pseudoreference (i.e. a translation by another MT system) or for the source sentence and the • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. When tokenizing, we also convert all outputs to lowercase. 7 677 http://www.itl.nist.gov/iad/mig/tools/ 3 Results Since Moses scorer is versioned on Github, we strongly encourage authors of highperforming metrics to add them to Moses scorer, as this will ensure that their metric can be easily included in future tasks. We discuss system-level results for news task systems in Section 3.1. The segment-level results are in Se"
W18-6450,W13-2305,1,0.82848,"sentence length restriction in WMT human evaluation prior to the introduction of DA, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT18. In the system-level evaluation, the goal is to assess the quality of translation of an MT system for the whole test set. Our manual scoring method, DA, nevertheless proceeds sentence by sentence, aggregating the final score as described below. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment"
W18-6450,W17-4768,1,0.855089,"Missing"
W18-6450,E14-1047,1,0.86335,"riction in WMT human evaluation prior to the introduction of DA, the simplified DA setup does not require restriction of the evaluation in this respect and no sentence length restriction was applied in DA WMT18. In the system-level evaluation, the goal is to assess the quality of translation of an MT system for the whole test set. Our manual scoring method, DA, nevertheless proceeds sentence by sentence, aggregating the final score as described below. Direct Assessment (DA) This year the translation task employed monolingual direct assessment (DA) of translation adequacy (Graham et al., 2013; Graham et al., 2014a; Graham et al., 2016). Since sufficient levels of agreement in human assessment of translation quality are difficult to achieve, the DA setup simplifies the task of translation assessment (conventionally a bilingual task) into a simpler monolingual assessment. In addition, DA avoids bias that has been problematic in previous evaluations introduced by assessment of several alternate translations on a single screen, where scores for translations had been unfairly penalized if often compared to high quality translations (Bojar et al., 2011). DA therefore employs assessment of individual transla"
W18-6450,W16-2342,0,0.312471,"sue that shorter translations normally achieve lower TER. Similarly to other character-level metrics, CharacTer is applied to non-tokenized outputs and references, which also holds for this year’s submission. This year tokenization was carried out for en-ru hypotheses and reference before calculating the scores, since this results in large improvements in terms of correlations. For other language pairs a tokenizer was not used for pre-processing. A python library was used for calculating the Levenshtein distance, so that the metric is now about 7 times faster than before. CharacTer CharacTer (Wang et al., 2016b; Wang et al., 2016a), identical to the 2016 setup, is a character-level metric inspired by the commonly applied translation edit rate (TER). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches the reference, normalized by the length of the hypothesis sentence. CharacTer calculates the characterlevel edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TER, a hypothesis word is considered to match a reference word and could be shifted, if the edit distance between them is below a th"
W18-6450,W13-2202,1,0.624348,"Sentence Encoder, designed with the aim to utilize global sentence information that cannot be captured by local features based on character or word n-grams. The sentence embeddings come from pre-trained models and the regression itself is trained on past manual judgements in WMT shared tasks. 2.4.7 • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 that is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Macháček and Bojar, 2013). UHH_TSKM UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that exploit the syntactic structure of the sentences. Optionally, the match can also be performed for the candidate and a pseudoreference (i.e. a translation by another MT system) or for the source sentence and the • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER"
W18-6450,W18-6455,0,0.047048,"R). It is defined as the minimum number of character edits required to adjust a hypothesis, until it completely matches the reference, normalized by the length of the hypothesis sentence. CharacTer calculates the characterlevel edit distance while performing the shift edit on word level. Unlike the strict matching criterion in TER, a hypothesis word is considered to match a reference word and could be shifted, if the edit distance between them is below a threshold value. The Levenshtein distance between the reference and 676 2.4.4 ITER candidate back-translated into the source language. ITER (Panja and Naskar, 2018) is an improved Translation Edit/Error Rate (TER) metric. In addition to the basic edit operations in TER (insertion, deletion, substitution and shift), ITER also allows stem matching and uses optimizable edit costs and better normalization. Note that for segment-level evaluation, we reverse the sign of the score, so that better translations get higher scores. For systemlevel confidence, we calculate the system-level scores for hybrids systems slightly differently than the original ITER definition would require. We use the unweighted arithmetic average of segment-level scores (macro-average) w"
W18-6450,P02-1040,0,0.104437,"ful metrics participate only in one year. We attempt to avoid this by regularly evaluating also a range of “baseline metrics”: RUSE RUSE (Shimanaka et al., 2018) is a perceptron regressor based on three types of sentence embeddings: Infersent, Quick-Thought and Universal Sentence Encoder, designed with the aim to utilize global sentence information that cannot be captured by local features based on character or word n-grams. The sentence embeddings come from pre-trained models and the regression itself is trained on past manual judgements in WMT shared tasks. 2.4.7 • Mteval. The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using the script mteval-v13a.pl7 that is used in the OpenMT Evaluation Campaign and includes its own tokenization. We run mteval with the flag --international-tokenization since it performs slightly better (Macháček and Bojar, 2013). UHH_TSKM UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that"
W18-6450,W15-3049,0,0.255195,"Missing"
W18-6450,W17-4770,0,0.11381,"Missing"
W18-6450,W18-6456,0,0.216819,"n performance. 6 Due to an error in the write-up for WMT17 (errata to follow), this second change was not properly reflected in the paper, only in the evaluation scripts. 675 Metric Seg-level Sys-level Hybrids • • • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • ⋆ ⊘ ⊘ ⊘ ⊘ BEER BLEND CharacTer ITER meteor++ RUSE UHH_TSKM YiSi-* Participant ILLC – University of Amsterdam (Stanojević and Sima’an, 2015) Tencent-MIG-AI Evaluation & Test Lab (Ma et al., 2017) RWTH Aachen University (Wang et al., 2016a) Jadavpur University (Panja and Naskar, 2018) Peking University (Guo et al., 2018) Tokyo Metropolitan University (Shimanaka et al., 2018) (Duma and Menzel, 2017) NRC (Lo, 2018) Table 2: Participants of WMT18 Metrics Shared Task. “•” denotes that the metric took part in (some of the language pairs) of the segment- and/or system-level evaluation and whether hybrid systems were also scored. “⊘” indicates that the system-level and hybrids are implied, simply taking arithmetic average of segment-level scores. “⋆” indicates that the original ITER system-level scores should be calculated as the micro-average of segment-level scores but we calculate them as simple macro-averaged for the hybrid systems. See the ITER paper for more detai"
W18-6450,2006.amta-papers.25,0,0.103061,"ter (Macháček and Bojar, 2013). UHH_TSKM UHH_TSKM (Duma and Menzel, 2017) is a non-trained metric utilizing kernel functions, i.e. methods for efficient calculation of overlap of substructures between the candidate and the reference translations. The metric uses both sequence kernels, applied on the tokenized input data, together with tree kernels, that exploit the syntactic structure of the sentences. Optionally, the match can also be performed for the candidate and a pseudoreference (i.e. a translation by another MT system) or for the source sentence and the • Moses Scorer. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. To tokenize the sentences, we used the standard tokenizer script as available in Moses toolkit. When tokenizing, we also convert all outputs to lowercase. 7 677 http://www.itl.nist.gov/iad/mig/tools/ 3 Results Since Moses scorer is versioned on Github, we strongly encourage authors of highperforming metrics to add them to Moses scorer, as this will ensure that their metric can be easily included in future tasks. We discuss system-level results for news task systems in Section"
W18-6450,W15-3050,0,0.0885999,"Missing"
W19-5301,W19-5424,1,0.858444,"Missing"
W19-5301,W19-5306,0,0.248769,"al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated"
W19-5301,D18-1549,0,0.116951,"s are available for this system. 2.5.7 BASELINE - RE - RERANK (no associated CUNI-T RANSFORMER -T2T2018 (Popel, 2018) is the exact same system as used last year. paper) BASELINE - RE - RERANK is a standard Transformer, with corpus filtering, pre-processing, postprocessing, averaging and ensembling as well as n-best list reranking. 2.5.8 CUNI-T RANSFORMER -M ARIAN (Popel et al., 2019) is a “reimplementation” of the last year’s system (Popel, 2018) in Marian (JunczysDowmunt et al., 2018). CA I RE (Liu et al., 2019) CUNI-U NSUPERVISED -NER- POST (Kvapilíková et al., 2019) follows the strategy of Artetxe et al. (2018), creating a seed phrase-based system where the phrase table is initialized from cross-lingual embedding mappings trained on monolingual data, followed by a neural machine translation system trained on synthetic parallel corpus. The synthetic corpus is produced by the seed phrase-based MT system or by a such a model refined through iterative back-translation. CUNI-U NSUPERVISED -NER- POST further focuses on the handling of named entities, i.e. the part of vocabulary where the cross-lingual embedding mapping suffer most. CA I RE is a hybrid system that took part only in the unsupervised track."
W19-5301,D18-1332,0,0.0215805,"et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔English translation task are that character-level model on the Chinese side can be used when translating into Chinese to improve the BLEU score. The same does not hold when transl"
W19-5301,W19-5351,0,0.0505622,"Missing"
W19-5301,W19-5423,0,0.0419015,"Missing"
W19-5301,W18-6412,1,0.856623,"Missing"
W19-5301,W19-5305,0,0.0496912,"Missing"
W19-5301,W12-3102,1,0.474924,"Missing"
W19-5301,W19-5310,0,0.0432396,"Missing"
W19-5301,W19-5425,0,0.0236032,"ys-Dowmunt et al., 2018) and Phrase-based machine translation system (implemented with Moses) and for the Spanish-Portuguese task. The system combination included features formerly presented in (Marie and Fujita, 2018), including scores left-to-right and right-to-left, sentence level translation probabilities and language model scores. Also authors provide contrastive results with an unsupervised phrase-based MT system which achieves quite close results to their primary system. Authors associate high performance of the unsupervised system to the language similarity. Incomslav: Team INCOMSLAV (Chen and Avgustinova, 2019) by Saarlad University participated in the Czech to Polish translation task only. The team’s primary submission builds on a transformer-based NMT baseline with back translation which has been submitted one of their contrastive submission. Incomslav’s primary system is a phoneme-based system re-scored using their NMT baseline. A second contrastive submission builds our phrase-based SMT system combined with a joint BPE model. NITS-CNLP: The NITS-CNLP team (Laskar et al., 2019) by the National Institute of Technology Silchar in India submitted results to the HI-NE translation task in both directi"
W19-5301,W07-0718,1,0.530103,"ojar Charles University Yvette Graham Barry Haddow Dublin City University University of Edinburgh Philipp Koehn JHU / University of Edinburgh Mathias Müller University of Zurich Marta R. Costa-jussà Christian Federmann UPC Microsoft Cloud + AI Shervin Malmasi Harvard Medical School Santanu Pal Saarland University Matt Post JHU Abstract Introduction The Fourth Conference on Machine Translation (WMT) held at ACL 20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to tra"
W19-5301,P16-2058,1,0.819865,"ipated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transformer (implemented with Fairseq (Ott et al., 2019)) for the Czechto-Polish task and a Phrase-based system (implemented with Moses (Koehn et al., 2007)) for Spanish-to-Portuguese. They tested adding monolingual data to the NMT system by copying the same data on the source and target sides, with negative results. Also, their system combination based on sentence-level BLEU in back-translation 5.4 Conclusion of Simi"
W19-5301,W08-0309,1,0.659809,"Missing"
W19-5301,W18-3931,1,0.820211,"or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language pairs: Spanish - Portuguese (Romance languages), Czech - Polis"
W19-5301,W19-5312,0,0.0773587,"Missing"
W19-5301,W19-5313,0,0.0931699,"University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of"
W19-5301,W19-5314,0,0.0200465,"Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and are therefore anonymized in a fashion consistent with previous years of the workshop. 2.5.6 BTRANS only the middle sentence was considered for the final translation hypothesis, otherwise shorter context of two sentences or just a single sentence was used. Unfortunately, no details are available for this system. 2.5.7 BASELINE - RE - RERANK (no associ"
W19-5301,D18-1045,0,0.0285693,"xt was morphologically segmented with Apertium. The UEDIN systems are supervised NMT systems based on the transformer architecture and trained using Marian (Junczys-Dowmunt et al., 2018). For English↔Gujarati, synthetic parallel data from two sources, backtranslation and pivoting through Hindi, is produced using unsupervised and semi-supervised NMT models, pre-trained using a cross-lingual language objective (Lample and Conneau, 2019) For German→English, the impact of vast amounts of back-translated training data on translation quality is studied, and some additional insights are gained over (Edunov et al., 2018). Towards the end of training, for German→English and Chinese↔English, the mini-batch size was increased up to fifty-fold by delaying gradient updates (Bogoychev et al., 2018) as an alternative to learning rate cooldown (Smith, 2018). For Chinese↔English, a comparison of different segmentation strategies showed that character-based decoding was superior to the translation of subwords when translating into Chinese. Pre-processing strategies were also investigated for English→Czech, showing that preprocessing can be simplified without loss to MT quality. UEDIN’s main findings on the Chinese↔Engl"
W19-5301,W18-6410,0,0.0193718,"ormance can be found in Hindi-Nepali (both directions), where the best performing system is around 50 BLEU (53 for Hindi-to-Nepali and 49.1 for Nepali-toHindi), and the lowest entry is 1.4 for Hindi-toNepali and 0 for Nepali-to-Hindi. The lowest variance is for Polish-to-Czech and it may be because only two teams participated. UHelsinki: The University of Helsinki team (Scherrer et al., 2019) participated with the Transformer (Vaswani et al., 2017) implemented in the OpenNMT toolkit. They focused on word segmentation methods and compared a cognate-aware segmentation method, Cognate Morfessor (Grönroos et al., 2018), with character segmentation and unsupervised segmentation methods. As primary submission they submitted this Cognate Morfessor that optimizes subword segmentations consistently for cognates. They participated for all translation directions in Spanish-Portuguese and Czech-Polish, and this Cognate Morfessor performed better for Czech-Polish, while characterbased segmentations (Costa-jussà and Fonollosa, 2016), while much more inefficient, were superior for Spanish-Portuguese. UPC-TALP: The UPC-TALP team (Biesialska et al., 2019) by the Universitat Politècnica de Catalunya submitted a Transform"
W19-5301,W19-5317,0,0.114089,"ed paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 201"
W19-5301,W19-5315,0,0.0266353,"Missing"
W19-5301,W19-5318,0,0.198338,"ance of the systems when translating from French to German seems to heavily depend on the 7 http://data.statmt.org/wmt19/ translation-task/dev.tgz 6 Systems MSRA.MADL eTranslation LIUM MLLP-UPV onlineA TartuNLP onlineB onlineY onlineG onlineX FULL 47.3 45.4 43.7 41.5 40.8 39.2 39.1 39.0 38.5 38.1 source FR 38.3 37.4 37.5 36.4 35.4 34.8 35.3 34.7 34.6 35.6 source DE 50.0 47.8 45.5 43.0 42.3 40.5 40.2 40.2 39.7 38.8 evaluations. In the rest of this sub-section, we provide brief details of the submitted systems, for those in cases where the authors provided such details. 2.5.1 AFRL - SYSCOMB 19 (Gwinnup et al., 2019) is a system combination of a Marian ensemble system, two distinct OpenNMT systems, a Sockeyebased Elastic Weight Consolidation system, and one Moses phrase-based system. Table 3: French→German Meteor scores. Systems MSRA.MADL LinguaCustodia MLLP_UPV Kyoto_University_T2T LIUM onlineY onlineB TartuNLP onlineA onlineX onlineG FULL 52.0 51.3 49.5 48.8 48.3 47.5 46.4 46.3 45.3 42.7 41.7 source FR 51.9 52.5 49.9 49.7 46.5 43.7 43.7 45.0 43.7 41.6 40.9 source DE 52.0 51.0 49.4 48.6 48.7 48.4 47.0 46.7 45.8 42.9 41.9 AFRL- EWC (Gwinnup et al., 2019) is a Sockeye Transformer system trained with the de"
W19-5301,W19-5316,0,0.109691,"boratory (Gwinnup et al., 2019) Apertium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of"
W19-5301,E14-1047,1,0.904335,"Missing"
W19-5301,W19-5427,0,0.0467733,"Missing"
W19-5301,W19-5322,1,0.807321,"Missing"
W19-5301,W19-5302,1,0.715203,"20191 hosts a number of shared tasks on various aspects of machine translation. This conference builds on 13 previous editions of WMT as workshops and conferences (Koehn and Monz, 2006; Callison-Burch et al., 2007, 2008, 2009, 2010, 2011, 2012; Bojar et al., 2013, 2014, 2015, 2016, 2017, 2018). This year we conducted several official tasks. We report in this paper on the news and similar translation tasks. Additional shared tasks are described in separate papers in these proceedings: • biomedical translation (Bawden et al., 2019b) • automatic post-editing (Chatterjee et al., 2019) • metrics (Ma et al., 2019) • quality estimation (Fonseca et al., 2019) • parallel corpus filtering (Koehn et al., 2019) • robustness (Li et al., 2019b) In the news translation task (Section 2), participants were asked to translate a shared test set, optionally restricting themselves to the provided training data (“constrained” condition). We 1 Christof Monz University of Amsterdam Marcos Zampieri University of Wolverhampton held 18 translation tasks this year, between English and each of Chinese, Czech (into Czech only), German, Finnish, Lithuanian, and Russian. New this year were Gujarati↔English and Kazakh↔English. B"
W19-5301,W19-5333,0,0.0923169,"Missing"
W19-5301,W19-5353,0,0.0658382,"Missing"
W19-5301,W19-5430,1,0.873847,"Missing"
W19-5301,W19-5431,0,0.0199622,"Universitat Politècnica de València (UPV) participated with a Transformer (implemented with FairSeq (Ott et al., 2019)) and a finetuning strategy for domain adaptaion in the task of Spanish-Portuguese. Fine-tunning on the development data provide improvements of almost 12 BLEU points, which may explain their clear best performance in the task for this language pair. As a contrastive system authors provided only for the Portuguese-to-Spanish a novel 2D alternating RNN model which did not respond so well when fine-tunning. UBC-NLP: Team UBC-NLP from the University of British Columbia in Canada (Przystupa and Abdul-Mageed, 2019) compared the performance of the LSTM plus attention (Bahdanau et al., 2015) and Transformer (Vaswani et al., 2017) (implemented in OpenNMT toolkit22 ) perform for the three tasks at hand. Authors use backtranslation to introduce monolingual data in their systems. LSTM plus attention outperformed Transformer for Hindi-Nepali, and viceversa for the other two tasks. As reported by the authors, Hindi-Nepali task provides much more shorter sentences than KYOTOUNIVERSITY: Kyoto University’s submission, listed simply as KYOTO in Table 25 for PT → ES task is based on transformer NMT system. They used"
W19-5301,P02-1040,0,0.11337,"ation of the source (CS), and a second encoder to encode sub-word (byte-pair-encoding) information of the source (CS). The results obtained by their system in translating from Czech→Polish and comment on the impact of out-of-domain test data in the performance of their system. UDSDFKI ranked second among ten teams in Czech– Polish translation. 5.3 Results We present results for the three language pairs, each of them in the two possible directions. For this first edition of the Similar Translation Task and differently from News task, evaluation was only performed on automatic basis using BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) measures. Each language direction is reported in one different table which contain information of the team; type of system, either contrastive (C) or primary (P), and the BLEU and TER results. In general, primary systems tend to be better than contrastive systems, as expected, but there are some exceptions. Even if we are presenting 3 pairs of languages each pair belonging to the same family, translation quality in terms of BLEU varies signficantly. While the best systems for Spanish-Portuguese are above 64 BLEU and below 21 TER (see Tables 26 and 27), best syste"
W19-5301,W19-5354,0,0.0611791,"Missing"
W19-5301,W18-6486,0,0.0189853,"the agglutinative nature of Kazakh, (ii) data from an additional language (Russian), given the scarcity of English–Kazakh data and (iii) synthetic data for the source language filtered using language-independent sentence similarity. RUG _ KKEN _ MORFESSOR Tilde developed both constrained and unconstrained NMT systems for English-Lithuanian and Lithuanian-English using the Marian toolkit. All systems feature ensembles of four to five transformer models that were trained using the quasi-hyperbolic Adam optimiser (Ma and Yarats, 2018). Data for the systems were prepared using TildeMT filtering (Pinnis, 2018) and preprocessing (Pinnis et al., 2018) methods. For unconstrained systems, data were additionally filtered using dual conditional cross-entropy filtering (Junczys-Dowmunt, 2018a). All systems were trained using iterative back-translation (Rikters, 2018) and feature synthetic data that allows training NMT systems to support handling of unknown phenomena (Pinnis et al., 2017). During translation, automatic named entity and nontranslatable phrase post-editing were performed. For constrained systems, named entities and nontranslatable phrase lists were extracted from the parallel training data."
W19-5301,W19-5335,0,0.0408704,"Missing"
W19-5301,W19-5344,1,0.904781,"n Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas e"
W19-5301,W19-5346,0,0.197908,"rtium (Pirinen, 2019) Apprentice (Li and Specia, 2019) Aylien Ltd. (Hokamp et al., 2019) Baidu (Sun et al., 2019) (no associated paper) (no associated paper) (Liu et al., 2019) Charles University (Popel et al., 2019; Kocmi and Bojar, 2019) and (Kvapilíková et al., 2019) Kumamoto University, Telkom University, Indonesian Institute of Sciences (Budiwati et al., 2019) DFKI (Zhang and van Genabith, 2019) eTranslation (Oravecz et al., 2019) Facebook AI Research (Ng et al., 2019) GTCOM (Bei et al., 2019) University of Helsinki (Talman et al., 2019) IIIT Hyderabad (Goyal and Sharma, 2019) IIT Patna (Sen et al., 2019) Johns Hopkins University (Marchisio et al., 2019) (no associated paper) University of Saarland (Mondal et al., 2019) Kingsoft AI (Guo et al., 2019) University of Kyoto (Cromieres and Kurohashi, 2019) Lingua Custodia (Burlot, 2019) LIUM (Bougares et al., 2019) LMU Munich (Stojanovski and Fraser, 2019; Stojanovski et al., 2019) MLLP, Technical University of Valencia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communicatio"
W19-5301,W19-5341,0,0.0172601,"A,B,G,X,Y. For presentation of the results, systems are treated as either constrained or unconstrained, depending on whether their models were trained only on the provided data. Since we do not know how they were built, the online systems are treated as unconstrained during the automatic and human AYLIEN _ MULTILINGUAL (Hokamp et al., 2019) The Aylien research team built a Multilingual NMT system which is trained on all WMT2019 language pairs in all directions, then fine-tuned for a small number of iterations on Gujarati-English data only, including some self-backtranslated data. 2.5.5 BAIDU (Sun et al., 2019) Baidu systems are based on the Transformer architecture with several improvements. Data selection, back translation, data augmentation, knowledge distillation, domain adaptation, model ensemble and re-ranking are employed and proven effective in our experiments. 7 Team AFRL A PERTIUM - FIN - ENG A PPRENTICE - C AYLIEN _ MULTILINGUAL BAIDU BTRANS BASELINE - RE - RERANK CA I RE CUNI DBMS-KU DFKI - NMT E T RANSLATION FACEBOOK FAIR GTCOM H ELSINKI NLP IIITH-MT IITP JHU JUMT JU_S AARLAND KSAI K YOTO U NIVERSITY L INGUA C USTODIA LIUM LMU-NMT MLLP-UPV MS T RANSLATOR MSRA N IU T RANS NICT NRC PARFDA"
W19-5301,P16-1162,1,0.310296,"ssible, 2.5.13 E T RANSLATION (Oravecz et al., 2019) E T RANSLATION En-De E T RANSLATION ’s EnDe system is an ensemble of 3 base Transformers and a Transformer-type language model, trained from all available parallel data (cleaned up and filtered with dual conditional cross-entropy filtering) and with additional back-translated data generated 9 2.5.17 from monolingual news. Each Transformer model is fine tuned on previous years’ test sets. H ELSINKI NLP is a Transformer (Vaswani et al., 2017) style model implemented in OpenNMTpy using a variety of corpus filtering techniques, truecasing, BPE (Sennrich et al., 2016), backtranslation, ensembling and fine-tuning for domain adaptation. E T RANSLATION Fr-De The Fr-De system is an ensemble of 2 big Transformers (with size 8192 FFN layers). Back-translation data was selected using topic modelling techniques to tune the model towards the domain defined in the task. 2.5.18 En-Lt The En-Lt system is an ensemble of 2 big Transformers (as for Fr-De) and a Transformer type language model. The training data contains the Rapid corpus and the news domain back-translated data sets 2 times oversampled. E T RANSLATION 2.5.19 FACEBOOK FAIR (Ng et al., 2019) 2.5.20 JHU (Mar"
W19-5301,W19-5339,0,0.0767002,"Missing"
W19-5301,W19-5347,0,0.0328257,"Missing"
W19-5301,W19-5342,1,0.887858,"encia (Iranzo-Sánchez et al., 2019) Microsoft Translator (Junczys-Dowmunt, 2019) Microsoft Research Asia (Xia et al., 2019) Northeastern University / NiuTrans Co., Ltd. (Li et al., 2019a) National Institute of Information and Communications Technology (Dabre et al., 2019; Marie et al., 2019b) National Research Council of Canada (Littell et al., 2019) Bo˘gaziçi University (Biçici, 2019) PROMT LLC (Molchanov, 2019) University of Groningen (Toral et al., 2019) RWTH Aachen (Rosendahl et al., 2019) TALP Research Center, Universitat Politècnica de Catalunya (Casas et al., 2019) University of Tartu (Tättar et al., 2019) Tilde (Pinnis et al., 2019) Universitat d’Alacant (Sánchez-Cartagena et al., 2019) University of Cambridge (Stahlberg et al., 2019) Saarland University, DFKI (España-Bonet and Ruiter, 2019) University of Edinburgh (Bawden et al., 2019a) University of Maryland (Briakou and Carpuat, 2019) (no associated paper) University of Sydney (Ding and Tao, 2019) (no associated paper) 8 participated in all language pairs. The translations from the Table 5: Participants in the shared translation task. Not all teams online systems were not submitted by their respective companies but were obtained by us, and"
W19-5301,W19-5355,1,0.869964,"Missing"
W19-5301,W19-5350,0,0.0441915,"Missing"
W19-5301,P98-2238,0,0.38957,"n trained to translate texts from and to English or they use English as a pivot language to translate between resource-poorer languages. The interest in English is reflected, for example, in the WMT translation tasks (e.g. News, Biomedical) which have always included language pairs in which texts are translated to and/or from English. With the widespread use of MT technology, there is more and more interest in training systems to translate between languages other than English. One evidence of this is the need of directly translating between pairs of similar languages, varieties, and dialects (Zhang, 1998; Marujo et al., 2011; Hassani, 2017; Costa-jussà et al., 2018). The main challenge is to take advantage of the similarity between languages to overcome the limitation given the low amount of available parallel data to produce an accurate output. Given the interest of the community in this topic we organize, for the first time at WMT, a shared task on ""Similar Language Translation"" to evaluate the performance of state-of-the-art translation systems on translating between pairs of languages from the same language family. We provide participants with training and testing data from three language"
W19-5302,W17-4755,1,0.837587,"al”) for some language pairs and reference-free (or “bilingual”) for others.3 Due to these different types of golden truth collection, reference-based language pairs are in a closer match with the standard referencebased metrics, while the reference-free language pairs are better fit for the “QE as a metric” subtask. Note that system-level manual scores are different than those of the segment-level. Since for segment-level evaluation, collecting enough DA judgements for each segment is infeasible, so we resort to converting DA judgements to 2.3.2 Segment-level Golden Truth: daRR Starting from Bojar et al. (2017), when WMT fully switched to DA, we had to come up with a solid golden standard for segment-level judgements. Standard DA scores are reliable only when averaged over sufficient number of judgments.4 Fortunately, when we have at least two DA scores for translations of the same source input, it is possible to convert those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as “daRR”, to distinguish it clearly from the relative ranking (“RR”) gol"
W19-5302,P17-1152,0,0.0220899,"metric, which builds upon CDER. It is defined as the minimum number of operations of an extension to the conventional edit distance containing a “jump” operation. The edit distance operations (insertions, deletions and substitutions) are performed at the character level and jumps are performed when a blank space is reached. Furthermore, the coverage of multiple characters in the hypothesis is penalised by the introduction of a coverage penalty. The sum of the length of the reference and the coverage penalty is used as the normalisation term. 4.5 ESIM Enhanced Sequential Inference Model (ESIM; Chen et al., 2017; Mathur et al., 2019) is a neural model proposed for Natural Language Inference that has been adapted for MT evaluation. It uses cross-sentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. The metric is trained on singly-annotated Direct Assessment data that has been collected for evaluating WMT systems: all WMT 2018 toEnglish data for the to-English language pairs, and all WMT 2018 data for all other language pairs. 4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy) Meteor++ 2.0 (Guo"
W19-5302,W19-5356,0,0.253083,", 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − http://github.com/kokeman/PReP − − http://github.com/poethan/LEPOR http://github.com/poethan/LEPOR http://github.com/nitikam/mteval-in-context http://github.com/rwth-i6/ExtendedEditDistance http://github.com/rwth-i6/CharacTER http://github.com/nitikam/mteval-in-context http://github.com/stanojevic/beer http://github.com/mjpost/sa"
W19-5302,W14-3348,0,0.0631558,"oposed for Natural Language Inference that has been adapted for MT evaluation. It uses cross-sentence attention and sentence matching heuristics to generate a representation of the translation and the reference, which is fed to a feedforward regressor. The metric is trained on singly-annotated Direct Assessment data that has been collected for evaluating WMT systems: all WMT 2018 toEnglish data for the to-English language pairs, and all WMT 2018 data for all other language pairs. 4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy) Meteor++ 2.0 (Guo and Hu, 2019) is a metric based on Meteor (Denkowski and Lavie, 2014) that takes syntactic-level paraphrase knowledge into consideration, where paraphrases may sometimes be skip-grams. i.e. (protect...from, protect...against). As the original Meteor-based metrics only pay attention to consecutive string matching, 68 they perform badly when reference-hypothesis pairs contain skip n-gram paraphrases. Meteor++ 2.0 extracts the knowledge from the Paraphrase Database (PPDB; Bannard and Callison-Burch, 2005) and integrates it into Meteor-based metrics. 4.8 aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted from BER"
W19-5302,N19-1423,0,0.0185514,"ntextual embeddings extracted from BERT to evaluate the crosslingual lexical semantic similarity between the input and MT output. Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well (denoted as YiSi-2_srl). PReP PReP (Yoshimura et al., 2019) is a method for filtering pseudo-references to achieve a good match with a gold reference. At the beginning, the source sentence is translated with some off-the-shelf MT systems to create a set of pseudo-references. (Here the MT systems were Google Translate and Microsoft Bing Translator.) The pseudoreferences are then filtered using BERT (Devlin et al., 2019) fine-tuned on the MPRC corpus (Dolan and Brockett, 2005), estimating the probability of the paraphrase between gold reference and pseudo-references. Thanks to the high quality of the underlying MT systems, a large portion of their outputs is indeed considered as a valid paraphrase. The final metric score is calculated simply with SentBLEU with these multiple references. 4.9 4.11 QE Systems In addition to the submitted standard metrics, 10 quality estimation systems were submitted to the “QE as a Metric” track. The submitted QE systems are evaluated in the same settings as metrics to facilitat"
W19-5302,P05-1074,0,0.0809195,", and all WMT 2018 data for all other language pairs. 4.7 Meteor++_2.0 (syntax), Meteor++_2.0 (syntax+copy) Meteor++ 2.0 (Guo and Hu, 2019) is a metric based on Meteor (Denkowski and Lavie, 2014) that takes syntactic-level paraphrase knowledge into consideration, where paraphrases may sometimes be skip-grams. i.e. (protect...from, protect...against). As the original Meteor-based metrics only pay attention to consecutive string matching, 68 they perform badly when reference-hypothesis pairs contain skip n-gram paraphrases. Meteor++ 2.0 extracts the knowledge from the Paraphrase Database (PPDB; Bannard and Callison-Burch, 2005) and integrates it into Meteor-based metrics. 4.8 aggregating the idf-weighted lexical semantic similarities based on the contextual embeddings extracted from BERT and optionally incorporating shallow semantic structures (denoted as YiSi-1_srl). YiSi-0 is the degenerate version of YiSi-1 that is ready-to-deploy to any language. It uses longest common character substring to measure the lexical similarity. YiSi-2 is the bilingual, reference-less version for MT quality estimation, which uses the contextual embeddings extracted from BERT to evaluate the crosslingual lexical semantic similarity bet"
W19-5302,I05-5002,0,0.0979701,"the crosslingual lexical semantic similarity between the input and MT output. Like YiSi-1, YiSi-2 can exploit shallow semantic structures as well (denoted as YiSi-2_srl). PReP PReP (Yoshimura et al., 2019) is a method for filtering pseudo-references to achieve a good match with a gold reference. At the beginning, the source sentence is translated with some off-the-shelf MT systems to create a set of pseudo-references. (Here the MT systems were Google Translate and Microsoft Bing Translator.) The pseudoreferences are then filtered using BERT (Devlin et al., 2019) fine-tuned on the MPRC corpus (Dolan and Brockett, 2005), estimating the probability of the paraphrase between gold reference and pseudo-references. Thanks to the high quality of the underlying MT systems, a large portion of their outputs is indeed considered as a valid paraphrase. The final metric score is calculated simply with SentBLEU with these multiple references. 4.9 4.11 QE Systems In addition to the submitted standard metrics, 10 quality estimation systems were submitted to the “QE as a Metric” track. The submitted QE systems are evaluated in the same settings as metrics to facilitate comparison. Their descriptions can be found in the Find"
W19-5302,C12-2044,0,0.187067,"Missing"
W19-5302,D14-1020,1,0.886024,"sment. 5.1.1 an adaptation of the conventional Kendall’s Tau coefficient. Since we do not have a total order ranking of all translations, it is not possible to apply conventional Kendall’s Tau (Graham et al., 2015). Our Kendall’s Tau-like formulation, τ , is as follows: System-Level Results Tables 3, 4 and 5 provide the system-level correlations of metrics evaluating translation of newstest2019. The underlying texts are part of the WMT19 News Translation test set (newstest2019) and the underlying MT systems are all MT systems participating in the WMT19 News Translation Task. As recommended by Graham and Baldwin (2014), we employ Williams significance test (Williams, 1959) to identify differences in correlation that are statistically significant. Williams test is a test of significance of a difference in dependent correlations and therefore suitable for evaluation of metrics. Correlations not significantly outperformed by any other metric for the given language pair are highlighted in bold in Tables 3, 4 and 5. Since pairwise comparisons of metrics may be also of interest, e.g. to learn which metrics significantly outperform the most widely employed metric BLEU, we include significance test results for ever"
W19-5302,2013.mtsummit-posters.3,0,0.0255629,"ystem-level and not at segmentlevel. In this submitted baseline version, hLEPOR_baseline was not tuned for each language pair separately but the default weights were applied across all submitted language pairs. Further improvements can be achieved by tuning the weights according to the development data, adding morphological information and applying n-gram factor scores into it (e.g. part-of-speech, n-gram precision and n-gram recall that were added into LEPOR in WMT13.). The basic model factors and further development with parameters setting were described in the paper (Han et al., 2012) and (Han et al., 2013). For sentence-level score, only hLEPORa_baseline was submitted with scores calculated as the weighted harmonic mean of all the designed factors using default parameters. For system-level score, both hLEPORa_baseline and hLEPORb_baseline were submitted, where hLEPORa_baseline is the the average score of all sentence-level scores, and hLEPORb_baseline is calculated via the same sentence-level hLEPOR equation but replacing each factor value with its system-level counterpart. EED EED (Stanchev et al., 2019) is a characterbased metric, which builds upon CDER. It is defined as the minimum number of"
W19-5302,W13-2305,1,0.868874,"and segment-level evaluation (Section 5.2). 2.3.1 System-level Golden Truth: DA Overall, the results are based on 233 systems across 18 language pairs.2 2.3 For the system-level evaluation, the collected continuous DA scores, standardized for each annotator, are averaged across all assessed segments for each MT system to produce a scalar rating for the system’s performance. The underlying set of assessed segments is different for each system. Thanks to the fact that the system-level DA score is an average over many judgments, mean scores are consistent and have been found to be reproducible (Graham et al., 2013). For more details see Findings 2019. Manual Quality Assessment Direct Assessment (DA, Graham et al., 2013, 2014a, 2016) was employed as the source of the “golden truth” to evaluate metrics again this year. The details of this method of human evaluation are provided in Findings 2019. The basis of DA is to collect a large number of quality assessments (a number on a scale of 1–100, i.e. effectively a continuous scale) for the outputs of all MT systems. These scores are then standardized per annotator. In the past years, the underlying manual scores were reference-based (human judges had access"
W19-5302,W04-3250,0,0.292959,"Missing"
W19-5302,E14-1047,1,0.934062,"Missing"
W19-5302,W06-3114,0,0.318343,"Missing"
W19-5302,2003.mtsummit-papers.32,0,0.308746,"Missing"
W19-5302,N16-1001,1,0.928052,"tandard DA scores are reliable only when averaged over sufficient number of judgments.4 Fortunately, when we have at least two DA scores for translations of the same source input, it is possible to convert those DA scores into a relative ranking judgement, if the difference in DA scores allows conclusion that one translation is better than the other. In the following, we denote these re-interpreted DA judgements as “daRR”, to distinguish it clearly from the relative ranking (“RR”) golden truth used in the past years.5 2 4 This year, we do not use the artificially constructed “hybrid systems” (Graham and Liu, 2016) because the confidence on the ranking of system-level metrics is sufficient even without hybrids. 3 Specifically, the reference-based language pairs were those where the anticipated translation quality was lower or where the manual judgements were obtained with the help of anonymous crowdsourcing. Most of these cases were translations into English (fien, gu-en, kk-en, lt-en, ru-en and zh-en) and then the language pairs not involving English (de-cs, de-fr and fr-de). The reference-less (bilingual) evaluations were those where mainly MT researchers themselves were involved in the annotations: e"
W19-5302,E06-1031,0,0.112399,"same source input sentence; “DA pairs” is the number of all possible pairs of translations of the same source input resulting from “DA>1”; and “daRR” is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin. BLEU and NIST The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using mteval-v13a.pl8 from the OpenMT Evaluation Campaign. The tool includes its own tokenization. We run mteval with the flag --international-tokenization.9 TER, WER, PER and CDER. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokenization. From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed to distinct translations of the same source were converted into daRR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evaluation of segment-level metrics. Conversion"
W19-5302,W19-5358,0,0.455314,"Missing"
W19-5302,W14-3333,1,0.926699,"Missing"
W19-5302,W18-6450,1,0.687061,"Missing"
W19-5302,N15-1124,1,0.828388,"ct translations of the same source input, s1 and s2 , is counted as a concordant (Conc) or disconcordant (Disc) pair is defined by the following matrix: s1 &lt; s2 s1 = s2 s1 > s2 s1 &lt; s2 Conc − Disc Metric s1 = s2 Disc − Disc s1 > s2 Disc − Conc In the notation of Macháček and Bojar (2014), this corresponds to the setup used in WMT12 (with a different underlying method of manual judgements, RR): Segment-Level Evaluation Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, again we were unable to follow the methodology outlined in Graham et al. (2015) for evaluation of segment-level metrics because the sampling of sentences did not provide sufficient number of assessments of the same segment. We therefore convert pairs of DA scores for competing translations to daRR better/worse preferences as described in Section 2.3.2. We measure the quality of metrics’ segmentlevel scores against the daRR golden truth using a Kendall’s Tau-like formulation, which is Human WMT12 &lt; = > &lt; 1 X -1 Metric = > -1 -1 X X -1 1 The key differences between the evaluation used in WMT14–WMT16 and evaluation used in WMT17–WMT19 were (1) the move from RR to daRR and ("
W19-5302,W14-3336,1,0.714999,"|Discordant| (2) Human where Concordant is the set of all human comparisons for which a given metric suggests the same order and Discordant is the set of all human comparisons for which a given metric disagrees. The formula is not specific with respect to ties, i.e. cases where the annotation says that the two outputs are equally good. The way in which ties (both in human and metric judgement) were incorporated in computing Kendall τ has changed across the years of WMT Metrics Tasks. Here we adopt the version used in WMT17 daRR evaluation. For a detailed discussion on other options, see also Macháček and Bojar (2014). Whether or not a given comparison of a pair of distinct translations of the same source input, s1 and s2 , is counted as a concordant (Conc) or disconcordant (Disc) pair is defined by the following matrix: s1 &lt; s2 s1 = s2 s1 > s2 s1 &lt; s2 Conc − Disc Metric s1 = s2 Disc − Disc s1 > s2 Disc − Conc In the notation of Macháček and Bojar (2014), this corresponds to the setup used in WMT12 (with a different underlying method of manual judgements, RR): Segment-Level Evaluation Segment-level evaluation relies on the manual judgements collected in the News Translation Task evaluation. This year, agai"
W19-5302,W19-5357,0,0.153853,"Missing"
W19-5302,W13-2202,1,0.696592,"xtracting daRR judgements from all possible pairs of translations of the same source input. We see that only German-French and esp. French-German can suffer from insufficient number of these simulated pairwise comparisons. The daRR judgements serve as the golden standard for segment-level evaluation in WMT19. 3 Baseline Metrics In addition to validating popular metrics, including baselines metrics serves as comparison and prevents “loss of knowledge” as mentioned by Bojar et al. (2016). Moses scorer6 is one of the MT evaluation tools that aggregated several useful metrics over the time. Since Macháček and Bojar (2013), we have been using Moses scorer to provide most of the baseline metrics and kept encouraging authors of well-performing MT metrics to include them in Moses scorer.7 The baselines we report are: newstest2019 Table 1: Number of judgements for DA converted to daRR data; “DA>1” is the number of source input sentences in the manual evaluation where at least two translations of that same source input segment received a DA judgement; “Ave” is the average number of translations with at least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possible pairs"
W19-5302,W16-2342,0,0.157687,"usch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi htt"
W19-5302,P19-1269,0,0.298995,"ch et al. (2006) Snover et al. (2006) Leusch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiS"
W19-5302,P02-1040,0,0.118027,"mber of judgements for DA converted to daRR data; “DA>1” is the number of source input sentences in the manual evaluation where at least two translations of that same source input segment received a DA judgement; “Ave” is the average number of translations with at least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possible pairs of translations of the same source input resulting from “DA>1”; and “daRR” is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin. BLEU and NIST The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using mteval-v13a.pl8 from the OpenMT Evaluation Campaign. The tool includes its own tokenization. We run mteval with the flag --international-tokenization.9 TER, WER, PER and CDER. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokenization. From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed t"
W19-5302,W19-5410,0,0.0820879,"dit distance, edit types edit distance, edit types edit distance, edit types character n-grams character n-grams n-grams n-grams Features yes yes ? ? ? ? yes yes Learned? Papineni et al. (2002) Doddington (2002) Leusch et al. (2006) Snover et al. (2006) Leusch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et"
W19-5302,W12-3116,0,0.224116,"similarity semantic similarity n-grams n-grams n-grams Levenshtein distance edit distance, edit types edit distance, edit types edit distance, edit types character n-grams character n-grams n-grams n-grams Features yes yes ? ? ? ? yes yes Learned? Papineni et al. (2002) Doddington (2002) Leusch et al. (2006) Snover et al. (2006) Leusch et al. (2003) Leusch et al. (2006) Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu,"
W19-5302,W19-5360,0,0.174611,"Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − http://github.com/kokeman/PReP − − http://github.com/poethan/LEPOR http://github.com/poethan/LEPOR http://github.com/nitikam/mteval-in-context http://github.com/rwth-i6/ExtendedEditDistance http://github.com/rwth-i6/CharacTER http://github.com/nitikam/mteval-in-context http://github.com/"
W19-5302,W15-3049,0,0.119189,"etrics smoothed version of BLEU for scoring at the segment-level. We used the standard tokenizer script as available in Moses toolkit for tokenization. Table 2 lists the participants of the WMT19 Shared Metrics Task, along with their metrics and links to the source code where available. We have collected 24 metrics from a total of 13 research groups, with 10 reference-less “metrics” submitted to the joint task “QE as a Metrich” with WMT19 Quality Estimation Task. The rest of this section provides a brief summary of all the metrics that participated. chrF and chrF+. The metrics chrF and chrF+ (Popović, 2015, 2017) are computed using their original Python implementation, see Table 2. We ran chrF++.py with the parameters -nw 0 -b 3 to obtain the chrF score and with -nw 1 -b 3 to obtain the chrF+ score. Note that chrF intentionally removes all spaces before matching the n-grams, detokenizing the segments but also concatenating words.10 4.1 BEER BEER (Stanojević and Sima’an, 2015) is a trained evaluation metric with a linear model that combines sub-word feature indicators (character n-grams) and global word order features (skip bigrams) to achieve a language agnostic and fast to compute evaluation m"
W19-5302,W17-4770,0,0.0695998,"Missing"
W19-5302,W18-6319,0,0.0768405,"ame source were converted into daRR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evaluation of segment-level metrics. Conversion of scores in this way produced a large set of daRR judgements for all language pairs, sentBLEU. The metric sentBLEU is computed using the script sentence-bleu, a part of the Moses toolkit. It is a 6 https://github.com/moses-smt/mosesdecoder/ blob/master/mert/evaluator.cpp 7 If you prefer standard BLEU, we recommend sacreBLEU (Post, 2018a), found at https://github.com/ mjpost/sacreBLEU. 8 http://www.itl.nist.gov/iad/mig/tools/ 9 International tokenization is found to perform slightly better (Macháček and Bojar, 2013). rely on judgements collected from known-reliable volunteers and crowd-sourced workers who passed DA’s quality control mechanism. Any inconsistency that could arise from reliance on DA judgements collected from low quality crowd-sourcing is thus prevented. 65 66 Baselines Metrics IBM1-morpheme IBM1-pos4gram LP LASIM UNI UNI+ USFD USFD-TL YiSi-2 YiSi-2_srl BEER BERTr characTER EED ESIM LEPORa LEPORb Meteor++_2.0 ("
W19-5302,2006.amta-papers.25,0,0.285519,"least one DA judgement available for the same source input sentence; “DA pairs” is the number of all possible pairs of translations of the same source input resulting from “DA>1”; and “daRR” is the number of DA pairs with an absolute difference in DA scores greater than the 25 percentage point margin. BLEU and NIST The metrics BLEU (Papineni et al., 2002) and NIST (Doddington, 2002) were computed using mteval-v13a.pl8 from the OpenMT Evaluation Campaign. The tool includes its own tokenization. We run mteval with the flag --international-tokenization.9 TER, WER, PER and CDER. The metrics TER (Snover et al., 2006), WER, PER and CDER (Leusch et al., 2006) were produced by the Moses scorer, which is used in Moses model optimization. We used the standard tokenizer script as available in Moses toolkit for tokenization. From the complete set of human assessments collected for the News Translation Task, all possible pairs of DA judgements attributed to distinct translations of the same source were converted into daRR better/worse judgements. Distinct translations of the same source input whose DA scores fell within 25 percentage points (which could have been deemed equal quality) were omitted from the evalua"
W19-5302,W19-5359,0,0.144887,"Popović (2015) Popović (2017) Post (2018a) Post (2018a) − • • • • • • ⊘ ⊘ • • ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ ⊘ • − − − − − − • • − − • • • • • • • • • • • • • • • • • • • • • • • • Dublin City University, ADAPT (Popovic, 2012) Dublin City University, ADAPT (Popovic, 2012) Univ. of Tartu (Yankovskaya et al., 2019) Univ. of Tartu (Yankovskaya et al., 2019) ? ? Univ. of Sheffield Univ. of Sheffield NRC (Lo, 2019) NRC (Lo, 2019) Univ. of Amsterdam, ILCC (Stanojević and Sima’an, 2015) Univ. of Melbourne (Mathur et al., 2019) RWTH Aachen Univ. (Wang et al., 2016a) RWTH Aachen Univ. (Stanchev et al., 2019) Univ. of Melbourne (Mathur et al., 2019) Dublin City University, ADAPT (Han et al., 2012, 2013) Dublin City University, ADAPT (Han et al., 2012, 2013) Peking University (Guo and Hu, 2019) Peking University (Guo and Hu, 2019) Tokyo Metropolitan Univ. (Yoshimura et al., 2019) Imperial College London (Chow et al., 2019a) NRC (Lo, 2019) NRC (Lo, 2019) NRC (Lo, 2019) Citation/Participant Scoring Level Seg Sys http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − − − − ? ? ? ? http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi http://github.com/chikiulo/YiSi − http://github"
W19-5302,W15-3050,0,0.0508599,"Missing"
