2000.iwpt-1.11,J99-2004,0,0.0974633,"Missing"
2000.iwpt-1.11,J95-4004,0,0.0465392,"Missing"
2000.iwpt-1.11,C96-1082,0,0.0400524,"Missing"
2000.iwpt-1.11,P90-1005,0,0.800333,"Missing"
2000.iwpt-1.11,W98-0509,1,0.873622,"Missing"
2020.lrec-1.292,W19-6128,1,0.846873,"Missing"
2020.lrec-1.292,D13-1143,0,0.0231757,"w. In recent years, with the advancements in eye-tracking technology, incorporating eye movements of a speaker or a listener starts to be seen as another beneficial tool in predicting / resolving which entity in a complex visual environment is referred to in the utterances (Mitev et al., 2018; Koleva et al., 2015). Uni-modal approaches like language models (low-level Ngram approaches or employing higher level methods like dependency structures) can be quite useful for extracting the intended meaning from text-only material (Asnani et al., 2015; Bickel et al., 2005; Mirowski and Vlachos, 2015; Gubbins and Vlachos, 2013). Semantic classification and clustering methods (e.g. word embeddings and ontologies) can also be used for this purpose. However, when it comes to the description of daily activities, the capability for multimodal integration by employing contextual information can be a very important feature in resolving references and / or performing commands, e.g. for a helper robot that aids people in their daily activities. Linguistic distributions alone could hardly provide enough clues to distinguish the action of bringing a pan from bringing a mug, which is a crucial difference for helper robots. Such"
2020.lrec-1.292,P15-2133,0,0.172332,"ge Processing (NLP) solutions for reference resolution, disambiguation or meaning recovery have already been well established in the literature (Coco and Keller, 2015; Snedeker and Trueswell, 2003; Ferreira et al., 2013; Spivey and Huette, 2013; Louwerse, 2008), also see Alac¸am et al. (2020) for a review. In recent years, with the advancements in eye-tracking technology, incorporating eye movements of a speaker or a listener starts to be seen as another beneficial tool in predicting / resolving which entity in a complex visual environment is referred to in the utterances (Mitev et al., 2018; Koleva et al., 2015). Uni-modal approaches like language models (low-level Ngram approaches or employing higher level methods like dependency structures) can be quite useful for extracting the intended meaning from text-only material (Asnani et al., 2015; Bickel et al., 2005; Mirowski and Vlachos, 2015; Gubbins and Vlachos, 2013). Semantic classification and clustering methods (e.g. word embeddings and ontologies) can also be used for this purpose. However, when it comes to the description of daily activities, the capability for multimodal integration by employing contextual information can be a very important fe"
2020.lrec-1.292,R09-1043,0,0.0939557,"Missing"
2020.lrec-1.292,P15-2084,0,0.0179155,"am et al. (2020) for a review. In recent years, with the advancements in eye-tracking technology, incorporating eye movements of a speaker or a listener starts to be seen as another beneficial tool in predicting / resolving which entity in a complex visual environment is referred to in the utterances (Mitev et al., 2018; Koleva et al., 2015). Uni-modal approaches like language models (low-level Ngram approaches or employing higher level methods like dependency structures) can be quite useful for extracting the intended meaning from text-only material (Asnani et al., 2015; Bickel et al., 2005; Mirowski and Vlachos, 2015; Gubbins and Vlachos, 2013). Semantic classification and clustering methods (e.g. word embeddings and ontologies) can also be used for this purpose. However, when it comes to the description of daily activities, the capability for multimodal integration by employing contextual information can be a very important feature in resolving references and / or performing commands, e.g. for a helper robot that aids people in their daily activities. Linguistic distributions alone could hardly provide enough clues to distinguish the action of bringing a pan from bringing a mug, which is a crucial differ"
2020.lrec-1.292,S12-1035,0,0.0173149,"s, all possible linguistic interpretations require different parsing results. In all cases, the object referred to in the repair/complement part shares either the property (e.g. blue) or the object class (e.g. mug) with the target object or location. In this study, one third of the sentences involve a negated statement. To our knowledge, there is no comparable dataset that involves negated statements presented in a systematically manipulated situated language setting. Identifying the scope and focus of negation is one of the challenging issues in the NLP community (e.g. *SEM 2012 shared task, Morante and Blanco (2012) ). The psycholinguistic literature agrees that sentences containing negation are harder to interpret than affirmative sentences (Orenes et al., 2014; Khemlani et al., 2012; Kaup et al., 2006; L¨udtke and Kaup, 2006; Carpenter and Just, 1975). On the other hand, it has been shown that when negation is supported by the right contextual support, the positive argument no longer needs to be represented, yielding faster verification compared to no-context situations (Tian et al., 2016; Dale and Duran, 2011; Nieuwland and Kuperberg, 2008). Although many more different visual arrangements are possibl"
2021.konvens-1.1,2020.coling-main.129,0,0.0149644,"oo dependent on their quality. To gain more insight into this dependence of de pendency parsing on embeddings, we have con ducted experiments with a neural dependency parser provided with deterministically uninform ative as well as random word embeddings and we report on the results. 2 Related Work To our knowledge, the mechanisms leading to neural parsers exhibiting better performance than conventional ones have not yet been investigated. It has been shown that recurrent neural networks are able to capture syntactic structures such as nest ing in practice as long as the depth is bounded (Bhattamishra et al., 2020), but this does not make a statement about whether or why they are better at it than conventional parsers, and it remains unclear what influence the input embeddings have on this capability. The question how a model output changes when trained and evaluated on different input embed dings, specifically word embeddings, has been ad dressed by Rios and Lwowski (2020). They train numerous word embeddings using Word2Vec, GloVe or fastText, each with various different initialization seeds and on different corpora, and compare the performance of models when using these different embeddings as input"
2021.konvens-1.1,2020.coling-main.450,0,0.0429126,"represent more taskspecific knowledge at the expense of general knowledge. A popular method for assessing what linguistic knowledge an embedding represents is probing tasks (a term that seems to have been coined by Conneau et al., 2018, based on Adi et al., 2017, and Shi et al., 2016), classifiers trained to reconstruct known explicit linguistic properties from embed dings. In one sense, dependency parsing can be seen as a probing task where the linguistic prop erty to be extracted is the dependency structure of a sentence, and has indeed been used as a probing task (Miaschi et al., 2020; Kunz and Kuhlmann, 2020). However, ‘viewing probing results in isol ation can lead to overestimating the linguistic cap abilities of a model’ (Mosbach et al., 2020, p. 780), and Kunz and Kuhlmann (2020) point out that in such scenarios, it is generally unknown to what ex tent the output is indeed present in and extracted from the embedding, as opposed to being learned by the model (‘probe’) built on top of it. They con sider embeddings to most likely lie between two extremes: no useful information being represen ted at all, or the information already being rep resented in a humanreadable way. Apart from re st"
2021.konvens-1.1,W16-2512,0,0.0203028,"morphosyntactic information encoded in a word embedding can be influential. Note though that neural dependency parsing based on word embeddings is different from probing sentence embeddings for dependen cies of the encoded sentence. One could say that the situation is the converse: In the probing scen ario, the embedding is the result of a procedure and is probed to investigate its dependence on the ori ginal input. In our case, the embeddings are the input, and we want to investigate the dependence of the procedure on it. There are similar findings to the above for word embeddings, due to Köhn (2016), attesting the choice of embeddings a no ticeable impact on parser performance. 3 Experimental Setup As we cannot directly inspect what the neural ar chitecture learns and whether it is indeed better than ‘conventional’ (nonneural) architectures at learning the syntactic knowledge needed for pars ing, we employ a proxy question instead and ask how the output of a neural parser changes when depriving it of the knowledge encoded in the in put word embeddings, as these embeddings are an additional input that most conventional parsers do not have at their disposal. If the neural parser per"
2021.konvens-1.1,P14-1130,0,0.042575,"Missing"
2021.konvens-1.1,foth-etal-2014-size,1,0.85016,"Missing"
2021.konvens-1.1,W03-3017,0,0.223483,"e network based on these inputs (and the gold dependency structure and labels), but it does not alter the embeddings provided nor save any other lexical information about words in the train ing data; in particular, there is no attempt to ob tain semantic knowledge about words not covered by the embedding.2 This implies a substantial de pendence on those embeddings. As a conventional baseline we employ the five nonneural parsers from Adelmann et al. (2018a), excluding JWCDG, but only report the perform ance of the best parser per test text as reference. In all cases this was either Malt3 (Nivre, 2003) with the ‘Covington nonprojective’ algorithm (Cov ington, 2001) or Mate4 (Bohnet, 2010). 3.2 Uninformative Embeddings As Sticker cannot be run without word embeddings as input, we cannot entirely turn off this input, but we can substitute artificially created pseudo (or ‘dummy’) embeddings that are ‘uninformative’ in the sense that they do not encode any properties of the words beyond the word form identity (in par ticular, no semantics at all). We experiment with such uninformative embeddings created in differ ent ways, two of them deterministic and four ran dom (sampled with respect to"
2021.konvens-1.1,2020.coling-main.299,0,0.0926941,"g better performance than conventional ones have not yet been investigated. It has been shown that recurrent neural networks are able to capture syntactic structures such as nest ing in practice as long as the depth is bounded (Bhattamishra et al., 2020), but this does not make a statement about whether or why they are better at it than conventional parsers, and it remains unclear what influence the input embeddings have on this capability. The question how a model output changes when trained and evaluated on different input embed dings, specifically word embeddings, has been ad dressed by Rios and Lwowski (2020). They train numerous word embeddings using Word2Vec, GloVe or fastText, each with various different initialization seeds and on different corpora, and compare the performance of models when using these different embeddings as input. We take a similar approach, except that we use ‘artificial’ embeddings, and while their focus is on the con sequences of embedding differences due to al gorithm and initialization, we are interested in the impact of the (distributional) semantics available through the embedding in the first place. For a short period in time there were even some neural parsing ar"
C00-2151,W98-0509,1,0.887256,"Missing"
C00-2151,C90-3051,0,0.0915511,"Missing"
C00-2151,1993.mtsummit-1.11,0,\N,Missing
C00-2151,P98-1086,1,\N,Missing
C00-2151,C98-1083,1,\N,Missing
C00-2151,P92-1041,0,\N,Missing
C88-2085,E87-1008,1,\N,Missing
C90-3092,C88-2085,1,\N,Missing
C90-3092,C88-2127,0,\N,Missing
C98-1083,C90-3030,0,0.0338124,"ained in the classical sense because at least preferential constraints are violated by the solution. We will give a more detailed introduction to constraint parsing in Section 2 and to the extension to graded constraints in Section 3. Section 4 presents algorithms for the solution of the previously defined parsing problem and the linguistic modeling for constraint parsing is finally described in Section 5. 2 Parsing as C o n s t r a i n t Satisfaction While eliminative approaches are quite customary for part-of-speech disambiguation (Padrb, 1996) and underspecified structural representations (Karlsson, 1990), it has hardly been used as a basis for full structural interpretation. M a r u y a m a (1990) describes full parsing by means of constraint satisfaction for the first time. Q, llil (a) The snake is chased by the cat. 1 (b) 2 3 v, va v~ -= == <nd,2> (nil, O> (pp, 4> v7 = (pc, 4 v2 v4 v6 5 = = = 6 7 <subj, 3> (ac, 3> (nd, 7) 5> Figure 1: (a) Syntactic dependency tree for an example utterance: For each word form an unambiguous subordination and a label, which characterizes of subordination, are to be found. (b) Labellings for a set of constraint variables: Each variable corresponds to a word fo"
C98-1083,P90-1005,0,0.267419,"Missing"
C98-1083,W98-0509,1,0.885292,"Missing"
C98-1083,1993.mtsummit-1.11,0,0.0184732,", it certainly makes an adequate modelling of feature percolation phenomena rather difficult. Again, the use of auxiliary levels provides a solution by allowing to transport the required information along the edges of the dependency tree by means of appropriately defined labels. For efficiency reasons (the complexity is exponential in the number of features to percolate over the same edge) the application of this technique should be restricted to a few carefully selected phenomena. The approach presented in this paper has been tested successfully on some 500 sentences of the Verbmobil domain (Wahlster, 1993). Currently, there are about 210 semantic constraints, including constraints on auxiliary levels. The syntax is defined by 240 constraints. Experiments with slightly distorted sentences resulted in correct structural trees in most cases. 6 Conclusion An approach to the parsing of dependency structures has been presented, which is based on the elimination of partial structural interpretations by means of constraint satisfaction techniques. Due to the graded nature of constraints (possibly conflicting) evidence from a wide variety of informational sources can be integrated into a uniform computa"
daum-etal-2004-automatic,bouma-kloosterman-2002-querying,0,\N,Missing
daum-etal-2004-automatic,J93-2004,0,\N,Missing
daum-etal-2004-automatic,E03-1052,1,\N,Missing
daum-etal-2004-automatic,W00-1307,0,\N,Missing
daum-etal-2004-automatic,H01-1014,0,\N,Missing
daum-etal-2004-automatic,J03-4003,0,\N,Missing
daum-etal-2004-automatic,E03-1014,0,\N,Missing
daum-etal-2004-automatic,P99-1065,0,\N,Missing
E03-1052,E99-1016,0,0.0712157,"Missing"
E03-1052,A00-1031,0,0.0958081,"Missing"
E03-1052,P99-1065,0,0.144581,"Missing"
E03-1052,2000.iwpt-1.11,1,0.88087,"weights turns the CSP into a constraint optimization problem, which is considerably harder to solve, since one particular 100 solution must be found rather than just one of a number of solutions. Furthermore, weighted constraints do not allow us to prune away an alternative, leading to very large search spaces. In fact, analyzing long sentences with WCDG usually leads to problems that are too large to be solved exactly. However, heuristic methods have been developed that can approximate the optimal solution, where the quality of the solution increases over time as more alternatives are tried (Foth et al., 2000). 3 Experimental setup For our experiments we analysed 1845 unedited sentences from German online newscasts on buying or selling events, with an average length of 24 word forms (see Table 1 for a distribution of sentence lengths). We employed a handwritten WCDG of German that aims to perform a very thorough analysis; in addition to establishing syntax structure, it assigns each dependency one of 28 labels. It must also determine the exact word class and morphological form of each word. At the same time the lexicon was kept incomplete; of the open word classes of German, only the verbs are mode"
E03-1052,P90-1005,0,0.155052,"he finite verb forming the root of the tree. To express different types of dependency relations, the vertices are often enriched with different labels, e.g. to distinguish subjects from objects. Instead of giving generative rules about how to construct a dependency tree, a Constraint dependency grammar declares constraints, i.e. properties that a well-formed tree should fulfill. Finding a well-formed syntactic structure takes on the form of a constraint satisfaction problem (CSP), which can be solved by a variety of solution methods. This kind of dependency grammar was introduced by Maruyama (Maruyama, 1990). The constraints about well-formedness are notated as logical formulas. A common rule is to postulate that subjects typically precede their finite verb and objects follow it. This can be expressed by requiring that for all edges with the label `SUBF , the governor must occur to the left of the modifier, and the inverse for edges labelled `OBJA'. Applying these constraints to an example analysis (Figure 1), we see that two of the dependencies actually violate them. Since this is not incorrect in German but merely marks a topicalization, it is useful to gradate the constraints. This is the defi"
E03-1052,P02-1035,0,0.0765188,"Missing"
E03-1052,C00-2105,0,\N,Missing
E03-1052,J03-4003,0,\N,Missing
E03-1052,P02-1056,0,\N,Missing
E87-1008,P84-1020,0,\N,Missing
foth-etal-2014-size,telljohann-etal-2004-tuba,0,\N,Missing
foth-etal-2014-size,C10-1011,0,\N,Missing
foth-etal-2014-size,E06-1011,0,\N,Missing
foth-etal-2014-size,W08-2121,0,\N,Missing
foth-etal-2014-size,W06-2920,0,\N,Missing
foth-etal-2014-size,H94-1020,0,\N,Missing
foth-etal-2014-size,P04-3008,1,\N,Missing
foth-etal-2014-size,H05-1066,0,\N,Missing
foth-etal-2014-size,P10-1110,0,\N,Missing
foth-etal-2014-size,W09-1201,0,\N,Missing
foth-etal-2014-size,P09-1039,0,\N,Missing
foth-etal-2014-size,P07-1077,0,\N,Missing
foth-etal-2014-size,P10-1151,0,\N,Missing
foth-etal-2014-size,D07-1096,0,\N,Missing
foth-etal-2014-size,D07-1101,0,\N,Missing
foth-etal-2014-size,W03-3017,0,\N,Missing
foth-etal-2014-size,P13-2109,0,\N,Missing
L18-1567,D16-1156,0,0.0267418,"he corresponding sentence and linked via semantic relations to the syntactic structure by applying constraints. But, that system is solely constraint-based and does no longer produce state-of-the-art results. Furthermore, it requires a complete grammar of the respective language it is applied to. Thus, that approach is language specific. In contrast, our system is based on a data-driven parser, which achieves state-of-the-art results and whose model is trained on annotated data that is provided by treebanks for various languages and which is, thus, language-independent. In a more recent work, Christie et al. (2016) proposed to jointly segment an input image semantically and resolve PP attachment ambiguities in its caption. Their approach generates several hypotheses for both segmentation and disambiguation and a model scores pairs of hypotheses for both tasks in order to find the most plausible hypotheses for both the visual and linguistic task. Compared to our work, they tried to improve not only the disambiguation but also the processing of the visual input. Furthermore, they do not assume the visual input to be perfect. Instead, we are interested in ambiguous attachments of relative clauses and, whil"
L18-1567,foth-etal-2014-size,1,0.838452,"Missing"
L18-1567,W07-2416,0,0.0186275,"lause and its relative pronoun agrees with both in number and gender. Therefore, a disambiguation is not possible without further evidence. For the experiments, full RBG models, which use all available RBG features, i. e. global as well as up to third-order local features, have been trained for English, German and Turkish, respectively. For English, sections 0 − 22 and 24 of the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1994) constitute the training set (≈ 46k sentences, duplicates excluded). They have been converted to dependency structure using the LTH converter (Johansson and Nugues, 2007). For German, the first ≈ 98k sen3590 xsemanticAnnotations xlinguistic RBG scoring function: tcurrent hill climbing sRBG&jwcdg tcurrent norm(sRBG) inverse(sjwcdg) sjwcdg jwcdg evaluation linking grammar tbest Figure 4: RBG applies hill climbing for the linguistic input xlinguistic , its scoring function is repeatedly called to evaluate entire trees tcurrent or their edges. jwcdg is called to evaluate whether the semantic roles xsemanticAnnotations from the context can be linked to tcurrent via the linking grammar. The normalized RBG score norm(sRBG ) is penalized by the inverse jwcdg score sjw"
L18-1567,P14-1130,0,0.0144275,"., 2014). Hill climbing is applied if all features are exploited to approximate the most plausible dependency tree. First, a random tree is uniformly sampled. Next, the heads of all dependents are exchanged so the edges of the tree are changed until a local optimum is reached. To increase the likelihood of finding the global optimum, hill climbing repeatedly restarts, always with random trees that are sampled independently of previous solutions, until the best solution converges. During hill climbing, edges as well as entire trees are scored by RBG’s scoring function, see (Zhang et al., 2014; Lei et al., 2014) for details. 3.2. Inclusion of Dynamic Contexts While RBG fulfills the requirements of languageindependence and state-of-the-art results, it is not able to deal with dynamic contexts, i. e. requirement R2). In theory, features could be extracted from the contexts, for example features capturing the relations the semantic roles, which the visual scenes are annotated with in our data-set, express. Either an RBG model might be trained on those features combined with the ones extracted from the input sentences or separate models might be trained. In both cases, the disambiguation results for the"
L18-1567,H94-1020,0,0.597455,"hing a relative clause to an NP or its genitive object is also ambiguous in Turkish. On the syntactic level, there are two possible antecedents for each relative clause and its relative pronoun agrees with both in number and gender. Therefore, a disambiguation is not possible without further evidence. For the experiments, full RBG models, which use all available RBG features, i. e. global as well as up to third-order local features, have been trained for English, German and Turkish, respectively. For English, sections 0 − 22 and 24 of the Wall Street Journal (WSJ) corpus of the Penn Treebank (Marcus et al., 1994) constitute the training set (≈ 46k sentences, duplicates excluded). They have been converted to dependency structure using the LTH converter (Johansson and Nugues, 2007). For German, the first ≈ 98k sen3590 xsemanticAnnotations xlinguistic RBG scoring function: tcurrent hill climbing sRBG&jwcdg tcurrent norm(sRBG) inverse(sjwcdg) sjwcdg jwcdg evaluation linking grammar tbest Figure 4: RBG applies hill climbing for the linguistic input xlinguistic , its scoring function is repeatedly called to evaluate entire trees tcurrent or their edges. jwcdg is called to evaluate whether the semantic roles"
L18-1567,P13-2109,0,0.0327145,"tbest . tences (duplicates excluded) of the HDT part A are used for training. For Turkish, the training data are the first 5k sentences of the METU-Sabanci Turkish Dependency Treebank (Oflazer et al., 2003). If a word from that corpus is separated into different inflectional groups, they will be concatenated as described in (Nivre et al., 2007) so that parsing is performed on word basis. All three corpora provide word forms, gold PoS tags and gold standard annotations. While the gold PoS tags have been used for Turkish, all German and English sentences have been PoS tagged by the TurboTagger (Martins et al., 2013). Ten-way jackknifing has been performed for the training sets, i. e. each is split into ten partitions and each partition is tagged by a model trained on the other nine partitions. The test sets have been PoS tagged by models trained on the entire training set of the respective corpus. Table 2 lists all experimental results. The accuracies of the relative clause attachments predicted by the original RBG are compared to the multi-modal disambiguations of our system. In general, RBG always favors one attachment, mostly the low one, irrespective of the supposed attachment because it does not hav"
L18-1567,R09-1043,0,0.0317167,"real-world setting. Their study provided further evidence that not only linguistic but also visual information influences the interpretation of a sentence. While the aforementioned empirical studies were psycho-linguistically motivated and provided insights how humans resolve linguistic ambiguities by exploiting visual cues, our work provides evidence that similar effects can be observed for automatically parsing syntactically ambiguous sentences in the presence of contextual information. An early approach to exploit external knowledge during parsing was proposed by (McCrae and Menzel, 2007; McCrae, 2009; Baumg¨artner et al., 2012). They suggested to utilize high-level representations of visual information from related scenes to resolve linguistic ambiguities in German, e.g. Genitive-Dative ambiguity of feminine nouns or PP attachment, and developed a syntactic parsing architecture for the integration of cross-modal information. Moreover, McCrae (2009) hypothesized multiple requirements for such a system. Like ours, their system uses visual scenes as context and the author did not discuss the auto3592 matic derivation of information from those scenes. Also, the instances of the context are ma"
L18-1567,P14-1019,0,0.0972283,"paints paints (b) corresponding to 2b Figure 3: Some semantic role annotations for the scenes in Figure 2. R3) The parsing results for the overall sentences, besides the investigated ambiguities, are supposed to be stateof-the-art results. Subsection 3.1. addresses requirements R1) and R3) by basing the system on a state-of-the-art, data-driven parser. To deal with the dynamic nature of the context, requirement R2), a grammar-based approach is taken, which is being outlined in Subsection 3.2.. 3.1. Language-Independent Parsing Our system is based on the data-driven, syntactic RBGParser (RBG) (Zhang et al., 2014), which performs graphbased dependency parsing. It possesses a scoring function to evaluate entire dependency trees respectively their edges and learns that function based on training data that are annotated dependency trees from treebanks, which exist for various languages. Furthermore, RBG does not require language specific knowledge like hand-written grammars. Therefore, it fulfills the requirement of languageindependence, i. e. R1). Also, requirement R3) is met because RBG achieves state-of-the-art results for several languages (Zhang et al., 2014). RBG extracts up to third-order local fea"
P04-3008,brants-plaehn-2000-interactive,0,0.0882873,"te dependency structures according to a variant of dependency grammar in a graphical environment. We have found such an integrated environment invaluable for the development of precise and large grammars of natural language. Compared to other approaches, c.f. (Kaplan and Maxwell, 1996), the built-in WCDG parser provides a much better feedback by pinpointing possible reasons for the current grammar being unable to produce the desired parsing result. This additional information can then be immediately used in subsequent development cycles. A similar tool, called Annotate, has been described in (Brants and Plaehn, 2000). This tool facilitates syntactic corpus annotation in a semiautomatic way by using a part-of-speech tagger and a parser running in the background. In comparison, Annotate is primarily used for corpus annotation, whereas XCDG supports the development of the parser itself also. Due to its ability to always compute the single best analysis of a sentence and to highlight possible shortcomings of the grammar, the XCDG system provides a useful framework in which human design decisions on rules and weights can be effectively combined with a corpus-driven evaluation of their consequences. An alternat"
P04-3008,P02-1043,0,0.0597554,"Missing"
P06-1037,daum-etal-2004-automatic,1,0.810419,"Missing"
P06-1037,P06-1041,1,0.884336,"Missing"
P06-1037,H05-1066,0,0.050721,"Missing"
P06-1037,J99-2004,0,0.544962,"with the supertag predictions are simply discarded. Drawing on the ability of Weighted Constraint Dependency Grammar (WCDG) (Schr¨oder et al., 2000) to deal with defeasible constraints, here we try another option for making available supertag information: Using a score to estimate the general reliability of unique supertag decisions, the information can be combined with evidence derived from other constraints of the grammar in a soft manner. It makes possible to rank parsing hypotheses according to their plausibility and allows the parser to even override potentially wrong supertag decisions. Bangalore and Joshi (1999) derived the notion of supertag within the framework of Lexicalized Tree-Adjoining Grammars (LTAG) (Schabes and Joshi, 1991). They considered supertagging a process of almost parsing, since all that needs to be done after having a sufficiently reliable sequence of supertags available is to decide on their combination into a spanning tree for the complete sentence. Thus the approach lends itself easily to preprocessing sentences or filtering parsing results with the goal of guiding the parser or reducing its output ambiguity. Starting from a range of possible supertag models, Section 2 explores"
P06-1037,W00-1605,0,0.0750783,"Missing"
P06-1037,A00-1031,0,0.0474557,"Missing"
P06-1037,W02-2236,0,0.0605223,"Missing"
P06-1037,W02-1031,0,0.386522,"of the parser is influenced by changing its preferences, without excluding alternative structural interpretations from being considered. The paper reports on a series of experiments using varying models of supertags that significantly increase the parsing accuracy. In addition, an upper bound on the accuracy that can be achieved with perfect supertags is estimated. A grammar formalism which seems particularly well suited to decompose structural descriptions into lexicalized tree fragments is dependency grammar. It allows us to define supertags on different levels of granularity (White, 2000; Wang and Harper, 2002), thus facilitating a fine grained analysis of how the different aspects of supertag information influence the parsing behaviour. In the following we will use this characteristic to study in more detail the utility of different kinds of supertag information for guiding the parsing process. 1 Introduction Supertagging is based on the combination of two powerful and influential ideas of natural language processing: On the one hand, parsing is (at least partially) reduced to a decision on the optimal sequence of categories, a problem for which efficient and easily trainable procedures exist. On t"
P06-1037,W04-0307,0,0.261415,"Missing"
P06-1037,C04-1041,0,0.263901,"Missing"
P06-1037,P04-1014,0,0.096938,"Missing"
P06-1037,E03-1052,1,0.863762,"Missing"
P06-1037,C94-1024,0,\N,Missing
P06-1037,C04-1056,0,\N,Missing
P06-1037,P05-1039,0,\N,Missing
P06-1037,P03-1013,0,\N,Missing
P06-1041,J99-2004,0,0.175766,"Missing"
P06-1041,A00-1031,0,0.411377,"Missing"
P06-1041,A00-2018,0,0.107978,"Missing"
P06-1041,C04-1041,0,0.0861028,"Missing"
P06-1041,daum-etal-2004-automatic,1,0.744453,"he structural (i.e. unlabelled) accuracy as the ratio of correctly attached words to all words; the labelled accuracy counts only those words that have the correct regent and also bear the correct label. For comparison with previous work, we used the next-to-last 1,000 sentences of the NEGRA corpus as our test set. Table 1 shows the accuracy obtained.1 The gold standard used for evaluation was derived from the annotations of the NEGRA treebank (version 2.0) in a semi-automatic procedure. First, the NEGRA phrase structures were automatically transformed to dependency trees with the DEPSY tool (Daum et al., 2004). However, before the parsing experiments, the results were manually corrected to (1) take care of systematic inconsistencies between the NEGRA annotations and the WCDG annotations (e.g. for nonprojectivities, which in our case are used only if necessary for an ambiguity free attachment of verbal arguments, relative clauses and coordinations, but not for other types of adjuncts) and (2) to remove inconsistencies with NEGRAs own annotation guidelines (e.g. with regard to elliptical and co-ordinated structures, adverbs and subordinated main clauses.) To illustrate the consequences of these corre"
P06-1041,P05-1039,0,0.094981,"available corpus data at all. To gain a benefit from such contextual cues, e.g. in a dialogue system, requires to integrate yet another kind of external information. Unfortunately, stochastic predictor components are usually not perfect, at best producing preferences and guiding hints instead of reliable certainties. Integrating a number of them into a single systems poses the problem of error propagation. Whenever one component decides on the input of another, the subsequent one will most probably fail whenever the decision was wrong; if not, the erroneous information was not crucial anyhow. Dubey (2005) reported how serious this problem can be when he coupled a tagger with a subsequent parser, and noted that tagging errors are by far the most important source of parsing errors. As soon as more than two components are involved, the combination of different error sources migth easily lead to a substantial decrease of the overall quality instead of achieving the desired synergy. Moreover, the likelihood of conflicting contributions will rise tremendously the more predictor components are involved. Therefore, it is far from obvious that additional information always helps. Certainly, a processin"
P06-1041,P90-1005,0,0.30448,"ive importance of two rules is clearly reflected in their weights (for instance, a misinflected determiner is a language error, but probably a less severe one than duplicate determiners). There have been attempts to compute the weights of a WCDG automatically by observing which weight vectors perform best on a given corpus (Schr¨oder et al., 2001), but weights computed completely automatically failed to improve on the original, handscored grammar. 3 WCDG An architecture which fulfills this requirement is Weighted Constraint Dependency Grammar, which was based on a model originally proposed by Maruyama (1990) and later extended with weights (Schr¨oder, 2002). A WCDG models natural language as labelled dependency trees on words, with no intermediate constituents assumed. It is entirely declarative: it only contains rules Weighted constraints provide an ideal interface to integrate arbitrary predictor components in a soft manner. Thus, external predictions are treated 322 the same way as grammar-internal preferences, e.g. on word order or distance. In contrast to a filtering approach such a strong integration does not blindly rely on the available predictions but is able to question them as long as"
P06-1041,H05-1066,0,0.220698,"Missing"
P06-1041,P06-2029,1,0.860228,"Missing"
P06-1041,2000.iwpt-1.11,1,0.863243,"Missing"
P06-1041,C02-1004,0,0.0763512,"Missing"
P06-1041,W04-0307,0,0.243821,"Missing"
P06-1041,J03-4003,0,\N,Missing
P06-1041,P06-1037,1,\N,Missing
P06-1041,W03-3017,0,\N,Missing
P06-2029,H05-1066,0,0.0870842,"Missing"
P06-2029,H94-1048,0,0.528124,"(Foth et al., 2000). Table 3 illustrates the structural accuracy2 of the unmodified system for various subordination types. For instance, of the 1892 dependency edges with the label ‘PP’ in the gold standard, 1285 are attached correctly by the parser, while 607 receive an incorrect regent. We see that PP attachment decisions are particularly prone to errors 5 Related Work (Hindle and Rooth, 1991) first proposed solving the prepositional attachment task with the help of statistical information, and also defined the prevalent formulation as a binary decision problem with three words involved. (Ratnaparkhi et al., 1994) 2 Note that the WCDG parser always succeeds in assigning exactly one regent to each word, so that there is no difference between precision and recall. We refer to structural accuracy as the ratio of words which have been attached correctly to all words. 228 extended the problem instances to quadruples by also considering the kernel noun of the PP, and used maximum entropy models to estimate the preferences. Both supervised and unsupervised training procedures for PP attachment have been investigated and compared in a number of studies, with supervised methods usually being slightly superior ("
P06-2029,C94-2195,0,0.194077,"Missing"
P06-2029,W95-0103,0,0.177225,"Missing"
P06-2029,P05-1039,0,0.0791872,"Missing"
P06-2029,P06-1041,1,0.817783,"die Firmen müssen noch die Bedenken der EU-Kommission gegen die Fusion ausräumen the companies have to yet the concerns the European commission about the merger address . Figure 1: Correct syntax analysis of the example sentence. overall verb attachment is more common than noun attachment in German. Therefore, the verb attachment leads to the globally best solution for this sentence. and do actually lead to mis-inflected phrases. In this way robustness against many types of error can be achieved while still preferring the correct variant. For more about the WCDG parser, see (Schr¨oder, 2002; Foth and Menzel, 2006) . The grammar of German available for this parser relies heavily on weighted constraints both to cope with many kinds of imperfect input and to resolve true ambiguities. For the example sentence, it retrieves the desired dependencies except for constructing the implausible dependency ‘ausr¨aumen’+‘gegen’ (address against). Let us briefly review the relevant constraints that cause this error: There are no lexicalized rules that capture the particular plausibility of the phrase ‘Bedenken gegen’ (concerns about). A constraint that describes this individual word pair would be trivial to write, bu"
P06-2029,2000.iwpt-1.11,1,0.788278,"Missing"
P06-2029,P91-1030,0,0.357285,"a comparatively difficult subtask for rule-based syntax analysis and has often been attacked by statistical methods. Because probabilistic approaches solve PP attachment as a natural subtask of parsing anyhow, the obvious application of a PP attacher is to integrate it into a rule-based system. Perhaps surprisingly, so far this has rarely been done. One reason for this is that many rule-driven syntax analyzers provide no obvious way to integrate uncertain, statistical information into their decisions. Another is the traditional emphasis on PP attachment as a binary classification task; since (Hindle and Rooth, 1991), research has concentrated on resolving the ambiguity in the category pattern ‘V+N+P+N’, i.e. predicting the PP attachment to either the verb or the first noun. It is often assumed that the correct attachment is always among these 1 Introduction Most NLP applications are either data-driven (classification tasks are solved by comparing possible solutions to previous problems and their solutions) or rule-based (general rules are formulated which must be applicable to all cases that might be encountered). Both methods face obvious problems: The data-driven approach is at the mercy of its trainin"
P06-2029,P98-2177,0,0.948597,"y few instances. (There are, in fact, 4 such instances, well above chance level but still a very small number.) Therefore we used the archived text from 18 volumes of the newspaper tageszeitung as a second source. This corpus contains about 295,000,000 words and should allow us to detect many more collocations. In fact, we do find 2338 instances of ‘Bedenken’+‘gegen’ in the same sentence. Of course, since we have no syntactic annotations for this corpus (and it would be infeasible to create them even by fully automatic parsing), not all of these instances may indicate a syntactic dependency. (Ratnaparkhi, 1998) solved this problem by regarding only prepositions in syntactically unambiguous configurations. Unfortunately, his patterns cannot directly be applied to German sentences because of their freer word order. As an approximation it would be possible to count only pairs of adjacent content words and prepositions. However, this would introduce systematic biases into the counts, because nouns do in fact very often occur adjacently to prepositions that modify them, but many verbs do not. For instance, the phrase ‘jmd. anklagen wegen etw.’ (to sue s.o. for s.th.) gives rise to a strong collocation be"
P06-2029,C04-1056,0,0.0378868,"Missing"
P06-2029,2003.mtsummit-papers.44,0,0.151659,"Missing"
P06-2029,P98-2201,0,0.323119,"Missing"
P06-2029,W97-0109,0,0.445708,"Missing"
P06-2029,C02-1004,0,0.855726,"Missing"
P06-2029,W04-0307,0,0.0862732,"Missing"
P06-2029,W97-0317,0,\N,Missing
P06-2029,J93-1005,0,\N,Missing
P06-2029,P00-1014,0,\N,Missing
P06-2029,J03-4003,0,\N,Missing
P06-2029,C98-2196,0,\N,Missing
P06-2029,C98-2172,0,\N,Missing
P13-3019,W11-2101,0,0.0149725,"German parser. The reason why we chose this parser was that, due to its architecture, it is able to handle ungrammatical and ambiguous input data. The experiments conducted so far show that using the data made available at the NAACL 2012 WMT workshop, CESM correlates well with the human judgments at the system level. One of the future experiments that we intend to perform is to assess metric quality on the entire evaluation set. Moreover, we plan to compare CESM with other tree-based MT metrics. Furthermore, the WMT12 workshop offers different ranking possibilities, like the ones presented in Bojar et al (2011) and in Lopez (2012). It will be determined how much are the segment level evaluation results influenced by these ranking orders. One limitation of the proposed metric is that, at the moment it is restricted to translations from any source language to German as a target language. Because of this reason, we plan to extend the metric to other languages and see how well it performs in different settings. In further experiments we also intend to test CESM using statistical based dependency parsers, like the Malt Parser (Nivre et al., 2007) and the MST parser (McDonald et al., 2006), in order to de"
P13-3019,P04-3008,1,0.842884,"Missing"
P13-3019,P06-1037,1,0.846535,"Missing"
P13-3019,W12-3105,0,0.0219441,"orrelation with human judgments, where Kendall tau (Callison-Burch et al., 2012) is defined as: In order to compute the value of Kendall tau, we determined the number of concordant pairs and the number of discordant pairs of judgments. Similarly to the guideline followed during the WMT12 workshop (Callison-Burch et al., 2012), we penalized ties given by CESM and ignored ties assigned by the human judgments. The obtained result was a correlation of 0.058. As a term of comparison, the highest correlation for segment level reported in Callinson-Burch et al. (2012) was 0.19 obtained by TerrorCat (Fishel et al., 2012) and the lowest was BlockErrCats (Popovic, 2012) with 0.040. However, these results were obtained by evaluating on the entire test set. The rather low correlation result we obtained can be partially explained by the fact that only one judgment of a pair of reference and translation was taken into account. It will be 133 interesting to see how the averaging of the ranks of a translation influences the correlation coefficient. produce new references has increased the BLEU score, therefore this is an approach that will be further investigated. 5 Acknowledgments Conclusions and future work In this"
P13-3019,W12-3134,0,0.026867,"e test data for the workshop consisted of 99 translated news articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (Ganitkevitch et al., 2012), KIT (Niehues et al., 2012), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC. After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman's rank correlation coefficient ρ: where n represents the number of MT systems considered duri"
P13-3019,W12-3144,0,0.0480928,"Missing"
P13-3019,niessen-etal-2000-evaluation,0,0.0864977,"translation for the candidate translation. 130 Proceedings of the ACL Student Research Workshop, pages 130–135, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Among the n-gram based metrics, one of the most popular methods of evaluation is BLEU (Papineni et al., 2001). It provides a score that is computed as the summed number of n-grams shared by the references and the output, divided by the total number of n-grams. Lexical metrics that use the edit distance are constructed using the Levenshtein distance applied at the word level. Among these metrics, WER (Niessen et al., 2000) is the one which is used more frequently; it calculates the minimal number of insertion, substitutions and deletions needed to transform the candidate translation into a reference. Metrics based on lexical matching suffer from not being able to consider the variation encountered in natural language. Thus, they reward a low score to an otherwise fluent and syntactically correct candidate translation, if it does not share a certain number of words with the set of references. Because of this, major disagreements between the scores assigned by BLEU and human judgments have been reported in Koehn"
P13-3019,W97-0802,0,0.31656,"Missing"
P13-3019,W07-0411,0,0.0235889,"Missing"
P13-3019,J10-4005,0,0.0385409,"Missing"
P13-3019,2001.mtsummit-papers.68,0,0.0536224,"system and the reference. This similarity can be computed at different levels: lexical, syntactic or semantic. At the lexical level, the metrics developed so far can be divided into two major categories: n-gram based and edit distance based. 1 We will use the term reference for the reference translation and the term translation for the candidate translation. 130 Proceedings of the ACL Student Research Workshop, pages 130–135, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Among the n-gram based metrics, one of the most popular methods of evaluation is BLEU (Papineni et al., 2001). It provides a score that is computed as the summed number of n-grams shared by the references and the output, divided by the total number of n-grams. Lexical metrics that use the edit distance are constructed using the Levenshtein distance applied at the word level. Among these metrics, WER (Niessen et al., 2000) is the one which is used more frequently; it calculates the minimal number of insertion, substitutions and deletions needed to transform the candidate translation into a reference. Metrics based on lexical matching suffer from not being able to consider the variation encountered in"
P13-3019,W12-3106,0,0.0314784,"llison-Burch et al., 2012) is defined as: In order to compute the value of Kendall tau, we determined the number of concordant pairs and the number of discordant pairs of judgments. Similarly to the guideline followed during the WMT12 workshop (Callison-Burch et al., 2012), we penalized ties given by CESM and ignored ties assigned by the human judgments. The obtained result was a correlation of 0.058. As a term of comparison, the highest correlation for segment level reported in Callinson-Burch et al. (2012) was 0.19 obtained by TerrorCat (Fishel et al., 2012) and the lowest was BlockErrCats (Popovic, 2012) with 0.040. However, these results were obtained by evaluating on the entire test set. The rather low correlation result we obtained can be partially explained by the fact that only one judgment of a pair of reference and translation was taken into account. It will be 133 interesting to see how the averaging of the ranks of a translation influences the correlation coefficient. produce new references has increased the BLEU score, therefore this is an approach that will be further investigated. 5 Acknowledgments Conclusions and future work In this paper, a new evaluation metric for MT was intro"
P13-3019,W05-0904,0,0.0251347,"in number of words with the set of references. Because of this, major disagreements between the scores assigned by BLEU and human judgments have been reported in Koehn and Monz (2006) and Callison-Burch et al. (2006). Another disadvantage is that many of them cannot be applied at the segment level, which is often needed in order to better assess the quality of MT output and to determine which improvements should be made to the MT system. Because of these disadvantages there is an increasing need for other approaches to MT evaluation that go beyond the lexical level of the phrases compared. In Liu and Gildea (2005), three syntactic evaluation metrics are presented. The first of these metrics, the Subtree Metric (SMT), is based on determining the number of subtrees that can be found in both the candidate translation and the reference phrase structure trees. The second metric, which is a kernel-based subtree metric, is defined as the maximum of the cosine measure between the MT output and the set of references. The third metric proposed computes the number of matching n-grams between the headword chains of the reference and the candidate translation dependency trees obtained using the parser described in"
P13-3019,W12-3101,0,0.0182815,"why we chose this parser was that, due to its architecture, it is able to handle ungrammatical and ambiguous input data. The experiments conducted so far show that using the data made available at the NAACL 2012 WMT workshop, CESM correlates well with the human judgments at the system level. One of the future experiments that we intend to perform is to assess metric quality on the entire evaluation set. Moreover, we plan to compare CESM with other tree-based MT metrics. Furthermore, the WMT12 workshop offers different ranking possibilities, like the ones presented in Bojar et al (2011) and in Lopez (2012). It will be determined how much are the segment level evaluation results influenced by these ranking orders. One limitation of the proposed metric is that, at the moment it is restricted to translations from any source language to German as a target language. Because of this reason, we plan to extend the metric to other languages and see how well it performs in different settings. In further experiments we also intend to test CESM using statistical based dependency parsers, like the Malt Parser (Nivre et al., 2007) and the MST parser (McDonald et al., 2006), in order to decide whether the cho"
P13-3019,W06-2932,0,0.0391264,"e ones presented in Bojar et al (2011) and in Lopez (2012). It will be determined how much are the segment level evaluation results influenced by these ranking orders. One limitation of the proposed metric is that, at the moment it is restricted to translations from any source language to German as a target language. Because of this reason, we plan to extend the metric to other languages and see how well it performs in different settings. In further experiments we also intend to test CESM using statistical based dependency parsers, like the Malt Parser (Nivre et al., 2007) and the MST parser (McDonald et al., 2006), in order to decide whether the choice of parser influences the performance of the metric. Another approach that we will explore for improving CESM is to compare dependency parse trees using the base form and the part-ofspeech of the tokens, instead of the exact lexical match. We will try this approach in order to avoid penalizing lexical variation. The accuracy of CESM can be further increased by the use of paraphrases, which can be obtained by using a German thesaurus or a lexical resource like GermaNet (Hamp and Feldweg, 1997). Furthermore, a technique like the one described in Owczarzak ("
P13-3019,W98-0509,1,0.545086,"T evaluation experiences a gradual shift of focus from lexical metrics to structural ones, whether they are syntactic or semantic or a combination of both. This paper introduces a new syntactic automatic MT evaluation method. At this stage of research the new metric is evaluating translations from any source language into German. Given that a set of constraint-based grammar rules are available for that language, extensions to other target languages are anytime possible. The chosen tool for providing syntactic information for German is the Weighted Constraints Dependency Grammar (WCDG) parser (Menzel and Schröder, 1998), which is preferred over other parsers because of its robustness to ungrammatical input, as it is typical for MT output. The rest of this paper is organized as follows. In Section 2 the state of the art in MT evaluation is presented, while in Section 3 the new syntactic metric is described. The experimental setup and results are presented in Section 4. The last section deals with the conclusions and future work. 2 State of the art Automatic evaluation of MT systems relies on the existence of at least one reference1 created by a human annotator. Using an automatic method of evaluation a score"
P13-3019,W12-3149,0,0.0253519,"h et al., 2012). The test data for the workshop consisted of 99 translated news articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (Ganitkevitch et al., 2012), KIT (Niehues et al., 2012), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC. After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman's rank correlation coefficient ρ: where n represents the num"
P13-3019,W12-3151,0,0.0158623,"articles in English, German, French, Spanish and Czech. At the system level, the initial German test set provided at the workshop was filtered according to the length of segments. This was done in order to limit the time requirements of WCDG. As a result, 500 segments with a length between 50 and 80 characters were extracted from the German reference file. In the next step, we arbitrarily selected the outputs of 7 of the 15 systems that were submitted for evaluation in the English to German translation task: DFKI (Vilar, 2012), JHU (Ganitkevitch et al., 2012), KIT (Niehues et al., 2012), UK (Zeman, 2012) and three anonymized system outputs referred to as OnlineA, OnlineB, OnlineC. After this initial step of filtering the data, the 7 systems were evaluated by calculating the CESM score for every pair of reference and translation segments corresponding to a system. The average scores obtained are depicted in Table 1. Evaluation of the metric at the system level was performed by measuring the correlation of the CESM metric with human judgments using Spearman's rank correlation coefficient ρ: where n represents the number of MT systems considered during evaluation, and di2 represents the differen"
P13-3019,W12-3102,0,\N,Missing
P13-3019,J03-4003,0,\N,Missing
P13-3019,P02-1040,0,\N,Missing
P13-3019,W06-3114,0,\N,Missing
P14-2130,W07-2416,0,0.0878361,"Missing"
P14-2130,R13-1048,1,0.875515,"Missing"
P14-2130,H94-1020,0,0.182318,", w ∈ Pr because it is required by a valency of the verb. While this procedure is language-independent, some language-specific transformations must be applied nonetheless. For English, parts of gapping coordinations can be predicted whereas others can not. For German, the transformations described in (Beuck and Menzel, 2013) have been used without further changes. Both sets of structurally and lexically determined roles are language dependent. The label sets for German have been adopted from (Beuck and Menzel, 2013), while the sets for English have been obtained by manually analyzing the PTB (Marcus et al., 1994) for predictability. For words marked as predictable their existence and word class, but not their lexicalization and position can be predicted. Therefore, we replace the lexical item with “[virtual]” and generalize the part-of-speech tag to a more coarse grained one. 4 creates an integer linear program – in the form of a factor graph – with the variables representing the possible edges of the analyses. Since well-formedness is enforced by factors, additional constraints on the shape of analyses can be imposed without changing the core algorithm of the parser. We define three additional restri"
P14-2130,P13-2109,0,0.0373886,"that is attached to unused may not have any dependents. 2) A VN may not be attached to 0 if it has no dependents. 3) Only VNs may be attached to the unused node. For a given sentence prefix, let A be the set of possible edges, V the set of all vertices, N ⊂ V the VNs and u ∈ V the unused node. Moreover, let B ⊂ A be the set of edges building a well-formed analysis and za , I(a ∈ B), where I(.) is the indicator function. The three additional conditions can be expressed as linear constraints which ensure that every output is a valid PDA: Predictive parsing with TurboParser We adapt TurboParser (Martins et al., 2013) for incremental parsing because it does not impose structural constraints such as single-headedness in its core algorithm. For each parsing problem, it 805 zhn, ji + zhu,ni ≤ 1, zh0,ni ≤ n ∈ N, j ∈ V ∑ zhn, ji , j∈V zhu,ii = 0, (1) n∈N (2) i ∈V N (3) The current implementation is pseudo-incremental. It reinitializes the ILP for every increment without passing intermediate results from one incremental processing step to the next, although this might be an option for further optimization. High quality incremental parsing results can not be expected from models which have only been trained on w"
P14-2130,R09-1025,0,\N,Missing
P14-2130,foth-etal-2014-size,1,\N,Missing
P98-1086,C90-3030,0,0.035845,"verconstrained in the classical sense because at least preferential constraints are violated by the solution. We will give a more detailed introduction to constraint parsing in Section 2 and to the extension to graded constraints in Section 3. Section 4 presents algorithms for the solution of the previously defined parsing problem and the linguistic modeling for constraint parsing is finally described in Section 5. 2 Parsing as Constraint Satisfaction While eliminative approaches are quite customary for part-of-speech disambiguation (Padr6, 1996) and underspecified structural representations (Karlsson, 1990), it has hardly been used as a basis for full structural interpretation. Maruyama (1990) describes full parsing by means of constraint satisfaction for the first time. 0&quot;. nil (a) The snake is chased by the cat. 1 (b) 2 3 vl = v3 = v5 = (nd, 2) (nil, O) (pp, 4) vT = (pc, 5) 4 v2 v4 v6 5 = = = 6 7 (subj,3) (ac,3) (nd, 7) Figure 1: (a) Syntactic dependency tree for an example utterance: For each word form an unambiguous subordination and a label, which characterizes of subordination, are to be found. (b) Labellings for a set of constraint variables: Each variable corresponds to a word form and t"
P98-1086,P90-1005,0,0.240779,"ated by the solution. We will give a more detailed introduction to constraint parsing in Section 2 and to the extension to graded constraints in Section 3. Section 4 presents algorithms for the solution of the previously defined parsing problem and the linguistic modeling for constraint parsing is finally described in Section 5. 2 Parsing as Constraint Satisfaction While eliminative approaches are quite customary for part-of-speech disambiguation (Padr6, 1996) and underspecified structural representations (Karlsson, 1990), it has hardly been used as a basis for full structural interpretation. Maruyama (1990) describes full parsing by means of constraint satisfaction for the first time. 0&quot;. nil (a) The snake is chased by the cat. 1 (b) 2 3 vl = v3 = v5 = (nd, 2) (nil, O) (pp, 4) vT = (pc, 5) 4 v2 v4 v6 5 = = = 6 7 (subj,3) (ac,3) (nd, 7) Figure 1: (a) Syntactic dependency tree for an example utterance: For each word form an unambiguous subordination and a label, which characterizes of subordination, are to be found. (b) Labellings for a set of constraint variables: Each variable corresponds to a word form and takes a pairing consisting of a label and a word form as a value. Dependency relations ar"
P98-1086,W98-0509,1,0.835229,"y are not guaranteed to do so in all cases. This allows to influence the temporal characteristics of the parsing procedure, a possibility which seems especially important in interactive applications: If the system has to deliver a reasonable solution within a specific time interval a dynamic scheduling of computational resources depending on the remaining ambiguity and available time is necessary (Menzel, 1994, anytime algorithm). While different kinds of search are more suitable with regard to the correctness property, local pruning strategies lend themselves to resource adaptive procedures. Menzel and SchrSder (1998b) give details about the decision procedures for constraint parsing. 5 Grammar modeling For experimental purposes a constraint grammar has been set up, which consists of two descriptive levels, one for syntactic (including morphology and agreement) and one for semantic relations. Whereas the syntactical description clearly follows a dependency approach, the second main level of our analysis, semantics, is limited to sortal restrictions and predicate-argument relations for verbs, predicative adjectives, and predicative nouns. In order to illustrate the interaction of syntactical and semantical"
P98-1086,1993.mtsummit-1.11,0,0.0196573,", it certainly makes an adequate modelling of feature percolation phenomena rather difficult. Again, the use of auxiliary levels provides a solution by allowing to transport the required information along the edges of the dependency tree by means of appropriately defined labels. For efficiency reasons (the complexity is exponential in the number of features to percolate over the same edge) the application of this technique should be restricted to a few carefully selected phenomena. The approach presented in this paper has been tested successfully on some 500 sentences of the Verbmobil domain (Wahlster, 1993). Currently, there are about 210 semantic constraints, including constraints on auxiliary levels. The syntax is defined by 240 constraints. Experiments with slightly distorted sentences resulted in correct structural trees in most cases. 6 Conclusion An approach to the parsing of dependency structures has been presented, which is based on the elimination of partial structural interpretations by means of constraint satisfaction techniques. Due to the graded nature of constraints (possibly conflicting) evidence from a wide variety of informational sources can be integrated into a uniform computa"
R09-1033,W06-2920,0,0.0827266,"Missing"
R09-1033,P06-1041,1,0.903548,"Missing"
R09-1033,2000.iwpt-1.11,1,0.784143,"Missing"
R09-1033,W99-0623,0,0.10226,"Missing"
R09-1033,P90-1005,0,0.0311633,"Missing"
R09-1033,W06-2932,0,0.0394285,"Missing"
R09-1033,D07-1013,0,0.0413837,"Missing"
R09-1033,H05-1066,0,0.258932,"Missing"
R09-1033,W06-2933,0,0.0394858,"Missing"
R09-1033,P08-1108,0,0.0295945,"Missing"
R09-1033,W05-1518,0,0.019331,"ally considered as two alternative roadmaps towards natural language processing solutions. Since the resulting components exhibit quite different performance characteristics with respect to coverage, robustness and output quality, they might be able to provide some kind of complementary information, which could even lead to a notable degree of synergy between them when combined within a single system solution. For the task of dependency parsing the high potential for such a synergy has indeed been demonstrated already. A popular approach for combining alternative decision procedures is voting [18]. It makes use of a symmetric architecture, where a meta component chooses from among the available candidate hypotheses by means of a (weighted) voting scheme. Such an approach not only requires the target structures of all components to be of the same kind, but in case of complex structures like parse trees also requires sophisticated decision procedures which are able to select the optimal hypotheses with respect to additional global constraints (e.g. the tree property). Since this optimization problem has to be solved by the individual parser anyhow, an asymmetric architecture suggests its"
R09-1033,N06-2033,0,\N,Missing
R09-1033,D08-1017,0,\N,Missing
R09-1077,gimenez-marquez-2004-svmtool,0,0.0923703,"Missing"
R09-1077,W01-0519,0,0.0573172,"Missing"
R09-1077,W05-0707,0,0.0617664,"Missing"
R09-1077,A00-1031,0,0.2353,"Missing"
R09-1077,C04-1022,0,0.0297801,"tored language models also for Amharic. 1.3 to choose a path that results in a better model. Therefore, choosing a backoff path is an important decision one has to make in FLM. There are three possible ways of choosing a backoff path: 1) Choosing a fixed path based on linguistic or other reasonable knowledge; 2) Generalized all-child backoff where multiple backoff paths are chosen at run time; and 3) Generalized constrained-child backoff where a subset of backoff paths is chosen at run time [14]. A genetic algorithm for learning the structure of a factored language model has been developed by [7]. Factored language modeling Factored language models (FLM) have first been introduced in [13] for incorporating various morphological information in Arabic language modeling. In FLM a word is viewed as a bundle or vector of K parallel factors, that is, wn ≡ fn1 , fn2 , ..., fnk . The factors of a given word can be the word itself, stem, root, pattern, morphological classes, or any other linguistic element into which a word can be decomposed. The goal of an FLM is, therefore, to produce a statistical model over these factors. There are two important points in the development of FLM: choosing t"
R09-1077,W09-0715,0,0.0127488,"DJP ADJC ADJPC ADJ PREP CONJ ADV NUMCR NUMOR NUMP NUMC NUMPC INT PUNC UNC Table 1: Amharic POS tagset (extracted from [6]) Annotation of Amharic News Documents” [6]. It consists of 210,000 manually annotated tokens of Amharic news documents. In this corpus, collocations have been annotated inconsistently. Sometimes a collocation assigned a single POS tag and sometimes each token in a collocation got a separate POS tag. For example, ’tmhrt bEt’, which means school, has got a single POS tag, N, in some places and a separate POS tags for each of the tokens in some other places. Therefore, unlike [8] who merged a collocation with a single tag, effort has been exerted to annotate collocations consistently by assigning separate POS tags for the individual words in a collocation. 2.3 The software We used two kinds of software, namely TnT and SVMTool, to train different taggers. TnT, Trigram’n’Tags, is a Markov model based, efficient, language independent statistical part of speech tagger [5]. It has been applied on many languages including German, English, Slovene, Hungarian and Swedish successfully. [15] showed that TnT is better than maximum entropy, memory- and transformationbased taggers"
R09-1077,J00-4006,0,\N,Missing
R13-1048,2000.iwpt-1.11,1,0.736659,"Missing"
R13-1048,P10-1151,0,0.0611033,"Missing"
R13-1048,W09-3816,1,0.843669,"better results. Foth and Menzel (2006) showed that WCDG can be augmented by trainable components to raise the accuracy of WCDG. The output of these predictors was converted into constraints to help WCDG finding a good analysis. One of the components was a shift-reduce parser modeled after Nivre (2003), which was the first description of the MaltParser architecture. Although the shift-reduce parser was relatively simple compared to MaltParser and had a labeled accuracy of only 80.7 percent, it helped to raise the accuracy of WCDG from 87.5 to 89.8 percent. This approach has later been used by Khmylko et al. (2009) to integrate MST-Parser (McDonald et al., 2006) (which does not work incrementally) as an external data source for WCDG. We integrated MaltParser in a similar way. MaltParser consumes the input from left to right and, if using the 2planar algorithm, constructs edges as soon as possible: An edge can only be created between the word on top of the stack and the current input word. This means that every edge has to be constructed as soon as the second word 2 All experiments have been carried out on a 48-core machine with four AMD Opteron 6168 processors. 376 number of threads used 1st Qu. Median"
R13-1048,W06-2932,0,0.0401483,"that WCDG can be augmented by trainable components to raise the accuracy of WCDG. The output of these predictors was converted into constraints to help WCDG finding a good analysis. One of the components was a shift-reduce parser modeled after Nivre (2003), which was the first description of the MaltParser architecture. Although the shift-reduce parser was relatively simple compared to MaltParser and had a labeled accuracy of only 80.7 percent, it helped to raise the accuracy of WCDG from 87.5 to 89.8 percent. This approach has later been used by Khmylko et al. (2009) to integrate MST-Parser (McDonald et al., 2006) (which does not work incrementally) as an external data source for WCDG. We integrated MaltParser in a similar way. MaltParser consumes the input from left to right and, if using the 2planar algorithm, constructs edges as soon as possible: An edge can only be created between the word on top of the stack and the current input word. This means that every edge has to be constructed as soon as the second word 2 All experiments have been carried out on a 48-core machine with four AMD Opteron 6168 processors. 376 number of threads used 1st Qu. Median Mean 3rd Qu. Max. np 1 2 4 8 16 32 48 43 86 161"
R13-1048,A00-1031,0,0.0942496,"ted onto the stack, the creation of the edge would no longer be possible. The parser works monotonically since edges are only added to but never removed from the set of edges. This means that all decisions by the parser are final; if word a is not attached to word b, we can be sure that a will never be attached to b in subsequent analyses. As a corollary, if MaltParser has an accuracy of X percent on whole sentences, the probability that a newly created edge is correct will also be X percent. As a delay is not acceptable for our application scenario, we will use MaltParser and the TnT tagger (Brants, 2000) without lookahead despite their inferior accuracy in that mode3 . 5.1 by the tagger predictor. Each time a new word w is pushed to jwcdg, MaltPredictor forwards w together with its PoS-tag onto MaltParsers input queue and runs MaltParser’s algorithm until a shift operations occurs. With this shift operation, w is consumed from the input queue. If the sentence is marked as being finished, MaltParser is run until it has fully parsed the sentence. MaltPredictor then reads the state of MaltParser and stores for each word the regent it has been assigned to by MaltParser. If Maltparser did not assi"
R13-1048,W04-0311,0,0.156627,"ing, pages 373–381, Hissar, Bulgaria, 7-13 September 2013. are used to describe the syntactic structure of sentence prefixes, different amounts of prediction can be provided. The interesting cases are those where either the regent or a dependent is not yet part of the sentence prefix. If the regent of a word w is not yet available, the parser can make a prediction about where and how w should be attached. One possibility is to simply state that the regent of w lies somewhere in the future without giving any additional information. This can be modelled by attaching w to a generic nonspec node (Daum, 2004). Beuck et al. (2011) call this minimal prediction. However, it is usually possible to predict more: The existence of upcoming words can be anticipated and w can then be attached to one of these words. Of course, most of the time it will not be possible to predict exact words but abstract pseudo-words can be used instead that stand for a certain type such as nouns or verbs. Beuck et al. (2011) call these pseudo-words virtual nodes and the approach of using virtual nodes structural prediction (because the virtual nodes accommodate crucial aspects of the upcoming structure of the sentence). A vi"
R13-1048,P06-1041,1,0.819257,"lower bound of about 200 ms per word on current hardware. The heart of the frobbing algorithm is the attackConflict method which, given an analysis a and a conflict c, systematically tries all changes of edges that are part of c. It then returns the best MaltParser as a Predictor for jwcdg When facing a tight time limit, jwcdg has only very little time to improve an analysis by transforming it. Therefore, with tighter time limits a good initial attachment becomes more important and a method which provides frobbing with a good initial analysis could help to achieve drastically better results. Foth and Menzel (2006) showed that WCDG can be augmented by trainable components to raise the accuracy of WCDG. The output of these predictors was converted into constraints to help WCDG finding a good analysis. One of the components was a shift-reduce parser modeled after Nivre (2003), which was the first description of the MaltParser architecture. Although the shift-reduce parser was relatively simple compared to MaltParser and had a labeled accuracy of only 80.7 percent, it helped to raise the accuracy of WCDG from 87.5 to 89.8 percent. This approach has later been used by Khmylko et al. (2009) to integrate MST-"
R13-1048,W03-3017,0,0.0473168,"edictor for jwcdg When facing a tight time limit, jwcdg has only very little time to improve an analysis by transforming it. Therefore, with tighter time limits a good initial attachment becomes more important and a method which provides frobbing with a good initial analysis could help to achieve drastically better results. Foth and Menzel (2006) showed that WCDG can be augmented by trainable components to raise the accuracy of WCDG. The output of these predictors was converted into constraints to help WCDG finding a good analysis. One of the components was a shift-reduce parser modeled after Nivre (2003), which was the first description of the MaltParser architecture. Although the shift-reduce parser was relatively simple compared to MaltParser and had a labeled accuracy of only 80.7 percent, it helped to raise the accuracy of WCDG from 87.5 to 89.8 percent. This approach has later been used by Khmylko et al. (2009) to integrate MST-Parser (McDonald et al., 2006) (which does not work incrementally) as an external data source for WCDG. We integrated MaltParser in a similar way. MaltParser consumes the input from left to right and, if using the 2planar algorithm, constructs edges as soon as pos"
S17-2024,P07-2045,0,0.00487468,"Missing"
S17-2024,N13-1090,0,0.034278,"between two vectors (obtained with Doc2Vec1 )? • Given a cross-lingual task, does averaging the similarity scores obtained using the Doc2Vec models trained on both language corpora result in an improvement over using only the scores from one model? 2.2 Semantic Textual Similarity via Paragraph Vector Paragraph Vector In order to assess the semantic textual similarity of two sentences, methods of representing them are crucial. Le and Mikolov (2014) propose a continuous, distributed vector representation of phrases, sentences and documents, Paragraph Vectors. It is a continuation of the work in Mikolov et al. (2013a) where word vectors (embeddings) are introduced in order to semantically represent words. The strength of capturing the semantics of words via word embeddings is visible not only when considering words with similar meaning like ”strong” and ”powerful” (Le and Mikolov, 2014), but also in learning relationships such as male/female where the vector representation for King - Man + Woman results in a vector very close to Queen (Mikolov et al., 2013b). In the Paragraph Vector framework, the paragraph vectors are concatenated with the word vectors to form one vector. The paragraph vector acts Track"
S17-2024,D15-1075,0,0.0434385,"sentences. Brychc´ın and Svoboda (2016) follow a similar approach but apply it also to the cross-lingual task. We raise three research questions regarding the usage of Paragraph Vector in STS: 3 3.1 • To which degree does the vector size matter? Corpora For training the Doc2Vec models we used various corpora available for the different language pairs. Following the rationale from Lau and Baldwin (2016), we concatenated to the corpora the test set too as the Doc2Vec training is purely unsupervised. The corpora we used are made available by Opus (Tiedemann, 2012) (except Commoncrawl2 and SNLI (Bowman et al., 2015)): Wikipedia (Wolk and Marasek, 2014), TED3 , MultiUN (Eisele and Chen, 2010), EUBookshop (Skadin¸sˇ et al., 2014), SETIMES4 , Tatoeba5 , WMT6 and News Commentary7 . The following table presents which corpora were used and how many sentences they consist of. The corpora marked with * were used only for the third run. • What could be a better alternative to the traditional Cosine metric for measuring the similarity between two vectors (obtained with Doc2Vec1 )? • Given a cross-lingual task, does averaging the similarity scores obtained using the Doc2Vec models trained on both language corpora r"
S17-2024,S16-1089,0,0.123021,"Missing"
S17-2024,P14-2034,0,0.0179615,"pora/setimes/ 5 http://tatoeba.org/ 6 http://www.statmt.org/wmt14/ 7 http://www.casmacat.eu/corpus/news-commentary 3 1 The terms Paragraph Vector and Doc2Vec are used interchangeably as follows. 171 a difference. For training the English models only the EN side of the ES-EN language pair was used. 3.2 range (0, 1] we multiply them by 5 in order to return a continuous valued similarity score on a scale from 0 to 5, as the competition requires. We submitted three runs to the competition: Preprocessing For the sub-tasks that included the Arabic language we utilized the Stanford Arabic Segmenter (Monroe et al., 2014) in order to reduce lexical sparsity. For all the other sub-tasks, we performed text normalization, tokenization and lowercasing using the scripts available in the Moses Machine Translation Toolkit (Koehn et al., 2007). 3.3 Model(size=200), Cosine similarity EN-ES: Model ES run1 AR-EN: Model AR TR-EN: Model TR Model(size=400), Cosine similarity EN-ES: Model ES run2 AR-EN: Model AR TR-EN: Model TR Model(size=200), Bray-Curtis similarity, more training data run3 EN-ES: Model EN AR-EN: Model EN TR-EN: Model EN Methods We assess the semantic similarity between two sentences based on their continuo"
S17-2024,S17-2001,0,0.390674,"nguage Systems Division {mduma, menzel}@informatik.uni-hamburg.de Abstract but differs in the input sentences which come from two languages. This year’s shared task features six sub-tasks: Arabic-Arabic, Arabic-English, Spanish-Spanish, Spanish-English (two test sets), English-English and a surprise task (Turkish-English) for which no annotated data is offered. For example, for the English monolingual STS track, the pair of sentences below had a score of 3 assigned by human annotators, meaning that the two sentences are roughly equivalent, but some essential information differs or is missing (Cer et al., 2017). Bayes’ theorem was named after Rev Thomas Bayes and is a method used in probability theory. As an official theorem, Bayes’ theorem is valid in all universal interpretations of probability. We present an unsupervised, knowledge-free approach that utilizes Paragraph Vector (Le and Mikolov, 2014) to represent sentences by means of continuous distributed vectors. In addition to experimenting with feature spaces of different dimensionality, we also compare three state-of-the-art similarity metrics (Cosine, Bray-Curtis and Correlation) for calculating the STS scores. We do not make use of any lexi"
S17-2024,skadins-etal-2014-billions,0,0.0580886,"Missing"
S17-2024,eisele-chen-2010-multiun,0,0.0216486,"y it also to the cross-lingual task. We raise three research questions regarding the usage of Paragraph Vector in STS: 3 3.1 • To which degree does the vector size matter? Corpora For training the Doc2Vec models we used various corpora available for the different language pairs. Following the rationale from Lau and Baldwin (2016), we concatenated to the corpora the test set too as the Doc2Vec training is purely unsupervised. The corpora we used are made available by Opus (Tiedemann, 2012) (except Commoncrawl2 and SNLI (Bowman et al., 2015)): Wikipedia (Wolk and Marasek, 2014), TED3 , MultiUN (Eisele and Chen, 2010), EUBookshop (Skadin¸sˇ et al., 2014), SETIMES4 , Tatoeba5 , WMT6 and News Commentary7 . The following table presents which corpora were used and how many sentences they consist of. The corpora marked with * were used only for the third run. • What could be a better alternative to the traditional Cosine metric for measuring the similarity between two vectors (obtained with Doc2Vec1 )? • Given a cross-lingual task, does averaging the similarity scores obtained using the Doc2Vec models trained on both language corpora result in an improvement over using only the scores from one model? 2.2 Semant"
S17-2024,tiedemann-2012-parallel,0,0.0113811,"ployed in obtaining the similarity score between sentences. Brychc´ın and Svoboda (2016) follow a similar approach but apply it also to the cross-lingual task. We raise three research questions regarding the usage of Paragraph Vector in STS: 3 3.1 • To which degree does the vector size matter? Corpora For training the Doc2Vec models we used various corpora available for the different language pairs. Following the rationale from Lau and Baldwin (2016), we concatenated to the corpora the test set too as the Doc2Vec training is purely unsupervised. The corpora we used are made available by Opus (Tiedemann, 2012) (except Commoncrawl2 and SNLI (Bowman et al., 2015)): Wikipedia (Wolk and Marasek, 2014), TED3 , MultiUN (Eisele and Chen, 2010), EUBookshop (Skadin¸sˇ et al., 2014), SETIMES4 , Tatoeba5 , WMT6 and News Commentary7 . The following table presents which corpora were used and how many sentences they consist of. The corpora marked with * were used only for the third run. • What could be a better alternative to the traditional Cosine metric for measuring the similarity between two vectors (obtained with Doc2Vec1 )? • Given a cross-lingual task, does averaging the similarity scores obtained using t"
S17-2024,S16-1113,0,0.029402,"rs are shared across all paragraphs, while the paragraph vector is shared across all contexts generated from the same paragraph. The vectors are trained using stochastic gradient descent with backpropagation (Le and Mikolov, 2014). Since the STS task requires assigning a similarity score between two sentences, we apply Paragraph Vector at the sentence level. The models are ˇ uˇrek and Sotrained using the Gensim library (Reh˚ jka, 2010). Semantic Textual Similarity We present in this subsection the state-of-the-art in STS-Task 1 using Paragraph Vector since it is the most relevant to our work. King et al. (2016), for instance make use of Paragraph Vectors as one approach in the English monolingual sub-task. Results are reported for a single vector size and the Cosine metric which is employed in obtaining the similarity score between sentences. Brychc´ın and Svoboda (2016) follow a similar approach but apply it also to the cross-lingual task. We raise three research questions regarding the usage of Paragraph Vector in STS: 3 3.1 • To which degree does the vector size matter? Corpora For training the Doc2Vec models we used various corpora available for the different language pairs. Following the ration"
S18-2006,D17-1130,0,0.014673,"phenomena that affect any approach relying either on CCG or graph algebraic approaches for AMR parsing. This includes differences of representation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonic construction process. To our knowledge, this paper provides the first analysis of these corpus issues. 1 1.1 Introduction Related Work There is an extensive body of work on semantic parsing of AMRs, using a range of techniques including maximum spanning tree-based parsing (Flanigan et al., 2014), transition-based parsing (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018), machine translation (van Noord and Bos, 2017), graph grammars (Peng et al., 2015; Groschwitz et al., 2017), and CCG parsing (Artzi et al., 2015; Misra and Artzi, 2016). The system of Artzi et al. (2015) is most similar to the present work. They induce a semantic CCG using a translation of AMR into λ-calculus. A key difference is that their training algorithm combines lexicon induction and parameter training into a single phase. New lexical items are generated during each training step and then filtered based upon the model’s current parameters. In contrast, in this work w"
S18-2006,W15-0127,0,0.292596,"uction, with parameter training to be performed in a subsequent step. Another related system is presented in Lewis et al. (2015), where a CCG parser is adapted to produce shallow semantic dependency graphs. In contrast, the meaning representations employed here are abstract and do not directly refer to the sentence under analysis. Word-to-node alignments on the AMR corpus play an important role in this work. We experiment with JAMR’s rule-based aligner (Flanigan et al., 2014) and the statistical ISI aligner (Pourdamghani et al., 2014). Graph algebras have recently been applied to AMR parsing (Koller, 2015; Groschwitz et al., 2017), but not in combination with CCG. In contrast, we use syntactic CCG derivations to constrain the space of possible derivations. However, the idea of using a constrained version of the HR algebra, introduced by Groschwitz et al. (2017), is also used here, and our Application operator effectively subsumes their Apply and Modify operations. 1.2 CCG is a grammar formalism centered around a transparent syntax-semantics interface (Steedman, 2000). A CCG consists of a small set of combinatory rules, along with a lexicon of entries defining each word’s syntactic and semantic"
S18-2006,W13-2322,0,0.0620036,"d by EasyCCG based on the CCGBankrebanked model (Honnibal et al., 2010). Word-to-node alignments During lexicon induction, we make use of alignments between tokens in the sentence and nodes in the meaning representation. We experiment with JAMR’s aligner (Flanigan et al., 2014) and the ISI aligner (Pourdamghani et al., 2014). Other tools We use Stanford CoreNLP (Manning et al., 2014) for tokenisation. 2 Combinatory Categorial Grammar 2.2 Abstract Meaning Representation The Abstract Meaning Representation (AMR) is a semantic meaning representation language that is purposefully syntax-agnostic (Banarescu et al., 2013). The data set used in this paper, the AMR 1.0 release (Knight et al., 2014), consists of English sentences which have been directly annotated with meaning representations by human annotators. AMR represents meaning as labeled, directed graphs. Nodes are labeled with concepts, while edges represent roles. Predicates and core roles are drawn from PropBank (Kingsbury and Palmer, 2002). In addition, a set of non-core roles has been defined, such as mod, poss, time, etc. Whether it was wise to define AMR independently of any derivational process has been debated (Bender et al., 2015), and in Secti"
S18-2006,D10-1119,0,0.189128,"Missing"
S18-2006,W15-0128,0,0.0832529,"-agnostic (Banarescu et al., 2013). The data set used in this paper, the AMR 1.0 release (Knight et al., 2014), consists of English sentences which have been directly annotated with meaning representations by human annotators. AMR represents meaning as labeled, directed graphs. Nodes are labeled with concepts, while edges represent roles. Predicates and core roles are drawn from PropBank (Kingsbury and Palmer, 2002). In addition, a set of non-core roles has been defined, such as mod, poss, time, etc. Whether it was wise to define AMR independently of any derivational process has been debated (Bender et al., 2015), and in Section 5 we will show some of the issues that arise when attempting to construct derivations for AMRs. Background The task of semantic parsing is concerned with building formal meaning representations for natural language text. While meaning representations can be elements of any formal language, in this paper we are concerned with Abstract Meaning Representations (AMRs). We use Combinatory Categorial Grammar (CCG) as an underlying framework to explain how AMRs may be derived from the surface form words. To do so, we equip CCG with graph construction operators drawn from the HR algeb"
S18-2006,D14-1107,0,0.498683,"ng a sentence. See Figure 1a for an example. CCG derivations are created by recursively applying combinators to the lexical syntactic categories, thus combining them into constituents. Besides Application, implementations of CCG also use other combinators such as Composition, as well as specialized combinators for conjunctions and punctuation. Semantic categories are represented as λcalculus terms. A combinator is always applied to two constituents’ syntactic and semantic categories at the same time, allowing semantic construction to be fully syntax-driven. Tools Syntax parser We use EasyCCG (Lewis and Steedman, 2014) to obtain syntax derivations. For robustness, we extract the ten best derivations produced by EasyCCG based on the CCGBankrebanked model (Honnibal et al., 2010). Word-to-node alignments During lexicon induction, we make use of alignments between tokens in the sentence and nodes in the meaning representation. We experiment with JAMR’s aligner (Flanigan et al., 2014) and the ISI aligner (Pourdamghani et al., 2014). Other tools We use Stanford CoreNLP (Manning et al., 2014) for tokenisation. 2 Combinatory Categorial Grammar 2.2 Abstract Meaning Representation The Abstract Meaning Representation"
S18-2006,P14-1134,0,0.527659,"examined sentences under ideal conditions. We also identify several phenomena that affect any approach relying either on CCG or graph algebraic approaches for AMR parsing. This includes differences of representation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonic construction process. To our knowledge, this paper provides the first analysis of these corpus issues. 1 1.1 Introduction Related Work There is an extensive body of work on semantic parsing of AMRs, using a range of techniques including maximum spanning tree-based parsing (Flanigan et al., 2014), transition-based parsing (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018), machine translation (van Noord and Bos, 2017), graph grammars (Peng et al., 2015; Groschwitz et al., 2017), and CCG parsing (Artzi et al., 2015; Misra and Artzi, 2016). The system of Artzi et al. (2015) is most similar to the present work. They induce a semantic CCG using a translation of AMR into λ-calculus. A key difference is that their training algorithm combines lexicon induction and parameter training into a single phase. New lexical items are generated during each training step and then"
S18-2006,P14-5010,0,0.00287632,"egories at the same time, allowing semantic construction to be fully syntax-driven. Tools Syntax parser We use EasyCCG (Lewis and Steedman, 2014) to obtain syntax derivations. For robustness, we extract the ten best derivations produced by EasyCCG based on the CCGBankrebanked model (Honnibal et al., 2010). Word-to-node alignments During lexicon induction, we make use of alignments between tokens in the sentence and nodes in the meaning representation. We experiment with JAMR’s aligner (Flanigan et al., 2014) and the ISI aligner (Pourdamghani et al., 2014). Other tools We use Stanford CoreNLP (Manning et al., 2014) for tokenisation. 2 Combinatory Categorial Grammar 2.2 Abstract Meaning Representation The Abstract Meaning Representation (AMR) is a semantic meaning representation language that is purposefully syntax-agnostic (Banarescu et al., 2013). The data set used in this paper, the AMR 1.0 release (Knight et al., 2014), consists of English sentences which have been directly annotated with meaning representations by human annotators. AMR represents meaning as labeled, directed graphs. Nodes are labeled with concepts, while edges represent roles. Predicates and core roles are drawn from PropBank (Kings"
S18-2006,W17-6810,0,0.0525813,"resentation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonic construction process. To our knowledge, this paper provides the first analysis of these corpus issues. 1 1.1 Introduction Related Work There is an extensive body of work on semantic parsing of AMRs, using a range of techniques including maximum spanning tree-based parsing (Flanigan et al., 2014), transition-based parsing (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018), machine translation (van Noord and Bos, 2017), graph grammars (Peng et al., 2015; Groschwitz et al., 2017), and CCG parsing (Artzi et al., 2015; Misra and Artzi, 2016). The system of Artzi et al. (2015) is most similar to the present work. They induce a semantic CCG using a translation of AMR into λ-calculus. A key difference is that their training algorithm combines lexicon induction and parameter training into a single phase. New lexical items are generated during each training step and then filtered based upon the model’s current parameters. In contrast, in this work we focus on lexicon inWith the release of the Abstract Meaning Representation (AMR) corpus (Knight et al., 2014), graph represent"
S18-2006,D16-1183,0,0.405181,"onstructions that are not expressible through a monotonic construction process. To our knowledge, this paper provides the first analysis of these corpus issues. 1 1.1 Introduction Related Work There is an extensive body of work on semantic parsing of AMRs, using a range of techniques including maximum spanning tree-based parsing (Flanigan et al., 2014), transition-based parsing (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018), machine translation (van Noord and Bos, 2017), graph grammars (Peng et al., 2015; Groschwitz et al., 2017), and CCG parsing (Artzi et al., 2015; Misra and Artzi, 2016). The system of Artzi et al. (2015) is most similar to the present work. They induce a semantic CCG using a translation of AMR into λ-calculus. A key difference is that their training algorithm combines lexicon induction and parameter training into a single phase. New lexical items are generated during each training step and then filtered based upon the model’s current parameters. In contrast, in this work we focus on lexicon inWith the release of the Abstract Meaning Representation (AMR) corpus (Knight et al., 2014), graph representations of meaning have taken centre stage in research on sema"
S18-2006,J07-3004,0,0.514741,"Missing"
S18-2006,P10-1022,0,0.60211,"m into constituents. Besides Application, implementations of CCG also use other combinators such as Composition, as well as specialized combinators for conjunctions and punctuation. Semantic categories are represented as λcalculus terms. A combinator is always applied to two constituents’ syntactic and semantic categories at the same time, allowing semantic construction to be fully syntax-driven. Tools Syntax parser We use EasyCCG (Lewis and Steedman, 2014) to obtain syntax derivations. For robustness, we extract the ten best derivations produced by EasyCCG based on the CCGBankrebanked model (Honnibal et al., 2010). Word-to-node alignments During lexicon induction, we make use of alignments between tokens in the sentence and nodes in the meaning representation. We experiment with JAMR’s aligner (Flanigan et al., 2014) and the ISI aligner (Pourdamghani et al., 2014). Other tools We use Stanford CoreNLP (Manning et al., 2014) for tokenisation. 2 Combinatory Categorial Grammar 2.2 Abstract Meaning Representation The Abstract Meaning Representation (AMR) is a semantic meaning representation language that is purposefully syntax-agnostic (Banarescu et al., 2013). The data set used in this paper, the AMR 1.0 r"
S18-2006,kingsbury-palmer-2002-treebank,0,0.334674,"2014) for tokenisation. 2 Combinatory Categorial Grammar 2.2 Abstract Meaning Representation The Abstract Meaning Representation (AMR) is a semantic meaning representation language that is purposefully syntax-agnostic (Banarescu et al., 2013). The data set used in this paper, the AMR 1.0 release (Knight et al., 2014), consists of English sentences which have been directly annotated with meaning representations by human annotators. AMR represents meaning as labeled, directed graphs. Nodes are labeled with concepts, while edges represent roles. Predicates and core roles are drawn from PropBank (Kingsbury and Palmer, 2002). In addition, a set of non-core roles has been defined, such as mod, poss, time, etc. Whether it was wise to define AMR independently of any derivational process has been debated (Bender et al., 2015), and in Section 5 we will show some of the issues that arise when attempting to construct derivations for AMRs. Background The task of semantic parsing is concerned with building formal meaning representations for natural language text. While meaning representations can be elements of any formal language, in this paper we are concerned with Abstract Meaning Representations (AMRs). We use Combina"
S18-2006,K15-1004,0,0.0497791,"differences of representation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonic construction process. To our knowledge, this paper provides the first analysis of these corpus issues. 1 1.1 Introduction Related Work There is an extensive body of work on semantic parsing of AMRs, using a range of techniques including maximum spanning tree-based parsing (Flanigan et al., 2014), transition-based parsing (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018), machine translation (van Noord and Bos, 2017), graph grammars (Peng et al., 2015; Groschwitz et al., 2017), and CCG parsing (Artzi et al., 2015; Misra and Artzi, 2016). The system of Artzi et al. (2015) is most similar to the present work. They induce a semantic CCG using a translation of AMR into λ-calculus. A key difference is that their training algorithm combines lexicon induction and parameter training into a single phase. New lexical items are generated during each training step and then filtered based upon the model’s current parameters. In contrast, in this work we focus on lexicon inWith the release of the Abstract Meaning Representation (AMR) corpus (Knight et a"
S18-2006,D14-1048,0,0.393706,"64 c New Orleans, June 5-6, 2018. 2018 Association for Computational Linguistics 2.1 duction, with parameter training to be performed in a subsequent step. Another related system is presented in Lewis et al. (2015), where a CCG parser is adapted to produce shallow semantic dependency graphs. In contrast, the meaning representations employed here are abstract and do not directly refer to the sentence under analysis. Word-to-node alignments on the AMR corpus play an important role in this work. We experiment with JAMR’s rule-based aligner (Flanigan et al., 2014) and the statistical ISI aligner (Pourdamghani et al., 2014). Graph algebras have recently been applied to AMR parsing (Koller, 2015; Groschwitz et al., 2017), but not in combination with CCG. In contrast, we use syntactic CCG derivations to constrain the space of possible derivations. However, the idea of using a constrained version of the HR algebra, introduced by Groschwitz et al. (2017), is also used here, and our Application operator effectively subsumes their Apply and Modify operations. 1.2 CCG is a grammar formalism centered around a transparent syntax-semantics interface (Steedman, 2000). A CCG consists of a small set of combinatory rules, alo"
S18-2006,N15-1040,0,0.0185628,"o identify several phenomena that affect any approach relying either on CCG or graph algebraic approaches for AMR parsing. This includes differences of representation between CCG and AMR, as well as non-compositional constructions that are not expressible through a monotonic construction process. To our knowledge, this paper provides the first analysis of these corpus issues. 1 1.1 Introduction Related Work There is an extensive body of work on semantic parsing of AMRs, using a range of techniques including maximum spanning tree-based parsing (Flanigan et al., 2014), transition-based parsing (Wang et al., 2015; Ballesteros and Al-Onaizan, 2017; Peng et al., 2018), machine translation (van Noord and Bos, 2017), graph grammars (Peng et al., 2015; Groschwitz et al., 2017), and CCG parsing (Artzi et al., 2015; Misra and Artzi, 2016). The system of Artzi et al. (2015) is most similar to the present work. They induce a semantic CCG using a translation of AMR into λ-calculus. A key difference is that their training algorithm combines lexicon induction and parameter training into a single phase. New lexical items are generated during each training step and then filtered based upon the model’s current param"
W06-2305,P03-1013,0,0.0527648,"Missing"
W06-2305,2006.amta-panels.1,0,0.0616174,"Missing"
W06-2305,A00-2018,0,0.155684,"Missing"
W06-2305,daum-etal-2004-automatic,1,0.876778,"Missing"
W06-2305,C94-2149,0,0.0570013,"ty of different text genres (Foth et al., 2005). By combining hand-written rules with an optimization procedure for hypothesis selection, such a parser makes it possible to successively exclude certain rare phenomena from the coverage of the grammar and to study the impact of these modifications on its output quality 1 Introduction Traditionally, broad coverage has always been considered to be a desirable property of a grammar: the more linguistic phenomena are treated properly by the grammar, the better results can be expected when applying it to unrestricted text (c.f. (Grover et al., 1993; Doran et al., 1994)). With the advent of empirical methods and the corresponding evaluation metrics, however, this view changed considerably. (Abney, 1996) was among the first who noted that the relationship between coverage and statistical parsing quality is a more complex one. Adding new rules to the grammar, i.e. increasing its coverage, does not only allow the parser to deal with more phenomena, hence more sentences; at the same time it opens up new possibilities for abusing the newly introduced rules to mis-analyse constructions which were already treated properly before. As a consequence, a net reduction i"
W09-3816,D08-1017,0,0.0122315,"If at least one edgecrossing is correctly identified in a non-projective sentence, it is added to the correctly identified 105 classifiers into the parsing model of the other one at training time. Since the two parsers are based on quite different model types (namely a historybased vs. a structure-based one), they exhibit a remarkable complementary behavior (McDonald and Nivre, 2007). Accordingly, significant mutual benefits have been observed. Note, however, that one of the major benefits of MaltParser, its incremental left-to-right processing, is sacrificed under such a combination scheme. Martins et al. (2008) use stacked learning to overcome the restriction to the single-edge features in both MaltParser and MSTParser. They suggest an architecture with two layers, where the output of a standard parser in the first level provides new features for a parser in the subsequent level. During the training phase, the second parser learns to correct mistakes made by the first one. It allows to involve higher-order predicted edges to simulate non-local features in the second parser. The results are competitive with McDonald and Nivre (2007) while O(n2 ) runtime of the spanning tree algorithm is preserved. si"
W09-3816,P90-1005,0,0.0163547,"t of all the weights for constraint violations occurring in the structure. It becomes possible to differentiate between the quality of different parse results: the analysis with a higher score is considered preferable. Although, under these conditions, an analysis having only a few grave conflicts may be preferred by the system against another one with a great number of smaller constraint violations, but it ensures that an analysis which violates any of the hard constraints always receives the lowest possible score. WCDG The formalism of a Constraint Dependency Grammar was first introduced by Maruyama (1990) and suggests modeling natural language with the help of constraints. Schr¨oder (2002) has extended the approach to Weighted Constraint Dependency Grammar, WCDG, where weights are used to further disambiguate between competing structural alternatives. A WCDG models natural language as labeled dependency trees and is entirely declarative. It has no derivation rules — instead, constraints license well-formed tree structures. The reference implementation of WCDG for the German language used for the experiments described below contains about 1, 000 manually compiled constraints.1 Every constraint"
W09-3816,D07-1013,0,0.0408289,"Missing"
W09-3816,H05-1066,0,0.198817,"Missing"
W09-3816,W06-2932,0,0.241917,"ence, and the correct parse can be obtained by searching the space of valid dependency graphs for a tree with a maximum score. This formalism allows to find efficient solutions for both projective and non-projective trees. When only features over single edges are taken into account, the complexity falls to O(n2 ) (McDonald et al., 2005). Not only a single edge, but also adjacent edges may be included into the scoring function. As a result, intractability problems arise for the nonprojective algorithm, but an efficient approximate algorithm based on exhaustive search is provided for this case (McDonald et al., 2006). This algo2 MSTParser is freely available from http:// sourceforge.net/projects/mstparser 101 verbs and subclauses). Unfortunately, these manually corrected data were only available for a small part (3, 000 sentences) of the NEGRA corpus, which is not sufficient for training MSTParser on WCDGconforming tree structures. Previous evaluations of the MSTParser have used much larger training sets. E.g., during the CoNLL-X shared task 39,216 sentences from the TIGER Treebank (Brants et al., 2002) were used. Therefore, we used 20, 000 sentences from the online archive of www.heise.de as an alternati"
W09-3816,P08-1108,0,0.0143662,"sed as a diagnostic tool for the errors of MSTParser. Possibly, a higher degree of synergy could be In two separate experiments, Sagae and Lavie (2006) combined a number of dependency and constituent parsers, respectively. They created a new weighted search space from the results of the individual component parsers using different weighting schemes for the candidates. They then reparsed this search space and found a consistent improvement for the dependency structures, but not for the constituent-based ones. While all these approaches attempt to integrate the available evidence at parse time, Nivre and McDonald (2008) pursued an alternative architecture, where integration is achieved already at training time. They combined the two state-of-theart data-driven dependency parsers, MaltParser (Nivre et al., 2006) and MSTParser (McDonald et al., 2006), by integrating the features of each of the 106 achieved if a stronger coupling of the components were established by also using the scores of MSTParser as additional information for WCDG, reflecting the intuitive notion of preference or plausibility of the predictions. This could be done for the optimal parse tree alone as well as for the complete hypothesis spac"
W09-3816,W06-2933,0,0.0372973,"Missing"
W09-3816,W06-2920,0,0.0222635,"the correctly attached words which also have the correct label. Still, it is difficult to directly compare the results reported for different parsers, as the evaluation results are influenced by the data used during the experiment, the domain of the data, and different annotation guidelines. Moreover, the particular kind of POS information might be relevant, which either can be obtained from the manual annotations or be provided by a real tagger. Even such a condition as the treatment of punctuation has not yet become a standard. Following the evaluation procedure in the CoNLL-X shared task (Buchholz and Marsi, 2006), we will not include punctuation into the performance measures, as was done in previous WCDG experiments (Foth and Menzel, 2006). The source of POS tagging information will need to be specified in each individual case. All the evaluations were performed on a thousand sentences (18, 602 – 19, 601) from the NEGRA treebank, the same data set that was previously used in the performance evaluations of WCDG, e.g. in (Foth, 2006). The NEGRA treebank is a collection of newspaper articles; in the original, it stores phrase structure annotations. These have been automatically translated into dependency"
W09-3816,P06-1041,1,0.875036,"knowledge from human experts are usually considered as two alternative roadmaps towards natural language processing solutions. Since the resulting components exhibit quite different performance characteristics with respect to coverage, robustness and output quality, they might be able to provide some kind of complementary information, which could even lead to a notable degree of synergy between them when combined within a single system solution. For the task of dependency parsing, the high potential for such a synergy has indeed been ˇ demonstrated already (e.g. Zeman and Zabokrtsk´ y (2005), Foth and Menzel (2006)). 99 Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 99–107, c Paris, October 2009. 2009 Association for Computational Linguistics very same kind of target structure as the master, and (2) the quality levels of each of the components in isolation are considered. As a helper component MSTParser (McDonald, 2006), a state-of-the-art dependency parser for non-projective structures based on a discriminative learning paradigm, is considered. The accuracy of MSTParser differs insignificatly from that of WCDG with all the previously used helper components active"
W09-3816,2000.iwpt-1.11,1,0.944162,"Missing"
W09-3816,W05-1518,0,0.0439729,"Missing"
W09-3816,W99-0623,0,0.100806,"Missing"
W09-3816,N06-2033,0,\N,Missing
W11-4605,W06-2920,0,0.0718543,"Missing"
W11-4605,gimenez-marquez-2004-svmtool,0,0.0638877,"Missing"
W11-4605,P07-2053,0,0.0315797,"Missing"
W11-4605,W04-0308,0,0.0265389,"asure the impact of different kinds of incremental POS tagging on the performance of the parser. Dependency parsing lends itself particularly well to such an investigation, because by assigning attachments to input words it shares crucial similarities to POS-tagging. Parsing differs from tagging however, since it considers relations instead of atomic labels. Therefore, the output of dependency arcs needs to be either delayed at least until both ends of the dependency are known or otherwise underspecified dependency arcs need to be produced. The former strategy is applied, e.g., in MaltParser (Nivre, 2004) where words are kept on the stack until a possible attachment becomes available, adding a dynamic delay in addition to the one already caused by lookahead. The incremental variant of the WCDG parser (Foth, 2006) used in Menzel (2009) instead applies the latter approach and provides a placeholder for future input words so that partially specified dependencies can be generated. MaltParser classifies as a monotonic but delayed incremental parser, while output of WCDG is timely but non-monotonic. To deal with and compare dynamic delay and underspecified dependencies the evaluation methods present"
W11-4605,W09-3905,0,0.222164,"ned lookahead size is used here. The final accuracy is determined for different lookahead sizes. The degree of non-monotonicity in a RA strategy is not as easy to define or guarantee. One could restrict the possibilities for reanalysis to a window of a certain size or constrain the kind of changes that are allowed. Another possibility consists in restricting reanalysis beforehand but to determine empirically how often a process actually does change its output. This approach was used by Baumann et al. (2009) for incremental speech 28 Decision Strategies for Incremental POS Tagging recognition. Schlangen et al. (2009) applied it to incremental reference resolution where the so called edit overhead was measured. We also use this latter approach by determining the percentage of output tokens which will not be be changed in further processing steps as a stability measure. To be able to decide on an acceptable trade-off between delay, accuracy and non-monotonicity we will plot stability and accuracy for different delays. To quantify inconclusiveness we use the number of output alternatives to be considered and measure accuracy for different numbers. This requires the number of alternatives to be configurable o"
W11-4605,N09-1043,0,0.131068,"suring the completeness of the output. As we only regard POS tagging strategies with a fixed delay, a predefined lookahead size is used here. The final accuracy is determined for different lookahead sizes. The degree of non-monotonicity in a RA strategy is not as easy to define or guarantee. One could restrict the possibilities for reanalysis to a window of a certain size or constrain the kind of changes that are allowed. Another possibility consists in restricting reanalysis beforehand but to determine empirically how often a process actually does change its output. This approach was used by Baumann et al. (2009) for incremental speech 28 Decision Strategies for Incremental POS Tagging recognition. Schlangen et al. (2009) applied it to incremental reference resolution where the so called edit overhead was measured. We also use this latter approach by determining the percentage of output tokens which will not be be changed in further processing steps as a stability measure. To be able to decide on an acceptable trade-off between delay, accuracy and non-monotonicity we will plot stability and accuracy for different delays. To quantify inconclusiveness we use the number of output alternatives to be consi"
W11-4605,A00-1031,0,0.118917,"a non-monotonic behavior every time a new word changes the optimal path for the previous words. Care has to be taken not to introduce errors by handling prefixes as full sentences. An example for this are end of sentence tags sometimes automatically inserted by a tagger at the end of the current tag sequence. They are likely to influence the best path and thus should not be used in prefixes. 4.1 POS taggers In order to compare the different strategies of incremental processing in the task of POS tagging we will compare different taggers in different configurations. The taggers used are TnT2 (Brants, 2000), SVMTool3 (Gim´enez and M`arquez, 2004) and HunPos (Hal´acsy et al., 2007) modified to work incrementally4 . TnT is a statistical POS tagger implementing the Viterbi algorithm for second order Markov models. As such it does not support incremental processing, but can be made to simulate a nonmonotonic incremental mode by tagging successively extended prefixes of the input sequence, thereby providing an incremental interface.5 To force TnT into a monotonic mode, only the tag for the new word in each prefix is added, the other tags are taken from the tagging results for the previous prefix. Thi"
W14-2403,basile-etal-2012-developing,0,0.0326692,"re the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses 12 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12–16, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics fore relate to the implementation of the SEMSPLIT function. ∃x Algorithm 1 A general splitting algorithm. C is the set of binary CCG combinators. The SEM SPLIT function returns possible splits of a meaning representation according to the reverse applicat"
W14-2403,P13-1042,0,0.0227007,"ammar (CCG) forms the basis of many current approaches to semantic parsing. It is attractive for semantic parsing due to its unified treatment of syntax and semantics, where the construction of the meaning representation directly follows the syntactic analysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses 12 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12–16, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics fore relate to the implementation of the SEMSPLIT function. ∃x Algorithm 1 A genera"
W14-2403,N04-1035,0,0.0606864,"rictive, as Figure 2 illustrates. Instead, we say that we decompose z into a hierarchy of components, with a split node at the root of each component. These components are labelled as f - and g-components in an alternating fashion. In this hierarchy, the members of an f component are not allowed to have alignments to words in l. A corresponding requirement holds for The first heuristic we introduce is borrowed from the field of statistical machine translation. There, alignments between words of two languages are used to identify corresponding phrase pairs, as in the well-known GHKM algorithm (Galley et al., 2004). In order to apply the same strategy to meaning representations, we represent them as their abstract syntax trees. Following Li et al. (2013), we can then align words in the sentence and nodes in the meaning representation to identify components that correspond to each other. This allows us to impose an extra constraint on the generation of splits: We require that nodes in f not be aligned to any words in the right sentencehalf r, and conversely, that nodes in g not be aligned to words in l. Alignment consistency helps the search to focus on more plausible splits by grouping elements of the m"
W14-2403,D10-1119,0,0.0329042,"lysis (Steedman, 2001). However, the supervised induction of semantic CCGs—the inference of a CCG from a corpus of sentence-meaning pairs—has so far only been partially solved. While approaches are available that work on small corpora focused on specific domains (such as Geoquery and Freebase QA for question answering (Zelle and Mooney, 1996; Cai and Yates, 2013)), we are not aware of any approach that allows the extraction of a semantic CCG from a wide-coverage corpus such as the Groningen Meaning Bank (GMB) (Basile et al., 2012). This work attempts to fill this gap. Analogous to the work of Kwiatkowski et al. (2010), we view grammar induction as a series of splitting steps, each of which essentially reverses 12 Proceedings of the ACL 2014 Workshop on Semantic Parsing, pages 12–16, c Baltimore, Maryland USA, June 26 2014. 2014 Association for Computational Linguistics fore relate to the implementation of the SEMSPLIT function. ∃x Algorithm 1 A general splitting algorithm. C is the set of binary CCG combinators. The SEM SPLIT function returns possible splits of a meaning representation according to the reverse application of a combinator. function SPLIT(x, z) if |x |= 1 then return {(x, z)} else G←∅ for 0"
W16-2331,D07-1036,0,0.0701866,"Missing"
W16-2331,2011.iwslt-papers.5,0,0.0167267,"ocuments. L¨u et al. (2007) also uses the cosine to select sentences for offline and online training data optimization. Tamchyna et al. (2012) presents a method where sentences are extracted from the general domain by translating the source side of a test set and using it in computing the cosine similarity to the general domain. In Mandal et al. (2008) and in Axelrod et al. (2011) language model perplexity was used to score sentences. Foster et al. (2010) used phrase pairs instead of sentences and learned weights for them using in-domain features based on word frequencies and perplexities. In Mansour et al. (2011), the cross-entropy score is used for language model filtering together with a translation model score that estimates the likelihood that a source and a target sentence are a translation of each other. Toral et al. (2015) introduced linguistic information such as lemmas, named entities and part-of-speech tags into the preprocessing of the data and then ranked the sentences by perplexity. The edit distance which computes the minimum number of edits needed to transform a sentence from the general domain into a sentence from the 1 Paragraph vector 2 Term frequency - Inverse document frequency 429"
W16-2331,P13-1126,0,0.0367802,"Missing"
W16-2331,P03-1021,0,0.00979801,"s the minimum number of edits needed to transform a sentence from the general domain into a sentence from the 1 Paragraph vector 2 Term frequency - Inverse document frequency 429 https://radimrehurek.com/gensim/ 4 Experiments 5-gram LMs using the SRILM toolkit (Stolcke, 2002) with Kneser-Ney discounting (Kneser and Ney, 1995) on the target side of the Commoncrawl and IT corpora. When LM interpolation was needed, the in-domain LM and the general domain LM were interpolated using weights tuned to minimize the perplexity on the tuning set. The same data was used for tuning the systems with MERT (Och, 2003). For the BLEU-cased scores training recasing was performed using the default configuration from the EMS script: language model trained using KenLM (Heafield, 2011) and order 3. Due to time limitations, we did not try to further improve the recaser model. For all the submitted systems, we used only the data distributed for the shared IT task. For the general domain training data we chose Commoncrawl3 (made available by WMT) because it is a relatively large corpus and contains crawled data from a variety of domains including the IT domain. As in-domain training data we concatenated the corpora"
W16-2331,J82-2005,0,0.727118,"Missing"
W16-2331,J03-1002,0,0.00666927,"that, punctuation was normalized using the normalize-punctuation.perl script. Approximately 25K sentences were removed because they were not considered EnglishGerman sentence pairs by the jlangdetect library4 and further 650 sentences have been discharged because they contained non-alpha characters. Table 1 presents some data statistics for both domains after preprocessing: Corpora Sentences Commoncrawl IT 2.34M 210K 4.4 Tokens English German 50.33M 46.11M 1.48M 1.44M Table 1: Corpora statistics after preprocessing 4.2 Baselines Experimental settings We performed word alignment using GIZA++ (Och and Ney, 2003) with the default grow-diagfinal-and alignment symmetrization method. For the language model (LM) estimation we trained models/doc2vec.html 3 http://commoncrawl.org/ 4 https://github.com/melix/jlangdetect 430 Data selection using Doc2vec In this section the submitted system U HDS doc2vec is described. The filtering procedure receives as input the bilingual indomain corpus In, the bilingual general domain Gen, the number of most similar sentences N that should be retrieved given a threshold δ that will be described later. Our approach is monolingual as we used only the source side of the corpus"
W16-2331,W11-2123,0,0.0472881,"ument frequency 429 https://radimrehurek.com/gensim/ 4 Experiments 5-gram LMs using the SRILM toolkit (Stolcke, 2002) with Kneser-Ney discounting (Kneser and Ney, 1995) on the target side of the Commoncrawl and IT corpora. When LM interpolation was needed, the in-domain LM and the general domain LM were interpolated using weights tuned to minimize the perplexity on the tuning set. The same data was used for tuning the systems with MERT (Och, 2003). For the BLEU-cased scores training recasing was performed using the default configuration from the EMS script: language model trained using KenLM (Heafield, 2011) and order 3. Due to time limitations, we did not try to further improve the recaser model. For all the submitted systems, we used only the data distributed for the shared IT task. For the general domain training data we chose Commoncrawl3 (made available by WMT) because it is a relatively large corpus and contains crawled data from a variety of domains including the IT domain. As in-domain training data we concatenated the corpora provided by the task. We tuned the systems with 2000 sentences from Batch1a and Batch2a provided by the shared task and evaluated them on Batch3a. Our systems have"
W16-2331,P02-1040,0,0.0939965,"Missing"
W16-2331,2006.amta-papers.25,0,0.0851032,"Missing"
W16-2331,R13-1094,0,0.470006,"es a set of candidate sentences that are most similar to the reference. If at least one of the re2 Related work A range of different methods for domain adaptation of models for statistical machine translation have been developed including mixture modeling, instance weighting, transductive learning, or data selection (Chen et al., 2013). The data selection approach is the focus of this paper. In the state of the art, data selection is used at the corpus-level, where the selected data is joined together, or at the model-level, where several models are combined together in the translation phase (Wang et al., 2013a). The main workflow of the data selection method consists of the 428 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 428–434, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics following steps: in-domain was used in Wang et al. (2013b). A combination of the three data selection approaches is presented in Wang et al. (2013a, 2013c). We propose a new approach of filtering general domain sentences using paragraph vectors (Le and Mikolov, 2014) to determine sentence similarity in a high-dimensional vector space. T"
W16-2331,W12-3148,0,\N,Missing
W16-2331,D11-1033,0,\N,Missing
W16-2331,P07-2045,0,\N,Missing
W16-2331,2005.eamt-1.19,0,\N,Missing
W17-4754,W16-2331,1,0.762381,"at scores general domain sentences according to their relevance to the in-domain and experimenting with various ratios of top ranked sentences in order to obtain the best result in terms of one or more MT evaluation metrics. The approaches most commonly adopted in the literature are based on information retrieval (Hildebrand et al. (2005); Tamchyna et al. (2012)), on perplexity (Moore and Lewis (2010); Axelrod et al. (2011)), or on edit distance similarity (Wang et al., 2013). Recently, a new direction has gained interest by making use of Word or Paragraph Vectors (embeddings). Chen and Huang (2016) use word embeddings along with in-domain selected sentences as positive samples and randomly selected sentences from the general domain as negative samples in training convolutional networks that yield good results. Also, Duma and Menzel (2016) developed a new scoring method using Paragraph Vectors with positive results. In this paper, we apply Paragraph Vectors for training FFNN classifiers that categorize the general domain sentences as being in-domain or outof-domain. One of the most challenging tasks in data selection consists in finding the optimal threshold (how many of the scored sente"
W17-4754,2005.eamt-1.19,0,0.0236958,"pool of general domain data and a small amount of in-domain data, selecting the sentences from the general domain that are most similar to the in-domain is referred in literature as data selection. The work-flow of performing data selection includes developing a metric or function that scores general domain sentences according to their relevance to the in-domain and experimenting with various ratios of top ranked sentences in order to obtain the best result in terms of one or more MT evaluation metrics. The approaches most commonly adopted in the literature are based on information retrieval (Hildebrand et al. (2005); Tamchyna et al. (2012)), on perplexity (Moore and Lewis (2010); Axelrod et al. (2011)), or on edit distance similarity (Wang et al., 2013). Recently, a new direction has gained interest by making use of Word or Paragraph Vectors (embeddings). Chen and Huang (2016) use word embeddings along with in-domain selected sentences as positive samples and randomly selected sentences from the general domain as negative samples in training convolutional networks that yield good results. Also, Duma and Menzel (2016) developed a new scoring method using Paragraph Vectors with positive results. In this pa"
W17-4754,D11-1033,0,0.0171281,"from the general domain that are most similar to the in-domain is referred in literature as data selection. The work-flow of performing data selection includes developing a metric or function that scores general domain sentences according to their relevance to the in-domain and experimenting with various ratios of top ranked sentences in order to obtain the best result in terms of one or more MT evaluation metrics. The approaches most commonly adopted in the literature are based on information retrieval (Hildebrand et al. (2005); Tamchyna et al. (2012)), on perplexity (Moore and Lewis (2010); Axelrod et al. (2011)), or on edit distance similarity (Wang et al., 2013). Recently, a new direction has gained interest by making use of Word or Paragraph Vectors (embeddings). Chen and Huang (2016) use word embeddings along with in-domain selected sentences as positive samples and randomly selected sentences from the general domain as negative samples in training convolutional networks that yield good results. Also, Duma and Menzel (2016) developed a new scoring method using Paragraph Vectors with positive results. In this paper, we apply Paragraph Vectors for training FFNN classifiers that categorize the gener"
W17-4754,W11-2131,0,0.0411045,"Missing"
W17-4754,D14-1014,0,0.0356144,"Missing"
W17-4754,P07-2045,0,0.031066,"016) developed a new scoring method using Paragraph Vectors with positive results. In this paper, we apply Paragraph Vectors for training FFNN classifiers that categorize the general domain sentences as being in-domain or outof-domain. One of the most challenging tasks in data selection consists in finding the optimal threshold (how many of the scored sentences to select). It is a time-consuming process in which several experiments need to be performed, usually aiming to obtain the best BLEU score. Moreover, Data and tools All SMT models were developed using the Moses phrase-based MT toolkit (Koehn et al., 2007) and the Experiment Management System (Koehn, 2010). The preprocessing of the data consisted in tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were provided by WMT 2016 (Bojar et al., 2016) and WMT 2017. The SRILM toolkit (Stolcke, 2002) and KneserNey discounting (Kneser and Ney, 1995) were used to estimate 5-gram language models (LM). All the trained SMT systems use a strong LM built by interpolating a LM for the in-domain and a LM for the general domain with weights that are tuned to minimize the perplexity on the tuning set (Schwenk and"
W17-4754,2010.amta-tutorials.4,0,0.0404533,"Missing"
W17-4754,I08-2089,0,0.0132865,"t al., 2007) and the Experiment Management System (Koehn, 2010). The preprocessing of the data consisted in tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were provided by WMT 2016 (Bojar et al., 2016) and WMT 2017. The SRILM toolkit (Stolcke, 2002) and KneserNey discounting (Kneser and Ney, 1995) were used to estimate 5-gram language models (LM). All the trained SMT systems use a strong LM built by interpolating a LM for the in-domain and a LM for the general domain with weights that are tuned to minimize the perplexity on the tuning set (Schwenk and Koehn, 2008). For word alignment we used GIZA++ (Och and Ney, 2003) with the default grow-diag-final-and alignment symmetrization method. Tuning of the SMT systems was performed with MERT (Och, 2003). Commoncrawl and Wikipedia were used as general domains for all language pairs except for EN↔PT where no Commoncrawl data was provided by WMT. As for the in-domain corpora, EMEA (Tiedemann, 2012) was used for all language pairs and Muchmore, ECDC, Pattr and Pubmed (all from UFAL Medical Corpus2 ) for those language pairs where data was available. We also made use of the training data provided by the previous"
W17-4754,P10-2041,0,0.0199576,"selecting the sentences from the general domain that are most similar to the in-domain is referred in literature as data selection. The work-flow of performing data selection includes developing a metric or function that scores general domain sentences according to their relevance to the in-domain and experimenting with various ratios of top ranked sentences in order to obtain the best result in terms of one or more MT evaluation metrics. The approaches most commonly adopted in the literature are based on information retrieval (Hildebrand et al. (2005); Tamchyna et al. (2012)), on perplexity (Moore and Lewis (2010); Axelrod et al. (2011)), or on edit distance similarity (Wang et al., 2013). Recently, a new direction has gained interest by making use of Word or Paragraph Vectors (embeddings). Chen and Huang (2016) use word embeddings along with in-domain selected sentences as positive samples and randomly selected sentences from the general domain as negative samples in training convolutional networks that yield good results. Also, Duma and Menzel (2016) developed a new scoring method using Paragraph Vectors with positive results. In this paper, we apply Paragraph Vectors for training FFNN classifiers th"
W17-4754,P03-1021,0,0.0087565,"st sets were provided by WMT 2016 (Bojar et al., 2016) and WMT 2017. The SRILM toolkit (Stolcke, 2002) and KneserNey discounting (Kneser and Ney, 1995) were used to estimate 5-gram language models (LM). All the trained SMT systems use a strong LM built by interpolating a LM for the in-domain and a LM for the general domain with weights that are tuned to minimize the perplexity on the tuning set (Schwenk and Koehn, 2008). For word alignment we used GIZA++ (Och and Ney, 2003) with the default grow-diag-final-and alignment symmetrization method. Tuning of the SMT systems was performed with MERT (Och, 2003). Commoncrawl and Wikipedia were used as general domains for all language pairs except for EN↔PT where no Commoncrawl data was provided by WMT. As for the in-domain corpora, EMEA (Tiedemann, 2012) was used for all language pairs and Muchmore, ECDC, Pattr and Pubmed (all from UFAL Medical Corpus2 ) for those language pairs where data was available. We also made use of the training data provided by the previous Biomedical task from 2016. The corpora corresponding to the general domain was concatenated into a single data source and the same procedure was applied for the in-domain corpora. The 2 4"
W17-4754,tiedemann-2012-parallel,0,0.0125625,"models (LM). All the trained SMT systems use a strong LM built by interpolating a LM for the in-domain and a LM for the general domain with weights that are tuned to minimize the perplexity on the tuning set (Schwenk and Koehn, 2008). For word alignment we used GIZA++ (Och and Ney, 2003) with the default grow-diag-final-and alignment symmetrization method. Tuning of the SMT systems was performed with MERT (Och, 2003). Commoncrawl and Wikipedia were used as general domains for all language pairs except for EN↔PT where no Commoncrawl data was provided by WMT. As for the in-domain corpora, EMEA (Tiedemann, 2012) was used for all language pairs and Muchmore, ECDC, Pattr and Pubmed (all from UFAL Medical Corpus2 ) for those language pairs where data was available. We also made use of the training data provided by the previous Biomedical task from 2016. The corpora corresponding to the general domain was concatenated into a single data source and the same procedure was applied for the in-domain corpora. The 2 484 http://ufal.mff.cuni.cz/ufal medical corpus Feed-forward Neural Network Classifier size of the corpora is presented in the following table (since the bilingual corpora remain the same for both"
W17-4754,R13-1094,0,0.0145551,"domain is referred in literature as data selection. The work-flow of performing data selection includes developing a metric or function that scores general domain sentences according to their relevance to the in-domain and experimenting with various ratios of top ranked sentences in order to obtain the best result in terms of one or more MT evaluation metrics. The approaches most commonly adopted in the literature are based on information retrieval (Hildebrand et al. (2005); Tamchyna et al. (2012)), on perplexity (Moore and Lewis (2010); Axelrod et al. (2011)), or on edit distance similarity (Wang et al., 2013). Recently, a new direction has gained interest by making use of Word or Paragraph Vectors (embeddings). Chen and Huang (2016) use word embeddings along with in-domain selected sentences as positive samples and randomly selected sentences from the general domain as negative samples in training convolutional networks that yield good results. Also, Duma and Menzel (2016) developed a new scoring method using Paragraph Vectors with positive results. In this paper, we apply Paragraph Vectors for training FFNN classifiers that categorize the general domain sentences as being in-domain or outof-domai"
W17-4754,J03-1002,0,0.00689356,"0). The preprocessing of the data consisted in tokenization, cleaning (6-80), lowercasing and normalizing punctuation. The tuning and the test sets were provided by WMT 2016 (Bojar et al., 2016) and WMT 2017. The SRILM toolkit (Stolcke, 2002) and KneserNey discounting (Kneser and Ney, 1995) were used to estimate 5-gram language models (LM). All the trained SMT systems use a strong LM built by interpolating a LM for the in-domain and a LM for the general domain with weights that are tuned to minimize the perplexity on the tuning set (Schwenk and Koehn, 2008). For word alignment we used GIZA++ (Och and Ney, 2003) with the default grow-diag-final-and alignment symmetrization method. Tuning of the SMT systems was performed with MERT (Och, 2003). Commoncrawl and Wikipedia were used as general domains for all language pairs except for EN↔PT where no Commoncrawl data was provided by WMT. As for the in-domain corpora, EMEA (Tiedemann, 2012) was used for all language pairs and Muchmore, ECDC, Pattr and Pubmed (all from UFAL Medical Corpus2 ) for those language pairs where data was available. We also made use of the training data provided by the previous Biomedical task from 2016. The corpora corresponding to"
W17-4754,P02-1040,0,0.108502,"rained classifiers on ≈200K sentences with an equal number of positive and negative samples. The positive samples were randomly selected from the in-domain data and the negative samples were randomly selected from the general domain data. EN-PT 1.6M 1.08M 613K 74K Table 1: Corpora used for ATD 3.2 Automatic Threshold Detection for Data Selection The data selection method we used for the WMT Biomedical task is described in this section with a special focus on Paragraph Vector and the FFNN classifier employed in developing the automatic threshold detection. 4 We report in this section the BLEU (Papineni et al., 2002) scores obtained by our submissions, as well as the classifiers accuracy. For each language pair and for each test set provided by the Biomedical task, we submitted three runs as follows: Paragraph Vector Sentences were represented using Paragraph Vectors (Le and Mikolov, 2014) which give a continuous distributed vector representation of the input. Paragraph Vector is an extension of word embeddings (Mikolov et al., 2013) to phrases or sentences. Given a sentence, Paragraph Vector learns its representation by mapping context words and a paragraph identifier to the word to be predicted. The par"
W17-4754,W16-2301,0,\N,Missing
W17-4754,K16-1031,0,\N,Missing
W17-4762,W12-3112,0,0.312158,"kernels is introduced in (Hardmeier, 2011), where a binary SVM classifier is trained to make predictions about the quality of the MT output. The datasets are syntactically analyzed using constituency and dependency parsers. The Subset Tree Kernel (Collins and Duffy, 2001) is used for the constituency trees, while the Partial Tree Kernel (Moschitti, 2006a) (Moschitti, 2006b) was judged as being more appropriate for the dependency trees. The evaluation shows that the combination between baseline features and the tree kernels achieves the best performance. These findings are further validated in Hardmeier et al. (2012) where a QE system is proposed based on a set of 82 explicit features combined with syntactic tree kernels. Syntactic tree kernels for QE are also explored in Kaljahi et al. (2014), where a set of hand crafted constituency and dependency based features together with subset tree kernels applied on the constituency and dependency tree representations are used. The evaluation results demonstrate that the source constituency trees perform better than the target sentence constituency trees. This work is further extended in Kaljahi (2015), where multiple QE systems based on syntactic and semantic fe"
W17-4762,C14-1194,0,0.290898,"Missing"
W17-4762,C12-1008,0,0.0303582,"ity: word, sentence or phrase and it involves classifying, ranking or predicting scores for the candidate translations. A sentence-level QE system is conventionally constructed based on a set of features encoding the information contained in the source and target sentences, which are used for learning a prediction model. The features employed for this task can be of different types, like surface features, language model features or linguistic features. The positive influence of syntactic features on the performance of QE systems has been extensively studied, including in Rubino et al. (2012), Avramidis (2012) or more recently in Kozlova et al. (2016). However, the process of identifying the best performing set of features, is a task that is both expensive and requires a considerable amount of engineering effort (Hardmeier, 2011). On the other hand, kernel methods do not require the explicit definition of the features, and rely on the scalar product between vectors for capturing the similarity shared by the sentence pairs. In this paper we present our submission to the WMT17 Shared Task on sentence level Quality Estimation, that makes use of sequence and tree kernels in predicting a continuous scor"
W17-4762,W16-3413,0,0.0301936,"on. On the other hand, the reference-free evaluation, also known as quality estimation (QE), predicts the quality of a candidate translation based solely on the information contained in the source and target sentences. QE can be performed at 556 Proceedings of the Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 556–561 c Copenhagen, Denmark, September 711, 2017. 2017 Association for Computational Linguistics use of a back-translation of the target sentence in the computation of the kernels. While backtranslations features have been previously utilized for QE (e.g.(Bechara et al., 2016)), their potential as an additional structural input representation for kernels has never been studied before. Furthermore, we exploit the potential of the scores of the kernel functions applied on the source and backtranslation sentences as additional hard-coded features. introduced feature vectors brings consistent improvement over the baseline system. This paper is organized as follows. The related work is presented in Section 2, while the methods employed and the implementation are described in Section 3. The experimental setup and the evaluation results are introduced in Section 4, while"
W17-4762,W16-2385,0,0.022043,"involves classifying, ranking or predicting scores for the candidate translations. A sentence-level QE system is conventionally constructed based on a set of features encoding the information contained in the source and target sentences, which are used for learning a prediction model. The features employed for this task can be of different types, like surface features, language model features or linguistic features. The positive influence of syntactic features on the performance of QE systems has been extensively studied, including in Rubino et al. (2012), Avramidis (2012) or more recently in Kozlova et al. (2016). However, the process of identifying the best performing set of features, is a task that is both expensive and requires a considerable amount of engineering effort (Hardmeier, 2011). On the other hand, kernel methods do not require the explicit definition of the features, and rely on the scalar product between vectors for capturing the similarity shared by the sentence pairs. In this paper we present our submission to the WMT17 Shared Task on sentence level Quality Estimation, that makes use of sequence and tree kernels in predicting a continuous score representing the post-editing effort for"
W17-4762,C10-1011,0,0.0660624,"on↑ 0.437 0.480 0.467 0.458 0.510 0.493 0.494 0.479 not exact MAE ↓ 0.141 0.138 0.142 0.136 0.132 0.137 0.136 0.141 Pearson↑ 0.443 0.484 0.479 0.465 0.537 0.503 0.503 0.489 MAE ↓ 0.139 0.139 0.140 0.135 0.130 0.137 0.135 0.140 Table 2: Evaluation results for the DE-EN test set. gramming approach to determine the number of common patterns between the two input sentences. In our experiments, the patterns taken into account were composed of the lexical items. In order to use the tree kernel functions, the source and the target sentences were parsed using the Bohnet graph-based dependency parser (Bohnet, 2010), which was chosen because of its high accuracy. The data was first preprocessed by performing lemmatization and pos-tagging. Publicly available 1 pre-trained models were used for analyzing the source, target and back-translation sentences. For learning using the Partial Tree Kernel, a transformation of the dependency parse tree is required, as introduced in Croce et al. (2011). We followed the lexical-centered-tree approach, where the grammatical relation and the pos-tag are encoded as the rightmost children of a dependency tree node. In the case of sequence kernels, the only preprocessing st"
W17-4762,E06-1015,0,0.889911,"tual Similarity (e.g. (Severyn et al., 2013)), Information Extraction (e.g. (Culotta and Sorensen, 2004)), Semantic Role Labeling (e.g. (Moschitti et al., 2008)) or Textual Entailment (e.g. (Wang and Neumann, 2007)). An approach for QE based on syntactic tree kernels is introduced in (Hardmeier, 2011), where a binary SVM classifier is trained to make predictions about the quality of the MT output. The datasets are syntactically analyzed using constituency and dependency parsers. The Subset Tree Kernel (Collins and Duffy, 2001) is used for the constituency trees, while the Partial Tree Kernel (Moschitti, 2006a) (Moschitti, 2006b) was judged as being more appropriate for the dependency trees. The evaluation shows that the combination between baseline features and the tree kernels achieves the best performance. These findings are further validated in Hardmeier et al. (2012) where a QE system is proposed based on a set of 82 explicit features combined with syntactic tree kernels. Syntactic tree kernels for QE are also explored in Kaljahi et al. (2014), where a set of hand crafted constituency and dependency based features together with subset tree kernels applied on the constituency and dependency tr"
W17-4762,J08-2003,0,0.0372798,"s brings consistent improvement over the baseline system. This paper is organized as follows. The related work is presented in Section 2, while the methods employed and the implementation are described in Section 3. The experimental setup and the evaluation results are introduced in Section 4, while the last section summarizes our findings and presents future work ideas. 2 Related work 3 Kernel functions have been used in a variety of NLP tasks, including Textual Similarity (e.g. (Severyn et al., 2013)), Information Extraction (e.g. (Culotta and Sorensen, 2004)), Semantic Role Labeling (e.g. (Moschitti et al., 2008)) or Textual Entailment (e.g. (Wang and Neumann, 2007)). An approach for QE based on syntactic tree kernels is introduced in (Hardmeier, 2011), where a binary SVM classifier is trained to make predictions about the quality of the MT output. The datasets are syntactically analyzed using constituency and dependency parsers. The Subset Tree Kernel (Collins and Duffy, 2001) is used for the constituency trees, while the Partial Tree Kernel (Moschitti, 2006a) (Moschitti, 2006b) was judged as being more appropriate for the dependency trees. The evaluation shows that the combination between baseline f"
W17-4762,D11-1096,0,0.239751,"n our experiments, the patterns taken into account were composed of the lexical items. In order to use the tree kernel functions, the source and the target sentences were parsed using the Bohnet graph-based dependency parser (Bohnet, 2010), which was chosen because of its high accuracy. The data was first preprocessed by performing lemmatization and pos-tagging. Publicly available 1 pre-trained models were used for analyzing the source, target and back-translation sentences. For learning using the Partial Tree Kernel, a transformation of the dependency parse tree is required, as introduced in Croce et al. (2011). We followed the lexical-centered-tree approach, where the grammatical relation and the pos-tag are encoded as the rightmost children of a dependency tree node. In the case of sequence kernels, the only preprocessing step applied was the tokenization of the input sentences. In order to investigate if prior lemmatization of the input sentences influences the results, we created two variants for each structural representation: an exact one containing the actual lexical items and a simplified non-exact one consisting of their corresponding lemmas. Furthermore, we incorporated a backtranslation o"
W17-4762,D09-1143,0,0.0321518,"resented. First, tree and sequence kernels will be introduced, followed by the description of the implementation of these kernels in the context of QE. Finally, the machine learning platform used for implementing the QE systems will be presented. 3.1 Kernels for Quality Estimation A kernel function computes the similarity between two structural representations without requiring the identification of the entire feature space (Moschitti, 2006a). To achieve this, the scalar product between vectors of substructure counts is computed in a vector space with a possibly infinite number of dimensions (Nguyen et al., 2009). Different kernel functions, depending on the type of structural input data they require, have been proposed including sequence, tree or graphs kernels. Tree kernels make use of tree representations for their computation, while sequence kernels calculate the similarity between the input sequence representations based on the number of common subsequences they share. In the case of tree kernels, a series of algorithms have been proposed, e.g. in Collins and Duffy (2001) or Moschitti (2006a), based on the type of tree fragments (e.g. subsets, subtrees or partial trees) they take into considerati"
W17-4762,P04-1054,0,0.109825,"as additional hard-coded features. introduced feature vectors brings consistent improvement over the baseline system. This paper is organized as follows. The related work is presented in Section 2, while the methods employed and the implementation are described in Section 3. The experimental setup and the evaluation results are introduced in Section 4, while the last section summarizes our findings and presents future work ideas. 2 Related work 3 Kernel functions have been used in a variety of NLP tasks, including Textual Similarity (e.g. (Severyn et al., 2013)), Information Extraction (e.g. (Culotta and Sorensen, 2004)), Semantic Role Labeling (e.g. (Moschitti et al., 2008)) or Textual Entailment (e.g. (Wang and Neumann, 2007)). An approach for QE based on syntactic tree kernels is introduced in (Hardmeier, 2011), where a binary SVM classifier is trained to make predictions about the quality of the MT output. The datasets are syntactically analyzed using constituency and dependency parsers. The Subset Tree Kernel (Collins and Duffy, 2001) is used for the constituency trees, while the Partial Tree Kernel (Moschitti, 2006a) (Moschitti, 2006b) was judged as being more appropriate for the dependency trees. The"
W17-4762,W12-3117,0,0.0319643,"ent levels of granularity: word, sentence or phrase and it involves classifying, ranking or predicting scores for the candidate translations. A sentence-level QE system is conventionally constructed based on a set of features encoding the information contained in the source and target sentences, which are used for learning a prediction model. The features employed for this task can be of different types, like surface features, language model features or linguistic features. The positive influence of syntactic features on the performance of QE systems has been extensively studied, including in Rubino et al. (2012), Avramidis (2012) or more recently in Kozlova et al. (2016). However, the process of identifying the best performing set of features, is a task that is both expensive and requires a considerable amount of engineering effort (Hardmeier, 2011). On the other hand, kernel methods do not require the explicit definition of the features, and rely on the scalar product between vectors for capturing the similarity shared by the sentence pairs. In this paper we present our submission to the WMT17 Shared Task on sentence level Quality Estimation, that makes use of sequence and tree kernels in predicting"
W17-4762,S13-1006,0,0.0293189,"s applied on the source and backtranslation sentences as additional hard-coded features. introduced feature vectors brings consistent improvement over the baseline system. This paper is organized as follows. The related work is presented in Section 2, while the methods employed and the implementation are described in Section 3. The experimental setup and the evaluation results are introduced in Section 4, while the last section summarizes our findings and presents future work ideas. 2 Related work 3 Kernel functions have been used in a variety of NLP tasks, including Textual Similarity (e.g. (Severyn et al., 2013)), Information Extraction (e.g. (Culotta and Sorensen, 2004)), Semantic Role Labeling (e.g. (Moschitti et al., 2008)) or Textual Entailment (e.g. (Wang and Neumann, 2007)). An approach for QE based on syntactic tree kernels is introduced in (Hardmeier, 2011), where a binary SVM classifier is trained to make predictions about the quality of the MT output. The datasets are syntactically analyzed using constituency and dependency parsers. The Subset Tree Kernel (Collins and Duffy, 2001) is used for the constituency trees, while the Partial Tree Kernel (Moschitti, 2006a) (Moschitti, 2006b) was jud"
W17-4762,P15-4004,0,0.282738,"Missing"
W17-4762,2011.eamt-1.32,0,0.538341,"tion contained in the source and target sentences, which are used for learning a prediction model. The features employed for this task can be of different types, like surface features, language model features or linguistic features. The positive influence of syntactic features on the performance of QE systems has been extensively studied, including in Rubino et al. (2012), Avramidis (2012) or more recently in Kozlova et al. (2016). However, the process of identifying the best performing set of features, is a task that is both expensive and requires a considerable amount of engineering effort (Hardmeier, 2011). On the other hand, kernel methods do not require the explicit definition of the features, and rely on the scalar product between vectors for capturing the similarity shared by the sentence pairs. In this paper we present our submission to the WMT17 Shared Task on sentence level Quality Estimation, that makes use of sequence and tree kernels in predicting a continuous score representing the post-editing effort for the target sentence. The novel contribution of our system is the combination of different types of kernels. Moreover, we use a back-translation of the target sentence into the sourc"
W17-4766,W13-2205,0,0.0594642,"Missing"
W17-4766,W13-3814,0,0.0241752,"entifies the type of kernel utilized (SK or PTK) and level refers to the input data tuple used in the calculation. The possible tuple types are: • (r,c) - the pair of reference and candidate translations • (c,st ) - the pair of candidate translations and translated source • (s,ct ) - the pair of source segment and backtranslated candidate In Table 1, the results of TSKM when applied to the Czech-English, Finnish-English, RussianEnglish and Turkish-English language pairs are 7 http://www.statmt.org/wmt16/results.html 586 et al., 2011) and the Compositional Smoothed Partial Tree Kernel (CSPTK) (Annesi et al., 2013) (Annesi et al., 2014). The SPTK uses a term similarity function to semantically match tree nodes. The term similarity function can be obtained through either word vector spaces or distributional analysis. On the other hand, the CSPTK represents a generalization of SPTK, which uses Distributional Compositional Semantics to determine the degree of similarity between tree fragments. The implementations for these kernels together with an example wordspace for English are also available in the KeLP package. The results show that by relaxing the matching constraints to allow for lexical variation t"
W17-4766,C10-1011,0,0.0217985,"e no high quality analysis tools (e.g lemmatizers, pos-taggers or parsers) for the target language are available, a situation that would prevent TSKM from being applied. In our experiments, both the pseudo-references together with the back-translations were obtained using the free online Google Translator Toolkit 2 . To apply the tree and sequence kernels for the task of Machine Translation evaluation, a preprocessing of the input data is necessary. In the case of PTK, the input data was first tokenized and pos-tagged, followed by a parsing step using the Bohnet graph-based dependency parser (Bohnet, 2010) and the publicly available syntactic analyP T K(r,c)+SK(r,c) 2 T SKMbasic = T SKM (r, c) = (1) with r and c denoting the reference and the candidate translations and PTK and SK referring to the scores of the Partial Tree Kernel and the Sequence Kernel. Furthermore, we experimented with using an additional pseudo-reference and a back-translation in the computation of the metric in order to explore how the different combination schemes influence the performance of TSKM. One possible kind of combination can be represented as: T SKMcomb = T SKMbasic +T SKMpseudo +T SKMback 3 (2) T SKMpseudo = T S"
W17-4766,E06-1032,0,0.0693015,"tions, which do not require reference translations. In the case of a reference-based evaluation, the target segment is compared with the reference translation resulting in a score that measures the similarity between the two sentences. Different approaches for computing the comparison have been implemented, with the most frequently used one being BLEU (Papineni et al., 2002), which measures the quality of the candidate translation by counting the number of n-grams it has in common with the reference translations. Nonetheless, multiple disadvantages of BLEU have already been pointed out, as in Callison-Burch et al. (2006), where it is shown that an increase of the BLEU score does not necessary correlate with a better performing system. This has motivated further research into additional MT evaluation methods that rely on more than lexical matching by additionally including the syntactic and semantic structure of the sentences (e.g. (Popovi´c and Ney, 2009), (Gautam and Bhattacharyya, 2014) ). We propose a new method for the evaluation of MT output, based on tree and sequence kernel functions, applied on the pair of reference and candidate translations. In addition, we study the impact of applying the kernels o"
W17-4766,E06-1031,0,0.651069,"the submission to the Shared Task. Generalizations of the Partial Tree Kernel were used, namely the Smoothed Partial Tree Kernel (SPTK) (Croce Table 2: Evaluation results in terms of Pearson correlation for the en-de and de-en language pairs 4.2 Results The results of the evaluation are presented in Tables 1 and 2, which contain the correlation scores for the different TSKM variants taken into account. For comparison purposes, the scores for some state-of-the-art MT evaluation methods are also presented: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), PER (Tillmann et al., 1997), CDER (Leusch et al., 2006) and WER. The results were obtained using the evaluation scripts made available by the WMT16 conference 7 . The following metric notation was adopted for each of the TSKM variants evaluated: Kernel[level], where Kernel identifies the type of kernel utilized (SK or PTK) and level refers to the input data tuple used in the calculation. The possible tuple types are: • (r,c) - the pair of reference and candidate translations • (c,st ) - the pair of candidate translations and translated source • (s,ct ) - the pair of source segment and backtranslated candidate In Table 1, the results of TSKM when a"
W17-4766,W12-3103,0,0.0200339,"ng them as input data for kernel functions. segment into the target language, while a backtranslation is obtained by translating the target segment into the source language. The evaluation results show that the new metric strongly correlates with human judgments, outperforming the state-of-the-art methods. 2 Related work MT evaluation methods can be categorized according to the level of analysis that they address into lexical ones (e.g BLEU (Papineni et al., 2002), TER (Snover et al., 2006)), syntactic ones (e.g. (Popovi´c and Ney, 2007), (Gim´enez and M`arquez, 2007)) or semantic ones (e.g. (Castillo and Estrella, 2012)), with hybrid combinations integrating more than one representational layer at a time. A syntactic evaluation method based on tree kernels is proposed in Liu and Gildea (2005). It uses the subtree kernel introduced in Collins and Duffy (2002) to calculate the similarity between the reference and the candidate translations. Besides this, a syntactic metric based on counting the number of fixed-depth subtrees shared by the two translations is also introduced, with both metrics being applied on the constituency trees of the input data. Additionally, a dependency tree based metric is presented, w"
W17-4766,W05-0904,0,0.104957,"ation results show that the new metric strongly correlates with human judgments, outperforming the state-of-the-art methods. 2 Related work MT evaluation methods can be categorized according to the level of analysis that they address into lexical ones (e.g BLEU (Papineni et al., 2002), TER (Snover et al., 2006)), syntactic ones (e.g. (Popovi´c and Ney, 2007), (Gim´enez and M`arquez, 2007)) or semantic ones (e.g. (Castillo and Estrella, 2012)), with hybrid combinations integrating more than one representational layer at a time. A syntactic evaluation method based on tree kernels is proposed in Liu and Gildea (2005). It uses the subtree kernel introduced in Collins and Duffy (2002) to calculate the similarity between the reference and the candidate translations. Besides this, a syntactic metric based on counting the number of fixed-depth subtrees shared by the two translations is also introduced, with both metrics being applied on the constituency trees of the input data. Additionally, a dependency tree based metric is presented, which computes the number of common headword chains, where a headword chain is defined as the concatenation of words that form a path in the dependency tree. Another MT evaluati"
W17-4766,P02-1034,0,0.191695,"human judgments, outperforming the state-of-the-art methods. 2 Related work MT evaluation methods can be categorized according to the level of analysis that they address into lexical ones (e.g BLEU (Papineni et al., 2002), TER (Snover et al., 2006)), syntactic ones (e.g. (Popovi´c and Ney, 2007), (Gim´enez and M`arquez, 2007)) or semantic ones (e.g. (Castillo and Estrella, 2012)), with hybrid combinations integrating more than one representational layer at a time. A syntactic evaluation method based on tree kernels is proposed in Liu and Gildea (2005). It uses the subtree kernel introduced in Collins and Duffy (2002) to calculate the similarity between the reference and the candidate translations. Besides this, a syntactic metric based on counting the number of fixed-depth subtrees shared by the two translations is also introduced, with both metrics being applied on the constituency trees of the input data. Additionally, a dependency tree based metric is presented, which computes the number of common headword chains, where a headword chain is defined as the concatenation of words that form a path in the dependency tree. Another MT evaluation method that makes use of tree kernels is introduced in Guzm´an e"
W17-4766,D09-1143,0,0.0348723,"without explicitly stating the feature spaces corresponding to the two representations (Moschitti, 2006a). The types of representations taken into account can be, among others, vectorial, sequential or treebased. The tree kernels developed so far distinguish themselves from one another by the types of tree fragments (e.g. subsets, subtrees or partial trees) and the type of syntactic trees (constituency or dependency) they employ in their computation, which influences their suitability for certain tasks (see Moschitti (2006a)). Contrastively, sequence kernels (e.g (Bunescu and Mooney, 2005), (Nguyen et al., 2009)) make use of subsequences in the computation of the kernel. The new method for the evaluation of Machine Translation proposed in this paper, denoted as TSKM, makes use of both tree and sequence kernels, which are applied on the pair of candidate and reference translations. The tree kernel used is represented by the Partial Tree Kernel (PTK) 583 Figure 1: Example of a dependency tree. (Moschitti, 2006a). It uses partial tree fragments, which are a generalization over subtrees and subset trees, so that a node and its partial descendants can constitute a valid fragment. An example of a dependenc"
W17-4766,D11-1096,0,0.0327482,"30 .943 .839 - - - - - .990 .982 .981 .995 .992 .988 .752 .770 .770 .846 .858 .924 .950 .958 .974 .968 .962 .966 .765 .680 .947 .836 .899 .952 .864 .848 .918 .911 .928 .958 mosesBLEU mosesWER mosesPER mosesCDER mtevalBLEU mtevalNIST Table 1: Evaluation results in terms of Pearson correlation for the different TSKM variants. The highlighted TSKM variant indicates the submission to the WMT17 Metrics Task. 4 Evaluation and results sis models 3 . The dependency parse trees obtained were converted to tree representations which can be used by the PTK. The lexical-centered-tree approach presented in Croce et al. (2011) was utilized, which required storing both the grammatical relation and the pos-tag information as the rightmost children of a dependency tree node. The score of the kernel functions were normalized using the formula from Croce et al. (2011): K(T 1, T 2) score = p K(T 1, T 1) ? K(T 2, T 2) 4.1 Experimental setup The evaluation of TSKM was performed using data pertaining to the News domain from the First Conference On Machine Translation (WMT16) 6 . For the results obtained in the WMT17 Metrics Task, please refer to the official results paper. The following language pairs were used in the evalu"
W17-4766,P02-1040,0,0.121627,"evaluation can be divided into two categories: reference-free evaluation and reference-based one. The referencefree evaluation, also known as Quality Estimation, aims at providing automatic methods, for assessing the quality of candidate translations, which do not require reference translations. In the case of a reference-based evaluation, the target segment is compared with the reference translation resulting in a score that measures the similarity between the two sentences. Different approaches for computing the comparison have been implemented, with the most frequently used one being BLEU (Papineni et al., 2002), which measures the quality of the candidate translation by counting the number of n-grams it has in common with the reference translations. Nonetheless, multiple disadvantages of BLEU have already been pointed out, as in Callison-Burch et al. (2006), where it is shown that an increase of the BLEU score does not necessary correlate with a better performing system. This has motivated further research into additional MT evaluation methods that rely on more than lexical matching by additionally including the syntactic and semantic structure of the sentences (e.g. (Popovi´c and Ney, 2009), (Gauta"
W17-4766,W07-0707,0,0.0805773,"Missing"
W17-4766,W09-0402,0,0.0559503,"Missing"
W17-4766,P15-4004,0,0.0249868,"y ranking five candidate translations, with ties being allowed. In order to compute a single TSKM score for an MT system, all the individual sentence scores were combined by averaging them. (5) with T1 and T2 standing for the input data tuple and K indicating the type of kernel function. Regarding SK, only a tokenization of the data was required, as the SK function was applied on substructures composed of the lexical items. For the computation of the kernel functions we used the Partial Tree Kernel4 and the Sequence Kernel 5 implementations, found in the KeLP (Kernel-based Learning Platform) (Filice et al., 2015b) (Filice et al., 2015a) library. KeLP is an open source Java platform encompassing kernel based Machine Learning algorithms together with multiple types of kernel functions. The implemented kernels support either vector based input representations or structural ones in the form of trees, sequences or graphs. Different variants of TSKM were taken into account for evaluation. To investigate how the lexical variation affects the performance of the metric, we also implemented versions of the metric where lemmas are used instead of the exact lexical items. 3 https://code.google.com/archive/p/mate"
W17-4766,P09-2034,0,0.0261137,"tion of words that form a path in the dependency tree. Another MT evaluation method that makes use of tree kernels is introduced in Guzm´an et al. (2014). It also uses the subtree kernel introduced in Collins and Duffy (2001), but in this case it calculates the similarity between the discourse trees of the candidate and reference translation. The evaluation combined the newly proposed metric with already existing ones and the results showed that the addition is beneficial for improving the correlation scores. The role of back-translations has also been investigated before, like in the case of Rapp (2009) where the quality of a candidate translation is assessed by measuring the similarity, in terms of a modified version of BLEU, between its backtranslation and the initial source segment. In the case of pseudo-references, they have been used as an additional source of data for tuning the parameters of MT systems, like in the case of Ammar et al. (2013). An evaluation method based 3 Methods and implementation A kernel function makes use of structural representations of the input data in order to calculate the number of substructures they share, without explicitly stating the feature spaces corre"
W17-4766,W14-3350,0,0.0164759,"2002), which measures the quality of the candidate translation by counting the number of n-grams it has in common with the reference translations. Nonetheless, multiple disadvantages of BLEU have already been pointed out, as in Callison-Burch et al. (2006), where it is shown that an increase of the BLEU score does not necessary correlate with a better performing system. This has motivated further research into additional MT evaluation methods that rely on more than lexical matching by additionally including the syntactic and semantic structure of the sentences (e.g. (Popovi´c and Ney, 2009), (Gautam and Bhattacharyya, 2014) ). We propose a new method for the evaluation of MT output, based on tree and sequence kernel functions, applied on the pair of reference and candidate translations. In addition, we study the impact of applying the kernels on the tuple consisting of the source segment and a back-translation, together with the pair comprised of the candidate translation and a pseudo-reference. A pseudoreference is the result of translating the source Introduction The evaluation of Machine Translation (MT) represents a very important domain of research, as providing meaningful, automatic and accurate methods fo"
W17-4766,W13-3711,0,0.0721709,"Missing"
W17-4766,2006.amta-papers.25,0,0.289719,"more, we extend on the previous work of pseudoreferences and back-translations by studying their impact in the context of using them as input data for kernel functions. segment into the target language, while a backtranslation is obtained by translating the target segment into the source language. The evaluation results show that the new metric strongly correlates with human judgments, outperforming the state-of-the-art methods. 2 Related work MT evaluation methods can be categorized according to the level of analysis that they address into lexical ones (e.g BLEU (Papineni et al., 2002), TER (Snover et al., 2006)), syntactic ones (e.g. (Popovi´c and Ney, 2007), (Gim´enez and M`arquez, 2007)) or semantic ones (e.g. (Castillo and Estrella, 2012)), with hybrid combinations integrating more than one representational layer at a time. A syntactic evaluation method based on tree kernels is proposed in Liu and Gildea (2005). It uses the subtree kernel introduced in Collins and Duffy (2002) to calculate the similarity between the reference and the candidate translations. Besides this, a syntactic metric based on counting the number of fixed-depth subtrees shared by the two translations is also introduced, with"
W17-4766,W07-0738,0,0.0629057,"Missing"
W17-4766,P07-1038,0,\N,Missing
W18-3005,P14-1019,0,0.013015,"as a tree node. The tree consists of the dependency relations between each word of the sentence and its head word (Nivre, 2004). The standard input of a parser is a natural language sentence. To supply such a parser with additional information required for text completion in a multi-modal environment we have to make it sensitive to cues from the context. In our previous research (Salama and Menzel, 2018, 2017), we have introduced a multi-modal dependency parser adopting the graph-based approach of Eisner (1996) and Mcdonald and Pereira (2006). Our model, called RBG-2, extends the RBG parser (Zhang et al., 2014) by enabling multi-channel input providing the parsing process with context information in addition to the natural language sentence. Integration is achieved by combining features from both input channels during the normal training procedure of the RBG parser. A Multi-Modal Approach for a Text Completion Task Although closing the gaps in a sentence based only on a language model is a simple way to tackle the issue, in extremely ambiguous situations, gap reconstruction is almost impossible on a purely unimodal base. In this paper, we work on multimodal data that consists of linguistic and conte"
W18-3005,W12-2704,0,0.385958,"t reference (such as mug 2 among several mug instances). 1 Introduction Text completion/prediction is a crucial element of communication systems, due to its role in increasing the fluency and the effectiveness of the communication in scenarios where the environment is noisy, or the communication partner suffers ∗ *These authors contributed equally to this work 41 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 41–49 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics closer. Using the Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2012), Gubbins and Vlachos (2013) have showed that incorporating syntactic information leads to grammatically better options for a semantic text completion task. On the other hand, semantic clustering or classification (like in ontologies) can be used to derive predictions on the semantic level. However, when it comes to the description of daily activities, contextual information coming from another modality would be more beneficial, since linguistic distributions alone could hardly contribute enough clues to distinguish the action of washing a pan from washing a mug, which is a crucial difference"
W18-3005,C96-1058,0,0.0315138,"cture of the input sentence in form of a dependency tree. Each token of the input is represented as a tree node. The tree consists of the dependency relations between each word of the sentence and its head word (Nivre, 2004). The standard input of a parser is a natural language sentence. To supply such a parser with additional information required for text completion in a multi-modal environment we have to make it sensitive to cues from the context. In our previous research (Salama and Menzel, 2018, 2017), we have introduced a multi-modal dependency parser adopting the graph-based approach of Eisner (1996) and Mcdonald and Pereira (2006). Our model, called RBG-2, extends the RBG parser (Zhang et al., 2014) by enabling multi-channel input providing the parsing process with context information in addition to the natural language sentence. Integration is achieved by combining features from both input channels during the normal training procedure of the RBG parser. A Multi-Modal Approach for a Text Completion Task Although closing the gaps in a sentence based only on a language model is a simple way to tackle the issue, in extremely ambiguous situations, gap reconstruction is almost impossible on a"
W18-3005,D13-1143,0,0.41057,"2 among several mug instances). 1 Introduction Text completion/prediction is a crucial element of communication systems, due to its role in increasing the fluency and the effectiveness of the communication in scenarios where the environment is noisy, or the communication partner suffers ∗ *These authors contributed equally to this work 41 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 41–49 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics closer. Using the Microsoft Research Sentence Completion Challenge (Zweig and Burges, 2012), Gubbins and Vlachos (2013) have showed that incorporating syntactic information leads to grammatically better options for a semantic text completion task. On the other hand, semantic clustering or classification (like in ontologies) can be used to derive predictions on the semantic level. However, when it comes to the description of daily activities, contextual information coming from another modality would be more beneficial, since linguistic distributions alone could hardly contribute enough clues to distinguish the action of washing a pan from washing a mug, which is a crucial difference for helper robots. A popular"
W18-3005,E06-1011,0,0.0115622,"sentence in form of a dependency tree. Each token of the input is represented as a tree node. The tree consists of the dependency relations between each word of the sentence and its head word (Nivre, 2004). The standard input of a parser is a natural language sentence. To supply such a parser with additional information required for text completion in a multi-modal environment we have to make it sensitive to cues from the context. In our previous research (Salama and Menzel, 2018, 2017), we have introduced a multi-modal dependency parser adopting the graph-based approach of Eisner (1996) and Mcdonald and Pereira (2006). Our model, called RBG-2, extends the RBG parser (Zhang et al., 2014) by enabling multi-channel input providing the parsing process with context information in addition to the natural language sentence. Integration is achieved by combining features from both input channels during the normal training procedure of the RBG parser. A Multi-Modal Approach for a Text Completion Task Although closing the gaps in a sentence based only on a language model is a simple way to tackle the issue, in extremely ambiguous situations, gap reconstruction is almost impossible on a purely unimodal base. In this p"
W18-3005,P15-2084,0,0.593488,"Missing"
W18-3005,W04-0308,0,0.0833141,"ectly determine the exact reference to an entity in the context description given the contextual information, in particular if the linguistic input is noisy and a token of the input sentence is missing. At this stage of research, we work only on one gap per sentence. 2.1 Context-integrating Dependency Parser Dependency parsing is an essential NLP task that determines the syntactic structure of the input sentence in form of a dependency tree. Each token of the input is represented as a tree node. The tree consists of the dependency relations between each word of the sentence and its head word (Nivre, 2004). The standard input of a parser is a natural language sentence. To supply such a parser with additional information required for text completion in a multi-modal environment we have to make it sensitive to cues from the context. In our previous research (Salama and Menzel, 2018, 2017), we have introduced a multi-modal dependency parser adopting the graph-based approach of Eisner (1996) and Mcdonald and Pereira (2006). Our model, called RBG-2, extends the RBG parser (Zhang et al., 2014) by enabling multi-channel input providing the parsing process with context information in addition to the na"
W18-6444,P07-2045,0,0.0259535,"Missing"
W18-6444,D11-1033,0,0.175708,"oits all available (bilingual) general domain corpora with the purpose of extracting sentences that have a strong relationship to a given in-domain. All sentences from the general domain pool are scored according to a similarity function/ algorithm/ method and after being sorted, the most similar ones are selected to take part in the MT 2 Related work Related work in data selection is ample, therefore this section only mentions methods that fit in the same category with our method and we also shortly describe the widely known state-of-the art method of performing data selection, introduced by Axelrod et al. (2011), since it is the chosen method for comparing results in this paper. Our scoring function relies heavily on term frequency. Therefore, it falls in the category of TFIDF1 based approaches. Hildebrand et al. (2005) uses TF-IDF to produce vector representations of sentences. Then the cosine of the angle between the sentence vectors is interpreted as the similar1 637 Term Frequency - Inverse Document Frequency Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 637–643 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Comput"
W18-6444,W05-0909,0,0.0250646,"un, for every language pair, are insignificant except for one language pair, therefore we conclude that either one of the algorithm variations can be successfully applied as a fast data selection technique that yields good translations (BLEU scores between 31 and 42 for four out of five language pairs). Results We report the automatic evaluation results obtained in the WMT task for five language pairs and then we present further experiments for the Spanish-English language pair. BLEU was used as an evaluation metric by the WMT Biomedical organizers and in addition to BLEU we also used METEOR (Lavie and Agarwal, 2005) for further evaluating the Spanish-English experiments. 4.1 EN-ES 350K 1.62M WMT Biomedical Results Each team was allowed to submit a maximum of three runs. For every language pair that we used to evaluate our method on, we submitted three runs as follows: the first run only considers the scores obtained using the English side of the training corpora, the second run made use of only the nonEnglish side of the training corpora and for the third run the scores for both the source and the target sides were summed up to form a single score. The aim of data selection is to identify in the general"
W18-6444,2005.iwslt-1.7,0,0.125837,"Missing"
W18-6444,P10-2041,0,0.0439044,"o used as means of determining sentence similarities. In contrast to these methods, we use only the term frequency in computing our similarity scores and we make no use of the cosine. Instead, we focus on the relative difference between a term that appears in the general domain and in the in-domain and simply multiply it by a weighting scheme that has empirically proved to be effective. Our method is also related to the other methods from the TF-IDF category with respect to its simplicity. To compare our results with other approaches we apply the modified Moore-Lewis method which is based on (Moore and Lewis, 2010): given the source side of an in-domain corpus and a random subsample of the source side of a general domain corpus, a language model (LM) is trained on each one of them. The sentences from the general domain are scored by the difference of the crossentropy of a sentence according to the in-domain LM and the cross-entropy of the same sentence according to the general domain LM. Axelrod et al. (2011) modified the scoring by applying the same procedure also to the target side of the corpora and afterwards summing the scores. We refer to this method as MML (modified Moore-Lewis) in the rest of th"
W18-6444,P11-2031,0,0.0206434,"1 run 2 run 3 EN-ES 31.32 31.05 31.33 ES-EN 36.16 35.17 36.05 EN-PT 34.92 34.19 34.49 PT-EN 41.84 41.80 41.79 EN-RO 14.60 14.39 14.07 Table 3: BLEU scores reported by WMT 4.2 Spanish-English Additional Experiments For Spanish-English, the best performing variant of our method was run 1 - using only the English side of the corpora in the algorithm. We evaluated our DSTF-EN method against a strong baseline (that uses an interpolated LM), a baseline trained using only the in-domain data and the state-of-theart method MML for the Spanish-English language pair7 . Following recommendations from H. Clark et al. (2011) and standard practices, we tuned the systems three times and report in Table 4 the averaged BLEU scores. 7 Due to time limitations, we will evaluate further language pairs against MML in the future work. 640 Figure 1: Paired bootstrap resampling graphs using BLEU differences between DSTF-EN and MML (left graph) and using F-measure differences (right graph) System BLEU METEOR Table 4: BS-strong 34.96 35.56 BS-IN 32.44 34.51 MML 34.62 35.42 DSTF-EN 35.40 35.54 difference in BLEU (respectively F-measure) between DSTF-EN and MML for all resamples. The p-value from the first graph in Figure 1 repo"
W18-6444,2005.eamt-1.19,0,0.0711132,"ing to a similarity function/ algorithm/ method and after being sorted, the most similar ones are selected to take part in the MT 2 Related work Related work in data selection is ample, therefore this section only mentions methods that fit in the same category with our method and we also shortly describe the widely known state-of-the art method of performing data selection, introduced by Axelrod et al. (2011), since it is the chosen method for comparing results in this paper. Our scoring function relies heavily on term frequency. Therefore, it falls in the category of TFIDF1 based approaches. Hildebrand et al. (2005) uses TF-IDF to produce vector representations of sentences. Then the cosine of the angle between the sentence vectors is interpreted as the similar1 637 Term Frequency - Inverse Document Frequency Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 637–643 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64071 ity between the sentences. A similar approach is given in Eck et al. (2005) where a weighting scheme based on TF-IDF by means of unseen ngrams and sentenc"
W18-6444,L16-1470,0,0.227038,"Missing"
W18-6444,P03-1021,0,0.00922654,"aning, lowercasing and normalizing punctuation. Our language model (LM) was obtained by interpolating (Schwenk and Koehn, 2008) the LM estimated using the general domain data and the LM estimated on the in-domain data. We used the SRILM toolkit (Stolcke, 2002) and Kneser-Ney discounting (Kneser and Ney, 1995) for estimating 5-grams LMs. All the experiments benefited from the interpolated language model, including the strong baseline and the MML experiment. As for the chosen state-of-the-art method, MML, we used the implementation available from Moses. Tuning of the systems was done with MERT (Och, 2003) and GIZA++ (Och and Ney, 2003) using the default grow-diag-final-and alignment symmetrization method for word alignment. 3.3 Algorithm 1 can be applied either on the source or on the target sides of the corpora. For example, when considering the source side, for every sentence from the lemmatized (or stemmed) general domain data, we iterate through all its words. Given sentence s and the word w, we square the relative difference between the term frequency of w in the in-domain profile, count(w, INside ), and the term frequency of w in the general domain profile, count(w, GENside ). We use the"
W18-6444,J03-1002,0,0.00906008,"normalizing punctuation. Our language model (LM) was obtained by interpolating (Schwenk and Koehn, 2008) the LM estimated using the general domain data and the LM estimated on the in-domain data. We used the SRILM toolkit (Stolcke, 2002) and Kneser-Ney discounting (Kneser and Ney, 1995) for estimating 5-grams LMs. All the experiments benefited from the interpolated language model, including the strong baseline and the MML experiment. As for the chosen state-of-the-art method, MML, we used the implementation available from Moses. Tuning of the systems was done with MERT (Och, 2003) and GIZA++ (Och and Ney, 2003) using the default grow-diag-final-and alignment symmetrization method for word alignment. 3.3 Algorithm 1 can be applied either on the source or on the target sides of the corpora. For example, when considering the source side, for every sentence from the lemmatized (or stemmed) general domain data, we iterate through all its words. Given sentence s and the word w, we square the relative difference between the term frequency of w in the in-domain profile, count(w, INside ), and the term frequency of w in the general domain profile, count(w, GENside ). We use the same relative difference formu"
W18-6444,P02-1040,0,0.105744,"Missing"
W18-6444,W04-3250,0,0.184087,"Missing"
W18-6444,I08-2089,0,0.0805653,"Missing"
W18-6444,tiedemann-2012-parallel,0,0.0286652,"l Wikipedia EMEA Scielo-gma 2016 Experiments This section describes the experimental settings including the corpora and the tools used, as well as the data selection algorithm we developed. 3.1 3 EN-PT 2.1M 1.6M 1.08M 613K EN-RO 2.4M 994K - Table 1: Corpora used for DSTF Corpora 3.2 The general domain data consisted of a concatenation of the Commoncrawl2 corpora and the Wikipedia (Wolk and Marasek, 2014) corpora for English-Spanish and Spanish-English, Paracrawl3 and Wikipedia for English-Portuguese and Portuguese-English and Paracrawl for EnglishRomanian. For the in-domain, we used the EMEA (Tiedemann, 2012) corpora for all language pairs and the Scielo corpora (health and biological) provided by the WMT 2016 Biomedical task (Neves et al., 2016) for all language pairs except 2 EN-ES 1.8M 1.6M 678K 166K Tools For text processing we used the nltk toolkit(Bird et al., 2009), the WordNet (Fellbaum, 1998) lemmatizer for English and the Snowball stemmer (F. Porter, 2001) for Spanish, Portuguese and Romanian. The SMT systems were trained using the Moses toolkit (Koehn et al., 2007) and the Experiment Management System (Koehn, 2010). The preprocessing of the data consisted in tokenization, 4 http://www.s"
W18-6460,C10-1011,0,0.0277721,"e integration of tree kernels can prove beneficial when compared to the strictly feature based QE systems. 776 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 776–781 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64087 nel. For the variants where both kernel functions are used, the superscript will be left unfilled. Examples for this notation can be found in Tables 1 and 2. TSKQE requires parsed input data, which was generated by means of the MATE parser (Bohnet, 2010), using English and German pre-trained models for tokenization, lemmatization, tagging and parsing itself 1 . The resulting dependency tree was further processed in order to remove the arc labels and encode all the syntactic information as tree nodes. For this, a variant of the Lexical-Centered-Tree (LCT) (Croce et al., 2011) method was applied, so that the dependency relation becomes the rightmost child of the dependency heads. For the generation of the pseudo-references and back-translations, the Google Translator Toolkit 2 was used. The actual TSKQE models were built with the help of the Ke"
W18-6460,D11-1096,0,0.0280284,"https://doi.org/10.18653/v1/W18-64087 nel. For the variants where both kernel functions are used, the superscript will be left unfilled. Examples for this notation can be found in Tables 1 and 2. TSKQE requires parsed input data, which was generated by means of the MATE parser (Bohnet, 2010), using English and German pre-trained models for tokenization, lemmatization, tagging and parsing itself 1 . The resulting dependency tree was further processed in order to remove the arc labels and encode all the syntactic information as tree nodes. For this, a variant of the Lexical-Centered-Tree (LCT) (Croce et al., 2011) method was applied, so that the dependency relation becomes the rightmost child of the dependency heads. For the generation of the pseudo-references and back-translations, the Google Translator Toolkit 2 was used. The actual TSKQE models were built with the help of the Kernel-based Learning Platform (KeLP) library (Filice et al., 2015b) (Filice et al., 2015a), where various kernel functions and learning algorithms are integrated. For our experiments, we used the Support Vector Machine epsilon-Regression algorithm to learn the HTER scores, together with the PTK and SK implementations. Tree ker"
W18-6460,E06-1015,0,0.0164422,"tained using an online MT system. The extension proposed in this paper uses the same input data. In addition, however, the ker2 Related work The benefit of kernel functions has already been investigated in the context of Quality Estimation. In the work presented by (Hardmeier, 2011) and further expanded in (Hardmeier et al., 2012), tree kernel functions in addition to feature vectors are used to predict MT output quality. Both constituency and dependency parse trees were considered, with the Subset Tree Kernels (Collins and Duffy, 2001) being applied to the former and the Partial Tree Kernel (Moschitti, 2006a)(Moschitti, 2006b) to the latter. The evaluation results revealed that the integration of tree kernels can prove beneficial when compared to the strictly feature based QE systems. 776 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 776–781 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64087 nel. For the variants where both kernel functions are used, the superscript will be left unfilled. Examples for this notation can be found in Tables 1 and 2. TSKQE re"
W18-6460,W14-3348,0,0.028789,"algorithm to learn the HTER scores, together with the PTK and SK implementations. Tree kernels have also been applied in the work of (Kaljahi et al., 2014) and (Kaljahi, 2015), where a QE system is built based on Subset Tree Kernels applied for the constituency and dependency parse trees corresponding to the source and candidate translation. The kernels were also combined with a series of manually designed features, while SVM regression was used, in order to predict different automatic MT evaluation methods, like for example BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014) scores. The QE method introduced in (Duma and Menzel, 2017), TSKQE, is based on a linear combination between tree and sequence kernels. As a tree kernel the Partial Tree Kernel (PTK) is used, while for the sequence kernel, the Subsequence Kernel (SK) (Bunescu and Mooney, 2005) was chosen. Similarly to the previously mentioned QE methods, the kernels are applied to the source and candidate translations, but in addition also on a back-translation. The work presented in this paper builds on this method, by additionally using kernel functions for pseudo-references. Pseudoreferences have been util"
W18-6460,P02-1040,0,0.100631,"riments, we used the Support Vector Machine epsilon-Regression algorithm to learn the HTER scores, together with the PTK and SK implementations. Tree kernels have also been applied in the work of (Kaljahi et al., 2014) and (Kaljahi, 2015), where a QE system is built based on Subset Tree Kernels applied for the constituency and dependency parse trees corresponding to the source and candidate translation. The kernels were also combined with a series of manually designed features, while SVM regression was used, in order to predict different automatic MT evaluation methods, like for example BLEU (Papineni et al., 2002), TER (Snover et al., 2006) and METEOR (Denkowski and Lavie, 2014) scores. The QE method introduced in (Duma and Menzel, 2017), TSKQE, is based on a linear combination between tree and sequence kernels. As a tree kernel the Partial Tree Kernel (PTK) is used, while for the sequence kernel, the Subsequence Kernel (SK) (Bunescu and Mooney, 2005) was chosen. Similarly to the previously mentioned QE methods, the kernels are applied to the source and candidate translations, but in addition also on a back-translation. The work presented in this paper builds on this method, by additionally using kerne"
W18-6460,W17-4762,1,0.779562,"earlier version as Tree and Sequence Kernel Quality Estimation (TSKQE), but the variant under consideration will be marked through the use of subscripts together with superscripts. This paper is organized as follows. In Section 2 related work is presented, focusing on kernel based QE methods. In the next section the implementation details for TSKQE are presented. This is followed by the evaluation setup and a discussion of the results. The paper concludes with future work ideas and final remarks. In this paper, a novel approach to Quality Estimation is introduced, which extends the method in (Duma and Menzel, 2017) by also considering pseudo-reference translations as data sources to the tree and sequence kernels used before. Two variants of the system were submitted to the sentence level WMT18 Quality Estimation Task for the English-German language pair. They have been ranked 4th and 6th out of 13 systems in the SMT track, while in the NMT track ranks 4 and 5 out of 11 submissions have been reached. 1 Introduction The purpose of Quality Estimation (QE), as a subfield of Machine Translation (MT), is to allow the evaluation of MT output without the necessity of providing a reference translation. This woul"
W18-6460,2014.eamt-1.21,0,0.513484,"al Tree Kernel (PTK) is used, while for the sequence kernel, the Subsequence Kernel (SK) (Bunescu and Mooney, 2005) was chosen. Similarly to the previously mentioned QE methods, the kernels are applied to the source and candidate translations, but in addition also on a back-translation. The work presented in this paper builds on this method, by additionally using kernel functions for pseudo-references. Pseudoreferences have been utilized before in the context of QE, but as a support for the generation of features, like for example in the work of (Soricut et al., 2012), (Shah et al., 2013) or (Scarton and Specia, 2014). In (Scarton and Specia, 2014) BLEU and TER were applied to the candidate translation and pseudo-references and their scores were used as additional features in the context of document level QE. 3 4 Evaluation The evaluation was performed measuring the correlation between the TSKQE scores and the HTER gold standards. This was achieved by computing the Pearson correlation coefficient, which results in a number between -1 and 1. A score of 1 indicates that there is a perfect agreement between the two sets of scores, while a score of -1 would suggest a negative agreement. In addition to the Pear"
W18-6460,P15-4004,0,0.0156117,"for tokenization, lemmatization, tagging and parsing itself 1 . The resulting dependency tree was further processed in order to remove the arc labels and encode all the syntactic information as tree nodes. For this, a variant of the Lexical-Centered-Tree (LCT) (Croce et al., 2011) method was applied, so that the dependency relation becomes the rightmost child of the dependency heads. For the generation of the pseudo-references and back-translations, the Google Translator Toolkit 2 was used. The actual TSKQE models were built with the help of the Kernel-based Learning Platform (KeLP) library (Filice et al., 2015b) (Filice et al., 2015a), where various kernel functions and learning algorithms are integrated. For our experiments, we used the Support Vector Machine epsilon-Regression algorithm to learn the HTER scores, together with the PTK and SK implementations. Tree kernels have also been applied in the work of (Kaljahi et al., 2014) and (Kaljahi, 2015), where a QE system is built based on Subset Tree Kernels applied for the constituency and dependency parse trees corresponding to the source and candidate translation. The kernels were also combined with a series of manually designed features, while S"
W18-6460,2013.mtsummit-papers.21,0,0.0344439,"a tree kernel the Partial Tree Kernel (PTK) is used, while for the sequence kernel, the Subsequence Kernel (SK) (Bunescu and Mooney, 2005) was chosen. Similarly to the previously mentioned QE methods, the kernels are applied to the source and candidate translations, but in addition also on a back-translation. The work presented in this paper builds on this method, by additionally using kernel functions for pseudo-references. Pseudoreferences have been utilized before in the context of QE, but as a support for the generation of features, like for example in the work of (Soricut et al., 2012), (Shah et al., 2013) or (Scarton and Specia, 2014). In (Scarton and Specia, 2014) BLEU and TER were applied to the candidate translation and pseudo-references and their scores were used as additional features in the context of document level QE. 3 4 Evaluation The evaluation was performed measuring the correlation between the TSKQE scores and the HTER gold standards. This was achieved by computing the Pearson correlation coefficient, which results in a number between -1 and 1. A score of 1 indicates that there is a perfect agreement between the two sets of scores, while a score of -1 would suggest a negative agre"
W18-6460,P15-1174,0,0.0259381,"ndards. This was achieved by computing the Pearson correlation coefficient, which results in a number between -1 and 1. A score of 1 indicates that there is a perfect agreement between the two sets of scores, while a score of -1 would suggest a negative agreement. In addition to the Pearson coefficient, the Mean Absolute Error (MAE) and the Root Mean Squared Error (RMSE) were also calculated. For both these evaluation methods, the closer their score is to 0, the better the QE system should be considered. The significance testing of the results was performed using the methodology presented in (Graham, 2015), which is based on pairwise testing using the Williams test (Williams, 1959). 3 Method details Different variants of TSKQE were defined in (Duma and Menzel, 2017) depending on the level where the kernel functions are applied (source segment, candidate translation or back-translation) and the type of kernel function (SK or PTK). To indicate these distinctions we will use a notation system, where the level will be marked as a subscript attached to the TSKQE method name, with the possible values being source in case of the source segments, basic corresponding to both source segments and candidat"
W18-6460,2006.amta-papers.25,0,0.174633,"ubfield of Machine Translation (MT), is to allow the evaluation of MT output without the necessity of providing a reference translation. This would be extremely beneficial in the development cycle of a MT system, as it would permit fast and cost efficient evaluation phases. In the case of the previous Quality Estimation Shared Task (Bojar et al., 2017) together with the current campaign (Specia et al., 2018a), the purpose for the sentence level track was to predict the effort required in order to post-edit a candidate translation as measured by the Human-mediated Translation Edit Rate (HTER) (Snover et al., 2006) score. In this paper an extension of the QE method introduced in (Duma and Menzel, 2017) is presented. Our earlier version of the metric was based on learning HTER scores using tree and sequence kernels. The kernel functions were applied not only on the source segments and the candidate translations, but also on the back-translations of the MT output into the source language. The back-translations were obtained using an online MT system. The extension proposed in this paper uses the same input data. In addition, however, the ker2 Related work The benefit of kernel functions has already been i"
W18-6460,W12-3118,0,0.0334075,"nd sequence kernels. As a tree kernel the Partial Tree Kernel (PTK) is used, while for the sequence kernel, the Subsequence Kernel (SK) (Bunescu and Mooney, 2005) was chosen. Similarly to the previously mentioned QE methods, the kernels are applied to the source and candidate translations, but in addition also on a back-translation. The work presented in this paper builds on this method, by additionally using kernel functions for pseudo-references. Pseudoreferences have been utilized before in the context of QE, but as a support for the generation of features, like for example in the work of (Soricut et al., 2012), (Shah et al., 2013) or (Scarton and Specia, 2014). In (Scarton and Specia, 2014) BLEU and TER were applied to the candidate translation and pseudo-references and their scores were used as additional features in the context of document level QE. 3 4 Evaluation The evaluation was performed measuring the correlation between the TSKQE scores and the HTER gold standards. This was achieved by computing the Pearson correlation coefficient, which results in a number between -1 and 1. A score of 1 indicates that there is a perfect agreement between the two sets of scores, while a score of -1 would su"
W18-6460,2011.eamt-1.32,0,0.0247837,"Menzel, 2017) is presented. Our earlier version of the metric was based on learning HTER scores using tree and sequence kernels. The kernel functions were applied not only on the source segments and the candidate translations, but also on the back-translations of the MT output into the source language. The back-translations were obtained using an online MT system. The extension proposed in this paper uses the same input data. In addition, however, the ker2 Related work The benefit of kernel functions has already been investigated in the context of Quality Estimation. In the work presented by (Hardmeier, 2011) and further expanded in (Hardmeier et al., 2012), tree kernel functions in addition to feature vectors are used to predict MT output quality. Both constituency and dependency parse trees were considered, with the Subset Tree Kernels (Collins and Duffy, 2001) being applied to the former and the Partial Tree Kernel (Moschitti, 2006a)(Moschitti, 2006b) to the latter. The evaluation results revealed that the integration of tree kernels can prove beneficial when compared to the strictly feature based QE systems. 776 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared"
W18-6460,W12-3112,0,0.132634,"rsion of the metric was based on learning HTER scores using tree and sequence kernels. The kernel functions were applied not only on the source segments and the candidate translations, but also on the back-translations of the MT output into the source language. The back-translations were obtained using an online MT system. The extension proposed in this paper uses the same input data. In addition, however, the ker2 Related work The benefit of kernel functions has already been investigated in the context of Quality Estimation. In the work presented by (Hardmeier, 2011) and further expanded in (Hardmeier et al., 2012), tree kernel functions in addition to feature vectors are used to predict MT output quality. Both constituency and dependency parse trees were considered, with the Subset Tree Kernels (Collins and Duffy, 2001) being applied to the former and the Partial Tree Kernel (Moschitti, 2006a)(Moschitti, 2006b) to the latter. The evaluation results revealed that the integration of tree kernels can prove beneficial when compared to the strictly feature based QE systems. 776 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 776–781 c Belgium, Brussels,"
W18-6460,C14-1194,0,0.0406973,"Missing"
W98-0509,P98-1086,1,0.882566,"Missing"
W98-0509,P90-1005,0,0.790249,"native Parsing The idea of eliminative parsing is not a novel one and virtually every tagger can be considered a candidate elimination procedure which removes items from the maximum set of tags according to different decision criteria. Interestingly, dependency-based parsing can be viewed as a generalized tagging procedure. One of the first parsing systems which built on this property is the Constraint Grammar approach (Karlsson et al., 1995). Underspecified dependency structures are represented as syntactic tags 1 and disambiguated by a set of constraints that exclude inappropriate readings. Maruyama (1990) first tried to extend the idea to allow the treatment of complete dependency structures. Therefore, he has to generalize the notion of a&quot;tag&quot; to pairs consisting of a label and the identifier of the dominating node, i. e., the tagset needs to become sensitive to the individual tokens of the utterance under consideration sacrificing the status of the tagset being fixed a-priori. As in the case of atomic tags, constraints are specified which delete inappropriate dependency relations from the initial space of possibilities. The approach is not restricted to linear input strings but can also trea"
