2020.dt4tp-1.2,2020.inlg-1.14,0,0.0803594,"pˆf lat and pf lat respectively for any 1 ≤ i ≤ m (both plans are of the same length – the shorter plan padded with empty lists when needed). We use Kendall’s ranking correlation coefficient to define: P 5 s0i ∩si Experiments We compare baseline data-to-text models which are trained to map end-to-end WebNLG 2020 data to text using the same T5 transformer-based architecture with the modular architecture (Planner, Realizer) where each of the modules is trained separately. We compare two end-to-end baselines: The first is a pre-trained T5 model which has shown promising results on data-to-text (Kale, 2020). It is fine-tuned to generate text given input triplets. In the second baseline, we use the same T5 backbone with a teacher exposure strategy during finetuning: each entry is composed of the input triplets as before concatenated with an incomplete prefix of the desired reference text that contains complete sentences. In this approach, the model learns to complete text given all triplets and a text prefix. The third model is the modular (Planner, Realizer) pipeline described above. Results (Table 1) indicate overall improvement in BLEU scores when using the modular approach. To assess the impa"
2020.dt4tp-1.2,2020.acl-main.703,0,0.0367905,"decisions could help explore. 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 1 Introduction 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 Traditional NLG pipelines distinguish distinct subtasks addressed by a generation system, including content determination, text structuring, sentence aggregation, lexicalization and surface realization (Gatt and Krahmer, 2018). Recent work on neural NLG has blurred the distinction among these sub-tasks and encouraged data-driven end-to-end approaches, such as transformer-based encoderdecoder architectures (Lewis et al., 2020). Recent data to text generation approaches are revisiting this decision, and show the benefit of dividing the full task into two steps: planning and realization (Moryossef et al., 2019; Castro Ferreira et al., 2019). The goal of micro-planning is to organize the input raw data into an interpretable and coherent information structure. Realization is then applied on this structure to generate coherent text that covers all the expected content without redundancy and without introducing unintended 1 061 062 063 064 065 066 067 068 069 070 071 072 073 074 075 076 077 078 079 080 081 082 083 084 08"
2020.dt4tp-1.2,D19-1052,0,0.033739,"Missing"
2020.dt4tp-1.2,N19-1236,0,0.0255365,"Missing"
2020.dt4tp-1.2,W04-2407,0,0.267742,"Missing"
2020.dt4tp-1.2,P17-1017,0,0.157611,"three new directions: (1) we study the extent to which a robust learned planning module can be derived (as opposed to a rule-based planning method); (2) we investigate whether an independent planning quality metric can be established, and the extent to which it correlates with end to end text quality metrics; (3) finally, we investigate specific aspects in realization that are directly related to micro-planning and cohesion and the extent to which good plans control their usage. Applying learning methods to solve the task of planning is difficult for two main reasons: (1) available datasets (Gardent et al., 2017a,b) do not reward variability in plans. They contain a few pairs (data, text) for a given input (usually 3 to 5 variants per input), but there is no incentive to demonstrate a variety of plans to realize the same input; an ideal dataset to learn planning would instead hold different paraphrases for each entry based on changes in micro-planning; (2) Given a target text to generate, micro-plannings are not observable. One can come up with methods to derive a plan from a given text but the nature of the plan, how it is related to We aim to prove the usefulness of separating data to text generati"
2020.lrec-1.727,P15-1039,0,0.220679,"ed in most research addressing cross-lingual SRL and linguistic annotation (Yarowsky et al., 2001; Pad´o, 2007; Pad´o and Lapata, 2009; van der Plas et al., 2011). In this line of work, we start with a resource-rich source language, usually English, and a parallel corpus of English and a resource-poor target language. The English sentences are annotated using an automatic tool, a semantic role labeling model in our case, and using word alignment, the annotations are projected to the target language. This process is called “direct projection.” The idea of “filtered projection” is introduced in Akbik et al. (2015): only alignments which satisfy the suggested filters are kept, while the rest are discarded. Such filters include verb filter (discard sentences where the predicate is not aligned to a verb), and translation filter (discard sentences where the aligned predicate is not a translation of the source predicate). We find in this work that filtered projection is essential to produce reliable annotations in Hebrew. In the rest of the paper, we present related work in the field of cross-lingual SRL annotation projection and the method we used to construct a Hebrew SRL dataset starting from a large par"
2020.lrec-1.727,W19-0417,0,0.0480726,"Missing"
2020.lrec-1.727,P98-1013,0,0.83547,"tilingual BERT transformer model, and provide the first available baseline model for Hebrew SRL as a reference point. The code we provide is generic and can be adapted to other languages to bootstrap SRL resources. Keywords: SRL, FrameNet, PropBank, Hebrew, Cross-lingual Linguistic Annotations 1. Introduction Semantic role labeling (SRL) is the task that consists of annotating sentences with labels that answer questions such as “Who did what to whom, when, and where?”, the answers to these questions are called “roles.” Two major SRL annotation schemes have been used in recent years, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Research in SRL has generated fullyannotated corpora in various languages, several CoNLL shared tasks, and many automatic SRL systems. Besides its important theoretical role, SRL has been found useful for information extraction (Christensen et al., 2010; Stanovsky et al., 2018) and question answering (FitzGerald et al., 2018). In this work, we set out to contribute a new Hebrew corpus and lexical resources to further the advancement of the Hebrew FrameNet Project and to provide the first SRL resource covering both FrameNet and PropBank annotations (Hayoun a"
2020.lrec-1.727,W11-4606,0,0.0215038,"sion 1.7 which consists of 1,221 frames, 13,572 LUs, and 11,428 FEs. For training purposes, there are 10,147 sentences in 107 fullyannotated texts. 1.2. FrameNet in other Languages A great effort is made to expand FrameNet to other languages. As of today, FrameNets have been developed in Finnish (Lind´en et al., 2017), Spanish (Subirats and Petruck, 2003), German (Burchardt et al., 2006), Japanese (Ohara et al., 2004), Chinese (You and Liu, 2005), Korean (Nam et al., 2014), Brazilian Portuguese (Torrent and Ellsworth, 2013), Swedish (Borin et al., 2010), French (Candito et al., 2014), Danish (Bick, 2011), Polish (Zawisławska et al., 2008), Italian (Tonelli and Pianta, 2008), Slovenian (L¨onneker-Rodman et al., 2008), and of course, Hebrew (Petruck, 2005; Hayoun and Elhadad, 2016). The methods to develop FrameNets presented in these papers are quite similar: assume the universality of the English frame inventory, and under that assumption, tag, either manually or semi-automatically, sentences in the desired language using this inventory. Almost every language has its specific corner cases where the English frames are insufficient. In those cases, usually new frames are created specifically for"
2020.lrec-1.727,burchardt-etal-2006-salsa,0,0.178854,"Missing"
2020.lrec-1.727,candito-etal-2014-developing,0,0.0461194,"Missing"
2020.lrec-1.727,W10-0907,0,0.0173613,"notations 1. Introduction Semantic role labeling (SRL) is the task that consists of annotating sentences with labels that answer questions such as “Who did what to whom, when, and where?”, the answers to these questions are called “roles.” Two major SRL annotation schemes have been used in recent years, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Research in SRL has generated fullyannotated corpora in various languages, several CoNLL shared tasks, and many automatic SRL systems. Besides its important theoretical role, SRL has been found useful for information extraction (Christensen et al., 2010; Stanovsky et al., 2018) and question answering (FitzGerald et al., 2018). In this work, we set out to contribute a new Hebrew corpus and lexical resources to further the advancement of the Hebrew FrameNet Project and to provide the first SRL resource covering both FrameNet and PropBank annotations (Hayoun and Elhadad, 2016). Our approach bootstraps Hebrew SRL annotations by projecting predicted annotations from English to Hebrew aligned sentences. To ensure that the resulting Hebrew annotations are reliable, we design confidence metrics on both the original English annotations and the projec"
2020.lrec-1.727,J14-1002,0,0.0372638,"ts-of-speech are Nouns with 22M instances, personal pronouns with 13M instances, and verbs with 13M instances (including modals). Dependency parse tree have an average depth of 2.96, which confirms that the average sentence is very short and simple, containing most often a single predicate with its arguments. We discuss below that as part of the overall preprocessing pipeline, we filter sentences that are either too short or too shallow to be of interest for SRL purposes. 5936 Figure 1: Overall SRL dataset production pipeline 3.3. English Preprocessing We apply the pre-trained SEMAFOR system (Das et al., 2014) on English sentences to obtain syntax and SRL annotations. As part of SEMAFOR’s pipeline, pre-processing is done on the input sentences using MaltParser (Nivre et al., 2007) pre-trained on sections 02-21 of the WSJ section of the Penn Treebank. On the English side, the dataset contains 148M tokens consisting of 1.5M types, with sentence length averaging at 6.45. 41M of them are nouns, 36M personal pronouns, and 23M verbs. Compare the 36M personal pronouns in English with 13M in Hebrew. This is a known syntactic aspect in Hebrew where personal pronouns are often unmarked and understood from th"
2020.lrec-1.727,D19-1056,0,0.0177585,"jections significantly improves on previous work: for example, for French, the reported performance for argument exact match is F1=80.0 compared to the 65.0 reported above. More recently, Aminian et al. (2019) presented a deep bidirectional character-level LSTM encoder-decoder model which uses annotation projection to label new languages, without using any syntactic features such as part-of-speech tags and dependency trees. This end to end model improves slightly over the baseline results presented in Akbik et al. (2015). A recent entry to the list of automatic cross-lingual SRL systems is by Daza and Frank (2019), which does not use annotation projection in order to label semantic roles in a different language, but rather learns to simultaneously translate and label the input sentence. The reported performance F1=77.2 for German and F1=72.4 for French. 3. Data Collection We adopt in this work the method of filtered projection of Akbik et al. (2015), and present a pipeline of filters that we introduce to control the quality of the generated dataset. We start from a noisy collection of aligned documents in large quantities (23.7M aligned sentences) in the OpenSubtitles 2016 dataset (Lison and Tiedemann,"
2020.lrec-1.727,N13-1073,0,0.0463867,"quent - 3.9M instances of E N TITY , 3.3M instances of AGENT , and 3.1M instances of T HEME. After the complete filtering pipeline, however, the most frequent frames are S TATEMENT, A RRIVING, B ECOMING, M OTION and K ILLING (reflecting the violent nature of movies) and the most frequent frame elements are T HEME, S PEAKER, M ESSAGE, AGENT and G OAL. 3.5. Computing Word Alignments The original data obtained from OpenSubtitles provides aligned sentence pairs. In order to project SRL annotations from English to Hebrew, we must compute word alignment. To this end, we use the fast align method of Dyer et al. (2013) on the set of aligned sentences. The output provides a directional mapping from English tokens in a sentence to the corresponding tokens in the associated Hebrew sentence. This alignment is not necessarily one to one. The word alignment process identified 181M word pairs. On average, a single English token is mapped to 1.25 Hebrew after segmentation. Of these 181M pairs, 115M are one-to-one alignments, 29M are cases of 1-n alignments. In the case of a 1-n alignment, the single English token is mapped to multiple Hebrew tokens and these 29M cases cover 66M pairs. Within the 181M collected pair"
2020.lrec-1.727,P18-1191,0,0.0207214,"nsists of annotating sentences with labels that answer questions such as “Who did what to whom, when, and where?”, the answers to these questions are called “roles.” Two major SRL annotation schemes have been used in recent years, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Research in SRL has generated fullyannotated corpora in various languages, several CoNLL shared tasks, and many automatic SRL systems. Besides its important theoretical role, SRL has been found useful for information extraction (Christensen et al., 2010; Stanovsky et al., 2018) and question answering (FitzGerald et al., 2018). In this work, we set out to contribute a new Hebrew corpus and lexical resources to further the advancement of the Hebrew FrameNet Project and to provide the first SRL resource covering both FrameNet and PropBank annotations (Hayoun and Elhadad, 2016). Our approach bootstraps Hebrew SRL annotations by projecting predicted annotations from English to Hebrew aligned sentences. To ensure that the resulting Hebrew annotations are reliable, we design confidence metrics on both the original English annotations and the projection method itself. We describe previous work in multilingual FrameNet and"
2020.lrec-1.727,L16-1688,1,0.939157,"., 1998) and PropBank (Palmer et al., 2005). Research in SRL has generated fullyannotated corpora in various languages, several CoNLL shared tasks, and many automatic SRL systems. Besides its important theoretical role, SRL has been found useful for information extraction (Christensen et al., 2010; Stanovsky et al., 2018) and question answering (FitzGerald et al., 2018). In this work, we set out to contribute a new Hebrew corpus and lexical resources to further the advancement of the Hebrew FrameNet Project and to provide the first SRL resource covering both FrameNet and PropBank annotations (Hayoun and Elhadad, 2016). Our approach bootstraps Hebrew SRL annotations by projecting predicted annotations from English to Hebrew aligned sentences. To ensure that the resulting Hebrew annotations are reliable, we design confidence metrics on both the original English annotations and the projection method itself. We describe previous work in multilingual FrameNet and cross-lingual SRL. We then describe the large unannotated dataset we selected, OpenSubtitles, and the method we use to align documents and sentences between English (EN) and Hebrew (HE), and to project SRL annotations from EN to HE. We present the data"
2020.lrec-1.727,L16-1147,0,0.0305488,"aza and Frank (2019), which does not use annotation projection in order to label semantic roles in a different language, but rather learns to simultaneously translate and label the input sentence. The reported performance F1=77.2 for German and F1=72.4 for French. 3. Data Collection We adopt in this work the method of filtered projection of Akbik et al. (2015), and present a pipeline of filters that we introduce to control the quality of the generated dataset. We start from a noisy collection of aligned documents in large quantities (23.7M aligned sentences) in the OpenSubtitles 2016 dataset (Lison and Tiedemann, 2016). We apply filters of different types on this data: starting with language identification, we push the data in Hebrew and English through automatic NLP annotation (POS, dependency parsing and SRL for English), we compute wordlevel alignment. We then design a method to rank annotated sentence pairs in terms of syntactic plausibility (that is how likely it is that the English and Hebrew parse trees will project onto each other in a clean manner). Modern Hebrew is characterized by a rich morphological system which impacts annotation schemas, especially because function words are often agglutinate"
2020.lrec-1.727,Q19-1003,0,0.0220004,"dataset. The removal of noise consists of running fastText’s language detection model (Joulin et al., 2016b; Joulin et al., 2016a) on each pair, and keeping only English-Hebrew pairs. After this first filtering, we have 22.4M sentence pairs. 3.2. Hebrew Preprocessing The importance of Hebrew preprocessing in our work is twofold: (a) we use features from the dependency parse tree later on in the pipeline, and (b) it acts as a quality gate for our data. The preprocessing pipeline consists of morphological analysis, morphological disambiguation, and dependency parsing, all using the YAP system (More et al., 2019). In Hebrew, common words including prepositions, conjunctions and articles are written in an aggregated manner. For example, the written form for the phrase “in the house” (be ha bayit) will be written as a single token (babayit). For the goal of SRL, it is particularly relevant to segment such compound tokens so that prepositions and conjunctions can be properly aligned with their English counterparts and projected with the corresponding English sentences. Before segmentation, the data includes 118M tokens from 2.4M word types. We predict that after segmentation, the number of tokens will ri"
2020.lrec-1.727,J05-1004,0,0.237267,"and provide the first available baseline model for Hebrew SRL as a reference point. The code we provide is generic and can be adapted to other languages to bootstrap SRL resources. Keywords: SRL, FrameNet, PropBank, Hebrew, Cross-lingual Linguistic Annotations 1. Introduction Semantic role labeling (SRL) is the task that consists of annotating sentences with labels that answer questions such as “Who did what to whom, when, and where?”, the answers to these questions are called “roles.” Two major SRL annotation schemes have been used in recent years, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Research in SRL has generated fullyannotated corpora in various languages, several CoNLL shared tasks, and many automatic SRL systems. Besides its important theoretical role, SRL has been found useful for information extraction (Christensen et al., 2010; Stanovsky et al., 2018) and question answering (FitzGerald et al., 2018). In this work, we set out to contribute a new Hebrew corpus and lexical resources to further the advancement of the Hebrew FrameNet Project and to provide the first SRL resource covering both FrameNet and PropBank annotations (Hayoun and Elhadad, 2016). Our approach boo"
2020.lrec-1.727,P16-1162,0,0.0281991,"P]. Shi and Lin (2019) report a performance of F1=82.7 on argument identification on CoNLL 2009 in English. 2 Mapping non-core elements such as space or time between FrameNet and PropBank is more interesting semantically, but we found it challenging to perform this mapping automatically. Noisy mapping would increase the risk of producing bad PropBank annotations, and we end up with a good enough number of sentences with the more stringent filter. 3 https://github.com/allenai/allennlp/ blob/master/training_config/bert_base_srl. jsonnet 4.2. Evaluation As BERT works with WordPiece tokenization (Sennrich et al., 2016), we hypothesized that the segmented Hebrew words, i.e., the phrase “and from you / ve min ata” is one Hebrew word which is segmented into three words, would perform worse, as BERT has not seen segmented sentences. To this end, we unsegment the sentences back to their original form (joining “and from you / ve min ata” back into one token “vemimkha”) to see if this would improve performance. Table 2 shows that this hypothesis is incorrect, as training with both segmented and unsegmented tokens provides roughly equivalent performance. 5. Conclusion We present the first available SRL resource in"
2020.lrec-1.727,N18-1081,0,0.0251901,"Semantic role labeling (SRL) is the task that consists of annotating sentences with labels that answer questions such as “Who did what to whom, when, and where?”, the answers to these questions are called “roles.” Two major SRL annotation schemes have been used in recent years, FrameNet (Baker et al., 1998) and PropBank (Palmer et al., 2005). Research in SRL has generated fullyannotated corpora in various languages, several CoNLL shared tasks, and many automatic SRL systems. Besides its important theoretical role, SRL has been found useful for information extraction (Christensen et al., 2010; Stanovsky et al., 2018) and question answering (FitzGerald et al., 2018). In this work, we set out to contribute a new Hebrew corpus and lexical resources to further the advancement of the Hebrew FrameNet Project and to provide the first SRL resource covering both FrameNet and PropBank annotations (Hayoun and Elhadad, 2016). Our approach bootstraps Hebrew SRL annotations by projecting predicted annotations from English to Hebrew aligned sentences. To ensure that the resulting Hebrew annotations are reliable, we design confidence metrics on both the original English annotations and the projection method itself. We de"
2020.lrec-1.727,tiedemann-2012-parallel,0,0.0480291,"because function words are often agglutinated together with the content words they modify. In terms of syntax, Modern Hebrew is mainly an SVO word order with no explicit case marking (except for pronouns). Personal pronouns are often skipped when the verb morphological inflection provides clear cues (e.g., “ahav-ti / I liked” instead of “ani ahav-ti”). The overall data annotation process is summarized in Fig. 1. We describe the steps of this process in the following paragraphs. 1 https://github.com/System-T/ UniversalPropositions 3.1. Filtering Noisy Subtitles The OpenSubtitles 2016 dataset (Tiedemann, 2012) provides aligned data in 62 languages from movie subtitles. It specifically includes 23.7 million English-Hebrew sentence pairs. Due to the nature of the dataset, some of the pairs are very noisy. Examples of such noise include special Unicode characters unrelated to the text, e.g., musical note symbol to let hearing impaired viewers know that a song is playing, joined words (either typos or OCR artefacts) such “Amanonce” instead of “A man once,” and pairs in which the Hebrew part is actually found in English, not translated. Given the large number of available sentence pairs, we filter noisy"
2020.lrec-1.727,tonelli-pianta-2008-frame,0,0.0482657,"11,428 FEs. For training purposes, there are 10,147 sentences in 107 fullyannotated texts. 1.2. FrameNet in other Languages A great effort is made to expand FrameNet to other languages. As of today, FrameNets have been developed in Finnish (Lind´en et al., 2017), Spanish (Subirats and Petruck, 2003), German (Burchardt et al., 2006), Japanese (Ohara et al., 2004), Chinese (You and Liu, 2005), Korean (Nam et al., 2014), Brazilian Portuguese (Torrent and Ellsworth, 2013), Swedish (Borin et al., 2010), French (Candito et al., 2014), Danish (Bick, 2011), Polish (Zawisławska et al., 2008), Italian (Tonelli and Pianta, 2008), Slovenian (L¨onneker-Rodman et al., 2008), and of course, Hebrew (Petruck, 2005; Hayoun and Elhadad, 2016). The methods to develop FrameNets presented in these papers are quite similar: assume the universality of the English frame inventory, and under that assumption, tag, either manually or semi-automatically, sentences in the desired language using this inventory. Almost every language has its specific corner cases where the English frames are insufficient. In those cases, usually new frames are created specifically for the language in question. An example of a corner case of Hebrew is mul"
2020.lrec-1.727,P11-2052,0,0.0613423,"Missing"
2020.lrec-1.727,H01-1035,0,0.147692,"tences in the desired language using this inventory. Almost every language has its specific corner cases where the English frames are insufficient. In those cases, usually new frames are created specifically for the language in question. An example of a corner case of Hebrew is multi-word lexical units like give up and turn in - in Hebrew, these LUs might not appear as contiguous words. This case was solved by allowing annotation of discontinuous units. 1.3. Annotation Projection The idea of annotation projection is used in most research addressing cross-lingual SRL and linguistic annotation (Yarowsky et al., 2001; Pad´o, 2007; Pad´o and Lapata, 2009; van der Plas et al., 2011). In this line of work, we start with a resource-rich source language, usually English, and a parallel corpus of English and a resource-poor target language. The English sentences are annotated using an automatic tool, a semantic role labeling model in our case, and using word alignment, the annotations are projected to the target language. This process is called “direct projection.” The idea of “filtered projection” is introduced in Akbik et al. (2015): only alignments which satisfy the suggested filters are kept, while the rest"
2020.lrec-1.727,C98-1013,0,\N,Missing
2020.lrec-1.727,W18-2501,0,\N,Missing
2020.lrec-1.727,E17-2068,0,\N,Missing
2021.findings-emnlp.259,2021.naacl-main.9,1,0.759325,"sk, observing that vanilla BERTstyle often masks ungrounded words like “umm” or “yeah”. We share the same motivation to mask highly visual words. 6.3 Challenges in VQA generalization Visual understanding Language and vision tasks inherently demand deep understanding of both the text and the image. However, many works show that models can succeed on VQA datasets using strong language priors, and by relying on superficial cues, and there are still challenges to overcome for tasks with more compositional structure (Jabri et al., 2016; Zhang et al., 2016; Goyal et al., 2017; Agarwal et al., 2020; Bitton et al., 2021; Dancette et al., 2021). Balanced datasets such as VQA 2.0 (Goyal et al., 2017) and GQA (Hudson and Manning, 2019) have been presented to address these challenges. Novel models with richer visual representations (Zhang et al., 2021) were also presented, and some works tried to encourage the model to look at the “correct” image regions (Liu et al., 2021; Yang et al., 2020). Bias Yang et al. (2021) and Hendricks et al. (2018) have shown that attention-based visionlanguage models suffer from bias that misleads the attention module to focus on spurious correlations in training data, and leads to"
2021.findings-emnlp.259,2020.tacl-1.5,0,0.0636908,"Missing"
2021.findings-emnlp.259,D19-1514,0,0.0944787,"ngs might be that the model is evaluated mostly on retrieving objects, and had we tested it on other classes, its performance would have substantially decreased. To test this hypothesis, we inspect the same model’s performance on questions with answers from different semantic types. To do so, we experiment with the GQA dataset, which includes partitioning of the answers into different semantic types, including Objects, Relations (subject or object of a described relation, e.g., “what is the girl wearing?&quot;), and Attributes (the properties or position of an object). Many works (Lu et al., 2019; Tan and Bansal, 2019; Chen et al., 2020) assume that a VLP model should include an MLM component that is capable of predicting every masked token, including objects, properties, but also stop words and punctuation. Does a model that uses our Objects strategy, and The results for the semantic type partition are masks only objects, learn to complete words from presented in Table 3. Comparing between the other classes? If not, can such a pre-training strat- models trained with Objects and Baseline MLM egy be effective? masking strategies, the Objects masking strategy To examine this questions, we extend the experi-"
2021.naacl-main.9,P19-1164,1,0.793014,"s from the scene graph. Bottom: relations among the objects in the scene graph. First line at the top is the original QA pair, while the following 3 lines show our pertubated questions: replacing a single element in the question (a fence) with other options (a wall, men, an elephant), leading to a change in the output label. For each QA pair, the LXMERT predicted output is shown. Introduction the out-of-domain performance of these models is often severely deteriorated (Jia and Liang, 2017; Ribeiro et al., 2018; Gururangan et al., 2018; Geva et al., 2019; McCoy et al., 2019; Feng et al., 2019; Stanovsky et al., 2019). Recently, Kaushik et al. (2019) and Gardner et al. (2020) introduced the contrast sets approach to probe out-of-domain generalization. Contrast sets are constructed via minimal modifications to test inputs, such that their label is modified. For example, in Fig. 1, replacing “a fence” with “a wall”, changes the answer NLP benchmarks typically evaluate in-distribution generalization, where test sets are drawn i.i.d from a distribution similar to the training set. Recent works showed that high performance on test sets sampled in this manner is often achieved by exploiting systematic gaps, anno"
2021.naacl-main.9,2020.emnlp-main.158,0,0.0161646,"his is due to model architecture or dataset design. Bogin et al. (2020) claim that both of these models are prone to fail on compositional generalization because they do not decompose the problem into smaller sub-tasks. Our results support this claim. On the other hand, it is possible that a different dataset could prevent these models from finding shortcuts. Is there a dataset that can prevent all shortcuts? Our automatic method for creating contrast sets allows us to ask those questions, while we believe that future work in better training mechanisms, as suggested in Bogin et al. (2020) and Jin et al. (2020), could help in making more robust models. We proposed an automatic method for creating contrast sets for VQA datasets that use annotated scene graphs. We created contrast sets for the GQA dataset, which is designed to be compositional, balanced, and robust against statistical biases. We observed a large performance drop between the original and augmented sets. As our contrast sets Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on"
2021.naacl-main.9,D19-1514,0,0.122313,"Missing"
2021.naacl-main.9,D18-1009,1,0.746347,"Missing"
2021.naacl-main.9,2020.blackboxnlp-1.12,0,0.0341525,"t 94 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 94–105 June 6–11, 2021. ©2021 Association for Computational Linguistics 2 from “Yes” to “No”. Since such perturbations introduce minimal additional semantic complexity, robust models are expected to perform similarly on the test and contrast sets. However, a range of NLP models severely degrade in performance on contrast sets, hinting that they do not generalize well (Gardner et al., 2020). Except two recent exceptions for textual datasets (Li et al., 2020; Rosenman et al., 2020), contrast sets have so far been built manually, requiring extensive human effort and expertise. Automatic Contrast Set Construction To construct automatic contrast sets for GQA we first identify a large subset of questions requiring specific reasoning skills (§2.1). Using the scene graph representation, we perturb each question in a manner which changes its gold answer (§2.2). Finally, we validate the automatic process via crowdsourcing (§2.3). 2.1 Identifying Recurring Patterns in GQA The questions in the GQA dataset present a diverse set of modelling challenges, as e"
2021.naacl-main.9,P19-1472,0,0.0438219,"Missing"
2021.unimplicit-1.3,D19-1299,0,0.0393216,"Missing"
2021.unimplicit-1.3,D17-1239,0,0.0606337,"Missing"
adler-etal-2008-tagging,J93-2004,0,\N,Missing
adler-etal-2008-tagging,W07-0808,1,\N,Missing
adler-etal-2008-tagging,P06-1087,1,\N,Missing
adler-etal-2008-tagging,P06-1084,1,\N,Missing
adler-etal-2008-tagging,dejean-2000-evaluate,0,\N,Missing
adler-etal-2008-tagging,W00-0730,0,\N,Missing
C90-3018,P84-1055,0,0.0458839,"from the meaning of each conjunct. In our work, we use a similar approach for the definition of connectives, but, since we work on generation (as opposed to interpretation), we describe the meaning of connectives as sets of constraints that must be satisfied between the conjuncts as opposed to &quot;instructions.&quot; We use the notion of thematization procedure to account for the homogeneousness condition (of. Section 5). In this paper, we concentrate on the distinctions between similar connectives rather than on the general properties of the class of connectives. Work on the structure of discourse (Cohen, 1984, Reichman, 1985, Grosz & Sidner, 1986) has identified the role of connectives in marking structural shifts. This work generally relies on the notion that hearers maintain a discourse model (which is often represented using stacks). Connectives give instructions to the hearer on how to update the discourse model. For example, &quot; n o w &quot; (Hirschberg & Litman, 1987) can indicate that the hearer needs to push or pop the current stack of the model. When used in this manner, connectives are called &quot;cue (or clue) words.&quot; This work indicates that the role of connectives is not only to indicate a logic"
C90-3018,P90-1020,1,0.904586,"n: Propositional Content, Argumentative Derivation, Functional Status, Speech Act and Utterance Act. In a complete text generation system, the &quot;deep component ''4 given certain information to convey, decides when it is possible to make some of it implicit by using a certain thematization procedure. The effect is to remove certain discoupse entities from the propositional content to be generated. Using a non-PC thematization procedure therefore allows to implicitly discuss certain features of an utterance that may be difficult to address explicitly. The deep module we are currently developing (Elhadad, 1990a) will use politeness constraints (Brown & Levinson, 1987) to decide which thematization is most appropriate. CUE VS. NON-CUE USAGE: Thematization procedures allow us to distinguish cue and non-cue usages of connectives. When a connective links on a feature that is not the propositional content, it does not affect the truth conditions of the propositions, at least in the traditional view. This suggests that non-content linking is in some ways similar to the cue/non-cue distinction discussed in section 2. Our approach does therefore capture this distinction, but with several differences. It de"
C90-3018,J86-3001,0,0.205758,"junct. In our work, we use a similar approach for the definition of connectives, but, since we work on generation (as opposed to interpretation), we describe the meaning of connectives as sets of constraints that must be satisfied between the conjuncts as opposed to &quot;instructions.&quot; We use the notion of thematization procedure to account for the homogeneousness condition (of. Section 5). In this paper, we concentrate on the distinctions between similar connectives rather than on the general properties of the class of connectives. Work on the structure of discourse (Cohen, 1984, Reichman, 1985, Grosz & Sidner, 1986) has identified the role of connectives in marking structural shifts. This work generally relies on the notion that hearers maintain a discourse model (which is often represented using stacks). Connectives give instructions to the hearer on how to update the discourse model. For example, &quot; n o w &quot; (Hirschberg & Litman, 1987) can indicate that the hearer needs to push or pop the current stack of the model. When used in this manner, connectives are called &quot;cue (or clue) words.&quot; This work indicates that the role of connectives is not only to indicate a logical or conceptual relation, but also to"
C90-3018,P87-1023,0,0.0955341,"cedure to account for the homogeneousness condition (of. Section 5). In this paper, we concentrate on the distinctions between similar connectives rather than on the general properties of the class of connectives. Work on the structure of discourse (Cohen, 1984, Reichman, 1985, Grosz & Sidner, 1986) has identified the role of connectives in marking structural shifts. This work generally relies on the notion that hearers maintain a discourse model (which is often represented using stacks). Connectives give instructions to the hearer on how to update the discourse model. For example, &quot; n o w &quot; (Hirschberg & Litman, 1987) can indicate that the hearer needs to push or pop the current stack of the model. When used in this manner, connectives are called &quot;cue (or clue) words.&quot; This work indicates that the role of connectives is not only to indicate a logical or conceptual relation, but also to indicate the structural organization of discourse. The distinction between cue and non-cue usages is an important one, and we also attempt to capture cue usages, but the structural indication (which often has the form of just push or pop) under-constrains the choice of a cue word -. it does not control how to choose among th"
C92-2096,P90-1020,1,0.801741,"and&apos;))))) ;; Neutral verbs ((lex ((ralt (&apos;contain"" ""involve&apos;)))))))) &lt;&lt;other concepts&gt;&gt;))) Figure 5: Fragment of the grammar (6) AI involves some programming. ((cat relation) (name topics-of) (roles ((class ((cat class) (name AI)}) (topics (Icat set} (kind ((cat topic))) (cardinality I} (Intenslon ((cat relation) (name area-of) (argument (^ roles topic}) (roles I(toplc ((cat topic))) (area ((name theory)))) (extension ((cat listl (elements ~(((name logic))))))) (AO The realization component is implemented in FUF, all extended functional unification grammar formalism which we have implemented (Elhadad, 1990, Elhadad, 1992). In the grammar we use, lexical choice and syntactic realization ate interleaved. For example, the choice of the verb is handled by the altemation shown in Figure 5. In this Figme, the notation a l t indicates a disjunction between alternatives; r a l t indicates a random alternation, and is used to indicate that the grammar does not account for the difference between the alternatives; the curly braces notation in pairs of the form ( ( g o ) v a l u e ) indicates that the g o feature is not embedded in the lexical verb constituent unified with the grammar but rather is a top l"
C92-2096,J86-3001,0,0.052299,"Missing"
C92-2096,P88-1020,0,0.199553,"pts to label the relation between propositions. RST (Mann & Thompson, 1987) was t-u&apos;st introduced as a descriptive theory aiming at enmnerating possible rhetodcal relations between discourse segments. RST relations include elaboration, anti-thesis, evidence and solutionhood. A relation connects two text spans, which can be either single propositions or recursively embedded rhetorical relations. One urgument of the relation is marked as its ""nucleus&apos;"" while the others are the ""satellites"" and are all optional. RST was made operational as a technique for planning the structure of paragraphs in (Hovy, 1988a) and (Moore & Paris, 1989). The idea is to attach a commanicative intent with each RST relation and to view the combining of relations into paragraphs as a planning process, decomposing a high-level intention into lowerlevel goals that eventually can be mapped to single propositions. The communicative goals associated with the leaves of the structure are then used to retrieve the content of each proposition fi&apos;om an underlying knowledge base. By making the intentional structure of a paragraph explicit, this work follows the discourse structure theory advanced in (Grosz & Sidner, 1986). Note"
C92-2096,C90-3018,1,0.902448,"Missing"
C92-2096,P89-1025,0,0.195939,"elation between propositions. RST (Mann & Thompson, 1987) was t-u&apos;st introduced as a descriptive theory aiming at enmnerating possible rhetodcal relations between discourse segments. RST relations include elaboration, anti-thesis, evidence and solutionhood. A relation connects two text spans, which can be either single propositions or recursively embedded rhetorical relations. One urgument of the relation is marked as its ""nucleus&apos;"" while the others are the ""satellites"" and are all optional. RST was made operational as a technique for planning the structure of paragraphs in (Hovy, 1988a) and (Moore & Paris, 1989). The idea is to attach a commanicative intent with each RST relation and to view the combining of relations into paragraphs as a planning process, decomposing a high-level intention into lowerlevel goals that eventually can be mapped to single propositions. The communicative goals associated with the leaves of the structure are then used to retrieve the content of each proposition fi&apos;om an underlying knowledge base. By making the intentional structure of a paragraph explicit, this work follows the discourse structure theory advanced in (Grosz & Sidner, 1986). Note also that, since in RST with"
D09-1119,J04-4004,0,0.0251548,"Missing"
D09-1119,P05-1010,0,0.0389206,"Missing"
D09-1119,P98-1034,0,0.0212369,"ng to the same phrase. These tasks are traditionally reduced to a tagging task, in which each word is to be classified as either Beginning a span, Inside a span, or Outside of a span. The decision is based on the word to be classified and its neighbors. Features supporting the classification usually include ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exac"
D09-1119,nivre-etal-2006-maltparser,0,0.563552,"his paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151, c"
D09-1119,A00-2018,0,0.0594379,"Missing"
D09-1119,P06-1055,0,0.0254728,"Missing"
D09-1119,P97-1003,0,0.0370144,"Missing"
D09-1119,P05-1045,0,0.0117487,"Missing"
D09-1119,W01-0521,0,0.0363145,"Missing"
D09-1119,P07-1029,1,0.914318,"act word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142–1151, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP We show that by using a variant of SVM – Anchored SVM Learning (Goldberg and Elhadad, 2007) with a polynomial kernel, one can learn accurate models for English NP-chunking (Marcus and Ramshaw, 1995), base-phrase chunking (Co"
D09-1119,P06-1087,1,0.890779,"strate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances. Specifically, the objective of the P learner is to minimize 21 ||w||2 + C i ξi subject to some margin constraints, where w is a weight vector to be learned and ξi is the misclassification error for instance i. This is equivalent to maximizing the dual problem: PM 1P i=1 αi − 2 i,j αi αj yi yj K(xi , xj ) Another variant is L2-penalty SVM (Koshiba and Abe, 2003), in which there is a quadratic penalty for misclassified instances. Here, the learning objective is to minimize: 1 1 P 2 2 i ξi or"
D09-1119,C02-1054,0,0.0367722,"y more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances. Specifically, the objective of the P learner is to minimize 21 ||w||2 + C i ξi subject to some margin constraints, where w is a weight vector to be learned and ξi is the misclassification error for instance i. This is equivalent to maximizing the dual problem: PM 1P i=1 αi − 2 i,j αi αj yi yj K(xi , xj ) Another variant is L2-penalty SVM (Koshiba and Abe, 2003), in which there is a quadratic penalty for misclassified instances. Here, the learning objective is to min"
D09-1119,W09-1119,0,0.0294297,"Missing"
D09-1119,W00-0726,0,0.707134,"sification usually include ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVM"
D09-1119,N03-1028,0,0.354089,"Missing"
D09-1119,W02-2024,0,0.330806,"ankel Center for Computer Sciences, Ben Gurion University the word forms themselves and properties derived from the word forms, such as prefixes, suffixes, capitalization information, and parts-of-speech. While early approaches to the NP-chunking task (Cardie and Pierce, 1998) relied on part-of-speech information alone, it is widely accepted that lexical information (word forms) is crucial for building accurate systems for these tasks. Indeed, all the better-performing systems in the CoNLL shared tasks competitions for Chunking (Sang and Buchholz, 2000) and Named Entity Recognition (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) make extensive use of such lexical information. Is this belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very"
D09-1119,P01-1069,0,0.0667811,"Missing"
D09-1119,P07-2052,0,0.0232416,"Missing"
D09-1119,P03-1054,0,0.0317059,"Missing"
D09-1119,W00-0730,0,0.45439,"his belief justified? In this paper, we show that the influence of lexical features on such sequence labeling tasks is more complex than is generally assumed. We find that exact word forms aren’t necessary for accurate classification. This observation is important because relying on the exact word forms that appear in a training corpus leads to over-fitting, as well as to larger models. In this work, we focus on learning with Support Vector Machines (SVMs) (Vapnik, 1995). SVM classifiers can handle very large feature spaces, and produce state-of-the-art results for NLP applications (see e.g. (Kudo and Matsumoto, 2000; Nivre et al., 2006)). Alas, when trained on pruned feature sets, in which rare lexical items are removed, SVM models suffer a loss in classification accuracy. It would seem that rare lexical items are indeed crucial for SVM classification performance. However, in Goldberg and Elhadad (2007), we suggested that the SVM learner is using the rare lexical features for singling out hard cases rather than for learning meaningful generalizations. We provide further evidence to support this claim in this paper. 1142 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processin"
D09-1119,N01-1025,0,0.11676,"exical features is not explained by the richness of information such rare features bring to the model. Instead, we believe that rare lexical features help the classifier because they make the data artificially more separable. To demonstrate this claim, we experiment with anchored SVM, which introduces artificial mechanical anchors into the model to achieve separability, and make rare lexical features unnecessary. 3 Learning Method SVM are discriminative, max-margin, linear classifiers (Vapnik, 1995), which can be kernelized. For the formulation of SVMs in the context of NLP applications, see (Kudo and Matsumoto, 2001). SVMs with a polynomial kernel of degree 2 were shown to provide state-of-the-art performance in many NLP application, see for example (Kudo and Matsumoto, 2000; Nivre et al., 2006; Isozaki and Kazawa, 2002; Goldberg et al., 2006). Relation to L2 SVM The classic soft-margin SVM formulation uses L1-penalty for misclassified instances. Specifically, the objective of the P learner is to minimize 21 ||w||2 + C i ξi subject to some margin constraints, where w is a weight vector to be learned and ξi is the misclassification error for instance i. This is equivalent to maximizing the dual problem: PM"
D09-1119,W95-0107,0,0.0274721,"ve an F-score of 90.9. This dataset proved to be quite resilient to feature pruning. Pruning features appearing less than 100 times results in just a slight decrease in F-score. Extremely aggressive pruning, keeping only features appearing more than 1,000 or 1,500 times in the training data, results in a big drop in F-score for the soft-margin SVM (from about 91 to 86). Much less so for the Anchored-SVM. Using Anchored SVM we achieve an F-score of 90.1 after pruning with k = 1, 000. This model has 1207 active features, and 27 unique active lexical forms. 5.2 NP Chunking The goal of this task (Marcus and Ramshaw, 1995) is the identification of non-recursive NPs. We use the data from the CoNLL 2000 shared task: NP chunks are extracted from Sections 15-18 (train) and 20 (test) of the Penn WSJ corpus. POS tagged are automatically assigned by the Brill Tagger. Features: We consider the POS and word-form of each token. P RUNING 0 1 2 5 10 20 50 100 #F EATURES 92,805 46,527 32,583 18,092 10,812 5,952 2,436 1,168 S OFT-M ARGIN 94.12 93.78 93.58 93.42 93.00 92.48 92.33 91.94 A NCHORED 94.08 94.09 94.00 94.01 93.98 93.92 93.96 93.83 Table 2: NP-Chunking results (F-score), with various pruning thresholds. Results are"
D09-1119,W03-0419,0,\N,Missing
D09-1119,C98-1034,0,\N,Missing
E09-1038,W07-2219,1,0.896176,"Missing"
E09-1038,C08-1112,1,0.875724,"Missing"
E09-1038,P08-1083,1,0.830098,"uter Sciences, Ben Gurion University † Funded by the Dutch Science Foundation (NWO), grant number 017.001.271. ‡ Post-doctoral fellow, Deutsche Telekom labs at Ben Gurion University 1 This is not the case with other languages, and also not true for English when adaptation scenarios are considered. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327–335, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 327 morphological structure. This rich structure yields a large number of distinct word forms, resulting in a high OOV rate (Adler et al., 2008a). This poses a serious problem for estimating lexical probabilities from small annotated corpora, such as the Hebrew treebank (Sima’an et al., 2001). Hebrew has a wide coverage lexicon / morphological-analyzer (henceforth, KC Analyzer) available2 , but its tagset is different than the one used by the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew TB tagset is syntactic in nature, while the KC tagset is lexicographic. This difference in perspective yields different performance for parsers induced from tagged data, and"
E09-1038,P06-3009,1,0.912813,"). The remaining question is how to estimate p(hw, tKC i|tKC ). Here, we use either the LexFilter (estimated over all rare events) or LexProbs (estimated via the semisupervised emission probabilities)models, as defined in Section 4.1 above. Parsing without a Segmentation Oracle When parsing real world data, correct token segmentation is not known in advance. For methodological reasons, this issue has either been setaside (Tsarfaty and Sima’an, 2007), or dealt with in a pipeline model in which a morphological disambiguator is run prior to parsing to determine the correct segmentation. However, Tsarfaty (2006) argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in a pipeline model. Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. This model uses a morphological analyzer to construct a lattice over all possible morphological analyses of an input sentence. The arcs of the lattice are hw, ti pairs, and a lattice parser is used to build a parse over the"
E09-1038,adler-etal-2008-tagging,1,0.917245,"uter Sciences, Ben Gurion University † Funded by the Dutch Science Foundation (NWO), grant number 017.001.271. ‡ Post-doctoral fellow, Deutsche Telekom labs at Ben Gurion University 1 This is not the case with other languages, and also not true for English when adaptation scenarios are considered. Proceedings of the 12th Conference of the European Chapter of the ACL, pages 327–335, c Athens, Greece, 30 March – 3 April 2009. 2009 Association for Computational Linguistics 327 morphological structure. This rich structure yields a large number of distinct word forms, resulting in a high OOV rate (Adler et al., 2008a). This poses a serious problem for estimating lexical probabilities from small annotated corpora, such as the Hebrew treebank (Sima’an et al., 2001). Hebrew has a wide coverage lexicon / morphological-analyzer (henceforth, KC Analyzer) available2 , but its tagset is different than the one used by the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew TB tagset is syntactic in nature, while the KC tagset is lexicographic. This difference in perspective yields different performance for parsers induced from tagged data, and"
E09-1038,D07-1022,0,0.0399562,"Section 4.1 above. Parsing without a Segmentation Oracle When parsing real world data, correct token segmentation is not known in advance. For methodological reasons, this issue has either been setaside (Tsarfaty and Sima’an, 2007), or dealt with in a pipeline model in which a morphological disambiguator is run prior to parsing to determine the correct segmentation. However, Tsarfaty (2006) argues that there is a strong interaction between syntax and morphological segmentation, and that the two tasks should be modeled jointly, and not in a pipeline model. Several studies followed this line, (Cohen and Smith, 2007) the most recent of which is Goldberg and Tsarfaty (2008), who presented a model based on unweighted lattice parsing for performing the joint task. This model uses a morphological analyzer to construct a lattice over all possible morphological analyses of an input sentence. The arcs of the lattice are hw, ti pairs, and a lattice parser is used to build a parse over the lattice. The Viterbi parse over the lattice chooses a lattice path, which induces a segmentation over the input sentence. Thus, parsing and segmentation are performed jointly. Lexical rules in the model are defined over the latt"
E09-1038,P08-1043,1,0.708181,"he original Treebank, KC which is the Treebank converted to use the KC Analyzer tagset, and Layered, which is the layered representation described above. The details of the lexical models vary according to the representation we choose to work with. For the TB setting, our lexical rules are of the form 9 Details of the grammar: all functional information is removed from the non-terminals, finite and non-finite verbs, as well as possessive and other PPs are distinguished, definiteness structure of constituents is marked, and parent annotation is employed. It is the same grammar as described in (Goldberg and Tsarfaty, 2008). 331 ttb → w. Only the Baseline models are relevant here, as the tagset is not compatible with that of the external lexicon. For the KC setting, our lexical rules are of the form tkc → w, and their probabilities are estimated as described above. Note that this setting requires our trees to be tagged with the new (KC) tagset, and parsed sentences are also tagged with this tagset. For the Layered setting, we use lexical rules of the form ttb → w. Reliable events are estimated as usual, via relative frequency over the original treebank. For rare events, we estimate p(ttb → w|ttb ) = p(ttb → tkc"
E09-1038,P08-1085,1,0.870295,"Missing"
E09-1038,W07-0808,1,0.858605,"Missing"
E09-1038,P06-1084,1,\N,Missing
J10-4009,J83-1005,0,0.567433,"ional Linguistics Volume 36, Number 4 Chapter 9 expands CFGs into feature structures and uniﬁcation grammars. The authors take this opportunity to tackle more advanced syntax: inversion, unbounded dependency. The material on parsing is good, but too short. In contrast to the section on tagging and chunking, the book does not conclude with a robust working parser. On the conceptual side, I would have liked to see a more in-depth chapter on syntax—a chapter similar in depth to Chapter 21 of Paradigms of AI Programming (Norvig 1992) or the legendary Appendix B of Language as a Cognitive Process (Winograd 1983). In my experience, students beneﬁt from a description of clausal arguments, relative clauses, and complex nominal constructs before they can properly gauge the complexity of syntax. On the algorithmic side, there is no coverage of probabilistic CFGs. The material on PCFGs is mature enough, and there is even excellent code in NLTK to perform tree binarization (Chomsky normal form) and node annotation, which makes it possible to build a competent PCFG constituent-based parser. The connection between probabilistic independence and context-freeness is a wonderful story that is missed in the book."
J13-1007,P06-1084,1,0.938931,"After establishing the tag set, it is relatively straightforward to add lemmas to the lexicon, and the automatic inflection process guarantees good coverage of all the possible inflections. This is much more efficient than annotating enough text to obtain a similar coverage. 2.3.4 Hebrew Morphological Disambiguator. The morphological analyzer provides the possible set of analyses for each token, but does not disambiguate the correct analysis in context. A morphological disambiguator (henceforth “the Hebrew tagger” or “tagger”) was developed by Meni Adler at Ben-Gurion University of the Negev (Adler and Elhadad 2006; Adler 2007; Goldberg, Adler, and Elhadad 2008). After the (extended) morphological analyzer assigns the possible analyses for each token in an 10 http://www.mila.cs.technion.ac.il/mila/files/treebank/Decisions-Corpus1-5001.v1.pdf. 129 Computational Linguistics Volume 39, Number 1 input sentence, the tagger takes the output of the analyzer as input and chooses the single best analysis for the entire sentence (performing both token segmentation of words and part-of-speech assignment for each word). The tagger is an HMM-based sequential model that is trained in a semi-supervised fashion using E"
J13-1007,P08-1083,1,0.932761,"exicon-based morphological analyzer which can assign morphological analyses (prefixes, suffixes, core POS, gender, number, person, etc.) to Hebrew tokens. The lexicon (henceforth the KC Analyzer) is developed and maintained by the Knowledge Center for Processing Hebrew (Itai and Wintner 2008). It is based on a lexicon of roughly 25,000 word lemmas and their inflection patterns. From these, 562,439 unique word forms are derived. These are then prefixed (subject to constraints) by 73 prepositional prefixes. Even with this seemingly large vocabulary, the KC Analyzer’s coverage is not perfect. In Adler et al. (2008a), we present a machine-learning method that is trained on the basis of the analyzer and that can guess possible analyses for words unknown to the analyzer with reasonable accuracies. Using this extension, the analyzer has perfect coverage (even though the quality is obviously better for words that are present in the analyzer’s database). The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussed in depth in BGU Computational Linguistics Group (2008). Creating a resource such as the morphological analyzer for a morphologically rich language is a worthwhile and cost-"
J13-1007,adler-etal-2008-tagging,1,0.954615,"exicon-based morphological analyzer which can assign morphological analyses (prefixes, suffixes, core POS, gender, number, person, etc.) to Hebrew tokens. The lexicon (henceforth the KC Analyzer) is developed and maintained by the Knowledge Center for Processing Hebrew (Itai and Wintner 2008). It is based on a lexicon of roughly 25,000 word lemmas and their inflection patterns. From these, 562,439 unique word forms are derived. These are then prefixed (subject to constraints) by 73 prepositional prefixes. Even with this seemingly large vocabulary, the KC Analyzer’s coverage is not perfect. In Adler et al. (2008a), we present a machine-learning method that is trained on the basis of the analyzer and that can guess possible analyses for words unknown to the analyzer with reasonable accuracies. Using this extension, the analyzer has perfect coverage (even though the quality is obviously better for words that are present in the analyzer’s database). The tag set used by the lexicon/analyzer is lexicographic in nature, and is discussed in depth in BGU Computational Linguistics Group (2008). Creating a resource such as the morphological analyzer for a morphologically rich language is a worthwhile and cost-"
J13-1007,W10-1408,0,0.0884311,"Missing"
J13-1007,W05-0706,0,0.058744,"Missing"
J13-1007,P89-1018,0,0.508769,"the PCFG-LA BerkeleyParser to accept lattice input at inference time. Lattice parsing allows us to preserve the segmentation ambiguity and present it to the parser, instead of committing to a specific segmentation prior to parsing. This way segmentation decisions are performed in the parser as part of the global search for the most probable structure, and can be affected by global syntactic considerations. We show in Section 9 that this methodology is indeed superior to the pipeline approach. Early descriptions of algorithms for parsing over word lattices can be found in Lang (1974, 1988) and Billott and Lang (1989). Lattice parsing was explored in the context of parsing of speech signals by Chappelier et al. (1999), Sima’an (1999), and Hall (2005), and in the context of joint word-segmentation and syntactic disambiguation in Cohen and Smith (2007), Goldberg and Tsarfaty (2008), and Green and Manning (2010). 20 Lattice parsing for Hebrew is explored also in Cohen and Smith (2007). There, lattice arc weights are assigned based on aggregate quantities (forward-backward tagging marginals) derived from a discriminative CRF tagging model. This approach is not ideal from a modeling perspective, as it makes eac"
J13-1007,P11-2037,1,0.903779,"Missing"
J13-1007,W09-3821,0,0.0478236,"Missing"
J13-1007,W09-1008,0,0.0276974,"Missing"
J13-1007,P05-1022,0,0.0427829,"ion mechanism is valid by applying the procedure to the gold-standard trees in the training-set and checking that (1) the propagated features agree with the manually marked ones, and (2) none of the training-set trees were filtered due to agreement violation. We did find a few cases in which the propagated features disagreed with the manually marked ones, and a few gold-standard trees that the mechanism marked as containing an agreement violation. All of these cases were due to mistakes in the manual annotation. Connections to parse-reranking. Our implementation is similar to parse-reranking (Charniak and Johnson 2005; Collins and Koo 2005). Indeed, if we were to model agreement as soft constraints, we could have incorporated this information as features in a reranking model. The filter approach differs in that it poses hard constraints and not soft ones, pruning away parts of the search space entirely. Thus, the use of k-best list is merely a technical detail in our implementation—the agreement information is 146 Goldberg and Elhadad Parsing System for Hebrew easily decomposable and the hard constraints can be efficiently incorporated into the CKY search procedure. 9. Evaluation and Results Data set. For"
J13-1007,D07-1022,0,0.570078,"Missing"
J13-1007,J05-1003,0,0.0303153,"pplying the procedure to the gold-standard trees in the training-set and checking that (1) the propagated features agree with the manually marked ones, and (2) none of the training-set trees were filtered due to agreement violation. We did find a few cases in which the propagated features disagreed with the manually marked ones, and a few gold-standard trees that the mechanism marked as containing an agreement violation. All of these cases were due to mistakes in the manual annotation. Connections to parse-reranking. Our implementation is similar to parse-reranking (Charniak and Johnson 2005; Collins and Koo 2005). Indeed, if we were to model agreement as soft constraints, we could have incorporated this information as features in a reranking model. The filter approach differs in that it poses hard constraints and not soft ones, pruning away parts of the search space entirely. Thus, the use of k-best list is merely a technical detail in our implementation—the agreement information is 146 Goldberg and Elhadad Parsing System for Hebrew easily decomposable and the hard constraints can be efficiently incorporated into the CKY search procedure. 9. Evaluation and Results Data set. For all the experiments we"
J13-1007,J85-1006,0,0.654541,"ties (forward-backward tagging marginals) derived from a discriminative CRF tagging model. This approach is not ideal from a modeling perspective, as it makes each POS tag be accounted for twice: once by the syntactic model, and once by the sequential one. In this work, a sequential tagging model is not used at all. If the use of a sequential model is desired, an alternative method for integrating a sequence model and a syntactic model is making the models “negotiate” an agreed upon structure that maximizes the score under both models, using optimization techniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced into natural language processing (Rush et al. 2010). 21 Note that finding the most probable segmentation requires summing over all the trees resulting in each segmentation—a much harder task, proven to be NP-complete in Sima’an (1996). 143 Computational Linguistics Volume 39, Number 1 Figure 4 Lattice initialization of the CKY chart. 8. Incorporating Morphological Agreement Inspecting the learned grammars reveal that they do not encode any knowledge of morphological agreement: The split categories for nouns, verbs, and adjectives do not group words according to"
J13-1007,P08-1085,1,0.909226,"Missing"
J13-1007,P11-2124,1,0.884835,"ctive and the noun it modifies). We suggest modeling agreement as a filtering process that is orthogonal to the grammar. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make, without introducing new mistakes. Aspects of the work presented in this article are discussed in earlier publications. Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberg et al. (2009) discuss ways of interfacing a treebank-derived PCFG-parser with an external lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LA BerkeleyParser. Here we provide a cohesive presentation of the entire system, as well as a more detailed description and an expanded evaluation. We also extend the previous work in several dimensions: We introduce a new method of interfacing the parser and the external lexicon, which contributes to an improved parsing accuracy, and suggest incorporating agreement information as a filter. The methodologies we suggest extend outside the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morpholo"
J13-1007,P08-1043,1,0.722051,"l (the input to the parser) may be uncertain. This happens in Hebrew when a space-delimited token such as ! בצלcan represent either a single word (‘[an] onion’) or a sequence of two words or three words (‘in shadow’ and ‘in the shadow,’ respectively). When computationally feasible, it is best to let the uncertainty be resolved by the parser rather than in a separate preprocessing step. We propose encoding the input-uncertainty in a word lattice, and use lattice parsing (Chappelier et al. 1999; Hall 2005) to perform joint word segmentation and syntactic disambiguation (Cohen and Smith 2007; Goldberg and Tsarfaty 2008). Performing the tasks jointly is effective, and substantially outperforms a pipeline-based model. Using morphological information to improve parsing accuracy. Morphology provides useful hints for resolving syntactic ambiguity, and the parsing model should have a way of utilizing these hints. There is a range of morphological hints than can be utilized: from functional marking elements (such as the ! אתmarker indicating a definite direct object); to elements marking syntactic properties such as definiteness (such as the Hebrew !ה marker); to agreement patterns requiring a compatibility in"
J13-1007,E09-1038,1,0.850733,"ies such as gender, number, and person between syntactic constituents (such as a verb and its subject or an adjective and the noun it modifies). We suggest modeling agreement as a filtering process that is orthogonal to the grammar. Although the constituency parser does not make many agreement mistakes to begin with, the filter mechanism is effective in fixing the agreement mistakes that the parser does make, without introducing new mistakes. Aspects of the work presented in this article are discussed in earlier publications. Goldberg and Tsarfaty (2008) suggest the lattice-parsing mechanism, Goldberg et al. (2009) discuss ways of interfacing a treebank-derived PCFG-parser with an external lexicon, and Goldberg and Elhadad (2011) present experiments using the PCFG-LA BerkeleyParser. Here we provide a cohesive presentation of the entire system, as well as a more detailed description and an expanded evaluation. We also extend the previous work in several dimensions: We introduce a new method of interfacing the parser and the external lexicon, which contributes to an improved parsing accuracy, and suggest incorporating agreement information as a filter. The methodologies we suggest extend outside the scope"
J13-1007,C10-1045,0,0.40227,"thodologies we suggest extend outside the scope of Hebrew processing, and are of general applicability to the NLP community. Hebrew is a specific case of a morphologically rich language, and ideas presented in this work are useful also for processing other languages, including English. The lattice-based parsing methodology is useful in any case where the input is uncertain. Indeed, we have used it to solve the problem of parsing while recovering null elements in both English and Chinese (Cai, Chiang, and Goldberg 2011), and others have used it for the joint segmentation and parsing of Arabic (Green and Manning 2010). Extending the lexical coverage of a treebank-derived parser using an external lexicon is relevant for any language with 123 Computational Linguistics Volume 39, Number 1 a small treebank, and also for domain adaptation scenarios for English. Finally, the agreement-as-filter methodology is applicable to any morphologically rich language, and although its contribution to the parsing task may be limited, it is of wide applicability to syntactic generation tasks, such as target-side-syntax machine translation in a morphologically rich language. 2. Modern Hebrew 2.1 Lexical and Syntactic Properti"
J13-1007,D09-1087,0,0.0218223,"imilarly, by annotating the symbols as: S → NP @S1 @S1 → VP @S2 @S2 → NP @S3 @S3 → PP the grammar effectively allows only the original rule to be produced. Initial experiments on Hebrew confirm that moving to higher order horizontal markovization (encoding more context in the initial binarized rules) degrades parsing performance, while producing much larger grammars. The PCFG-LA parsing methodology is very robust, producing state-of-the-art accuracies for English, as well as many other languages including German (Petrov and Klein 2008), French (Candito, Crabb´e, and Seddah 2009), and Chinese (Huang and Harper 2009). 4. Baseline Experiments The baseline system is an “out-of-the-box” PCFG-LA parser, as described in Petrov et al. (2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12 The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact experimental settings) after stripping all the functional and morphological information from the non-terminals. We evaluate the resulting models on the development set, and consider three settings: Seg+POS Oracle: The parser has access to the gold segmentation and POS tags. Seg Oracle: The parser has access to the gold segment"
J13-1007,P09-1059,0,0.0407947,"Missing"
J13-1007,J98-4004,0,0.0403645,"r PPs. We also experimented with splits based on morphological agreement features, which are discussed in Section 8.1. Overall, the learning procedure is capable of producing good splits on its own. We did, however, manage to improve upon it with the following annotation (the annotations were removed prior to evaluation). Subject NPs. Hebrew phrase order is rather flexible, and the subject can appear before or after the verb. Identifying the subject can thus help in grounding the overall structure of the sentence. The subject is also dependent on agreement constraints with the verb. Following Johnson (1998), Klein and Manning (2003) implicitly annotate subject-NPs in English using parent annotation (distinguishing NPs under S from other NPs), with good results. When applied to English, the PCFG-LA also learns to model subject NPs well. Hebrew’s non-configurationality, however, put both Subjects and Objects directly under S, making it much harder to learn the distinction automatically. Explicit marking of subject NPs contributes slightly to the accuracy of the parser. Perhaps more important than the small increase in accuracy is the fact that the parser can identify subjects relatively well. In c"
J13-1007,P03-1054,0,0.54249,"nce of lexical items in context based on a sequential model. The constituency treebank can be used to learn the parameters of a syntactic-model of Hebrew, and the morphological analyzer can be used to provide broad-coverage lexical knowledge. Unfortunately, the treebank and the lexicon/disambiguator follow different annotation schemes, and are therefore incompatible with each other. The annotation gap between the two resources must be bridged before they can be used together. We now turn to survey the components of our Hebrew parsing system. 3. Latent-Annotation State-Split Grammars (PCFG-LA) Klein and Manning (2003) demonstrated that linguistically informed splitting of nonterminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and state-splitting (Matsuzaki, Miyao, and Tsujii 2005; Prescher 2005), which was then perfected in work by Petrov and colleagues (Petrov et al. 2006; Petrov and Klein 2007; Petrov 2009). State-split models assume that each non-terminal label has a latent annotation that should be recovered. Instead of a single NP symbol, these models hypothesize that there are many different NP symbols, NP1"
J13-1007,C88-1075,0,0.244605,"Missing"
J13-1007,P05-1010,0,0.0893584,"Missing"
J13-1007,W07-0808,1,0.926302,"2.3.5 A Resource Incompatibility Issue. Unfortunately, the KC Analyzer adopted a different tag set than the one used in the treebank, and analyses produced by the KC Analyzer (and hence by the morphological disambiguator) are incompatible with the Hebrew Treebank. These are not mere technical differences, but derive from different perspectives on the data. The Hebrew Treebank (TB) tag set is syntactic in nature (“if the word in this particular position functions as an adverb, tag it as an adverb, even though it is listed in the dictionary only as a noun”), whereas the KC tag set (Adler 2007; Netzer et al. 2007; Adler et al. 2008b) takes a lexical approach to POS tagging (“a word can assume only POS tags that would be assigned to it in a dictionary”). The lexical approach does not accommodate generic modification POS tags such as MOD, nor does it allow listing of demonstrative pronouns as adjectives. These divergent perspectives are reflected in different guidelines to human taggers, different principles underlying tag definitions, and different verification procedures. This difference in perspective yields different performances for parsers induced from tagged data, and a simple mapping between the"
J13-1007,N10-1003,0,0.0126867,"tial level, but also caused the parser to, again, not model agreement very well. The reason for this is clear in hindsight: Morphological agreement is an absolute concept, not a fuzzy one (things can either agree or not). Smoothing the probabilities between the different morphology-based split-licensed grammar rules that allow morphological disagreement, and made the grammar lose its discrimination power. This was then reinforced by the training process, which picked on other syntactic factors instead, and further phased out the agreement knowledge. A note on product-grammars. In recent work, Petrov (2010) showed that a committee of latent-variable grammars encoding different grammatical preferences can be combined into a product-grammar that is better than the individual ensemble members. Petrov created the ensemble by training several PCFG-LA parsers on the same data, but using different random seeds when initializing the EM starting point. We attempted to create a similar ensemble by providing the learning process with different linguistically motivated tree annotations (with and without encoding agreement features, with and without encoding definiteness, etc.). The combined parser did incre"
J13-1007,P06-1055,0,0.503651,"small treebank. Several natural questions arise: Can the small size of the treebank be compensated for using other available resources or sources of information? How should the word segmentation issue (that function words do not appear in isolation but attach to the next word, forming ambiguous letter patterns) be handled? Can morphological information be used effectively in order to improve parsing accuracy? We present a system which is based on a state-of-the-art model for constituency parsing, namely, the probabilistic context-free grammar (PCFG) with latent annotations (PCFG-LA) model of Petrov et al. (2006), as implemented in the BerkeleyParser. After evaluating the out-of-the-box performance of the BerkeleyParser on the Hebrew treebank, we discuss some of its limitations and then go on to extend the PCFG-LA parsing model in several directions, making it more suitable for parsing Hebrew and related languages. Our extensions are based on the following themes. Separation of lexical and syntactic knowledge. There are two kinds of knowledge inherent in a parsing system. One of them is syntactic knowledge governing the way in which words can be combined to form structures, which, in turn, can be comb"
J13-1007,W08-1005,0,0.0164719,"ces the VP to be produced before the NP, but still allows the NP to be dropped. Similarly, by annotating the symbols as: S → NP @S1 @S1 → VP @S2 @S2 → NP @S3 @S3 → PP the grammar effectively allows only the original rule to be produced. Initial experiments on Hebrew confirm that moving to higher order horizontal markovization (encoding more context in the initial binarized rules) degrades parsing performance, while producing much larger grammars. The PCFG-LA parsing methodology is very robust, producing state-of-the-art accuracies for English, as well as many other languages including German (Petrov and Klein 2008), French (Candito, Crabb´e, and Seddah 2009), and Chinese (Huang and Harper 2009). 4. Baseline Experiments The baseline system is an “out-of-the-box” PCFG-LA parser, as described in Petrov et al. (2006) and Petrov and Klein (2007) and implemented in the BerkeleyParser.12 The parser is trained on the Modern Hebrew Treebank (see Section 9 for the exact experimental settings) after stripping all the functional and morphological information from the non-terminals. We evaluate the resulting models on the development set, and consider three settings: Seg+POS Oracle: The parser has access to the gold"
J13-1007,D10-1001,0,0.0417674,". This approach is not ideal from a modeling perspective, as it makes each POS tag be accounted for twice: once by the syntactic model, and once by the sequential one. In this work, a sequential tagging model is not used at all. If the use of a sequential model is desired, an alternative method for integrating a sequence model and a syntactic model is making the models “negotiate” an agreed upon structure that maximizes the score under both models, using optimization techniques such as dual decomposition (Dantzig and Wolfe 1960), which was recently introduced into natural language processing (Rush et al. 2010). 21 Note that finding the most probable segmentation requires summing over all the trees resulting in each segmentation—a much harder task, proven to be NP-complete in Sima’an (1996). 143 Computational Linguistics Volume 39, Number 1 Figure 4 Lattice initialization of the CKY chart. 8. Incorporating Morphological Agreement Inspecting the learned grammars reveal that they do not encode any knowledge of morphological agreement: The split categories for nouns, verbs, and adjectives do not group words according to any relevant morphological property such as gender or number, making it impossible"
J13-1007,C96-2215,0,0.191338,"Missing"
J13-1007,H05-1060,0,0.115559,"Missing"
J13-1007,P06-3009,0,0.657311,"e seen once. 7. Joint Segmentation and Parsing When applied to real text (for which the gold word-segmentation is not available), the baseline PCFG-LA parser is supplied with word segmentation produced by a separate tagging process.18 This seriously degrades parsing performance. A major reason for the performance drop is that the word-segmentation task and the syntactic-disambiguation task are highly related. Segmentation mistakes drive the parser toward wrong syntactic structures, and many segmentation decisions require long-distance information that is not available to a sequential process (Tsarfaty 2006a). For these reasons, we claim that parsing and segmentation should be performed jointly. 18 Although the tagger also produces POS tag assignments, we ignore them and use only the word segmentation. This is done for two reasons: first, the tag set of the tagger is the one used by the morphological analyzer, and is not compatible with the treebank. Second, we believe it is better for the parser to produce its own tag assignments. 141 Computational Linguistics Volume 39, Number 1 Figure 3 The lattice for the Hebrew sequence !( בצלם הנעיםsee footnote 19). Joint segmentation and parsing can b"
J13-1007,W07-2219,0,0.0409388,"Missing"
J13-1007,C08-1112,0,0.0305197,"Missing"
J13-1007,W10-1405,0,0.115984,"Missing"
J13-1007,D09-1088,0,0.0376607,"Missing"
J13-1007,N07-1051,0,\N,Missing
J97-2001,P83-1011,0,0.171801,"Missing"
J97-2001,C90-3059,0,0.0723417,"Missing"
J97-2001,W96-0501,1,0.133279,"determining the form of that text (McDonald 1983; McKeown 1985). Typically, a generator has two modules, each corresponding to one of these two tasks, a content planner and a linguistic realizer. While many systems allow for interaction across these components, there is general consensus that these two components can be separated (Reiter 1994). Furthermore, within the linguistic component, there appears to be further consensus that the task of syntactic realization can be isolated. As evidence, note that a number of dedicated syntactic realization components have been developed such as SURGE (Elhadad and Robin 1996), NICEL (Matthiessen 1991), MUMBLE(Meteer et al. 1987), and TAGs (Yang, McCoy, and Vijay-Shanker 1991; Harbusch 1994). Such components expect as input a specification of the thematic structure of the sentence to generate, with the syntactic category and open-class words of each thematic role. 3 Thematic structure involves roles such as agent, p a t i e n t , instrument, etc. It is opposed to surface syntactic structure which involves roles such as s u b j e c t , o b j e c t , adjunct, etc. Due to general syntactic alternations (Levin 1993) such as passive, dative, it-extraposition, or cleftin"
J97-2001,J85-4002,0,0.057011,"lexical choice is not part of the syntactic realization component, then all decisions regarding open-class word selection must be made before the grammar is invoked# They then must occur either as part of content planning or after all content has been determined and expressed in a language-independent manner. While some researchers have directly associated words with each concept in the domain-knowledge base (e.g., Reiter 1991; Swartout 1983), this approach does not allow for consideration of syntactic and lexical constraints unless a phrasal lexicon is used (e.g., Kukich 1983b; Danlos 1986; Jacobs 1985; Hovy 1988). Using a phrasal lexicon, however, means hand-encoding the many mappings of multiple constraints onto multiword phrasings. It thus does not allow for compositional lexical representation (Pustejovsky and Boguraev 1993), and entails a combinatorial explosion in the number of entries to cover the variations of phrases that are possible in different contexts. This approach thus does not allow for scaling up paraphrasing power (see Robin and McKeown [1996] for a quantitative evaluation of the scalability gains resulting from the compositional word-based approach). By waiting until con"
J97-2001,P83-1022,0,0.033265,"between semantic units. If lexical choice is not part of the syntactic realization component, then all decisions regarding open-class word selection must be made before the grammar is invoked# They then must occur either as part of content planning or after all content has been determined and expressed in a language-independent manner. While some researchers have directly associated words with each concept in the domain-knowledge base (e.g., Reiter 1991; Swartout 1983), this approach does not allow for consideration of syntactic and lexical constraints unless a phrasal lexicon is used (e.g., Kukich 1983b; Danlos 1986; Jacobs 1985; Hovy 1988). Using a phrasal lexicon, however, means hand-encoding the many mappings of multiple constraints onto multiword phrasings. It thus does not allow for compositional lexical representation (Pustejovsky and Boguraev 1993), and entails a combinatorial explosion in the number of entries to cover the variations of phrases that are possible in different contexts. This approach thus does not allow for scaling up paraphrasing power (see Robin and McKeown [1996] for a quantitative evaluation of the scalability gains resulting from the compositional word-based appr"
J97-2001,P81-1014,0,0.26012,"Missing"
J97-2001,A94-1002,1,0.877314,"Missing"
J97-2001,P93-1031,1,0.880409,"Missing"
J97-2001,W94-0319,0,0.260874,"ossible placements of lexical choice within a generator's architecture. 2.1 Lexical Choice within a Generation System Architecture Generation systems perform two types of tasks: one conceptual, determining the content of the text to be generated, and one linguistic, determining the form of that text (McDonald 1983; McKeown 1985). Typically, a generator has two modules, each corresponding to one of these two tasks, a content planner and a linguistic realizer. While many systems allow for interaction across these components, there is general consensus that these two components can be separated (Reiter 1994). Furthermore, within the linguistic component, there appears to be further consensus that the task of syntactic realization can be isolated. As evidence, note that a number of dedicated syntactic realization components have been developed such as SURGE (Elhadad and Robin 1996), NICEL (Matthiessen 1991), MUMBLE(Meteer et al. 1987), and TAGs (Yang, McCoy, and Vijay-Shanker 1991; Harbusch 1994). Such components expect as input a specification of the thematic structure of the sentence to generate, with the syntactic category and open-class words of each thematic role. 3 Thematic structure involve"
J97-2001,J83-1005,0,0.323545,"Missing"
J97-2001,W90-0104,0,\N,Missing
J97-2001,C90-1021,0,\N,Missing
J97-2001,E87-1001,0,\N,Missing
J97-2001,C92-3158,0,\N,Missing
L16-1688,P06-1084,1,0.745597,"al information, such as inter-frame relationships and standard lexical patterns of the realization of frames in natural language, but these details are beyond the scope on this paper. 2 http://www.cs.bgu.ac.il/~nlpproj/newhebfn/ 4341 developed in the past 15 years. We rely on the Hebrew lexicon described by Itai and Wintner (2008) and a Hebrew corpus, which contains about 1.75M sentences (an expansion of the Corpus of Contemporary Hebrew (Itai and Wintner, 2008)). We annotated all sentences using current state of the art automatic Hebrew annotators: a morphological analyzer and disambiguator (Adler and Elhadad, 2006; Tsarfaty and Goldberg, 2008), a POS tagger (Goldberg et al., 2008) and a syntactic parser (Goldberg and Elhadad, 2010). As part of this annotation effort, we used the Hebrew POS tagset (Netzer et al., 2007; Adler et al., 2008) together with this toolset (see Section 4.3.). 3. Development and Annotation Processes 3. Mapping words directly to Hebrew frames may have resulted in a sparse dataset, meaning we would have many frames with few LUs, as opposed to a smaller set of complete frames. To verify that the frame frequency we estimated in the English FrameNet corresponds to similar coverage in"
L16-1688,adler-etal-2008-tagging,1,0.816738,"sult, instead of adding an extra annotation layer5 , like in the SweFN project, we decided to followed Petruck’s recommendation; borrowing from the Spanish FrameNet project, we annotate such roles as “externally constructionally null instantiated” (ECNI) (Subirats, 2009). 5 4343 Since the pronoun compounding in Hebrew is not limv.אכל Figure 2: An exemplar sentence for the LU achal in the frame Ingestion. ate.v 4.3. What Units can Evoke Frames: POS Tagset Traditionally, verbs, nouns, adjectives and prepositions are considered as candidate LUs (Fillmore et al., 2002). However, as discussed by Adler et al. (2008), part-of-speech tagsets must sometimes be modified for different languages. In the case of Hebrew, we use the Beinoni tag, which occupies a middle place between noun and verb and is most closely related to participial forms. From a semantic point of view, according to traditional descriptions, Hebrew Beinoni forms do not denote a fixed state, but activities, in contrast to nouns and adjectives. As an example, consider the difference between the semantic information carried by the word  מטפסin the following sentences: 1. שלמה Shlomo Solomon 2. הילד ha’yeled The child הוא hu is מטפס הר"
L16-1688,W12-3614,0,0.0212301,"cal resources) and pre-processed these sentences with automatic full morphological analysis and automatic syntactic parsing, as described in Section 2.. Annotators can access this annotated corpus from the Hebrew FrameNet annotation tool through a full text search interface, where annotators can search for lexical items irrespective of morphological inflection and refine the query by specifying part of speech, morphological features (number, gender, person etc.), and syntactic context (e.g., word appearing as the subject of a specific verb). We apply the syntactic diversification algorithm of Borin et al. (2012) to the search result set, so that the top N sentences presented to each annotator exhibit a wide range of syntactic constructs and lexical items, allowing annotators to quickly create a range of syntactic examples for a single semantic concept. 4 FrameNet LUs do have definitions, but they are not structured in a manner that enables automatic analysis. 4342 Figure 1: A list of automatic translations of the LU leave.v suggested as LUs for the Hebrew implementation of the Abandonment frame. 3.4. Exemplar Sentence Annotation The final stage in the annotation process of a single Frame consists of"
L16-1688,candito-etal-2014-developing,0,0.13871,"Missing"
L16-1688,fillmore-etal-2002-framenet,0,0.040215,"n and some other Latin-derived languages. As a result, instead of adding an extra annotation layer5 , like in the SweFN project, we decided to followed Petruck’s recommendation; borrowing from the Spanish FrameNet project, we annotate such roles as “externally constructionally null instantiated” (ECNI) (Subirats, 2009). 5 4343 Since the pronoun compounding in Hebrew is not limv.אכל Figure 2: An exemplar sentence for the LU achal in the frame Ingestion. ate.v 4.3. What Units can Evoke Frames: POS Tagset Traditionally, verbs, nouns, adjectives and prepositions are considered as candidate LUs (Fillmore et al., 2002). However, as discussed by Adler et al. (2008), part-of-speech tagsets must sometimes be modified for different languages. In the case of Hebrew, we use the Beinoni tag, which occupies a middle place between noun and verb and is most closely related to participial forms. From a semantic point of view, according to traditional descriptions, Hebrew Beinoni forms do not denote a fixed state, but activities, in contrast to nouns and adjectives. As an example, consider the difference between the semantic information carried by the word  מטפסin the following sentences: 1. שלמה Shlomo Solomon 2."
L16-1688,heppin-gronostaj-2012-rocky,0,0.125032,"annotation would be: אכל akhal Ate 4.1. Multi-word Lexical Units The English FrameNet project contains several multiword LUs (MWLUs), such as give up.v and turn in.v, which are annotated as contiguous units. In Hebrew, we found many complex morphological and syntactic מזל רע variants of MWLUs. For example, while mazal ra bad luck הוא hu He תי [ ti I תפוח Ingestor] [ tapuach an apple Ingestibles]. ��� �� The Agent is instead embedded in another token, since the subject is actually realized as a morpheme of the verb. This issue is somewhat similar to that of compound words in Swedish (Heppin and Gronostaj, 2012). In Hebrew, however, the compounding of role-bearing elements is limited to pronouns, and more closely resembles the same issue in Spanish, Italian and some other Latin-derived languages. As a result, instead of adding an extra annotation layer5 , like in the SweFN project, we decided to followed Petruck’s recommendation; borrowing from the Spanish FrameNet project, we annotate such roles as “externally constructionally null instantiated” (ECNI) (Subirats, 2009). 5 4343 Since the pronoun compounding in Hebrew is not limv.אכל Figure 2: An exemplar sentence for the LU achal in the frame Inges"
L16-1688,kingsbury-palmer-2002-treebank,0,0.131506,"duction frames and their structures in natural language. Recent years have seen growing interest in the task of Semantic Role Labeling (SRL) of natural language text (sometimes called “shallow semantic parsing”). The task is usually described as the act of identifying the semantic roles, which are the set of semantic properties and relationships defined over constituents of a sentence, given a semantic context. The creation of resources that document the realization of semantic roles in natural language texts, such as FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) and PropBank (Kingsbury and Palmer, 2002) have advanced the field of semantic analysis no end and have allowed the development of learning algorithms for automatically analyzing the semantic structure of text. Shallow semantic analysis has been shown to contribute to the advancement of a wide spectrum of natural language processing tasks, ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and abstractive summarization (Melli et al., 2005). 1.1. FrameNet FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) is a human-annotated"
L16-1688,P14-2050,0,0.021156,"Missing"
L16-1688,W07-0808,1,0.848775,"Missing"
L16-1688,J12-1005,0,0.0543147,"Missing"
L16-1688,N10-1115,1,0.762482,"ral language, but these details are beyond the scope on this paper. 2 http://www.cs.bgu.ac.il/~nlpproj/newhebfn/ 4341 developed in the past 15 years. We rely on the Hebrew lexicon described by Itai and Wintner (2008) and a Hebrew corpus, which contains about 1.75M sentences (an expansion of the Corpus of Contemporary Hebrew (Itai and Wintner, 2008)). We annotated all sentences using current state of the art automatic Hebrew annotators: a morphological analyzer and disambiguator (Adler and Elhadad, 2006; Tsarfaty and Goldberg, 2008), a POS tagger (Goldberg et al., 2008) and a syntactic parser (Goldberg and Elhadad, 2010). As part of this annotation effort, we used the Hebrew POS tagset (Netzer et al., 2007; Adler et al., 2008) together with this toolset (see Section 4.3.). 3. Development and Annotation Processes 3. Mapping words directly to Hebrew frames may have resulted in a sparse dataset, meaning we would have many frames with few LUs, as opposed to a smaller set of complete frames. To verify that the frame frequency we estimated in the English FrameNet corresponds to similar coverage in contemporary Hebrew, we estimated the coverage of our corpus by the LUs of the selected English frames. We found a cove"
L16-1688,Q15-1032,0,0.0219365,"POS types (modal, copula, etc.). There are 423 annotated exemplar sentences across 66 LUs, with an average of 6.41 sentences per LU. Before starting a more intense annotation campaign, we are now reviewing the linguistic issues faced during the initial annotation trial and assessing the potential to speed up annotation with semi-supervised expansion. 7 https://www.mongodb.org/ https://www.elastic.co/products/elasticsearch This work provides a basis on which an automatic SRL system for the Hebrew language can be constructed. We are investigating the current state-of-the-art English SRL system (Roth and Lapata, 2015) as a starting point. We are planning to apply the ideas put forth by Boas (2005) as a form of evaluation for automatic SRL systems in multiple languages. We use an aligned HebrewEnglish corpus and estimate the accuracy of an automatic annotator by comparing the annotations of an English SRL system with those produced by the Hebrew system. The Hebrew annotation is deemed correct when we verify that the same frames are identified across the English and Hebrew sides of an aligned sentence pair, and that the assigned roles can be aligned across languages using lexical alignment methods. We have c"
L16-1688,D07-1002,0,0.052556,"xt. The creation of resources that document the realization of semantic roles in natural language texts, such as FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) and PropBank (Kingsbury and Palmer, 2002) have advanced the field of semantic analysis no end and have allowed the development of learning algorithms for automatically analyzing the semantic structure of text. Shallow semantic analysis has been shown to contribute to the advancement of a wide spectrum of natural language processing tasks, ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and abstractive summarization (Melli et al., 2005). 1.1. FrameNet FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) is a human-annotated linguistic resource with rich semantic content based on the linguistic theory of Frame Semantics proposed by Fillmore (1982). FrameNet defines a formal structure for semantic frames, and various relationships between and within them. Each frame contains a list of frame-evoking words which also serve as the predicates of events described by the frames. These words are called Frame Evoking Elements (FEEs)"
L16-1688,J13-1007,1,0.905905,"Missing"
L16-1688,P08-1085,1,0.71637,"l patterns of the realization of frames in natural language, but these details are beyond the scope on this paper. 2 http://www.cs.bgu.ac.il/~nlpproj/newhebfn/ 4341 developed in the past 15 years. We rely on the Hebrew lexicon described by Itai and Wintner (2008) and a Hebrew corpus, which contains about 1.75M sentences (an expansion of the Corpus of Contemporary Hebrew (Itai and Wintner, 2008)). We annotated all sentences using current state of the art automatic Hebrew annotators: a morphological analyzer and disambiguator (Adler and Elhadad, 2006; Tsarfaty and Goldberg, 2008), a POS tagger (Goldberg et al., 2008) and a syntactic parser (Goldberg and Elhadad, 2010). As part of this annotation effort, we used the Hebrew POS tagset (Netzer et al., 2007; Adler et al., 2008) together with this toolset (see Section 4.3.). 3. Development and Annotation Processes 3. Mapping words directly to Hebrew frames may have resulted in a sparse dataset, meaning we would have many frames with few LUs, as opposed to a smaller set of complete frames. To verify that the frame frequency we estimated in the English FrameNet corresponds to similar coverage in contemporary Hebrew, we estimated the coverage of our corpus by the"
L16-1688,P03-1002,0,0.0919546,"stituents of a sentence, given a semantic context. The creation of resources that document the realization of semantic roles in natural language texts, such as FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) and PropBank (Kingsbury and Palmer, 2002) have advanced the field of semantic analysis no end and have allowed the development of learning algorithms for automatically analyzing the semantic structure of text. Shallow semantic analysis has been shown to contribute to the advancement of a wide spectrum of natural language processing tasks, ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and abstractive summarization (Melli et al., 2005). 1.1. FrameNet FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) is a human-annotated linguistic resource with rich semantic content based on the linguistic theory of Frame Semantics proposed by Fillmore (1982). FrameNet defines a formal structure for semantic frames, and various relationships between and within them. Each frame contains a list of frame-evoking words which also serve as the predicates of events described by the frames. These"
L16-1688,tsarfaty-goldberg-2008-word,0,0.0274488,"nter-frame relationships and standard lexical patterns of the realization of frames in natural language, but these details are beyond the scope on this paper. 2 http://www.cs.bgu.ac.il/~nlpproj/newhebfn/ 4341 developed in the past 15 years. We rely on the Hebrew lexicon described by Itai and Wintner (2008) and a Hebrew corpus, which contains about 1.75M sentences (an expansion of the Corpus of Contemporary Hebrew (Itai and Wintner, 2008)). We annotated all sentences using current state of the art automatic Hebrew annotators: a morphological analyzer and disambiguator (Adler and Elhadad, 2006; Tsarfaty and Goldberg, 2008), a POS tagger (Goldberg et al., 2008) and a syntactic parser (Goldberg and Elhadad, 2010). As part of this annotation effort, we used the Hebrew POS tagset (Netzer et al., 2007; Adler et al., 2008) together with this toolset (see Section 4.3.). 3. Development and Annotation Processes 3. Mapping words directly to Hebrew frames may have resulted in a sparse dataset, meaning we would have many frames with few LUs, as opposed to a smaller set of complete frames. To verify that the frame frequency we estimated in the English FrameNet corresponds to similar coverage in contemporary Hebrew, we estim"
L16-1688,N09-2004,0,0.0131483,"realization of semantic roles in natural language texts, such as FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) and PropBank (Kingsbury and Palmer, 2002) have advanced the field of semantic analysis no end and have allowed the development of learning algorithms for automatically analyzing the semantic structure of text. Shallow semantic analysis has been shown to contribute to the advancement of a wide spectrum of natural language processing tasks, ranging from information extraction (Surdeanu et al., 2003) and question answering (Shen and Lapata, 2007), to machine translation (Wu and Fung, 2009) and abstractive summarization (Melli et al., 2005). 1.1. FrameNet FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2010) is a human-annotated linguistic resource with rich semantic content based on the linguistic theory of Frame Semantics proposed by Fillmore (1982). FrameNet defines a formal structure for semantic frames, and various relationships between and within them. Each frame contains a list of frame-evoking words which also serve as the predicates of events described by the frames. These words are called Frame Evoking Elements (FEEs) or Lexical Units (LUs). Additionally, each"
N06-2027,W05-1602,1,0.917667,"the verb in the utterance, filling the slots with the rest of the content words given. The system uses the semantic representation to re-generate fluent text, relying on lexical resources and NLG techniques. The main questions at stake in this approach are how good can a semantic parser be, in order to reconstruct the full structure of the sentence from telegraphic input and are pragmatic gaps in the given telegraphic utterances recoverable in general. 3 Generating Messages via Semantic Authoring Our approach differs from previous NLG-AAC systems in that, with the model of semantic authoring (Biller et al., 2005), we intervene during the process of composing the input sequence, and thus can provide early feedback (in the form of display composition and partial text feedback), while preventing the need for parsing a telegraphic sequence. Semantic parsing is avoided by constructing a semantic structure explicitly while the user inputs the sequence incrementally. It combines three aspects 106 • Semantic authoring drives a natural language realization system and provides rich semantic input. • A display is updated on the fly as the authoring system requires the user to select options. • Ready-made inputs,"
N06-2027,H89-1034,0,0.0471681,"nguistic realizer. 2 into an integrated approach for the design of an AAC system: Generating Messages via Translation A major difficulty when parsing a telegraphic sequence of words or symbols, is that many of the hints that are used to capture the structure of the text and, accordingly, the meaning of the utterance, are missing. Moreover, as an AAC device is usually used for real-time conversation, the interpretation of utterances relies heavily on pragmatics – time of mentioned events, reference to the immediate environment. Previous works dealing with translating telegraphic text, such as (Grishman and Sterling, 1989), (Lee et al., 1997) requires to identify dependency relations among the tokens of the telegraphic input. Rich lexical knowledge is needed to identify possible dependencies in a given utterance, i.e., to find the predicate and to apply constraints, such as selectional restrictions to recognize its arguments. Similar methods were used for AAC applications, COMPANSION (McCoy, 1997) for example – where the telegraphic text is expanded to full sentences, using a word order parser, and a semantic parser to build the case frame structure of the verb in the utterance, filling the slots with the rest"
N06-2027,P97-1016,0,0.0234593,"tegrated approach for the design of an AAC system: Generating Messages via Translation A major difficulty when parsing a telegraphic sequence of words or symbols, is that many of the hints that are used to capture the structure of the text and, accordingly, the meaning of the utterance, are missing. Moreover, as an AAC device is usually used for real-time conversation, the interpretation of utterances relies heavily on pragmatics – time of mentioned events, reference to the immediate environment. Previous works dealing with translating telegraphic text, such as (Grishman and Sterling, 1989), (Lee et al., 1997) requires to identify dependency relations among the tokens of the telegraphic input. Rich lexical knowledge is needed to identify possible dependencies in a given utterance, i.e., to find the predicate and to apply constraints, such as selectional restrictions to recognize its arguments. Similar methods were used for AAC applications, COMPANSION (McCoy, 1997) for example – where the telegraphic text is expanded to full sentences, using a word order parser, and a semantic parser to build the case frame structure of the verb in the utterance, filling the slots with the rest of the content words"
N06-2027,W97-0503,0,0.0549423,"ss is a graphic meaning-referenced language, created by Charles Bliss to be used as a written universal language (Bliss, 1965); since 1971, Blissymbols are used for communication with severely languageimpaired children. Bliss is designed to be a writtenonly language, with non-arbitrary symbols. Symbols are constructed from a composition of atomic icons. Because words are structured from semantic components, the graphic representation by itself provides information on words’ connectivity 1 . In the last decade, several systems that integrate NLG techniques for AAC systems have been developed ((McCoy, 1997), (Vaillant, 1997) for example). These systems share a common architecture: a telegraphic input sequence (words or symbols) is first parsed, and then a grammatical sentence that represents the message is generated. This paper presents an NLG-AAC system that generates messages through a controlled process of authoring, where each step in the selection of symbols is controlled by the input specification defined 1 See http://www.bci.org for reference on the language 105 Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 105–108, c New York, Jun"
N06-2027,P98-2173,0,0.0383135,"r by semantic/graphic component: searching all words in the lexicon that contain both food and meat returns the symbols hamburger, hot-dog, meatball etc. (see Fig. 1). The lexicon currently includes 2,200 entries. the cognitive load and can affect the rate of communication. 4 Figure 1: A snapshot of the Bliss Lexicon Web Application The core of the processing machinery of the AAC message generation system is based on SAUT (Biller et al., 2005) – an authoring system for logical forms encoded as conceptual graphs (CG). The system belongs to the family of WYSIWYM (What You See Is What You Mean) (Power and Scott, 1998) text generation systems: logical forms are entered interactively and the corresponding linguistic realization of the expressions is generated in several languages. The system maintains a model of the discourse context corresponding to the authored documents to enable reference planning in the generation process. Generating language from pictorial inputs, and specifically from Bliss symbols using semantic authoring in the WYSIWYM approach is not only a pictorial application of the textual version, but it also addresses specific needs of augmentative communication. As was mentioned above, gener"
N06-2027,C98-2168,0,\N,Missing
N10-1115,W06-2920,0,0.0732293,"m already built structures both to the left and to the right of the attachment point. The parser learns both the attachment preferences and the order in which they should be performed. The result is a deterministic, best-first, O(nlogn) parser, which is significantly more accurate than best-first transition based parsers, and nears the performance of globally optimized parsing models. 1 Introduction Dependency parsing has been a topic of active research in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved p"
N10-1115,D07-1101,0,0.484622,"e input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2 -154 jumped with"
N10-1115,W02-1001,0,0.244891,"r attachments. Unfortunately, this kind of ordering information is not directly encoded in the data. We must, therefore, learn how to order the decisions. We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function. 4 Learning Algorithm We use a linear model score(x) = w ~ · φ(x), where φ(x) is a feature representation and w ~ is a weight vector. We write φact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). As usual, we use parameter averaging to prevent the perceptron from overfitting. The training algorithm is initialized with a zero parameter vector w. ~ The algorithm makes several passes over the data. At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set. At training time, each sentence is parsed using the parsing algorithm and the current w. ~ Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6). Instead, we update the parameter v"
N10-1115,W06-2929,0,0.0294554,"Missing"
N10-1115,W05-1504,0,0.0155726,"t left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG parsing, with good results. We build on top of that work and present a concrete and efficient greedy non-directional dependency parsing algorithm. 749 Structure Restrictions Eisner and Smith (2005) propose to improve the efficiency of a globally optimized parser by posing hard constraints on the lengths of arcs it can produce. Such constraints pose an explicit upper bound on parser accuracy.10 Our parsing model does not pose such restrictions. Shorter edges are arguably easier to predict, and our parses builds them early in time. However, it is also capable of producing long dependencies at later stages in the parsing process. Indeed, the distribution of arc lengths produced by our parser is similar to those produced by the M ALT and M ST parsers. 9 Discussion We presented a non-directi"
N10-1115,D09-1127,0,0.0334056,"in pi−2 , pi−1 , pi , pi+1 , pi+2 , pi+3 Bigram for p,q in (pi , pi+1 ),(pi , pi+2 ),(pi−1 , pi ),(pi−1 , pi+2 ),(pi+1 , pi+2 ) lenp , ncp ∆qp , ∆qp tp tq tp , wp , tp lcp , tp rcp , tp rcp lcp tp tq , wp wq , tp wq , wp tq tp tq lcp lcq , tp tq rcp lcq tp tq lcp rcq , tp tq rcp rcq PP-Attachment if pi is a preposition if pi+1 is a preposition wpi−1 wpi rcpi , tpi−1 wpi rcwpi wpi−1 wpi+1 rcpi+1 , tpi−1 wpi+1 rcwpi+1 wpi wpi+1 rcpi+1 , tpi wpi+1 rcwpi+1 wpi+1 wpi+2 rcpi+2 , tpi+1 wpi+2 rcwpi+2 wpi wpi+2 rcpi+2 , tpi wpi+2 rcwpi+2 if pi+2 is a preposition Figure 2: Feature Templates scribed in (Huang et al., 2009). We extended that feature set to include the structure on both sides of the proposed attachment point. In the case of unigram features, we added features that specify the POS of a word and its left-most and right-most children. These features provide the nondirectional model with means to prefer some attachment points over others based on the types of structures already built. In English, the left- and rightmost POS-tags are good indicators of constituency. The pp-attachment features are similar to the bigram features, but fire only when one of the structures is headed by a preposition (IN)."
N10-1115,W07-2416,0,0.0272115,"s 2-21 for training, Section 22 for development, and Section 23 as the final test set. The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing. Each section is tagged after training the tagger on all other sections. The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set. While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more 5 http://www.cs.bgu.ac.il/∼yoavg/software/ http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html 7 While other and better conversions exist (see, e.g., (Johansson and Nugues, 2007; Sangati and Mazza, 2009)), this conversion heuristic is still the most widely used. Using the same conversion facilitates comparison with previous works. 6 747 representative of the tagging performance on nonWSJ corpus texts. Parsers We evaluate our parser against the transition-based M ALT parser and the graph-based M ST parser. We use version 1.2 of M ALT parser8 , with the settings used for parsing English in the CoNLL 2007 shared task. For the M ST parser9 , we use the default first-order, projective parser settings, which provide state-of-the-art results for English. All parsers are tra"
N10-1115,D07-1013,0,0.180011,"use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the next two or three input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The"
N10-1115,E06-1011,0,0.281678,"y, only the next two or three input tokens are available to the parser. This limited look-ahead window leads to error propagation and worse performance on root and long distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2"
N10-1115,P05-1012,0,0.316746,"search in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricte"
N10-1115,D07-1100,0,0.04797,"2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2 -154 jumped with -232 with fox fox joy fox a a brown joy 10 (6) 430 joy 246 with brown ATTACH L EFT (1) 186 with jumped fox 12 -146 -149 -133 246 brown (4) ATTACH R IGHT (1) a brown joy brown Figure 1: Parsing the sentence “a brown fox jumped with joy”. Rounded arcs represent possible actions. tionally intensive sampling-based methods (Nakagawa, 2007). As a result, these models, while accurate, are slow (O(n3 ) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models). We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007): nondirectional easy-first parsing. This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms. By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take int"
N10-1115,P08-1108,0,0.0402283,"t part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment point: traditionally, only the"
N10-1115,nivre-etal-2006-maltparser,0,0.0624865,"d parsing models. 1 Introduction Dependency parsing has been a topic of active research in natural language processing in the last several years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural"
N10-1115,W04-0308,0,0.139318,"action) 2 if (∃c0 : (c, c0 ) ∈ Gold ∧ (c, c0 ) 6∈ Arcs) ∨ (p, c) 6∈ Gold then 3 return false 4 return true The function isV alid(act(i), gold, arcs) (line 4) is used to decide if the chosen action/location pair is valid. It returns True if two conditions apply: (a) (pi , pj ) is present in gold, (b) all edges (2, pj ) in gold are also in arcs. In words, the function verifies that the proposed edge is indeed present in the gold parse and that the suggested daughter already found all its own daughters.2 2 This is in line with the Arc-Standard parsing strategy of shift-reduce dependency parsers (Nivre, 2004). We are currently experimenting also with an Arc-Eager variant of the non745 5 Feature Representation The feature representation for an action can take into account the original sentence, as well as the entire parse history: φact(i) above is actually φ(act(i), sentence, Arcs, pending). We use binary valued features, and each feature is conjoined with the type of action. When designing the feature representation, we keep in mind that our features should not only direct the parser toward desired actions and away from undesired actions, but also provide the parser with means of choosing between"
N10-1115,W06-1616,0,0.0324742,"distant dependencies relative to graphbased parsers (McDonald and Nivre, 2007). Graph-based parsers, on the other hand, are globally optimized. They perform an exhaustive search over all possible parse trees for a sentence, and find the highest scoring tree. In order to make the search tractable, the feature set needs to be restricted to features over single edges (first-order models) or edges pairs (higher-order models, e.g. (McDonald and Pereira, 2006; Carreras, 2007)). There are several attempts at incorporating arbitrary tree-based features but these involve either solving an ILP problem (Riedel and Clarke, 2006) or using computa742 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 742–750, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics (1) ATTACH R IGHT (2) -157 a brown (2) jumped fox -27 -152 -197 -68 403 231 -47 3 -243 ATTACH R IGHT (1) (3) -176 -159 -52 a joy with jumped fox 314 0 joy with 270 a ATTACH L EFT (2) (5) -161 jumped -435 jumped -2 -154 jumped with -232 with fox fox joy fox a a brown joy 10 (6) 430 joy 246 with brown ATTACH L EFT (1) 186 with jumped fox 12 -146 -149 -133 246 brown (4) ATTACH R IG"
N10-1115,P06-2089,0,0.226166,"veral years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment poi"
N10-1115,N06-2033,0,0.378845,"veral years. An important part of this research effort are the CoNLL 2006 and 2007 shared tasks (Buchholz and Marsi, 2006; Nivre et al., 2007), which allowed for a comparison of many algorithms and approaches for this task on many languages. ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University Current dependency parsers can be categorized into three families: local-and-greedy transitionbased parsers (e.g., M ALT PARSER (Nivre et al., 2006)), globally optimized graph-based parsers (e.g., M ST PARSER (McDonald et al., 2005)), and hybrid systems (e.g., (Sagae and Lavie, 2006b; Nivre and McDonald, 2008)), which combine the output of various parsers into a new and improved parse, and which are orthogonal to our approach. Transition-based parsers scan the input from left to right, are fast (O(n)), and can make use of rich feature sets, which are based on all the previously derived structures. However, all of their decisions are very local, and the strict left-to-right order implies that, while the feature set can use rich structural information from the left of the current attachment point, it is also very restricted in information to the right of the attachment poi"
N10-1115,D08-1052,0,0.244314,"is not directly encoded in the data. We must, therefore, learn how to order the decisions. We first describe the learning algorithm (Section 4) and a feature representation (Section 5) which enables us to learn an effective scoring function. 4 Learning Algorithm We use a linear model score(x) = w ~ · φ(x), where φ(x) is a feature representation and w ~ is a weight vector. We write φact(i) to denote the feature representation extracted for action act at location i. The model is trained using a variant of the structured perceptron (Collins, 2002), similar to the algorithm of (Shen et al., 2007; Shen and Joshi, 2008). As usual, we use parameter averaging to prevent the perceptron from overfitting. The training algorithm is initialized with a zero parameter vector w. ~ The algorithm makes several passes over the data. At each pass, we apply the training procedure given in Algorithm 2 to every sentence in the training set. At training time, each sentence is parsed using the parsing algorithm and the current w. ~ Whenever an invalid action is chosen by the parsing algorithm, it is not performed (line 6). Instead, we update the parameter vector w ~ by decreasing the weights of the features associated with the"
N10-1115,P07-1096,0,0.760996,"with -232 with fox fox joy fox a a brown joy 10 (6) 430 joy 246 with brown ATTACH L EFT (1) 186 with jumped fox 12 -146 -149 -133 246 brown (4) ATTACH R IGHT (1) a brown joy brown Figure 1: Parsing the sentence “a brown fox jumped with joy”. Rounded arcs represent possible actions. tionally intensive sampling-based methods (Nakagawa, 2007). As a result, these models, while accurate, are slow (O(n3 ) for projective, first-order models, higher polynomials for higher-order models, and worse for richer tree-feature models). We propose a new category of dependency parsing algorithms, inspired by (Shen et al., 2007): nondirectional easy-first parsing. This is a greedy, deterministic parsing approach, which relaxes the leftto-right processing order of transition-based parsing algorithms. By doing so, we allow the explicit incorporation of rich structural features derived from both sides of the attachment point, and implicitly take into account the entire previously derived structure of the whole sentence. This extension allows the incorporation of much richer features than those available to transition- and especially to graph-based parsers, and greatly reduces the locality of transition-based algorithm d"
N10-1115,D07-1099,0,0.00911446,"cisions to later passes over the sentence, and allows late decisions to make use of rich syntactic information (built in earlier passes) on both sides of the decision point. However, the model is not explicitly trained to optimize attachment ordering, has an O(n2 ) runtime complexity, and produces results which are inferior to current single-pass shift-reduce parsers. Beam Search Several researchers dealt with the early-commitment and error propagation of deterministic parsers by extending the greedy decisions with various flavors of beam-search (Sagae and Lavie, 2006a; Zhang and Clark, 2008; Titov and Henderson, 2007). This approach works well and produces highly competitive results. Beam search can be incorporated into our parser as well. We leave this investigation to future work. Strict left-to-right ordering is also prevalent in sequence tagging. Indeed, one major influence on our work is Shen et.al.’s bi-directional POS-tagging algorithm (Shen et al., 2007), which combines a perceptron learning procedure similar to our own with beam search to produce a state-of-the-art POStagger, which does not rely on left-to-right processing. Shen and Joshi (2008) extends the bidirectional tagging algorithm to LTAG"
N10-1115,W03-3023,0,0.57272,"ecoding for left-toright parsers (Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beamsize. The reported results in (Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k = 6. Our Python-based implementation5 (the perceptron is implemented in a C extension module) parses about 40 tagged sentences per second on an Intel based MacBook laptop. 7 Experiments and Results We evaluate the parser using the WSJ Treebank. The trees were converted to dependency structures with the Penn2Malt conversion program,6 using the headfinding rules from (Yamada and Matsumoto, 2003).7 We use Sections 2-21 for training, Section 22 for development, and Section 23 as the final test set. The text is automatically POS tagged using a trigram HMM based POS tagger prior to training and parsing. Each section is tagged after training the tagger on all other sections. The tagging accuracy of the tagger is 96.5 for the training set and 96.8 for the test set. While better taggers exist, we believe that the simpler HMM tagger overfits less, and is more 5 http://www.cs.bgu.ac.il/∼yoavg/software/ http://w3.msi.vxu.se/∼nivre/research/Penn2Malt.html 7 While other and better conversions ex"
N10-1115,D08-1059,0,0.487865,"in practice than the heap based one, as both are dominated by the O(n) feature extraction, while the cost of the O(n) max calculationis negligible compared to the constants involved in heap maintenance. parser to other dependency parsing frameworks. Parser M ALT M ST M ST 2 B EAM N ON D IR (This Work) Runtime O(n) O(n3 ) O(n3 ) O(n ∗ beam) O(nlogn) Features / Scoring O(n) O(n2 ) O(n3 ) O(n ∗ beam) O(n) Table 1: Complexity of different parsing frameworks. M ST: first order MST parser, M ST 2: second order MST parser, M ALT: shift-reduce left-to-right parsing. B EAM: beam search parser, as in (Zhang and Clark, 2008) In terms of feature extraction and score calculation operations, our algorithm has the same cost as traditional shift-reduce (M ALT) parsers, and is an order of magnitude more efficient than graph-based (M ST) parsers. Beam-search decoding for left-toright parsers (Zhang and Clark, 2008) is also linear, but has an additional linear dependence on the beamsize. The reported results in (Zhang and Clark, 2008) use a beam size of 64, compared to our constant of k = 6. Our Python-based implementation5 (the perceptron is implemented in a C extension module) parses about 40 tagged sentences per secon"
N10-1115,D07-1096,0,\N,Missing
N19-1395,P16-1046,0,0.0394081,", APES reports the average number of questions correctly answered from the summaries produced by the system. This method is especially relevant for the main headline generation dataset used in recent years, the CNN/Daily Mail dataset, as it was initially created for the question answering task by Hermann et al. (2015). It contains 312,085 articles with relevant questions scraped from the two news agencies’ websites. The questions were created by removing different entities from the manually produced highlights to create 1,384,887 fill-inthe-blank questions. The dataset was later repurposed by Cheng and Lapata (2016) and Nallapati et al. (2016) to the summarization task by reconstructing the original highlights from the questions. Fig. 3 shows an example for creating questions out of a given summary. 3.1 Using APES as an Evaluation Metric for any News Datasets When questions are not intrinsically available, one requires to (1) automatically generate relevant questions; (2) use an appropriate automatic QA system. Similarly to the method used in Hermann et al. (2015), we produce fill-in-the-blank questions in the following way: given a reference summary, we find all possible entities, (i.e., Name, Nationali"
N19-1395,D18-1443,0,0.120401,"the reader to See et al. (2017) for a more detailed description of this architecture. Unlike See et al. (2017), we do not train a specific coverage mechanism to avoid repetitions. Instead, we incorporate Wu et al. (2016)’s refinements of beam search in order to manipulate both the summaries’ coverage and their length. In the standard beam search, we search for a sequence Y that maximizes a score function s(Y, X) = log(P (Y |X)). Wu et al. (2016) introduce two additional regularization factors, coverage penalty and length penalty. These two penalties, with an additional refinement suggested in Gehrmann et al. (2018), yield the following score function: 4 s(Y, X) = log(P (Y |X))/lp(Y ) − cp(X; Y ) (5 + |Y |)α lp(Y ) = (5 + 1)α cp(X; Y ) = β(−TX + TX X TY X max( ai,j , 1.0)) i=1 j=1 (2) where α, β are hyper-parameters that control the length and coverage penalties respectively and ai,j is the attention probability of the j-th target word on the i-th source word. cp(X; Y ), the coverage penalty, is designed to discourage repeated attention to the same source word and favor summaries that cover more of the source document with respect to the attention distribution. lp(Y ), the length normalization, is design"
N19-1395,D18-1450,0,0.0196047,"), an automated procedure for finding short fragments of content, has been suggested to automate a method related to Pyramid. Like Pyramid, this method requires multiple human-made gold summaries, making this method expensive in time and cost. Responsiveness (Dang, 2005), another manual metric is a measure of overall quality combining both content selection, like Pyramid, and linguistic quality. Both Pyramid and Responsiveness are the standard manual approaches for content evaluation of summaries. Automated Pyramid evaluation has been attempted in the past (Owczarzak, 2009; Yang et al., 2016; Hirao et al., 2018). This task is complex because it requires (1) identifying SCUs in a text, which requires syntactic parsing and the extraction of key subtrees from the identified units, and (2) the clustering of these extracted textual elements into semantically similar SCUs. These two operations are noisy, and the compounded performance summary evaluation is relying on noisy intermediary representation accordingly suffers. Other relevant quantities for summaries quality assessment include: readability (or fluency), grammaticality, coherence and structure, focus, referential clarity, and non-redundancy. Altho"
N19-1395,D15-1162,0,0.011988,"the original highlights from the questions. Fig. 3 shows an example for creating questions out of a given summary. 3.1 Using APES as an Evaluation Metric for any News Datasets When questions are not intrinsically available, one requires to (1) automatically generate relevant questions; (2) use an appropriate automatic QA system. Similarly to the method used in Hermann et al. (2015), we produce fill-in-the-blank questions in the following way: given a reference summary, we find all possible entities, (i.e., Name, Nationality, Organization, Geopolitical Entity or Facility) using an NER system (Honnibal and Johnson, 2015) and we create fill-in-the-blank type questions where the answers are these entities. We provide code for this procedure and apply it on the APES on the TAC2011 AESOP Task To evaluate if an automatic metric can accurately measure a summarization system performance, we measure its correlation to manual metrics. The TAC 2011 Automatically Evaluating Summaries of Peers (AESOP) task (Owczarzak and Dang, 2011) has provided a dataset that includes, alongside the source documents and reference summaries, three manual metrics: Pyramid (Nenkova et al., 2007), Overall Responsiveness (Dang, 2005) and Ove"
N19-1395,hovy-etal-2006-automated,0,0.343812,"An evaluation library which receives the same input as ROUGE and produces both APES and ROUGE scores.1 (b) Our PyTorch (Paszke et al., 2017) based summarizer that optimizes APES scores together with trained models.2 2 Related Work 2.1 Evaluation Methods Automatic evaluation metrics of summarization methods can be categorized into either intrinsic or extrinsic metrics. Intrinsic metrics measure a summary’s quality by measuring its similarity to a manually produced target gold summary or by inspecting properties of the summary. Examples of such metrics include ROUGE (Lin, 2004), Basic Elements (Hovy et al., 2006) and Pyramid (Nenkova et al., 2007). Alternatively, extrinsic metrics test the ability of a summary to support performing related tasks and compare the performance of humans or systems when completing a task that requires understanding the source document (Steinberger and Jeˇzek, 2012). Such extrinsic tasks may include text categorization, infor3939 1 2 www.github.com/mataney/APES www.github.com/mataney/APES-optimizer mation retrieval, question answering (Jing et al., 1998) or assessing the relevance of a document to a query (Hobson et al., 2007). ROUGE, or “Recall-Oriented Understudy for Gist"
N19-1395,N18-1150,0,0.0159173,"ion. Current summarization models are divided into two approaches, extractive and abstractive. In extractive summarization, summaries are created by selecting a collection of key sentences from the source document (e.g., Nallapati et al. (2017); Narayan et al. (2018)). Abstractive summarization, on the other hand, aims to rephrase and compress the input text in order to create the summary. Progress in sequence-to-sequence models (Sutskever et al., 2014) has led to recent success in abstractive summarization models. Current models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018) made various adjustments to sequence-to-sequence models to gain improvements in ROUGE (Lin, 2004) scores. ROUGE has achieved its status as the most common method for summaries evaluation by showing high correlation to manual evaluation methods, e.g., the Pyramid method (Nenkova Figure 1: Example 3083 from the test set. et al., 2007). Tasks like TAC AESOP (Owczarzak and Dang, 2011) used ROUGE as a strong baseline and confirmed the correlation of ROUGE with manual evaluation. While it has been shown that ROUGE is correlated to Pyramid, Louis and Nenkova (2013) show that this summary level corre"
N19-1395,P17-4012,0,0.0211034,"t 2.6 salient entities are mentioned on average vs. 4.9 in the reference summaries). Notice that solely increasing the number of entities is damaging: mentioning too many entities causes 3942 a decrease in the QA accuracy, as the number of possible answers increases, which would distract the QA system. This has motivated us in suggesting the following model. 5.1 Baseline Model To experiment with direct optimization of APES, we reconstruct as a starting point a model that encapsulates the key techniques used in recent abstractive summarization models. Our model is based on the OpenNMT project (Klein et al., 2017). All PyTorch (Paszke et al., 2017) code, including entities attention and beam search refinement is available online4 . We also include generated summaries and trained models in this repository. Recent work in the field of abstractive summarization (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017) share a common architecture as the foundation for their neural models: an encoder-decoder model (Sutskever et al., 2014) with an attention mechanism (Bahdanau et al., 2014). Nallapati et al. (2016) and See et al. (2017) augment this model with a copy mechanism (Vinya"
N19-1395,P16-1223,0,0.0682124,"Missing"
N19-1395,W04-1013,0,0.300325,"arization, summaries are created by selecting a collection of key sentences from the source document (e.g., Nallapati et al. (2017); Narayan et al. (2018)). Abstractive summarization, on the other hand, aims to rephrase and compress the input text in order to create the summary. Progress in sequence-to-sequence models (Sutskever et al., 2014) has led to recent success in abstractive summarization models. Current models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018) made various adjustments to sequence-to-sequence models to gain improvements in ROUGE (Lin, 2004) scores. ROUGE has achieved its status as the most common method for summaries evaluation by showing high correlation to manual evaluation methods, e.g., the Pyramid method (Nenkova Figure 1: Example 3083 from the test set. et al., 2007). Tasks like TAC AESOP (Owczarzak and Dang, 2011) used ROUGE as a strong baseline and confirmed the correlation of ROUGE with manual evaluation. While it has been shown that ROUGE is correlated to Pyramid, Louis and Nenkova (2013) show that this summary level correlation decreases significantly when only a single reference is given. In contrast to the smaller m"
N19-1395,J13-2002,0,0.267546,"t al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018) made various adjustments to sequence-to-sequence models to gain improvements in ROUGE (Lin, 2004) scores. ROUGE has achieved its status as the most common method for summaries evaluation by showing high correlation to manual evaluation methods, e.g., the Pyramid method (Nenkova Figure 1: Example 3083 from the test set. et al., 2007). Tasks like TAC AESOP (Owczarzak and Dang, 2011) used ROUGE as a strong baseline and confirmed the correlation of ROUGE with manual evaluation. While it has been shown that ROUGE is correlated to Pyramid, Louis and Nenkova (2013) show that this summary level correlation decreases significantly when only a single reference is given. In contrast to the smaller manually curated DUC datasets used in the past, more recent large-scale summarization and headline generation datasets (CNN/Daily Mail (Hermann et al., 2015), Gigaword (Graff et al., 2003), New York Times (Sandhaus, 2008)) provide only a single reference summary for each source document. In this work, 3938 Proceedings of NAACL-HLT 2019, pages 3938–3948 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics we introduce a ne"
N19-1395,K16-1028,0,0.489314,"sion of a source document while preserving its central information. Current summarization models are divided into two approaches, extractive and abstractive. In extractive summarization, summaries are created by selecting a collection of key sentences from the source document (e.g., Nallapati et al. (2017); Narayan et al. (2018)). Abstractive summarization, on the other hand, aims to rephrase and compress the input text in order to create the summary. Progress in sequence-to-sequence models (Sutskever et al., 2014) has led to recent success in abstractive summarization models. Current models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018) made various adjustments to sequence-to-sequence models to gain improvements in ROUGE (Lin, 2004) scores. ROUGE has achieved its status as the most common method for summaries evaluation by showing high correlation to manual evaluation methods, e.g., the Pyramid method (Nenkova Figure 1: Example 3083 from the test set. et al., 2007). Tasks like TAC AESOP (Owczarzak and Dang, 2011) used ROUGE as a strong baseline and confirmed the correlation of ROUGE with manual evaluation. While it has been shown that ROUGE is correlated to Py"
N19-1395,N10-3002,0,0.0315685,"h requires syntactic parsing and the extraction of key subtrees from the identified units, and (2) the clustering of these extracted textual elements into semantically similar SCUs. These two operations are noisy, and the compounded performance summary evaluation is relying on noisy intermediary representation accordingly suffers. Other relevant quantities for summaries quality assessment include: readability (or fluency), grammaticality, coherence and structure, focus, referential clarity, and non-redundancy. Although some automatic methods were suggested as summarization evaluation metrics (Vadlapudi and Katragadda, 2010; Tay et al., 2017), these metrics are commonly assessed manually, and, therefore, rarely reported as part of experiments. Our proposed evaluation method, APES, attempts to capture the capability of a summary to enable readers to answer questions – similar to the manual task initially discussed in Jing et al. (1998) and recently reported in Narayan et al. (2018). Our contribution consists of automating this method and assessing the feasibility of the resulting approximation. 2.2 Neural Methods for Abstractive and Extractive Summarization The first paper to use an end-to-end neural network for"
N19-1395,N18-1158,0,0.120423,"strength of this metric by comparing it to known manual evaluation metrics. We then present an end-to-end neural abstractive model that maximizes APES, while increasing ROUGE scores to competitive results. 1 Introduction The task of automatic text summarization aims to produce a concise version of a source document while preserving its central information. Current summarization models are divided into two approaches, extractive and abstractive. In extractive summarization, summaries are created by selecting a collection of key sentences from the source document (e.g., Nallapati et al. (2017); Narayan et al. (2018)). Abstractive summarization, on the other hand, aims to rephrase and compress the input text in order to create the summary. Progress in sequence-to-sequence models (Sutskever et al., 2014) has led to recent success in abstractive summarization models. Current models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018) made various adjustments to sequence-to-sequence models to gain improvements in ROUGE (Lin, 2004) scores. ROUGE has achieved its status as the most common method for summaries evaluation by showing high correlation to manual evaluation metho"
N19-1395,D15-1044,0,0.452302,"metrics are commonly assessed manually, and, therefore, rarely reported as part of experiments. Our proposed evaluation method, APES, attempts to capture the capability of a summary to enable readers to answer questions – similar to the manual task initially discussed in Jing et al. (1998) and recently reported in Narayan et al. (2018). Our contribution consists of automating this method and assessing the feasibility of the resulting approximation. 2.2 Neural Methods for Abstractive and Extractive Summarization The first paper to use an end-to-end neural network for the summarization task was Rush et al. (2015): this work is based on a sequence-to-sequence model (Sutskever et al., 2014) augmented with an attention mechanism (Bahdanau et al., 2014). Nallapati et al. (2016) was the first to tackle the headline generation problem using the CNN/Daily Mail dataset (Hermann et al., 2015) adopted for the summarization task. See et al. (2017) followed the work of Nallapati et al. (2016) and added an additional loss term to reduce repetitions at decoding time. Paulus et al. (2017) introduces intra-attention in order to attend over both the input and previously generated outputs. The authors also present a hy"
N19-1395,P17-1099,0,0.351863,"t while preserving its central information. Current summarization models are divided into two approaches, extractive and abstractive. In extractive summarization, summaries are created by selecting a collection of key sentences from the source document (e.g., Nallapati et al. (2017); Narayan et al. (2018)). Abstractive summarization, on the other hand, aims to rephrase and compress the input text in order to create the summary. Progress in sequence-to-sequence models (Sutskever et al., 2014) has led to recent success in abstractive summarization models. Current models (Nallapati et al., 2016; See et al., 2017; Paulus et al., 2017; Celikyilmaz et al., 2018) made various adjustments to sequence-to-sequence models to gain improvements in ROUGE (Lin, 2004) scores. ROUGE has achieved its status as the most common method for summaries evaluation by showing high correlation to manual evaluation methods, e.g., the Pyramid method (Nenkova Figure 1: Example 3083 from the test set. et al., 2007). Tasks like TAC AESOP (Owczarzak and Dang, 2011) used ROUGE as a strong baseline and confirmed the correlation of ROUGE with manual evaluation. While it has been shown that ROUGE is correlated to Pyramid, Louis and N"
N19-1395,1983.tc-1.13,0,0.4772,"Missing"
P06-1084,W05-0708,0,0.0221881,"Missing"
P06-1084,A94-1009,0,0.0211251,"Missing"
P06-1084,P05-1071,0,0.237769,"Missing"
P06-1084,J95-3004,0,0.545421,"Missing"
P06-1084,W05-0706,0,0.647135,"Missing"
P06-1084,J94-2001,0,0.185003,"Missing"
P06-1084,J95-4004,0,0.172726,"Missing"
P06-1084,P99-1023,0,0.0640614,"Missing"
P06-1084,W05-0702,0,0.0762126,"(into prefix, baseform, and suffix). Our main result is that best performance is obtained when learning segmentation and morpheme tagging in one step, which is made possible by an appropriate text representation. 2 Hebrew and Arabic Tagging Previous Work Several works have dealt with Hebrew tagging in the past decade. In Hebrew, morphological analysis requires complex processing according to the rules of Hebrew word formation. The task of a morphological analyzer is to produce all possible analyses for a given word. Recent analyzers provide good performance and documentation of this process (Yona and Wintner, 2005; Segal, 2000). Morphological analyzers rely on a dictionary, and their performance is, therefore, impacted by the occurrence of unknown words. The task of a morphological disambiguation system is to pick the most likely analysis produced by an analyzer in the context of a full sentence. Levinger et al. (1995) developed a context-free method in order to acquire the morpho-lexical probabilities, from an untagged corpus. Their method handles the data sparseness problem by using a set of similar words for each word, built according to a set of rules. The rules produce variations of the morphologi"
P06-1084,N04-4038,0,0.09729,"Missing"
P06-1087,P06-1084,1,0.892794,"Missing"
P06-1087,P98-1034,0,0.568272,"brew. We describe here our experiment settings, and provide the best scores obtained for each method, in comparison to the reported scores for English. All tests were done on the corpus derived from the Hebrew Tree Bank. The corpus contains 5,000 sentences, for a total of 120K tokens (agglutinated words) and 27K NP chunks (more details on the corpus appear below). The last 500 sentences were used as the test set, and all the other sentences were used for training. The results were evaluated using the CoNLL shared task evaluation tools 5 . The approaches tested were Error Driven Pruning (EDP) (Cardie and Pierce, 1998) and Transformational Based Learning of IOB tagging (TBL) (Ramshaw and Marcus, 1995). The Error Driven Pruning method does not take into account lexical information and uses only the PoS tags. For the Transformation Based method, we have used both the PoS tag and the word itself, with the same templates as described in (Ramshaw and Marcus, 1995). We tried the Transformational Based method with more features than just the PoS and the word, but obtained lower performance. Our best results for these methods, as well as the CoNLL baseline (BASE), are presented in Table 3. These results confirm tha"
P06-1087,N04-4038,0,0.0302203,"2 4.3 words surrounding the given word, and their PoS tags). One model that allows for this prediction is Support Vector Machines - SVM (Vapnik, 1995). SVM is a supervised machine learning algorithm which can handle gracefully a large set of overlapping features. SVMs learn binary classifiers, but the method can be extended to multiclass classification (Allwein et al., 2000; Kudo and Matsumoto, 2000). SVMs have been successfully applied to many NLP tasks since (Joachims, 1998), and specifically for base phrase chunking (Kudo and Matsumoto, 2000; 2003). It was also successfully used in Arabic (Diab et al., 2004). The traditional setting of SVM for chunking uses for the context of the token to be classified a window of two tokens around the word, and the features are the PoS tags and lexical items (word forms) of all the tokens in the context. Some settings (Kudo and Matsumoto, 2000) also include the IOB tags of the two “previously tagged” tokens as features (see Fig. 1). This setting (including the last 2 IOB tags) performs nicely for the case of Hebrew Simple NPs chunking as well. Linguistic features are mapped to SVM feature vectors by translating each feature such as “PoS at location n-2 is NOUN”"
P06-1087,P05-1071,0,0.049593,"Missing"
P06-1087,W00-0730,0,0.495824,"Missing"
P06-1087,P03-1004,0,0.0996795,"Missing"
P06-1087,W98-1418,1,0.781308,"Missing"
P06-1087,W95-0107,0,0.299455,"d for each method, in comparison to the reported scores for English. All tests were done on the corpus derived from the Hebrew Tree Bank. The corpus contains 5,000 sentences, for a total of 120K tokens (agglutinated words) and 27K NP chunks (more details on the corpus appear below). The last 500 sentences were used as the test set, and all the other sentences were used for training. The results were evaluated using the CoNLL shared task evaluation tools 5 . The approaches tested were Error Driven Pruning (EDP) (Cardie and Pierce, 1998) and Transformational Based Learning of IOB tagging (TBL) (Ramshaw and Marcus, 1995). The Error Driven Pruning method does not take into account lexical information and uses only the PoS tags. For the Transformation Based method, we have used both the PoS tag and the word itself, with the same templates as described in (Ramshaw and Marcus, 1995). We tried the Transformational Based method with more features than just the PoS and the word, but obtained lower performance. Our best results for these methods, as well as the CoNLL baseline (BASE), are presented in Table 3. These results confirm that the task of Simple NP chunking is harder in Hebrew than in English. 4.2 Support Ve"
P06-1087,N03-1028,0,0.283133,"Missing"
P06-1087,W00-0726,0,0.269406,"Missing"
P06-1087,C98-1034,0,\N,Missing
P07-1029,P06-1084,1,0.81795,"Missing"
P07-1029,P98-1034,0,0.0933902,"d for Prepositions and Punctuation marks, followed by Adverbs, and Conjunctions. Strikingly, lexical information for most open-class PoS (including Proper Names and Nouns) has very little impact on Hebrew chunking performance. From this observation, one could conclude that enriching a model based only on PoS with lexical features for only a few closed-class PoS (prepositions and punctuation) could provide appropriate results even with a simpler learning method, one that cannot deal with a large number of features. We tested this hypothesis by training the Error-Driven Pruning (EDP) method of (Cardie and Pierce, 1998) with an extended set of features. EDP with PoS features only produced an F-result of 76.3 on HEBGold . By adding lexical features only for prepositions {}של כ ה ב מ, one conjunction { }וand punctuation, the F-score on HEBGold indeed jumps to 85.4. However, when applied on HEBErr , EDP falls down again to 59.4. This striking disparity, by comparison, lets us appreciate the resilience of the SVM model to PoS tagging errors, and its generalization capability even with a reduced number of lexical features. Another implication of this data is that commas and quotation marks play a major role i"
P07-1029,W00-0730,0,0.11155,"Missing"
P07-1029,P03-1004,0,0.121983,"Missing"
P07-1029,W95-0107,0,0.203013,"Missing"
P07-1029,C02-1101,0,0.0595552,"Missing"
P07-1029,W00-0726,0,0.105694,"Missing"
P07-1029,W99-0606,0,\N,Missing
P07-1029,C98-1034,0,\N,Missing
P07-1029,P06-1087,1,\N,Missing
P08-1083,W05-0706,0,0.58326,"Missing"
P08-1083,N04-4038,0,0.0459524,"Missing"
P08-1083,P06-1086,0,0.0261585,"eceded by the identified prefix, we remove this possible analysis. The eventual outcome of the 732 Patterns Word formation in Hebrew is based on root+pattern and affixation. Patterns can be used to identify the lexical category of unknowns, as well as other inflectional properties. Nir (1993) investigated word-formation in Modern Hebrew with a special focus on neologisms; the most common wordformation patterns he identified are summarized in Table 3. A naive approach for unknown resolution would add all analyses that fit any of these patterns, for any given unknown token. As recently shown by Habash and Rambow (2006), the precision of such a strategy can be pretty low. To address this lack of precision, we learn a maximum entropy model on the basis of the following binary features: one feature for each pattern listed in column Formation of Table 3 (40 distinct patterns) and one feature for “no pattern”. Pattern-Letters This maximum entropy model is learned by combining the features of the letters model and the patterns model. Linear-Context-based p(t|c) approximation The three models above are context free. The linear-context model exploits information about the lexical context of the unknown words: to es"
P08-1083,J95-3004,0,0.422895,"r “no pattern”. Pattern-Letters This maximum entropy model is learned by combining the features of the letters model and the patterns model. Linear-Context-based p(t|c) approximation The three models above are context free. The linear-context model exploits information about the lexical context of the unknown words: to estimate the probability for a tag t given a context c – p(t|c) – based on all the words in which a context occurs, the algorithm works on the known words in the corpus, by starting with an initial tag-word estimate p(t|w) (such as the morpho-lexical approximation, suggested by Levinger et al. (1995)), and iteratively re-estimating: P pˆ(t|c) = pˆ(t|w) = p(t|w)p(w|c) Z P c∈C p(t|c)p(c|w)allow(t, w) Z w∈W where Z is a normalization factor, W is the set of all words in the corpus, C is the set of contexts. allow(t, w) is a binary function indicating whether t is a valid tag for w. p(c|w) and p(w|c) are estimated via raw corpus counts. Loosely speaking, the probability of a tag given a context is the average probability of a tag given any Category Verb Participle Noun Adjective Adverb Formation ’iCCeC miCCeC CiCCen Template CiCCet tiCCeC meCuCaca Template muCCaC maCCiC ut ay an Suffixation o"
P08-1083,W07-0813,0,0.0235512,"Missing"
P08-1083,J97-3003,0,0.119714,"Missing"
P08-1083,C04-1067,0,0.0723092,"Missing"
P08-1083,P99-1023,0,0.0662197,"Missing"
P08-1083,J93-2006,0,0.24914,"Missing"
P08-1083,J95-2001,0,\N,Missing
P08-1085,C04-1080,0,0.8171,"Missing"
P08-1085,J95-4004,0,0.25719,"Missing"
P08-1085,W95-0101,0,0.853773,"Missing"
P08-1085,P00-1035,0,0.0238717,"each analysis of a word. This set is composed of morphological variations of the word under the given analysis. For example, the Hebrew token  ילדcan be analyzed as either a noun (boy) or a verb (gave birth). The noun SW set for this token is composed of the definiteness and number inflections הילדים,ילדים,( הילדthe boy, boys, the boys), while the verb SW set is composed of gender and tense inflections ילדו,( ילדהshe/they gave birth). The approximated probability of each analysis is based on the corpus frequency of its SW set. For the complete details, refer to the original paper. Cucerzan and Yarowsky (2000) proposed a similar method for the unsupervised estimation of p(t|w) in English, relying on simple spelling features to characterize similar word classes. Linear-Context-based p(t|w) approximation The method of Levinger et al. makes use of Hebrew inflection patterns in order to estimate context free approximation of p(t|w) by relating a word to its different inflections. However, the context in which a word occurs can also be very informative with respect to its POS-analysis (Sch¨utze, 1995). We propose a novel algorithm for estimating p(t|w) based on the contexts in which a word occurs.3 The"
P08-1085,J95-2001,0,0.0163513,"(w|c) and p(c|w) via relative frequency over all the events w1, w2, w3 occurring at least 10 times in the corpus. allow(t, w) follows the dictionary. Because of the wide coverage of the Hebrew lexicon, we take RELC to be C (all available contexts). Application to Hebrew In Hebrew, several words combine into a single token in both agglutinative and fusional ways. This results in a potentially high number of tags for each token. On average, in our corpus, the number of possible analyses per known word reached 2.7, with the ambiguity level of the extended POS tagset in corpus for English (1.41) (Dermatas and Kokkinakis, 1995). In this work, we use the morphological analyzer of MILA – Knowledge Center for Processing Hebrew (KC analyzer). In contrast to English tagsets, the number of tags for Hebrew, based on all combinations of the morphological attributes, can grow theoretically to about 300,000 tags. In practice, we found ‘only’ about 3,560 tags in a corpus of 40M tokens training corpus taken from Hebrew news material and Knesset transcripts. For testing, we manually tagged the text which is used in the Hebrew Treebank (Sima’an et al., 2001) (about 90K tokens), according to our tagging guidelines. 4.1 Initial Con"
P08-1085,A94-1009,0,0.322322,"Missing"
P08-1085,P07-1094,0,0.523311,"Missing"
P08-1085,N06-1041,0,0.0680158,"unsupervised PoS tagging of Hebrew text and for the common WSJ English test sets. We show that our method achieves state-ofthe-art results for the English setting, even with a relatively small dictionary. Furthermore, while recent work report results on a reduced English tagset of 17 PoS tags, we also present results for the complete 45 tags tagset of the WSJ corpus. This considerably raises the bar of the EM-HMM baseline. We also report state-of-the-art results for Hebrew full mor1 Another notable work, though within a slightly different framework, is the prototype-driven method proposed by (Haghighi and Klein, 2006), in which the dictionary is replaced with a very small seed of prototypical examples. 747 phological disambiguation. Our primary conclusion is that the problem of learning effective stochastic classifiers remains primarily a search task. Initial conditions play a dominant role in solving this task and can rely on linguistically motivated approximations. A robust learning method (EM-HMM) combined with good initial conditions based on a robust feature set can go a long way (as opposed to a more complex learning method). It seems that computing initial conditions is also the right place to captu"
P08-1085,J95-3004,0,0.0692931,"experiment with constraining the p(t|t−1 , t+1 ) distribution. 2 Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). 748 General syntagmatic constraints We set linguistically motivated constraints on the p(t|t−1 , t+1 ) distribution. In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”). Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus. The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word. This set is composed of morphological variations of the word under the given analysis. For example, the Hebrew token  ילדcan be analyzed as either a noun (boy) or a verb (gave birth). The noun SW set for this token is composed of the definiteness and number inflections הילדים,ילדים,( הילדthe boy, boys, the boys), while the verb SW set is composed of gender and t"
P08-1085,J94-2001,0,0.927128,", 2007) (GG), (Toutanova and Johnson, 2008) (TJ). Introduction The task of unsupervised (or semi-supervised) partof-speech (POS) tagging is the following: given a dictionary mapping words in a language to their possible POS, and large quantities of unlabeled text data, learn to predict the correct part of speech for a given word in context. The only supervision given to the learning process is the dictionary, which in a realistic scenario, contains only part of the word types observed in the corpus to be tagged. Unsupervised POS tagging has been traditionally approached with relative success (Merialdo, 1994; Kupiec, 1992) by HMM-based generative models, employing EM parameters estimation using the Baum-Welch algorithm. However, as recently noted ∗ This work is supported in part by the Lynn and William Frankel Center for Computer Science. All the work mentioned above focuses on unsupervised English POS tagging. The dictionaries are all derived from tagged English corpora (all recent work uses the WSJ corpus). As such, the setting of the research is artificial: there is no reason to perform unsupervised learning when an annotated corpus is available. The problem is rather approached as a workbench"
P08-1085,E95-1020,0,0.660222,"Missing"
P08-1085,D07-1046,0,0.0215821,"Missing"
P08-1085,P05-1044,0,0.596615,"Missing"
P08-1085,P99-1023,0,0.0305171,"del can be estimated by applying the Baum-Welch EM algorithm (Baum, 1972), on a large-scale corpus of unlabeled text. The estimated parameters are then used in conjunction with Viterbi search, to find the most probable sequence of tags for a given sentence. In this work, we follow Adler (2007) and use a variation of second-order HMM in which the probability of a tag is conditioned by the tag that precedes it and by the one that follows it, and the probability of an emitted word is conditioned by its tag and the tag that follows it2 . In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t−1 , t+1 ) distribution. 2 Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). 748 General syntagmatic constraints We set linguistically m"
P08-1085,N03-1033,0,0.0364511,"In all experiments, we use the backoff smoothing method of (Thede and Harper, 1999), with additive smoothing (Chen, 1996) for the lexical probabilities. We investigate methods to approximate the initial parameters of the p(t|w) distribution, from which we obtain p(w|t) by marginalization and Bayesian inversion. We also experiment with constraining the p(t|t−1 , t+1 ) distribution. 2 Technically this is not Markov Model but a Dependency Net. However, bidirectional conditioning seem more suitable for language tasks, and in practice the learning and inference methods are mostly unaffected. See (Toutanova et al., 2003). 748 General syntagmatic constraints We set linguistically motivated constraints on the p(t|t−1 , t+1 ) distribution. In our setting, these are used to force the probability of some events to 0 (e.g., “Hebrew verbs can not be followed by the of preposition”). Morphology-based p(t|w) approximation Levinger et al. (1995) developed a context-free method for acquiring morpho-lexical probabilities (p(t|w)) from an untagged corpus. The method is based on language-specific rules for constructing a similar words (SW) set for each analysis of a word. This set is composed of morphological variations of"
P08-1085,J93-2006,0,0.0324374,"Missing"
P08-2060,P07-1029,1,0.833607,"Zipfian nature of language phenomena is reflected in the distribution of features in the support vectors. It is because of common features that the PKI reverse indexing method does not yield great improvements: if at least one of the features of the current instance is active in a support vector, this vector is taken into account in the sum calculation, and the common features are active in many support vectors. On the other hand, the long tail of rare features is the reason the Kernel Expansion methods requires 2 This loss of accuracy in comparison to the PKE approach is to be expected, as (Goldberg and Elhadad, 2007) showed that the effect of removing features prior to the learning stage is much more severe than removing them after the learning stage. 3 Our presentation is for the case where d = 2, as this is by far the most useful kernel. However, the method can be easily adapted to higher degree kernels as well. For completeness, our toolkit provides code for d = 3 as well as 2. 239 so much space: every rare feature adds many possible feature pairs. We propose a combined method. We first split common from rare features. We then use Kernel Expansion on the few common features, and PKI for the remaining r"
P08-2060,C02-1054,0,0.179192,"ned popularity as they constantly outperform other learning algorithms for many NLP tasks. Unfortunately, once a model is trained, the decision function for kernel-based classifiers such as SVM is expensive to compute, and can grow linearly with the size of the training data. In contrast, the computational complexity for the decisions functions of most non-kernel based classifiers does not depend on the size of the training data, making them orders of magnitude faster to compute. For this reason, research effort was directed at speeding up the classification process of polynomial-kernel SVMs (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Wu et al., 2007). Existing accelerated SVM solutions, however, either require large amounts of Background and Previous Work In classification based NLP algorithms, a word and its context is considered a learning sample, and encoded as Feature Vectors. Usually, context data includes the word being classified (w0 ), its part-ofspeech (PoS) tag (p0 ), word forms and PoS tags of neighbouring words (w−2 , . . . , w+2 , p−2 , . . . , p+2 , etc.). Computed features such as the length of a word or its suffix may also be added. A feature vector (F ) is encoded as an indexed"
P08-2060,P03-1004,0,0.261932,"stantly outperform other learning algorithms for many NLP tasks. Unfortunately, once a model is trained, the decision function for kernel-based classifiers such as SVM is expensive to compute, and can grow linearly with the size of the training data. In contrast, the computational complexity for the decisions functions of most non-kernel based classifiers does not depend on the size of the training data, making them orders of magnitude faster to compute. For this reason, research effort was directed at speeding up the classification process of polynomial-kernel SVMs (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Wu et al., 2007). Existing accelerated SVM solutions, however, either require large amounts of Background and Previous Work In classification based NLP algorithms, a word and its context is considered a learning sample, and encoded as Feature Vectors. Usually, context data includes the word being classified (w0 ), its part-ofspeech (PoS) tag (p0 ), word forms and PoS tags of neighbouring words (w−2 , . . . , w+2 , p−2 , . . . , p+2 , etc.). Computed features such as the length of a word or its suffix may also be added. A feature vector (F ) is encoded as an indexed list of all the features p"
P08-2060,nivre-etal-2006-maltparser,0,0.10878,"Department of Computer Science POB 653 Be’er Sheva, 84105, Israel {yoavg,elhadad}@cs.bgu.ac.il Abstract memory, or resort to heuristics – computing only an approximation to the real decision function. This work aims at speeding up the decision function computation for low-degree polynomial kernel classifiers while using only a modest amount of memory and still computing the exact function. This is achieved by taking into account the Zipfian nature of natural language data, and structuring the computation accordingly. On a sample application (replacing the libsvm classifier used by MaltParser (Nivre et al., 2006) with our own), we observe a speedup factor of 30 in parsing time. We present a fast, space efficient and nonheuristic method for calculating the decision function of polynomial kernel classifiers for NLP applications. We apply the method to the MaltParser system, resulting in a Java parser that parses over 50 sentences per second on modest hardware without loss of accuracy (a 30 time speedup over existing methods). The method implementation is available as the open-source splitSVM Java library. 1 Introduction 2 Over the last decade, many natural language processing tasks are being cast as cla"
P08-2060,P07-2017,0,0.777603,"earning algorithms for many NLP tasks. Unfortunately, once a model is trained, the decision function for kernel-based classifiers such as SVM is expensive to compute, and can grow linearly with the size of the training data. In contrast, the computational complexity for the decisions functions of most non-kernel based classifiers does not depend on the size of the training data, making them orders of magnitude faster to compute. For this reason, research effort was directed at speeding up the classification process of polynomial-kernel SVMs (Isozaki and Kazawa, 2002; Kudo and Matsumoto, 2003; Wu et al., 2007). Existing accelerated SVM solutions, however, either require large amounts of Background and Previous Work In classification based NLP algorithms, a word and its context is considered a learning sample, and encoded as Feature Vectors. Usually, context data includes the word being classified (w0 ), its part-ofspeech (PoS) tag (p0 ), word forms and PoS tags of neighbouring words (w−2 , . . . , w+2 , p−2 , . . . , p+2 , etc.). Computed features such as the length of a word or its suffix may also be added. A feature vector (F ) is encoded as an indexed list of all the features present in the trai"
P11-2124,P08-1083,1,0.819379,"shing lexical items from non-terminals by a specified marking instead of by their position in the chart. We 706 modified the PCFG-LA Berkeley parser to accept lattice input at inference time (training is performed as usual on fully observed treebank trees). Lattice Construction We construct the token lattices using MILA, a lexicon-based morphological analyzer which provides a set of possible analyses for each token (Itai and Wintner, 2008). While being a high-coverage lexicon, its coverage is not perfect. For the future, we consider using unknown handling techniques such as those proposed in (Adler et al., 2008). Still, the use of the lexicon for lattice construction rather than relying on forms seen in the treebank is essential to achieve parsing accuracy. Lexical Probabilities Estimation Lexical p(t → w) probabilities are defined over individual segments rather than for complete tokens. It is the role of the syntactic model to assign probabilities to contexts which are larger than a single segment. We use the default lexical probability estimation of the Berkeley parser.3 Goldberg et al. (2009) suggest to estimate lexical probabilities for rare and unseen segments using emission probabilities of an"
P11-2124,P11-2037,1,0.894174,"Missing"
P11-2124,W09-1008,0,0.068176,"Missing"
P11-2124,P08-1043,1,0.867707,"1) A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process, in which the input string is pre-segmented prior to handing it to a parser. The shortcoming of this method, as noted by (Tsarfaty, 2006), is that many segmentation decisions cannot be resolved based on local context alone. Rather, they may depend on long distance relations and interact closely with the syntactic structure of the sentence. Thus, segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of"
P11-2124,P08-1085,1,0.89567,"Missing"
P11-2124,E09-1038,1,0.918851,"Missing"
P11-2124,C10-1045,0,0.324071,"d be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser (Petrov et al., 2006). 704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Aspects of Modern Hebrew Some aspects that make Hebrew challenging from a language-processing perspective are: Affixation Common function words a"
P11-2124,D09-1087,0,0.0458827,"lit non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At inference time, the latent annotations are (approximately) marginalized out, resulting in the (approximate) most probable unannotated tree according to the refined grammar. This parsing methodology is very robust, producing state of the art accuracies for English, as well as many other languages including German (Petrov and Klein, 2008), French (Candito et al., 2009) and Chinese (Huang and Harper, 2009) among others. The grammar learning process is applied to binarized parse trees, with 1st-order vertical and 0thorder horizontal markovization. This means that in 2 http://code.google.com/p/berkeleyparser/ Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the unsegmented form, such as the definite article h (1-3) and the pronomi"
P11-2124,P03-1054,0,0.0239838,") makes it hard to guess the morphological analyses 705 of an unknown word based on its prefix and suffix, as usually done in other languages. Unvocalized writing system Most vowels are not marked in everyday Hebrew text, which results in a very high level of lexical and morphological ambiguity. Some tokens can admit as many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2 , works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitti"
P11-2124,P05-1010,0,0.0851098,"level of lexical and morphological ambiguity. Some tokens can admit as many as 15 distinct readings. Agreement Hebrew grammar forces morphological agreement between Adjectives and Nouns (which should agree on Gender and Number and definiteness), and between Subjects and Verbs (which should agree on Gender and Number). 3 PCFG-LA Grammar Estimation Klein and Manning (2003) demonstrated that linguistically informed splitting of non-terminal symbols in treebank-derived grammars can result in accurate grammars. Their work triggered investigations in automatic grammar refinement and statesplitting (Matsuzaki et al., 2005; Prescher, 2005), which was then perfected by (Petrov et al., 2006; Petrov, 2009). The model of (Petrov et al., 2006) and its publicly available implementation, the Berkeley parser2 , works by starting with a bare-bones treebank derived grammar and automatically refining it in split-merge-smooth cycles. The learning works by iteratively (1) splitting each non-terminal category in two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tr"
P11-2124,W08-1005,0,0.0368367,"two, (2) merging back non-effective splits and (3) smoothing the split non-terminals toward their shared ancestor. Each of the steps is followed by an EM-based parameter re-estimation. This process allows learning tree annotations which capture many latent syntactic interactions. At inference time, the latent annotations are (approximately) marginalized out, resulting in the (approximate) most probable unannotated tree according to the refined grammar. This parsing methodology is very robust, producing state of the art accuracies for English, as well as many other languages including German (Petrov and Klein, 2008), French (Candito et al., 2009) and Chinese (Huang and Harper, 2009) among others. The grammar learning process is applied to binarized parse trees, with 1st-order vertical and 0thorder horizontal markovization. This means that in 2 http://code.google.com/p/berkeleyparser/ Figure 1: Lattice representation of the sentence bclm hneim. Double-circles denote token boundaries. Lattice arcs correspond to different segments of the token, each lattice path encodes a possible reading of the sentence. Notice how the token bclm have analyses which include segments which are not directly present in the un"
P11-2124,P06-1055,0,0.39215,"imented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better grammars contribute to better segmentation accuracies. Goldberg et al. (2009) showed that segmentation and parsing accuracies can be further improved by extending the lexical coverage of a lattice-parser using an external resource. Recently, Green and Manning (2010) demonstrated the effectiveness of lattice-parsing for parsing Arabic. Here, we report the results of experiments coupling lattice parsing together with the currently best grammar learning method: the Berkeley PCFG-LA parser (Petrov et al., 2006). 704 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 704–709, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Aspects of Modern Hebrew Some aspects that make Hebrew challenging from a language-processing perspective are: Affixation Common function words are prefixed to the following word. These include: m(“from”) f (“who”/“that”) h(“the”) w(“and”) k(“like”) l(“to”) and b(“in”). Several such elements may attach together, producing forms such as wfmhfmf (w-f-m-hfmf “and-that-from-the-sun”). Notice"
P11-2124,W10-1405,0,0.078517,"Missing"
P11-2124,P06-3009,0,0.282706,"affixes to content bearing words, sharing the same space-delimited token. For example, the Hebrew token bcl1 can be interpreted as the single noun meaning “onion”, or as a sequence of a preposition and a noun b-cl meaning “in (the) shadow”. In such languages, the sequence of lexical 1 We adopt here the transliteration scheme of (Sima’an et al., 2001) A common method of approaching the discrepancy between input strings and space delimited tokens is using a pipeline process, in which the input string is pre-segmented prior to handing it to a parser. The shortcoming of this method, as noted by (Tsarfaty, 2006), is that many segmentation decisions cannot be resolved based on local context alone. Rather, they may depend on long distance relations and interact closely with the syntactic structure of the sentence. Thus, segmentation decisions should be integrated into the parsing process and not performed as an independent preprocessing step. Goldberg and Tsarfaty (2008) demonstrated the effectiveness of lattice parsing for jointly performing segmentation and parsing of Hebrew text. They experimented with various manual refinements of unlexicalized, treebank-derived grammars, and showed that better gra"
P14-1086,P07-2049,0,0.0571809,"Missing"
P14-1086,J11-1001,0,0.0476672,"Missing"
P14-1086,P06-1039,0,0.0763386,"Missing"
P14-1086,N09-1041,0,0.0721736,"Missing"
P14-1086,W07-1426,0,0.0182486,"Missing"
P14-1086,W04-1013,0,0.00941564,"topic published at later dates. This task helps us understand how update summaries identified and focused on new information while reducing redundancy compared to the original summaries. The TAC 2008 dataset includes 48 sets of 20 documents, each cluster split in two subsets of 10 documents (called A and B). Subset B documents were more recent. Original summaries were generated for the A subsets and update summaries were then produced for the B subsets. Human summaries and candidate systems are evaluated using the Pyramid method (Nenkova and Passonneau, 2004). For automatic evaluation, ROUGE (Lin, 2004) variants have been proposed (Conroy et al., 2011). In contrast to this setup, QCFS distinguishes the subsets of documents considered at each step of the process by facets of the underlying topic, and not by chronology. In addition, the document subsets are not identified as part of the task in QCFS (as opposed to the explicit split in A and B subsets in Update Summarization). Most systems working on Update Summarization have focused on removing redundancy. DualSum (Delort and Alfonseca, 2012) is notable in attempting to directly model novelty using a specialized topic-model to distinguish wor"
P14-1086,N04-1019,0,0.18024,"ti-document summary for a different set of articles on the same topic published at later dates. This task helps us understand how update summaries identified and focused on new information while reducing redundancy compared to the original summaries. The TAC 2008 dataset includes 48 sets of 20 documents, each cluster split in two subsets of 10 documents (called A and B). Subset B documents were more recent. Original summaries were generated for the A subsets and update summaries were then produced for the B subsets. Human summaries and candidate systems are evaluated using the Pyramid method (Nenkova and Passonneau, 2004). For automatic evaluation, ROUGE (Lin, 2004) variants have been proposed (Conroy et al., 2011). In contrast to this setup, QCFS distinguishes the subsets of documents considered at each step of the process by facets of the underlying topic, and not by chronology. In addition, the document subsets are not identified as part of the task in QCFS (as opposed to the explicit split in A and B subsets in Update Summarization). Most systems working on Update Summarization have focused on removing redundancy. DualSum (Delort and Alfonseca, 2012) is notable in attempting to directly model novelty using"
P14-1086,E12-1022,0,\N,Missing
P14-1086,P11-1138,0,\N,Missing
P90-1020,P84-1008,0,0.021126,"that we call input and grammar. We define L as a set of labels or attribute names and C as a set of constants, or simple atomic values. A string of labels (that is an element of L*) is called a path, and is noted <11...11,&gt;. A grammar defines a domain of admissible paths, A c L*. A defines the skeleton of well-formed FDs. • An F D can be an atom (element of 6&apos;) or a set of features. One of the most attractive characteristics of FU is that non-atomic FDs can be abstractly viewed in two ways: either as a fiat list of equations or as a structure equivalent to a directed graph with labeled arcs (Karttunen, 1984). The possibility of using a nonstructured representation removes the emphasis that has traditionally been placed on structure and constituency in language. I I Pronoun --I I [ Question Personal Demonstrative Quantified Noun Proper I Count Common ---I I Mass Figure l: A systemforNPs Let&apos;s consider a fragment of grammar describing noun-phrases (NPs) (cf Figure 1) using the systemic notation given in (Winograd, 1983). Systemic networks, such as this one, encode the choices that need to be made to produce a complex linguistic entity. They indicate how features can be combined or whether features"
P90-1020,P86-1038,0,0.0166968,"text generation. 1 INTRODUCTION Unification-based formalisms are increasingly used in linguistic theories (Shieber, 1986) and computational linguistics. In particular, one type of unification formalism, functional unification grammar (FUG) is widely used for text generation (Kay, 1979, McKeown, 1985, Appelt, 1985, Paris, 1987, McKeown & Elhadad, 1990) and is beginning to be used for parsing (Kay, 1985, Kasper, 1987). FUG enjoys such popularity mainly because it allies expressiveness with a simple economical formalism. It uses very few primitives, has a clean semantics (Pereira&Shieber, 1984, Kasper & Rounds, 1986, E1hadad, 1990), is monotonic, and grants equal status to function and structure in the descriptions. We have implemented a functional unifier (EIhadad, 1988) covering all the features described in (Kay, 1979) and (McKeown & Paris, 1987). Having used this implementation extensively, we have found all these properties very useful, but we also have met with limitations. The functional unification (FU) formalism is not well suited for the expression of simple, yet very common, taxonomic relations. The traditional way to implement such relations in FUG is verbose, inefficient and unreadable. It i"
P90-1020,P87-1014,0,0.0253723,"UG) is widely used for text generation (Kay, 1979, McKeown, 1985, Appelt, 1985, Paris, 1987, McKeown & Elhadad, 1990) and is beginning to be used for parsing (Kay, 1985, Kasper, 1987). FUG enjoys such popularity mainly because it allies expressiveness with a simple economical formalism. It uses very few primitives, has a clean semantics (Pereira&Shieber, 1984, Kasper & Rounds, 1986, E1hadad, 1990), is monotonic, and grants equal status to function and structure in the descriptions. We have implemented a functional unifier (EIhadad, 1988) covering all the features described in (Kay, 1979) and (McKeown & Paris, 1987). Having used this implementation extensively, we have found all these properties very useful, but we also have met with limitations. The functional unification (FU) formalism is not well suited for the expression of simple, yet very common, taxonomic relations. The traditional way to implement such relations in FUG is verbose, inefficient and unreadable. It is also impossible to express completeness constraints on descriptions. In this paper, we present several extensions to the FU formalism that address these limitations. These extensions are based on the formal semantics presented in (Elhad"
P90-1020,P85-1017,0,0.0309036,"larations. The second advantage is that it can be used to define more efficient data-structures to represent FDs. As suggested by the definition of FDs, two types of data-structures can be used to internally represent FDs: a fiat list of equations (which is more appropriate for a language like Prolog) and a structured representation (which is more natural for a language like Lisp). When all constituents are typed, it becomes possible to use arrays or hash-tables to store FDs in Lisp, which is much more efficient We are currently investigating alternative internal representations for FDs (cf. (Pereira, 1985, Karttunen, 1985, Boyer, 1988, Hirsh, 1988) for discussions of data-structures and compilation of FUGs). 5 CONCLUSION Functional Descriptions are built from two components: a set C of primitives and a set L of labels. Traditionally, all structuring of FDs is done using strings of labels. We have shown in this paper that there is much to be gained by delegating some of the structuring to a set of primitives. The set C is no longer a fiat set of symbols, but is viewed as a richly structured world. The idea of typed-unification is not new (Ait-Kaci, 1984), but we have integrated it for the first"
P90-1020,P84-1027,0,0.0384047,"Missing"
P90-1020,P90-1032,0,0.0519784,"ent and reason about complete information in certain situations. The structure of C can be used as a metadescription of the grammar: the type declarations specify what the grammar knows, and are used to check input FDs. It allows the writing of much more concise grammars, which perform more efficiently. It is a great resource for documenting the grammar. The extended formalism described in this paper is implemented in Common Lisp using the Union-Find algorithm (Elhadad, 1988), as suggested in (Huet, 1976, Ait-Kaci, 1984, Escalada-Imaz & Ghallab, 1988) and is used in several research projects (Smadja & McKeown, 1990, Elhadad et al, 1989, McKeown & Elhadad, 1990, McKeown et al, 1991). The source code for the unifier is available to other researchers. Please contact the author for further details. We are investigating other extensions to the FU formalism, and particularly, ways to modify control over grammars: we have developed indexing schemes for more efficient search through the grammar and have extended the formalism to allow the expression of complex constraints (set union and intersection). We are now exploring ways to integrate these later extensions more tightly to the FUG formalism. ACKNOWLEDGMENT"
P90-1020,C86-1045,0,\N,Missing
P90-1020,P85-1016,0,\N,Missing
P99-1019,P99-1071,1,0.762124,"selecting a syntactic realization component is whether its input specification language fits the desired application. Traditionally, syntactic realization components have attempted to raise the abstraction level of input specifications for two reasons: (1) to preserve the possibility of paraphrasing and (2) to make it easy for the sentence planner to map from semantic data to syntactic input As new applications appear, that cannot start generation from a semantic input because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (Barzilay et al., 1999) or generation of complex NPs in a hybrid template system for business letters (Gedalia, 1996)), this motivation has lost some of its strength. Consequently, &quot;shallow surface generators&quot; have recently appeared (Lavoie and Rambow, 1997) (Busemann and Horacek, 1998) that require an input considerably less abstract than those required by more traditional realization components such as SURGE (E1hadad and Robin, 1996) or KPML (Bateman, 1997). In this paper, we contribute to the debate on selecting an appropriate level of abstraction by considering the case of bilingual generation. We present result"
P99-1019,W98-1425,0,0.0753821,"ish Generation of Possessives and Partitives: Raising the Input Abstraction Level Yael Dahan Netzer and Michael Elhadad Ben Gurion University Department of Mathematics and Computer Science, Beer Sheva, 84105, Israel (yaeln I elhadad) @cs. bgu. ac. il Abstract Syntactic realization grammars have traditionally attempted to accept inputs with the highest possible level of abstraction, in order to facilitate the work of the components (sentence planner) preparing the input. Recently, the search for higher abstraction has been, however, challenged (E1hadad and Robin, 1996)(Lavoie and Rambow, 1997)(Busemann and Horacek, 1998). In this paper, we contribute to the issue of selecting the &quot;ideal&quot; abstraction level in the input to syntactic realization grammar by considering the case of partitives and possessives in a bilingual Hebrew-English generation grammar. In the case of bilingual generation, the ultimate goal is to provide a single input structure, where only the openclass lexical entries are specific to the language. In that case, the minimal abstraction required must cover the different syntactic constraints of the two languages. We present a contrastive analysis of the syntactic realizations of possessives an"
P99-1019,W98-1012,1,0.787071,"ctive in Hebrew and yet very constrained (Dahan-Netzer and E1hadad, 1998b). 146 Free genitive constructs use a prepositional phrase with the preposition Sel. Many studies treat Sel as a case marker only (cf. (Berman, 1978) (Yzhar, 1993) (Borer, 1988)). The choice of one of the three forms seems to be stylistic and vary in spoken and written Hebrew (cf. (Berman, 1978), (Glineft, 1989), (Ornan, 1964), and discussion in (Seikevicz, 1979)). But, in addition to these pragmatic factors and as is the case for the English genitive, the construct state can realize a wide variety of semantic relations (Dahan-Netzer and Elhadad, 1998b), (Azar, 1985), (Levi, 1976). The selection is also a matter of preference ranking among competitors for the same syntactic slot. For example, we have shown in (Dahan-Netzer and Elhadad, 1998b) that the semantic relations that can be realized by a construct state are the ones defined as classifier in SURGE. Therefore, the co-occurrence of such a relation with another classifier leads to a competition for the syntactic slot of &quot;classifier&quot; and also contributes to the decision of how to realize a possessive. Consider the following example: cat head classifier possessor common lex &quot;Simlah&quot;/&quot;dre"
P99-1019,W98-1418,1,0.937747,"ctive in Hebrew and yet very constrained (Dahan-Netzer and E1hadad, 1998b). 146 Free genitive constructs use a prepositional phrase with the preposition Sel. Many studies treat Sel as a case marker only (cf. (Berman, 1978) (Yzhar, 1993) (Borer, 1988)). The choice of one of the three forms seems to be stylistic and vary in spoken and written Hebrew (cf. (Berman, 1978), (Glineft, 1989), (Ornan, 1964), and discussion in (Seikevicz, 1979)). But, in addition to these pragmatic factors and as is the case for the English genitive, the construct state can realize a wide variety of semantic relations (Dahan-Netzer and Elhadad, 1998b), (Azar, 1985), (Levi, 1976). The selection is also a matter of preference ranking among competitors for the same syntactic slot. For example, we have shown in (Dahan-Netzer and Elhadad, 1998b) that the semantic relations that can be realized by a construct state are the ones defined as classifier in SURGE. Therefore, the co-occurrence of such a relation with another classifier leads to a competition for the syntactic slot of &quot;classifier&quot; and also contributes to the decision of how to realize a possessive. Consider the following example: cat head classifier possessor common lex &quot;Simlah&quot;/&quot;dre"
P99-1019,J94-4004,0,0.0117385,"onstruct state&quot; (smixut): The King&apos;s palace Armon ha-melex Palace-cs the-king Segovia&apos;s pupil * talmyd segovyah The pupil of Segovia talmyd Sel segovyah ? The house&apos;s windows Haionot ha-bayit The windows of the house ha-Halonot Sel ha-bayit Our goal, therefore, is to design an input structure that is abstract enough to let the grammar decide whether to use a possessive marker vs. an of-construct in English or a Sel-construct vs. a smixut-construction in Hebrew. A similar approach has been adopted in generation (Bateman, 1997), (Bateman et al., 1991) and in machine translation most notably in (Dorr, 1994). Dorr focuses on divergences at the clause level as illustrated by the following example: I like Mary Maria me gusta a mi Mary pleases me Dorr selects a representation structure based on Jackendoff&apos;s Lexical Conceptual Structures (LCS) (Jackendoff, 1990). In the KPML system, the proposed solution is based on the systemic notion of &quot;delicacy&quot; and the assumption is that lowdelicacy input features (the most abstract ones) remain common to the two target languages and high-delicacy features would differ. In this paper, we focus on the input specification for complex NPs. The main reason for this"
P99-1019,W96-0501,1,0.893892,"Missing"
P99-1019,A97-1039,0,0.0241799,"ns for two reasons: (1) to preserve the possibility of paraphrasing and (2) to make it easy for the sentence planner to map from semantic data to syntactic input As new applications appear, that cannot start generation from a semantic input because such an input is not available (for example re-generation of sentences from syntactic fragments to produce summaries (Barzilay et al., 1999) or generation of complex NPs in a hybrid template system for business letters (Gedalia, 1996)), this motivation has lost some of its strength. Consequently, &quot;shallow surface generators&quot; have recently appeared (Lavoie and Rambow, 1997) (Busemann and Horacek, 1998) that require an input considerably less abstract than those required by more traditional realization components such as SURGE (E1hadad and Robin, 1996) or KPML (Bateman, 1997). In this paper, we contribute to the debate on selecting an appropriate level of abstraction by considering the case of bilingual generation. We present results obtained while developing the HUGG syntactic realization component for Hebrew (DahanNetzer, 1997). One of the goals of this system is to design a generator with an input specification language as similar as possible to that of an Eng"
P99-1071,W97-0703,1,0.18292,"document summarization systems, then we will detail our sentence comparison technique, and describe the sentence generation component. We provide examples of generated summaries and conclude with a discussion of evaluation. 2 Related Work Automatic summarizers typically identify and extract the most important sentences from an input article. A variety of approaches exist for determining the salient sentences in the text: statistical techniques based on word distribution (Salton et al., 1991), symbolic techniques based on discourse structure (Marcu, 1997), and semantic relations between words (Barzilay and Elhadad, 1997). Extraction techniques can work only if summary sentences already appear in the article. Extraction cannot handle the task we address, because summarization of multiple documents requires information about similarities and differences across articles. While most of the summarization work has focused on single articles, a few initial projects have started to study multi-document summarization documents. In constrained domains, e.g., terrorism, a coherent summary of several articles can be generated, when a detailed semantic representation of the source text is available. For example, informati"
P99-1071,P96-1025,0,0.0195348,"opositions from an underlying knowledge base to form text content. A sentence planner determines how to combine propositions into a single sentence, and a sentence generator realizes each set of combined propositions as a sentence, mapping from concepts to words and building syntactic structure. Our approach differs in the following ways: Content p l a n n i n g o p e r a t e s o v e r full sentences, producing s e n t e n c e fragm e n t s . Thus, content planning straddles the border between interpretation and generation. We preprocess the similar sentences using an existing shallow parser (Collins, 1996) and a mapping to predicateargument structure. The content planner finds an intersection of phrases by comparing the predicate-argument structures; through this process it selects the phrases that can adequately convey the common information of the theme. It also orders selected phrases and augments t h e m with 550 On 3th of September 1995, 120 hostages were released by Bosnian Serbs. Serbs were holding over 250 U.N. personnel. Bosnian serb leader Radovan Karadjic said he expected ""a sign of goodwill"" from the international community. U.S. F-16 fighter jet was shot down by Bosnian ! Serbs. El"
P99-1071,P97-1004,0,0.00607779,"Missing"
P99-1071,A97-1042,0,0.0830727,"fusion of similar information across multiple documents using language generation to produce a concise summary. We propose a m e t h o d for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for s u m m a r y generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while exMichael Elhadad Dept. of C o m p u t e r Science B e n - G u r i o n University Beer-Sheva, Israel tracting s o m e similar sentences could produce a s u m m a r y biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that shou"
P99-1071,W97-0713,0,0.158286,"nformation across multiple documents using language generation to produce a concise summary. We propose a m e t h o d for summarizing a specific type of input: news articles presenting different descriptions of the same event. Hundreds of news stories on the same event are produced daily by news agencies. Repeated information about the event is a good indicator of its importancy to the event, and can be used for s u m m a r y generation. Most research on single document summarization, particularly for domain independent tasks, uses sentence extraction to produce a summary (Lin and Hovy, 1997; Marcu, 1997; Salton et al., 1991). In the case of multidocument summarization of articles about the same event, the original articles can include both similar and contradictory information. Extracting all similar sentences would produce a verbose and repetitive summary, while exMichael Elhadad Dept. of C o m p u t e r Science B e n - G u r i o n University Beer-Sheva, Israel tracting s o m e similar sentences could produce a s u m m a r y biased towards some sources. Instead, we move beyond sentence extraction, using a comparison of extracted similar sentences to select the phrases that should be include"
P99-1071,J98-3005,1,0.606113,"tic way. However, some of the rules can only be approximated to a certain degree. For example, identification of similarity based on semantic relations between words depends on the coverage of the thesaurus. We 553 identify word similarity using synonym relations from WordNet. Currently, paraphrasing using part of speech transformations is not supported by the system. All other paraphrase classes we identified are implemented in our algorithm for theme intersection. 3.3 T e m p o r a l O r d e r i n g A property that is unique to multi-document summarization is the effect of time perspective (Radev and McKeown, 1998). When reading an original text, it is possible to retrieve the correct temporal sequence of events which is usually available explicitly. However, when we p u t pieces of text from different sources together, we must provide the correct time perspective to the reader, including the order of events, the temporal distance between events and correct temporal references. In single-document summarization, one of the possible orderings of the extracted information is provided by the input document itself. However, in the case of multiple-document summarization, some events may not be described in t"
W00-1428,dorr-etal-1998-thematic,0,0.0445085,"Missing"
W00-1428,W96-0501,1,0.821455,"his integration prouse synsets to find words that can lexicalize the secess helps to automate the development of a lexical mantic concepts in the semantic input. By choosing realization component. different words in a synset, we can therefore gen4.1 F U F / S U R G E a n d t h e lexical c h o o s e r erate lexical paraphrases. For instance, using the above synset, the system can generate the following FUF (Elhadad, 1992) uses a functional unification paraphrases: formalism for generation. It unifies the input that a user provides with a grammar to generate sentences. ""He seems happy. "" SURGE (Elhadad and Robin, 1996) is a comprehen""He looks happy. "" sive English Grammar written in FUF. Tile role of ""He appears happy.'"" a lexical realization component is to map a semantic representation drawn from the application domain Secondly, the subcategorization information in the to an input format acceptable by SURGE, adding lexicon prevents generating a non-grammatical outnecessary lexical and syntactic information during put. As shown in Figure 1, the lexicon lists applithis process. cable subcategorizations for each sense of a verb. It Figure 2 shows a sample semantic input (a), the will not allow the generation"
W00-1428,C94-1042,0,0.0287808,"nts: connotations of words o stylistic constraints: familiarity of words * syntactic constraints: government patterns of words, e.g., thematic structure of verbs. We show in this p a p e r how the separation of the syntactic and conceptual interfaces of lexical item definitions allows us to reuse a large amount of lexical knowledge across appli.cations. 3 The lexicon generation 3.1 and its benefits to ""there-insertion"" transforms A ship appeared ~-on..the horizon_to There,appeared a ship..o~....the horizon. A total of 80 alternations for 3,104 verbs were studied. The COMLEX syntax dictionary (Grishman et al., 1994). C O M L E X contains syntactic information for over 38,000 English words. The Brown Corpus tagged with WordNet senses (Miller et al., 1993). We use this corpus for frequency measurement. . In combining these resources, we focused on verbs, since they play a more important role in deciding sentence structures. The combined lexicon includes rich lexical and syntactic knowledge for 5,676 verbs. It is indexed by WordNet synsets(which are at the semantic concept level) as required by the generation task. The knowledge in the lexicon includes: Q A complete list of subcategorizations for each sense"
W00-1428,W98-0718,1,0.854845,"Missing"
W00-1428,W98-1426,0,0.0298781,"ecome ~ i s i b l e ' ' ] word ""appear""a 'a small vessel for travel on water'' ] 1 concept word ""boa~""a concept ' C t h e l i n e a t which t h e sky and E a r t h a p p e a r to meet ] 2 word ""hor,izon ''a ppb relation ] J args ] struct argl 2 ~ given cat lexical-roles ""Enriched bEnriched CEnriched dEnriched in in in in clause c d first step second step third step fourth step Figure 6: S U R G E input for ""A boat appeared on the horizon"" object or an adverb. Such decisions are constrained by syntactic properties of verbs. The integrated lexicon is useful to verify these properties. Nitrogen (Langkilde and Knight, 1998), a natural language generation system developed at ISI, also includes a large-scale lexicon to support the generation process. Given that Nitrogen and F U F / S U R G E use very different methods for generation, the way that we integrate the lexicon with the generation system is also very different. Nitrogen combines symbolic rules with statistics learned from text corpora, while F U F / S U R G E is based on Functional Unification Grammar. Other related work includes (Stede, 1998), which suggests a lexicon structure for multilingual generation in a knowledge-based generation system. The main"
W00-1428,H93-1061,0,0.0558336,"structure of verbs. We show in this p a p e r how the separation of the syntactic and conceptual interfaces of lexical item definitions allows us to reuse a large amount of lexical knowledge across appli.cations. 3 The lexicon generation 3.1 and its benefits to ""there-insertion"" transforms A ship appeared ~-on..the horizon_to There,appeared a ship..o~....the horizon. A total of 80 alternations for 3,104 verbs were studied. The COMLEX syntax dictionary (Grishman et al., 1994). C O M L E X contains syntactic information for over 38,000 English words. The Brown Corpus tagged with WordNet senses (Miller et al., 1993). We use this corpus for frequency measurement. . In combining these resources, we focused on verbs, since they play a more important role in deciding sentence structures. The combined lexicon includes rich lexical and syntactic knowledge for 5,676 verbs. It is indexed by WordNet synsets(which are at the semantic concept level) as required by the generation task. The knowledge in the lexicon includes: Q A complete list of subcategorizations for each sense of a verb. o A large variety of alternations for each sense of a verb. o Frequency of lexical items and verb subcategorizations in the tagge"
W00-1428,W94-0319,0,0.0875625,"Missing"
W00-1428,J98-3003,0,\N,Missing
W00-1428,P98-1099,1,\N,Missing
W00-1428,C98-1096,1,\N,Missing
W05-1602,W98-1435,0,0.0777504,"Missing"
W05-1602,W94-0308,0,0.0799615,"Missing"
W05-1602,W00-1428,1,0.884899,"Missing"
W05-1602,P98-2173,0,0.347872,"Missing"
W05-1602,C92-1038,0,0.0816802,"Missing"
W05-1602,C94-1055,0,0.0565029,"Missing"
W05-1602,A94-1002,0,0.0755993,"Missing"
W05-1602,P95-1053,0,0.0706072,"Missing"
W05-1602,C98-2168,0,\N,Missing
W09-2005,W07-2323,0,0.0137451,"the general NLP tasks and requires understanding and modelling knowledge which, almost by definition, cannot be formalized (i.e., terms like beautiful, touching, funny or intriguing). On the other hand, this vagueness itself may enable a less restrictive formalization and allow a variety of quality judgments. Such vague formalizations are naturally more useful when a computational creativity system does not attempt to model the creativity process itself, but instead focuses on ’creative products’ such as poetry (see Section 2.3), prose and narrative (Montfort, 2006), cryptic crossword clues (Hardcastle, 2007) and many others. Some research focus on the creative process itself (see (Ritchie, 2006) for a comprehensive review of the field). We discuss in this paper what Boden (1998) calls P-Creativity (Psychological Creativity) which is defined relative to the initial state of knowledge, and H-Creativity (Historical Creativity) which is relative to a specific reference culture. Boden claims that, while hard to reproduce, exploratory creativity is most successful in computer models of creativity. This is because the other kinds of creativity are even more elusive due to the difficulty of ap33 proachin"
W09-2005,J91-1002,0,0.0432403,"rds to be the number of edges in the shortest path between the words in the associations-graph. Interestingly, almost any word pair in the association graph is connected with a path of at most 3 edges. Thus, we take two words to be associatively related if their associative distance is 1 or 2. Similarly, we define the WordNet distance between two stemmed words to be the number of edges in the shortest path between any synset of one word to any synset of the other word2 . Two words are WordNet-related if their WordNet distance is less than 4 (this is consistent with works on lexical-cohesion, (Morris and Hirst, 1991)). We take the associativity of a piece of text to be the number of associated word pairs in the text, normalized by the number of word pairs in the text of which both words are in the WAN.3 We take the WordNet-relations level of a piece of text to be the number of WordNet-related word pairs in the text. 2 This is the inverse of the path-similarity measure of (Pedersen et al., 2004). 3 This normalization is performed to account for the limited lexical coverage of the WAN. We don’t want words that appear in a text, but are not covered by the WAN, to affect the associativity level of the text. 3"
W09-2005,N04-3012,0,0.0259862,"Missing"
W09-3819,W06-2920,0,0.0943666,"arsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific properties of Arabic making it easy or hard to parse in comparison to other languages. Our aim is to evaluate current state-of-the-art dependency parsers and approaches on Hebrew dependency parsing, to understand some of"
W09-3819,de-marneffe-etal-2006-generating,0,0.0230394,"Missing"
W09-3819,P06-1087,1,0.752163,"gmented and POS-tagged text. We present an evaluation measure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of"
W09-3819,P08-1085,1,0.833549,"ining set, and report results on parsing the development set, Section 1 (sentences 0-483). We do not evaluate on the test set in this work. The data in the Treebank is segmented and POS-tagged. All of the models were trained on the gold-standard segmented and tagged data. When evaluating the parsing models, we perform two sets of evaluations. The first one is an oracle experiment, assuming gold segmentation and tagging is available. The second one is a real-world experiment, in which we segment and POS-tag the test-set sentences using the morphological disambiguator described in (Adler, 2007; Goldberg et al., 2008) prior to parsing. Parsers and parsing models We use the freely available implementation of MaltParser2 and MSTParser3 , with default settings for each of the parsers. For MaltParser, we experiment both with the default feature representation (M ALT) and the feature representation used for parsing Arabic in CoNLL 2006 and 2007 multilingual dependency parsing shared tasks (M ALT-A RA). For MST parser, we experimented with firstorder (M ST 1) and second-order (M ST 2) models. We varied the amount of lexical information available to the parser. Each of the parsers was trained on 3 datasets: L EX"
W09-3819,E09-1038,1,0.913871,"into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models, or the specific propert"
W09-3819,D07-1013,0,0.036053,"ages (CoNLL Shared Task 2006, 2007). Briefly, a graph-based parsing model works by assigning a score to every possible attachment between a pair (or a triple, for a second-order model) of words, and then inferring a global tree structure that maximizes the sum of these local scores. Transition-based models work by building the dependency graph in a sequence of steps, where each step is dependent on the next input word(s), the previous decisions, and the current state of the parser. For more details about these parsing models as well as a discussion on the relative benefits of each model, see (McDonald and Nivre, 2007). Contrary to constituency-based parsers, dependency parsing models expect a morphologically segmented and POS tagged text as input. 4 Experiments Data We follow the train-test-dev split established in (Tsarfaty and Sima’an, 2008). Specifically, we use Sections 2-12 (sentences 484-5724) of the Hebrew Dependency Treebank as our training set, and report results on parsing the development set, Section 1 (sentences 0-483). We do not evaluate on the test set in this work. The data in the Treebank is segmented and POS-tagged. All of the models were trained on the gold-standard segmented and tagged d"
W09-3819,nivre-etal-2006-maltparser,0,0.194296,"Missing"
W09-3819,C08-1112,0,0.27714,"Missing"
W09-3819,P06-3009,0,0.116554,"sure that takes into account the possibility of incompatible token segmentation between the gold standard and the parsed data. Results indicate that (a) MST-parser performs better on Hebrew data than MaltParser, and (b) both parsers do not make good use of morphological information when parsing Hebrew. 1 Introduction Hebrew is a Semitic language with rich morphological structure and free constituent order. Previous computational work addressed unsupervised Hebrew POS tagging and unknown word resolution (Adler, 2007), Hebrew NP-chunking (Goldberg et al., 2006), and Hebrew constituency parsing (Tsarfaty, 2006; Golderg et al., 2009). Here, we focus on Hebrew dependency parsing. Dependency-parsing got a lot of research attention lately, in part due to two CoNLL shared tasks focusing on multilingual dependency parsing (Buchholz and Erwin, 2006; Nivre et al., 2007). These tasks include relatively many parsing results for Arabic, a Semitic language similar to Hebrew. However, parsing accuracies for Arabic usually lag behind non-semitic languages. Moreover, while there are many published results, we could not find any error analysis or even discussion of the results of Arabic dependency parsing models,"
W09-3819,D07-1096,0,\N,Missing
W10-1412,W09-3819,1,0.87354,"hadad, 2010) that this rich conditioning can be especially beneficial in situations where informative structural information is available, such as in morphologically rich languages. In this paper, we investigate the non-directional easy-first parser performance on Modern Hebrew, a semitic language with rich morphology, relatively free constituent order, and a small treebank compared to English. We are interested in two main questions: (a) how well does the non-directional parser perform on Hebrew data? and (b) can the parser make effective use of morphological features, such as agreement? In (Goldberg and Elhadad, 2009), we describe a newly created Hebrew dependency treebank, and 103 Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 103–107, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics report results on parsing this corpus with both M ALT PARSER and first- and second- order variants of M ST PARSER. We find that the secondorder M ST PARSER outperforms the first order M STPARSER, which in turn outperforms the transition based M ALT PARSER. In addition, adding morphological information to the default configura"
W10-1412,N10-1115,1,0.847625,"provement due to the morphological agreement information is persistent both when gold-standard and automatically-induced morphological information is used. 1 Introduction Data-driven Dependency Parsing algorithms are broadly categorized into two approaches (K¨ubler et al., 2009). Transition based parsers traverse the sentence from left to right1 using greedy, local inference. Graph based parsers use global inference and seek a tree structure maximizing some scoring function defined over trees. This scoring function is usually decomposed over tree edges, or pairs of such edges. In recent work (Goldberg and Elhadad, 2010), we proposed another dependency parsing approach: Easy First, Non-Directional dependency ∗ Supported by the Lynn and William Frankel Center for Computer Sciences, Ben Gurion University 1 Strictly speaking, the traversal order is from start to end. This distinction is important when discussing Hebrew parsing, as the Hebrew language is written from right-to-left. We keep the left-to-right terminology throughout this paper, as this is the common terminology. However, “left” and “right” should be interpreted as “start” and “end” respectively. Similarly, “a token to the left” should be interpreted"
W10-1412,P08-1085,1,0.824381,"the development set, Section 1 (sentences 0-483). As in (Goldberg and Elhadad, 2009), we do not evaluate on the test set in this work. The data in the treebank is segmented and POStagged. Both the parsing models were trained on the gold-standard segmented and tagged data. When evaluating the parsing models, we perform two sets of evaluations. The first one is an oracle experiment, assuming gold segmentation and tagging is available. The second one is a real-world experiment, in which we segment and POS-tag the testset sentences using the morphological disambiguator described in (Adler, 2007; Goldberg et al., 2008) prior to parsing. 105 Parsers and parsing models We use our freely available implementation3 of the non-directional parser. Evaluation Measure We evaluate the resulting parses in terms of unlabeled accuracy – the percent of correctly identified (child,parent) pairs4 . To be precise, we calculate: number of correctly identif ied pairs number of pairs in gold parse For the oracle case in which the gold-standard token segmentation is available for the parser, this is the same as the traditional unlabeled-accuracy evaluation metric. However, in the real-word setting in which the token segmentatio"
W10-1412,P05-1012,0,0.0758202,"interpreted as “the previous token”. parsing. Like transition based methods, the easyfirst method adopts a local, greedy policy. However, it abandons the strict left-to-right processing order, replacing it with an alternative order, which attempts to make easier attachments decisions prior to harder ones. The model was applied to English dependency parsing. It was shown to be more accurate than M ALT PARSER, a state-of-the-art transition based parser (Nivre et al., 2006), and near the performance of the first-order M ST PARSER, a graph based parser which decomposes its score over tree edges (McDonald et al., 2005), while being more efficient. The easy-first parser works by making easier decisions before harder ones. Each decision can be conditioned by structures created by previous decisions, allowing harder decisions to be based on relatively rich syntactic structure. This is in contrast to the globally optimized parsers, which cannot utilize such rich syntactic structures. It was hypothesized in (Goldberg and Elhadad, 2010) that this rich conditioning can be especially beneficial in situations where informative structural information is available, such as in morphologically rich languages. In this pa"
W10-1412,nivre-etal-2006-maltparser,0,0.044069,"on terminology. However, “left” and “right” should be interpreted as “start” and “end” respectively. Similarly, “a token to the left” should be interpreted as “the previous token”. parsing. Like transition based methods, the easyfirst method adopts a local, greedy policy. However, it abandons the strict left-to-right processing order, replacing it with an alternative order, which attempts to make easier attachments decisions prior to harder ones. The model was applied to English dependency parsing. It was shown to be more accurate than M ALT PARSER, a state-of-the-art transition based parser (Nivre et al., 2006), and near the performance of the first-order M ST PARSER, a graph based parser which decomposes its score over tree edges (McDonald et al., 2005), while being more efficient. The easy-first parser works by making easier decisions before harder ones. Each decision can be conditioned by structures created by previous decisions, allowing harder decisions to be based on relatively rich syntactic structure. This is in contrast to the globally optimized parsers, which cannot utilize such rich syntactic structures. It was hypothesized in (Goldberg and Elhadad, 2010) that this rich conditioning can b"
W10-1412,P06-1084,1,\N,Missing
W10-2927,nivre-etal-2006-maltparser,0,0.0331188,"were used to train the Boosting algorithm and find structural predictors candidates. Sections 4-7 were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the weights are tuned to be used as part of a linear classifier. In our application, we disregard the boosting weights, and instead rank the predictors based on their n"
W10-2927,W04-0308,0,0.0816925,"Missing"
W10-2927,J08-4003,0,0.0318264,"e seen in Table 1, the resulting parsers are still accurate. 4 http://sourceforge.net/projects/mstparser/ 5 http://maltparser.org/ 237 M ST1 88.8 M ST2 89.8 A RC E 87.6 A RC S 87.4 are produced by the parser more often than they appear in the language).8 Specifically, we manually inspected the predictors where the ratio between language and parser was high, ranked by absolute number of occurrences. Table 1: Unlabeled accuracies of the analyzed parsers Parser M ST1 M ST2 A RC E A RC S Train Accuracy 65.4 62.8 69.2 65.1 Val Accuracy 57.8 56.6 65.3 60.1 4 We analyze two transition-based parsers (Nivre, 2008). The parsers differ in the transition systems they adopt. The A RC E system makes use of a transition system with four transitions: L EFT,R IGHT,S HIFT,R EDUCE. The semantics of this transition system is described in (Nivre, 2004). The A RC S system adopts an alternative transition system, with three transitions: ATTACH L,ATTACH R,S HIFT . The semantics of the system is described in (Huang et al., 2009). The main difference between the systems is that the A RC E system makes attachments as early as possible, while the A RC S system should not attach a parent to its dependent until the depende"
W10-2927,W06-2920,0,0.0606229,"parsers, and first- and second-order graph-based parsers). We show that all four parsers are biased with respect to the kind of annotation they are trained to parse. We present a detailed analysis of the biases that highlights specific differences and commonalities between the parsing systems, and improves our understanding of their strengths and weaknesses. 1 Introduction Dependency Parsing, the task of inferring a dependency structure over an input sentence, has gained a lot of research attention in the last couple of years, due in part to to the two CoNLL shared tasks (Nivre et al., 2007; Buchholz and Marsi, 2006) in which various dependency parsing algorithms were compared on various data sets. As a result of this research effort, we have a choice of several robust, efficient and accurate parsing algorithms. ∗ We would like to thank Reut Tsarfaty for comments and discussions that helped us improve this paper. This work is supported in part by the Lynn and William Frankel Center for Computer Science. 234 Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 234–242, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics M ST Parser. In wh"
W10-2927,N10-1049,0,0.0292755,"e pose the following question: Assuming we are given two parses of the same sentence. Can we tell, by looking at the parses and without knowing the correct parse, which parser produced which parse? Any predictor which can help in answering this question is an indicator of a structural bias. Structural Bias Language is a highly structured phenomena, and sentences exhibit structure on many levels. For example, in English sentences adjectives appear before nouns, subjects tend to appear before their verb, and syntactic trees show a tendency toward right-branching structures.1 1 As noted by (Owen Rambow, 2010), there is little sense in talking about the structure of a language without referring to a specific annotation scheme. In what follow, we assume a fixed annotation strategy is chosen. 235 Definition: structural bias between sets of trees Given two sets of parse trees, A and B, over the same sentences, a structural bias between these sets is the collection of all predictors which can help us decide, for a tree t, whether it belongs to A or to B. 3 (a) (b) NN VB IN/with Figure 1: Structural Elements Examples. (a) is an adjective with a parent 3 words to its right. (b) is a verb whose parent is"
W10-2927,P10-1075,0,0.105891,"the strengths, weaknesses and inner working of the parser. In Section 2.2 we propose a Boosting-based algorithm for uncovering these structural biases. Then, in Section 3 we go on to apply our analysis methodology to four parsing systems for English: two transition-based systems and two graph-based systems (Sections 4 and 5). The analysis shows that the different parsing systems indeed possess different biases. Furthermore, the analysis highlights the differences and commonalities among the different parsers, and sheds some more light on the specific behaviours of each system. Recent work by Dickinson (2010), published concurrently with this one, aims to identify dependency errors in automatically parsed corpora by inspecting grammatical rules which appear in the automatically parsed corpora and do not fit well with the grammar learned from a manually annotated treebank. While Dickinson’s main concern is with automatic identification of errors rather than characterizing parsers behaviour, we feel that his work shares many intuitions with this one: automatic parsers fail in predictable ways, those ways can be analyzed, and this analysis should be carried out on structures which are larger than sin"
W10-2927,N06-2033,0,0.0976003,"Missing"
W10-2927,D09-1127,0,0.0147348,"were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the weights are tuned to be used as part of a linear classifier. In our application, we disregard the boosting weights, and instead rank the predictors based on their number of occurrences in a validation set. We seek predictors which appear many times in one tree-se"
W10-2927,W07-2416,0,0.0256553,"s encoded in the node name, while the optional lexical item and distance to parent are encoded as daughters. 3 3.1 Experimental Setup In what follows, we analyze and compare the structural biases of 4 parsers, with respect to a dependency representation of English. tive branch-and-bound technique for efficiently searching for the maximum gain tree at each round. The reader is referred to their paper for the details. Syntactic representation The dependency treebank we use is a conversion of the English WSJ treebank (Marcus et al., 1993) to dependency structure using the procedure described in (Johansson and Nugues, 2007). We use the Mel’ˇcuk encoding of coordination structure, in which the first conjunct is the head of the coordination structure, the coordinating conjunction depends on the head, and the second conjunct depend on the coordinating conjunction (Johansson, 2008). Structural elements as subtrees The boosting algorithm works on labeled, ordered trees. Such trees are different than dependency trees in that they contain information about nodes, but not about edges. We use a simple transformation to encode dependency trees and structural elements as labeled, ordered trees. The transformation works by"
W10-2927,W04-3239,0,0.0308097,"ith information about the incoming edge to the root of the subtree. Examples of such structural elements are given in Figure 1. This class of predictors is not complete – it does not directly encode, for instance, information about the number of siblings Boosting Algorithm with Subtree Features The number of possible predictors is exponential in the size of each tree, and an exhaustive search is impractical. Instead, we solve the search problem using a Boosting algorithm for tree classification using subtree features. The details of the algorithm and its efficient implementation are given in (Kudo and Matsumoto, 2004). We briefly describe the main idea behind the algorithm. The Boosting algorithm with subtree features gets as input two parse sets with labeled, ordered trees. The output of the algorithm is a set of subtrees ti and their weights wi . These weighted subtrees P define a linear classifier over trees f (T ) = ti ∈T wi , where f (T ) > 0 for trees in set A and f (T ) &lt; 0 for trees in set B. The algorithm works in rounds. Initially, all input trees are given a uniform weight. At each round, the algorithm seeks a subtree t with a maximum gain, that is the subtree that classifies correctly the subse"
W10-2927,J93-2004,0,0.0421062,"he tree encodings of the structural elements in Figure 1. Direction to parent is encoded in the node name, while the optional lexical item and distance to parent are encoded as daughters. 3 3.1 Experimental Setup In what follows, we analyze and compare the structural biases of 4 parsers, with respect to a dependency representation of English. tive branch-and-bound technique for efficiently searching for the maximum gain tree at each round. The reader is referred to their paper for the details. Syntactic representation The dependency treebank we use is a conversion of the English WSJ treebank (Marcus et al., 1993) to dependency structure using the procedure described in (Johansson and Nugues, 2007). We use the Mel’ˇcuk encoding of coordination structure, in which the first conjunct is the head of the coordination structure, the coordinating conjunction depends on the head, and the second conjunct depend on the coordinating conjunction (Johansson, 2008). Structural elements as subtrees The boosting algorithm works on labeled, ordered trees. Such trees are different than dependency trees in that they contain information about nodes, but not about edges. We use a simple transformation to encode dependency"
W10-2927,D07-1013,0,0.036424,"and how to blend and stack their outputs, little effort was directed toward understanding the behavior of different parsing systems in terms of structures they produce and errors they make. Question such as which linguistic phenomena are hard for parser Y? and what kinds of errors are common for parser Z?, as well as the more ambitious which parsing approach is most suitable to parse language X?, remain largely unanswered. The current work aims to fill this gap by proposing a methodology to identify systematic biases in various parsing models and proposing and initial analysis of such biases. McDonald and Nivre (2007) analyze the difference between graph-based and transition-based parsers (specifically the M ALT and M ST parsers) by comparing the different kinds of errors made by both parsers. They focus on single edge errors, and learn that M ST is better for longer dependency arcs while M ALT is better on short dependency arcs, that M ALT is better than M ST in predicting edges further from the root and vice-versa, that M ALT has a slight advantage when predicting the parents of nouns and pronouns, and that M ST is better at all other word categories. They also conclude that the greedy M ALT Parser suffe"
W10-2927,E06-1011,0,0.0195033,"nal. 2.3 Biases in Dependency Parsers Data Sections 15-18 were used for training the parsers3 . The first 4,000 sentences from sections 10-11 were used to train the Boosting algorithm and find structural predictors candidates. Sections 4-7 were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the weights are tuned to be used as"
W10-2927,P05-1012,0,0.0382478,"lexical item are optional. 2.3 Biases in Dependency Parsers Data Sections 15-18 were used for training the parsers3 . The first 4,000 sentences from sections 10-11 were used to train the Boosting algorithm and find structural predictors candidates. Sections 4-7 were used as a validation set for ranking the structural predictors. In all experiments, we used the gold-standard POS tags. We binned the distance-to-parent values to 1,2,3,4-5,6-8 and 9+. Parsers For graph-based parsers, we used the projective first-order (M ST1) and secondorder (M ST2) variants of the freely available M ST parser4 (McDonald et al., 2005; McDonald and Pereira, 2006). For the transition-based parsers, we used the arc-eager (A RC E) variant of the freely available M ALT parser5 (Nivre et al., 2006), and our own implementation of an arcstandard parser (A RC S) as described in (Huang et al., 2009). The unlabeled attachment accuracies of the four parsers are presented in Table 1. Structural Bias Predictors The output of the boosting algorithm is a set of weighted subtrees. These subtrees are good candidates for structural bias predictors. However, some of the subtrees may be a result of over-fitting the training data, while the we"
W10-2927,P08-1108,0,0.0471104,"Missing"
W10-2927,D07-1096,0,\N,Missing
W12-3308,D07-1119,0,0.0557477,"Missing"
W12-3308,de-marneffe-etal-2006-generating,0,0.0148493,"Missing"
W12-3308,D07-1112,0,0.0478353,"Missing"
W12-3308,D08-1094,0,0.0177877,"ective and headprep-pobj. 3.5 Effect on NER Since most of the improvement comes from the N-N relation, we expect improvement for downstream applications such as Named Entity Recognition, a basic task frequently used in the biomedical domain. 47 Preference of predicate-argument pairs has been studied in depth with a number of approaches. Resnik (1993) suggested a class-based model for preference of predicates combining WordNet classes with mutual information techniques for associating an argument with a predicate class from WordNet. Another approach models words in a corpus as context vectors (Erk and Pado 2008; Turney and Pantel 2010) for discovering predicate or argument classes using large corpora or the Web. Recently, semantic classes were successfully induced using LDA topic modeling. These methods have shown success in modeling verb argument relationship to a single predicate (Ritter, Mausam et al. 2010) or a predicate pair (Séaghdha 2010), as well as for adjective-noun preference (Hartung and Frank 2011). 4.2 Learning SP for improving dependency parsing The argument-predicate choice learned in SP is directly related to the decision of creating an edge between them in a parse tree. Van Noord ("
W12-3308,N10-1115,1,0.843576,"ween the news and bio-medical domains. 2.2 Co-training to exploit domain features At this stage, we have acquired a domain-specific model of word affinity that exploits semantic classes and depends on specific syntactic configurations (head-prep-obj and noun-adj). We now attempt to exploit this model to adapt our source parser to the target domain. To this end, we want to re-train the parser using new features based on the SP model in addition to the original features. We use the framework of co-training to achieve this goal (Sagae 45 and Tsujii 2007): we use two different parsers: EasyFirst (Goldberg and Elhadad 2010) and MALT (Nivre, Hall et al. 2006) trained on the same WSJ source domain. We apply these two parsers on a large set of target-domain sentences. We select those sentences where the 2 parsers agree (produce identical trees) and add them to the original source-domain training set. We thus obtain an extended training set with many indomain samples. We can now re-train the parser using the new SP features. 2.3 SP as features for the Easy First parser We use the deterministic non-directional Easy-First parser for re-training. This parser incrementally adds edges between words starting with the easi"
W12-3308,D11-1050,0,0.0366243,"Missing"
W12-3308,W04-1213,0,0.129263,"Missing"
W12-3308,P08-1068,0,0.142253,"Missing"
W12-3308,H05-1066,0,0.141723,"Missing"
W12-3308,H05-1105,0,0.0709934,"Missing"
W12-3308,nivre-etal-2006-maltparser,0,0.0746835,"Missing"
W12-3308,W07-2201,0,0.250214,"Missing"
W12-3308,W11-1812,0,0.0242072,"Missing"
W12-3308,P10-1044,0,0.306555,"Missing"
W12-3308,D07-1111,0,0.0410453,"Missing"
W12-3308,P10-1045,0,0.1797,"pecific model of word-pairs affinities. Our parsing model (EasyFirst) allows us to use such bilexical features in an efficient manner. Because of data sparseness, however, we aim to acquire classbased features, and decide to model these lexical preferences using the LDA approach. Our method proceeds in two stages: 1. Learn selectional preferences from an automatically parsed corpus using LDA on selected syntactic configurations 2. Integrate the preferences into the parsing model as new features using co-training. 2.1 Learning Selectional Preferences Following (Ritter, Mausam et al. 2010) and (Séaghdha 2010), we model lexical affinity between words in specific syntactic configurations using LDA. Traditionally, LDA learns a set of ""topics"" from observed documents, based on observed word cooccurrences. In our case, we form artificial documents, which we call syntactic contexts, by collecting headdaughter pairs from parse trees. A syntactic context is constructed for each head word, which contains the related words to which it was found attached. In the collection process, we identify two syntactic configurations that yield high error rates: head-prepnoun and noun-adj. We collect two types of syntac"
W12-3308,I05-2038,0,0.090945,"Missing"
W12-3308,P11-1156,0,0.0398631,"Missing"
W12-3308,W09-1401,0,\N,Missing
W12-3308,D07-1096,0,\N,Missing
W12-3308,N06-1020,0,\N,Missing
W13-1915,E09-1050,0,0.0228138,"Missing"
W13-1915,W06-3327,0,0.0613348,"Missing"
W13-3102,saggion-szasz-2012-concisus,0,0.0785121,"om the MultiLing Workshop will be available after the workshop at the MultiLing Community website3 , as an addenum to the proceedings. What can definitely be derived from all the effort and discussion related to the gathering of summarization corpora is that it is a research challenge in itself. If the future we plan to broaden the scope of the MultiLing effort, integrating all the findings in tools that will support the whole process and allow quantifying the apparent problems in the different stages of corpus creation. We have also been considering to generate comparable corpora (e.g., see (Saggion and Szasz, 2012)) for future MultiLing efforts. We examine this course of action to avoid the significant overhead by the translation process required for parallel corpus generation. We should note here that so far we have been using parallel corpora to: • allow for secondary studies, related to the human summarization effort in different languages. Having a parallel corpus is such cases can prove critical, in that it provides a common working base. Conclusions and lessons learnt • be able to study topic-related or domainrelated summarization difficulty across languages. The findings from the languages presen"
W13-3102,C10-2122,0,0.306234,"Missing"
W13-3102,W13-3102,1,0.0512624,"Missing"
W13-3102,J13-2002,0,\N,Missing
W13-3102,W04-1013,0,\N,Missing
W16-2526,W13-3520,0,0.0845999,"Missing"
W16-2526,P14-2131,0,0.0283295,"e show that our method can produce a good amount of testing data, analyze the quality of the testing data, perform test on several leading embedding methods, and finally explain the downstream usages of our task and its significance. 1 Introduction Word vector embeddings [Mikolov et al. 2013] have become a standard building block for NLP applications. By representing words using continuous multi-dimensional vectors, applications take advantage of the natural associations among words to improve task performance. For example, POS tagging [Al Rfou et al. 2014], NER [Passos et al. 2014], parsing [Bansal et al. 2014], Semantic Role Labeling [Herman et al. 2014] or sentiment analysis [Socher et al. 2011] - have all been shown to benefit from word embeddings, either as additional features in existing supervised machine learning architectures, or as exclusive word representation features. In deep learning applications, word embeddings are typically used as pre-trained initial layers in deep architectures, and have been shown to improve performance on a wide range of tasks as well (see for example, [Cho et al., 2014; Karpathy and Fei-Fei 2015; Erhan et al,. 2010]). One of the key benefits of word embeddings"
W16-2526,D15-1075,0,0.0140908,"possible and many parameters that can greatly affect the results of each algorithm. It remains difficult to predict which word embeddings are most appropriate to a given task, whether fine tuning of the embeddings is required, and which parameters perform best for a given application. We introduce a novel dataset for comparing embedding algorithms and their settings on the specific task of comparing short clauses. The current state-of-the-art paraphrase dataset [Dolan and Brockett, 2005] is quite small with 4,076 sentence pairs (2,753 positive). The Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) corpus contains 570k sentences pairs labeled with one of the tags: entailment, contradiction, and neutral. SNLI improves on previous paraphrase datasets by eliminating indeterminacy 145 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 145–149, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics of event and entity coreference which make human entailment judgment difficult. Such indeterminacies are avoided by eliciting descriptions of the same images by different annotators. matic summarization. It has been included as a m"
W16-2526,P14-1136,0,0.0247827,"Missing"
W16-2526,P14-2050,0,0.0619344,"Missing"
W16-2526,N13-1090,0,0.0176232,"paper we repurpose the &quot;Pyramid Method&quot; annotations used for evaluating automatic summarization to create a benchmark for comparing embedding models when identifying paraphrases of text snippets containing a single clause. We present a method of converting pyramid annotation files into two distinct sentence embedding tests. We show that our method can produce a good amount of testing data, analyze the quality of the testing data, perform test on several leading embedding methods, and finally explain the downstream usages of our task and its significance. 1 Introduction Word vector embeddings [Mikolov et al. 2013] have become a standard building block for NLP applications. By representing words using continuous multi-dimensional vectors, applications take advantage of the natural associations among words to improve task performance. For example, POS tagging [Al Rfou et al. 2014], NER [Passos et al. 2014], parsing [Bansal et al. 2014], Semantic Role Labeling [Herman et al. 2014] or sentiment analysis [Socher et al. 2011] - have all been shown to benefit from word embeddings, either as additional features in existing supervised machine learning architectures, or as exclusive word representation features"
W16-2526,N04-1019,0,0.185999,"Missing"
W16-2526,W14-1609,0,0.0692576,"Missing"
W16-2526,D14-1162,0,0.0750021,"Missing"
W16-2526,D11-1014,0,0.0420781,"the testing data, perform test on several leading embedding methods, and finally explain the downstream usages of our task and its significance. 1 Introduction Word vector embeddings [Mikolov et al. 2013] have become a standard building block for NLP applications. By representing words using continuous multi-dimensional vectors, applications take advantage of the natural associations among words to improve task performance. For example, POS tagging [Al Rfou et al. 2014], NER [Passos et al. 2014], parsing [Bansal et al. 2014], Semantic Role Labeling [Herman et al. 2014] or sentiment analysis [Socher et al. 2011] - have all been shown to benefit from word embeddings, either as additional features in existing supervised machine learning architectures, or as exclusive word representation features. In deep learning applications, word embeddings are typically used as pre-trained initial layers in deep architectures, and have been shown to improve performance on a wide range of tasks as well (see for example, [Cho et al., 2014; Karpathy and Fei-Fei 2015; Erhan et al,. 2010]). One of the key benefits of word embeddings is that they can bring to tasks with small annotated datasets and small observed vocabul"
W16-2526,I05-5002,0,\N,Missing
W16-2526,P15-2050,0,\N,Missing
W97-0703,J91-1002,0,\N,Missing
W97-0703,C94-1056,0,\N,Missing
W97-0703,P94-1002,0,\N,Missing
W97-0703,P92-1032,0,\N,Missing
W97-0703,W97-0704,0,\N,Missing
W97-0703,P93-1020,0,\N,Missing
W98-1418,J91-4003,0,0.022363,"Missing"
W98-1418,P97-1018,0,\N,Missing
