2021.findings-acl.347,Minimally-Supervised Morphological Segmentation using {A}daptor {G}rammars with Linguistic Priors,2021,-1,-1,5,0.67522,8324,ramy eskander,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.lrec-1.879,"{M}orph{AG}ram, Evaluation and Framework for Unsupervised Morphological Segmentation",2020,-1,-1,4,0.67522,8324,ramy eskander,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Computational morphological segmentation has been an active research topic for decades as it is beneficial for many natural language processing tasks. With the high cost of manually labeling data for morphology and the increasing interest in low-resource languages, unsupervised morphological segmentation has become essential for processing a typologically diverse set of languages, whether high-resource or low-resource. In this paper, we present and release MorphAGram, a publicly available framework for unsupervised morphological segmentation that uses Adaptor Grammars (AG) and is based on the work presented by Eskander et al. (2016). We conduct an extensive quantitative and qualitative evaluation of this framework on 12 languages and show that the framework achieves state-of-the-art results across languages of different typologies (from fusional to polysynthetic and from high-resource to low-resource)."
W19-4222,Unsupervised Morphological Segmentation for Low-Resource Polysynthetic Languages,2019,0,0,2,0.67522,8324,ramy eskander,"Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology",0,"Polysynthetic languages pose a challenge for morphological analysis due to the root-morpheme complexity and to the word class {``}squish{''}. In addition, many of these polysynthetic languages are low-resource. We propose unsupervised approaches for morphological segmentation of low-resource polysynthetic languages based on Adaptor Grammars (AG) (Eskander et al., 2016). We experiment with four languages from the Uto-Aztecan family. Our AG-based approaches outperform other unsupervised approaches and show promise when compared to supervised methods, outperforming them on two of the four languages."
W18-4801,Computational Challenges for Polysynthetic Languages,2018,-1,-1,1,1,8328,judith klavans,Proceedings of the Workshop on Computational Modeling of Polysynthetic Languages,0,"Given advances in computational linguistic analysis of complex languages using Machine Learning as well as standard Finite State Transducers, coupled with recent efforts in language revitalization, the time was right to organize a first workshop to bring together experts in language technology and linguists on the one hand with language practitioners and revitalization experts on the other. This one-day meeting provides a promising forum to discuss new research on polysynthetic languages in combination with the needs of linguistic communities where such languages are written and spoken."
W18-1921,Challenges in Speech Recognition and Translation of High-Value Low-Density Polysynthetic Languages,2018,0,1,1,1,8328,judith klavans,Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 2: User Track),0,None
2012.amta-government.8,Government Catalog of Language Resources ({GCLR}),2012,-1,-1,1,1,8328,judith klavans,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Government MT User Program,0,"The purpose of this presentation is to discuss recent efforts within the government to address issues of evaluation and return on investment. Pressure to demonstrate value has increased with the growing amount of foreign language information available, with the variety of languages needing to be exploited, and with the increasing gaps between numbers of language-enabled people and the amount of work to be done. This pressure is only growing as budgets shrink, and as global development grows. Over the past year, the ODNI has led an effort to pull together different government stakeholders to determine some baseline standards for determining Return on Investment via task-based evaluation. Stakeholder consensus on major HLT tasks has involved examination of the different approaches to determining return on investment and how it relates use of HLT in the workflow. In addition to reporting on the goals and progress of this group, we will present future directions and invite community input."
2010.amta-government.9,"Task-based evaluation methods for machine translation, in practice and theory",2010,-1,-1,1,1,8328,judith klavans,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Government MT User Program,0,"A panel of industry and government experts will discuss ways in which they have applied task-based evaluation for Machine Translation and other language technologies in their organizations and share ideas for new methods that could be tried in the future. As part of the discussion, the panelists will address some of the following points: What task-based evaluation means within their organization, i.e., how task-based evaluation is defined; How task-based evaluation impacts the use of MT technologies in their work environment; Whether task-based evaluation correlates with MT developers' automated metrics and if not, how do we arrive at automated metrics that do correlate with the more expensive task-based evaluation; What ``lessons-learned'' resulted from the course of performing task-based evaluation; How task-based evaluations can be generalized to multiple workflow environments."
passonneau-etal-2008-relation,Relation between Agreement Measures on Human Labeling and Machine Learning Performance: Results from an Art History Domain,2008,18,7,4,0,721,rebecca passonneau,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We discuss factors that affect human agreement on a semantic labeling task in the art history domain, based on the results of four experiments where we varied the number of labels annotators could assign, the number of annotators, the type and amount of training they received, and the size of the text span being labeled. Using the labelings from one experiment involving seven annotators, we investigate the relation between interannotator agreement and machine learning performance. We construct binary classifiers and vary the training and test data by swapping the labelings from the seven annotators. First, we find performance is often quite good despite lower than recommended interannotator agreement. Second, we find that on average, learning performance for a given functional semantic category correlates with the overall agreement among the seven annotators for that category. Third, we find that learning performance on the data from a given annotator does not correlate with the quality of that annotatorÂs labeling. We offer recommendations for the use of labeled data in machine learning, and argue that learners should attempt to accommodate human variation. We also note implications for large scale corpus annotation projects that deal with similarly subjective phenomena."
W07-2312,Measuring Variability in Sentence Ordering for News Summarization,2007,17,23,6,0,16057,nitin madnani,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts. We present results of a sentence reordering experiment with three experimental conditions. Our findings indicate a very high degree of variability in the orderings that the eighteen subjects produce. In addition, the variability of reorderings is significantly greater when the initial ordering seen by subjects is different from the original summary. We conclude that evaluation of sentence ordering should use multiple reference orderings. Our evaluation presents several metrics that might prove useful in assessing against multiple references. We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment."
W07-0904,Concept Disambiguation for Improved Subject Access Using Multiple Knowledge Sources,2007,9,3,2,0,49028,tandeep sidhu,Proceedings of the Workshop on Language Technology for Cultural Heritage Data ({L}a{T}e{CH} 2007).,0,"We address the problem of mining text for relevant image metadata. Our work is situated in the art and architecture domain, where highly specialized technical vocabulary presents challenges for NLP techniques. To extract high quality metadata, the problem of word sense disambiguation must be addressed in order to avoid leading the searcher to the wrong image as a result of ambiguous xe2x80x94 and thus faulty xe2x80x94 metadata. In this paper, we present a disambiguation algorithm that attempts to select the correct sense of nouns in textual descriptions of art objects, with respect to a rich domain-specific thesaurus, the Art and Architecture Thesaurus (AAT). We performed a series of intrinsic evaluations using a data set of 600 subject terms extracted from an online National Gallery of Art (NGA) collection of images and text. Our results showed that the use of external knowledge sources shows an improvement over a baseline."
passonneau-etal-2006-climb,{CL}i{MB} {T}ool{K}it: A Case Study of Iterative Evaluation in a Multidisciplinary Project,2006,7,0,5,0,721,rebecca passonneau,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Digital image collections in libraries and other curatorial institutions grow too rapidly to create new descriptive metadata for subject matter search or browsing. CLiMB (Computational Linguistics for Metadata Building) was a project designed to address this dilemma that involved computer scientists, linguists, librarians, and art librarians. The CLiMB project followed an iterative evaluation model: each next phase of the project emerged from the results of an evaluation. After assembling a suite of text processing tools to be used in extracting metada, we conducted a formative evaluation with thirteen participants, using a survey in which we varied the order and type of four conditions under which respondents would propose or select image search terms. Results of the formative evaluation led us to conclude that a CLiMB ToolKit would work best if its main function was to propose terms for users to review. After implementing a prototype ToolKit using a browser interface, we conducted an evaluation with ten experts. Users found the ToolKit very habitable, remained consistently satisfied throughout a lengthy evaluation, and selected a large number of terms per image."
N04-3001,{C}olumbia Newsblaster: Multilingual News Summarization on the Web,2004,13,64,2,0,19953,david evans,Demonstration Papers at {HLT}-{NAACL} 2004,0,"We present the new multilingual version of the Columbia Newsblaster news summarization system. The system addresses the problem of user access to browsing news from multiple languages from multiple sites on the internet. The system automatically collects, organizes, and summarizes news in multiple source languages, allowing the user to browse news topics with English summaries, and compare perspectives from different countries on the topics."
N03-4008,{C}olumbia{'}s Newsblaster: New Features and Future Directions,2003,0,24,6,0,895,kathleen mckeown,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"Columbia's Newsblaster tracking and summarization system is a robust system that clusters news into events, categorizes events into broad topics and summarizes multiple articles on each event. Here we outline our most current work on tracking events over days, producing summaries that update a user on new information about an event, outlining the perspectives of news coming from different countries and clustering and summarizing non-English sources."
kan-etal-2002-using,Using the Annotated Bibliography as a Resource for Indicative Summarization,2002,10,20,2,1,1460,minyen kan,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We report on a language resource consisting of 2000 annotated bibliography entries, which is being analyzed as part of our research on indicative document summarization. We show how annotated bibliographies cover certain aspects of summarization that have not been well-covered by other summary corpora, and motivate why they constitute an important form to study for information retrieval. We detail our methodology for collecting the corpus, and overview our document feature markup that we introduced to facilitate summary analysis. We present the characteristics of the corpus, methods of collection, and show its use in finding the distribution of types of information included in indicative summaries and their relative ordering within the summaries. Automatic text summarization has largely been synonymous with domain-independent, sentence extraction techniques (for an overview, see Paice (1990)). These approaches have used a battery of indicators such as cue phrases, term frequency, and sentence position to choose sentences to extract and form into a summary. An alternative approach is to collect sample summaries and apply machine learning techniques to identify what types of information are included in a summary, and identify their stylistic, grammatical, and lexical choice characteristics and to generate or regenerate a summary based on these characteristics. In this paper, we examine the first step towards this goal: the collection of an appropriate summary corpus. We focus on annotated bibliography entries, because they are written without reliance on sentence extraction. Futhermore, these entries contain both informative (i.e., details and topics of the resource) as well as indicative (e.g., metadata such as author or purpose) information. We believe that summary texts similar in form to annotated bibliography entries, such as the one shown in Figure 1, can better serve users and replace standard -top sentence or query word in context summaries commonly found in current generation search engines. Our corpus of summaries consists of 2000 annotated bibliography entries collected from various Internet websites using search engines. We first review aspects and dimensions of text summaries, and detail reasons for collecting a corpus of annotated bibliography entries. We follow with details on the collection methodology and a description of our annotation of the entries. We conclude with some current applications of the corpus to automatic text summarization research."
muresan-klavans-2002-method,A Method for Automatically Building and Evaluating Dictionary Resources,2002,12,50,2,1,1561,smaranda muresan,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper describes a method toward automatically building dictionaries from text. We present DEFINDER, a rule-based system for extraction of definitions from on-line consumer-oriented medical articles. We provide an extensive evaluation on three dimensions: i) performance of the definition extraction technique in terms of precision and recall, ii) quality of the built dictionary as judged both by specialists and lay users, iii) coverage of existing on-line dictionaries. The corpus we used for the study is publicly available. A major contribution of the paper is the range of quantitative and qualitative evaluation methods."
W01-1011,{GIST}-{IT}: Combining Linguistic and Machine Learning Techniques for Email Summarization,2001,0,3,3,0,4939,evelyne tzoukermann,Proceedings of the {ACL} 2001 Workshop on Human Language Technology and Knowledge Management,0,None
W01-0906,Verification and validation of language processing systems: Is it evaluation?,2001,22,12,2,0,51322,valerie barr,Proceedings of the {ACL} 2001 Workshop on Evaluation Methodologies for Language and Dialogue Systems,0,"If Natural Language Processing (NLP) systems are viewed as intelligent systems then we should be able to make use of verification and validation (V&V) approaches and methods that have been developed in the intelligent systems community. This paper addresses language engineering infrastructure issues by considering whether standard V&V methods are fundamentally different than the evaluation practices commonly used for NLP systems, and proposes practical approaches for applying V&V in the context of language processing systems. We argue that evaluation, as it is performed in the NL community, can be improved by supplementing it with methods from the V&V community."
W01-0813,Applying Natural Language Generation to Indicative Summarization,2001,11,26,3,1,1460,minyen kan,Proceedings of the {ACL} 2001 Eighth {E}uropean Workshop on Natural Language Generation ({EWNLG}),0,"The task of creating indicative summaries that help a searcher decide whether to read a particular document is a difficult task. This paper examines the indicative summarization task from a generation perspective, by first analyzing its required content via published guidelines and corpus analysis. We show how these summaries can be factored into a set of document features, and how an implemented content planner uses the topicality document feature to create indicative multidocument query-based summaries."
W01-0719,Combining linguistic and machine learning techniques for email summarization,2001,13,41,3,1,1561,smaranda muresan,Proceedings of the {ACL} 2001 Workshop on Computational Natural Language Learning ({C}on{LL}),0,"This paper shows that linguistic techniques along with machine learning can extract high quality noun phrases for the purpose of providing the gist or summary of email messages. We describe a set of comparative experiments using several machine learning algorithms for the task of salient noun phrase extraction. Three main conclusions can be drawn from this study: (i) the modifiers of a noun phrase can be semantically as important as the head for the task of gisting, (ii) linguistic filtering improves the performance of machine learning algorithms, (iii) a combination of classifiers improves accuracy."
klavans-etal-2000-evaluation,Evaluation of Computational Linguistic Techniques for Identifying Significant Topics for Browsing Applications,2000,10,1,1,1,8328,judith klavans,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Evaluation of natural language processing tools and systems must focus on two complementary aspects: first, evaluation of the accuracy of the output, and second, evaluation of the functionality of the output as embedded in an application. This paper presents evaluations of two aspects of LinkIT, a tool for noun phrase identification linking, sorting and filtering. LinkIT [Evans 1998] uses a head sorting method [Wacholder 1998] to organize and rank simplex noun phrases (SNPs). LinkIT is to identify significant topics in domainindependent documents. The first evaluation, reported in D.K.Evans et al. 2000 compares the output of the Noun Phrase finder in LinkIT to two other systems. Issues of establishing a gold standard and criteria for matching are discussed. The second evaluation directly concerns the construction of the browsing application. We present results from Wacholder et al. 2000 on a qualitative evaluation which compares three shallow processing methods for extracting"
A00-1042,Evaluation of Automatically Identified Index Terms for Browsing Electronic Documents,2000,15,4,2,0,38348,nina wacholder,Sixth Applied Natural Language Processing Conference,0,"We present an evaluation of domainindependent natural language tools for use in the identification of significant concepts in documents. Using qualitative evaluation, we compare three shallow processing methods for extracting index terms, i.e., terms that can be used to model the content of documents. We focus on two criteria: quality and coverage. In terms of quality alone, our results show that technical term (TT) extraction [Justeson and Katz 1995] receives the highest rating. However, in terms of a combined quality and coverage metric, the Head Sorting (HS) method, described in [Wacholder 1998], outperforms both other methods, keyword (KW) and TT."
W99-0625,Detecting Text Similarity over Short Passages: Exploring Linguistic Feature Combinations via Machine Learning,1999,18,158,2,0,45547,vasileios hatzivassiloglou,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"We present a new composite similarity metric that combines information from multiple linguistic indicators to measure semantic distance between pairs of small textual units. Several potential features are investigated and an optimal combination is selected via machine learning. We discuss a more restrictive definition of similarity than traditional, document-level and information retrieval-oriented, notions of similarity, and motivate it by showing its relevance to the multi-document text summarization problem. Results from our system are evaluated against standard information retrieval techniques, establishing that the new method is more effective in identifying closely related textual units."
W98-1123,Linear Segmentation and Segment Significance,1998,18,95,2,1,1460,minyen kan,Sixth Workshop on Very Large Corpora,0,"We present a new method for discovering a segmental discourse structure of a document while categorizing each segment's function and importance. Segments are determined by a zero-sum weighting scheme, used on occurrences of noun phrases and pronominal forms retrieved from the document. Segment roles are then calculated from the distribution of the terms in the segment. Finally, we present results of evaluation in terms of precision and recall which surpass earlier approaches'."
P98-1112,Role of Verbs in Document Analysis,1998,23,57,1,1,8328,judith klavans,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"We present results of two methods for assessing the event profile of news articles as a function of verb type. The unique contribution of this research is the focus on the role of verbs, rather than nouns. Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile. The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases. An evaluation is performed on the results using Kendall's xcfx84. We present convincing evidence for using verb semantic classes as a discriminant in document classification."
C98-1108,Role of Verbs in Document Analysis,1998,23,57,1,1,8328,judith klavans,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"We present results of two methods for assessing the event profile of news articles as a function of verb type. The unique contribution of this research is the focus on the role of verbs, rather than nouns. Two algorithms are presented and evaluated, one of which is shown to accurately discriminate documents by type and semantic properties, i.e. the event profile. The initial method, using WordNet (Miller et al. 1990), produced multiple cross-classification of articles, primarily due to the bushy nature of the verb tree coupled with the sense disambiguation problem. Our second approach using English Verb Classes and Alternations (EVCA) Levin (1993) showed that monosemous categorization of the frequent verbs in WSJ made it possible to usefully discriminate documents. For example, our results show that articles in which communication verbs predominate tend to be opinion pieces, whereas articles with a high percentage of agreement verbs tend to be about mergers or legal cases. An evaluation is performed on the results using Kendall's xcfx84. We present convincing evidence for using verb semantic classes as a discriminant in document classification."
P97-1004,Expansion of Multi-Word Terms for Indexing and Retrieval Using Morphology and Syntax,1997,23,59,2,0,53812,christian jacquemin,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"A system for the automatic production of controlled index terms is presented using linguistically-motivated techniques. This includes a finite-state part of speech tagger, a derivational morphological processor for analysis and generation, and a unification-based shallow-level parser using transformational rules over syntactic patterns. The contribution of this research is the successful combination of parsing over a seed term list coupled with derivational morphology to achieve greater coverage of multi-word terms for indexing and retrieval. Final results are evaluated for precision and recall, and implications for indexing and retrieval are discussed."
C94-2156,Machine-Readable Dictionaries in Text-to-Speech Systems,1994,8,2,1,1,8328,judith klavans,{COLING} 1994 Volume 2: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"This paper presents the results of an experiment using machine-readable dictionaries (MRDs) and corpora for building concatenative units for text to speech (TTS) systems. Theoretical questions concerning the nature of phonemic data in dictionaries are raised; phonemic dictionary data is viewed as a representative corpus over which to extract n-gram phonemic frequencies in the language. Dictionary data are compared to corpus data, and phoneme inventories are evaluated for coverage. A methodology is defined to compute phonemic n-grams for incorporation into a TTS system."
C92-4177,Degrees of Stativity: The Lexical Representation of Verb Aspect,1992,13,21,1,1,8328,judith klavans,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"L'acquisition automatique de connaissance lexicale a partir de larges corpus s'est essentiellement occupee des phenomenes de co-occurrence, aux depens des traits lexicaux inherents. Nous presentons ici une methodologie qui permet d'obtenir l'information semantique sur l'aspect du verbe en analysant automatiquement un corpus et en appliquant des tests linguistiques a l'aide d'une serie d'outils d'analyse structurale. Lorsque ces deux txc3xa2ches sont accomplies, nous proposons une representation de l'aspect du verbe qui associe une valeur de mesure pour les differents types d'evenements. Les mesures refletent l'usage typique du verbe, et par consequent une mesure de resistance ou de non-resistance a la coercion dans le contexte de la phrase. Les resultats que nous rapportons ici ont ete obtenus de deux manieres: en extrayant l'information necessaire a partir du corpus etiquete de Francis and Kucera (1982), et en faisant tourner un analyseur syntaxique (McCord 1980, 1990) sur le corpus du Reader's Digest afin d'extraire une information plus precise sur l'usage du verbe dans le texte."
C90-3031,The {BICORD} System Combining Lexical Information from Bilingual Corpora and Machine Readable Dictionaries,1990,8,33,1,1,8328,judith klavans,{COLING} 1990 Volume 3: Papers presented to the 13th International Conference on Computational Linguistics,0,"Our goal is to explore methods for combining structured but incomplete information from dictionaries with the unstructured but more complete information available in corpora for the creation of a bilingual lexical data base. This paper concentrates on the class of action verbs of movement, and builds on earlier work on lexical correspondences between languages and specific to this verb class. The languages we explore here are English and French. We first examine the way prototypical verbs of movement are translated in the Collins-Robert (Collins 1978, henceforth CR) bilingual dictionary. We then analyze the behavior of some of these verbs in a large bilingual corpus. We take advantage of the results of linguistic research on verb types (e.g. Levin, to appear) coupled with data from machine readable dictionaries to motivate corpus-based text analysis for the purpose of establishing lexical correspondences with the full range of associated translations and then attach frequencies to translations."
C88-2166,{COMPLEX}: A Computational Lexicon for Natural Language Systems,1988,20,9,1,1,8328,judith klavans,{C}oling {B}udapest 1988 Volume 2: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Although every natural language system needs a computational lexicon, each system puts different amounts and types of information into its lexicon according to its individual needs. However, some of the information needed across systems is shared or identical information. This paper presents our experience in planning and building COMPLEX, a computational lexicon designed to be a repository of shared lexical information for use by Natural Language Processing (NLP) systems. We have drawn primarily on explicit and implicit information from machine-readable dictionaries (MRD's) to create a broad coverage lexicon."
J87-3003,Tools and Methods for Computational Linguistics,1987,21,98,4,0,52731,roy byrd,Computational Linguistics,0,"This paper presents a set of tools and methods for acquiring, manipulating, and analyzing machine-readable dictionaries. We give several detailed examples of the use of these tools and methods for particular analyses. A novel aspect of our work is that it allows the combined processing of multiple machine-readable dictionaries. Our examples describe analyses of data from Webster's Seventh Collegiate Dictionary, the Longman Dictionary of Contemporary English, the Collins bilingual dictionaries, the Collins Thesaurus, and the Zingarelli Italian dictionary. We describe existing facilities and results they have produced as well as planned enhancements to those facilities, particularly in the area of managing associations involving the senses of polysemous words. We show how these enhancements expand the ways in which we can exploit machine-readable dictionaries in the construction of large lexicons for natural language processing systems."
P86-1019,Computer Methods for Morphological Analysis,1986,11,17,2,0,52731,roy byrd,24th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes our current research on the properties of derivational affixation in English. Our research arises from a more general research project, the Lexical Systems project at the IBM Thomas J. Watson Research laboratories, the goal for which is to build a variety of computerized dictionary systems for use both by people and by computer programs. An important sub-goal is to build reliable and robust word recognition mechanisms for these dictionaries. One of the more important issues in word recognition for all morphologically complex languages involves mechanisms for dealing with affixes."
