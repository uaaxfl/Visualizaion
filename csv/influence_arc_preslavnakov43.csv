2009.iwslt-evaluation.14,I05-3025,1,0.891679,"follows: 1. Introduction This is the first year that the National University of Singapore (NUS) participated in the evaluation campaign of the International Workshop on Spoken Language Translation (IWSLT). We submitted a run for the Chinese-English BTEC task1 , where we were ranked second out of twelve participating teams, based on the average of the normalized scores of ten automatic evaluation metrics. We adopted a phrase-based statistical machine translation (SMT) approach, and we investigated the effectiveness of different Chinese word segmentation standards. Using a maximum entropy model [1] and various data sources, we trained six different Chinese word segmenters. Each segmenter was then used to preprocess the Chinese side of the training/development/testing bi-texts, from which a separate phrase-based SMT system was built. Some of the resulting six systems yielded substantial translation performance gains as compared to a system that used the default segmentation provided by the organizers. Finally, we combined the output of all seven systems. The rest of this paper is organized as follows: Section 2 introduces the phrase-based SMT model, Section 3 presents our pre-processing"
2009.iwslt-evaluation.14,N03-1017,0,0.0069836,"Missing"
2009.iwslt-evaluation.14,P03-1021,0,0.0499017,"Missing"
2009.iwslt-evaluation.14,2002.tmi-tutorials.2,0,0.0814266,"Missing"
2009.iwslt-evaluation.14,P07-2045,0,0.0126909,"Missing"
2009.iwslt-evaluation.14,W04-1118,0,0.068797,"ned and used as follows: 4. Word Segmentation and Re-ranking In this section, we describe our experiments with different Chinese word segmentations and how we combine them into a single system using re-ranking. 4.1. Chinese Word Segmentation 1. We ran all seven candidate systems on the development data. The output included the English translation and thirteen associated scores from the SMT toolkit, which we used as features: Chinese word segmentation (CWS) has been shown conclusively as an essential step in machine translation, at least as far as current phrase-based SMT methods are concerned [6]. However, CWS is complicated by the fact that a word is not a well-defined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has b"
2009.iwslt-evaluation.14,2005.iwslt-1.18,0,0.037429,"efined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentat"
2009.iwslt-evaluation.14,P08-1115,0,0.0349163,"efined concept in Chinese, where characters, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentat"
2009.iwslt-evaluation.14,W08-0336,0,0.0478417,"ers, words, and phrases form a blurry continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers,"
2009.iwslt-evaluation.14,W08-0335,0,0.0323456,"continuum. As a consequence, multiple standards exist for the CWS task. For example, two of the SIGHAN CWS bakeoffs offered data according to five different standards: Academia Sinica (AS), UPenn Chinese Treebank (CTB), City University of Hong Kong (CITYU), Peking University (PKU), and Microsoft Research (MSR). It has been hypothesized that different standards may be best suited for different tasks, and the effect of CWS on machine translation has been studied in several recent works, including lattice-based methods [7, 8], segmentation granularity tuning [9], and CWS standards interpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers, and ICTCLAS-generated [11] segmentati"
2009.iwslt-evaluation.14,W03-1730,0,0.0713535,"erpolation [10]. In our experiments, we adopted a very pragmatic approach: we prepared seven candidate systems, each using a different CWS. The final translation output was then selected in a system combination step. Five of the seven segmentations (AS, CTB, CITYU, PKU, and MSR) were generated by an in-house segmenter described in [1], which was ranked first in the AS, CITYU, and PKU open tasks and second in the MSR open task in the SIGHAN 2005 bakeoff (CTB was absent in that year). The two remaining systems were the default segmentation provided by the IWSLT organizers, and ICTCLAS-generated [11] segmentation respectively. Although ICTCLAS was also based on the PKU standard, the output seemed different enough from our PKU segmenter to be included as a separate candidate. (a) five (5) from the distortion model; (b) two (2) from the phrase translation model; (c) two (2) from the lexical translation model; (d) one (1) for the language model; (e) one (1) for the phrase penalty; (f) one (1) for the word penalty; and (g) one (1) for the final overall translation score (as calculated by Moses from all individual scores above and the MERT-tuned parameters). 2. A global fourteenth feature repe"
2009.iwslt-evaluation.14,P09-1104,0,0.0301126,"d various parameters of the phrase-based SMT system. We further describe a novel retraining technique yielding sizeable improvements in BLEU. 5.1. Parameter Tuning 1. We used the training bi-text to build a phrase table and to train an English language model. The Moses phrase-based SMT toolkit has a large number of options. While it comes with very sensible defaults, we found experimentally that varying some of them had a significant impact on the translation quality. Table 1 shows some non-standard settings used in our submission. Note that, for word alignments, we used the Berkeley Aligner2 [12] in unsupervised mode, which we found to outperform GIZA++ significantly. We used the default parameters of the aligner, except that we increased the number of iterations to 40. 2. We used the development dataset to tune the weights of the log-linear model of the phrase-based SMT system using MERT. 3. We concatenated the training and the development datasets; we then re-built the phrase table and retrained the language model on this new dataset. 4. We repeated the above three steps for each of the seven Chinese word segmenters, thus obtaining seven candidate systems. 5.2. Re-training on the De"
2009.iwslt-evaluation.14,P07-1005,1,0.881248,"Missing"
2013.iwslt-evaluation.8,P07-2045,0,0.0254044,"ibe a specialized normalization scheme for evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by"
2013.iwslt-evaluation.8,W10-1738,0,0.130794,"r evaluating Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, a"
2013.iwslt-evaluation.8,P10-4002,0,0.0806549,"Arabic output, which was adopted for the IWSLT’2013 evaluation campaign. 1. Introduction We describe the Arabic-English and English-Arabic statistical machine translation (SMT) systems developed by the Qatar Computing Research Institute (QCRI) for the 2013 open evaluation campaign on spoken language translation organized in conjunction with the International Workshop on Spoken Language Translation (IWSLT). Below we give an overview of the settings we experimented with: • Decoders: We used a phrase-based SMT (PBSMT), as implemented in Moses [1], and two hierarchical decoders: Jane [2] and cdec [3]. See Section 6 for details. • Decoder settings: There are a variety of settings available for the above decoders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc."
2013.iwslt-evaluation.8,N06-2013,0,0.10965,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P08-2039,0,0.073174,"coders. We explored a number of them, most notably, operation sequence model, minimum Bayes risk decoding, monotone-at-punctuation, dropping out-of-vocabulary words, etc. We selected to retain those settings that improved the overall translation quality as measured on the dev-test set. See Section 4 for further details. • Arabic segmentation: To reduce data sparseness, Arabic words are typically segmented into multiple tokens, e.g., by segmenting out conjunctions, pronouns, articles, etc. We experimented with standard segmentation schemes such as D0, D1, D2, D3, S2 and ATB, as defined in MADA [4, 5]. See Section 5 for details. • Domain adaptation: We experimented with three domain adaptation methods to make better use of the huge UN data, which is out-of-domain: (i) Modified Moore-Lewis filtering, (ii) phrase table merging, and (iii) phrase table backoff. See Section 7 for details. For our final submission, we synthesized a translation by combining the output of our best individual system with the output of other systems that are both relatively strong and can contribute to having more diversity, e.g., using a different decoder or a different segmentation scheme. We achieved the most not"
2013.iwslt-evaluation.8,P12-1016,0,0.268259,"lish BLEU 1-TER System English IWSLT mono 109 English-French SETimes UN (Es-En + En-Fr) UN (Ar-En) News Crawl 2007-2009 News Crawl 2009-2012 Common Crawl Wiki Headlines Europarl v.7 News Commentary v.8 Gigaword v.5 2.7M 575M 4.2M 597M 115M 643M 745M 185M 1.1M 54M 5.3M 4,032M Arabic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We"
2013.iwslt-evaluation.8,J93-2003,0,0.0246271,"abic IWSLT mono UN News Commentary Arabic v.8 Gigaword Arabic v.5 2.7M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard"
2013.iwslt-evaluation.8,N03-1017,0,0.0173015,"M 134M 4.8M 1,373M Table 1: Admissible training data for language modeling. Here English is tokenized, and Arabic is ATB-segmented. Preprocessing. We segmented the Arabic side of the bitext following the ATB scheme and using the Stanford word segmenter [6]. For the English side, we used the standard tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, th"
2013.iwslt-evaluation.8,2005.iwslt-1.8,0,0.0225691,"rd tokenizer of Moses, and we further applied truecasing/lowercasing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. De"
2013.iwslt-evaluation.8,W11-2123,0,0.408919,"sing when English was the target/source language. Training. We built separate directed word alignments for English-to-Arabic and for Arabic-to-English using IBM model 4 [7], and we symmetrized them using the grow-diagfinal-and heuristics [8]. We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing, thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On"
2013.iwslt-evaluation.8,P02-1040,0,0.0879883,", thus obtaining a phrase table where each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers"
2013.iwslt-evaluation.8,D11-1125,0,0.032033,"each phrase pair has the standard five translation model features. We also built a lexicalized reordering model [9]: msd-bidirectional-fe. For language modeling, we used KenLM [10] to build a 5-gram Kneser-Ney smoothed model, trained on the target side of the training bi-text. Finally, we built a large joint log-linear model, which used standard PBSMT feature functions: language model probability, word penalty, the parameters from the phrase table, and those from the reordering model. Tuning. We tuned the weights in the log-linear model by optimizing BLEU [11] on the tuning dataset, using PRO [12]. We allowed the optimizer to run for up to 10 iterations, and to extract 1000-best lists on each iteration. Decoding. On tuning and testing, we used monotone-atpunctuation. On testing, we further used cube pruning. Table 2 shows the results3 for the baseline English-toArabic and Arabic-to-English SMT systems, compared to the baseline results reported on the WIT3 webpage. 3 For tst2010, we report MultEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSL"
2013.iwslt-evaluation.8,C12-1121,1,0.915839,"ltEval BLEU and TER0.8: on tokenized and recased output for English, and on QCRI-normalized output for Arabic. For tst2011, tst2012, and tst2013, the organizers used slightly different scorers. IWSLT baseline Our baseline 23.6 24.7 English-Arabic BLEU 1-TER 43.0 45.6 11.9 12.6 28.6 29.1 Table 2: Our vs. IWSLT baseline results for English-toArabic and Arabic-to-English SMT, evaluated on tst2010. 4. System Settings Below we discuss the decoder settings and extensions we experimented with, focusing on Arabic-to-English. Table 3 shows the impact of each feature when added to the baseline. Tuning. [13] have shown that PRO tends to generate too short translations.4 They have suggested that the root of the problem was that PRO optimizes sentence-level BLEU+1, which smooths the precision component of BLEU, but leaves the brevity penalty intact, which destroys the balance between them. They have proposed a number of fixes, the simplest and most efficient among them being to smooth the brevity penalty as well.5 In our experiments, this yielded +0.2 BLEU for Arabic-to-English on tst2010. Operation sequence model. The operation sequence model (OSM) is an n-gram-based model, which represents the al"
2013.iwslt-evaluation.8,P13-2003,1,0.839319,"Missing"
2013.iwslt-evaluation.8,P13-2071,0,0.0200742,"of operations, e.g., generate a sequence of source and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0"
2013.iwslt-evaluation.8,W13-2212,0,0.0142268,"urce and target words or perform reordering. The model memorizes Markov chains over such sequences, thus fusing lexical generation and reordering into a single generative model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Trans"
2013.iwslt-evaluation.8,N04-1022,0,0.108744,"e model. OSM offers two advantages. First, it considers bilingual contextual information that goes beyond phrase boundaries. Second, it provides a better reordering mechanism that has richer conditioning than a lexicalized reordering model: the probability of an operation is conditioned on the n previous translation and reordering decisions. We used the Moses implementation of OSM [15], which has yielded improvements at WMT’13 [16]. In our experiments, it yielded +0.6 BLEU for Arabic-to-English on tst2010. Minimum Bayes risk decoding. We also experimented with minimum Bayes risk decoding (MBR)[17], which, instead of outputting the translation with the highest probability, prefers the one that is most similar to best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1"
2013.iwslt-evaluation.8,P11-1044,1,0.871958,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P12-1049,1,0.860515,"o best n translations. In our case, using MBR did not improve over the baseline. Translation options per input phrase. By default, Moses uses up to 20 translation options per input phrase, but [16] have shown better results with 100. In our experiments, this yielded +0.1 BLEU for Arabic-to-English on tst2010. Transliterating OOVs. Out-of-vocabulary (OOV) words are problematic for languages with different scripts. Thus, we tried transliteration as post-processing: we extracted 1-1 word alignments from a subset of the UN bitext, and we used them to train a character-level transliteration system [18, 19] using Moses. As Table 3 shows this did not help, probably due to the small number of OOVs in tst2010. 4 See [14] for a discussion about more potential issues with PRO. --proargs=’--smooth-brevity-penalty’ 5 Available in Moses: System Baseline (B) OSM MBR Ttable 100 PRO-fix [13] TRANSLIT Drop UNK Arabic-English (tst2010) BLEU 1-TER 24.7 25.3 24.7 24.8 24.9 24.7 24.8 Arabic-English (tst2010) BLEU 1-TER System 45.6 46.1 45.7 45.6 44.7 45.6 45.7 SEG-D0 SEG-D1 SEG-D2 SEG-D3 SEG-S2 SEG-ATB 22.4 23.6 24.1 24.4 24.5 24.7 43.0 44.2 45.2 45.5 45.7 45.6 Table 5: Using different Arabic segmentation schem"
2013.iwslt-evaluation.8,P03-1021,0,0.0120716,"hierarchical cdec decoder [3]. We used its default features: forward and backward translation features, singleton features, a glue-rule probability, and a pass-through feature (to handle OOVs). We tuned the parameters using MIRA with IBM BLEU as the objective function and a k-best forest size of 250. Jane. We also used another hierarchical phrase-based decoder: Jane 2.2 [2]. We used the standard features: phrase translation probabilities and lexical smoothing in both directions, word and phrase penalties, a distance-based distortion model, and a 5-gram LM. We optimized the weights using MERT [21] on 100-best candidates with BLEU as objective. 5. Arabic Segmentation 7. Adaptation In Arabic, various clitics such as pronouns, conjunctions and articles appear concatenated to content words such as nouns and verbs. This can cause data sparseness issues, and thus clitics are typically segmented in a preprocessing step. There are various standard segmentation schemes defined in MADA [4, 5] such as D0, D1, D2, D3 and S2, for which we used the MADA+TOKAN toolkit [20], as well as ATB, which we performed using the Stanford segmenter [6]. Table 5 shows the results when training on the TED bitext o"
2013.iwslt-evaluation.8,P10-2041,0,0.0888063,"Missing"
2013.iwslt-evaluation.8,D11-1033,0,0.090854,"Missing"
2013.iwslt-evaluation.8,W08-0320,1,0.891463,"Missing"
2013.iwslt-evaluation.8,D09-1141,1,0.919233,"Missing"
2013.iwslt-evaluation.8,W09-0408,0,0.0608322,"Missing"
2013.iwslt-evaluation.8,W12-5611,0,0.0448251,"could build a strong LM through interpolation, similarly to our Arabic-to-English LM, that also used the Gigaword Arabic, UN, and News Commentary data (see Table 1). Desegmentation. Unlike the Arabic-to-English direction, where the segmentation was on the input side and thus the output was unaffected, here the segmentation had to be undone. For example, if we use an ATB-segmented target side, we end up with an ATB-segmented translation output, which we have to desegment in order to obtain proper Arabic. Desegmentation is not a trivial task since it involves some morphological adjustments, see [27] for a broader discussion. For desegmentation, we used the best approach described in [27]; in fact, we used their implementation. Normalization. Translating into Arabic is tricky because the Arabic spelling is often inconsistent in terms of punctuation (using both Arabic UTF8 and English punctuation symbols), digits (appearing as both Arabic and Indian characters), diacritics (can be used or omitted, and can often be wrong), spelling (there are many errors in the spelling of some Arabic characters, esp. Alef and Ta Marbuta; also, Waa appears sometimes separated). These problems are especially"
2020.acl-demos.32,D19-1565,1,0.816288,"Missing"
2020.acl-demos.32,N19-1423,0,0.00636415,"ge [0,1], which allows us to show a confidence for each prediction. Further details about the techniques, the model, the data, and the experiments can be found in (Da San Martino et al., 2019).4 3 Figure 1: The architecture of our model. Model Our model is based on multi-task learning with the following two tasks: FLC Fragment-level classification. Given a sentence, identify all spans of use of propaganda techniques in it and the type of technique. SLC Sentence-level classification. Given a sentence, predict whether it contains at least one propaganda technique. Our model adds on top of BERT (Devlin et al., 2019) a set of layers that combine information from the fragment- and the sentence-level annotations to boost the performance of the FLC task on the basis of the SLC task. The network architecture is shown in Figure 1, and we refer to it as a multigranularity network. It features 19 output units for each input token in the FLC task, standing for one of the 18 propaganda techniques or “no technique.” A complementary output focuses on the SLC task, which is used to generate, through a trainable gate, a weight w that is multiplied by the input of the FLC task. The gate consists of a projection layer t"
2020.acl-demos.32,D17-2002,0,0.0670506,", the application allows users to input and to analyze any text or URL of interest; this is also possible via an API, which allows other applications to be built on top of the system. Prta relies on a supervised multi-granularity gated BERT-based model, which we train on a corpus of news articles annotated at the fragment level with 18 propaganda techniques, a total of 350K word tokens (Da San Martino et al., 2019). Consider the game Argotario, which educates people to recognize and create fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda (Habernal et al., 2017, 2018a). Unlike them, we have a richer inventory of techniques and we show them in the context of actual news. The remainder of this paper is organized as follows. Section 2 introduces the machine learning model at the core of the Prta system. Section 3 sketches the full architecture of Prta, with focus on the process of collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propa"
2020.acl-demos.32,L18-1526,0,0.0984893,"f collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propaganda has been tackled primarily at the article level (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Barr´on-Cede˜no et al., 2019). It is also different from work in the related field of computational argumentation, which deals with some specific logical fallacies related to propaganda, such as ad hominem fallacy (Habernal et al., 2018b). 288 2 Data and Model Data We train our model on a corpus of 350K tokens (Da San Martino et al., 2019; Yu et al., 2019), manually annotated by professional annotators with the instances of use of eighteen propaganda techniques. See Table 1 for a complete list and examples for each of these techniques.2 2 Detailed list with definitions and examples is available at http://propaganda.qcri.org/annotations/definitions.html For the Prta system, we applied a softmax operator to turn its output into a bounded value in the range [0,1], which allows us to show a confidence for each prediction. Furthe"
2020.acl-demos.32,N18-1036,0,0.0123929,"f collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propaganda has been tackled primarily at the article level (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Barr´on-Cede˜no et al., 2019). It is also different from work in the related field of computational argumentation, which deals with some specific logical fallacies related to propaganda, such as ad hominem fallacy (Habernal et al., 2018b). 288 2 Data and Model Data We train our model on a corpus of 350K tokens (Da San Martino et al., 2019; Yu et al., 2019), manually annotated by professional annotators with the instances of use of eighteen propaganda techniques. See Table 1 for a complete list and examples for each of these techniques.2 2 Detailed list with definitions and examples is available at http://propaganda.qcri.org/annotations/definitions.html For the Prta system, we applied a softmax operator to turn its output into a bounded value in the range [0,1], which allows us to show a confidence for each prediction. Furthe"
2020.acl-demos.32,D17-1317,0,0.130782,"chniques and we show them in the context of actual news. The remainder of this paper is organized as follows. Section 2 introduces the machine learning model at the core of the Prta system. Section 3 sketches the full architecture of Prta, with focus on the process of collection and processing of the input articles. Section 4 describes the system interface and its functionality, and presents some examples. Section 5 draws conclusions and discusses possible directions for future work. Our work is in contrast to previous efforts, where propaganda has been tackled primarily at the article level (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019; Barr´on-Cede˜no et al., 2019). It is also different from work in the related field of computational argumentation, which deals with some specific logical fallacies related to propaganda, such as ad hominem fallacy (Habernal et al., 2018b). 288 2 Data and Model Data We train our model on a corpus of 350K tokens (Da San Martino et al., 2019; Yu et al., 2019), manually annotated by professional annotators with the instances of use of eighteen propaganda techniques. See Table 1 for a complete list and examples for each of these techniques.2 2 Detailed list with defi"
2020.acl-main.308,K19-1096,1,0.888577,"unlike the articles, the number of media profiles is too small to fine-tune BERT, and (ii) most Twitter descriptions have sentence-like structure and length. If a medium has no Twitter account, we used a vector of zeros. 3.2 Who Read it We argue that the audience of a news medium can be indicative of the political orientation of that medium. We thus propose a number of features to model this, which we describe below. 3.2.1 Twitter Followers Bio Previous research has used the followers’ networks and the retweeting behavior in order to infer the political bias of news media (Wong et al., 2013; Atanasov et al., 2019; Darwish et al., 2020). Here, we analyze the self-description (bio) of Twitter users that follow the target news medium. The assumption is that (i) followers would likely agree with the news medium’s bias, and (ii) they might express their own bias in their self-description. 3367 3.3 What Was Written About the Target Medium We retrieved the public profiles of 5,000 followers for each target news medium with a Twitter account, and we excluded those with non-English bios since our dataset is mostly about US media. Then, we encoded each follower’s bio using SBERT (Reimers and Gurevych, 2019). As"
2020.acl-main.308,D18-1389,1,0.0716118,"ns for future work. 2 Related Work While leveraging social information and temporal structure to predict the factuality of reporting of a news medium is not new (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016), modeling this at the medium level is a mostly unexplored problem. A popular approach to predict the factuality of a medium is to check the general stance of that medium concerning already fact-checked claims (Mukherjee and Weikum, 2015; Popat et al., 2017, 2018). Therefore, stance detection became an essential component in fact-checking systems (Baly et al., 2018b). In political science, media profiling is essential for understanding media choice (Iyengar and Hahn, 2009), voting behavior (DellaVigna and Kaplan, 2007), and polarization (Graber and Dunaway, 2017). The outlet-level bias is measured as a similarity of the language used in news media to political speeches of congressional Republicans or Democrats, also used to measure media slant (Gentzkow and Shapiro, 2006). Article-level bias was also measured via crowd-sourcing (Budak et al., 2016). Nevertheless, public awareness of media bias is limited (Elejalde et al., 2018). Political bias was tradi"
2020.acl-main.308,N19-1216,1,0.833934,"y introducing new sources, mostly related to the social media context, thus achieving sizable improvements on the same dataset. 3365 Figure 1: The architecture of our system for predicting the political bias and the factuality of reporting of news media. The features inside {curly brackets} are calculated at a finer level of granularity and are then aggregated at the medium level. The upper gray box shows the resources used to generate features, e.g., the OpenSmile toolkit is used to extract low-level descriptors (LLD) from YouTube videos; see Section 3 for further details. In follow-up work (Baly et al., 2019), we showed that jointly predicting the political bias and the factuality is beneficial, compared to predicting each of them independently. We used the same sources of information as in (Baly et al., 2018a), but the results were slightly lower. While here we focus on analyzing political bias and factuality separately, future work may analyze how the newly proposed features and sources affect the joint prediction. 3 System and Features In this section, we present our system. For each target medium, it extracts a variety of features to model (i) what was written by the medium, (ii) the audience"
2020.acl-main.308,N18-2004,1,0.915404,"Missing"
2020.acl-main.308,D19-1565,1,0.908899,"Missing"
2020.acl-main.308,N19-1423,0,0.00673533,"veraged the NELA features for the individual articles in order to obtain a NELA representation for a news medium. Using arithmetic averaging is a good idea as it captures the general trend of articles in a medium, while limiting the impact of outliers. For instance, if a medium is known to align with left-wing ideology, this should not change if it published a few articles that align with right-wing ideology. We use this method to aggregate all features that we collected at a level of granularity that is finer than the medium-level. 3366 Embedding Features: We encoded each article using BERT (Devlin et al., 2019) by feeding the first 510 WordPieces2 from the article3 and then averaging the word representations extracted from the second-to-last layer.4 In order to obtain representations that are relevant to our tasks, we finetuned BERT by training a softmax layer on top of the [CLS] output vector to predict the label (bias or factuality) of news articles that are scrapped from an external list of media to avoid overfitting. The articles’ labels are assumed to be the same as those of the media in which they are published (a form of distant supervision). This is common practice in tasks such as “fake new"
2020.acl-main.308,D18-1388,0,0.0668289,"to measure media slant (Gentzkow and Shapiro, 2006). Article-level bias was also measured via crowd-sourcing (Budak et al., 2016). Nevertheless, public awareness of media bias is limited (Elejalde et al., 2018). Political bias was traditionally used as a feature for fact verification (Horne et al., 2018b). In terms of modeling, Horne et al. (2018a) focused on predicting whether an article is biased or not. Political bias prediction was explored by Potthast et al. (2018) and Saleh et al. (2019), where news articles were modeled as left vs. right, or as hyperpartisan vs. mainstream. Similarly, Kulkarni et al. (2018) explored the left vs. right bias at the article level, modeling both textual and URL contents of articles. In our earlier research (Baly et al., 2018a), we analyzed both the political bias and the factuality of news media. We extracted features from several sources of information, including articles published by each medium, what is said about it on Wikipedia, metadata from its Twitter profile, in addition to some web features (URL structure and traffic information). The experiments on the Media Bias/Fact Check (MBFC) dataset showed that combining features from these different sources of info"
2020.acl-main.308,N18-1070,1,0.908968,"Missing"
2020.acl-main.308,D19-1410,0,0.020435,"l., 2013; Atanasov et al., 2019; Darwish et al., 2020). Here, we analyze the self-description (bio) of Twitter users that follow the target news medium. The assumption is that (i) followers would likely agree with the news medium’s bias, and (ii) they might express their own bias in their self-description. 3367 3.3 What Was Written About the Target Medium We retrieved the public profiles of 5,000 followers for each target news medium with a Twitter account, and we excluded those with non-English bios since our dataset is mostly about US media. Then, we encoded each follower’s bio using SBERT (Reimers and Gurevych, 2019). As we had plenty of followers’ bios, this time fine-tuning BERT would have been feasible. However, we were afraid to use distant supervision for labeling as we did with the articles since people sometimes follow media with different political ideologies. Thus, we opted for SBERT, and we averaged the SBERT representations across the bios in order to obtain a medium-level representation. Wikipedia contents describing news media were useful for predicting the political bias and the factuality of these media (Baly et al., 2018a). We automatically retrieved the Wikipedia page for each medium, and"
2020.acl-main.308,S19-2182,1,0.898507,"Missing"
2020.acl-main.308,2020.acl-main.332,1,0.876076,"Missing"
2020.acl-main.308,2020.acl-main.50,1,0.52838,"of the Facebook audience, and information from the profiles of the media followers on Twitter. We further modeled different modalities: text, metadata, and speech signal. The evaluation results have shown that while what was written matters most, the social media context is also important as it is complementary, and putting them all together yields sizable improvements over the state of the art. In future work, we plan to perform user profiling with respect to polarizing topics such as gun control (Darwish et al., 2020), which can then be propagated from users to media (Atanasov et al., 2019; Stefanov et al., 2020). We further want to model the network structure, e.g., using graph embeddings (Darwish et al., 2020). Another research direction is to profile media based on their stance with respect to previously fact-checked claims (Mohtarami et al., 2018; Shaar et al., 2020), or by the proportion and type of propaganda techniques they use (Da San Martino et al., 2019, 2020). Finally, we plan to experiment with other languages. Acknowledgments This research is part of the Tanbih project9 , which aims to limit the effect of “fake news,” propaganda and media bias by making users aware of what they are readin"
2020.acl-main.308,P18-1022,0,0.0863147,"evel bias is measured as a similarity of the language used in news media to political speeches of congressional Republicans or Democrats, also used to measure media slant (Gentzkow and Shapiro, 2006). Article-level bias was also measured via crowd-sourcing (Budak et al., 2016). Nevertheless, public awareness of media bias is limited (Elejalde et al., 2018). Political bias was traditionally used as a feature for fact verification (Horne et al., 2018b). In terms of modeling, Horne et al. (2018a) focused on predicting whether an article is biased or not. Political bias prediction was explored by Potthast et al. (2018) and Saleh et al. (2019), where news articles were modeled as left vs. right, or as hyperpartisan vs. mainstream. Similarly, Kulkarni et al. (2018) explored the left vs. right bias at the article level, modeling both textual and URL contents of articles. In our earlier research (Baly et al., 2018a), we analyzed both the political bias and the factuality of news media. We extracted features from several sources of information, including articles published by each medium, what is said about it on Wikipedia, metadata from its Twitter profile, in addition to some web features (URL structure and tr"
2020.acl-main.332,D19-3004,0,0.0130621,"b et al., 2019a,b; Huynh and Papotti, 2019). Yet another direction performs factchecking based on tables (Chen et al., 2019). There is also recent work on using language models as knowledge bases (Petroni et al., 2019). Ours is yet another research direction. While our main contribution here is the new task and the new dataset, we should also mentioned some work on retrieving documents. In our experiments, we perform retrieval using BM25 (Robertson and Zaragoza, 2009) and re-ranking using BERT-based similarity, which is a common strategy in recent state-of-the-art retrieval models (Akkalyoncu Yilmaz et al., 2019a; Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019b). Our approach is most similar to that of (Akkalyoncu Yilmaz et al., 2019a), but we differ, as we perform matching, both with BM25 and with BERT, against the normalized claim, against the title, and against the full text of the articles in the fact-checking dataset; we also use both scores and reciprocal ranks when combining different scores and rankings. Moreover, we use sentence-BERT instead of BERT. Previous work has argued that BERT by itself does not yield good sentence representation. Thus, approaches such as sentenceBERT (Reimer"
2020.acl-main.332,D19-1352,0,0.0167937,"b et al., 2019a,b; Huynh and Papotti, 2019). Yet another direction performs factchecking based on tables (Chen et al., 2019). There is also recent work on using language models as knowledge bases (Petroni et al., 2019). Ours is yet another research direction. While our main contribution here is the new task and the new dataset, we should also mentioned some work on retrieving documents. In our experiments, we perform retrieval using BM25 (Robertson and Zaragoza, 2009) and re-ranking using BERT-based similarity, which is a common strategy in recent state-of-the-art retrieval models (Akkalyoncu Yilmaz et al., 2019a; Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019b). Our approach is most similar to that of (Akkalyoncu Yilmaz et al., 2019a), but we differ, as we perform matching, both with BM25 and with BERT, against the normalized claim, against the title, and against the full text of the articles in the fact-checking dataset; we also use both scores and reciprocal ranks when combining different scores and rankings. Moreover, we use sentence-BERT instead of BERT. Previous work has argued that BERT by itself does not yield good sentence representation. Thus, approaches such as sentenceBERT (Reimer"
2020.acl-main.332,D19-1475,0,0.160696,"Missing"
2020.acl-main.332,D18-1389,1,0.891132,"ork with naturally occurring claims as they were made in political debates and speeches or in social media. There has also been a lot of research on automatic fact-checking of claims and rumors, going in several different directions. One research direction focuses on the social aspects of the claim and how users in social media react to it (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Gorrell et al., 2019; Ma et al., 2019). Another direction mines the Web for information that proves or disproves the claim (Mukherjee and Weikum, 2015; Karadzhov et al., 2017; Popat et al., 2017; Baly et al., 2018b; Mihaylova et al., 2018; Nadeem et al., 2019). In either case, it is important to model the reliability of the source as well as the stance of the claim with respect to other claims; in fact, it has been proposed that a claim can be fact-checked based on its source alone (Baly et al., 2018a) or based on its stance alone (Dungs et al., 2018). A third direction performs fact-checking against Wikipedia (Thorne et al., 2018; Nie et al., 2019), or against a general collection of documents (Miranda et al., 2019). A fourth direction uses a knowledge base or a knowledge graph (Ciampaglia et al., 201"
2020.acl-main.332,N18-2004,1,0.876091,"Missing"
2020.acl-main.332,S17-2001,0,0.0439101,"rks that are fine-tuned on NLI and STS-B data. Indeed, in our experiments, we found sentence-BERT to perform much better than BERT. The Universal Sentence Encoder (Cer et al., 2018) is another alternative, but sentence-BERT worked better in our experiments. Finally, our task is related to semantic relatedness tasks, e.g., from the GLUE benchmark (Wang et al., 2018), such as natural language inference, or NLI task (Williams et al., 2018), recognizing textual entailment, or RTE (Bentivogli et al., 2009), paraphrase detection (Dolan and Brockett, 2005), and semantic textual similarity, or STS-B (Cer et al., 2017). However, it also differs from them, as we will see in the following section. 3 Task Definition We define the task as follows: Given a checkworthy input claim and a set of verified claims, rank those verified claims, so that the claims that can help verify the input claim, or a sub-claim in it, are ranked above any claim that is not helpful to verify the input claim. Table 1 shows some examples of input–verified claim pairs, where the input claims are sentences from the 2016 US Presidential debates, and the verified claims are the corresponding fact-checked counter-parts in PolitiFact. 3609 N"
2020.acl-main.332,D18-2029,0,0.0266817,"Missing"
2020.acl-main.332,N19-1423,0,0.0129058,"al time, in which case the system would return a short list of 3-5 claims that the journalist can quickly skim and make sure they are indeed a true match. We further report MAP@k and HasPositive@k for k ∈ {10, 20} as well as MAP (untruncated), which would be more suitable in a non-real-time scenario, where recall would be more important. 6.2 BERT-based Models The BM25 algorithm focuses on exact matches, but as lines 2–5 in Table 1 and line 3 in Table 2 show, the input claim can use quite different words. Thus, we further try semantic matching using BERT. Initially, we tried to fine-tune BERT (Devlin et al., 2019), but this did not work well, probably because we did not have enough data to perform the fine-tuning. Thus, eventually we opted to use BERT (and variations thereof) as a sentence encoder, and to perform max-pooling on the penultimate layer to obtain a representation for an input piece of text. Then, we calculate the cosine similarity between the representation of the input claim and of the verified claims in the dataset, and we use this similarity for ranking. 3612 • BERT:base,uncased: model of BERT; the base, uncased • RoBERTa:base: the base, cased model of RoBERTa (Liu et al., 2019); • sent"
2020.acl-main.332,I05-5002,0,0.206712,"tence-level representations. This is achieved using Siamese BERT networks that are fine-tuned on NLI and STS-B data. Indeed, in our experiments, we found sentence-BERT to perform much better than BERT. The Universal Sentence Encoder (Cer et al., 2018) is another alternative, but sentence-BERT worked better in our experiments. Finally, our task is related to semantic relatedness tasks, e.g., from the GLUE benchmark (Wang et al., 2018), such as natural language inference, or NLI task (Williams et al., 2018), recognizing textual entailment, or RTE (Bentivogli et al., 2009), paraphrase detection (Dolan and Brockett, 2005), and semantic textual similarity, or STS-B (Cer et al., 2017). However, it also differs from them, as we will see in the following section. 3 Task Definition We define the task as follows: Given a checkworthy input claim and a set of verified claims, rank those verified claims, so that the claims that can help verify the input claim, or a sub-claim in it, are ranked above any claim that is not helpful to verify the input claim. Table 1 shows some examples of input–verified claim pairs, where the input claims are sentences from the 2016 US Presidential debates, and the verified claims are the"
2020.acl-main.332,C18-1284,0,0.0196088,"(Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Gorrell et al., 2019; Ma et al., 2019). Another direction mines the Web for information that proves or disproves the claim (Mukherjee and Weikum, 2015; Karadzhov et al., 2017; Popat et al., 2017; Baly et al., 2018b; Mihaylova et al., 2018; Nadeem et al., 2019). In either case, it is important to model the reliability of the source as well as the stance of the claim with respect to other claims; in fact, it has been proposed that a claim can be fact-checked based on its source alone (Baly et al., 2018a) or based on its stance alone (Dungs et al., 2018). A third direction performs fact-checking against Wikipedia (Thorne et al., 2018; Nie et al., 2019), or against a general collection of documents (Miranda et al., 2019). A fourth direction uses a knowledge base or a knowledge graph (Ciampaglia et al., 2015; Shiadralkar et al., 2017; Gad-Elrab et al., 2019a,b; Huynh and Papotti, 2019). Yet another direction performs factchecking based on tables (Chen et al., 2019). There is also recent work on using language models as knowledge bases (Petroni et al., 2019). Ours is yet another research direction. While our main contribution here is the new tas"
2020.acl-main.332,gencheva-etal-2017-context,1,0.899587,"Missing"
2020.acl-main.332,S19-2147,0,0.0300313,"for automatic fact-checking of individual claims, not for checking whether an input claim was fact-checked previously. Note that while the above work used manually normalized claims as input, we work with naturally occurring claims as they were made in political debates and speeches or in social media. There has also been a lot of research on automatic fact-checking of claims and rumors, going in several different directions. One research direction focuses on the social aspects of the claim and how users in social media react to it (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Gorrell et al., 2019; Ma et al., 2019). Another direction mines the Web for information that proves or disproves the claim (Mukherjee and Weikum, 2015; Karadzhov et al., 2017; Popat et al., 2017; Baly et al., 2018b; Mihaylova et al., 2018; Nadeem et al., 2019). In either case, it is important to model the reliability of the source as well as the stance of the claim with respect to other claims; in fact, it has been proposed that a claim can be fact-checked based on its source alone (Baly et al., 2018a) or based on its stance alone (Dungs et al., 2018). A third direction performs fact-checking against Wikipedia (T"
2020.acl-main.332,karadzhov-etal-2017-fully,1,0.901387,"Missing"
2020.acl-main.332,2021.ccl-1.108,0,0.060864,"Missing"
2020.acl-main.332,N19-4014,0,0.013039,"were made in political debates and speeches or in social media. There has also been a lot of research on automatic fact-checking of claims and rumors, going in several different directions. One research direction focuses on the social aspects of the claim and how users in social media react to it (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Gorrell et al., 2019; Ma et al., 2019). Another direction mines the Web for information that proves or disproves the claim (Mukherjee and Weikum, 2015; Karadzhov et al., 2017; Popat et al., 2017; Baly et al., 2018b; Mihaylova et al., 2018; Nadeem et al., 2019). In either case, it is important to model the reliability of the source as well as the stance of the claim with respect to other claims; in fact, it has been proposed that a claim can be fact-checked based on its source alone (Baly et al., 2018a) or based on its stance alone (Dungs et al., 2018). A third direction performs fact-checking against Wikipedia (Thorne et al., 2018; Nie et al., 2019), or against a general collection of documents (Miranda et al., 2019). A fourth direction uses a knowledge base or a knowledge graph (Ciampaglia et al., 2015; Shiadralkar et al., 2017; Gad-Elrab et al.,"
2020.acl-main.332,D19-1250,0,0.046948,"Missing"
2020.acl-main.332,D17-1317,0,0.0408621,"le sources, organized into a knowledge graph (KG). The system can perform data exploration, e.g., it can find all claims that contain a certain named entity or keyphrase. In contrast, we are interested in detecting whether a claim was previously fact-checked. Other work has focused on creating datasets of textual fact-checked claims, without building KGs. Some of the larger ones include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), and the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), the 10K claims Truth of Various Shades (Rashkin et al., 2017) dataset, among several other datasets, which were used for automatic fact-checking of individual claims, not for checking whether an input claim was fact-checked previously. Note that while the above work used manually normalized claims as input, we work with naturally occurring claims as they were made in political debates and speeches or in social media. There has also been a lot of research on automatic fact-checking of claims and rumors, going in several different directions. One research direction focuses on the social aspects of the claim and how users in social media react to it (Canin"
2020.acl-main.332,D19-1410,0,0.18311,", 2019a; Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019b). Our approach is most similar to that of (Akkalyoncu Yilmaz et al., 2019a), but we differ, as we perform matching, both with BM25 and with BERT, against the normalized claim, against the title, and against the full text of the articles in the fact-checking dataset; we also use both scores and reciprocal ranks when combining different scores and rankings. Moreover, we use sentence-BERT instead of BERT. Previous work has argued that BERT by itself does not yield good sentence representation. Thus, approaches such as sentenceBERT (Reimers and Gurevych, 2019) have been proposed, which are specifically trained to produce good sentence-level representations. This is achieved using Siamese BERT networks that are fine-tuned on NLI and STS-B data. Indeed, in our experiments, we found sentence-BERT to perform much better than BERT. The Universal Sentence Encoder (Cer et al., 2018) is another alternative, but sentence-BERT worked better in our experiments. Finally, our task is related to semantic relatedness tasks, e.g., from the GLUE benchmark (Wang et al., 2018), such as natural language inference, or NLI task (Williams et al., 2018), recognizing textu"
2020.acl-main.332,N18-1074,0,0.0899973,"Missing"
2020.acl-main.332,R19-1141,1,0.856915,"Missing"
2020.acl-main.332,W18-5446,0,0.0223141,"Missing"
2020.acl-main.332,P17-2067,0,0.420811,"claims online are true. Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking organizations plus another 92 inactive.2 While some organizations debunked just a couple of hundred claims, others such as Politifact,3 FactCheck.org,4 Snopes,5 and Full Fact6 have fact-checked thousands or even tens of thousands of claims. The value of these collections of resources has been recognized in the research community, and they have been used to train systems to perform automatic fact-checking (Popat et al., 2017; Wang, 2017; Zlatkova et al., 2019) or to detect checkworthy claims in political debates (Hassan et al., 2015; Gencheva et al., 2017; Patwari et al., 2017; Vasileva et al., 2019). There have also been datasets that combine claims from multiple fact-checking organizations (Augenstein et al., 2019), again with the aim of performing automatic fact-checking. The recent proliferation of “fake news” has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases t"
2020.acl-main.332,N18-1101,0,0.0181467,"sentenceBERT (Reimers and Gurevych, 2019) have been proposed, which are specifically trained to produce good sentence-level representations. This is achieved using Siamese BERT networks that are fine-tuned on NLI and STS-B data. Indeed, in our experiments, we found sentence-BERT to perform much better than BERT. The Universal Sentence Encoder (Cer et al., 2018) is another alternative, but sentence-BERT worked better in our experiments. Finally, our task is related to semantic relatedness tasks, e.g., from the GLUE benchmark (Wang et al., 2018), such as natural language inference, or NLI task (Williams et al., 2018), recognizing textual entailment, or RTE (Bentivogli et al., 2009), paraphrase detection (Dolan and Brockett, 2005), and semantic textual similarity, or STS-B (Cer et al., 2017). However, it also differs from them, as we will see in the following section. 3 Task Definition We define the task as follows: Given a checkworthy input claim and a set of verified claims, rank those verified claims, so that the claims that can help verify the input claim, or a sub-claim in it, are ranked above any claim that is not helpful to verify the input claim. Table 1 shows some examples of input–verified claim"
2020.acl-main.332,D19-1216,1,0.872867,"e are true. Over time, the number of such initiatives grew substantially, e.g., at the time of writing, the Duke Reporters’ Lab lists 237 active fact-checking organizations plus another 92 inactive.2 While some organizations debunked just a couple of hundred claims, others such as Politifact,3 FactCheck.org,4 Snopes,5 and Full Fact6 have fact-checked thousands or even tens of thousands of claims. The value of these collections of resources has been recognized in the research community, and they have been used to train systems to perform automatic fact-checking (Popat et al., 2017; Wang, 2017; Zlatkova et al., 2019) or to detect checkworthy claims in political debates (Hassan et al., 2015; Gencheva et al., 2017; Patwari et al., 2017; Vasileva et al., 2019). There have also been datasets that combine claims from multiple fact-checking organizations (Augenstein et al., 2019), again with the aim of performing automatic fact-checking. The recent proliferation of “fake news” has triggered a number of responses, most notably the emergence of several manual fact-checking initiatives. As a result and over time, a large number of fact-checked claims have been accumulated, which increases the likelihood that a new"
2020.acl-main.50,P13-2144,0,0.07718,"Missing"
2020.acl-main.50,K19-1096,1,0.467789,"Missing"
2020.acl-main.50,W13-1106,0,0.0531734,"Missing"
2020.acl-main.50,W11-3702,0,0.145937,"Missing"
2020.acl-main.50,I13-1191,0,0.0312558,"cation is aided by the tendency of users to form so-called “echo chambers”, where they engage with like-minded users (Himelboim et al., 2013; Magdy et al., 2016a), and the tendency of users’ beliefs to be persistent over time (Borge-Holthoefer et al., 2015; Magdy et al., 2016a; Pennacchiotti and Popescu, 2011b). Studies have examined the effectiveness of different features for stance detection, including textual features such as word n-grams and hashtags, network interactions such as retweeted accounts and mentions, and profile information such as user location (Borge-Holthoefer et al., 2015; Hasan and Ng, 2013; Magdy et al., 2016a,b; Weber et al., 2013). Network interaction features were shown to yield better results compared to using textual features (Magdy et al., 2016a; Wong et al., 2013). Sridhar et al. (2015) leveraged both user interactions and textual information when modeling stance and disagreement, using a probabilistic programming system that allows models to be specified using a declarative language. 528 Trabelsi and Za¨ıane (2018) described an unsupervised stance detection method that determines the viewpoints of comments and of their authors. It analyzes online forum discussion thread"
2020.acl-main.50,D14-1083,0,0.019639,"vior to infer the ideological leanings of online media sources and popular Twitter accounts. Barber´a and Sood (2015) proposed a statistical model based on the follower relationships to media sources and Twitter personalities in order to estimate their ideological leaning. As for individual users, much recent work focused on stance detection to determine a person’s position on a topic including the deduction of political preferences (Barber´a, 2015; Barber and Rivero, 2015; Borge-Holthoefer et al., 2015; Cohen and Ruths, 2013; Colleoni et al., 2014; Conover et al., 2011b; Fowler et al., 2011; Hasan and Ng, 2014; Himelboim et al., 2013; Magdy et al., 2016a,b; Makazhanov et al., 2014; Trabelsi and Za¨ıane, 2018; Weber et al., 2013). User stance classification is aided by the tendency of users to form so-called “echo chambers”, where they engage with like-minded users (Himelboim et al., 2013; Magdy et al., 2016a), and the tendency of users’ beliefs to be persistent over time (Borge-Holthoefer et al., 2015; Magdy et al., 2016a; Pennacchiotti and Popescu, 2011b). Studies have examined the effectiveness of different features for stance detection, including textual features such as word n-grams and hashtag"
2020.acl-main.50,E17-2068,0,0.0473605,"ering and supervised classification. For the projection and clustering step, we identify clusters of core vocal users using the unsupervised method described in (Darwish et al., 2020). In this step, users are mapped to a lower dimensional space based on their similarity, and then they are clustered. After performing this unsupervised learning step, we train a supervised classifier using the two largest identified clusters in order to tag many more users. For that, we use FastText, a deep neural network text classifier, that has been shown to be effective for various text classification tasks (Joulin et al., 2017). 529 Topic Keywords Date Range Climate change #greendeal, #environment, #climate, #climatechange, #carbonfootprint, #climatehoax, #climategate, #globalwarming, #agw, #renewables #gun, #guns, #weapon, #2a, #gunviolence, #secondamendment, #shooting, #massshooting, #gunrights, #GunReformNow, #GunControl, #NRA IlhanOmarIsATrojanHorse, #IStandWithIlhan, #ilhan, #Antisemitism, #IlhanOmar, #IlhanMN, #RemoveIlhanOmar, #ByeIlhan, #RashidaTlaib, #AIPAC, #EverydayIslamophobia, #Islamophobia, #ilhan #border, #immigration, #immigrant, #borderwall, #migrant, #migrants, #illegal, #aliens midterm, election,"
2020.acl-main.50,S16-1003,0,0.128731,"Missing"
2020.acl-main.50,C14-1019,0,0.0621783,"Missing"
2020.acl-main.50,P15-1012,0,0.0173173,"persistent over time (Borge-Holthoefer et al., 2015; Magdy et al., 2016a; Pennacchiotti and Popescu, 2011b). Studies have examined the effectiveness of different features for stance detection, including textual features such as word n-grams and hashtags, network interactions such as retweeted accounts and mentions, and profile information such as user location (Borge-Holthoefer et al., 2015; Hasan and Ng, 2013; Magdy et al., 2016a,b; Weber et al., 2013). Network interaction features were shown to yield better results compared to using textual features (Magdy et al., 2016a; Wong et al., 2013). Sridhar et al. (2015) leveraged both user interactions and textual information when modeling stance and disagreement, using a probabilistic programming system that allows models to be specified using a declarative language. 528 Trabelsi and Za¨ıane (2018) described an unsupervised stance detection method that determines the viewpoints of comments and of their authors. It analyzes online forum discussion threads, and therefore assumes a certain structure of the posts. It also assumes that users tend to reply to each others’ comments when they are in disagreement, whereas we assume the opposite in this paper. Their"
2020.acl-main.50,P14-1018,0,0.0621415,"Missing"
2020.emnlp-main.404,D18-1389,1,0.897223,"s. center vs. right political leaning. Predicting the bias of individual news articles can be useful in a number of scenarios. For news media, it could be an important element of internal quality assurance as well as of internal or external monitoring for regulatory compliance. For news aggregator applications, such as Google News, it could enable balanced search, similarly to what is found on AllSides.1 For journalists, it could enable news exploration from a left/center/right angle. It could also be an important building block in a system that detects bias at the level of entire news media (Baly et al., 2018, 2019, 2020), such as the need to offer explainability, i.e., if a website is classified as left-leaning, the system should be able to pinpoint specific articles that support this decision. In this paper, we focus on predicting the bias of news articles as left-, center-, or right-leaning. Previous work has focused on doing so at the level of news media (Baly et al., 2020) or social media users (Darwish et al., 2020), but rarely at the article level (Kulkarni et al., 2018). The scarce article-level research has typically used distant supervision, assuming that all articles from a given medium"
2020.emnlp-main.404,2020.acl-main.308,1,0.770374,"y to what is found on AllSides.1 For journalists, it could enable news exploration from a left/center/right angle. It could also be an important building block in a system that detects bias at the level of entire news media (Baly et al., 2018, 2019, 2020), such as the need to offer explainability, i.e., if a website is classified as left-leaning, the system should be able to pinpoint specific articles that support this decision. In this paper, we focus on predicting the bias of news articles as left-, center-, or right-leaning. Previous work has focused on doing so at the level of news media (Baly et al., 2020) or social media users (Darwish et al., 2020), but rarely at the article level (Kulkarni et al., 2018). The scarce article-level research has typically used distant supervision, assuming that all articles from a given medium should share its overall bias, which is not always the case. Here, we revisit this assumption. 1 http://allsides.com/ 4982 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4982–4991, c November 16–20, 2020. 2020 Association for Computational Linguistics Our contributions can be summarized as follows: • We create a new dataset fo"
2020.emnlp-main.404,N19-1216,1,0.845565,"the same media, and thus models could easily learn to predict the article’s source rather than its bias. In their models, they used both the text and the URL contents of the articles. Overall, political bias has been studied at the level of news outlet (Dinkov et al., 2019; Baly et al., 2018, 2020; Zhang et al., 2019), user (Darwish et al., 2020), article (Potthast et al., 2018; Saleh et al., 2019), and sentence (Sim et al., 2013; SaezTrumper et al., 2013). In particular, Baly et al. (2018) developed a system to predict the political bias and the factuality of news media. In a followup work, Baly et al. (2019) showed that bias and factuality of reporting should be predicted jointly. A finer-grained analysis is performed in (Horne et al., 2018), where a model was trained on 10K sentences from a dataset of reviews (Pang and Lee, 2004), and used to discriminate objective versus non-objective sentences in news articles. Lin et al. (2006) presented a sentence-level classifier, where the labels were projected from the document level. 3 Furthermore, AllSides uses the annotated articles to enable its Balanced Search, which shows news coverage on a given topic from media with different political bias. In ot"
2020.emnlp-main.404,2020.semeval-1.186,1,0.822563,"Missing"
2020.emnlp-main.404,D19-1565,1,0.885835,"ree and the direction of the media bias, and on the voters’ reliance on such media (DellaVigna and Kaplan, 2007; Iyengar and Hahn, 2009; Saez-Trumper et al., 2013; Graber and Dunaway, 2017). Thus, making the general public aware, e.g., by tracking and exposing bias in the news is important for a healthy public debate given the important role media play in a democratic society. Media bias can come in many different forms, e.g., by omission, by over-reporting on a topic, by cherry-picking the facts, or by using propaganda techniques such as appealing to emotions, prejudices, fears, etc. (Da San Martino et al., 2019, 2020a,b) Bias can occur with respect to a specific topic, e.g., COVID-19, immigration, climate change, gun control, etc. (Darwish et al., 2020; Stefanov et al., 2020) It could also be more systematic, as part of a political ideology, which in the Western political system is typically defined as left vs. center vs. right political leaning. Predicting the bias of individual news articles can be useful in a number of scenarios. For news media, it could be an important element of internal quality assurance as well as of internal or external monitoring for regulatory compliance. For news aggregat"
2020.emnlp-main.404,N19-1423,0,0.0111726,"assifier to predict one of C classes (in our case, C = 3: left, center, and right). In our experiments, we use two deep learning architectures: (i) Long Short-Term Memory networks (LSTMs), which are Recurrent Neural Networks (RNNs), which use gating mechanisms to selectively pass information across time and to model long-term dependencies (Hochreiter and Schmidhuber, 1997), and (ii) Bidirectional Encoder Representations from Transformers (BERT), with a complex architecture yielding high-quality contextualized embeddings, which have been successful in several Natural Language Processing tasks (Devlin et al., 2019). 4.2 Removing Media Bias Ultimately, our goal is to develop a model that can predict the political ideology of a news article. Our dataset, along with some others, has a special property that might stand in the way of achieving this goal. Most articles published by a given source have the same ideological leaning. This might confuse the model and cause it to erroneously associate the output classes with features that characterize entire media outlets (such as detecting specific writing patterns, or stylistic markers in text). Consequently, the model would fail when applied to articles that we"
2020.emnlp-main.404,S19-2146,0,0.0167275,"ating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually annotated articles only and ignoring the articles that were labeled using distant supervision, which illustrates the dangers of relying on distant supervision. Barr´on-Cedeno et al. (2019) extensively discussed the limitations of distant supervision in a text classification task about article-level propaganda detection, in a setup that is similar to what we deal with in this paper: the learning systems may learn to model the source of the article instead of solving the task they are"
2020.emnlp-main.404,S19-2145,0,0.0605912,"s very time-consuming, requires domain expertise, and it could be also subjective, such annotations are rarely available at the article level. As a result, automating systems for political bias detection have opted for using distant supervision as an easy way to obtain large datasets, which are needed to train contemporary deep learning models. Distant supervision is a popular technique for annotating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually annotated articles only and ignoring the articles that were labeled using distant supervi"
2020.emnlp-main.404,D18-1388,0,0.385691,"ter/right angle. It could also be an important building block in a system that detects bias at the level of entire news media (Baly et al., 2018, 2019, 2020), such as the need to offer explainability, i.e., if a website is classified as left-leaning, the system should be able to pinpoint specific articles that support this decision. In this paper, we focus on predicting the bias of news articles as left-, center-, or right-leaning. Previous work has focused on doing so at the level of news media (Baly et al., 2020) or social media users (Darwish et al., 2020), but rarely at the article level (Kulkarni et al., 2018). The scarce article-level research has typically used distant supervision, assuming that all articles from a given medium should share its overall bias, which is not always the case. Here, we revisit this assumption. 1 http://allsides.com/ 4982 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 4982–4991, c November 16–20, 2020. 2020 Association for Computational Linguistics Our contributions can be summarized as follows: • We create a new dataset for predicting the political ideology of news articles. The dataset is annotated at the article level an"
2020.emnlp-main.404,W06-2915,0,0.355075,"l. They analyzed a number of representations and machine learning models, showing which ones tend to overfit more, but, unlike our work here, they fell short of recommending a practical solution. Budak et al. (2016) measured the bias at the article level using crowd-sourcing. This is risky as public awareness of media bias is limited (Elejalde et al., 2018). Moreover, the annotation setup does not scale. Finally, their dataset is not freely available, and their approach of randomly crawling articles does not ensure that topics and events are covered from different political perspectives. 4983 Lin et al. (2006) built a dataset annotated with the ideology of 594 articles related to the IsraeliPalestinian conflict published on bitterlemons. org. The articles were written by two editors and 200 guests, which minimizes the risk of modeling the author style. However, the dataset is too small to train modern deep learning approaches. Kulkarni et al. (2018) built a dataset using distant supervision and labels from AllSides. Distant supervision is fine for the purpose of training, but they also used it for testing, which can be problematic. Moreover, their training and test sets contain articles from the sa"
2020.emnlp-main.404,D17-1317,0,0.0255972,"al annotation at the article level is very time-consuming, requires domain expertise, and it could be also subjective, such annotations are rarely available at the article level. As a result, automating systems for political bias detection have opted for using distant supervision as an easy way to obtain large datasets, which are needed to train contemporary deep learning models. Distant supervision is a popular technique for annotating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually annotated articles only and ignoring the articles that"
2020.emnlp-main.404,D19-1410,0,0.0146421,"cal leaning. To a lesser extent, the content of a Wikipedia page describing a medium can also help unravel its political leaning. Therefore, we concatenated these representations to the encoded articles, at the output of the encoder and right before the SOFTMAX layer, so that both the article encoder and the classification layer that is based on the article and the external media representations are trained jointly and end-to-end. Similarly to (Baly et al., 2020), we retrieved the profiles of up to a 1,000 Twitter followers for each medium, we encoded their bios using the Sentence-BERT model (Reimers and Gurevych, 2019), and we then averaged these encodings to obtain a single representation for that medium. As for the Wikipedia representation, we automatically retrieved the content of the page describing each medium, whenever applicable. Then, we used the pre-trained base BERT model to encode this content by averaging the word representations extracted from BERT’s second-to-last layer, which is common practice, since the last layer may be biased towards the pre-training objectives of BERT. 5 Experiments and Results We evaluated both the LSTM and the BERT models, assessing the impact of (i) de-biasing and (ii"
2020.emnlp-main.404,S19-2182,1,0.867215,"Missing"
2020.emnlp-main.404,D13-1010,0,0.0267926,"es. Distant supervision is fine for the purpose of training, but they also used it for testing, which can be problematic. Moreover, their training and test sets contain articles from the same media, and thus models could easily learn to predict the article’s source rather than its bias. In their models, they used both the text and the URL contents of the articles. Overall, political bias has been studied at the level of news outlet (Dinkov et al., 2019; Baly et al., 2018, 2020; Zhang et al., 2019), user (Darwish et al., 2020), article (Potthast et al., 2018; Saleh et al., 2019), and sentence (Sim et al., 2013; SaezTrumper et al., 2013). In particular, Baly et al. (2018) developed a system to predict the political bias and the factuality of news media. In a followup work, Baly et al. (2019) showed that bias and factuality of reporting should be predicted jointly. A finer-grained analysis is performed in (Horne et al., 2018), where a model was trained on 10K sentences from a dataset of reviews (Pang and Lee, 2004), and used to discriminate objective versus non-objective sentences in news articles. Lin et al. (2006) presented a sentence-level classifier, where the labels were projected from the docum"
2020.emnlp-main.404,P04-1035,0,0.0574369,"at the level of news outlet (Dinkov et al., 2019; Baly et al., 2018, 2020; Zhang et al., 2019), user (Darwish et al., 2020), article (Potthast et al., 2018; Saleh et al., 2019), and sentence (Sim et al., 2013; SaezTrumper et al., 2013). In particular, Baly et al. (2018) developed a system to predict the political bias and the factuality of news media. In a followup work, Baly et al. (2019) showed that bias and factuality of reporting should be predicted jointly. A finer-grained analysis is performed in (Horne et al., 2018), where a model was trained on 10K sentences from a dataset of reviews (Pang and Lee, 2004), and used to discriminate objective versus non-objective sentences in news articles. Lin et al. (2006) presented a sentence-level classifier, where the labels were projected from the document level. 3 Furthermore, AllSides uses the annotated articles to enable its Balanced Search, which shows news coverage on a given topic from media with different political bias. In other words, for each trending event or topic (e.g., impeachment or coronavirus pandemic), the platform pushes news articles from all sides of the political spectrum, as shown in Figure 1. We took advantage of this and downloaded"
2020.emnlp-main.404,2020.acl-main.50,1,0.631944,"aber and Dunaway, 2017). Thus, making the general public aware, e.g., by tracking and exposing bias in the news is important for a healthy public debate given the important role media play in a democratic society. Media bias can come in many different forms, e.g., by omission, by over-reporting on a topic, by cherry-picking the facts, or by using propaganda techniques such as appealing to emotions, prejudices, fears, etc. (Da San Martino et al., 2019, 2020a,b) Bias can occur with respect to a specific topic, e.g., COVID-19, immigration, climate change, gun control, etc. (Darwish et al., 2020; Stefanov et al., 2020) It could also be more systematic, as part of a political ideology, which in the Western political system is typically defined as left vs. center vs. right political leaning. Predicting the bias of individual news articles can be useful in a number of scenarios. For news media, it could be an important element of internal quality assurance as well as of internal or external monitoring for regulatory compliance. For news aggregator applications, such as Google News, it could enable balanced search, similarly to what is found on AllSides.1 For journalists, it could enable news exploration from a"
2020.emnlp-main.404,D14-1162,0,0.0834781,"ing the seeds of the random weights initialization. For LSTM, we varied the length of the input (128–1,024 tokens), the number of layers (1–3), the size of the LSTM cell (200–400), the dropout rate (0–0.8), the learning rate (1e−3 to 1e−5), the gradient clipping value (0–5), and the batch size (8–256). The best results were obtained with a 512-token input, a 2-layer LSTM of size 256, a dropout rate of 0.7, a learning rate of 1e−3, gradient clipping at 0.5, and a batch size of 32. This model has around 1.1M trainable parameters, and was trained with 300-dimensional GloVe input word embeddings (Pennington et al., 2014). 4987 For BERT, we varied the length of the input, the learning rate, and the gradient clipping value. The best results were obtained using a 512-token input, a learning rate of 2e−5, and gradient clipping at 1. This model has 110M trainable parameters. We trained our models on 4 Titan X Pascal GPUs, and the runtime for each epoch was 25 seconds for the LSTM-based models and 22 minutes for the BERT-based models. For each experiment, the model was trained only once with fixed seeds used to initialize the models’ weights. For the Adversarial Adaptation (AA), we have an additional hyper-paramete"
2020.emnlp-main.404,D19-3038,1,0.890046,"Missing"
2020.emnlp-main.404,P18-1022,0,0.126909,"lsides.com/ 4 http://mediabiasfactcheck.com 3 As manual annotation at the article level is very time-consuming, requires domain expertise, and it could be also subjective, such annotations are rarely available at the article level. As a result, automating systems for political bias detection have opted for using distant supervision as an easy way to obtain large datasets, which are needed to train contemporary deep learning models. Distant supervision is a popular technique for annotating datasets for related text classification tasks, such as detecting hyper-partisanship (Horne et al., 2018; Potthast et al., 2018) and propaganda/satire/hoaxes (Rashkin et al., 2017). For example, Kiesel et al. (2019) created a large corpus for detecting hyper-partisanship (i.e., articles with extreme left/right bias) consisting of 754,000 articles, annotated via distant supervision, and additional 1,273 manually annotated articles, part of which was used as a test set for the SemEval-2019 task 4 on Hyper-partisan News Detection. The winning system was an ensemble of character-level CNNs (Jiang et al., 2019). Interestingly, all topperforming systems in the task achieved their best results when training on the manually an"
2020.emnlp-main.438,2020.acl-main.421,0,0.512652,"s from a different domain (Sun et al., 2019; Khashabi et al., 2020) and carefully selected background knowledge (Banerjee et al., 2019; Ni et al., 2019) could improve such models further. The success of large-scale pre-trained models and the development of their multilingual versions (Devlin et al., 2019; Conneau et al., 2020) gives hopes for supposedly better performance in multilingual question answering. Therefore, several new datasets have been released for multilingual reading comprehension and open-domain question answering in the Wikipedia domain (Liu et al., 2019a; Lewis et al., 2020; Artetxe et al., 2020; Clark et al., 2020). 5427 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5427–5444, c November 16–20, 2020. 2020 Association for Computational Linguistics Here, we present Eχαµs, a new dataset and benchmark for multilingual and cross-lingual evaluation of models and methods for answering diverse school science questions (see Figure 1). Our contributions are as follows: • We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. • We collect a new challenging dataset Eχαµs from multilingual high school"
2020.emnlp-main.438,P19-1615,0,0.147793,"the field of Natural Language Processing (NLP) in general, due to the invention of the Transformer (Vaswani et al., 2017), and the subsequent rise of large-scale pre-trained models (Peters et al., 2018; Radford et al., 2018, 2019; Devlin et al., 2019; Lan et al., 2020; Yang et al., 2019; Liu et al., 2019c; Raffel et al., 2020). Nowadays, fine-tuning such models on task-specific data has become an essential element of any topscoring QA system. Yet, for science QA, training on datasets from a different domain (Sun et al., 2019; Khashabi et al., 2020) and carefully selected background knowledge (Banerjee et al., 2019; Ni et al., 2019) could improve such models further. The success of large-scale pre-trained models and the development of their multilingual versions (Devlin et al., 2019; Conneau et al., 2020) gives hopes for supposedly better performance in multilingual question answering. Therefore, several new datasets have been released for multilingual reading comprehension and open-domain question answering in the Wikipedia domain (Liu et al., 2019a; Lewis et al., 2020; Artetxe et al., 2020; Clark et al., 2020). 5427 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing"
2020.emnlp-main.438,D18-2018,0,0.027885,"d we test on a subset of Ltgt .6 The last three columns of Table 3 show the number of examples used for training and validation with the corresponding language. 3.5 Reasoning and Knowledge Types In order to give a better understanding of the reasoning, and the knowledge types in Eχαµs, we sampled and annotated 250 questions, all of which are from the multilingual Dev. For each question, we provided English translations as not all annotators were native speakers of the questions’ language. We followed the procedure and re-used the annotation types presented in earlier work (Clark et al., 2018; Boratko et al., 2018). However, as they were designed mainly for Nature Science questions, we extended them with two new annotation types: “Domain Facts and Knowledge” and “Negation” (see Appendix C for examples). 5 Sometimes, grouping parallel questions in the same split slightly violates the splitting ratios. 6 To ensure that the cross-lingual evaluation is comparable to the multilingual one, we use the same subset of questions from language Ltgt that are used in TestM ul 5431 Figure 4: Relative size of the Eχαµs knowledge types. The relative sizes of the knowledge and the reasoning types are shown in Figures 3"
2020.emnlp-main.438,2020.lrec-1.677,0,0.0767799,"Missing"
2020.emnlp-main.438,2020.tacl-1.30,0,0.315539,"ain (Sun et al., 2019; Khashabi et al., 2020) and carefully selected background knowledge (Banerjee et al., 2019; Ni et al., 2019) could improve such models further. The success of large-scale pre-trained models and the development of their multilingual versions (Devlin et al., 2019; Conneau et al., 2020) gives hopes for supposedly better performance in multilingual question answering. Therefore, several new datasets have been released for multilingual reading comprehension and open-domain question answering in the Wikipedia domain (Liu et al., 2019a; Lewis et al., 2020; Artetxe et al., 2020; Clark et al., 2020). 5427 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 5427–5444, c November 16–20, 2020. 2020 Association for Computational Linguistics Here, we present Eχαµs, a new dataset and benchmark for multilingual and cross-lingual evaluation of models and methods for answering diverse school science questions (see Figure 1). Our contributions are as follows: • We advance the task of science Question Answering (QA) with multilingual and crosslingual evaluations. • We collect a new challenging dataset Eχαµs from multilingual high school examinations, which"
2020.emnlp-main.438,L18-1440,0,0.141514,"l. (2019) worked on reasoning about qualitative relationships, and declarative texts, among others. Unlike these English-only datasets, Eχαµs offers questions in 16 languages. Moreover, it contains questions about multiple subjects, which are presumably harder as they were extracted mostly from matriculation examinations (8-12th grade). Finally, Eχαµs contains over 24,000 questions, which is more than three times as many as in ARC. Multilingual and Cross-lingual QA Recently, several QA datasets have been created that cover languages other than English, but still focusing on one such language. Gupta et al. (2018) proposed a parallel QA task for English and Hindi, Liu et al. (2019b) collected a bilingual cloze-style dataset in Chinese and English. Jing et al. (2019) crowdsourced parallel paragraphs from novels in Chinese and English. A few datasets investigated multiple-choice school QA (Hardalov et al., 2019; Van Nguyena et al., 2020), albeit in a limited domain, and for lower school grades (1st-5th). Other efforts focused on building bi-lingual datasets that are similar in spirit to SQuAD (Rajpurkar et al., 2016) – extractive reading comprehension over open-domain articles. Such datasets are collecte"
2020.emnlp-main.438,2020.acl-main.740,0,0.0521891,"Missing"
2020.emnlp-main.438,R19-1053,1,0.761797,"riculation examinations (8-12th grade). Finally, Eχαµs contains over 24,000 questions, which is more than three times as many as in ARC. Multilingual and Cross-lingual QA Recently, several QA datasets have been created that cover languages other than English, but still focusing on one such language. Gupta et al. (2018) proposed a parallel QA task for English and Hindi, Liu et al. (2019b) collected a bilingual cloze-style dataset in Chinese and English. Jing et al. (2019) crowdsourced parallel paragraphs from novels in Chinese and English. A few datasets investigated multiple-choice school QA (Hardalov et al., 2019; Van Nguyena et al., 2020), albeit in a limited domain, and for lower school grades (1st-5th). Other efforts focused on building bi-lingual datasets that are similar in spirit to SQuAD (Rajpurkar et al., 2016) – extractive reading comprehension over open-domain articles. Such datasets are collected by crowdsourcing questions, following a procedure similar to (Rajpurkar et al., 2016), in Russian (Efimov et al., 2020), Korean (Lim et al., 2019), French (d’Hoffschmidt et al., 2020), or by translating existing English QA pairs to Spanish (Carrino et al., 2020). Recently, some multilingual dataset"
2020.emnlp-main.438,P19-1227,0,0.327591,"(Clark et al., 2016). This inevitably made research in schoollevel science Question Answering (QA) hard for languages other than English due to the scarceness of resources (Clark et al., 2014; Khot et al., 2017, 2018; Bhakthavatsalam et al., 2020). There has been a recent mini-revolution in QA, as well as in the field of Natural Language Processing (NLP) in general, due to the invention of the Transformer (Vaswani et al., 2017), and the subsequent rise of large-scale pre-trained models (Peters et al., 2018; Radford et al., 2018, 2019; Devlin et al., 2019; Lan et al., 2020; Yang et al., 2019; Liu et al., 2019c; Raffel et al., 2020). Nowadays, fine-tuning such models on task-specific data has become an essential element of any topscoring QA system. Yet, for science QA, training on datasets from a different domain (Sun et al., 2019; Khashabi et al., 2020) and carefully selected background knowledge (Banerjee et al., 2019; Ni et al., 2019) could improve such models further. The success of large-scale pre-trained models and the development of their multilingual versions (Devlin et al., 2019; Conneau et al., 2020) gives hopes for supposedly better performance in multilingual question answering. Therefo"
2020.emnlp-main.438,D19-1169,0,0.402334,"(Clark et al., 2016). This inevitably made research in schoollevel science Question Answering (QA) hard for languages other than English due to the scarceness of resources (Clark et al., 2014; Khot et al., 2017, 2018; Bhakthavatsalam et al., 2020). There has been a recent mini-revolution in QA, as well as in the field of Natural Language Processing (NLP) in general, due to the invention of the Transformer (Vaswani et al., 2017), and the subsequent rise of large-scale pre-trained models (Peters et al., 2018; Radford et al., 2018, 2019; Devlin et al., 2019; Lan et al., 2020; Yang et al., 2019; Liu et al., 2019c; Raffel et al., 2020). Nowadays, fine-tuning such models on task-specific data has become an essential element of any topscoring QA system. Yet, for science QA, training on datasets from a different domain (Sun et al., 2019; Khashabi et al., 2020) and carefully selected background knowledge (Banerjee et al., 2019; Ni et al., 2019) could improve such models further. The success of large-scale pre-trained models and the development of their multilingual versions (Devlin et al., 2019; Conneau et al., 2020) gives hopes for supposedly better performance in multilingual question answering. Therefo"
2020.emnlp-main.438,W17-4413,0,0.0278915,"8th grade examinations in the Natural Science domain. As in Eχαµs, the questions in ARC are created by experts, albeit our dataset covers a wide variety of high school (8th-12th grade) subjects including but not limited to, Natural Sciences, Social Sciences, Applied Studies, Arts, Religion, etc. (see Section 3.2 for details). We provide definitions of the less known subjects in Eχαµs in Appendix B.1. 1 The Eχαµs dataset and code are publicly available at http://github.com/mhardalov/exams-qa The early versions of ARC (Clark, 2015; Schoenick et al., 2017) inspired several crowdsourced datasets: Welbl et al. (2017) proposed a scalable approach for crowdsourcing science questions given a set of basic supporting science facts. Dalvi et al. (2019) focused on specific phenomena including understanding science procedural texts, Mihaylov et al. (2018) and Khot et al. (2020) studied multi-step reasoning, given a set of science facts and commonsense knowledge, Tafjord et al. (2019), and Mitra et al. (2019) worked on reasoning about qualitative relationships, and declarative texts, among others. Unlike these English-only datasets, Eχαµs offers questions in 16 languages. Moreover, it contains questions about mult"
2020.emnlp-main.438,Q18-1021,0,0.0269065,"41.2 35.6 36.3 37.1 34.2 33.8 32.3 35.8 it lt mk fr tr vi All sr ar bg de es pt sq hr hu Figure 5: Fine-grained evaluation by language and school subjects. We can see that the Natural Science questions are the most challenging ones, which is mostly due to Chemistry and Physics. Those questions require very complex reasoning and knowledge such as understanding physical models, processes and causes, comparisons, algebraic skills and multihop reasoning (see Section 3.5). These skills are currently beyond the capabilities of the current QA models, and pose interesting challenges for future work (Welbl et al., 2018; Yang et al., 2018; Saxton et al., 2019; Lample and Charton, 2020). Informatics is another challenging subject, as it requires understanding programming code and positional numerical systems among others. Moreover, we need a better knowledge context for a given question–choice pair (the last row in Table 4). Knowing that the context retrieved from the noisy Wikipedia corpus is relevant for answering Eχαµs questions, suggests that we need a better multilingual science corpus, similar to Clark et al. (2018); Pan et al. (2019); Bhakthavatsalam et al. (2020). We further need better multilingual k"
2020.emnlp-main.640,D14-1179,0,0.0232119,"Missing"
2020.emnlp-main.640,P19-1285,0,0.0285349,"Table 4 shows some Frobenius norm losses from these experiments. We can see that VVMA’s expressiveness is comparable to standard low-rank approximation; note, however, that standard low-rank approximation does not yield the inference speedups of VVMA. 4.5 Extension to Language Modelling Even though the main focus of this paper is the contribution of VVMA to neural machine translation, we also demonstrate that VVMA is compatible to state-of-the-art language modelling architectures. For that purpose, we perform an experiment on WikiText-103 (Merity et al., 2017) using the Transformer-XL model (Dai et al., 2019). VVMA Low-rank Optimal 3.0 6.1 12.2 2.9 5.9 11.9 2.9 5.8 11.7 Table 4: VVMA’s closeness of fit to a target matrix is comparable to that of (i) standard low-rank approximation and (ii) optimal approximation, but it is orders of magnitude faster at inference time. In this experiment, we directly integrate VVMA into the Transformer-XL architecture, keeping all hyper-parameter values as in the original Transformer-XL paper (Dai et al., 2019), except for reducing the batch size to 30, in order to fit the optimization on two GPUs. We chose to replace the weights of the attention mechanism with VVMA"
2020.emnlp-main.640,N18-1073,0,0.0198819,"re work. 2 Background Here, we look at efficient models from the software and the hardware side, and we discuss the advantages of merging them in a co-design manner. We further discuss the importance of wall-clock speed versus floating point operations and why from this perspective our weight sharing matrices will decrease inference rather than training time. 2.1 Efficient Models from the Software Side for Training and Inference Efficient model architectures can decrease the complexity of neural networks. Some techniques to achieve this are described in (Chen et al., 2015; Zhang et al., 2018; Gao et al., 2018). Zhang et al. (2018) added a new type of layer, a channel shuffle layer, to neural networks that use group convolution. By shuffling the data between layers, they reduced the number of parameters in the other layers while retaining similar accuracy. Gao et al. (2018) used a technique similar to group convolution, but applied it to recurrent neural networks. They used shuffling operations with a group recurrent neural network and showed improvements for NMT and text summarization. Chen et al. (2015) compressed a weight matrix into a learned vector of weights. They used a hash function to map e"
2020.emnlp-main.640,P17-4012,0,0.0383204,"loads a single piece (for broadcasting) and adds diagonals for element-wise multiplication, which is faster. Introduction Artificial neural networks have become increasingly popular over the last decade as they excel in tasks such as object detection and speech recognition (LeCun et al., 2015), which are becoming more commonplace with the use of self-driving cars and virtual assistants. The rapid development of deep neural networks has also made them the dominant approach for natural language processing (NLP) applications, ranging from neural machine translation (NMT) (Bahdanau et al., 2015; Klein et al., 2017; Wu et al., 2016) and text summarization (Rush et al., 2015; Nallapati et al., 2016; Liu et al., 2018) to virtual assistants such as Apple Siri, Amazon Alexa, and Google Home. Unfortunately, neural networks are slow for training, inference and use due to their vast computational complexity. Several approaches have been proposed to address these issues including (a) quantization and pruning, (b) efficient models with less computational demand, and (c) specialized hardware accelerators (Sze et al., 2017). While direction (a) has been well-studied (LeCun et al., 1990; Han et al., 2016b,a; Guo, 2"
2020.emnlp-main.640,P19-1032,0,0.0216807,"ral network and showed improvements for NMT and text summarization. Chen et al. (2015) compressed a weight matrix into a learned vector of weights. They used a hash function to map entries in the weight matrix to elements in the vector. As a result, many matrix entries share a single weight in the vector. As Transformers are becoming the standard building block for NLP tasks, there is a growing effort to make them efficient, since their inference time scales as O(N 2 ), where N is the number of input tokens. Child et al. (2019) pro√ posed Sparse Transformers with O(N N ) complexity. Likewise, Sukhbaatar et al. (2019) developed Adaptive Attention Span and Kitaev et al. (2020) proposed Reformer using locality-sensitive hashing, and achieved O(N log N ) complexity. See (Ganesh et al., 2020) for a broader overview. In a similar fashion, our VVMA is an efficient model because it reduces the computational complexity at inference time without much decrease in performance. However, unlike the above models, VVMAs focus on the low levels of execution: the VVMA is an architecture that speeds up matrix multiplications. Thus, it is an efficient model that relates to hardware accelerators directly and it is universal,"
2020.emnlp-main.640,N18-3014,0,0.0200528,"et al., 2016) and text summarization (Rush et al., 2015; Nallapati et al., 2016; Liu et al., 2018) to virtual assistants such as Apple Siri, Amazon Alexa, and Google Home. Unfortunately, neural networks are slow for training, inference and use due to their vast computational complexity. Several approaches have been proposed to address these issues including (a) quantization and pruning, (b) efficient models with less computational demand, and (c) specialized hardware accelerators (Sze et al., 2017). While direction (a) has been well-studied (LeCun et al., 1990; Han et al., 2016b,a; Guo, 2018; Quinn and Ballesteros, 2018), and can be considered complementary to (b,c), optimizing the combination of (b) and (c) has not been considered, to the best of our knowledge. Thus, here we propose a novel vector-vectormatrix architecture (VVMA) that compresses neural networks, while optimizing for hardware performance at inference time. Therefore, we optimize (b) and (c), without conflicting with (a), i.e., using quantization and pruning can potentially further boost the efficiency of our framework. Figure 1 illustrates this VVMA in contrast to a traditional vector-matrix architecture. 7975 Proceedings of the 2020 Conferen"
2020.emnlp-main.640,D15-1044,0,0.0434411,"or element-wise multiplication, which is faster. Introduction Artificial neural networks have become increasingly popular over the last decade as they excel in tasks such as object detection and speech recognition (LeCun et al., 2015), which are becoming more commonplace with the use of self-driving cars and virtual assistants. The rapid development of deep neural networks has also made them the dominant approach for natural language processing (NLP) applications, ranging from neural machine translation (NMT) (Bahdanau et al., 2015; Klein et al., 2017; Wu et al., 2016) and text summarization (Rush et al., 2015; Nallapati et al., 2016; Liu et al., 2018) to virtual assistants such as Apple Siri, Amazon Alexa, and Google Home. Unfortunately, neural networks are slow for training, inference and use due to their vast computational complexity. Several approaches have been proposed to address these issues including (a) quantization and pruning, (b) efficient models with less computational demand, and (c) specialized hardware accelerators (Sze et al., 2017). While direction (a) has been well-studied (LeCun et al., 1990; Han et al., 2016b,a; Guo, 2018; Quinn and Ballesteros, 2018), and can be considered com"
2020.emnlp-tutorials.2,2020.acl-main.656,0,0.0202034,"Missing"
2020.emnlp-tutorials.2,D19-1475,0,0.0204818,"Missing"
2020.emnlp-tutorials.2,K19-1096,1,0.894765,"Missing"
2020.emnlp-tutorials.2,D18-1389,1,0.870978,"Missing"
2020.emnlp-tutorials.2,2020.acl-main.308,1,0.845395,"Missing"
2020.emnlp-tutorials.2,N19-1216,1,0.89277,"Missing"
2020.emnlp-tutorials.2,N18-2004,1,0.850235,"Missing"
2020.emnlp-tutorials.2,N18-1036,0,0.0537936,"Missing"
2020.emnlp-tutorials.2,N18-5006,1,0.898237,"Missing"
2020.emnlp-tutorials.2,2020.acl-main.761,0,0.0213562,"Missing"
2020.emnlp-tutorials.2,N18-1070,1,0.872753,"Missing"
2020.emnlp-tutorials.2,D19-1452,1,0.858514,"Missing"
2020.emnlp-tutorials.2,P13-1162,0,0.0515661,"Missing"
2020.emnlp-tutorials.2,C18-1287,0,0.0434379,"Missing"
2020.emnlp-tutorials.2,2020.acl-main.332,1,0.865467,"Missing"
2020.emnlp-tutorials.2,P18-1022,0,0.0607386,"Missing"
2020.emnlp-tutorials.2,D17-1317,0,0.0494796,"Missing"
2020.emnlp-tutorials.2,C18-1283,0,0.0116136,"lopments and Future Challenges (ii) Datasets, e.g., FakeNewsNet, NELA-GT2018 (i) Deep fakes: images, voice, video, text (iii) The language of fake news (ii) Text generation: GPT-2, GPT-3, GROVER (iv) Tasks and approaches (iii) Defending against neural fake news 2.6 (iv) Fighting the COVID-19 Infodemic Stance Detection (i) Task definitions and examples 3 (ii) Datasets We recommend several surveys. Shu et al. (2017), which adopted a data mining perspective on “fake news” and focused on social media. Another survey (Zubiaga et al., 2018a) focused on rumor detection in social media. The survey by Thorne and Vlachos (2018) took a fact-checking perspective on “fake news” and related problems. The survey by Li et al. (2016) covering truth discovery in general. Lazer et al. (2018) offers a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focuses on the process of proliferation of true and false news online. Other recent surveys focus on stance detection (K¨uc¸u¨ k and Can, 2020), on propaganda (Da San Martino et al., 2020b), on social bots (Ferrara et al., 2016), on false information (Zannettou et al., 2019b) and on bias on the Web (Baeza-Yates, 2018). See also the list o"
2020.emnlp-tutorials.2,N18-1074,0,0.0352329,"Missing"
2020.emnlp-tutorials.2,D19-1292,0,0.0237915,"Missing"
2020.emnlp-tutorials.2,W18-5501,0,0.0512848,"Missing"
2020.emnlp-tutorials.2,2020.acl-main.50,1,0.824809,"Missing"
2020.emnlp-tutorials.2,D19-6601,0,0.0223228,"Missing"
2020.emnlp-tutorials.2,R19-1141,1,0.852922,"Missing"
2020.emnlp-tutorials.2,W14-2508,0,0.074535,"Missing"
2020.emnlp-tutorials.2,W17-4214,0,0.0261117,"Missing"
2020.emnlp-tutorials.2,P17-2067,0,0.0574321,"Missing"
2020.emnlp-tutorials.2,D19-3038,1,0.893118,"Missing"
2020.emnlp-tutorials.2,2020.acl-main.97,0,0.0197198,"Missing"
2020.emnlp-tutorials.2,2020.acl-main.549,0,0.0205041,"Missing"
2020.emnlp-tutorials.2,D19-1216,1,0.86393,"Missing"
2020.semeval-1.186,2020.semeval-1.241,0,0.183778,"ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWallE(SI:8) (Blaschke et al., 2020) used features modeling sentiment, rhetorical structure, and POS tags, while team UTMN(SI:23) injected the sentiment intensity from VADER and it wa"
2020.semeval-1.186,I17-1078,0,0.0125125,"pective: a fine-grained analysis of the text that complements existing approaches and can, in principle, be combined with them. Propaganda in text (and in other channels) is conveyed through the use of diverse propaganda techniques (Miller, 1939), which range from leveraging on the emotions of the audience —such as using loaded language or appeals to fear— to using logical fallacies —such as straw men (misrepresenting someone’s opinion), hidden ad-hominem fallacies, and red herring (presenting irrelevant data). Some of these techniques have been studied in tasks such as hate speech detection (Gao et al., 2017) and computational argumentation (Habernal et al., 2018). Figure 1 shows the fine-grained propaganda identification pipeline, including the two targeted subtasks. Our goal is to facilitate the development of models capable of spotting text fragments where propaganda techniques are used. The task featured the following subtasks: Subtask SI (Span Identification): Given a plain-text document, identify those specific fragments that contain at least one propaganda technique. (This is a binary sequence tagging task.) Subtask TC (Technique Classification): Given a propagandistic text snippet and its"
2020.semeval-1.186,2020.semeval-1.187,0,0.0431519,"Missing"
2020.semeval-1.186,2020.semeval-1.238,0,0.192615,"8. CyberWallE     10. Duth   11. DiSaster  Ë    13. SocCogCom  Ë Ë  Ë 14. TTUI     15. JUST   16. NLFIIT Ë  Ë Ë Ë 17. UMSIForeseer    18. BPGC  Ë Ë  Ë   19. UPB   20. syrapropa      21. WMD  Ë Ë Ë Ë   22. YNUHPCC   Ë 24. DoNotDistribute     25. NTUAAILS   26. UAIC1860 ËË Ë    27. UNTLing     1. (Jurkiewicz et al., 2020) 2. (Chernyavskiy et al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4: Overview of the approaches to the technique classification subtask. =part of the official submission; Ë=considered in internal experiments. The references to t"
2020.semeval-1.186,2020.semeval-1.230,0,0.187088,"n distributional semantics. Finally, team WMD(SI:33) (Daval-Frerot and Yannick, 2020) applied multiple strategies to augment the data such as back translation, synonym replacement and TF.IDF replacement (replace unimportant words, based on TF.IDF score, by other unimportant words). Closing the top-three submissions, Team aschern(SI:3) (Chernyavskiy et al., 2020) fine-tuned an ensemble of two differently intialized RoBERTa models, each with an attached CRF for sequence labeling and span character boundary post-processing. There have been several other promising strategies. Team LTIatCMU(SI:4) (Khosla et al., 2020) used a multi-granular BERT BiLSTM model with additional syntactic and semantic features at the word, sentence and document level, including PoS, named entities, sentiment, and subjectivity. It was trained jointly for token and sentence propaganda classification, with class balancing. They further fine-tuned BERT on persuasive language using 10,000 articles from propaganda websites, which turned out to be important. Team PsuedoProp(SI:14) (Chauhan and Diddee, 2020) built a preliminary sentence-level classifier using an ensemble of XLNet and RoBERTa, before it fine-tuned a BERT-based CRF sequen"
2020.semeval-1.186,2020.semeval-1.240,0,0.194543,"ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the"
2020.semeval-1.186,2020.semeval-1.196,0,0.206471,"CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal"
2020.semeval-1.186,2020.semeval-1.232,0,0.203255,". 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification"
2020.semeval-1.186,J15-3003,0,0.0243002,"Missing"
2020.semeval-1.186,S19-2149,1,0.79131,"of 24.88 only (and only 10.43 in the datathon). This is why, here we decided to split the task into subtasks in order to allow researchers to focus on one subtask at a time. Moreover, we merged some of the original 18 propaganda techniques to reduce data sparseness issues. Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on determining the veracity of rumors (Derczynski et al., 2017; Gorrell et al., 2019) and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019). Also, the CLEF 2018–2020 CheckThat! labs’ shared tasks (Nakov et al., 2018; Elsayed et al., 2019a; Elsayed et al., 2019b; Barr´on-Cede˜no et al., 2020a; Barr´on-Cede˜no et al., 2020b), which featured tasks on automatic identification (Atanasova et al., 2018; Atanasova et al., 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019; Hasanain et al., 2020; Shaar et al., 2020) of claims in political debates and in social media. 8 You can also try the Prta system (Da San Martino et al., 2020b) online at: http://www.tanbih.org/prta http://www.datasciencesociety.net/hack-news-d"
2020.semeval-1.186,2020.semeval-1.245,0,0.0349937,"Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWallE(SI:8) (Blaschke et al., 2020) used features modeling sentiment, rhetorical s"
2020.semeval-1.186,2020.semeval-1.228,0,0.577546,"bindex on the right of each team represents their official rank in the subtasks. Appendix A includes brief descriptions of all systems. 4.1 Span Identification Subtask Table 3 shows a quick overview of the systems that took part in the SI subtask.7 All systems in the top-10 positions relied on some kind of Transformer, in combination with an LSTM or a CRF. In most cases, the Transformer-generated representations were complemented by engineered features, such as named entities and the presence of sentiment and subjectivity clues. Team Hitachi(SI:1) achieved the top performance in this subtask (Morio et al., 2020). They used a BIO encoding, which is typical for related segmentation and labeling tasks (e.g., named entity recognition). They relied upon a complex heterogeneous multi-layer neural network, trained end-to-end. The network uses pre-trained language models, which generate a representation for each input token. They further added part-of-speech (PoS) and named entity (NE) embeddings. As a result, there are three representations for each token, which are concatenated and used as an input to bi-LSTMs. At this moment, the network branches, as it is trained with three objectives: (i) the main BIO t"
2020.semeval-1.186,2020.semeval-1.244,0,0.326496,"S UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that"
2020.semeval-1.186,2020.semeval-1.226,0,0.607064,"syntactic and semantic features at the word, sentence and document level, including PoS, named entities, sentiment, and subjectivity. It was trained jointly for token and sentence propaganda classification, with class balancing. They further fine-tuned BERT on persuasive language using 10,000 articles from propaganda websites, which turned out to be important. Team PsuedoProp(SI:14) (Chauhan and Diddee, 2020) built a preliminary sentence-level classifier using an ensemble of XLNet and RoBERTa, before it fine-tuned a BERT-based CRF sequence tagger to identify the exact spans. Team BPGC(SI:21) (Patil et al., 2020) went beyond these multigranularity approaches. Information both at the article and at the sentence level was considered when classifying each word as propaganda or not, by computing and concatenating vectorial representations for the three inputs. 7 Tables 3 and 4 only include the systems for which a description paper was submitted. 1384 Transformers Learning Models Representations Misc BERT RoBERTa XLNet XLM XLM RoBERTa ALBERT GPT-2 SpanBERT LaserTagger LSTM CNN SVM Na¨ıve Bayes Boosting Log regressor Random forest CRF Embeddings ELMo NEs Words/n-grams Chars/n-grams PoS Trees Sentiment Subje"
2020.semeval-1.186,2020.semeval-1.243,0,0.229367,"al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4: Overview of the approaches to the technique classification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. Team aschern(TC:2) (Chernyavskiy et al., 2020) was the second best, and it based its success on a RoBERTa ensemble with several interesting techniques. They treated the task as one of sequence classification, using an average embedding of the surrounding tokens and the length of the span as contextual features. They further incorporated knowledge from the spa"
2020.semeval-1.186,C10-2115,1,0.593446,"Missing"
2020.semeval-1.186,2020.semeval-1.236,0,0.0349132,"Team 1. ApplicaAI    2. aschern     3. Hitachi           4. Solomon  Ë    5. newsSweeper Ë  ËË Ë 6. NoPropaganda   7. Inno Ë Ë ËË Ë Ë 8. CyberWallE     10. Duth   11. DiSaster  Ë    13. SocCogCom  Ë Ë  Ë 14. TTUI     15. JUST   16. NLFIIT Ë  Ë Ë Ë 17. UMSIForeseer    18. BPGC  Ë Ë  Ë   19. UPB   20. syrapropa      21. WMD  Ë Ë Ë Ë   22. YNUHPCC   Ë 24. DoNotDistribute     25. NTUAAILS   26. UAIC1860 ËË Ë    27. UNTLing     1. (Jurkiewicz et al., 2020) 2. (Chernyavskiy et al., 2020) 3. (Morio et al., 2020) 4. (Raj et al., 2020) 5. (Singh et al., 2020) 6. (Dimov et al., 2020) 7. (Grigorev and Ivanov, 2020) 8. (Blaschke et al., 2020) 10. (Bairaktaris et al., 2020) 11. (Kaas et al., 2020) 13. (Krishnamurthy et al., 2020) 14. (Kim and Bethard, 2020) 15. (Altiti et al., 2020) 16. (Martinkovic et al., 2020) 17. (Jiang et al., 2020) 18. (Patil et al., 2020) 19. (Paraschiv and Cercel, 2020) 20. (Li and Xiao, 2020) 21. (Daval-Frerot and Yannick, 2020) 22. (Dao et al., 2020) 24. (Kranzlein et al., 2020) 25. (Arsenos and Siolas, 2020) 26. (Ermurachi and Gifu, 2020) 27. (Petee and Palmer, 2020) 29. (Verma et al., 2020) Table 4:"
2020.semeval-1.186,D17-1317,0,0.400865,"ching very large audiences (Muller, 2018; Tard´aguila et al., 2018; Glowacki et al., 2018). Propaganda is most successful when it goes unnoticed by the reader, and it often takes some training for people to be able to spot it. The task is way more difficult for inexperienced users, and the volume of text produced on a daily basis makes it difficult for experts to cope with it manually. With the recent interest in “fake news”, the detection of propaganda or highly biased texts has emerged as an active research area. However, most previous work has performed analysis at the document level only (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a) or has analyzed the general patterns of online propaganda (Garimella et al., 2015; Chatfield et al., 2015). SemEval-2020 Task 11 offers a different perspective: a fine-grained analysis of the text that complements existing approaches and can, in principle, be combined with them. Propaganda in text (and in other channels) is conveyed through the use of diverse propaganda techniques (Miller, 1939), which range from leveraging on the emotions of the audience —such as using loaded language or appeals to fear— to using logical fallacies —such as straw men (misrepres"
2020.semeval-1.186,2020.semeval-1.231,0,0.293475,"1. 2. 3. 4. 5. 7. 8. 9. 11. 13. 14. 16. 17. 20. 21. 22. 23. 25. 26. 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et"
2020.semeval-1.186,2020.semeval-1.247,0,0.0380213,"processing Rank. Team 1. 2. 3. 4. 5. 7. 8. 9. 11. 13. 14. 16. 17. 20. 21. 22. 23. 25. 26. 27. 28. 31. 33. – Hitachi ApplicaAI aschern LTIatCMU UPB NoPropaganda CyberWallE Transformers YNUtaoxin newsSweeper PsuedoProp YNUHPCC NLFIIT TTUI BPGC DoNotDistribute UTMN syrapropa SkoltechNLP NTUAAILS UAIC1860 3218IR WMD UNTLing 1. 2. 3. 4. 5. 7. 8. 9.            Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 202"
2020.semeval-1.186,C18-1283,0,0.020638,"classification F1 performance on the development set. The systems are ordered based on the final ranking on the test set (cf. Table 6), whereas the ranking is the one on the development set. Columns 1 to 14 show the performance on each class (cf. Section 2). The best score for each class is bold. Rnk 1 2 8 16 5 10 11 13 7 6 23 9 26 28 17 24 20 19 18 21 3 33 29 25 30 32 34 37 36 41 43 27 4 12 14 15 22 31 35 38 39 40 42 44 45 46 47 6 Related Work Propaganda is particularly visible in the context of “fake news” on social media, which have attracted a lot of research recently (Shu et al., 2017). Thorne and Vlachos (2018) surveyed fact-checking approaches to fake news and related problems, and Li et al. (2016) focused on truth discovery in general. Two recent articles in Science offered a general discussion on the science of “fake news” (Lazer et al., 2018) and the process of proliferation of true and false news online (Vosoughi et al., 2018). We are particularly interested here in how different forms of propaganda are manifested in text. So far, the computational identification of propaganda has been tackled mostly at the article level. Rashkin et al. (2017) created a corpus, where news articles are labeled a"
2020.semeval-1.186,N18-1074,0,0.0272213,"d ELMo, or context-independent representations based on lexical, sentiment, readability, and TF-IDF features. As in the task at hand, ensembles were also popular. Still, the most successful submissions achieved an F1 -score of 24.88 only (and only 10.43 in the datathon). This is why, here we decided to split the task into subtasks in order to allow researchers to focus on one subtask at a time. Moreover, we merged some of the original 18 propaganda techniques to reduce data sparseness issues. Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on determining the veracity of rumors (Derczynski et al., 2017; Gorrell et al., 2019) and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019). Also, the CLEF 2018–2020 CheckThat! labs’ shared tasks (Nakov et al., 2018; Elsayed et al., 2019a; Elsayed et al., 2019b; Barr´on-Cede˜no et al., 2020a; Barr´on-Cede˜no et al., 2020b), which featured tasks on automatic identification (Atanasova et al., 2018; Atanasova et al., 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019; Hasanain et al., 2"
2020.semeval-1.186,2020.semeval-1.239,0,0.211588,"Ë  Ë   Ë Ë   Ë                            Ë  Ë          Ë  Ë  Ë   Ë ËË  Ë Ë     Ë Ë Ë   Ë Ë Ë        ËË  Ë ËË  Ë 11. 13. 14. 16. 17. 20. 21. 22. Ë       Ë  Ë Ë (Tao and Zhou, 2020) (Singh et al., 2020) (Chauhan and Diddee, 2020) (Dao et al., 2020) (Martinkovic et al., 2020) (Kim and Bethard, 2020) (Patil et al., 2020) (Kranzlein et al., 2020)   Ë   (Morio et al., 2020) (Jurkiewicz et al., 2020) (Chernyavskiy et al., 2020) (Khosla et al., 2020) (Paraschiv and Cercel, 2020) (Dimov et al., 2020) (Blaschke et al., 2020) (Verma et al., 2020)         Ë 23. 25. 26. 27. 28. 31. 33. – ËËË (Mikhalkova et al., 2020) (Li and Xiao, 2020) (Dementieva et al., 2020) (Arsenos and Siolas, 2020) (Ermurachi and Gifu, 2020) (Dewantara et al., 2020) (Daval-Frerot and Yannick, 2020) (Krishnamurthy et al., 2020) Table 3: Overview of the approaches to the span identification subtask. =part of the official submission; Ë=considered in internal experiments. The references to the description papers appear at the bottom. A large number of the participating teams built systems that rely heavily on engineered features. For instance, Team CyberWall"
2020.semeval-1.186,E17-1017,1,0.83519,"tained using distant supervision, assuming that all articles from a given news source share the label of that source, which introduces noise (Horne et al., 2018). Barr´on-Cede˜no et al. (2019b) experimented with a binary version of the problem: propaganda vs. no propaganda. See (Da San Martino et al., 2020a) for a recent survey on computational propaganda detection. In general, propaganda techniques serve as a means to persuade people, often in argumentative settings. While they may increase the rhetorical effectiveness of arguments, they naturally harm other aspects of argumentation quality (Wachsmuth et al., 2017). In particular, many of the span propaganda techniques considered in this shared task relate to the notion of fallacies, i.e. arguments whose reasoning is flawed in some way, often hidden and often on purpose (Tindale, 2007). Some recent work in computational argumentation has dealt with such fallacies. Among these, Habernal et al. (2018) presented and analyzed a corpus of web forum discussions with Ad hominem fallacies, and Habernal et al. (2017) introduced Argotario, a game that educates people to recognize fallacies. Argotario also had a corpus as a by-product, with 1.3k arguments annotate"
2020.semeval-1.188,2020.semeval-1.206,0,0.0947199,"Missing"
2020.semeval-1.188,C18-1139,0,0.018965,"ømberg-Derczynski et al., 2020), etc. 4 1428 Many teams also used context-independent embeddings from word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014), including language-specific embeddings such as Mazajak (Farha and Magdy, 2019) for Arabic. Some teams used other techniques: word n-grams, character n-grams, lexicons for sentiment analysis, and lexicon of offensive words. Other representations included emoji priors extracted from the weakly supervised SOLID dataset for English, and sentiment analysis using NLTK (Bird et al., 2009), Vader (Hutto and Gilbert, 2014), and FLAIR (Akbik et al., 2018). Machine learning models In terms of machine learning models, most teams used some kind of pretrained Transformers: typically BERT, but RoBERTa, XLM-RoBERTa (Conneau et al., 2020), ALBERT (Lan et al., 2019), and GPT-2 (Radford et al., 2019) were also popular. Other popular models included CNNs (Fukushima, 1980), RNNs (Rumelhart et al., 1986), and GRUs (Cho et al., 2014). Older models such as SVMs (Cortes and Vapnik, 1995) were also used, typically as part of ensembles. 5 English Track A total of 87 teams made submissions for the English track (23 of them participated in the 2019 edition of th"
2020.semeval-1.191,D19-5016,0,0.0684257,"nto an ensemble.2 2 Related Work The dataset for the task comes from (Da San Martino et al., 2019b), which used a BERT-based model with multi-task learning and a gated architecture; the system can be tried online (Da San Martino et al., 2020c). There was also a related previous task on fine-grained propaganda detection (Da San Martino et al., 2019a), where the participants used Transformer-style models, LSTMs and ensembles (Fadel et al., 2019; Hou and Chen, 2019; Hua, 2019). Some approaches further used non-contextualized word embeddings, e.g., based on FastText and GloVe (Gupta et al., 2019; Al-Omari et al., 2019), or handcrafted features such as LIWC, quotes and questions (Alhindi et al., 2019). For the fragment-classification subtask (a combination of two subtasks of the current SemEval) the LSTM-CRF (Gupta et al., 2019) or biLSTM-CRF (Alhindi et al., 2019) models were applied besides BERT (Yu et al., 2019). Moreover, some efforts have been made to increase the size of the dataset using unsupervised language model pre-training (Yoosuf and Yang, 2019). Finally, there is a recent survey on computational propaganda detection (Da San Martino et al., 2020b). 1 2 The official task webpage: http://propagand"
2020.semeval-1.191,D19-5013,0,0.207831,"et al., 2019b), which used a BERT-based model with multi-task learning and a gated architecture; the system can be tried online (Da San Martino et al., 2020c). There was also a related previous task on fine-grained propaganda detection (Da San Martino et al., 2019a), where the participants used Transformer-style models, LSTMs and ensembles (Fadel et al., 2019; Hou and Chen, 2019; Hua, 2019). Some approaches further used non-contextualized word embeddings, e.g., based on FastText and GloVe (Gupta et al., 2019; Al-Omari et al., 2019), or handcrafted features such as LIWC, quotes and questions (Alhindi et al., 2019). For the fragment-classification subtask (a combination of two subtasks of the current SemEval) the LSTM-CRF (Gupta et al., 2019) or biLSTM-CRF (Alhindi et al., 2019) models were applied besides BERT (Yu et al., 2019). Moreover, some efforts have been made to increase the size of the dataset using unsupervised language model pre-training (Yoosuf and Yang, 2019). Finally, there is a recent survey on computational propaganda detection (Da San Martino et al., 2020b). 1 2 The official task webpage: http://propaganda.qcri.org/semeval2020-task11/ The code of our systems is available at http://githu"
2020.semeval-1.191,D19-5024,1,0.902682,"Missing"
2020.semeval-1.191,D19-1565,1,0.888882,"Missing"
2020.semeval-1.191,2020.semeval-1.186,1,0.83693,"Missing"
2020.semeval-1.191,2020.acl-demos.32,1,0.716972,"Missing"
2020.semeval-1.191,D19-5020,0,0.0866065,"rk architecture, and we further added some post-processing steps. We further applied transfer learning between the two subtasks, and finally, we combined different models into an ensemble.2 2 Related Work The dataset for the task comes from (Da San Martino et al., 2019b), which used a BERT-based model with multi-task learning and a gated architecture; the system can be tried online (Da San Martino et al., 2020c). There was also a related previous task on fine-grained propaganda detection (Da San Martino et al., 2019a), where the participants used Transformer-style models, LSTMs and ensembles (Fadel et al., 2019; Hou and Chen, 2019; Hua, 2019). Some approaches further used non-contextualized word embeddings, e.g., based on FastText and GloVe (Gupta et al., 2019; Al-Omari et al., 2019), or handcrafted features such as LIWC, quotes and questions (Alhindi et al., 2019). For the fragment-classification subtask (a combination of two subtasks of the current SemEval) the LSTM-CRF (Gupta et al., 2019) or biLSTM-CRF (Alhindi et al., 2019) models were applied besides BERT (Yu et al., 2019). Moreover, some efforts have been made to increase the size of the dataset using unsupervised language model pre-training"
2020.semeval-1.191,D19-5012,0,0.161862,"Missing"
2020.semeval-1.191,D19-5010,0,0.306196,"we further added some post-processing steps. We further applied transfer learning between the two subtasks, and finally, we combined different models into an ensemble.2 2 Related Work The dataset for the task comes from (Da San Martino et al., 2019b), which used a BERT-based model with multi-task learning and a gated architecture; the system can be tried online (Da San Martino et al., 2020c). There was also a related previous task on fine-grained propaganda detection (Da San Martino et al., 2019a), where the participants used Transformer-style models, LSTMs and ensembles (Fadel et al., 2019; Hou and Chen, 2019; Hua, 2019). Some approaches further used non-contextualized word embeddings, e.g., based on FastText and GloVe (Gupta et al., 2019; Al-Omari et al., 2019), or handcrafted features such as LIWC, quotes and questions (Alhindi et al., 2019). For the fragment-classification subtask (a combination of two subtasks of the current SemEval) the LSTM-CRF (Gupta et al., 2019) or biLSTM-CRF (Alhindi et al., 2019) models were applied besides BERT (Yu et al., 2019). Moreover, some efforts have been made to increase the size of the dataset using unsupervised language model pre-training (Yoosuf and Yang, 20"
2020.semeval-1.191,D19-5019,0,0.0931215,"me post-processing steps. We further applied transfer learning between the two subtasks, and finally, we combined different models into an ensemble.2 2 Related Work The dataset for the task comes from (Da San Martino et al., 2019b), which used a BERT-based model with multi-task learning and a gated architecture; the system can be tried online (Da San Martino et al., 2020c). There was also a related previous task on fine-grained propaganda detection (Da San Martino et al., 2019a), where the participants used Transformer-style models, LSTMs and ensembles (Fadel et al., 2019; Hou and Chen, 2019; Hua, 2019). Some approaches further used non-contextualized word embeddings, e.g., based on FastText and GloVe (Gupta et al., 2019; Al-Omari et al., 2019), or handcrafted features such as LIWC, quotes and questions (Alhindi et al., 2019). For the fragment-classification subtask (a combination of two subtasks of the current SemEval) the LSTM-CRF (Gupta et al., 2019) or biLSTM-CRF (Alhindi et al., 2019) models were applied besides BERT (Yu et al., 2019). Moreover, some efforts have been made to increase the size of the dataset using unsupervised language model pre-training (Yoosuf and Yang, 2019). Finally"
2020.semeval-1.191,N16-1030,0,0.0281715,"(CRF) model (Lafferty et al., 2001) as an additional layer, in order to model the dependency between the labels predicted for the individual tokens; this model can observe that the sequence O I-PROP is never present in the training data, and thus it can assign a very low probability to the transition from an O tag to an I-PROP tag. We trained the resulting RoBERTa-CRF model in an end-to-end fashion as shown in Figure 1. The CRF receives the logits for each input token, and makes a prediction for the entire input sequence, taking into account the dependencies between the labels, similarly to (Lample et al., 2016). Note that RoBERTa works with byte pair encoding (BPE) units, while for the CRF it makes more sense to work with words. Thus, in the input to the CRF, we only used tokens that started a word, and we skipped any word continuation tokens, e.g., a token like ##smth would not be passed to the CRF. Post-processing We further applied two post-processing steps to obtain the final prediction from the token classification. First, we made sure that each predicted propaganda span began and ended by either a letter or a number (alphanumerical); otherwise, we shortened the span by advancing its beginning"
2020.semeval-1.191,W09-1119,0,0.0643782,"es non-starting word pieces (##eared in the example). 3 Our Systems In this section, we provide a general overview of our systems for the two subtasks. For both subtasks, we trained ensembles based on RoBERTa with some postprocessing. 3.1 Subtask 1: Span Identification Model We addressed the span identification subtask as a sequence labeling problem. To that end, we transformed the initial span markup into a BIO tagging format (Begin, Inside, Outside), as our preliminary experiments had shown that it performed better than alternatives such as IO and BIOUL (Begin, Inside, Outside, Unit, Last) (Ratinov and Roth, 2009). As we have only one possible entity class PROP, each token can be assigned one of the following three labels: O, B-PROP, and I-PROP. Then, we fine-tuned a RoBERTa model to predict the above BIO tags for each token in the input sentence. One problem with the above setup is that each token is classified independently of the surrounding tokens: while these surrounding tokens are taken into account in the contextualized embeddings that RoBERTa produces, there is no modeling of the dependency between the predicted labels: for example, logically I-PROP cannot follow O, but RoBERTa does not model i"
2020.semeval-1.191,D19-5011,0,0.0145564,"Hou and Chen, 2019; Hua, 2019). Some approaches further used non-contextualized word embeddings, e.g., based on FastText and GloVe (Gupta et al., 2019; Al-Omari et al., 2019), or handcrafted features such as LIWC, quotes and questions (Alhindi et al., 2019). For the fragment-classification subtask (a combination of two subtasks of the current SemEval) the LSTM-CRF (Gupta et al., 2019) or biLSTM-CRF (Alhindi et al., 2019) models were applied besides BERT (Yu et al., 2019). Moreover, some efforts have been made to increase the size of the dataset using unsupervised language model pre-training (Yoosuf and Yang, 2019). Finally, there is a recent survey on computational propaganda detection (Da San Martino et al., 2020b). 1 2 The official task webpage: http://propaganda.qcri.org/semeval2020-task11/ The code of our systems is available at http://github.com/aschern/semeval2020_task11 1462 Proceedings of the 14th International Workshop on Semantic Evaluation, pages 1462–1468 Barcelona, Spain (Online), December 12, 2020. O B-PROP I-PROP CRF RoBERTa Democrats app ##eared stunned Figure 1: SI subtask: Our RoBERTa-CRF model with BIO encoding. It is trained end-to-end and the CRF model ignores non-starting word pie"
2020.semeval-1.191,N19-1144,1,0.823025,"nd labels from similar spans in training. We achieved sizable improvements over baseline RoBERTa models, and the official evaluation ranked our system 3rd (almost tied with the 2nd) out of 36 teams on the span identification subtask, and 2nd (almost tied with the 1st) out of 31 teams on the techniques classification subtask. In future work, we plan to explore other neural architectures such as T5 (Raffel et al., 2019) and GPT-3 (Brown et al., 2020). We further want to explore transfer learning from other tasks such as argumentation mining (Stede et al., 2018) and offensive language detection (Zampieri et al., 2019; Zampieri et al., 2020). 7 Acknowledgments Anton Chernyavskiy and Dmitry Ilvovsky performed this research within the framework of the HSE University Basic Research Program, funded by the Russian Academic Excellence Project ‘5-100’. Preslav Nakov contributed as part of the Propaganda Analysis Project (propaganda.qcri.org), part of the Tanbih megaproject (tanbih.qcri.org), developed at the Qatar Computing Research Institute, HBKU. Tanbih aims to limit the effect of “fake news”, propaganda, and media bias by making users aware of what they are reading, thus promoting media literacy and critical"
2021.acl-long.516,passonneau-2006-measuring,0,0.0109376,"also a phase to annotate propaganda techniques when showing the image only; however, this is hard to do in practice as the text is embedded as part of the pixels in the image. 6608 Quality of the Annotations We assessed the quality of the annotations for the individual annotators from phases 2 and 4 (thus, combining the annotations for text and images) to the final consolidated labels at phase 5, following the setting in (Da San Martino et al., 2019). Since our annotation is multilabel, we computed Krippendorff’s α, which supports multi-label agreement computation (Artstein and Poesio, 2008; Passonneau, 2006). The results are shown in Table 1 and indicate moderate to perfect agreement (Landis and Koch, 1977). Agreement Pair Annotator 1 vs. Consolidated Annotator 2 vs. Consolidated Annotator 3 vs. Consolidated 0.83 0.91 0.56 Average 0.77 Table 1: Inter-annotator agreement. Loaded Language Name calling/Labeling Smears Doubt Exaggeration/Minimisation Slogans Appeal to fear/prejudice Whataboutism Glittering generalities (Virtue) Flag-waving Repetition Causal Oversimplification Thought-terminating clich´e Black-and-white Fallacy/Dictatorship Straw Man Appeal to authority Reductio ad hitlerum Obfuscatio"
2021.acl-long.516,D17-1317,0,0.181598,"ble to reach well-targeted communities at high velocity. We believe that being aware and able to detect propaganda campaigns would contribute to a healthier online environment. Propaganda appears in various forms and has been studied by different research communities. There has been work on exploring network structure, looking for malicious accounts and coordinated inauthentic behavior (Cresci et al., 2017; Yang et al., 2019; Chetan et al., 2019; Pacheco et al., 2020). In the natural language processing community, propaganda has been studied at the document level (Barr´on-Cedeno et al., 2019; Rashkin et al., 2017), and at the sentence and the fragment levels (Da San Martino et al., 2019). There have also been notable datasets developed, including (i) TSHP-17 (Rashkin et al., 2017), which consists of document-level annotation labeled with four classes (trusted, satire, hoax, and propaganda); (ii) QProp (Barr´on-Cedeno et al., 2019), which uses binary labels (propaganda vs. non-propaganda), and (iii) PTC (Da San Martino et al., 2019), which uses fragment-level annotation and an inventory of 18 propaganda techniques. While that work has focused on text, here we aim to detect propaganda techniques from a m"
2021.acl-long.516,P18-1238,0,0.0721911,"Missing"
2021.bsnlp-1.15,doddington-etal-2004-automatic,0,0.292153,"y recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Worth mentioning in this context is Entity Discovery and Linking (EDL) (Ji et al., 2014, 2015), a track of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of documents in multiple languages (English, Chinese, and Spanis"
2021.bsnlp-1.15,huttunen-etal-2002-diversity,1,0.607213,"ukasz Kobyli´nski, 2018, 2020). and a recent shared task on NE Recognition in Russian (Starostin et al., 2016). ing for Slavic Languages, (Piskorski et al., 2017, 2019), which, to the best of our knowledge, are the first attempts at such shared tasks covering multiple Slavic languages. High-quality recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE"
2021.bsnlp-1.15,P16-1060,0,0.0711935,"Missing"
2021.bsnlp-1.15,W19-3709,1,0.734272,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.13,0,0.0284518,"lovenian. The system uses contemporary BERT and RoBERTa multilingual pre-trained models, which include Slovene among other languages. The system was further trained on the SlavNER dataset for the NER task and used the Dedupe method for the Entity Matching task. The best performing models were pre-trained on Slovene. The results also indicate that two-step prediction of NE could be beneficial. The team made their code publicly available. The Priberam Labs system, (Ferreira et al., 2021), focuses on the NER task. It uses three components: a multilingual contextual embedding The TraSpaS system, (Suppa and Jariabka, 2021), tests the assumption that the universal open-source NLP toolkits (such as SpaCy, Stanza or Trankit) could achieve competitive performance on the Multilingual NER task, using large pretrained Transformer-based language models available from HuggingfaceTransformers, which have not been available in previous editions of the Shared Task. The team tests the generalizability of the models to new low-resourced domains, and to languages such as Slovene and Ukrainian. The UWr-VL system, (Rychlikowski et al., 2021), utilizes large collections of unstructured and structured documents for unsupervised t"
2021.bsnlp-1.15,W17-1412,1,0.890203,"jubeši´c, 2014), tools for NE recognition in Slovene (Štajner et al., 2013; Ljubeši´c et al., 2013), a Czech corpus of 11K annotated NEs (Ševˇcíková et al., 2007), NER tools for Czech (Konkol and Konopík, 2013), tools and resources for fine-grained annotation of NEs in the National Corpus of Polish (Waszczuk et al., 2010; Savary and Piskorski, 2011), NER shared tasks for Polish organized under the umbrella of POLEVAL2 evaluation campaigns (Ogrodniczuk and Łukasz Kobyli´nski, 2018, 2020). and a recent shared task on NE Recognition in Russian (Starostin et al., 2016). ing for Slavic Languages, (Piskorski et al., 2017, 2019), which, to the best of our knowledge, are the first attempts at such shared tasks covering multiple Slavic languages. High-quality recognition and analysis of NEs is an essential step not only for information access, such as document retrieval and clustering, but it also constitutes a fundamental processing step in a wide range of NLP pipelines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japane"
2021.bsnlp-1.15,W02-2024,0,0.125599,"elines built for higher-level analysis of text, such as Information Extraction, see, e.g. (Huttunen et al., 2002). Other NER-related shared tasks have been organized previously. The first non-English monolingual NER evaluations—covering Chinese, Japanese, Spanish, and Arabic—were held in the context of the Message Understanding Conferences (MUCs) (Chinchor, 1998) and the ACE Programme (Doddington et al., 2004). The first multilingual NER shared task, which covered several European languages, including Spanish, German, and Dutch, was organized in the context of the CoNLL conferences (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). The NE types covered in these campaigns were similar to the NE types covered in our Challenge. Worth mentioning in this context is Entity Discovery and Linking (EDL) (Ji et al., 2014, 2015), a track of the NIST Text Analysis Conferences (TAC). EDL aimed to extract entity mentions from a collection of documents in multiple languages (English, Chinese, and Spanish), and to partition the entities into cross-document equivalence classes, by either linking mentions to a knowledge base or directly clustering them. An important difference between EDL and our ta"
2021.bsnlp-1.15,W03-0419,0,0.550493,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.9,0,0.0409736,"aptation algorithm. It also uses other techniques to improve system’s NER performance, such as marking and enrichment of uppercase tokens, prediction of NE boundaries with a multitask approach, prediction of masked tokens, fine-tuning the language model to the domain of the document. Six teams submitted descriptions of their systems as BSNLP Workshop papers. We briefly review these systems here; for complete descriptions, please see the corresponding papers. Two additional teams submitted their results with short descriptions of their systems, which appear in this section. The UL FRI system, (Prelevikj and Zitnik, 2021), generated results for several settings, models and languages, although the team’s main motivation is to develop effective NER tools for Slovenian. The system uses contemporary BERT and RoBERTa multilingual pre-trained models, which include Slovene among other languages. The system was further trained on the SlavNER dataset for the NER task and used the Dedupe method for the Entity Matching task. The best performing models were pre-trained on Slovene. The results also indicate that two-step prediction of NE could be beneficial. The team made their code publicly available. The Priberam Labs sy"
2021.bsnlp-1.15,2021.bsnlp-1.11,0,0.0823823,"Missing"
2021.bsnlp-1.15,2021.bsnlp-1.14,0,0.0748319,"Missing"
2021.emnlp-main.110,W19-4828,0,0.0288008,"Rulerules expressed in a synthetic language to Takers (Clark et al., 2020). However, we differ PLMs through fine-tuning (modeled as binary (i) by using a larger subclass of first-order logic classification). rules (with more variables and various forms), and We create and release the first dataset for this (ii) by incorporating soft rules. task, which contains 3.2M examples derived Our proposal is different from work on Question from 161 rules describing real common-sense Answering (QA) with implicit reasoning based on patterns with the target probability for the task common-sense knowledge (Clark et al., 2019a), as obtained from a formal reasoner (Section 4). we rely purely on deductive logic from explicitly stated rules. We introduce techniques to predict the corOur approach also differs from methods that serect probability of the reasoning output for the mantically parse natural language into a formal given soft rules and facts. Our solution rerepresentation on which a formal reasoner can be lies on a revised loss function that effectively applied (Liang, 2016), as we directly reason with models the uncertainty of the rules (Section 5). language. Yet, we are also different from Natural Our appro"
2021.emnlp-main.110,D19-1250,0,0.0291641,"rules. In particular, RULE BERT achieves new state-of-the-art results on three external datasets. The data, the code, and the fine-tuned model are available at http://github.com/MhmdSaiid/ RuleBert. 2 Related Work PLMs have been shown to have some reasoning capabilities (Talmor et al., 2020b), but fail on basic reasoning tasks (Talmor et al., 2020a) and are inconsistent (Elazar et al., 2021), especially when it comes to negation (Kassner and Schütze, 2020). Our work focuses on deductive reasoning. Note that it is different from previous work, e.g., on measuring the factual knowledge of PLMs (Petroni et al., 2019), on probing the commonsense capabilities of PLMs at the token or at the sentence level (Zhou et al., 2020), or on testing the reasoning capabilities of PLMs on tasks such as age comparison and taxonomy conjunction (Talmor et al., 2020a). Our work relates to Task #15 in We introduce the problem of teaching soft the bAbI dataset (Weston et al., 2016) and to Rulerules expressed in a synthetic language to Takers (Clark et al., 2020). However, we differ PLMs through fine-tuning (modeled as binary (i) by using a larger subclass of first-order logic classification). rules (with more variables and va"
2021.emnlp-main.110,2020.acl-main.442,0,0.134998,"established tools for capturing both linguistic and factual knowledge (Clark et al., 2019b; Rogers et al., 2020). However, even the largest models fail on basic reasoning tasks. If we consider common relations between entities, we see that such models are not aware of negation, inversion (e.g., parent-child), symmetry (e.g., spouse), implication, and composition. While these are obvious to a human, they are challenging to learn from With the above considerations in mind, here we text corpora as they go beyond linguistic and fac- show how to reason over soft logical rules with tual knowledge (Ribeiro et al., 2020; Kassner and PLMs. We provide facts and rules expressed in Schütze, 2020). We claim that such reasoning prim- natural language, and we ask the PLM to come up itives can be transferred to the PLMs by leveraging with a logical conclusion for a hypothesis, together logical rules, such as those shown in Figure 1. with the probability for it being true. 1460 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1460–1476 c November 7–11, 2021. 2021 Association for Computational Linguistics Unlike previous approaches (Clark et al., 2020), we enable deductive"
2021.emnlp-main.110,2020.tacl-1.54,0,0.035715,"Missing"
2021.emnlp-main.110,D19-1339,0,0.0127242,"020) or build proof iteratively using 1-hop inference (Tafjord et al., 2021). Acknowledgments This work is partially supported by a Google Fac3 ulty Research Award and the ANR JCJC Grant On the much easier QQP test set, RULE BERT achieved 0.89 accuracy after one epoch, and 0.91 after three epochs. InfClean. 1468 Ethics and Broader Impact Data Collection While we generated the facts in our examples, the logical rules have been mined from the data in the DBpedia knowledge graph, which in turn has been generated from Wikipedia. Biases We are aware of (i) the biases and abusive language patterns (Sheng et al., 2019; Zhang et al., 2020; Bender et al., 2021; Liang et al., 2021) that PLMs impose, and (ii) the imperfectness and the biases of our rules as data from Wikipedia has been used to mine the rules and compute their confidences (Janowicz et al., 2018; Demartini, 2019). However, our goal is to study PLM’s capability of deductive soft reasoning. For (i), there has been some work on debiasing PLMs (Liang et al., 2020), while for (ii), we used mined rules to have more variety, but could resort to user-specified rules validated by consensus to relieve the bias. Environmental Impact The use of large-scale"
2021.emnlp-main.110,P19-1355,0,0.0348427,"Missing"
2021.emnlp-main.110,2021.findings-acl.317,0,0.0221069,"Ms could reason with soft rules over natural language. We experimented with one flavor of probabilistic answer set programming (LPMLN ), but other semantics can be also used with the proposed methodology. We further explored the inference capabilities of Transformerbased PLMs, focusing on positive and negative textual entailment. We leave non-entailment for future work. We also leave open the development of explainable models. Some approaches use occlusion that removes parts of the input and checks the impact on the output (Clark et al., 2020) or build proof iteratively using 1-hop inference (Tafjord et al., 2021). Acknowledgments This work is partially supported by a Google Fac3 ulty Research Award and the ANR JCJC Grant On the much easier QQP test set, RULE BERT achieved 0.89 accuracy after one epoch, and 0.91 after three epochs. InfClean. 1468 Ethics and Broader Impact Data Collection While we generated the facts in our examples, the logical rules have been mined from the data in the DBpedia knowledge graph, which in turn has been generated from Wikipedia. Biases We are aware of (i) the biases and abusive language patterns (Sheng et al., 2019; Zhang et al., 2020; Bender et al., 2021; Liang et al., 2"
2021.emnlp-main.110,2020.tacl-1.48,0,0.0396392,"tly mimicking the results for the same problem modeled with LPMLN . Our contributions can be summarized as follows: • • • • • We demonstrate that our fine-tuning approach effectively transfers knowledge about predicate negation and symmetry to the lower levels of the transformer, which benefits from the logical notions in the rules. In particular, RULE BERT achieves new state-of-the-art results on three external datasets. The data, the code, and the fine-tuned model are available at http://github.com/MhmdSaiid/ RuleBert. 2 Related Work PLMs have been shown to have some reasoning capabilities (Talmor et al., 2020b), but fail on basic reasoning tasks (Talmor et al., 2020a) and are inconsistent (Elazar et al., 2021), especially when it comes to negation (Kassner and Schütze, 2020). Our work focuses on deductive reasoning. Note that it is different from previous work, e.g., on measuring the factual knowledge of PLMs (Petroni et al., 2019), on probing the commonsense capabilities of PLMs at the token or at the sentence level (Zhou et al., 2020), or on testing the reasoning capabilities of PLMs on tasks such as age comparison and taxonomy conjunction (Talmor et al., 2020a). Our work relates to Task #15 in"
2021.emnlp-main.110,P19-1176,0,0.108637,"andle soning over multiple input rules. Horn rules (MacCartney and Manning, 2009; DaWe show that our approach enables fine-tuned gan et al., 2013). models to yield prediction probability very Unlike previous work (Hamilton et al., 2018; close to that produced by a formal reasoner Yang et al., 2017; Minervini et al., 2020), we do not (Section 6). Our PLM fine-tuned on soft rules, design a new, ad-hoc module for neural reasoning, RULE BERT, can effectively reason with facts but we rely solely on the transformer’s capability and rules that it has not seen at training, even to emulate algorithms (Wang et al., 2019b; Lample when fine-tuned with only 20 rules. and Charton, 2020). 1461 3 Background Language Models. We focus on language models pre-trained with bidirectional transformer encoders using masked language modeling (Devlin et al., 2019). For fine-tuning, we create examples for sequence classification to teach the models how to emulate reasoning given facts and soft rules. Logical Rules. We rely on existing corpora of declarative Horn rules mined from large RDF knowledge bases (KBs) (Galárraga et al., 2015; Ortona et al., 2018; Ahmadi et al., 2020). An RDF KB is a database representing information"
2021.emnlp-main.110,2020.emnlp-main.555,0,0.0216414,"iments are summarized in Table 1. 6.1 Experimental Setup Rules. We use a corpus of 161 soft rules mined from DBpedia. We chose a pool of distinct rules with varying number of variables, number of predicates, rule conclusions, and confidences. Reasoner. We use the official implementation1 of the LPMLN reasoner. We set the reasoner to compute the exact probabilities for the triples. PLM. We use the HuggingFace pre-trained RoBERTaLARGE (Liu et al., 2020) model as our base model, as it is trained on more data compared to BERT (Devlin et al., 2019), and is better at learning positional embeddings (Wang and Chen, 2020). We fine-tune the PLM2 with the weighted binary cross-entropy (wBCE) loss from Section 5. More details can be found in Appendix C. Evaluations Measures. For the examples in the test set, we use accuracy (Acc) and F1 score (F1) for balanced and unbalanced settings, respectively. As these measures do not take into account the uncertainty of the prediction probability, we further introduce Confidence Accuracy@k (CA@k), which measures the proportion of examples whose absolute error between the predicted and the actual probabilities is less than a threshold k: CA@k = 1 2 #{xi , |wi − w ˆi |&lt; k} #{"
2021.emnlp-main.710,N18-2004,1,0.842859,"(Mohammad et al., 2017; Aldayel and Magdy, 2019). Debating platforms were used as data source for stance (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain le"
2021.emnlp-main.710,N19-1053,0,0.110634,"ion techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out-ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance. 1 Introduction vs. long news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016) vs. news outlets (Stefanov et al., 2020) vs. people (Darwish et al., 2020), (ii) with respect to a claim (Chen et al., 2019) vs. a topic, either explicit (Qazvinian et al., 2011; Walker et al., 2012) or implicit (Hasan and Ng, 2013; Gorrell et al., 2019). Moreover, there is substantial variation in (iii) the label inventory, in the exact label definition, in the data collection, in the annotation setup, in the domain, etc. The most crucial of these, which has not been investigated, currently preventing cross-domain studies, is that the label inventories differ between the settings, as shown in Table 1. Labels include not only variants of agree, disagree, and unrelated, but also difficult to cross-map ones, such as"
2021.emnlp-main.710,P07-1033,0,0.456725,"Missing"
2021.emnlp-main.710,N19-1423,0,0.0104585,"is set to a small positive number to prevent this regulariser from dominating the overall loss. We set γ to 0.01. Furthermore, since our dataset is quite diverse even in the four source domains that we outlined, we optimise the domain-adaptive loss towards a meta-class for each dataset, instead of the domain. 5 Experiments Baselines Logistic Regression A logistic regression trained using TF.IDF word unigrams. The input is a concatenation of the target and context vectors. Multi-task learning (MTL) A single projection layer for each dataset is added on top of a pretrained language model (BERT (Devlin et al., 2019) or RoBERTa (Liu et al., 2019)). We then pass the [CLS] token representations through the datasetspecific layer. Finally, we propagate the errors only through that layer (and the base model), without updating parameters for other datasets. 5.2 Evaluation Results In-Domain Experiments We train and test on all datasets; the results are in Table 5. First, to find the best base model and set a baseline for MoLE, we evaluate two strong models: BERTBase uncased (Devlin et al., 2019), and RoBERTaBase cased6 (Liu et al., 2019). On our 16 datasets, RoBERTa outperforms BERT by 2 F1 points absolute on av"
2021.emnlp-main.710,N16-1138,0,0.533109,"end-to-end unsupervised framework for outof-domain prediction of unseen, user-defined labels. In particular, we combine domain adaptation techniques such as mixture of experts and domain-adversarial training with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out-ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance. 1 Introduction vs. long news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016) vs. news outlets (Stefanov et al., 2020) vs. people (Darwish et al., 2020), (ii) with respect to a claim (Chen et al., 2019) vs. a topic, either explicit (Qazvinian et al., 2011; Walker et al., 2012) or implicit (Hasan and Ng, 2013; Gorrell et al., 2019). Moreover, there is substantial variation in (iii) the label inventory, in the exact label definition, in the data collection, in the annotation setup, in the domain, etc. The most crucial of these, which has not been investigated, currently preventing cross-domain studies, is that the label inventories differ between the settings, as shown i"
2021.emnlp-main.710,N09-1068,0,0.0598913,"Missing"
2021.emnlp-main.710,2020.acl-main.157,0,0.0330081,"Missing"
2021.emnlp-main.710,2021.acl-long.127,0,0.0829382,"Missing"
2021.emnlp-main.710,C18-1158,0,0.183785,"Missing"
2021.emnlp-main.710,K19-1046,0,0.0167675,"gdy, 2019). Debating platforms were used as data source for stance (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain eval"
2021.emnlp-main.710,N18-1070,1,0.816523,"Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain evaluation. Another direction is data-centric adaptation: Han and Eisenstein (2019); Rietzler et al. (2020) used MLM fin"
2021.emnlp-main.710,D19-1452,1,0.945163,", 2019) or explicit topics (Augenstein et al., 2016; Stab et al., 2018; Allaway and McKeown, 2020), claims (Baly et al., 2018; Chen et al., 2019; Hanselowski et al., 2019; Conforti et al., 2020a,b) or headlines (Ferreira and Vlachos, 2016; Habernal et al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain evaluation. Another direction is data-centric adaptation: Han and Eisenstein (2019); Rietzler et al. (2020) used MLM fine-tuning on target-domain data. Gururangan et al. (2020) showed alternate domain-adaptive (in-domain data) and task-adaptive (out-of-domain unlabelled data) pre-training. Label Embeddings Label embeddings can capture, in an unsupervised fashion, the complex relations between target labels for multiple datasets or tasks. They can boost"
2021.emnlp-main.710,D14-1162,0,0.0841684,"Missing"
2021.emnlp-main.710,D11-1147,0,0.0543127,"Missing"
2021.emnlp-main.710,2020.lrec-1.607,0,0.035895,"al., 2018; Mohtarami et al., 2018). The focus, however, has been on homogeneous text, as opposed to cross-platform or cross-domain. Exceptions are Stab et al. (2018), who worked on heterogeneous text, but limited to eight topics, and Schiller et al. (2021), who combined datasets from different domains, but used in-domain multi-task learning, and Mohtarami et al. (2019) and Hardalov et al. (2021a), who used a cross-lingual setup. In contrast, we focus on cross-domain learning on 16 datasets, and out-of-domain evaluation. Another direction is data-centric adaptation: Han and Eisenstein (2019); Rietzler et al. (2020) used MLM fine-tuning on target-domain data. Gururangan et al. (2020) showed alternate domain-adaptive (in-domain data) and task-adaptive (out-of-domain unlabelled data) pre-training. Label Embeddings Label embeddings can capture, in an unsupervised fashion, the complex relations between target labels for multiple datasets or tasks. They can boost the end-task performance for various deep learning architectures, e.g., CNNs (Zhang et al., 2018; Pappas and Henderson, 2019), RNNs (Augenstein et al., 2018, 2019), and Transformers (Chang et al., 2020). Recent work has proposed different perspective"
2021.emnlp-main.710,D18-1131,1,0.824208,"te the source groupings used in our experiments and analysis (Section 3.3). Domain Adaptation Domain adaptation was studied in supervised settings, where in addition to the source-domain data, a (small) amount of labeled data in the target domain is also available (Daumé III, 2007; Finkel and Manning, 2009; 3.1 Datasets Donahue et al., 2013; Yao et al., 2015; Mou et al., 2016; Lin and Lu, 2018), and in unsupervised set- arc The Argument Reasoning Comprehension dataset has posts from the New York Times debate tings, without labeled target-domain data (Blitzer et al., 2006; Lipton et al., 2018; Shah et al., 2018; section on immigration and international affairs. argmin The Argument Mining corpus presents arMohtarami et al., 2019; Bjerva et al., 2020; Wright and Augenstein, 2020). Recently, domain adap- guments relevant to a particular topic from heterogenous texts. Topics include controversial keytation was applied to pre-trained Transformers words like death penalty and gun control. (Lin et al., 2020). One direction therein are aremergent The Emergent2 dataset is a collection of chitectural changes (method-centric): Ma et al. (2019) proposed curriculum learning with domain- articles from rumour site"
2021.emnlp-main.710,W15-0509,0,0.0250797,"y, we release our code, models, and data.1 substantial differences in the settings, e.g., stance 1 (i) expressed in tweets (Qazvinian et al., 2011; The datasets and code are available for research purposes: Mohammad et al., 2016; Conforti et al., 2020b) https://github.com/checkstep/mole-stance 9011 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9011–9028 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Related Work Stance Detection Prior work on stance explored its connection to argument mining (Boltuži´c and Šnajder, 2014; Sobhani et al., 2015), opinion mining (Wang et al., 2019), and sentiment analysis (Mohammad et al., 2017; Aldayel and Magdy, 2019). Debating platforms were used as data source for stance (Somasundaran and Wiebe, 2010; Hasan and Ng, 2014; Aharoni et al., 2014), and more recently it was Twitter (Mohammad et al., 2016; Gorrell et al., 2019). With time, the definition of stance has become more nuanced (Küçük and Can, 2020), as well as its applications (Zubiaga et al., 2018; Hardalov et al., 2021b). Settings vary with respect to implicit (Hasan and Ng, 2013; Gorrell et al., 2019) or explicit topics (Augenstein et al.,"
2021.emnlp-main.710,E17-2088,0,0.0583081,"Missing"
2021.emnlp-main.710,walker-etal-2012-corpus,0,0.015902,"with label embeddings, and we demonstrate sizable performance gains over strong baselines, both (i) indomain, i.e., for seen targets, and (ii) out-ofdomain, i.e., for unseen targets. Finally, we perform an exhaustive analysis of the crossdomain results, and we highlight the important factors influencing the model performance. 1 Introduction vs. long news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016) vs. news outlets (Stefanov et al., 2020) vs. people (Darwish et al., 2020), (ii) with respect to a claim (Chen et al., 2019) vs. a topic, either explicit (Qazvinian et al., 2011; Walker et al., 2012) or implicit (Hasan and Ng, 2013; Gorrell et al., 2019). Moreover, there is substantial variation in (iii) the label inventory, in the exact label definition, in the data collection, in the annotation setup, in the domain, etc. The most crucial of these, which has not been investigated, currently preventing cross-domain studies, is that the label inventories differ between the settings, as shown in Table 1. Labels include not only variants of agree, disagree, and unrelated, but also difficult to cross-map ones, such as discuss and question. Our goal in this paper is to design a common stance d"
2021.findings-acl.246,2020.semeval-1.159,0,0.0427166,"multimodal dataset containing images, titles, upvotes, downvotes, #comments, etc., all collected from Reddit. Recently, SemEval-2020 Task 9 on Memotion Analysis (Sharma et al., 2020a) introduced a dataset of 10k memes, annotated with sentiment, emotions, and emotion intensity. Most participating systems in this challenge used fusion of visual and textual features computed using models such as Inception, ResNet, CNN, VGG-16 and DenseNet for image representation (Morishita et al., 2020; Sharma et al., 2020b; Yuan et al., 2020), and BERT, XLNet, LSTM, GRU and DistilBERT for text representation (Liu et al., 2020; Gundapu and Mamidi, 2020). Due to class imbalance in the dataset, approaches such as GMM and Training Signal Annealing (TSA) were also found useful. Morishita et al. (2020); Bonheme and Grzes (2020); Guo et al. (2020); Sharma et al. (2020b) proposed ensemble learning, whereas Gundapu and Mamidi (2020); De la Pe˜na Sarrac´en et al. (2020) and several others used multimodal approaches. A few others leveraged transfer-learning using pre-trained models such as BERT (Devlin et al., 2019), VGG16 (Simonyan and Zisserman, 2015), and ResNet (He et al., 2016). Finally, state-of-the-art results for all"
2021.findings-acl.246,2020.semeval-1.149,0,0.0360898,"ng of image–text pairs along with associated tags, by collecting posts from the TUMBLR platform. Thang Duong et al. (2017) prepared a multimodal dataset containing images, titles, upvotes, downvotes, #comments, etc., all collected from Reddit. Recently, SemEval-2020 Task 9 on Memotion Analysis (Sharma et al., 2020a) introduced a dataset of 10k memes, annotated with sentiment, emotions, and emotion intensity. Most participating systems in this challenge used fusion of visual and textual features computed using models such as Inception, ResNet, CNN, VGG-16 and DenseNet for image representation (Morishita et al., 2020; Sharma et al., 2020b; Yuan et al., 2020), and BERT, XLNet, LSTM, GRU and DistilBERT for text representation (Liu et al., 2020; Gundapu and Mamidi, 2020). Due to class imbalance in the dataset, approaches such as GMM and Training Signal Annealing (TSA) were also found useful. Morishita et al. (2020); Bonheme and Grzes (2020); Guo et al. (2020); Sharma et al. (2020b) proposed ensemble learning, whereas Gundapu and Mamidi (2020); De la Pe˜na Sarrac´en et al. (2020) and several others used multimodal approaches. A few others leveraged transfer-learning using pre-trained models such as BERT (Devl"
2021.findings-acl.246,2020.semeval-1.115,0,0.0999319,"Missing"
2021.findings-acl.246,N16-3020,0,0.784864,"rac´en et al. (2020) and several others used multimodal approaches. A few others leveraged transfer-learning using pre-trained models such as BERT (Devlin et al., 2019), VGG16 (Simonyan and Zisserman, 2015), and ResNet (He et al., 2016). Finally, state-of-the-art results for all three tasks —sentiment classification, emotion classification and emotion quantification on this dataset,— were reported by Pramanick et al. (2021), who proposed a deep neural model that combines sentence demarcation and multi-hop attention. They also studied the interpretability of the model using the LIME framework (Ribeiro et al., 2016). Meme propagation. Dupuis and Williams (2019) surveyed personality traits of social media users who are more active in spreading misinformation in the form of memes. Crovitz and Moran (2020) studied the characteristics of memes as a vehicle for spreading potential misinformation and disinformation. Zannettou et al. (2020a) discussed the quantitative aspects of large-scale dissemination of racist and hateful memes among polarized communities on platforms such as 4chan’s /pol/. Ling et al. (2021) examined the artistic composition and the aesthetics of memes, the subjects they communicate, and t"
2021.findings-acl.246,2020.semeval-1.99,1,0.900443,"Missing"
2021.findings-acl.246,2020.semeval-1.154,0,0.247124,"are typically innocent and designed to look funny. WARNING: This paper contains meme examples and words that are offensive in nature. Over time, memes started being used for harmful purposes in the context of contemporary political and socio-cultural events, targeting individuals, groups, businesses, and society as a whole. At the same time, their multimodal nature and often camouflaged semantics make their analysis highly challenging (Sabat et al., 2019). Meme analysis. The proliferation of memes online and their increasing importance have led to a growing body of research on meme analysis (Sharma et al., 2020a; Reis et al., 2020; Pramanick et al., 2021). It has also been shown that off-the-shelf multimodal tools may be inadequate to unfold the underlying semantics of a meme as (i) memes are often context-dependent, (ii) the visual and the textual content are often uncorrelated, and (iii) meme images are mostly morphed, and the embedded text is sometimes hard to extract using standard OCR tools (Bonheme and Grzes, 2020). The dark side of memes. Recently, there has been a lot of effort to explore the dark side of memes, e.g., focusing on hate (Kiela et al., 2020) and offensive (Suryawanshi et al., 2"
2021.findings-acl.246,P18-1238,0,0.198819,"Missing"
2021.findings-acl.246,P19-1355,0,0.0594824,"tion within the text. Intervention with human moderation would be required in order to ensure that this does not occur. Intended Use. We present our dataset to encourage research in studying harmful memes on the web. We distribute the dataset for research purposes only, without a license for commercial use. We believe that it represents a useful resource when used in the appropriate manner. Environmental Impact. Finally, we would also like to warn that the use of large-scale Transformers requires a lot of computations and the use of GPUs/TPUs for training, which contributes to global warming (Strubell et al., 2019). This is a bit less of an issue in our case, as we do not train such models from scratch; rather, we fine-tune them on relatively small datasets. Moreover, running on a CPU for inference, once the model has been finetuned, is perfectly feasible, and CPUs contribute much less to global warming. Acknowledgments The work was partially supported by the Wipro research grant and the Infosys Centre for AI, IIIT Delhi, India. It is also part of the Tanbih megaproject, developed at the Qatar Computing Research Institute, HBKU, which aims to limit the impact of “fake news,” propaganda, and media bias b"
2021.findings-acl.246,2020.trac-1.6,0,0.328868,"(Sharma et al., 2020a; Reis et al., 2020; Pramanick et al., 2021). It has also been shown that off-the-shelf multimodal tools may be inadequate to unfold the underlying semantics of a meme as (i) memes are often context-dependent, (ii) the visual and the textual content are often uncorrelated, and (iii) meme images are mostly morphed, and the embedded text is sometimes hard to extract using standard OCR tools (Bonheme and Grzes, 2020). The dark side of memes. Recently, there has been a lot of effort to explore the dark side of memes, e.g., focusing on hate (Kiela et al., 2020) and offensive (Suryawanshi et al., 2020) memes. However, the harm a meme can cause can be much broader. For instance, the meme1 in Figure 1c is neither hateful nor offensive, but it is harmful to the media shown on the top left (ABC, CNN, etc.), as it compares them to China, suggesting that they adopt strong censorship policies. In short, the scope of harmful meme detection is much broader, and it may encompass other aspects such as cyberbullying, fake news, etc. Moreover, harmful memes have a target (e.g., news organization such as ABC and CNN in our previous example), which requires separate analysis not only to decipher their und"
2021.findings-acl.246,2020.semeval-1.116,0,0.0414474,"ags, by collecting posts from the TUMBLR platform. Thang Duong et al. (2017) prepared a multimodal dataset containing images, titles, upvotes, downvotes, #comments, etc., all collected from Reddit. Recently, SemEval-2020 Task 9 on Memotion Analysis (Sharma et al., 2020a) introduced a dataset of 10k memes, annotated with sentiment, emotions, and emotion intensity. Most participating systems in this challenge used fusion of visual and textual features computed using models such as Inception, ResNet, CNN, VGG-16 and DenseNet for image representation (Morishita et al., 2020; Sharma et al., 2020b; Yuan et al., 2020), and BERT, XLNet, LSTM, GRU and DistilBERT for text representation (Liu et al., 2020; Gundapu and Mamidi, 2020). Due to class imbalance in the dataset, approaches such as GMM and Training Signal Annealing (TSA) were also found useful. Morishita et al. (2020); Bonheme and Grzes (2020); Guo et al. (2020); Sharma et al. (2020b) proposed ensemble learning, whereas Gundapu and Mamidi (2020); De la Pe˜na Sarrac´en et al. (2020) and several others used multimodal approaches. A few others leveraged transfer-learning using pre-trained models such as BERT (Devlin et al., 2019), VGG16 (Simonyan and Ziss"
2021.findings-acl.246,D19-1514,0,0.0592322,"ollection of 5, 020 memes for hate speech detection. Similarly, the Hateful Memes Challenge by Facebook introduced a dataset consisting of 10k+ memes, annotated as hateful or non-hateful (Kiela et al., 2020). The memes were generated artificially, so that they resemble real ones shared on social media, along with “benign confounders.” As part of this challenge, an array of approaches with different architectures and features have been tried, including Visual BERT, ViLBERT, VLP, UNITER, LXMERT, VILLA, ERNIE-Vil, Oscar and other Transformers (Li et al., 2019; Su et al., 2020; Zhou et al., 2020; Tan and Bansal, 2019; Gan et al., 2020; Yu et al., 2021; Li et al., 2020; Vaswani et al., 2017; Lippe et al., 2020; Zhu, 2020; Muennighoff, 2020). Other approaches include multimodal feature augmentation and cross-modal attention mechanism using inferred image descriptions (Das et al., 2020; Sandulescu, 2020; Zhou and Chen, 2020), as well as up-sampling confounders and loss re-weighting to complement multimodality (Lippe et al., 2020), web entity detection along with fair face classification (Karkkainen and Joo, 2021) from memes (Zhu, 2020), cross-validation ensemble learning and semi-supervised learning (Zhong,"
2021.findings-acl.80,S19-2007,0,0.510452,"mmission, 2020) in the EU. Even in the United States, content moderation or the lack thereof can have significant impact on business (e.g., Parler was denied server space), government (U.S. Capitol Riots), and individuals (hate speech is linked to self-harm). Explainability is needed to indicate in detail why content has WARNING: This paper contains tweet examples and words that are offensive in nature. been deleted or flagged as inappropriate. Moreover, users can be educated by such feedback to avoid future biases. There have been several areas of work in the detection of offensive language (Basile et al., 2019; Fortuna and Nunes, 2018; Ranasinghe and Zampieri, 2020), covering overlapping characteristics such as toxicity, hate speech, cyberbullying, and cyber-aggression. Further, using a hierarchical approach to analyze different aspects of the offensive content, such as the type and the target of the offense, helps provide explainability. The Offensive Language Identification Dataset, or OLID, (Zampieri et al., 2019a) is one such example, and it has been widely used in research. OLID contains 14,100 English tweets, which were manually annotated using a three-level taxonomy: A: Offensive Language De"
2021.findings-acl.80,P19-1271,0,0.0187585,"annotation taxonomy. Section 4 introduces the computational models used in this study. Section 5 presents the SOLID dataset. Section 6 discusses the experimental results and Section 6.3 offers additional discussion and analysis. Finally, Section 7 concludes and discusses possible directions for future work. 2 Related Work There have been several recent studies on offensive language detection and related tasks such as hate speech, cyberbulling, aggression, and toxic comment detection. Hate speech identification is by far the most studied abusive language detection task (Ousidhoum et al., 2019; Chung et al., 2019; Mathew et al., 2021). One of the most widely used datasets is the one by Davidson et al. (2017), which contains over 24,000 English tweets labeled as non-offensive, hate speech, and profanity. A recent shared task on the topic is HatEval (Basile et al., 2019). In cyberbullying detection, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics. Dadvar et al. (2013) and Safi Samghabadi et al. (2020) studied the use of the conversational context for detecting cyberbullying. In particular, Dadvar et al. (2013) used userrelated features such as the frequency of profa"
2021.findings-acl.80,2020.lrec-1.758,0,0.105274,"ntext of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Level C is 1/3 smaller than Level A), and the data sizes for B and C are rath"
2021.findings-acl.80,N19-1423,0,0.277554,"w-toxic-comm ent-classification-challenge 916 sented in OLID’s taxonomy. We create a largescale semi-supervised dataset using the same annotation taxonomy as in OLID. 3 Tweet The OLID (Zampieri et al., 2019a) dataset tackles the challenge of detecting offensive language using a labeling schema that classifies each example using the following three-level hierarchy: contribution of SOLID for offensive language identification. We use a suite of heterogeneous machine learning models: PMI (Turney and Littman, 2003), FastText (Joulin et al., 2017), LSTM (Hochreiter and Schmidhuber, 1997), and BERT (Devlin et al., 2019). They have diverse inductive biases, which is an essential prerequisite for our semi-supervised setup (see Section 4.5). We assume that an ensemble of models with different inductive biases decreases each individual model’s bias. Level B: Categorization of Offensive Language Is the offensive text targeted? TIN Targeted insult or threat towards a group or individual. UNT Untargeted profanity or swearing. 4.1 PMI We use a PMI-based model that computes the ngram-based similarity of a tweet to the tweets of a particular class c in the training dataset. The model is considered naïve as it accounts"
2021.findings-acl.80,W18-4416,0,0.0227143,"ification Challenge2 at Kaggle provided participants with comments from Wikipedia annotated using six labels: toxic, severe toxic, obscene, threat, insult, and identity hate. The recent SemEval-2021 Toxic Spans Detection shared task addressed the identification of the token spans that made a post toxic (Pavlopoulos et al., 2021). There were several shared tasks that have focused specifically on offensive language identification. For example, GermEval 2018 (Wiegand et al., 2018) which focused on offensive language identification in German tweets, HASOC 2019 (Mandl et al., 2019), and TRAC 2018 (Fortuna et al., 2018). In this paper, we extend the prior work of the OLID dataset (Zampieri et al., 2019a). OLID is annotated using a hierarchical annotation schema as in (Basile et al., 2019; Mandl et al., 2019). In contrast to prior approaches, it takes both the target and the type of offensive content into account. This allows multiple types of offensive content (e.g., hate speech and cyberbullying) to be repre1 Available at: https://sites.google.com/sit e/offensevalsharedtask/solid 2 http://kaggle.com/c/jigsaw-toxic-comm ent-classification-challenge 916 sented in OLID’s taxonomy. We create a largescale semi-s"
2021.findings-acl.80,L18-1550,0,0.0186439,"use learning rates of 0.00002 for Levels A and B, and 0.00004 for Level C. We apply per-class weights to cope with the data imbalance in Level C as follows: IND=1, GRP=2, OTH=10. We use the Adam optimizer and a linear warm-up schedule with a 0.05 warm-up ratio. LSTM In contrast to the prior models, the LSTM model (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) can account for long-distance relations between words. First is an embedding layer initialized with a concatenation of the GloVe 300-dimensional (Pennington et al., 2014) and FastText’s Common Crawl 300-dimensional embeddings (Grave et al., 2018). It is followed by a dropout and a bi-directional LSTM layer with an attention mechanism on top of it. We concatenate the attention mechanism’s output with averaged and maximum global poolings on the outputs of 918 4.5 Democratic Co-training Democratic co-training (Zhou and Goldman, 2004) is a semi-supervised technique used to create large datasets with noisy labels when provided with a set of diverse models trained in a supervised way. This approach has been successfully applied in tasks like time series prediction with missing data (Mohamed et al., 2007), early prognosis of academic perform"
2021.findings-acl.80,E17-2068,0,0.327106,"s.google.com/sit e/offensevalsharedtask/solid 2 http://kaggle.com/c/jigsaw-toxic-comm ent-classification-challenge 916 sented in OLID’s taxonomy. We create a largescale semi-supervised dataset using the same annotation taxonomy as in OLID. 3 Tweet The OLID (Zampieri et al., 2019a) dataset tackles the challenge of detecting offensive language using a labeling schema that classifies each example using the following three-level hierarchy: contribution of SOLID for offensive language identification. We use a suite of heterogeneous machine learning models: PMI (Turney and Littman, 2003), FastText (Joulin et al., 2017), LSTM (Hochreiter and Schmidhuber, 1997), and BERT (Devlin et al., 2019). They have diverse inductive biases, which is an essential prerequisite for our semi-supervised setup (see Section 4.5). We assume that an ensemble of models with different inductive biases decreases each individual model’s bias. Level B: Categorization of Offensive Language Is the offensive text targeted? TIN Targeted insult or threat towards a group or individual. UNT Untargeted profanity or swearing. 4.1 PMI We use a PMI-based model that computes the ngram-based similarity of a tweet to the tweets of a particular clas"
2021.findings-acl.80,W18-4401,1,0.910821,"Missing"
2021.findings-acl.80,2020.trac-1.1,1,0.909142,"Missing"
2021.findings-acl.80,2021.wanlp-1.13,0,0.170384,"ts only for the ngram frequencies in the discrete token space and only in the context of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Le"
2021.findings-acl.80,2020.lrec-1.430,0,0.0212663,"crete token space and only in the context of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Level C is 1/3 smaller than Level A), and the data sizes for"
2021.findings-acl.80,D19-1474,0,0.0248631,"es the OLID dataset and annotation taxonomy. Section 4 introduces the computational models used in this study. Section 5 presents the SOLID dataset. Section 6 discusses the experimental results and Section 6.3 offers additional discussion and analysis. Finally, Section 7 concludes and discusses possible directions for future work. 2 Related Work There have been several recent studies on offensive language detection and related tasks such as hate speech, cyberbulling, aggression, and toxic comment detection. Hate speech identification is by far the most studied abusive language detection task (Ousidhoum et al., 2019; Chung et al., 2019; Mathew et al., 2021). One of the most widely used datasets is the one by Davidson et al. (2017), which contains over 24,000 English tweets labeled as non-offensive, hate speech, and profanity. A recent shared task on the topic is HatEval (Basile et al., 2019). In cyberbullying detection, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics. Dadvar et al. (2013) and Safi Samghabadi et al. (2020) studied the use of the conversational context for detecting cyberbullying. In particular, Dadvar et al. (2013) used userrelated features such as th"
2021.findings-acl.80,2021.semeval-1.6,0,0.0283613,"omments in English and Hindi for training and validation. Facebook and Twitter datasets were used for testing. The goal was to discriminate between three classes: non-aggressive, covertly aggressive, and overly aggressive. Two other shared tasks addressed toxic language. The Toxic Comment Classification Challenge2 at Kaggle provided participants with comments from Wikipedia annotated using six labels: toxic, severe toxic, obscene, threat, insult, and identity hate. The recent SemEval-2021 Toxic Spans Detection shared task addressed the identification of the token spans that made a post toxic (Pavlopoulos et al., 2021). There were several shared tasks that have focused specifically on offensive language identification. For example, GermEval 2018 (Wiegand et al., 2018) which focused on offensive language identification in German tweets, HASOC 2019 (Mandl et al., 2019), and TRAC 2018 (Fortuna et al., 2018). In this paper, we extend the prior work of the OLID dataset (Zampieri et al., 2019a). OLID is annotated using a hierarchical annotation schema as in (Basile et al., 2019; Mandl et al., 2019). In contrast to prior approaches, it takes both the target and the type of offensive content into account. This allo"
2021.findings-acl.80,D14-1162,0,0.0926922,"We fine-tune BERT for 2, 3, and 3 epochs for Level A, B, and C, respectively. We use learning rates of 0.00002 for Levels A and B, and 0.00004 for Level C. We apply per-class weights to cope with the data imbalance in Level C as follows: IND=1, GRP=2, OTH=10. We use the Adam optimizer and a linear warm-up schedule with a 0.05 warm-up ratio. LSTM In contrast to the prior models, the LSTM model (Hochreiter and Schmidhuber, 1997; Vaswani et al., 2017) can account for long-distance relations between words. First is an embedding layer initialized with a concatenation of the GloVe 300-dimensional (Pennington et al., 2014) and FastText’s Common Crawl 300-dimensional embeddings (Grave et al., 2018). It is followed by a dropout and a bi-directional LSTM layer with an attention mechanism on top of it. We concatenate the attention mechanism’s output with averaged and maximum global poolings on the outputs of 918 4.5 Democratic Co-training Democratic co-training (Zhou and Goldman, 2004) is a semi-supervised technique used to create large datasets with noisy labels when provided with a set of diverse models trained in a supervised way. This approach has been successfully applied in tasks like time series prediction w"
2021.findings-acl.80,2020.lrec-1.629,1,0.862054,"frequencies in the discrete token space and only in the context of n neighboring tokens. We compute the PMI score (Turney and Littman, 2003) of each n-gram in the training set w.r.t. each class:  P M I(wi , cj ) = log2 4 Models In this section, we describe the models used for semi-supervised annotation and for evaluating the C Table 1: Examples from the OLID dataset. Level A: Offensive Language Detection Is the text offensive? OFF Inappropriate language, insults, or threats. NOT Neither offensive, nor profane. The taxonomy was successfully adopted for several languages (Mubarak et al., 2021; Pitenis et al., 2020; Sigurbergsson and Derczynski, 2020; Çöltekin, 2020), and it was used in a series of shared tasks (Zampieri et al., 2019b; Mandl et al., 2019). Tweets from the OLID dataset labeled with the taxonomy are shown in Table 1. The OLID dataset consists of 13,241 training and 860 test tweets. Table 2 presents detailed statistics about the distribution of the labels. There is a substantial class imbalance on each level of annotation, especially at Level B. Furthermore, there is a sizable difference in the total number of annotations between the levels due to the schema (e.g., Level C is 1/3 smaller t"
2021.findings-acl.80,2020.emnlp-main.470,1,0.91373,"Missing"
2021.findings-acl.80,N12-1084,0,0.0385142,"2 Related Work There have been several recent studies on offensive language detection and related tasks such as hate speech, cyberbulling, aggression, and toxic comment detection. Hate speech identification is by far the most studied abusive language detection task (Ousidhoum et al., 2019; Chung et al., 2019; Mathew et al., 2021). One of the most widely used datasets is the one by Davidson et al. (2017), which contains over 24,000 English tweets labeled as non-offensive, hate speech, and profanity. A recent shared task on the topic is HatEval (Basile et al., 2019). In cyberbullying detection, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics. Dadvar et al. (2013) and Safi Samghabadi et al. (2020) studied the use of the conversational context for detecting cyberbullying. In particular, Dadvar et al. (2013) used userrelated features such as the frequency of profanity in previous messages. More recent work has addressed the issues of scalable and timely detection of cyberbullying in online social networks. To this end, Rafiq et al. (2018) employed a dynamic priority scheduler, and Yao et al. (2019) proposed a sequential hypothesis testing. Safi Samghabadi et al. (2"
2021.findings-acl.80,N19-1144,1,0.929775,"deleted or flagged as inappropriate. Moreover, users can be educated by such feedback to avoid future biases. There have been several areas of work in the detection of offensive language (Basile et al., 2019; Fortuna and Nunes, 2018; Ranasinghe and Zampieri, 2020), covering overlapping characteristics such as toxicity, hate speech, cyberbullying, and cyber-aggression. Further, using a hierarchical approach to analyze different aspects of the offensive content, such as the type and the target of the offense, helps provide explainability. The Offensive Language Identification Dataset, or OLID, (Zampieri et al., 2019a) is one such example, and it has been widely used in research. OLID contains 14,100 English tweets, which were manually annotated using a three-level taxonomy: A: Offensive Language Detection B: Categorization of Offensive Language C: Offensive Language Target Identification The taxonomy proposed in OLID makes it possible to represent different kinds of offensive content as a function of the type and the target of a post. For example, offensive messages targeting a group are likely hate speech, whereas offensive messages targeting an individual are likely cyberbullying. OLID has been used to"
2021.findings-acl.80,S19-2010,1,0.931649,"deleted or flagged as inappropriate. Moreover, users can be educated by such feedback to avoid future biases. There have been several areas of work in the detection of offensive language (Basile et al., 2019; Fortuna and Nunes, 2018; Ranasinghe and Zampieri, 2020), covering overlapping characteristics such as toxicity, hate speech, cyberbullying, and cyber-aggression. Further, using a hierarchical approach to analyze different aspects of the offensive content, such as the type and the target of the offense, helps provide explainability. The Offensive Language Identification Dataset, or OLID, (Zampieri et al., 2019a) is one such example, and it has been widely used in research. OLID contains 14,100 English tweets, which were manually annotated using a three-level taxonomy: A: Offensive Language Detection B: Categorization of Offensive Language C: Offensive Language Target Identification The taxonomy proposed in OLID makes it possible to represent different kinds of offensive content as a function of the type and the target of a post. For example, offensive messages targeting a group are likely hate speech, whereas offensive messages targeting an individual are likely cyberbullying. OLID has been used to"
2021.findings-emnlp.379,2020.acl-main.308,1,0.795974,"19, with aditional examples and a new topic (US Politics), thus ending up with two datasets: Harm-C and Harm-P. • • • • 2 2.1 Related Work Harm and Multimodality Various aspects of harm, such as hate speech, misinformation, and offensiveness, have been studied in isolation. Ahn and Jang (2019) addressed harmfulness in terms of obscenity and violence using multimodal approaches involving video and images. Hirschberg et al. (2005), Kopev et al. (2019), and Dinkov et al. (2019) studied intentional deception and bias using textual and acoustic cues from the speech signal. Gogate et al. (2017) and Baly et al. (2020) designed robust systems for deception detection by combining acoustic, textual, and other information (visual, social). In recent work on detecting offensiveness in memes, Suryawanshi et al. (2020) showed improvements using an early-fusion multimodal approach that combines representations from unimodal models. Critical aspects such as prevalence of racial biases within the datasets and the modeling approaches were addressed in (Mills and Unsworth, 2018; Davidson et al., 2019; Mozafari et al., 2020; Xia et al., 2020; Zhou et al., 2021a); they characterized the biases and proposed de-biasing me"
2021.findings-emnlp.379,C18-1201,0,0.0194135,"ud.google.com/vision/docs/ and its OCR-extracted text T , we extract a CLIP object-localizer 9 image embedding FI and a CLIP text embedding cloud.google.com/vision/docs/ FT ; both FI and FT are 512-dimensional vectors. detecting-web 4443 5.4 (a) Detected faces, foreground objects and image attributes for a harmless meme from the Harm-C dataset. Cross-Modality Attention Fusion For some memes, the text modality is more relevant, while for others, the image plays a crucial role. CMAF uses an attention mechanism to fuse the representations from the textual and the visual modalities. Motivated by (Gu et al., 2018), we design our CMAF module with two major parts: modality attention generation and weighted feature concatenation. In the first part, we use a sequence of dense layers followed by a softmax layer to generate the attention scores [av , at ] for the two modalities. In the second part, we weigh the original unimodal features using their respective attention scores and we concatenate them together. We also use residual connections for better gradient flow. V res FM (4) eme = (1 + av )FI T res FM eme = (1 + at )FT (b) Detected faces, foreground objects and image attributes for a very harmful meme"
2021.findings-emnlp.379,2021.findings-acl.246,1,0.747394,"Missing"
2021.findings-emnlp.379,N16-3020,0,0.045261,"harmful memes and their targets. We further proposed MOMENTA, a novel multimodal deep neural network that systematically analyzes the local and the global perspective of the input meme (in both modalities) and relates it to the background context. Extensive experiments on the two datasets showed the efficacy of MOMENTA, which outperforms ten baselines for both tasks. We further demonstrated its transferability and interpretability. In future work, we plan to extend the datasets with more domains and languages. We visualize the explainability of MOMENTA and we compare it to ViLBERT using LIME (Ribeiro et al., 2016). We take the example from Figure 3b for our analysis. MOMENTA correctly classified it as very harmful with a dominant probability of 0.673, but ViLBERT fails. Figures 4a and 4b highlight the most important super-pixels contributing to the decision of MOMENTA and ViLBERT, respectively. We notice that the face of Joe Biden and the manAcknowledgments nequin, which are presented in a very insulting way The work was partially supported by a Wipro rein this meme, contribute heavily to the prediction search grant, the Infosys Centre for AI, IIIT Delhi, of MOMENTA. However, as Biden’s face is partial"
2021.findings-emnlp.379,2021.naacl-main.185,0,0.0124288,"s, respectively. Self-supervised pre-training using crossmodal and multimodal information saw an early reinstation with the work of Frome et al. (2013), where se- 4.1 Data Collection and Deduplication. mantic information from vast unannotated textual To collect potentially harmful memes, we condata was leveraged to classify images in a zeroducted keyword-based1 web search on different shot setup. Similarly, Natural Language Processsources, mainly Google Image. To alleviate poing (NLP) recently saw the emergence of Patterntential biases from this search, we intentionally inExploiting Learning (Schick and Schütze, 2021), cluded non-harmful examples using the same keywhich allows smaller models to outperform much words. We used an extension2 of Google Chrome to larger ones such as GPT-3 (Brown et al., 2020) download the images. We further scraped various when fine-tuned using a very small number of expublicly available meme pages on Reddit, Faceamples in a few-shot learning setup. book, and Instagram. Unlike the Hateful Memes There have been also innovations towards better Challenge (Kiela et al., 2020), which offered synmultimodal systems. Ramesh et al. (2021) prothetically generated memes, our datasets cont"
2021.findings-emnlp.379,2020.semeval-1.99,1,0.828478,"Missing"
2021.findings-emnlp.379,P18-1238,0,0.025407,"Missing"
2021.findings-emnlp.379,D19-1514,0,0.0266033,"with data augmentation. Lippe et al. (2020) found UNITER (Chen et al., 2020) to be a very strong We perform extensive experiments on both choice for multimodal content. Sandulescu (2020) datasets, and we show that MOMENTA outperused a multimodal deep ensemble, while examining forms the ten baselines in terms of accuracy both single-stream models such as ViLBERT (Lu by 1.3–2.6 points absolute for both tasks. et al., 2019), VLP (Zhou et al., 2020), and UNITER Finally, we establish the generalizability and (Chen et al., 2020), and dual-stream models like the interpretability of MOMENTA. LXMERT (Tan and Bansal, 2019). 4440 Wang et al. (2021) proposed a multimodal deep neural network with semantic and task-level attention for detecting medical misinformation. Another shared task, on memotion analysis (Sharma et al., 2020), asked to recognize expressive emotions via sentiment (positive, negative, neutral), type of emotion (sarcastic, funny, offensive, motivation), and their intensity. Recently, Chandra et al. (2021) investigated antisemitism, its subtypes, and its use in memes. However, none of these studies addressed the broader concept of harmful memes. In our previous work (Pramanick et al., 2021b), we d"
2021.findings-emnlp.379,2020.socialnlp-1.2,0,0.0206482,"g textual and acoustic cues from the speech signal. Gogate et al. (2017) and Baly et al. (2020) designed robust systems for deception detection by combining acoustic, textual, and other information (visual, social). In recent work on detecting offensiveness in memes, Suryawanshi et al. (2020) showed improvements using an early-fusion multimodal approach that combines representations from unimodal models. Critical aspects such as prevalence of racial biases within the datasets and the modeling approaches were addressed in (Mills and Unsworth, 2018; Davidson et al., 2019; Mozafari et al., 2020; Xia et al., 2020; Zhou et al., 2021a); they characterized the biases and proposed de-biasing mechanisms for tasks such as detecting toxic/abusive language and hate speech, as well as for identifying racial prejudices. Finally, recent research and a shared task focused on propaganda in memes (Dimitrov et al., 2021a,b), but did not target harmfulness per se. 2.2 Harm and Memes There was a recent shared task on troll meme classification (Suryawanshi and Chakravarthi, 2021), and two tasks on hateful meme detection: (Kiela We benchmark the two datasets against ten et al., 2020) and (Zhou et al., 2021b). A numstate"
2021.findings-emnlp.379,P19-1355,0,0.0132126,"Our high inter-annotator agreement makes us confident that the labeling of the data is correct most of the time. Intended Use. We release our dataset aiming to encourage research in studying harmful memes on the web. We distribute the dataset for research purposes only, without a license for commercial use. We believe that it represents a useful resource when used in the appropriate manner. Environmental Impact. Finally, we would also like to warn that the use of large-scale Transformers requires a lot of computations and the use of GPUs/TPUs for training, which contributes to global warming (Strubell et al., 2019). This is a bit less of an issue in our case, as we do not train such models from scratch; rather, we fine-tune them on relatively small datasets. Moreover, running on a CPU for inference, once the model has been finetuned, is perfectly feasible, and CPUs contribute much less to global warming. References Byeongtae Ahn and Seok-Woo Jang. 2019. Multimodal approach for multimedia injurious contents blocking. Multimedia Tools and Applications, 79:16459–16472. Michael A Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. 2019. Strike (with) a pose: Neural networks ar"
2021.findings-emnlp.379,2021.dravidianlangtech-1.16,0,0.178847,"ough to understand it. 4439 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4439–4455 November 7–11, 2021. ©2021 Association for Computational Linguistics Second, unlike other multimodal tasks, the image and the textual content in the meme are often uncorrelated, and its overall semantics is presented holistically. Finally, real-world memes can be noisy, and the text embedded in them can be hard to extract using standard OCR tools. The proliferation of virulent memes has stimulated research focusing on their dark sides: hate (Kiela et al., 2020) and offensiveness (Suryawanshi and Chakravarthi, 2021). Recently, Pramanick et al. (2021b) defined the notion of harmful meme and demonstrated its dependency on the background context. For example, the meme in Figure 1a is somewhat harmful to Joe Biden in the context of an election, but it is arguably neither hateful nor offensive. Moreover, the notion of harm is often apparent only when the two modalities are combined. For example, in Figure 1b, the unimodal cues are not harmful, but the meme as a whole is harmful to Donald Trump. Moreover, identifying the target of harmful memes (e.g., Joe Biden and Donald Trump) requires separate analysis, whi"
2021.findings-emnlp.379,2021.eacl-main.274,0,0.104992,"groups, e.g., minorities. Despite memes being so influential, their multimodal nature and camouflaged semantics makes them very challenging to analyze. The abundant quantity, fecundity and escalating diversity of online memes has led to a growing body of research on meme analysis, which has focused on tasks such as meme emotion analysis (Sharma et al., 2020; Pramanick et al., 2021a), sar1 Introduction castic meme detection (Kumar and Garg, 2019), The growing popularity of social media platforms and hateful meme detection (Kiela et al., 2020; has given rise to a new form of multimodal entity: Zhou et al., 2021b; Velioglu and Rose, 2020). Rethe meme, which is an image, embedded with a search on these problems has shown that off-theshort piece of text. Memes are easily shared and shelf multimodal systems, which often perform can spread fast on the Internet, especially in social well on a range of visual-linguistic tasks, struggle media. They are typically humorous and amusing when applied to memes. There are a number of reain nature; however, by using an adroit combination sons for that. First, memes are context-dependent, of images and texts in the context of contemporary and thus focusing only on t"
2021.findings-emnlp.379,2020.trac-1.6,0,0.0362329,"as hate speech, misinformation, and offensiveness, have been studied in isolation. Ahn and Jang (2019) addressed harmfulness in terms of obscenity and violence using multimodal approaches involving video and images. Hirschberg et al. (2005), Kopev et al. (2019), and Dinkov et al. (2019) studied intentional deception and bias using textual and acoustic cues from the speech signal. Gogate et al. (2017) and Baly et al. (2020) designed robust systems for deception detection by combining acoustic, textual, and other information (visual, social). In recent work on detecting offensiveness in memes, Suryawanshi et al. (2020) showed improvements using an early-fusion multimodal approach that combines representations from unimodal models. Critical aspects such as prevalence of racial biases within the datasets and the modeling approaches were addressed in (Mills and Unsworth, 2018; Davidson et al., 2019; Mozafari et al., 2020; Xia et al., 2020; Zhou et al., 2021a); they characterized the biases and proposed de-biasing mechanisms for tasks such as detecting toxic/abusive language and hate speech, as well as for identifying racial prejudices. Finally, recent research and a shared task focused on propaganda in memes ("
2021.findings-emnlp.56,2021.nlp4if-1.9,1,0.705503,"tions. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino"
2021.findings-emnlp.56,N19-1216,1,0.795082,"tweets in Arabic, Bulgarian, Dutch, and English, and we are making it freely available to the research community. We further reported a number of evaluation results for all languages using various transformer architectures. Moreover, we performed advanced experiments, including multilingual training, modeling the Twitter context, the use of propagandistic language, and whether the user is likely to be a bot, as well as multitask learning. In future work, we plan to explore multimodality and explainability (Yu et al., 2021). We further want to model the task as a multitask ordinal regression (Baly et al., 2019), as Q2–Q5 are defined on an ordinal scale. Moreover, we would like to put the data and the system in some practical use; in fact, we have already used them to analyze disinformation about COVID-19 in Bulgaria (Nakov et al., 2021a) and Qatar (Nakov et al., 2021b). Finally, the data will be used in a shared task at the CLEF2022 CheckThat! lab; part of it was used for the NLP4IF-2021 shared task (Shaar et al., 2021a). Acknowledgments We thank Akter Fatema, Al-Awthan Ahmed, AlDobashi Hussein, El Messelmani Jana, Fayoumi 6.3 Multitask Learning Sereen, Mohamed Esraa, Ragab Saleh, and Shurafa For th"
2021.findings-emnlp.56,N18-2004,1,0.90491,"Missing"
2021.findings-emnlp.56,2020.acl-main.747,0,0.0346481,"a URL, and the factuality of the website it points to.4 Models Large-scale pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96"
2021.findings-emnlp.56,2020.semeval-1.186,1,0.850189,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,2020.acl-demos.32,1,0.926725,"., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER"
2021.findings-emnlp.56,S19-2147,0,0.0285024,"s, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers, and the society. mors (Derczynski et al., 2017; Gorrell et al., 2019), propaganda detection in news articles and memes • We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar"
2021.findings-emnlp.56,2021.wanlp-1.9,0,0.0367297,"llected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter datasets: some unlabeled (Chen et al., 2020; Banda et al., 2021; Haouari et al., 2021), some automatically labeled with location information (Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting me"
2021.findings-emnlp.56,2020.nlpcovid19-2.11,0,0.043054,"Abdul-Mageed et al., 2021; Qazi et al., 2020), some labeled using distant supervision (Cinelli et al., 2020; Zhou et al., 2020), and some manually annotated (Song et al., 2020; Vidgen et al., 2020; Shahi and Nandini, 2020; Pulido et al., 2020; Dharawat et al., 2020). There is also work on credibility (Cinelli et al., 2020; Pulido et al., 2020; Zhou et al., 2020), racial prejudices and fear (Medford et al., 2020; Vidgen et al., 2020), as well as situational information, e.g., caution and advice (Li et al., 2020), as well as on detecting mentions and stance with respect to known misconceptions (Hossain et al., 2020). The closest work to ours is that of Song et al. (2020), who collected false and misleading claims about COVID-19 from IFCN Poynter, and annotated them as (1) Public authority, (2) Community spread and impact, (3) Medical advice, selftreatments, and virus effects, (4) Prominent actors, (5) Conspiracies, (6) Virus transmission, (7) Virus Figure 2: The keywords used to collect the tweets. origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests. These categories partially overlap with ours, but account 3.2 Annotation Task for less perspectives. Moreover, we c"
2021.findings-emnlp.56,N18-5006,1,0.802211,"tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (typically English; except for CLEF), and did not focus on COVID-19. Check-Worthiness Estimation Another relevant research line is on detecting check-worthy claims in political debates using manual annotations (Hassan et al., 2015) or by observing the selection of fact-checkers (Gencheva et al., 2017; Patwari et al., 2017; Jaradat et al., 2018; Vasileva et al., 2019). 3 3.1 Dataset Data Collection We collected tweets by specifying a target language (English, Arabic, Bulgarian, or Dutch), a set of COVID-19 related keywords, as shown in Figure 2, and different time frames: from January 2020 till March 2021. We collected original tweets (no retweets or replies), we removed duplicates using a similarity-based approach (Alam et al., 2021b), and we filtered out tweets with less than five words. Finally, we selected the most frequently liked and retweeted tweets for annotation. COVID-19 Research There are a number of COVID-19 Twitter data"
2021.findings-emnlp.56,E17-2068,0,0.0257945,"le pretrained Transformer models have achieved state-of-the-art performance for several NLP tasks. We experimented with several such models to evaluate their efficacy under various training scenarios such as, binary vs. multiclass classification, multilingual setup, etc. We used BERT (Devlin et al., 2019) and RoBERTa for English, AraBERT (Antoun et al., 2020) for Arabic, and BERTje (de Vries et al., 2019) for Dutch. We further used multilingual transformers such as (Liu et al., 2019), multilingual BERT (mBERT) and XLM-r (Conneau et al., 2020). Finally, we used static embeddings from FastText (Joulin et al., 2017). 616 4 From http://mediabiasfactcheck.com English Q. Cls. Arabic Maj. FT BT RT Bulgarian Maj. FT ArBT XLM-r Dutch Maj. FT mBT XLM-r Maj. FT BTje XLM-r Binary (Coarse-grained) Q1 Q2 Q3 Q4 Q5 Q6 Q7 2 2 2 2 2 2 2 Avg. 48.7 91.6 96.3 66.7 67.7 86.7 78.3 77.7 89.0 69.3 96.3 83.8 92.1 80.6 76.5 92.1 96.4 85.6 80.6 88.9 85.5 78.6 92.7 96.9 89.0 84.4 90.5 86.1 76.6 84.1 86.5 88.3 83.8 84.0 96.0 90.3 65.9 88.9 77.4 84.2 83.1 96.3 89.0 66.7 89.8 77.4 58.3 95.0 96.5 86.8 70.5 83.2 80.1 84.0 94.7 96.0 87.7 80.5 84.5 81.6 87.6 95.0 96.5 88.4 82.9 85.1 81.7 36.5 64.9 62.3 63.9 44.4 84.7 65.6 75.4 75.1 76.9"
2021.findings-emnlp.56,2020.emnlp-demos.2,0,0.0153692,"80.2 69.2 68.3 Finally, we should note the strong performance Avg. 73.3 73.1 60.7 59.8 71.4 71.5 55.3 54.9 of context-free models such as FastText. We believe that it is suitable for the noisy text of Table 6: Multilingual experiments using mBERT. tweets due to its ability to model not only words Shown are results for monolingual vs. multilingual models (weighted F1 ). Mul is trained on the combined but also character n-grams. In future work, we English, Arabic, Bulgarian, and Dutch data. plan to try transformers specifically trained on tweets and/or on COVID-19 related data such as BERTweet (Nguyen et al., 2020) and COVID5 Twitter-BERT (Müller et al., 2020). We also tried XLM-r, but it performed worse. 618 6.2 Twitter/Propagandistic/Botometer We conducted experiments with Twitter, propaganda, and botness features alongside the posteriors from the BERT classifier, which we combined using XGBoost (Chen and Guestrin, 2016). The results are shown in Table 7. We can see that many of the combinations yielded improvements, with botness being the most useful, followed by propaganda, and finally by the Twitter object features. Binary (Coarse-grained) Q. Cls BERT B+TF B+Prop B+Bot B+All Q1 Q2 Q3 Q4 Q5 Q6 Q7 2"
2021.findings-emnlp.56,D17-1317,0,0.0286494,"onversations with a Ministry of Public Health. Our contributions can be summarized as follows: 2 Related Work Fact-Checking Research on fact-checking claims is largely based on datasets mined from major fact-checking organizations. Some of the larger datasets include the Liar, Liar dataset of 12.8K claims from PolitiFact (Wang, 2017), the ClaimsKG dataset and system (Tchechmedjiev et al., 2019) of 28K claims from eight factchecking organizations, the MultiFC dataset of 38K claims from 26 fact-checking organizations (Augenstein et al., 2019), and the 10K claims Truth of Various Shades dataset (Rashkin et al., 2017). There have been also datasets for other languages, • We develop a large manually annotated created in a similar fashion, e.g., for Arabic (Baly dataset of 16K tweets related to the COVID- et al., 2018; Alhindi et al., 2021). 19 infodemic in four languages (Arabic, BulA number of datasets were created as part of garian, Dutch, and English), using a schema shared tasks. In most cases, they performed their that combines the perspective of journalists, own annotation, either (a) manually, e.g., the Sefact-checkers, social media platforms, policymEval tasks on determining the veracity of rumakers"
2021.findings-emnlp.56,2021.nlp4if-1.12,1,0.887854,"We demonstrate sizable performance gains over popular deep contextualized text repre- (Da San Martino et al., 2020a; Dimitrov et al., sentations (such as BERT), when using mul- 2021a,b), fact-checking in community question answering forums (Mihaylova et al., 2019), the CLEF titask learning, cross-language learning, and when modeling the social context of the tweet, CheckThat! lab on identification and verification of claims (Nakov et al., 2018; Elsayed et al., 2019; as well as the propagandistic nature of the Barrón-Cedeño et al., 2020; Shaar et al., 2020; language used. Nakov et al., 2021c; Shaar et al., 2021b,c), or (b) us• We make our data and code freely available.1 ing crowdsourcing, e.g., the FEVER task on fact ex1 traction and verification, focusing on claims about https://github.com/firojalam/ COVID-19-disinformation Wikipedia content (Thorne et al., 2018, 2019). 612 Unlike our work, the above datasets did not focus on tweets (they used claims from news, speeches, political debates, community question answering fora, or were just made up by human annotators; RumourEval is a notable exception), targeted factuality only (we cover a number of other issues), were limited to a single language (t"
2021.nlp4if-1.12,2020.findings-emnlp.58,0,0.0684544,"Missing"
2021.nlp4if-1.12,2021.wanlp-1.8,0,0.0202452,"her use distant supervision, and very few are manually annotated. Cinelli et al. (2020) studied COVID-19 rumor amplification in five social media platforms; their data was labeled using distant supervision. Other datasets include a multi-lingual dataset of 123M tweets (Chen et al., 2020), another one of 383M tweets (Banda et al., 2020), a billion-scale dataset of 65 languages and 32M geo-tagged tweets (Abdul-Mageed et al., 2021), and the GeoCoV19 dataset, consisting of 524M multilingual tweets, including 491M with GPS coordinates (Qazi et al., 2020). There are also Arabic datasets, both with (Haouari et al., 2021; Mubarak and Hassan, 2021) and without manual annotations (Alqurashi et al., 2020). We are not aware of Bulgarian datasets. Zhou et al. (2020) created the ReCOVery dataset, which combines 2,000 news articles about COVID19, annotated for their factuality, with 140,820 tweets. Vidgen et al. (2020) studied COVID-19 prejudices using a manually labeled dataset of 20K tweets with the following labels: hostile, criticism, prejudice, and neutral. 2.2 Censorship Detection There has been a lot of research aiming at developing strategies to detect and to evade censorship. Most work has focused on exploi"
2021.nlp4if-1.12,2021.nlp4if-1.17,0,0.140699,"0.803 0.669 0.599 0.480 0.819 0.748 0.672 0.631 0.687 0.556 0.399 0.678 0.605 0.606 0.606 0.650 0.303 0.498 0.706 0.686 0.630 0.630 0.700 0.631 0.528 3 4 Table 6: Task 1, Bulgarian: Evaluation results. For Q1 to Q7 results are in terms of weighted F1 score. Team HunterSpeechLab (Panda and Levitan, 2021) participated in all three languages. They explored the cross-lingual generalization ability of multitask models trained from scratch (logistic regression, transformers) and pre-trained models (English BERT, mBERT) for deception detection. They were 2nd for Arabic and Bulgarian. Team iCompass (Henia and Haddad, 2021) had a late submission for Arabic, and would have ranked 2nd. They used contextualized text representations from ARBERT, MARBERT, AraBERT, Arabic ALBERT and BERT-base-arabic, which they fine-tuned on the training data for task 1. They found that BERT-base-arabic performed best. 87 1. 2. 3. 4. 7. Team NARNIA (Kumar et al., 2021) experimented with a number of Deep Learning models, including different word embeddings such as Glove and ELMo, among others. They found that the BERTweet model achieved the best overall F1score of 0.881, securing them the third place on the English subtask. TOKOFOU dun"
2021.nlp4if-1.12,2020.osact-1.2,0,0.0468958,"tasks, similar to the shared task’s topic (e.g., hate speech and sarcasm detection). They fine-tuned each of these models on the task 1 training data, projecting a label from the sequence classification token for each of the seven questions in parallel. After model selection on the basis of development set F1 performance, they combined the models in a majority-class ensemble. Table 3: Task 2: Topics featured in the dataset. 4 Task Organization The Arabic Winner: Team R00 had the best performing system for Arabic. They used an ensemble of the follwoing fine-tuned Arabic transformers: AraBERT (Antoun et al., 2020), AsafayaBERT (Safaya et al., 2020), ARBERT. In addition, they also experimented with MARBERT (AbdulMageed et al., 2020). In this section, we describe the overall task organization, phases, and evaluation measures. 4.1 Task Phases We ran the shared tasks in two phases: Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and"
2021.nlp4if-1.12,2020.nlpcovid19-2.11,0,0.24614,"racies, (6) Virus transmission, (7) Virus origins and properties, (8) Public reaction, and (9) Vaccines, medical treatments, and tests, and (10) Cannot determine. Another related dataset study by (Pulido et al., 2020) analyzed 1,000 tweets and categorized them based on factuality into the following categories: (i) False information, (ii) Science-based evidence, (iii) Fact-checking tweets, (iv) Mixed information, (v) Facts, (vi) Other, and (vii) Not valid. Ding et al. (2020) have a position paper discussing the challenges in combating the COVID-19 infodemic in terms of data, tools, and ethics. Hossain et al. (2020) developed the COVIDLies dataset by matching a known misconceptions with tweets, and manually annotated the tweets with stance: whether the target tweet agrees, disagrees, or has no position with respect to a known misconception. Finally, (Shuja et al., 2020) provided a comprehensive survey categorizing the COVID-19 literature into four groups: diagonisis related, transmission and mobility, social media analysis, and knowledge-based approaches. The most relevant previous work is (Alam et al., 2021b, 2020), where tweets about COVID-19 in Arabic and English were annotated based on an annotation"
2021.nlp4if-1.12,2021.nlp4if-1.13,0,0.121497,"rformance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and the participants were given a few days to submit their predictions. The Bulgarian Winner: We did not receive a submission for the best performing team for Bulgarian. The second best team, HunterSpeechLab (Panda and Levitan, 2021), explored the crosslingual generalization ability of multitask models trained from scratch (logistic regression, transformer encoder) and pre-trained models (English BERT, and mBERT) for deception detection. 4.2 5.3 Evaluation Measures DamascusTeam (Hussein et al., 2021) used a two-step pipeline, where the first step involves a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text. In the second step, a version of AraBERT is fine-tuned and used to classify the tweets. Their system was ranked 5th for Arabic. The official evaluation measure for task 1 was the average of the weighted F1 scores for each of the seven questions; for task 2, it was accuracy. 5 Evaluation Results for Task 1 Below, we describe the baselines, the evaluation results, and the best systems for each language. 5.1 Team dunder_miffli"
2021.nlp4if-1.12,2021.louhi-1.1,0,0.0248397,"ision, and very few are manually annotated. Cinelli et al. (2020) studied COVID-19 rumor amplification in five social media platforms; their data was labeled using distant supervision. Other datasets include a multi-lingual dataset of 123M tweets (Chen et al., 2020), another one of 383M tweets (Banda et al., 2020), a billion-scale dataset of 65 languages and 32M geo-tagged tweets (Abdul-Mageed et al., 2021), and the GeoCoV19 dataset, consisting of 524M multilingual tweets, including 491M with GPS coordinates (Qazi et al., 2020). There are also Arabic datasets, both with (Haouari et al., 2021; Mubarak and Hassan, 2021) and without manual annotations (Alqurashi et al., 2020). We are not aware of Bulgarian datasets. Zhou et al. (2020) created the ReCOVery dataset, which combines 2,000 news articles about COVID19, annotated for their factuality, with 140,820 tweets. Vidgen et al. (2020) studied COVID-19 prejudices using a manually labeled dataset of 20K tweets with the following labels: hostile, criticism, prejudice, and neutral. 2.2 Censorship Detection There has been a lot of research aiming at developing strategies to detect and to evade censorship. Most work has focused on exploiting technological limitati"
2021.nlp4if-1.12,2021.nlp4if-1.14,0,0.0895043,"Missing"
2021.nlp4if-1.12,W18-4202,1,0.829941,"uld be factchecked, for example “The sky is blue.”, albeit being a claim, is not interesting to the general public and thus should not be fact-checked. Finally, there has been research that uses linguistic and content clues to detect censorship. Knockel et al. (2015) and Zhu et al. (2013) proposed detection mechanisms to categorize censored content and to automatically learn keywords that get censored. Bamman et al. (2012) uncovered a set of politically sensitive keywords and found that the presence of some of them in a Weibo blogpost contributed to a higher chance of the post being censored. Ng et al. (2018b) also targeted a set of topics that had been suggested to be sensitive, but unlike Bamman et al. (2012), they covered areas not limited to politics. Ng et al. (2018b), Ng et al. (2019), and Ng et al. (2020) investigated how the textual content might be relevant to censorship decisions when both censored and uncensored blogposts include the same sensitive keyword(s). 4. Harmfulness: To what extent is the tweet harmful to the society/person(s)/company(s)/product(s)? The purpose of this question is to determine whether the content of the tweet aims to and can negatively affect the society as a"
2021.nlp4if-1.12,W19-2105,1,0.881735,"Missing"
2021.nlp4if-1.12,C18-1283,0,0.0193051,"glish, and we further add an additional language: Bulgarian. In this section, we discuss studies relevant to the COVID-19 infodemic and to censorship detection. 2.1 COVID-19 Infodemic Disinformation, misinformation, and “fake news” thrive in social media. Lazer et al. (2018) and Vosoughi et al. (2018) in Science provided a general discussion on the science of “fake news” and the process of proliferation of true and false news online. There have also been several interesting surveys, e.g., Shu et al. (2017) studied how information is disseminated and consumed in social media. Another survey by Thorne and Vlachos (2018) took a fact-checking perspective on “fake news” and related problems. Yet another survey (Li et al., 2016) covered truth discovery in general. Some very recent surveys focused on stance for misinformation and disinformation detection (Hardalov et al., 2021), on automatic fact-checking to assist human fact-checkers (Nakov et al., 2021a), on predicting the factuality and the bias of entire news outlets (Nakov et al., 2021c), on multimodal disinformation detection (Alam et al., 2021a), and on abusive language in social media (Nakov et al., 2021b). A number of Twitter datasets have been developed"
2021.nlp4if-1.12,2021.nlp4if-1.19,0,0.519343,"measures. 4.1 Task Phases We ran the shared tasks in two phases: Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and the participants were given a few days to submit their predictions. The Bulgarian Winner: We did not receive a submission for the best performing team for Bulgarian. The second best team, HunterSpeechLab (Panda and Levitan, 2021), explored the crosslingual generalization ability of multitask models trained from scratch (logistic regression, transformer encoder) and pre-trained models (English BERT, and mBERT) for deception detection. 4.2 5.3 Evaluation Measures DamascusTeam (Hussein et al., 2021) used a two-step pipeline, where the first step involves a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text. In the second step, a version of AraBERT is fine-tuned and used to classify the tweets. Their system was ranked 5th for Arabic. The official evaluation mea"
2021.nlp4if-1.12,2021.nlp4if-1.18,0,0.220605,"ples in the training, development and test sets for the three languages. Note that, we have more data for Arabic and Bulgarian than for English. 2 http://freeweibo.com http://weiboscope.jmsc.hku.hk 4 http://open.weibo.com/wiki/API文档/en 3 85 Topic Censored Uncensored cultural revolution human rights family planning censorship & propaganda democracy patriotism China Trump Meng Wanzhou kindergarten abuse 55 53 15 32 119 70 186 320 55 48 60 67 25 54 107 105 194 244 76 5 Total 953 937 Below, we give a brief summary of the best performing systems for each language. The English Winner: Team TOKOFOU (Tziafas et al., 2021) performed best for English. They gathered six BERT-based models pre-trained in relevant domains (e.g., Twitter and COVID-themed data) or fine-tuned on tasks, similar to the shared task’s topic (e.g., hate speech and sarcasm detection). They fine-tuned each of these models on the task 1 training data, projecting a label from the sequence classification token for each of the seven questions in parallel. After model selection on the basis of development set F1 performance, they combined the models in a majority-class ensemble. Table 3: Task 2: Topics featured in the dataset. 4 Task Organization"
2021.nlp4if-1.12,2021.nlp4if-1.20,0,0.0827001,"Missing"
2021.nlp4if-1.12,2021.nlp4if-1.15,0,0.0237019,"Ë Ë   ËË Ë  1 (Tziafas et al., 2021) 2 (Suhane and Kowshik, 2021) 3 (Kumar et al., 2021) 4 (Uyangodage et al., 2021) 7 (Panda and Levitan, 2021) Table 7: Task 1: Overview of the approaches used by the participating systems for English. =part of the official submission; Ë=considered in internal experiments; Trans. is for Transformers; Repres. is for Representations. References to system description papers are shown below the table. Trans. Models Misc BERT multilingual AraBERT Asafaya-BERT ARBERT ALBERT MARBERT Logistic Regression Ranks Team Ensemble Under/Over-Sampling Team R00 (Qarqaz et al., 2021) had the best performing system for the Arabic subtask. They used an ensemble of neural networks combining a linear layer on top of one out of the following four pre-trained Arabic language models: AraBERT, Asafaya-BERT, ARBERT. In addition, they also experimented with MARBERT. Team TOKOFOU (Tziafas et al., 2021) participated in English only and theirs was the winning system for that language. They gathered six BERT-based models pre-trained in relevant domains (e.g., Twitter and COVID-themed data) or fine-tuned on tasks, similar to the shared task’s topic (e.g., hate speech and sarcasm detecti"
2021.nlp4if-1.12,2020.alw-1.19,0,0.0284727,"one of 383M tweets (Banda et al., 2020), a billion-scale dataset of 65 languages and 32M geo-tagged tweets (Abdul-Mageed et al., 2021), and the GeoCoV19 dataset, consisting of 524M multilingual tweets, including 491M with GPS coordinates (Qazi et al., 2020). There are also Arabic datasets, both with (Haouari et al., 2021; Mubarak and Hassan, 2021) and without manual annotations (Alqurashi et al., 2020). We are not aware of Bulgarian datasets. Zhou et al. (2020) created the ReCOVery dataset, which combines 2,000 news articles about COVID19, annotated for their factuality, with 140,820 tweets. Vidgen et al. (2020) studied COVID-19 prejudices using a manually labeled dataset of 20K tweets with the following labels: hostile, criticism, prejudice, and neutral. 2.2 Censorship Detection There has been a lot of research aiming at developing strategies to detect and to evade censorship. Most work has focused on exploiting technological limitations with existing routing protocols (Leberknight et al., 2012; Katti et al., 2005; Levin et al., 2015; Weinberg et al., 2012; Bock et al., 2020). Research that pays more attention to the linguistic properties of online censorship in the context of censorship evasion inc"
2021.nlp4if-1.12,2020.semeval-1.271,0,0.0720189,"s topic (e.g., hate speech and sarcasm detection). They fine-tuned each of these models on the task 1 training data, projecting a label from the sequence classification token for each of the seven questions in parallel. After model selection on the basis of development set F1 performance, they combined the models in a majority-class ensemble. Table 3: Task 2: Topics featured in the dataset. 4 Task Organization The Arabic Winner: Team R00 had the best performing system for Arabic. They used an ensemble of the follwoing fine-tuned Arabic transformers: AraBERT (Antoun et al., 2020), AsafayaBERT (Safaya et al., 2020), ARBERT. In addition, they also experimented with MARBERT (AbdulMageed et al., 2020). In this section, we describe the overall task organization, phases, and evaluation measures. 4.1 Task Phases We ran the shared tasks in two phases: Development Phase In the first phase, only training and development data were made available, and no gold labels were provided for the latter. The participants competed against each other to achieve the best performance on the development set. Test Phase In the second phase, the test set (unlabeled input only) was released, and the participants were given a few d"
2021.nlp4if-1.12,2021.nlp4if-1.16,0,0.208605,"ed a two-step pipeline, where the first step involves a series of pre-processing procedures to transform Twitter jargon, including emojis and emoticons, into plain text. In the second step, a version of AraBERT is fine-tuned and used to classify the tweets. Their system was ranked 5th for Arabic. The official evaluation measure for task 1 was the average of the weighted F1 scores for each of the seven questions; for task 2, it was accuracy. 5 Evaluation Results for Task 1 Below, we describe the baselines, the evaluation results, and the best systems for each language. 5.1 Team dunder_mifflin (Suhane and Kowshik, 2021) built a multi-output model using task-wise multi-head attention for inter-task information aggregation. This was built on top of the representations obtained from RoBERTa. To tackle the small size of the dataset, they used back-translation for data augmentation. Their loss function was weighted for each output, in accordance with the distribution of the labels for that output. They were the runners-up in the English subtask with a mean F1-score of 0.891 on the test set, without the use of any task-specific embeddings or ensembles. Baselines The baselines for Task 1 are (i) majority class, (ii"
2021.nlp4if-1.9,N18-2004,1,0.0700937,"a, 5 Qatar Computing Research Institute, HBKU tariq@cs.columbia.edu, amal@gatech.edu, alimoham@buffalo.edu muhammad.mageed@ubc.ca, pnakov@hbku.edu.qa Abstract Our work here contributes to these efforts a new dataset and baseline results on it. In particular, we create a new dataset for stance detection of claims collected from a number of websites covering different domains such as politics, health, and economics. The websites cover several Arab countries, which enables wider applicability of our dataset. This compares favorably to previous work for Arabic stance detection such as the work of Baly et al. (2018), who focused on a single country. We use the websites as our source to collect true and false claims, and we carefully crawl web articles related to these claims. Using the claim–article pairs, we then manually assign stance labels to the articles. By stance we mean whether an article agrees, disagrees, discusses a claim or it is just unrelated. This allows us to exploit the resulting dataset to build models that automatically identify the stance with respect to a given claim, which is an important component of fact-checking and fake news detection systems. To develop these models, we resort"
2021.nlp4if-1.9,W18-5521,1,0.830685,"te range to two months before and after the date of the claim, prepending named entities and removing extra clauses using parse trees. In order to emphasize the presence of the main entity(s) in the claim, we extracted named entities using the Arabic NER corpus by Benajiba et al. (2007) and Stanford’s CoreNLP Arabic NER tagger (Manning et al., 2014). We further used Stanford’s CoreNLP Arabic parser to extract the first verb phrase (VP) and all its preceeding tokens in the claim, as this has been shown to improve document retrieval results for claim verification, especially for lengthy claims (Chakrabarty et al., 2018). For the two examples shown above, we would keep the claims until the comma for the first example and the word and for the second one, and we would consider those as the queries. 3.3 Stance Annotation We set up the annotation task as follows: given a claim–article pair, what is the stance of the document towards the claim? The stance was to be annotated using one of the following labels: agree, disagree, discuss, or unrelated, which were also used in previous work (Pomerleau and Rao, 2017; Baly et al., 2018). 6 We used Google News as a reference of news sources for the three countries of the"
2021.nlp4if-1.9,N19-1423,0,0.158717,"ation). These circumstances motivate a need to develop tools for detecting fake news online, including for a region with opposing forces and ongoing conflicts such as the Arab world. Our contributions can be summarized as follows: 1. We release a new multi-domain, multi-country dataset labeled for both stance and veracity. 2. We introduce a multi-query related document retrieval approach for claims from diverse topics in Arabic, resulting in a dataset with balanced label distributions across classes. 3. We compare our dataset to two other Arabic stance detection datasets using four BERTbased (Devlin et al., 2019) models. 1 The data can be found at http://github.com/ Tariq60/arastance. 57 Proceedings of the 4th NLP4IF Workshop on NLP for Internet Freedom, pages 57–65 June 6, 2021. ©2021 Association for Computational Linguistics 2 3 Related Work AraStance Construction We constructed our AraStance dataset similarly to the way this was done for the English Fake News Challenge (FNC) dataset (Pomerleau and Rao, 2017) and for the Arabic dataset of Baly et al. (2018). Our dataset contains true and false claims, where each claim is paired with one or more documents. Each claim–article pair has a stance label:"
2021.nlp4if-1.9,N18-1070,1,0.852606,"social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory networks, and transformers (Hanselowski et al., 2018; Conforti et al., 2018; Mohtarami et al., 2018; Zhang et al., 2019; Schiller et al., 2021; Schütz et al., 2021). There are two datasets for Arabic stance detection with respect to claims. The first one collected their false claims from a single political source (Baly et al., 2018), while we cover three sources from multiple countries and topics. They retrieved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Kh"
2021.nlp4if-1.9,D19-1452,1,0.871488,"Missing"
2021.nlp4if-1.9,2020.wanlp-1.7,1,0.751705,"s and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Khouja (2020) uses headlines from news sources and generated true and false claims by modifying the headlines. They used a three-class labeling scheme of stance by merging the discuss and the unrelated classes in one class called other. Our work is also related to detecting machinegenerated and manipulated text (Jawahar et al., 2020; Nagoudi et al., 2020). 3.1 Claim Collection and Preprocessing We collected false claims from three fact-checking websites: A RAANEWS2 , DABEGAD3 , and N ORU MORS 4 , based in the UAE, Egypt, and Saudi Arabia, respectively. The claims were from 2012 to 2018 and covered multiple domains such as politics, sports, and health. As the three fact-checking websites only debunk false claims, we looked for another source for true claims: following Baly et al. (2018), we collected true claims from the Arabic website of R EUTERS5 , assuming that their content was trustworthy. We added topic and date restrictions when collecti"
2021.nlp4if-1.9,2020.semeval-1.271,0,0.0348712,"which all the experimental downstream three datasets are derived. Also, it seems that ARBERT and MARBERT are better than the other two models at predicting the stance between a pair of sentences, as it is the case with the Khouja (2020) dataset. Models We fine-tuned the following four models for each of the three Arabic datasets: 1. Multilingual BERT (mBERT), base size, which is trained on the Wikipedias of 100 different languages, including Arabic (Devlin et al., 2019). 2. ArabicBERT, base size, which is trained on 8.2 billion tokens from the OSCAR corpus8 as well as on the Arabic Wikipedia (Safaya et al., 2020). 3. ARBERT, which is trained on 6.2 billion tokens of mostly Modern Standard Arabic text (Abdul-Mageed et al., 2020). 4. MARBERT, which is trained on one billion Arabic tweets, which in turn use both Modern Standard Arabic and Dialectal Arabic (AbdulMageed et al., 2020). 8 Results http://oscar-corpus.com 62 Model Baly et al. (2018) Dataset D Ds U Acc F1 A mBERT ArabicBERT ARBERT MARBERT .63 .58 .56 .44 0 .14 .14 .14 .11 .24 .30 .23 .84 .82 .83 .78 .73 .69 .70 .62 .40 .45 .46 .40 Khouja (2020) Dataset A D O Acc F1 A D AraStance Ds U Acc F1 .74 .74 .81 .80 .81 .85 .85 .85 .68 .75 .82 .80 .58 .5"
2021.nlp4if-1.9,W18-5501,0,0.0218529,"ation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory net"
2021.nlp4if-1.9,W14-2508,0,0.0214982,", and (iii) stance annotations. Stance detection started as a standalone task, unrelated to fact-checking (Küçük and Can, 2020). One type of stance models the relation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yield"
2021.nlp4if-1.9,N16-1138,0,0.0188798,"d. Below, we decribe the three steps of building AraStance: (i) claim collection and pre-processing, (ii) relevant document retrieval, and (iii) stance annotations. Stance detection started as a standalone task, unrelated to fact-checking (Küçük and Can, 2020). One type of stance models the relation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset"
2021.nlp4if-1.9,S19-2147,0,0.0579166,"ortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory networks, and transformers (Hanselowski et al., 2018; Conforti et al., 2018; Mohtarami et al., 2018; Zhang et al"
2021.nlp4if-1.9,C18-1158,0,0.118668,"u and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pairs from the Emergent dataset (Ferreira and Vlachos, 2016), which itself pairs 300 claims to 2,500 articles. In FNC, this pairing is done at random, and it yielded a large number of unrelated claim– article pairs. There are several approaches attempting to predict the stance on the FNC dataset using LSTMs, memory networks, and transformers (Hanselowski et al., 2018; Conforti et al., 2018; Mohtarami et al., 2018; Zhang et al., 2019; Schiller et al., 2021; Schütz et al., 2021). There are two datasets for Arabic stance detection with respect to claims. The first one collected their false claims from a single political source (Baly et al., 2018), while we cover three sources from multiple countries and topics. They retrieved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most str"
2021.nlp4if-1.9,2021.emnlp-main.710,1,0.734252,"ee, discuss, or unrelated. Below, we decribe the three steps of building AraStance: (i) claim collection and pre-processing, (ii) relevant document retrieval, and (iii) stance annotations. Stance detection started as a standalone task, unrelated to fact-checking (Küçük and Can, 2020). One type of stance models the relation (e.g., for, against, neutral) of a text segment towards a topic, usually a controversial one such as abortion or gun control (Mohammad et al., 2016; Abbott et al., 2016). Another one models the relation (e.g., agree, disagree, discuss, unrelated) between two pieces of text (Hardalov et al., 2021b; Ferreira and Vlachos, 2016). The latter definition is used in automatic fact-checking, fake news detection, and rumour verification (Vlachos and Riedel, 2014). There are several English datasets that model fact-checking as a stance detection task on text from multiple genres such as Wikipedia (Thorne et al., 2018), news articles (Pomerleau and Rao, 2017; Ferreira and Vlachos, 2016), and social media (Gorrell et al., 2019; Derczynski et al., 2017). Most related to our work here is the Fake News Challenge, or FNC, (Pomerleau and Rao, 2017), which is built by randomly matching claim–article pa"
2021.nlp4if-1.9,2020.coling-main.208,1,0.748171,"eved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Khouja (2020) uses headlines from news sources and generated true and false claims by modifying the headlines. They used a three-class labeling scheme of stance by merging the discuss and the unrelated classes in one class called other. Our work is also related to detecting machinegenerated and manipulated text (Jawahar et al., 2020; Nagoudi et al., 2020). 3.1 Claim Collection and Preprocessing We collected false claims from three fact-checking websites: A RAANEWS2 , DABEGAD3 , and N ORU MORS 4 , based in the UAE, Egypt, and Saudi Arabia, respectively. The claims were from 2012 to 2018 and covered multiple domains such as politics, sports, and health. As the three fact-checking websites only debunk false claims, we looked for another source for true claims: following Baly et al. (2018), we collected true claims from the Arabic website of R EUTERS5 , assuming that their content was trustworthy. We added topic and date res"
2021.nlp4if-1.9,2020.fever-1.2,0,0.153909,"18; Zhang et al., 2019; Schiller et al., 2021; Schütz et al., 2021). There are two datasets for Arabic stance detection with respect to claims. The first one collected their false claims from a single political source (Baly et al., 2018), while we cover three sources from multiple countries and topics. They retrieved relevant documents and annotated the claim–article pairs using the four labels listed earlier (i.e., agree, disagree, discuss, unrelated). They also annotated “rationales,” which are segments in the articles where the stance is most strongly expressed. The other Arabic dataset by Khouja (2020) uses headlines from news sources and generated true and false claims by modifying the headlines. They used a three-class labeling scheme of stance by merging the discuss and the unrelated classes in one class called other. Our work is also related to detecting machinegenerated and manipulated text (Jawahar et al., 2020; Nagoudi et al., 2020). 3.1 Claim Collection and Preprocessing We collected false claims from three fact-checking websites: A RAANEWS2 , DABEGAD3 , and N ORU MORS 4 , based in the UAE, Egypt, and Saudi Arabia, respectively. The claims were from 2012 to 2018 and covered multiple"
2021.nlp4if-1.9,P14-5010,0,0.00308725,"J ¢  K ÕæK Ï @ YJ« á  JKQË@ á  J kYÖ ÈCg áÓ    ÐAK @ èQå« èYÖÏ ZAÖÏ @ ð á .ÊË@ PAm'. Õæ Lungs of smokers are cleaned by smelling the steam of milk and water for ten days To remedy this, we boosted the quality of the retrieved documents by restricting the date range to two months before and after the date of the claim, prepending named entities and removing extra clauses using parse trees. In order to emphasize the presence of the main entity(s) in the claim, we extracted named entities using the Arabic NER corpus by Benajiba et al. (2007) and Stanford’s CoreNLP Arabic NER tagger (Manning et al., 2014). We further used Stanford’s CoreNLP Arabic parser to extract the first verb phrase (VP) and all its preceeding tokens in the claim, as this has been shown to improve document retrieval results for claim verification, especially for lengthy claims (Chakrabarty et al., 2018). For the two examples shown above, we would keep the claims until the comma for the first example and the word and for the second one, and we would consider those as the queries. 3.3 Stance Annotation We set up the annotation task as follows: given a claim–article pair, what is the stance of the document towards the claim?"
2021.nlp4if-1.9,S16-1003,0,0.0863519,"Missing"
2021.semeval-1.7,2020.acl-demos.32,1,0.899294,"Missing"
2021.semeval-1.7,S19-2147,0,0.0208834,"lated shared task is the NLP4IF-2019 task on Fine-Grained Propaganda Detection, which asked to detect the spans of use in news articles of each of 18 propaganda techniques (Da San Martino et al., 2019a). While these tasks focused on the text of news articles, here we target memes and multimodality, and we further use an extended inventory of 22 propaganda techniques. Other related shared tasks include the FEVER 2018 and 2019 tasks on Fact Extraction and VERification (Thorne et al., 2018), the SemEval 2017 and 2019 tasks on predicting the veracity of rumors in Twitter (Derczynski et al., 2017; Gorrell et al., 2019), the SemEval-2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), the NLP4IF-2021 shared task on Fighting the COVID-19 Infodemic (Shaar et al., 2021). We should also mention the CLEF 2018–2021 CheckThat! lab (Nakov et al., 2018; Elsayed et al., 2019a,b; Barr´on-Cede˜no et al., 2020; Barr´on-Cede˜no et al., 2020), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019, 2020; Shaar et al., 2020; Nakov et al., 2021) of claims in political debates and social media."
2021.semeval-1.7,D19-1565,1,0.897846,"Missing"
2021.semeval-1.7,2021.semeval-1.149,0,0.359113,"cision and recall that can account for the imbalance in the corpus: 1 X P (S, T ) = C(s, t, |s|), (2) |S| C(s, t, h) = s ∈ S, t∈T 2 75 http://propaganda.math.unipd.it/semeval2021task6/ Transformers Models Repres. Misc BERT RoBERTa XLNet ALBERT DistilBERT DeBERTa LSTM CNN SVM Naive Bayes Random Forest CRF Embeddings Char n-grams PoS Ensemble Data augmentation Postprocessing Rank. Team 1. MinD 2. Alpha 3. Volta 5. AIMH 6. LeCun 7. WVOQ 9. NLyticsFKIE 12. YNU-HPCC 13. CSECUDSG 15. NLP-IITR      Ë  Ë       Ë   Ë       Ë    Ë 1 (Tian et al., 2021) 2 (Feng et al., 2021) 3 (Gupta et al., 2021) 5 (Messina et al., 2021)   Ë ËËË   6 (Dia et al., 2021) 13 (Hossain et al., 2021) 15 (Gupta and Sharma, 2021) 7 (Roele, 2021) 9 (Pritzkau, 2021) 12 (Zhu et al., 2021) Table 2: ST1: Overview of the approaches used by the participating systems. =part of the official submission; Ë=considered in internal experiments; Repres. stand for Representations. References to system description papers are shown below the table. 6 Participants and Results Rank Team 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Below, we give a general description of the systems that participated in the three subtasks and their re"
2021.semeval-1.7,D17-2002,0,0.0590751,"l Workshop on Semantic Evaluation (SemEval-2021), pages 70–98 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Based on our annotations, we offered the following three subtasks: They performed massive experiments, investigated writing style and readability level, and trained models using logistic regression and SVMs. Their findings confirmed that using distant supervision, in conjunction with rich representations, might encourage the model to predict the source of the article, rather than to discriminate propaganda from non-propaganda. The study by Habernal et al. (2017, 2018) also proposed a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring, and irrelevant authority, which directly relate to propaganda techniques. A more fine-grained propaganda analysis was done by Da San Martino et al. (2019b), who developed a corpus of news articles annotated with the spans of use of 18 propaganda techniques, from an invetory they put together. They targeted two tasks: (i) binary classification —given a sentence, predict whether any of the techniques was used in it; and (ii) multi-label multi-class classification and span detectio"
2021.semeval-1.7,D18-1235,0,0.0208553,"second, and they used a multi-task learning (MTL) and additional datasets such as the PTC corpus from SemEval-2020 task 11 (Da San Martino et al., 2020a), and a fake news corpus (Przybyla, 2020). They used BERT, followed by several output layers that perform auxiliary tasks of propaganda detection and credibility assessment in two distinct scenarios: sequential and parallel MTL. Their final submission used the latter. Team TeamFPAI (Xiaolong et al., 2021) formulated the task as a question answering problem using machine reading comprehension, thus improving over the ensemble-based approach of Liu et al. (2018). They further explored data augmentation and loss design techniques, in order to alleviate the problem of data sparseness and data imbalance. The approaches for this task varied from modeling it as a question answering (QA) task to performing multi-task learning. Table 4 presents a high-level summary. We can see that BERT dominated, while RoBERTa was much less popular. We further see a couple of systems using data augmentation. Unfortunately, there are too few systems with system description papers for this subtask, and thus it is hard to do a very deep analysis. Volta HOMADOS TeamFPAI WVOQ C"
2021.semeval-1.7,L18-1526,0,0.012613,"tion campaigns. Traditionally a monopoly of states and large organizations, now such campaigns have become within the reach of even small organisations and individuals (Da San Martino et al., 2020b). Such propaganda campaigns are often carried out using posts spread on social media, with the aim to reach very large audience. While the rhetorical and the psychological devices that constitute the basic building blocks of persuasive messages have been thoroughly studied (Miller, 1939; Weston, 2008; Torok, 2015), only few isolated efforts have been made to devise automatic systems to detect them (Habernal et al., 2018; Habernal et al., 2018; Da San Martino et al., 2019b). 1 In order to avoid potential copyright issues, all memes we show in this paper are our own recreation of existing memes, using images with clear copyright. WARNING: This paper contains meme examples and wording that might be offensive to some readers. 70 Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021), pages 70–98 Bangkok, Thailand (online), August 5–6, 2021. ©2021 Association for Computational Linguistics Based on our annotations, we offered the following three subtasks: They performed massive experi"
2021.semeval-1.7,2020.acl-main.537,0,0.0368444,"Missing"
2021.semeval-1.7,N18-1074,0,0.0659449,"Missing"
2021.semeval-1.7,C10-2115,0,0.0711287,"Missing"
2021.semeval-1.7,2021.semeval-1.143,0,0.0979117,"math.unipd.it/semeval2021task6/ Transformers Models Repres. Misc BERT RoBERTa XLNet ALBERT DistilBERT DeBERTa LSTM CNN SVM Naive Bayes Random Forest CRF Embeddings Char n-grams PoS Ensemble Data augmentation Postprocessing Rank. Team 1. MinD 2. Alpha 3. Volta 5. AIMH 6. LeCun 7. WVOQ 9. NLyticsFKIE 12. YNU-HPCC 13. CSECUDSG 15. NLP-IITR      Ë  Ë       Ë   Ë       Ë    Ë 1 (Tian et al., 2021) 2 (Feng et al., 2021) 3 (Gupta et al., 2021) 5 (Messina et al., 2021)   Ë ËËË   6 (Dia et al., 2021) 13 (Hossain et al., 2021) 15 (Gupta and Sharma, 2021) 7 (Roele, 2021) 9 (Pritzkau, 2021) 12 (Zhu et al., 2021) Table 2: ST1: Overview of the approaches used by the participating systems. =part of the official submission; Ë=considered in internal experiments; Repres. stand for Representations. References to system description papers are shown below the table. 6 Participants and Results Rank Team 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Below, we give a general description of the systems that participated in the three subtasks and their results, with focus on those ranked among the top-3. Appendix C gives a description of every system. 6.1 Subtask 1 (Unimodal: Text) Table 2 gives an ov"
2021.semeval-1.7,2021.semeval-1.150,0,0.497878,"Given Eq. (1), we now define variants of precision and recall that can account for the imbalance in the corpus: 1 X P (S, T ) = C(s, t, |s|), (2) |S| C(s, t, h) = s ∈ S, t∈T 2 75 http://propaganda.math.unipd.it/semeval2021task6/ Transformers Models Repres. Misc BERT RoBERTa XLNet ALBERT DistilBERT DeBERTa LSTM CNN SVM Naive Bayes Random Forest CRF Embeddings Char n-grams PoS Ensemble Data augmentation Postprocessing Rank. Team 1. MinD 2. Alpha 3. Volta 5. AIMH 6. LeCun 7. WVOQ 9. NLyticsFKIE 12. YNU-HPCC 13. CSECUDSG 15. NLP-IITR      Ë  Ë       Ë   Ë       Ë    Ë 1 (Tian et al., 2021) 2 (Feng et al., 2021) 3 (Gupta et al., 2021) 5 (Messina et al., 2021)   Ë ËËË   6 (Dia et al., 2021) 13 (Hossain et al., 2021) 15 (Gupta and Sharma, 2021) 7 (Roele, 2021) 9 (Pritzkau, 2021) 12 (Zhu et al., 2021) Table 2: ST1: Overview of the approaches used by the participating systems. =part of the official submission; Ë=considered in internal experiments; Repres. stand for Representations. References to system description papers are shown below the table. 6 Participants and Results Rank Team 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Below, we give a general description of the systems that pa"
2021.semeval-1.7,D17-1317,0,0.0156011,"by each technique. This is a multilabel sequence tagging task. Subtask 3 (ST3) Given a meme, identify which techniques (out of 22 possible ones) are used in the meme, considering both the text and the image. This is a multilabel classification problem. A total of 71 teams registered for the task, 22 of them made an official submission on the test set and 15 of the participating teams submitted a system description paper. 2 Related Work Propaganda Detection Previous work on propaganda detection has focused on analyzing textual content (Barr´on-Cedeno et al., 2019; Da San Martino et al., 2019b; Rashkin et al., 2017). See (Martino et al., 2020) for a recent survey on computational propaganda detection. Rashkin et al. (2017) developed the TSHP-17 corpus, which had document-level annotations with four classes: trusted, satire, hoax, and propaganda. Note that TSHP-17 was labeled using distant supervision, i.e., all articles from a given news outlet were assigned the label of that news outlet. The news articles were collected from the English Gigaword corpus (which covers reliable news sources), as well as from seven unreliable news sources, including two propagandistic ones. They trained a model using word n"
2021.semeval-1.7,2021.semeval-1.32,0,0.401496,"tp://propaganda.math.unipd.it/semeval2021task6/ Transformers Models Repres. Misc BERT RoBERTa XLNet ALBERT DistilBERT DeBERTa LSTM CNN SVM Naive Bayes Random Forest CRF Embeddings Char n-grams PoS Ensemble Data augmentation Postprocessing Rank. Team 1. MinD 2. Alpha 3. Volta 5. AIMH 6. LeCun 7. WVOQ 9. NLyticsFKIE 12. YNU-HPCC 13. CSECUDSG 15. NLP-IITR      Ë  Ë       Ë   Ë       Ë    Ë 1 (Tian et al., 2021) 2 (Feng et al., 2021) 3 (Gupta et al., 2021) 5 (Messina et al., 2021)   Ë ËËË   6 (Dia et al., 2021) 13 (Hossain et al., 2021) 15 (Gupta and Sharma, 2021) 7 (Roele, 2021) 9 (Pritzkau, 2021) 12 (Zhu et al., 2021) Table 2: ST1: Overview of the approaches used by the participating systems. =part of the official submission; Ë=considered in internal experiments; Repres. stand for Representations. References to system description papers are shown below the table. 6 Participants and Results Rank Team 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Below, we give a general description of the systems that participated in the three subtasks and their results, with focus on those ranked among the top-3. Appendix C gives a description of every system. 6.1 Subtask 1 (Unimodal: Text)"
boyanov-etal-2017-building,P02-1040,0,\N,Missing
boyanov-etal-2017-building,N13-1090,0,\N,Missing
boyanov-etal-2017-building,P15-1152,0,\N,Missing
boyanov-etal-2017-building,S15-2047,1,\N,Missing
boyanov-etal-2017-building,D16-1230,0,\N,Missing
boyanov-etal-2017-building,S16-1136,1,\N,Missing
boyanov-etal-2017-building,Q17-1010,0,\N,Missing
boyanov-etal-2017-building,P16-2075,1,\N,Missing
boyanov-etal-2017-building,C16-2001,1,\N,Missing
boyanov-etal-2017-building,W17-5534,0,\N,Missing
boyanov-etal-2017-building,D16-1165,1,\N,Missing
C12-1121,N12-1062,0,0.176306,"Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and sc"
C12-1121,J93-2003,0,0.0439624,"ling the effective reference length, grounding the precision component, and unclipping the brevity penalty, which yield sizable improvements in test BLEU on two Arabic-English datasets: IWSLT (+0.65) and NIST (+0.37). KEYWORDS: Statistical machine translation, parameter optimization, MERT, PRO, MIRA. Proceedings of COLING 2012: Technical Papers, pages 1979–1994, COLING 2012, Mumbai, December 2012. 1979 1 Introduction Early work on statistical machine translation (SMT) has relied on generative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually inc"
C12-1121,N12-1047,0,0.065585,"dd-one smoothed sentence-level version of BLEU, known as BLEU+1 (Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoo"
C12-1121,N09-1025,0,0.106178,"Missing"
C12-1121,D08-1024,0,0.692837,"imization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). While the number of parameters that an optimizer can handle has become a major concern recently, there are many other important aspects that researchers have paid attention to, e.g., the performance of parameters when translating unseen test data, the speed of convergence, the stability across multiple reruns, the objective function being optimized (e.g., BLEU vs. an approximation of BLEU), the mode of learning (e.g., online vs. batch). Here we study a different, and so far neglected, aspect: the characteristics of the translati"
C12-1121,federico-etal-2012-iwslt,0,0.0123357,"length). On testing, we further used cube pruning and minimum Bayes Risk decoding (the latter yielded slightly longer translations). 7 Still, for comparison purposes, we also report BLEU calculated with respect of the original references using NIST v13a, after detokenization and recasing of the system’s output (shown in small script in the tables). 1986 4.2 Datasets We experimented with the Arabic-English datasets from two machine translation evaluation campaigns: (1) the NIST 2012 Open Machine Translation Evaluation8 , and (2) the IWSLT 2011 Evaluation Campaign on Automatic Talk Translation (Federico et al., 2012). 1. NIST: We trained the phrase and the reordering tables on all training datasets from NIST 2012 (except for UN), we tuned on MT06 and tested on MT09. For language modeling, we built a separate LM from the English side of each training dataset, and from each year of the English GigaWord; we then interpolated them into a single LM. 2. IWSLT: We trained the phrase and the reordering tables on the TED training dataset, we tuned on dev2010, and we tested on tst2010. Since there was a small mismatch in the source/reference length ratios between dev2010 and tst2010, we also experimented with rever"
C12-1121,W09-0439,0,0.0442971,"or future work. 1 That is why the Moses toolkit has an option to run a few iterations of MERT after PRO – to get the length right. 1980 2 Related Work The dominant approach for parameters optimization in SMT is to use MERT (Och, 2003), a batch tuning algorithm that iterates between two modes: (i) generating a k-best list of translation hypotheses using the current parameters values, and (ii) parameter optimization using the k-best lists from all previous iterations. MERT optimizes expected BLEU. It works well for a small number of parameters, but suffers from scalability and stability issues (Foster and Kuhn, 2009). Most importantly for our discussion, it tends not to have length biases; this is also confirmed by our own experiments (see Table 4). Various alternatives to MERT have been proposed, motivated primarily by scalability considerations. One popular alternative is MIRA (Watanabe et al., 2007; Chiang et al., 2008, 2009), which is a perceptron-like online tuning algorithm with passive-aggressive updates. It uses an approximation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of paramete"
C12-1121,N12-1023,0,0.132132,"paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and scaled reference length. However, this was not done for the purpose of studying the length bias of PRO; moreover, as we will see below, the use of BLEU+1 is not the only reason for this bias. 3 The Length Bias with PRO We explore the following hypoth"
C12-1121,P12-1016,0,0.0176715,"y, in order to avoid stability issues, we report results averaged over three runs. 4.1 Experimental Setup Preprocessing: We tokenized the English side of all bi-texts and the monolingual data for language modeling using the standard tokenizer of Moses. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus; for lines containing ALL CAPS, we did this for each word. We segmented the words on the Arabic side using the ATB segmentation scheme: we used MADA (Roth et al., 2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. For language modeling, we trai"
C12-1121,P12-1031,0,0.144739,"sion of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al., 2012), but also many original algorithms that use a variety of machine learning methods and loss functions. We refer the interested reader to some excellent recent overviews: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). To the best of our knowledge, no prior work has tried to study the reasons for the length bias of optimizers like PRO. However, researchers have previously expressed concerns about sentence-level BLEU+1, and some have proposed improvements, e.g., He and Deng (2012) used different smoothing for higher-order n-grams, unclipped brevity penalty, and scaled reference length. However, this was not done for the purpose of studying the length bias of PRO; moreover, as we will see below, the use of BLEU+1 is not the only reason for this bias. 3 The Length Bias with PRO We explore the following hypotheses about the length bias with PRO: • PRO’s optimization: The bias could be due to the optimization mechanism of PRO. • BLEU+1: PRO uses BLEU+1, where the add-one smoothing is applied to the precision component but does not touch the brevity penalty, which introduce"
C12-1121,D11-1125,0,0.340142,"), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). While the number of parameters that an optimizer can handle has become a major concern recently, there are many other important aspects that researchers have paid attention to, e.g., the performance of parameters when translating unseen test data, the speed of convergence, the stability across multiple reruns, the objective function being optimized (e.g., BLEU vs. an approximation of BLEU), the mode of learning (e.g., online vs. batch). Here we study a different, and so far neglected, aspect: the characteristics of the translations generated using weights found by different optimizers. More"
C12-1121,2005.iwslt-1.8,0,0.0245682,"2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. For language modeling, we trained a separate 5-gram Kneser-Ney smoothed model on each corpus (target side of a training bi-text or monolingual dataset); we then interpolated these models minimizing the perplexity on the target side of the tuning dataset. Finally, we built a log-linear model including the language model probability, the word penalty, and the parameters from the phrase and the reordering tables. Tuning. We tuned the weights in the log-linear model by optimizing BLEU (Papineni et al., 2002) on the tuning dataset, using MERT, PRO, or MIRA. We allowed optimi"
C12-1121,P07-2045,0,0.00793493,"s found to work well in general, but many other values yielded a similar result since the BLEU score gets dominated by examples from the current iteration very quickly, making this value irrelevant. 6 We only allow up to 25 iterations, which means there could be up to 24 accumulated one-best hypotheses per sentence, while we accept up to Ξ = 50 pairs per sentence. 1985 4 Experiments and Evaluation We compare variations of three parameter optimization algorithms: MERT, PRO, and MIRA. In all experiments, we use the phrase-based SMT model (Koehn et al., 2003) as implemented in the Moses toolkit (Koehn et al., 2007), and we report evaluation results over two datasets: NIST, which has four reference translations, and IWSLT, with a single reference translation. In order to be able to directly compare the candidate/reference length ratios on the development and on the testing datasets, we need to make sure that we use the same tokenization when calculating BLEU on tuning and on testing. Such differences can arise because many standard scoring tools, e.g., those of NIST, work on detokenized text, which they retokenize again internally; this retokenization typically differs from the one used by the SMT system"
C12-1121,N03-1017,0,0.00889571,"n BLEU points on the NIST datasets. 5 The value of 0.9 was found to work well in general, but many other values yielded a similar result since the BLEU score gets dominated by examples from the current iteration very quickly, making this value irrelevant. 6 We only allow up to 25 iterations, which means there could be up to 24 accumulated one-best hypotheses per sentence, while we accept up to Ξ = 50 pairs per sentence. 1985 4 Experiments and Evaluation We compare variations of three parameter optimization algorithms: MERT, PRO, and MIRA. In all experiments, we use the phrase-based SMT model (Koehn et al., 2003) as implemented in the Moses toolkit (Koehn et al., 2007), and we report evaluation results over two datasets: NIST, which has four reference translations, and IWSLT, with a single reference translation. In order to be able to directly compare the candidate/reference length ratios on the development and on the testing datasets, we need to make sure that we use the same tokenization when calculating BLEU on tuning and on testing. Such differences can arise because many standard scoring tools, e.g., those of NIST, work on detokenized text, which they retokenize again internally; this retokenizat"
C12-1121,C04-1072,0,0.657577,"mation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of parameters and generally has no length bias (see Table 4). Another recent, but already popular alternative to MERT is PRO (Hopkins and May, 2011), which models parameter tuning as pairwise ranking optimization. This is a batch tuning algorithm, which iterates between translation and optimization, just like MERT, but scales to thousands of parameters. It uses an add-one smoothed sentence-level version of BLEU, known as BLEU+1 (Lin and Och, 2004), and suffers from a length bias: the parameters it finds yield translations that are too short compared to the references (see Table 4). Exploring the reasons for this bias and proposing ways to solve it is the main focus of this paper. There are many other tuning strategies, which fall outside of the scope of the current study, but to many of which some of our general finding and conclusions should apply. This includes improved versions of some of the above-mentioned algorithms, e.g., a batch version of MIRA (Cherry and Foster, 2012), or a linear regression version of PRO (Bazrafshan et al.,"
C12-1121,P03-1021,0,0.533365,"ative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most notably, the margin infused relaxed algorithm (MIRA) (Chiang et al., 2008) and pairwise ranking optimization (PRO) (Hopkins and May, 2011). Whil"
C12-1121,P02-1040,0,0.107284,"Papers, pages 1979–1994, COLING 2012, Mumbai, December 2012. 1979 1 Introduction Early work on statistical machine translation (SMT) has relied on generative training using maximum likelihood parameter estimation. This was inspired by the noisy channel model (Brown et al., 1993), which asked for calculating the product of two components, a language model and a translation model, giving them equal weights. As mainstream research has moved towards combining multiple scores, the field has switched to discriminative tuning in a log-linear fashion. The standard approach has been to maximize BLEU (Papineni et al., 2002) on a tuning dataset using a coordinate descent optimization algorithm known as minimum error rate training (MERT), as proposed by Och (2003). MERT has dominated the SMT field for years, until the number of parameters in the loglinear model has gradually increased, in some cases to hundreds and even to hundreds of thousands of scores, which has called for new tuning algorithms since MERT was unable to scale beyond just a handful of parameters. Many alternatives to MERT have been proposed over the years, but it is only recently that some of them have gained popularity in the community, most not"
C12-1121,P08-2030,0,0.0550344,"he same models that were used for training and tuning.7 Finally, in order to avoid stability issues, we report results averaged over three runs. 4.1 Experimental Setup Preprocessing: We tokenized the English side of all bi-texts and the monolingual data for language modeling using the standard tokenizer of Moses. We further truecased this data by changing the casing of each sentence-initial word to its most frequent casing in the training corpus; for lines containing ALL CAPS, we did this for each word. We segmented the words on the Arabic side using the ATB segmentation scheme: we used MADA (Roth et al., 2008) for NIST, and the Stanford word segmenter (Green and DeNero, 2012) for IWSLT. Training. We built separate directed word alignments using IBM model 4 (Brown et al., 1993), we symmetrized them with the grow-diag-final-and heuristic of Moses, and we extracted phrase pairs of length up to seven. We scored these pairs using maximum likelihood with Kneser-Ney smoothing, to build a phrase table with five standard scores: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We also built a lexicalized reordering model (Koehn"
C12-1121,2006.amta-papers.25,0,0.0697215,"lting translations, which we have attributed to the use of sentence-level BLEU+1 as an objective function. We have thus suggested a number of simple modifications, which do improve the length ratio in practice, ultimately yielding better BLEU scores, while also preserving the sentence-level nature of BLEU+1, which makes optimizers simpler conceptually and implementation-wise. In future work, we plan a more systematic study of the relationship between optimizers and objective functions with respect to the target/reference length ratio, which would be extended with other optimizers such as TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). Overall, we see two promising general directions in which the present study can be extended. First, explore the relationship between sentence-level and corpuslevel optimization and the possibility to combine them. Second, study the characteristics of translations generated using weights from different optimizers: while here we have only touched length, we believe there are many other important aspects that are worth exploring. Acknowledgments We thank the anonymous reviewers for their comments, which helped us improve the paper. 11 For example, the aver"
C12-1121,D07-1080,0,0.167475,"s: (i) generating a k-best list of translation hypotheses using the current parameters values, and (ii) parameter optimization using the k-best lists from all previous iterations. MERT optimizes expected BLEU. It works well for a small number of parameters, but suffers from scalability and stability issues (Foster and Kuhn, 2009). Most importantly for our discussion, it tends not to have length biases; this is also confirmed by our own experiments (see Table 4). Various alternatives to MERT have been proposed, motivated primarily by scalability considerations. One popular alternative is MIRA (Watanabe et al., 2007; Chiang et al., 2008, 2009), which is a perceptron-like online tuning algorithm with passive-aggressive updates. It uses an approximation to BLEU, where a sentence is scored in the context of a pseudo-document formed from the n-gram statistics for the last few updates. MIRA can scale to thousands of parameters and generally has no length bias (see Table 4). Another recent, but already popular alternative to MERT is PRO (Hopkins and May, 2011), which models parameter tuning as pairwise ranking optimization. This is a batch tuning algorithm, which iterates between translation and optimization,"
C14-1020,P02-1034,0,0.423722,"itional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using"
C14-1020,P06-2034,0,0.0263295,"derable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a"
C14-1020,J02-3001,0,0.163552,"cognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of conc"
C14-1020,D12-1083,1,0.768831,"low trees. A sentence containing multiple clauses exhibits a coherence structure. For instance, in our example, the first clause “along my route tell me the next steak house” is elaborated by the second clause “that is within a mile”. The relations by which clauses in a text are linked are called coherence relations (e.g., Elaboration, Contrast). Discourse structures capture this coherence structure of text and provide additional semantic information that could be useful for the CSL task (Stede, 2011). To build the discourse structure of a sentence, we use a state-of-the-art discourse parser (Joty et al., 2012) which generates discourse trees in accordance with the Rhetorical Structure Theory of discourse (Mann and Thompson, 1988), as exemplified in Figure 1b. Notice that a text span linked by a coherence relation can be either a nucleus (i.e., the core part) or a satellite (i.e., a supportive one) depending on how central the claim is. 3.3 New features In order to compare to the structured representation, we also devoted significant effort towards engineering a set of features to be used in a flat feature-vector representation; they can be used in isolation or in combination with the kernel-based a"
C14-1020,P06-1115,0,0.033166,"eranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-grams. In this paper, we study the impact of more advanced linguistic processing on CSL, such as shallow and full syntactic parsing and discourse structure. We use a reranking approach to select the best hypothesis annotated with concepts derived by a local model, where the hyp"
C14-1020,H05-1064,0,0.0355336,"ated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly bas"
C14-1020,P05-1024,0,0.0308755,"o be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures f"
C14-1020,J08-2001,1,0.787599,"Missing"
C14-1020,H94-1053,0,0.881717,"{price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of CSL approaches can be fo"
C14-1020,W06-2909,1,0.869136,"of CSL approaches can be found in (De Mori et al., 2008). CSL is also connected to a large body of work on shallow semantic parsing; see (Gildea and Jurafsky, 2002; M`arquez et al., 2008) for an overview. Another relevant line of research with a considerable body of work is reranking in NLP. Tree kernels for reranking syntactic parse trees were first proposed in (Collins and Duffy, 2002). Some variants used explicit spaces (Kudo et al., 2005), and feature vector approaches were proposed in (Koo and Collins, 2005). Other reranking work using tree kernels regards predicate argument structures (Moschitti et al., 2006) and named entities (Nguyen and Moschitti, 2012). In (Dinarelli et al., 2011), we rerank CSL hypotheses using structures built on top of concepts, words and features that are simpler than those studied in this paper. The work of Ge and Mooney (2006) and Kate and Mooney (2006) is also similar to ours, as it models the extraction of semantics as a reranking task using string kernels. 1.3 Syntactic and semantic structures for CSL The related work has highlighted that automatic CSL is mostly based on powerful machine learning algorithms and simple feature representations based on word and tag n-gr"
C14-1020,H91-1020,0,0.794759,"Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 193 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 193–202, Dublin, Ireland, August 23-29 2014. Finally, a database query is formed from the list of labels and values, and is then executed against the database, e.g., MongoDB; a backoff mechanism may be used if the query does not succeed. {$and [{cuisine:&quot;lebanese&quot;}, {city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied"
C14-1020,H89-1026,0,0.078377,"city:&quot;doha&quot;}, 1.2 {price:&quot;low&quot;}, {amenity:&quot;carry out&quot;}]} Related work on CSL Pieraccini et al. (1991) used Hidden Markov Models (HMMs) for CSL, where the observations were word sequences and the hidden states were meaning units, i.e, concepts. In subsequent work (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007; De Mori et al., 2008), other generative models were applied, which model the joint probability of a word sequence and a concept sequence, as well as discriminative models, which directly model a conditional probability over the concepts in the input text. Seneff (1989) and Miller et al. (1994) used stochastic grammars for CSL. In particular, they applied stochastic Finite State Transducers (FST) for recognizing constituent annotations. FSTs describe local syntactic structures with a sequence of words, e.g., noun phrases or even constituents. Papineni et al. (1998) proposed and evaluated exponential models, but, nowadays, Conditional Random Fields (Lafferty et al., 2001) are considered to be the state-of-the-art. More recently, Wang et al. (2009) illustrated an approach for CSL that is specific to query understanding for web applications. A general survey of"
C16-1147,baccianella-etal-2010-sentiwordnet,0,0.155662,"Missing"
C16-1147,S16-1130,1,0.873513,"Missing"
C16-1147,J90-1003,0,0.204371,"Missing"
C16-1147,S16-1173,0,0.051773,"Missing"
C16-1147,esuli-sebastiani-2006-sentiwordnet,0,0.137304,"Missing"
C16-1147,S15-2080,0,0.0609345,"Missing"
C16-1147,R15-1034,1,0.70984,"Missing"
C16-1147,R15-1036,1,0.771019,"Missing"
C16-1147,S16-1004,0,0.0386646,"Missing"
C16-1147,N10-1087,0,0.0701066,"Missing"
C16-1147,W16-4801,1,0.822252,"Missing"
C16-1147,S13-2053,0,0.0415414,"Missing"
C16-1147,S16-1003,0,0.0361868,"Missing"
C16-1147,S12-1033,0,0.0800624,"Missing"
C16-1147,S13-2052,1,0.875748,"Missing"
C16-1147,S16-1001,1,0.865052,"Missing"
C16-1147,P05-1015,0,0.0936553,"Missing"
C16-1147,W02-1011,0,0.0231725,"Missing"
C16-1147,S14-2004,0,0.0792525,"Missing"
C16-1147,S16-1002,0,0.0240976,"Missing"
C16-1147,R09-1065,1,0.891668,"Missing"
C16-1147,S14-2009,1,0.888442,"Missing"
C16-1147,S15-2078,1,0.878591,"Missing"
C16-1147,S15-2077,0,0.0311819,"Missing"
C16-1147,N15-1159,0,0.044446,"Missing"
C16-1147,C08-1103,0,0.07929,"Missing"
C16-1147,S07-1013,0,0.147366,"Missing"
C16-1147,P02-1053,0,0.0211662,"Missing"
C16-1147,J04-3002,0,0.0856929,"Missing"
C16-1147,H05-1044,0,0.27212,"Missing"
C16-1147,W15-5401,1,0.884168,"Missing"
C16-1147,S14-2076,0,0.0583095,"Missing"
C16-1147,S15-2082,0,\N,Missing
C16-2001,P15-2113,1,0.887768,"Missing"
C16-2001,S16-1138,1,0.853194,"Missing"
C16-2001,D15-1068,1,0.89046,"Missing"
C16-2001,P03-1054,0,0.0264421,"larity value using a similarity matrix. The similarity and the embeddings along with other additional similarity features are then passed through a hidden layer and next to the output layer for classification. The qe and ce are learned by backpropagating the (cross entropy) errors from the output layer. qe and ce vectors are finally concatenated and used as features in our SVM model. Tree kernels We use tree kernels to measure the syntactic similarity between the question and the comment. First, we produce shallow syntactic trees for the question and for the comment using the Stanford parser (Klein and Manning, 2003). Following Severyn and Moschitti (2012), we link the two trees by connecting nodes such as NP, PP, VP, when there is at least one lexical overlap between the corresponding phrases of the trees, and we mark those links using a specific tag. The kernel function K is defined as: K((t1 , t2 ), (c1 , c2 )) = T K(t1 , c1 )+T K(t2 , c2 ), where T K(t, c) is a tree kernel function operating over a pair of question (t) and comment (c) trees.3 Classification Performance We evaluated our comment classifier on the SemEval-2016 Task 3 test set with the official scorer, obtaining the following results: MAP"
C16-2001,S15-2047,1,0.903863,"Missing"
C16-2001,S15-2036,1,0.910076,"Missing"
D09-1141,P05-1074,0,0.122401,"Missing"
D09-1141,P07-1083,0,0.0560722,"and Portuguese. Malay and Indonesian are mutually intelligible, but differ in pronunciation and vocabulary. An example follows5 : • Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hakhak. Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸a˜ o corresponds to the Spanish suffix -ci´on, e.g., evoluc¸a˜ o vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4 . Even 3 more frequent can be the inflectional variations. For example,"
D09-1141,W07-0702,0,0.0504892,"Missing"
D09-1141,J93-2003,0,0.022832,"acted pairs of sentences from the matched document pairs using competitive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phr"
D09-1141,N06-1003,0,0.0650842,"what inconsistent in that respect. The latter method performs worst and is the only one to go below the baseline (for 10K ml-en pairs). Table 3 shows the results when using pt-en data to improve Spanish→English SMT. Overall, the results and the conclusions that can be made are consistent with those for Table 2. We can further observe that, as the size of the original bi-text increases, the gain in Bleu decreases, which is to be expected. Note also that here transliteration is very important: it doubles the absolute gain in Bleu. Finally, Table 4 shows a comparison to the pivoting technique of Callison-Burch et al. (2006). for English→Spanish SMT. Despite using just Portuguese, we achieve an improvement that is, in five out of six cases, much better than what they achieve with eight pivot languages (which include not only Portuguese, but also two other Romance languages, French and Italian, which are closely related to Spanish). Moreover, our method yields improvements for very large original datasets – 1.2M pairs, while theirs stops improving at 160K. However, our improvements are only statistically significant for 160K original pairs or less. Finally, note that our translation direction is reversed. Based on"
D09-1141,P05-1066,0,0.0517566,"Missing"
D09-1141,I08-8003,0,0.0348734,"Missing"
D09-1141,W09-0431,0,0.0442964,"Missing"
D09-1141,A00-1002,0,0.438597,"Missing"
D09-1141,2005.iwslt-1.8,0,0.0183452,"omatically extracted likely cognates. The system was applied on the Portuguese side of the pt-en training bi-text. Classic approaches to automatic cognate extraction look for non-stopwords with similar spelling that appear in parallel sentences in a bi-text (Kondrak et al., 2003). In our case, however, we need to extract cognates between Spanish and Portuguese given pt-en and es-en bi-texts only, i.e., without having a pt-es bi-text. Although it is easy to construct a pt-es bi-text from the Europarl corpus, we chose not to do so since, in general, synthe8 We also tried lexicalized reordering (Koehn et al., 2005). While it yielded higher absolute Bleu scores, the relative improvement for a sample of our experiments was very similar to that achieved with distance-based re-ordering. 9 We used version 11b of the NIST scoring tool: http://www.nist.gov/speech/tools/ 1361 sizing a bi-text for X1 -X2 would be impossible: e.g., it cannot be done for ml-in given our training datasets for in-en and ml-en since the English sides of these bi-texts have no sentences in common. Thus, we extracted the list of likely cognates between Portuguese and Spanish from the training pt-en and es-en bi-texts using English as a"
D09-1141,P07-2045,0,0.00797891,"parallel sentences for X1 -Y and a larger bi-text for X2 -Y for some resource-rich language X2 that is closely related to X1 . The evaluation for Indonesian→English (using Malay) and Spanish→English (using Portuguese and pretending Spanish is resource-poor) shows an absolute gain of up to 1.35 and 3.37 Bleu points, respectively, which is an improvement over the rivaling approaches, while using much less additional data. 1 Introduction Recent developments in statistical machine translation (SMT), e.g., the availability of efficient implementations of integrated open-source toolkits like Moses (Koehn et al., 2007), have made it possible to build a prototype system with decent translation quality for any language pair in a few days or even hours. In theory. In practice, doing so requires having a large set of parallel sentencealigned bi-lingual texts (a bi-text) for that language pair, which is often unavailable. Large highquality bi-texts are rare; except for Arabic, Chinese, and some official languages of the European Union (EU), most of the 6,500+ world languages remain resource-poor from an SMT viewpoint. While manually creating a small bi-text could be relatively easy, building a large one is hard,"
D09-1141,2005.mtsummit-papers.11,0,0.0119969,"t of the 6,500+ world languages remain resource-poor from an SMT viewpoint. While manually creating a small bi-text could be relatively easy, building a large one is hard, e.g., because of copyright. Most bi-texts for SMT come from parliament debates and legislation of multi-lingual countries (e.g., French-English from Canada, and Chinese-English from Hong Kong), or from international organizations like the United Nations and the European Union. For example, the Europarl corpus of parliament proceedings consists of about 1.3M parallel sentences (up to 44M words) per language for 11 languages (Koehn, 2005), and the JRC-Acquis corpus provides a comparable amount of European legislation in 22 languages (Steinberger et al., 2006). The official languages of the EU are especially lucky in that respect; while this includes such “classic SMT languages” like English and French, and some important international ones like Spanish and Portuguese, most of the rest have a limited number of speakers and were resource-poor until recently; this is changing quickly because of the increasing volume of EU parliament debates and the ever-growing European legislation. Thus, becoming an official language of the EU h"
D09-1141,N03-2016,0,0.586373,"inally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9 . 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates. The system was applied on the Portuguese side of the pt-en training bi-text. Classic approaches to automatic cognate extraction look for non-stopwords with similar spelling that appear in parallel sentences in a bi-text (Kondrak et al., 2003). In our case, however, we need to extract cognates between Spanish and Portuguese given pt-en and es-en bi-texts only, i.e., without having a pt-es bi-text. Although it is easy to construct a pt-es bi-text from the Europarl corpus, we chose not to do so since, in general, synthe8 We also tried lexicalized reordering (Koehn et al., 2005). While it yielded higher absolute Bleu scores, the relative improvement for a sample of our experiments was very similar to that achieved with distance-based re-ordering. 9 We used version 11b of the NIST scoring tool: http://www.nist.gov/speech/tools/ 1361 si"
D09-1141,N01-1020,0,0.0617917,"Indonesian are mutually intelligible, but differ in pronunciation and vocabulary. An example follows5 : • Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hakhak. Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸a˜ o corresponds to the Spanish suffix -ci´on, e.g., evoluc¸a˜ o vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4 . Even 3 more frequent can be the inflectional variations. For example, in Portuguese and Spanis"
D09-1141,W95-0115,0,0.228601,"ng pj is conditionally independent of sk given ei , we can simplify the above expression: Pr(pj |sk ) = P i Pr(pj |ei )Pr(ei |sk ) Similarly, for Pr(sk |pj ), we obtain Pr(sk |pj ) = P i Pr(sk |ei )Pr(ei |pj ) We excluded all stopwords, words of length less than three, and those containing digits. We further calculated Prod(pj , sk ) = Pr(pj |sk )Pr(sk |pj ), and we excluded all Portuguese-Spanish word pairs (pj , sk ) for which Prod(pj , sk ) &lt; 0.01. From the remaining pairs, we extracted likely cognates based on Prod(pj , sk ) and on the orthographic similarity between pj and sk . Following Melamed (1995), we measured the orthographic similarity using the longest common subsequence ratio (LCSR), defined as follows: LCSR (s1 , s2 ) = |LCS(s1 ,s2 )| max(|s1 |,|s2 |) where LCS(s1 , s2 ) is the longest common subsequence of s1 and s2 , and |s |is the length of s. We retained as likely cognates all pairs for which LCSR was 0.58 or higher; that value was found by Kondrak et al. (2003) to be optimal for a number of language pairs in the Europarl corpus. Finally, we performed competitive linking (Melamed, 2000), assuming that each Portuguese wordform had at most one Spanish best cognate match. Thus, u"
D09-1141,J99-1003,0,0.0125866,"ntelligible, but differ in pronunciation and vocabulary. An example follows5 : • Malay: Semua manusia dilahirkan bebas dan samarata dari segi kemuliaan dan hakhak. Transliteration As we mentioned above, our method relies on the existence of a large number of cognates between related languages. While linguists define cognates as words derived from a common root3 (Bickford and Tuggy, 2002), computational linguists typically ignore origin, defining them as words in different languages that are mutual translations and have a similar orthography (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999). In this paper, we adopt the latter definition. Cognates between related languages often exhibit minor spelling variations, which can be simply due to different rules of orthography, (e.g., senhor vs. se˜nor in Portuguese and Spanish), but often stem from real phonological differences. For example, the Portuguese suffix -c¸a˜ o corresponds to the Spanish suffix -ci´on, e.g., evoluc¸a˜ o vs. evoluci´on. Such correspondences can be quite frequent and thus easy to learn automatically4 . Even 3 more frequent can be the inflectional variations. For example, in Portuguese and Spanish respectively,"
D09-1141,J00-2004,0,0.0250236,"uilt from release v.3 of the Europarl corpus, excluding the Q4/2000 portion out of which we created our testing and development datasets. We built the in-en bi-texts from texts that we downloaded from the Web. We translated the Indonesian texts to English using Google Translate, and we matched7 them against the English texts using a cosine similarity measure and heuristic constraints based on document length in words and in sentences, overlap of numbers, words in uppercase, and words in the title. Next, we extracted pairs of sentences from the matched document pairs using competitive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and ext"
D09-1141,J03-1002,0,0.0113431,"ive linking (Melamed, 2000), and we retained the ones whose similarity was above a pre-specified threshold. The ml-en was built in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 200"
D09-1141,J04-4002,0,0.0195743,"in a similar manner. 7 Note that the automatic translations were used for matching only; the final bi-text contained no automatic translations. Baseline SMT System In the baseline, we used the following setup: We first tokenized and lowercased both sides of the training bi-text. We then built separate directed word alignments for English→X and X→English (X∈{Indonesian, Spanish}) using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish)"
D09-1141,P03-1021,0,0.0047784,"s of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English. Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9 . 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list of automatically extracted likely cognates. The system"
D09-1141,P02-1040,0,0.0912337,"(Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length seven using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each pair is associated with five parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using standard SMT feature functions: trigram language model probability, word penalty, distance-based8 distortion cost, and the parameters from the phrase table. We set all weights by optimizing Bleu (Papineni et al., 2002) using minimum error rate training (MERT) (Och, 2003) on a separate development set of 2,000 sentences (Indonesian or Spanish), and we used them in a beam search decoder (Koehn et al., 2007) to translate 2,000 test sentences (Indonesian or Spanish) into English. Finally, we detokenized the output, and we evaluated it against a lowercased gold standard using Bleu9 . 3.4 Transliteration As was mentioned in Section 2, transliteration can be helpful for languages with regular spelling differences. Thus, we built a system for transliteration from Portuguese into Spanish that was trained on a list o"
D09-1141,W99-0626,0,0.0192186,"eration can help a lot in case of systematic spelling variations between the original and the additional source languages. 5 Related Work In this section, we describe two general lines of related previous research: using cognates between the source and the target language, and sourcelanguage side paraphrasing with a pivot language. 5.1 Cognates Many researchers have used likely cognates to obtain improved word alignments and thus build better SMT systems. Al-Onaizan et al. (1999) extracted such likely cognates for Czech-English using one of the variations of LCSR (Melamed, 1995) described in (Tiedemann, 1999) as a similarity measure. They used these cognates to improve word alignments with IBM models 1-4 in three different ways: (1) by seeding the parameters of IBM model 1, (2) by constraining the word cooccurrences when training IBM models 1-4, and (3) by adding the cognate pairs to the bi-text as additional “sentence pairs”. The last approach performed best and was later used by Kondrak et al. (2003) who demonstrated improved SMT for nine European languages. Unlike these approaches, which extract cognates between the source and the target language, we use cognates between the source and some oth"
D09-1141,N07-1061,0,0.157311,"Missing"
D09-1141,P07-1108,0,0.123603,"ven our training datasets for in-en and ml-en since the English sides of these bi-texts have no sentences in common. Thus, we extracted the list of likely cognates between Portuguese and Spanish from the training pt-en and es-en bi-texts using English as a pivot as follows: We started with IBM model 4 word alignments, from which we extracted four conditional lexical translation probabilities: Pr(pj |ei ) and Pr(ei |pj ) for Portuguese-English, and Pr(sk |ei ) and Pr(ei |sk ) for Spanish-English, where pj , ei and sk stand for a Portuguese, an English and a Spanish word respectively. Following Wu and Wang (2007), we then induced conditional lexical translation probabilities Pr(pj |sk ) and Pr(sk |pj ) for Portuguese-Spanish as follows: Pr(pj |sk ) = P i Pr(pj |ei , sk )Pr(ei |sk ) Assuming pj is conditionally independent of sk given ei , we can simplify the above expression: Pr(pj |sk ) = P i Pr(pj |ei )Pr(ei |sk ) Similarly, for Pr(sk |pj ), we obtain Pr(sk |pj ) = P i Pr(sk |ei )Pr(ei |pj ) We excluded all stopwords, words of length less than three, and those containing digits. We further calculated Prod(pj , sk ) = Pr(pj |sk )Pr(sk |pj ), and we excluded all Portuguese-Spanish word pairs (pj , sk"
D09-1141,W10-1751,0,\N,Missing
D09-1141,N04-1035,0,\N,Missing
D09-1141,steinberger-etal-2006-jrc,0,\N,Missing
D09-1141,2008.iwslt-papers.1,0,\N,Missing
D09-1141,N06-1011,0,\N,Missing
D09-1141,H93-1039,0,\N,Missing
D09-1141,C96-2141,0,\N,Missing
D09-1141,P00-1037,0,\N,Missing
D09-1141,W02-2026,0,\N,Missing
D09-1141,W09-1117,0,\N,Missing
D09-1141,D07-1005,0,\N,Missing
D09-1141,D08-1021,0,\N,Missing
D09-1141,P06-1012,1,\N,Missing
D09-1141,P07-1034,0,\N,Missing
D09-1141,P98-2238,0,\N,Missing
D09-1141,C98-2233,0,\N,Missing
D09-1141,D08-1090,0,\N,Missing
D09-1141,C10-1027,0,\N,Missing
D09-1141,W07-0705,0,\N,Missing
D09-1141,P07-1007,1,\N,Missing
D09-1141,N09-2056,0,\N,Missing
D09-1141,W02-0902,0,\N,Missing
D09-1141,P07-3010,0,\N,Missing
D09-1141,W09-0412,1,\N,Missing
D09-1141,2009.eamt-1.3,0,\N,Missing
D09-1141,N09-1025,0,\N,Missing
D09-1141,P08-1088,0,\N,Missing
D09-1141,P05-1033,0,\N,Missing
D09-1141,N03-1017,0,\N,Missing
D09-1141,P05-1034,0,\N,Missing
D09-1141,J10-3007,0,\N,Missing
D09-1141,E09-1082,0,\N,Missing
D09-1141,P07-1092,0,\N,Missing
D09-1141,W06-2003,0,\N,Missing
D09-1141,2005.eamt-1.19,0,\N,Missing
D09-1141,mulloni-pekar-2006-automatic,0,\N,Missing
D09-1141,W06-2005,0,\N,Missing
D10-1015,P08-1087,0,0.380254,"ith a word-based language model. These systems, however, do not attempt to incorporate their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into"
D10-1015,P09-1106,0,0.0190146,"m . Second, we re-tokenize P Tw at the morpheme level, thus obtaining a new phrase table P Tw→m , which is of the same granularity as P Tm . Finally, we merge P Tw→m and P Tm , and we input the resulting phrase table to the decoder. Word Morpheme GIZA++ GIZA++ Word alignment Morpheme alignment Phrase Extrac""on Enriching the Translation Model Phrase Extrac""on PTw Another general strategy for combining evidence from the word-token and the morpheme-token representations is to build two separate SMT systems and then combine them. This can be done as a post-processing system combination step; see (Chen et al., 2009a) for an overview of such approaches. 3 We use the term “hypothesis” to collectively refer to the following (Koehn, 2003): the source phrase covered, the corresponding target phrase, and most importantly, a reference to the previous hypothesis that it extends. 151 PTm Morphological segmenta""on PTw→m PT merging Decoding Figure 3: Building a twin phrase table (PT). First, separate PTs are generated for different input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Mer"
D10-1015,P05-1066,0,0.121879,"Missing"
D10-1015,W09-0430,0,0.0116041,"generated for different input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Merging and Normalizing Phrase Tables Below we first describe the two general phrase table combination strategies used in previous work: (1) direct merging using additional feature functions, and (2) phrase table interpolation. We then introduce our approach. Add-feature methods. The first line of research on phrase table merging is exemplified by (Niehues et al., 2009; Chen et al., 2009b; Do et al., 2009; Nakov and Ng, 2009). The idea is to select one of the phrase tables as primary and to add to it all nonduplicating phrase pairs from the second table together with their associated scores. For each entry, features can be added to indicate its origin (whether from the primary or from the secondary table). Later in our experiments, we will refer to these baseline methods as add-1 and add-2, depending on how many additional features have been added. The values we used for these features in the baseline are given in Section 5.4; their weights in the log-linear model were set in the standard way"
D10-1015,H05-1085,0,0.182633,"lish to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments. 1 Introduction The fast progress of statistical machine translation (SMT) has boosted translation quality significantly. While research keeps diversifying, the word remains the atomic token-unit of translation. This is fine for languages with limited morphology like English and French, or no morphology at all like Chinese, but it is inadequate for morphologically rich languages like Arabic, Czech or Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pre- or post-processing. For exa"
D10-1015,W09-0429,0,0.0117414,"er, do not attempt to incorporate their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PR"
D10-1015,D07-1091,0,0.33608,"igure 2. The word-token LM can capture much longer phrases and more complete contexts such as “, ep¨ademokraattisen maahanmuuttopolitiikan” compared to the morpheme-token LM. Note that scoring with two LMs that see the output sequence as different numbers of tokens is not readily offered by the existing SMT decoders. For example, the phrase-based model in Moses (Koehn et al., 2007) allows scoring with multiple LMs, but assumes they use the same token granularity, which is useful for LMs trained on different monolingual corpora, but cannot handle our case. While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. Note that scoring with twin LMs is conceptually superior to n-best re-scoring with a word-token LM, e.g., (Oflazer and El-Kahlout, 2007), since it is tightly integrated into decoding: it scores partial hypotheses and influenced the search process directly. 4 However, for phrase-based SMT systems, it is theoretically more appealing to combine their phrase tables since this allows the"
D10-1015,W05-0820,0,0.0288671,"t (f¯m ,¯ em ) into a word-token pair (f¯m→w ,¯ em→w ), and then induce a corresponding word alignment from the morpheme-token alignment of (f¯m ,¯ em ). We then estimate a lexicalized phrase score using the original formula given in (Koehn et al., 2003), where we plug this induced word alignment and word-token lexical translation probabilities estimated from the word-token dataset The case when (f¯w , e¯w ) is present in P Tw , but (f¯m , e¯m ) is not, is solved similarly. 5 Experiments and Evaluation 5.1 Datasets In our experiments, we use the English-Finnish data from the 2005 shared task (Koehn and Monz, 2005), which is split into training, development, and test portions; see Table 1 for details. We further split the training dataset into four subsets T1 , T2 , T3 , and T4 of sizes 40K, 80K, 160K, and 320K parallel sentence pairs, which we use for studying the impact of training data size on translation performance. Sent. Train Dev Test 714K 2K 2K Avg. words en fi 21.62 15.80 29.33 20.99 28.98 20.72 Avg. morph. en fi 24.68 26.15 33.40 34.94 33.10 34.47 Table 1: Dataset statistics. Shown are the number of parallel sentences, and the average number of words and Morfessor morphemes on the English and"
D10-1015,N03-1017,0,0.379639,"tput can be described by the following regular expression: WORD = ( PRE* STM SUF* )+ For example, uncarefully is analyzed as un/PRE+ care/STM+ ful/SUF+ ly/SUF The above token sequence forms the input to our system. We keep the PRE/STM/SUF tags as part of the tokens, and distinguish between care/STM+ and care/STM. Note also that the “+” sign is appended to each nonfinal tag so that we can distinguish word-internal from word-final morphemes. 3.2 Word Boundary-aware Phrase Extraction 3 Morphological Enhancements We present a morphologically-enhanced version of the classic phrase-based SMT model (Koehn et al., 2003). We use a hybrid morpheme-word representation where the basic unit of translation is the morpheme, but word boundaries are respected at all stages of the translation process. This is in contrast with previous work, where morphological enhancements are typically performed as pre-/postprocessing steps only. In addition to changing the basic translation token unit from a word to a morpheme, our model extends the phrase-based SMT model with the following: 1. word boundary-aware morpheme-level phrase extraction; 2. minimum error-rate training for a morphemelevel model using word-level BLEU; 3. joi"
D10-1015,P07-2045,0,0.0111836,"n LM, the morpheme-token sequence is concatenated into word-tokens before scoring. it can be enhanced with an appropriate word-token “view” on the partial morpheme-level hypotheses3 . The interaction of the twin LMs is illustrated in Figure 2. The word-token LM can capture much longer phrases and more complete contexts such as “, ep¨ademokraattisen maahanmuuttopolitiikan” compared to the morpheme-token LM. Note that scoring with two LMs that see the output sequence as different numbers of tokens is not readily offered by the existing SMT decoders. For example, the phrase-based model in Moses (Koehn et al., 2007) allows scoring with multiple LMs, but assumes they use the same token granularity, which is useful for LMs trained on different monolingual corpora, but cannot handle our case. While the factored translation model (Koehn and Hoang, 2007) in Moses does allow scoring with models of different granularity, e.g., lemma-token and word-token LMs, it requires a 1:1 correspondence between the tokens in the different factors, which clearly is not our case. Note that scoring with twin LMs is conceptually superior to n-best re-scoring with a word-token LM, e.g., (Oflazer and El-Kahlout, 2007), since it i"
D10-1015,N04-4015,0,0.0175227,"tion on English to Finnish using Europarl (714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments. 1 Introduction The fast progress of statistical machine translation (SMT) has boosted translation quality significantly. While research keeps diversifying, the word remains the atomic token-unit of translation. This is fine for languages with limited morphology like English and French, or no morphology at all like Chinese, but it is inadequate for morphologically rich languages like Arabic, Czech or Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pr"
D10-1015,D09-1141,1,0.438212,"fferent input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Merging and Normalizing Phrase Tables Below we first describe the two general phrase table combination strategies used in previous work: (1) direct merging using additional feature functions, and (2) phrase table interpolation. We then introduce our approach. Add-feature methods. The first line of research on phrase table merging is exemplified by (Niehues et al., 2009; Chen et al., 2009b; Do et al., 2009; Nakov and Ng, 2009). The idea is to select one of the phrase tables as primary and to add to it all nonduplicating phrase pairs from the second table together with their associated scores. For each entry, features can be added to indicate its origin (whether from the primary or from the secondary table). Later in our experiments, we will refer to these baseline methods as add-1 and add-2, depending on how many additional features have been added. The values we used for these features in the baseline are given in Section 5.4; their weights in the log-linear model were set in the standard way using MERT. Interpola"
D10-1015,W09-0413,0,0.0189108,"phrase table (PT). First, separate PTs are generated for different input granularities: word-token and morpheme-token. Second, the wordtoken PT is retokenized at the morpheme-token level. Finally, the two PTs are merged and used by the decoder. 4.2 Merging and Normalizing Phrase Tables Below we first describe the two general phrase table combination strategies used in previous work: (1) direct merging using additional feature functions, and (2) phrase table interpolation. We then introduce our approach. Add-feature methods. The first line of research on phrase table merging is exemplified by (Niehues et al., 2009; Chen et al., 2009b; Do et al., 2009; Nakov and Ng, 2009). The idea is to select one of the phrase tables as primary and to add to it all nonduplicating phrase pairs from the second table together with their associated scores. For each entry, features can be added to indicate its origin (whether from the primary or from the secondary table). Later in our experiments, we will refer to these baseline methods as add-1 and add-2, depending on how many additional features have been added. The values we used for these features in the baseline are given in Section 5.4; their weights in the log-linea"
D10-1015,J04-4002,0,0.0270322,"um error-rate training for a morphemelevel model using word-level BLEU; 3. joint scoring with morpheme- and word-level language models. We first introduce our morpheme-level representation, and then describe our enhancements. 1 Avramidis and Koehn (2008) improved by 0.15 BLEU over a 18.05 English-Greek baseline; Toutanova et al. (2008) improved by 0.72 BLEU over a 36.00 English-Russian baseline. 149 The core translation structure of a phrase-based SMT model is the phrase table, which is learned from a bilingual parallel sentence-aligned corpus, typically using the alignment template approach (Och and Ney, 2004). It contains a set of bilingual phrase pairs, each associated with five scores: forward and backward phrase translation probabilities, forward and backward lexicalized translation probabilities, and a constant phrase penalty. The maximum phrase length n is normally limited to seven words; higher values of n increase the table size exponentially without actually yielding performance benefit (Koehn et al., 2003). However, things are different when translating with morphemes, for two reasons: (1) morpheme-token phrases of length n can span less than n words; and (2) morphemetoken phrases may onl"
D10-1015,P03-1021,0,0.0375837,"i.e., morphemetoken phrases span a sequence of whole words. This is a fair extension of the morpheme-token system with respect to a word-token one since both are restricted to span up to n word-tokens. 3.3 Morpheme-Token MERT Optimizing Word-Token BLEU Modern phrase-based SMT systems use a log-linear model with the following typical feature functions: language model probabilities, word penalty, distortion cost, and the five parameters from the phrase table. Their weights are set by optimizing BLEU score (Papineni et al., 2001) directly using minimum error rate training (MERT), as suggested by Och (2003). In previous work, phrase-based SMT systems using morpheme-token input/output naturally per2 This means that we miss the opportunity to generate new wordforms for known baseforms, but removes the problem of proposing nonwords in the target language. 150 formed MERT at the morpheme-token level as well. This is not optimal since the final expected system output is a sequence of words, not morphemes. The main danger is that optimizing a morpheme-token BLEU score could lead to a suboptimal weight for the word penalty feature function: this is because the brevity penalty of BLEU is calculated with"
D10-1015,W07-0704,0,0.482548,"process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PRE (prefix), STM (stem), SUF (suffix). Multiple prefixes and suffixes can be pr"
D10-1015,2001.mtsummit-papers.68,0,0.193817,"ong as they span n words or less. We further require that word boundaries be respected2 , i.e., morphemetoken phrases span a sequence of whole words. This is a fair extension of the morpheme-token system with respect to a word-token one since both are restricted to span up to n word-tokens. 3.3 Morpheme-Token MERT Optimizing Word-Token BLEU Modern phrase-based SMT systems use a log-linear model with the following typical feature functions: language model probabilities, word penalty, distortion cost, and the five parameters from the phrase table. Their weights are set by optimizing BLEU score (Papineni et al., 2001) directly using minimum error rate training (MERT), as suggested by Och (2003). In previous work, phrase-based SMT systems using morpheme-token input/output naturally per2 This means that we miss the opportunity to generate new wordforms for known baseforms, but removes the problem of proposing nonwords in the target language. 150 formed MERT at the morpheme-token level as well. This is not optimal since the final expected system output is a sequence of words, not morphemes. The main danger is that optimizing a morpheme-token BLEU score could lead to a suboptimal weight for the word penalty fe"
D10-1015,P06-1001,0,0.0483452,"and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pre- or post-processing. For example, Sadat and Habash (2006) use different combinations of Arabic pre-processing schemes 148 Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 148–157, c MIT, Massachusetts, USA, 9-11 October 2010. 2010 Association for Computational Linguistics for Arabic-English SMT, whereas Oflazer and ElKahlout (2007) post-processes Turkish morphemelevel translations by re-scoring n-best lists with a word-based language model. These systems, however, do not attempt to incorporate their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We"
D10-1015,P08-1059,0,0.250483,"their analysis as part of the decoding process, but rather rely on models designed for word-token translation. We should also note the importance of the translation direction: it is much harder to translate from a morphologically poor to a morphologically rich language, where morphological distinctions not present in the source need to be generated in the target language. Research in translating into morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PRE (prefix), STM (stem), SUF (suffi"
D10-1015,2007.mtsummit-papers.65,0,0.36438,"morphologically rich languages, has attracted interest for languages like Arabic (Badr et al., 2008), Greek (Avramidis and Koehn, 2008), Hungarian (Nov´ak, 2009; Koehn and Haddow, 2009), Russian (Toutanova et al., 2008), and Turkish (Oflazer and El-Kahlout, 2007). These approaches, however, either only succeed in enhancing the performance for small bi-texts (Badr et al., 2008; Oflazer and El-Kahlout, 2007), or improve only modestly for large bi-texts1 . 3.1 Morphological Representation Our morphological representation is based on the output of an unsupervised morphological analyzer. Following Virpioja et al. (2007), we use Morfessor, which is trained on raw tokenized text (Creutz and Lagus, 2007). The tool segments words into morphemes annotated with the following labels: PRE (prefix), STM (stem), SUF (suffix). Multiple prefixes and suffixes can be proposed for each word; word compounding is allowed as well. The output can be described by the following regular expression: WORD = ( PRE* STM SUF* )+ For example, uncarefully is analyzed as un/PRE+ care/STM+ ful/SUF+ ly/SUF The above token sequence forms the input to our system. We keep the PRE/STM/SUF tags as part of the tokens, and distinguish between car"
D10-1015,P07-1108,0,0.0204132,"e not normalized any more. Theoretically, this is not necessarily a problem since the log-linear model used by the decoder does not assume that the scores for the feature functions come from a normalized probability distribution. While it is possible to re-normalize the scores to convert them into probabilities, this is rarely done; it also does not solve the problem with the dropped scores for the duplicated phrases. Instead, the conditional probabilities in the two phrase tables are often interpolated directly, e.g., using linear interpolation. Representative work adopting this approach is (Wu and Wang, 2007). We refer to this method as interpolation. Our method. The above phrase merging approaches have been proposed for phrase tables derived from different sources. This is in contrast with our twin translation scenario, where the morphemetoken phrase tables are built from the same training dataset; the main difference being that word alignments and phrase extraction were performed at the 152 word-token level for P Tw→m and at the morphemetoken level for P Tm . Thus, we propose different merging approaches for the phrase translation probabilities and for the lexicalized probabilities. In phrase-ba"
D10-1015,E06-1006,0,0.0173467,"(714K sentence pairs; 15.5M English words) shows statistically significant improvements over the classic model based on BLEU and human judgments. 1 Introduction The fast progress of statistical machine translation (SMT) has boosted translation quality significantly. While research keeps diversifying, the word remains the atomic token-unit of translation. This is fine for languages with limited morphology like English and French, or no morphology at all like Chinese, but it is inadequate for morphologically rich languages like Arabic, Czech or Finnish (Lee, 2004; Goldwater and McClosky, 2005; Yang and Kirchhoff, 2006). ∗ This research was sponsored in part by CSIDM (grant # 200805) and by a National Research Foundation grant entitled “Interactive Media Search” (grant # R-252-000-325-279). 2 Related Work Most previous work on morphology-aware approaches relies heavily on language-specific tools, e.g., the TreeTagger (Schmid, 1994) or the Buckwalter Arabic Morphological Analyzer (Buckwalter, 2004), which hampers their portability to other languages. Moreover, the prevalent method for incorporating morphological information is by heuristically-driven pre- or post-processing. For example, Sadat and Habash (200"
D10-1015,P02-1040,0,\N,Missing
D10-1015,W09-0401,0,\N,Missing
D10-1015,P08-2039,0,\N,Missing
D10-1015,W09-0428,0,\N,Missing
D11-1060,P98-1015,0,0.0789602,"deletion, the modifier is derived from the object of the underlying relative clause; however, the first three verbs also allow for it to be derived from the subject. Levi expresses the distinction using indexes. For example, music box is M AKE1 (object-derived), i.e., the box makes music, while chocolate bar is M AKE2 (subject-derived), i.e., the bar is made of chocolate (note the passive). Due to time constraints, we focused on one relation of Levi’s, M AKE2 , which is among the most frequent relations an NC can express and is present in some form in many relation inventories (Warren, 1978; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001; Nastase and Szpakowicz, 2003; Girju et al., 2005; Girju et al., 2007; Girju et al., 2009; Hendrickx et al., 2010; Tratz and Hovy, 2010). In Levi’s theory, M AKE2 means that the head of the noun compound is made up of or is a product of its modifier. There are three subtypes of this relation (we do not attempt to distinguish between them): 2. we select the top 20 most frequent patterns; (a) the modifier is a unit and the head is a configuration, e.g., root system; 3. we filter out all patterns that were extracted less than N times (we tried 5 and 10) and with less th"
D11-1060,P99-1008,0,0.0239839,"e (2008) adopted a similar fine-grained verbcentered approach to NC semantics. Using a distribution over verbs as a semantic interpretation was also carried out in a recent challenge: SemEval-2010 Task 9 (Butnariu et al., 2009; Butnariu et al., 2010). In noun compound interpretation, verbs and prepositions can be seen as patterns connecting the two nouns in a paraphrase. Similar pattern-based approaches have been popular in information extraction and ontology learning. For example, Hearst (1992) extracted hyponyms using patterns such as X, Y, and/or other Zs, where Z is a hypernym of X and Y. Berland and Charniak (1999) used similar patterns to extract meronymy (part-whole) relations, e.g., parts/NNS of/IN wholes/NNS matches basements of buildings. Unfortunately, matches are rare, which makes it difficult to build large semantic inventories. In order to overcome data sparseness, pattern-based approaches are often combined with bootstrapping. For example, Riloff and Jones (1999) used a multi-level bootstrapping algorithm to learn both a semantic lexicon and extraction patterns, e.g., owned by X extracts C OMPANY and facilities in X extracts L OCATION. That is, they learned semantic lexicons using extraction p"
D11-1060,C08-1011,0,0.467425,"ce National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number of semantically interprete"
D11-1060,W09-2416,1,0.878239,"Missing"
D11-1060,fillmore-etal-2002-seeing,0,0.0251555,"odifier instead of gold. The problem also arose on Step 1, where we used WordNet to check whether the NC candidates were composed of two nouns. Since words like clear, friendly, and single are listed in WordNet as nouns (which is possible in some contexts), we extracted wrong NCs such as clear cube, friendly team, and single chain. There were similar issues with verbparticle constructions since some particles can be used as nouns as well, e.g., give back, break down. Some errors were due to semantic transparency issues, where the syntactic and the semantic head of a target NP were mismatched (Fillmore et al., 2002; Fontenelle, 1999). For example, from the sentence “This wine is made from a range of white grapes.”, we would extract range rather than grapes as the potential modifier of wine. In some cases, the NC-pattern pair was correct, but the NC did not express the target relation, e.g., while contain is a good paraphrase for toy box, the noun compound itself is not an instance of M AKE2 . 656 60 ’Acc.i1’ ’Acc.i2’ ’Acc.i3’ 50 40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 Figure 2: NC accuracy vs. collocation strength. 9 Conclusion and Future Work We have presented a framework for building a very large dat"
D11-1060,S07-1003,1,0.824769,"Missing"
D11-1060,P07-1072,0,0.0713507,"ion Using Bootstrapping and the Web as a Corpus Su Nam Kim Computer Science & Software Engineering University of Melbourne Melbourne, VIC 3010 Australia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize th"
D11-1060,C92-2082,0,0.049182,"ng conjunctions; they also used this distribution to predict coarse-grained abstract relations. Butnariu and Veale (2008) adopted a similar fine-grained verbcentered approach to NC semantics. Using a distribution over verbs as a semantic interpretation was also carried out in a recent challenge: SemEval-2010 Task 9 (Butnariu et al., 2009; Butnariu et al., 2010). In noun compound interpretation, verbs and prepositions can be seen as patterns connecting the two nouns in a paraphrase. Similar pattern-based approaches have been popular in information extraction and ontology learning. For example, Hearst (1992) extracted hyponyms using patterns such as X, Y, and/or other Zs, where Z is a hypernym of X and Y. Berland and Charniak (1999) used similar patterns to extract meronymy (part-whole) relations, e.g., parts/NNS of/IN wholes/NNS matches basements of buildings. Unfortunately, matches are rare, which makes it difficult to build large semantic inventories. In order to overcome data sparseness, pattern-based approaches are often combined with bootstrapping. For example, Riloff and Jones (1999) used a multi-level bootstrapping algorithm to learn both a semantic lexicon and extraction patterns, e.g.,"
D11-1060,S10-1006,1,0.865178,"Missing"
D11-1060,P06-2064,1,0.932221,"csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one"
D11-1060,P10-1150,0,0.103813,"uce a large number of semantically interpreted noun compounds from a small number of seeds. In each iteration, the method replaced one component of an NC with its synonyms, hypernyms and hyponyms to generate a new NC. These new NCs were further filtered based on their semantic similarity with the original NC. While the method acquired a large number of noun compounds without significant semantic drifting, its accuracy degraded rapidly after each iteration. More importantly, the variation of the sense pairs was limited since new NCs had to be semantically similar to the original NCs. Recently, Kozareva and Hovy (2010) combined patterns and bootstrapping to learn the selectional restrictions for various semantic relations. They used patterns involving the coordinating conjunction and, e.g., “* and John fly to *”, and learned arguments such as Mary/Tom and France/New York. Unlike in NC interpretation, it is not necessary for their arguments to form an NC, e.g., Mary France and France Mary are not NCs. Rather, they were interested in building a semantic ontology with a predefined set of semantic relations, similar to YAGO (Suchanek et al., 2007), where the pattern work for would have arguments like a company/"
D11-1060,J02-3004,0,0.0192778,"stralia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, w"
D11-1060,P09-1045,0,0.0892433,"s (verbs and prepositions) that interpret them for a given abstract relation. First, we extract NCs using a small number of seed patterns from a given abstract relation. Then, using the extracted NCs, we harvest more patterns. This is repeated until no new NCs and patterns can be extracted or for a pre-specified number of iterations. Our approach combines pattern-based extraction and bootstrapping, which is novel for NC interpretation; however, such combinations have been used in other areas, e.g., named entity recognition (Riloff and Jones, 1999; Thelen and Riloff, 2002; Curran et al., 2007; McIntosh and Curran, 2009). The remainder of the paper is organized as follows: Section 2 gives an overview of related work, Section 3 motivates our semantic representation, Sections 4, 5, and 6 explain our method, dataset and experiments, respectively, Section 7 discusses the results, Section 8 provides error analysis, and Section 9 concludes with suggestions for future work. 2 Related Work As we mentioned above, the implicit relation between the two nouns forming a noun compound can often be expressed overtly using verbal and prepositional paraphrases. For example, student loan is “loan given to a student”, while mor"
D11-1060,W04-2609,0,0.0904546,"Missing"
D11-1060,P08-1052,1,0.947622,"rtment of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In evaluation, we found that having one compound noun fixed yields both a higher number"
D11-1060,N04-3012,0,0.0116789,"stantial agreement (Landis and Koch, 1977). The accuracy for NC-only strict bootstrapping is a bit higher than for strict bootstrapping, but the actual differences are probably smaller since the evaluation of the former on iteration 2 was done for the most frequent NCs, which are more accurate. 7 As a comparison, we implemented the method of Kim and Baldwin (2007), which generates new semantically interpreted NCs by replacing either the head or the modifier of a seed NC with suitable synonyms, hypernyms and sister words from WordNet, followed by similarity filtering using WordNet::Similarity (Pedersen et al., 2004). Discussion Tables 2 and 3 show that fixing one of the two nouns in the pattern, as in strict bootstrapping and NC-only strict bootstrapping, yields significantly higher accuracy (χ2 test) for both NC and NC-pattern pair extraction compared to loose bootstrapping. 655 Note that the number of extracted NCs is much higher with the strict methods because of the higher number of possible instantiations of the generalized query patterns. For NC-only strict bootstrapping, the number of extracted NCs grows exponentially since the number of patterns does not diminish as in the other two methods. The"
D11-1060,W01-0511,0,0.0395006,"rived from the object of the underlying relative clause; however, the first three verbs also allow for it to be derived from the subject. Levi expresses the distinction using indexes. For example, music box is M AKE1 (object-derived), i.e., the box makes music, while chocolate bar is M AKE2 (subject-derived), i.e., the bar is made of chocolate (note the passive). Due to time constraints, we focused on one relation of Levi’s, M AKE2 , which is among the most frequent relations an NC can express and is present in some form in many relation inventories (Warren, 1978; Barker and Szpakowicz, 1998; Rosario and Hearst, 2001; Nastase and Szpakowicz, 2003; Girju et al., 2005; Girju et al., 2007; Girju et al., 2009; Hendrickx et al., 2010; Tratz and Hovy, 2010). In Levi’s theory, M AKE2 means that the head of the noun compound is made up of or is a product of its modifier. There are three subtypes of this relation (we do not attempt to distinguish between them): 2. we select the top 20 most frequent patterns; (a) the modifier is a unit and the head is a configuration, e.g., root system; 3. we filter out all patterns that were extracted less than N times (we tried 5 and 10) and with less than M NCs per pattern (we t"
D11-1060,P02-1032,0,0.0370902,"Missing"
D11-1060,N09-2060,0,0.0352601,"Missing"
D11-1060,H05-1047,0,0.0151203,"(Downing, 1977), and they can capture fine-grained aspects of the meaning. For example, while both wrinkle treatment and migraine treatment express the same abstract relation T REATMENT-F OR -D ISEASE, fine-grained differences can be revealed using verbs, e.g., smooth can paraphrase the former, but not the latter. In many theories, verbs play an important role in NC derivation (Levi, 1978). Moreover, speakers often use verbs to make the hidden relation between the noun in a noun compound overt. This allows for simple extraction and for straightforward use in NLP tasks like textual entailment (Tatu and Moldovan, 2005) and machine translation (Nakov, 2008a). Finally, a single verb is often not enough, and the meaning is better approximated by a collection of verbs. For example, while malaria mosquito expresses C AUSE (and is paraphrasable using cause), further aspects of the meaning can be captured with more verbs, e.g., carry, spread, be responsible for, be infected with, transmit, pass on, etc. 4 Method We harvest noun compounds expressing some target abstract semantic relation (in the experiments below, this is Levi’s M AKE2 ), starting from a small number of initial seed patterns: paraphrasing verbs and"
D11-1060,W02-1028,0,0.0311195,"p algorithm to jointly harvest NCs and patterns (verbs and prepositions) that interpret them for a given abstract relation. First, we extract NCs using a small number of seed patterns from a given abstract relation. Then, using the extracted NCs, we harvest more patterns. This is repeated until no new NCs and patterns can be extracted or for a pre-specified number of iterations. Our approach combines pattern-based extraction and bootstrapping, which is novel for NC interpretation; however, such combinations have been used in other areas, e.g., named entity recognition (Riloff and Jones, 1999; Thelen and Riloff, 2002; Curran et al., 2007; McIntosh and Curran, 2009). The remainder of the paper is organized as follows: Section 2 gives an overview of related work, Section 3 motivates our semantic representation, Sections 4, 5, and 6 explain our method, dataset and experiments, respectively, Section 7 discusses the results, Section 8 provides error analysis, and Section 9 concludes with suggestions for future work. 2 Related Work As we mentioned above, the implicit relation between the two nouns forming a noun compound can often be expressed overtly using verbal and prepositional paraphrases. For example, stu"
D11-1060,P10-1070,0,0.255759,"e Web as a Corpus Su Nam Kim Computer Science & Software Engineering University of Melbourne Melbourne, VIC 3010 Australia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphras"
D11-1060,C94-2125,0,0.712245,"bourne, VIC 3010 Australia snkim@csse.unimelb.edu.au Abstract Preslav Nakov Department of Computer Science National University of Singapore 13 Computing Drive Singapore 117417 nakov@comp.nus.edu.sg Most work on noun compound interpretation has focused on two-word NCs. There have been two general lines of research: the first one derives the NC semantics from the semantics of the nouns it is made of (Rosario and Hearst, 2002; Moldovan et al., 2004; Kim and Baldwin, 2005; Girju, 2007; S´eaghdha, 2009; Tratz and Hovy, 2010), while the second one models the relationship between the nouns directly (Vanderwende, 1994; Lapata, 2002; Kim and Baldwin, 2006; Nakov and Hearst, 2006; Nakov and Hearst, 2008; Butnariu and Veale, 2008). Responding to the need for semantic lexical resources in natural language processing applications, we examine methods to acquire noun compounds (NCs), e.g., orange juice, together with suitable fine-grained semantic interpretations, e.g., squeezed from, which are directly usable as paraphrases. We employ bootstrapping and web statistics, and utilize the relationship between NCs and paraphrasing patterns to jointly extract NCs and such patterns in multiple alternating iterations. In"
D11-1060,I05-1082,1,\N,Missing
D11-1060,S10-1007,1,\N,Missing
D11-1060,C98-1015,0,\N,Missing
D12-1027,P06-2005,0,0.0601586,"ypically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general problem, which arises with informal sources like SMS messages and Tweets for just any language (Aw et al., 2006; Han and Baldwin, 2011). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language into another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. 1 The Egyptian Wikipedia is one notable exception. 287 For example, Marujo et al. (2011) described a rule-based system for adapting Brazilia"
D12-1027,baldwin-awab-2006-open,0,0.0531602,"h bitext but are relatively frequent in the larger Malay– English one; it also helps for some frequent words. Cross-lingual morphological variants. We increase the Indonesian options for a Malay word using morphology. Since the set of Indonesian options for a Malay word in pivoting is restricted to the Indonesian vocabulary of the small Indonesian– English bi-text, this is a severe limitation of pivoting. Thus, assuming a large monolingual Indonesian text, we first build a lexicon of the words in the text. Then, we lemmatize these words using two different lemmatizers: the Malay lemmatizer of Baldwin and Awab (2006), and a similar Indonesian lemmatizer. Since these two analyzers have different strengths and weaknesses, we combine their outputs to increase recall. Next, we group all Indonesian words that share the same lemma, e.g., for minum, we obtain {diminum, diminumkan, diminumnya, makan-minum, 4.1.3 Further Refinements makananminuman, meminum, meminumkan, meminumnya, meminumMany of our paraphrases are bad: some have very low probabilities, while others involve rare words for which the probability estimates are unreliable. minuman, minum, minum-minum, minum-minuman, minuman, minu5 For balance, in case"
D12-1027,W07-0702,0,0.0222216,"ns its original scores, which are further augmented with 1–3 additional feature scores indicating its origin: the first/second/third feature is 1 if the pair came from the first/second/both table(s), and 0 otherwise. We experiment using all three, the first two, or the first feature only; we also try setting the features to 0.5 instead of 0. This makes the following six combinations (0, 00, 000, .5, .5.5, .5.5.5); on testing, we use the one that achieves the highest BLEU score on the development set. Other possibilities for combining the phrase tables include using alternative decoding paths (Birch et al., 2007), simple linear interpolation, and direct phrase table merging with extra features (CallisonBurch et al., 2006); they were previously found inferior to the last two approaches above (Nakov and Ng, 2009; Nakov and Ng, 2012). 291 5 Experiments We run two kinds of experiments: (a) isolated, where we train on the synthetic “Indonesian”– English bi-text only, and (b) combined, where we combine it with the Indonesian–English bi-text. 5.1 Datasets In our experiments, we use the following datasets, normally required for Indonesian–English SMT: • Indonesian–English train bi-text (IN2EN): 28,383 sentenc"
D12-1027,N06-1003,0,0.232179,"Missing"
D12-1027,P07-1092,0,0.110876,"ttempt language adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches. Another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bi-text, which we do not have. Pivoting over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bi-text). 3 Malay and Indonesian Malay and Indonesian are closely related, mutual"
D12-1027,P05-1066,0,0.0330941,"Missing"
D12-1027,A00-1002,0,0.757957,"Missing"
D12-1027,P11-1038,0,0.020239,"the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general problem, which arises with informal sources like SMS messages and Tweets for just any language (Aw et al., 2006; Han and Baldwin, 2011). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language into another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. 1 The Egyptian Wikipedia is one notable exception. 287 For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian Portuguese (BP) to Eur"
D12-1027,2011.eamt-1.19,0,0.574391,"Missing"
D12-1027,D09-1141,1,0.921289,"ource-rich language, with whom they overlap in vocabulary and share cognates, which offers opportunities for bi-text reuse. Example pairs of such resource rich–poor languages include Spanish–Catalan, Finnish–Estonian, Swedish–Norwegian, Russian–Ukrainian, Irish– Gaelic Scottish, Standard German–Swiss German, Modern Standard Arabic–Dialectical Arabic (e.g., Gulf, Egyptian), Turkish–Azerbaijani, etc. Previous work has already demonstrated the benefits of using a bi-text for a related resource-rich language to X (e.g., X=English) to improve machine translation from a resource-poor language to X (Nakov and Ng, 2009; Nakov and Ng, 2012). Here we take a different, orthogonal approach: we adapt the resource-rich language to get closer to the resource-poor one. We assume a small bi-text for the resource-poor language, which we use to learn word-level and phrase-level paraphrases and cross-lingual morphological variants between the two languages. Assuming translation into the same target language X, we adapt (the source side of) a large training bi-text for a related resource-rich language and X. Training on the adapted large bi-text yields very significant improvements in translation quality compared to bot"
D12-1027,P11-1130,1,0.835885,"ants, relying on the Indonesian language model to make the right contextual choice. We also try to model context more directly by generating adaptation options at the phrase level. 7 While the different morphological forms typically have different meanings, e.g., minum (‘drink’) vs. peminum (‘drinker’), in some cases the forms could have the same translation in English, e.g., minum (‘drink’, verb) vs. minuman (‘drink’, noun). This is our motivation for trying morphological variants, even though they are almost exclusively derivational, and thus quite risky as translational variants; see also (Nakov and Ng, 2011). 290 Phrase-level paraphrase induction. We use standard phrase-based SMT techniques to build separate phrase tables for the Indonesian–English and the Malay–English bi-texts, where we have four conditional probabilities: forward/reverse phrase translation probability, and forward/reverse lexicalized phrase translation probability. We pivot over English to generate Indonesian-Malay phrase pairs, whose probabilities are derived from the corresponding ones in the two phrase tables using Eq. 1. Cross-lingual morphological variants. While phrase-level paraphrasing models context better, it remains"
D12-1027,P12-2059,1,0.766512,"tional Linguistics 2 Related Work One relevant line of research is on machine translation between closely related languages, which is arguably simpler than general SMT, and thus can be handled using word-for-word translation, manual language-specific rules that take care of the necessary morphological and syntactic transformations, or character-level translation/transliteration. This has been tried for a number of language pairs including Czech–Slovak (Hajiˇc et al., 2000), Turkish– Crimean Tatar (Altintas and Cicekli, 2002), Irish– Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are"
D12-1027,P02-1040,0,0.0838775,"e. 292 System ML2EN IN2EN Simple concatenation Balanced concatenation Sophisticated phrase table combination BLEU 14.50 18.67 18.49 19.79 20.10(.5.5) Table 2: The five baselines. The subscript indicates the parameters found on IN2EN-dev and used for IN2EN-test. The scores that are statistically significantly better than ML2EN and IN2EN (p &lt; 0.01, Collins’ sign test) are shown in bold and are underlined, respectively. 6.1 Baseline Experiments The results for the baseline systems are shown in Table 2. We can see that training on ML2EN instead of IN2EN yields over 4 points absolute drop in BLEU (Papineni et al., 2002) score, even though ML2EN is about 10 times larger than IN2EN and both bi-texts are from the same domain. This confirms the existence of important differences between Malay and Indonesian. While simple concatenation does not help, balanced concatenation with repetitions improves by 1.12 BLEU points over IN2EN, which shows the importance of giving IN2EN a proper weight in the combined bi-text. This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points. 6.2 Isolated Experiments Table 3 shows the results for the isolated"
D12-1027,W11-2602,0,0.0342981,"Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are typically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general problem, which arises with inform"
D12-1027,2010.amta-papers.5,0,0.191633,"002), Irish– Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are typically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. In fact, this is a more general proble"
D12-1027,N07-1061,0,0.09152,"our previous work did not attempt language adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches. Another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bi-text, which we do not have. Pivoting over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bi-text). 3 Malay and Indonesian Malay and Indonesian are"
D12-1027,P09-1018,0,0.122162,"ion, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; since it could not substitute one word for a completely different word, it did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bi-text, it makes sense to try to combine it further with the small bi-text; thus, below we will directly compare and combine these two approaches. Another alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bi-text, which we do not have. Pivoting over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bi-text). 3 Malay and Indonesian Malay and Indonesian are closely related, mutually intelligible Aust"
D12-1027,P98-2238,0,0.356824,"haracter-level translation/transliteration. This has been tried for a number of language pairs including Czech–Slovak (Hajiˇc et al., 2000), Turkish– Crimean Tatar (Altintas and Cicekli, 2002), Irish– Scottish Gaelic (Scannell, 2006), and Bulgarian– Macedonian (Nakov and Tiedemann, 2012). In contrast, we have a different objective – we do not carry out full translation but rather adaptation since our ultimate goal is to translate into a third language X. A special case of this same line of research is the translation between dialects of the same language, e.g., between Cantonese and Mandarin (Zhang, 1998), or between a dialect of a language and a standard version of that language, e.g., between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011). Here again, manual rules and/or language-specific tools are typically used. In the case of Arabic dialects, a further complication arises by the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online communities 1 such as social networks, chats, Twitter and SMS messages. This causes further mismatch in domain a"
D12-1027,C98-2233,0,\N,Missing
D14-1027,W14-3352,1,0.726645,"Missing"
D14-1027,W07-0718,0,0.384691,"Missing"
D14-1027,W11-2103,0,0.0593209,"ourse parser can be downloaded from http://alt.qcri.org/tools/ 216 In particular, let r and r0 be the references for the pairs ht1 , t2 i and ht01 , t02 i, we can redefine all the members of Eq. 1, e.g., K(t1 , t01 ) becomes K(ht1 , ri, ht01 , r0 i) = PTK(φM (t1 , r), φM (t01 , r0 )) + PTK(φM (r, t1 ), φM (r0 , t01 )), In other words, we only consider the trees enriched by markers separately, and ignore the edges connecting both trees. 3 Experiments and Discussion We experimented with datasets of segment-level human rankings of system outputs from the WMT11 and the WMT12 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012): we used the WMT11 dataset for training and the WMT12 dataset for testing. We focused on translating into English only, for which the datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr). There were about 10,000 non-tied human judgments per language pair per dataset. We scored our pairwise system predictions with respect to the WMT12 human judgments using the Kendall’s Tau (τ ), which was official at WMT12. Table 1 presents the τ scores for all metric variants introduced in this paper: for the individual language pairs"
D14-1027,W05-0904,0,0.114217,"ndation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans hav"
D14-1027,W12-3102,0,0.167189,"Missing"
D14-1027,W12-3129,0,0.108627,".org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translatio"
D14-1027,W10-1750,1,0.877508,"Missing"
D14-1027,P07-1098,1,0.765065,"ework we propose consists in: (i) designing a structural representation, e.g., using syntactic and discourse trees of translation hypotheses and a references; and (ii) applying structural kernels (Moschitti, 2006; Moschitti, 2008), to such representations in order to automatically inject structural features in the preference re-ranking algorithm. We use this method with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are"
D14-1027,W08-0331,0,0.225781,"y, which were widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the fe"
D14-1027,W07-0738,1,0.935253,"Missing"
D14-1027,P14-1065,1,0.878614,"Missing"
D14-1027,P02-1040,0,0.0928706,"ranslations Francisco Guzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges"
D14-1027,D12-1083,1,0.785191,"relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feat"
D14-1027,W07-0707,0,0.511133,"Missing"
D14-1027,P13-1048,1,0.81161,"L "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0 Kmt (ht1 , t2 i, ht1 , t2 i) = φmt (t1 , t2 ) · φmt (t1 , t2 ), where φmt maps pairs into the feature space. Consideri"
D14-1027,C14-1020,1,0.886934,"Missing"
D14-1027,D14-1050,1,0.886979,"Missing"
D14-1027,W13-3509,1,0.930152,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,P13-2125,1,0.924292,"thod with translation-reference pairs to directly learn the features themselves, instead of learning the importance of a predetermined set of features. A similar learning framework has been proven to be effective for question answering (Moschitti et al., 2007), and textual entailment recognition (Zanzotto and Moschitti, 2006). 2.1 Representations To represent a translation-reference pair (t, r), we adopt shallow syntactic trees combined with RSTstyle discourse trees. Shallow trees have been successfully used for question answering (Severyn and Moschitti, 2012) and semantic textual similarity (Severyn et al., 2013b); while discourse information has proved useful in MT evaluation (Guzm´an et al., 2014; Joty et al., 2014). Combined shallow syntax and discourse trees worked well for concept segmentation and labeling (Saleh et al., 2014a). Our goals are twofold: (i) in the short term, to demonstrate that structural kernel learning is suitable for this task, and can effectively learn to rank hypotheses at the segment-level; and (ii) in the long term, to show that this approach provides a unified framework that allows to integrate several layers of linguistic analysis and information and to improve over the"
D14-1027,2006.amta-papers.25,0,0.625255,"uzm´an Shafiq Joty Llu´ıs M`arquez Alessandro Moschitti Preslav Nakov Massimo Nicosia ALT Research Group Qatar Computing Research Institute — Qatar Foundation {fguzman,sjoty,lmarquez,amoschitti,pnakov,mnicosia}@qf.org.qa Abstract As a result, this has enabled rapid development in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some ev"
D14-1027,W11-2113,0,0.0478224,"re widely used in the past, are now discontinued in favor of ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems instead. It has been shown that using such ranking-based assessments yields much higher inter-annotator agreement (CallisonBurch et al., 2007). While evaluation metrics still produce numerical scores, in part because MT evaluation shared tasks at NIST and WMT ask for it, there has also been work on a ranking formulation of the MT evaluation task for a given set of outputs. This was shown to yield higher correlation with human judgments (Duh, 2008; Song and Cohn, 2011). We present a pairwise learning-to-rank approach to machine translation evaluation that learns to differentiate better from worse translations in the context of a given reference. We integrate several layers of linguistic information encapsulated in tree-based structures, making use of both the reference and the system output simultaneously, thus bringing our ranking closer to how humans evaluate translations. Most importantly, instead of deciding upfront which types of features are important, we use the learning framework of preference re-ranking kernels to learn the features automatically."
D14-1027,N03-1033,0,0.00879476,"o-REL .-REL &apos;&apos;-REL to think . "" to think . "" relation propagation direction DIS:ELABORATION Bag-of-words relations to TO-REL "" `` give them no VB-REL PRP-REL DT NP-REL VP time NN-REL TO-REL VP-REL NP .-REL &apos;&apos;-REL o-REL o-REL EDU:SATELLITE-REL EDU:NUCLEUS b) Reference VB-REL DIS:ELABORATION Figure 1: Hypothesis and reference trees combining discourse, shallow syntax and POS. Figure 1 shows two example trees combining discourse, shallow syntax and POS: one for a translation hypothesis (top) and the other one for the reference (bottom). To build such structures, we used the Stanford POS tagger (Toutanova et al., 2003), the Illinois chunker (Punyakanok and Roth, 2001), and the discourse parser1 of (Joty et al., 2012; Joty et al., 2013). The lexical items constitute the leaves of the tree. The words are connected to their respective POS tags, which are in turn grouped into chunks. Then, the chunks are grouped into elementary discourse units (EDU), to which the nuclearity status is attached (i.e., N UCLEUS or S ATELLITE). Finally, EDUs and higher-order discourse units are connected by discourse relations (e.g., D IS :E LABORATION). 2.2 More specifically, KMs carry out learning using the scalar product 0 0 0 0"
D14-1027,D12-1097,0,0.0991997,"elopment in the field of statistical machine translation (SMT), by allowing to train and tune systems as well as to track progress in a way that highly correlates with human judgments. Today, MT evaluation is an active field of research, and modern metrics perform analysis at various levels, e.g., lexical (Papineni et al., 2002; Snover et al., 2006), including synonymy and paraphrasing (Lavie and Denkowski, 2009); syntactic (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005); semantic (Gim´enez and M`arquez, 2007; Lo et al., 2012); and discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014; Joty et al., 2014). Automatic MT evaluation metrics compare the output of a system to one or more human references in order to produce a similarity score. The quality of such a metric is typically judged in terms of correlation of the scores it produces with scores given by human judges. As a result, some evaluation metrics have been trained to reproduce the scores assigned by humans as closely as possible (Albrecht and Hwa, 2008). Unfortunately, humans have a hard time assigning an absolute score to a translation. Hence, direct human evaluation scores such as adequacy"
D14-1027,P06-1051,1,\N,Missing
D14-1050,N09-1003,0,0.0327503,"Missing"
D14-1050,J92-4003,0,0.375881,"reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic similarity, while also combining them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the rerankin"
D14-1050,S13-2060,0,0.012357,"a baseline, we picked the best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100."
D14-1050,W10-2802,0,0.024579,"best-scoring hypothesis in the list, i.e., the output by the regular semi-CRF parser. The setting is exactly the same as that described in (Saleh et al., 2014). Evaluation measure. In all experiments, we used the harmonic mean of precision and recall (F1 ) (van Rijsbergen, 1979), computed at the token level and micro-averaged across the different semantic types.6 Similarity matrix for SK. We compute the lexical similarity for SK by applying LSA (Furnas et al., 1988) to Tripadvisor data. The dataset and the exact procedure for creating the LSA matrix are described in (Castellucci et al., 2013; Croce and Previtali, 2010). 4.2 Results Oracle accuracy. Table 2 shows the oracle F1 score for N -best lists of different lengths, i.e., the F1 that is achieved by picking the best candidate in the N -best list for various values of N . Considering 5-best lists yields an increase in oracle F1 of almost ten absolute points. Going up to 10-best lists only adds 2.5 extra F1 points. The complete 100-best lists add 3.5 extra F1 points, for a total of 98.72. This very high value is explained by the fact that often the total number of different annotations for a given question is smaller than 100. In our experiments, we will"
D14-1050,H94-1053,0,0.260291,"e list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with respect to the gold standard"
D14-1050,D11-1096,1,0.95073,"SKS) (b) SKS with Brown Clusters Figure 1: CSL structures: standard and with Brown Clusters. Another relevant line of research are the semantic kernels, i.e., kernels that use lexical similarity between features. One of the first that applyed LSA was (Cristianini et al., 2002), whereas (Bloehdorn et al., 2006; Basili et al., 2006) used WordNet. Semantic structural kernels of the type we use in this paper were first introduced in (Bloehdorn and Moschitti, 2007a; Bloehdorn and Moschitti, 2007b). The most advanced model based on tree kernels, which we also use in this paper, is the Smoothed PTK (Croce et al., 2011). We further apply a semantic kernel (SK), namely the Smoothed Partial Tree Kernel (Croce et al., 2011), which uses the lexical similarity between the tree nodes, while computing the substructure space. This is the first time that SKs are applied to reranking hypotheses. This (i) makes the global sentence structure along with concepts available to the learning algorithm, and (ii) enables computing the similarity between lexicals in syntactic patterns that are enriched by concepts. We tested our models on the Restaurant domain. Our results show that: (i) The basic CRF parser, which uses semi-Ma"
D14-1050,W06-2909,1,0.683466,"d Brown clusters, and another one using semantic similarity among the words composing the structure. The results on a corpus from the restaurant domain show that our semantic kernels exploiting similarity measures outperform state-of-the-art rerankers. 1 {$and [{cuisine:&quot;lebanese&quot;},{city:&quot;doha&quot;}, {price:&quot;low&quot;},{amenity:&quot;carry out&quot;}]} The state-of-the-art of CSL is represented by conditional models for sequence labeling such as Conditional Random Fields (CRFs) (Lafferty et al., 2001) trained with simple morphological and lexical features. The basic CRF model was improved by means of reranking (Moschitti et al., 2006; Dinarelli et al., 2012) using structural kernels (Moschitti, 2006). Although these methods exploited sentence structure, they did not use syntax at all. More recently, we applied shallow syntactic structures and discourse parsing with slightly better results (Saleh et al., 2014). However, the most obvious models for semantic parsing, i.e., rerankers based on semantic structural kernels (Bloehdorn and Moschitti, 2007b), had not been applied to semantic structures yet. In this paper, we study the impact of semantic information conveyed by Brown Clusters (BCs) (Brown et al., 1992) and semantic"
D14-1050,C10-5001,1,0.846723,"ng them with innovative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word se"
D14-1050,P12-4002,1,0.857346,"vative features. We use reranking, similarly to (Saleh et al., 2014), to select the best hypothesis annotated with concepts predicted by a local model. The competing hypotheses are represented as innovative trees enriched with the semantic concepts and BC labels. The trees can capture dependencies between sentence constituents, concepts and BCs. However, extracting explicit features from them is rather difficult as their number is exponentially large. Thus, we rely on (i) Support Vector Machines (Joachims, 1999) to train the reranking functions and on (ii) structural kernels (Moschitti, 2010; Moschitti, 2012; Moschitti, 2013) to automatically encode tree fragments that represent syntactic and semantic dependencies from words and concepts. Introduction Spoken Language Understanding aims to interpret user utterances and to convert them to logical forms or, equivalently, to database queries, which can then be used to satisfy the user’s information needs. This process is known as Concept Segmentation and Labeling (CSL), also called semantic parsing in the speech community: it maps utterances into meaning representations based on semantic constituents. The latter are basically word sequences, often re"
D14-1050,H89-1026,0,0.0214012,"hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whether Hi is better than Hj . Given a training question Q, positive and negative examples are built for training the classifier. Let H1 be the hypothesis with the lowest error rate with resp"
D14-1050,P10-1023,0,0.0382136,"Missing"
D14-1050,J07-2002,0,0.0577582,"Missing"
D14-1050,N03-1033,0,0.00778762,"the hypothesis. 3.3 Semantic structures Tree kernels allow us to compute structural similarities between two trees; thus, we engineered a special structure for the CSL task. In order to capture the structural dependencies between the semantic tags,1 we use a basic tree (see for example Figure 1a), where the words of a sentence are tagged with their semantic tags. 4 Experiments The experiments aim at investigating the role of feature vectors, PTK, SK and BCs in reranking. We first describe the experimental setting and then we move into the analysis of the results. 2 We use the Stanford tagger (Toutanova et al., 2003). For instance, if the output sequence is Other-RatingOther-Amenity the 3-gram patterns would be: S-OtherRating, Other-Rating-Other, Rating-Other-Amenity, and Other-Amenity-E. 3 1 They are associated with the following IDs: 0-Other, 1-Rating, 2-Restaurant, 3-Amenity, 4-Cuisine, 5-Dish, 6Hours, 7-Location, and 8-Price. 438 semi-CRF Reranker Train 6,922 7,000 Devel. 739 3,695 Test 1,521 7,605 Total 9,182 39,782 N F1 2 87.76 5 92.63 10 95.23 100 98.72 Table 2: Oracle F1 score for N -best lists. Table 1: Number of instances and pairs used to train the semi-CRF and rerankers, respectively. 4.1 1 83"
D14-1050,N04-3012,0,0.201946,"Missing"
D14-1050,H91-1020,0,0.482894,"probability to be globally correct as estimated using local classifiers or global classifiers that only use local features. Then, a reranker, typically a meta-classifier, tries to select the best hypothesis from the list. The reranker can exploit global information, and specifically, the dependencies between the different concepts, which are made available by the local model. We use semi-CRFs for the local model as they yield the highest accuracy in CSL (when using a single model) and preference reranking for the global reranker. Related Work 3.1 One of the early approaches to CSL was that of Pieraccini et al. (1991), where the word sequences and concepts were modeled using Hidden Markov Models (HMMs) as observations and hidden states, respectively. Generative models were exploited by Seneff (1989) and Miller et al. (1994), who used stochastic grammars for CSL. Other discriminative models followed such preliminary work, e.g., (Rubinstein and Hastie, 1997; Santaf´e et al., 2007; Raymond and Riccardi, 2007). CRF-based models are considered to be the state of the art in CSL (De Mori et al., 2008). Preference Reranking (PR) PR uses a classifier C, which takes a pair of hypotheses hHi , Hj i and decides whethe"
D14-1050,C14-1020,1,0.793271,"Missing"
D14-1050,C10-5000,0,\N,Missing
D14-1145,W07-1106,0,0.0207113,"present tense, pl: plural, poss: possesive, prog:progressive tense, sg: singular 1392 3 http://www.nisanyansozluk.com/ However, indefinite objects of the verb are left unmarked for case. In example (6), yapmak takes an indefinite object (food) as the complement. The boundary between [N V] constructions with indefinite nominal objects and LVCs are somewhat blurry. In both cases, the meaning of the verbal complement is bleached out and the nominal complement weighs heavier than the verbal one. We will not dwell further on this subtle distinction, but we plan future work on this topic following Cook et al. (2007) and Vincze et al. (2013). Example 6: Bazen [yemek yap-ar-dı-m] Sometimes [food do-pres-past-1sg] I used to sometimes prepare food. Since Dutch does not mark objects of the verb morphologically, NL-Turkish speakers have difficulty (e.g., unnecessary addition or omission of case markers) in determining the definiteness of the nominal complements in [N V] constructions (Do˘gru¨oz and Backus, 2009). Therefore, we expect this feature to differentiate well between NLTurkish and TR-Turkish [N V] constructions and LVCs with yapmak/etmek as verbal complements. 2.2 Finiteness The verbs in LVCs are assu"
D14-1145,A00-1002,0,0.130321,"Missing"
D14-1145,D09-1141,1,0.823397,"ng four linguistic features seem to be largely irrelevant. 6 Conclusion and Future Work Language technologies are usually developed for standard dialects, ignoring the linguistic differences in other dialects such as those in immigrant contexts. One of the reasons for this is the difficulty of assessing and predicting linguistic differences across dialects. This is similar to efforts to translate well-established Arabic dialects (Bakr et al., 2008; Sawaf, 2010), or to adapt between Brazilian and European Portuguese (Marujo et al., 2011), Czech–Slovak (Hajiˇc et al., 2000), Spanish–Portuguese (Nakov and Ng, 2009; Nakov and Ng, 2012), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Bulgarian–Macedonian (Nakov and Tiedemann, 2012), Malay–Indonesian (Wang et al., 2012) or Mandarin–Cantonese (Zhang, 1998). In this work, we have built a classifier that uses LVCs to differentiate between two different Turkish dialects: standard and immigrant. The results indicate that contextual features are most useful for this task. Although this requires further investigation, we can explain it by the thousands of features context generates: each contextual word is a feature."
D14-1145,P12-2059,1,0.851327,"ignoring the linguistic differences in other dialects such as those in immigrant contexts. One of the reasons for this is the difficulty of assessing and predicting linguistic differences across dialects. This is similar to efforts to translate well-established Arabic dialects (Bakr et al., 2008; Sawaf, 2010), or to adapt between Brazilian and European Portuguese (Marujo et al., 2011), Czech–Slovak (Hajiˇc et al., 2000), Spanish–Portuguese (Nakov and Ng, 2009; Nakov and Ng, 2012), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Bulgarian–Macedonian (Nakov and Tiedemann, 2012), Malay–Indonesian (Wang et al., 2012) or Mandarin–Cantonese (Zhang, 1998). In this work, we have built a classifier that uses LVCs to differentiate between two different Turkish dialects: standard and immigrant. The results indicate that contextual features are most useful for this task. Although this requires further investigation, we can explain it by the thousands of features context generates: each contextual word is a feature. Thus, it is very hard for our grammatical features to compete against contextual features but they do have an impact. We are planning to extend our study to dialec"
D14-1145,C12-3048,0,0.0167145,"07; Butt, 2010). NL-Turkish LVCs are changing due to Dutch influence (Do˘gru¨oz and Backus, 2007; Do˘gru¨oz and Backus, 2009; Do˘gru¨oz and Gries, 2012). However, assessing Dutch influence is not always easy since NL-Turkish LVCs still co-exist with the TRTurkish LVCs. This study aims to automatically identify the features that can distinguish between NL-Turkish and TR-Turkish LVCs. Our study would benefit Machine Translation systems targeting dialectal variation. It differs from studies concerning the well-established dialectal variations of Arabic, e.g., Levantine, Gulf, Egyptian, Maghrebi (Salloum and Habash, 2012), EU vs. Brazilian Portuguese (Marujo et al., 2011) or Turkish vs. Tatar (Altintas and Cicekli, 2002). In contrast, we are interested in developing language technologies for immigrant dialects, which are often understudied and lack written resources due to their unofficial status. When immigrant speakers face communication difficulties (e.g., bureaucratic affairs with the local officials, teacherparent meetings, doctor-patient conversations) in the local languages (e.g., Dutch) of the host country, they are often provided with translation equivalents in the standard dialect (e.g., TR-Turkish)"
D14-1145,2010.amta-papers.5,0,0.0193071,"t split). The other useful features are the nominal complements and the information about the etymological origin of the borrowed LVCs. The remaining four linguistic features seem to be largely irrelevant. 6 Conclusion and Future Work Language technologies are usually developed for standard dialects, ignoring the linguistic differences in other dialects such as those in immigrant contexts. One of the reasons for this is the difficulty of assessing and predicting linguistic differences across dialects. This is similar to efforts to translate well-established Arabic dialects (Bakr et al., 2008; Sawaf, 2010), or to adapt between Brazilian and European Portuguese (Marujo et al., 2011), Czech–Slovak (Hajiˇc et al., 2000), Spanish–Portuguese (Nakov and Ng, 2009; Nakov and Ng, 2012), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Bulgarian–Macedonian (Nakov and Tiedemann, 2012), Malay–Indonesian (Wang et al., 2012) or Mandarin–Cantonese (Zhang, 1998). In this work, we have built a classifier that uses LVCs to differentiate between two different Turkish dialects: standard and immigrant. The results indicate that contextual features are most useful for this"
D14-1145,P13-2046,0,0.0190987,"ral, poss: possesive, prog:progressive tense, sg: singular 1392 3 http://www.nisanyansozluk.com/ However, indefinite objects of the verb are left unmarked for case. In example (6), yapmak takes an indefinite object (food) as the complement. The boundary between [N V] constructions with indefinite nominal objects and LVCs are somewhat blurry. In both cases, the meaning of the verbal complement is bleached out and the nominal complement weighs heavier than the verbal one. We will not dwell further on this subtle distinction, but we plan future work on this topic following Cook et al. (2007) and Vincze et al. (2013). Example 6: Bazen [yemek yap-ar-dı-m] Sometimes [food do-pres-past-1sg] I used to sometimes prepare food. Since Dutch does not mark objects of the verb morphologically, NL-Turkish speakers have difficulty (e.g., unnecessary addition or omission of case markers) in determining the definiteness of the nominal complements in [N V] constructions (Do˘gru¨oz and Backus, 2009). Therefore, we expect this feature to differentiate well between NLTurkish and TR-Turkish [N V] constructions and LVCs with yapmak/etmek as verbal complements. 2.2 Finiteness The verbs in LVCs are assumed to be flexible for in"
D14-1145,D12-1027,1,0.816672,"ialects such as those in immigrant contexts. One of the reasons for this is the difficulty of assessing and predicting linguistic differences across dialects. This is similar to efforts to translate well-established Arabic dialects (Bakr et al., 2008; Sawaf, 2010), or to adapt between Brazilian and European Portuguese (Marujo et al., 2011), Czech–Slovak (Hajiˇc et al., 2000), Spanish–Portuguese (Nakov and Ng, 2009; Nakov and Ng, 2012), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Bulgarian–Macedonian (Nakov and Tiedemann, 2012), Malay–Indonesian (Wang et al., 2012) or Mandarin–Cantonese (Zhang, 1998). In this work, we have built a classifier that uses LVCs to differentiate between two different Turkish dialects: standard and immigrant. The results indicate that contextual features are most useful for this task. Although this requires further investigation, we can explain it by the thousands of features context generates: each contextual word is a feature. Thus, it is very hard for our grammatical features to compete against contextual features but they do have an impact. We are planning to extend our study to dialects in other immigrant settings (e.g.,"
D14-1145,P98-2238,0,0.121173,"s. One of the reasons for this is the difficulty of assessing and predicting linguistic differences across dialects. This is similar to efforts to translate well-established Arabic dialects (Bakr et al., 2008; Sawaf, 2010), or to adapt between Brazilian and European Portuguese (Marujo et al., 2011), Czech–Slovak (Hajiˇc et al., 2000), Spanish–Portuguese (Nakov and Ng, 2009; Nakov and Ng, 2012), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Bulgarian–Macedonian (Nakov and Tiedemann, 2012), Malay–Indonesian (Wang et al., 2012) or Mandarin–Cantonese (Zhang, 1998). In this work, we have built a classifier that uses LVCs to differentiate between two different Turkish dialects: standard and immigrant. The results indicate that contextual features are most useful for this task. Although this requires further investigation, we can explain it by the thousands of features context generates: each contextual word is a feature. Thus, it is very hard for our grammatical features to compete against contextual features but they do have an impact. We are planning to extend our study to dialects in other immigrant settings (e.g., Turkish in Germany) and to other typ"
D14-1145,C98-2233,0,\N,Missing
D15-1068,S15-2047,1,0.877035,"Missing"
D15-1068,S12-1059,0,0.0177823,"Missing"
D15-1068,S15-2036,1,0.541482,"Missing"
D15-1068,P15-2113,1,0.19975,"Missing"
D15-1068,P04-1035,0,0.00602109,"og sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The mixing parameter λ ∈ [0, 1] determines the relative strength of the two components. Our approach is inspired by Pang and Lee (2004), where they model the proximity relation between sentences for finding subjective sentences in product reviews, whereas we are interested in global inference based on local classifiers. The optimization problem can be efficiently solved by finding a minimum cut of a weighted undirected graph G = (V, E). The set of nodes V = {v1 , v2 , · · · , vn , s, t} represent the n comments in a thread, the source and the sink. We connect each comment node vi to the source node s by adding an edge w(vi , s) with capacity siG , and to the sink node t by adding an edge w(vi , t) with capacity siB . Finally,"
D15-1068,I11-1164,0,0.0214203,"ssification in Community Question Answering ˜ Giovanni Da San Martino, Simone Filice, Shafiq Joty, Alberto Barr´on-Cedeno, Llu´ıs M`arquez, Alessandro Moschitti, and Preslav Nakov, Qatar Computing Research Institute, HBKU {sjoty,albarron,gmartino,sfilice, lmarquez,amoschitti,pnakov}@qf.org.qa Abstract As question-comment threads can get quite long, finding good answers in a thread can be timeconsuming. This has triggered research in trying to automatically determine which answers might be good and which ones are likely to be bad or irrelevant. One early work going in this direction is that of Qu and Liu (2011), who tried to determine whether a question is “solved” or not, given its associated thread of comments. As a first step in the process, they performed a comment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local feat"
D15-1068,W04-2401,0,0.398636,"he CQA-QL dataset: after merging Bad and Potential into Bad. The Task 1 Train 2,600 16,541 8,069 8,472 3 http://www.qatarliving.com/moving-qatar/posts/can-iobtain-driving-license-my-qid-written-employee http://www.qatarliving.com/forum http://alt.qcri.org/semeval2015/task3/ 574 3.1 ci and cj have the same label; assigning 0 to xijS means that ci and cj do not have the same label. The same interpretation holds for the other possible classes (in this case only Different).4 Let ciG be the cost of classifying ci as Good, cijS be the cost of assigning the same labels to ci and cj , etc. Following (Roth and Yih, 2004), these costs are obtained from local classifiers by taking log probabilities, i.e., ciG = − log siG , cijS = − log sij , etc. The goal of the ILP problem is to find an assignment A to all variables xiG , xiB , xijS , xijD that minimizes the cost function: Graph Partition Approach Here our goal is to find a partition P = (G, B) that minimizes the following cost: hX i X X C(P ) = λ siB + siG + (1 − λ) sij ci ∈G ci ∈B ci ∈G,cj ∈B The first part of the cost function discourages misclassification of individual comments, while the second part encourages similar comments to be in the same class. The"
D15-1068,S15-2035,0,0.084801,"ment-level classification, considering four classes: problem, solution, good feedback, and bad feedback. More recently, the shared task at SemEval 2015 on Answer Selection in CQA (Nakov et al., 2015), whose benchmark datasets we will use below, tackled the task of identifying good, potentially useful, and bad comments within a thread. In that task, the top participating systems used threadlevel features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in"
D15-1068,W03-0402,0,0.128215,"Missing"
D15-1068,P15-2117,0,0.0999102,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,W01-0515,0,0.0287716,"he comment-pair variables are consistent: xijD = xiG ⊕ xjG , ∀i, j 1 ≤ i < j ≤ n. λ ∈ [0, 1] is a parameter used to balance the contribution of the two sources of information. 4 Local Classifiers For classification, we use Maximum Entropy, or MaxEnt, (Murphy, 2012), as it yields a probability distribution over the class labels, which we then use directly for the graph arcs and the ILP costs. 4.1 Good-vs-Bad Classifier Our most important features measure the similarity between the question (q) and the comment (c). We compare lemmata and POS [1, 2, 3, 4]-grams using Jaccard (1901), containment (Lyon et al., 2001), and cosine, as well as using some similarities from DKPro (B¨ar et al., 2012) such as longest common substring (Allison and Dix, 1986) and greedy string tiling (Wise, 1996). We also compute similarity using partial tree kernels (Moschitti, 2006) on shallow syntactic trees. Forty-three Boolean features express whether (i) c includes URLs or emails, the words “yes”, “sure”, “no”, “neither”, “okay”, etc., as well as ‘?’ and ‘@’ or starts with “yes” (12 features); (ii) c includes a word longer than fifteen characters (1); Integer Linear Programming Approach Here we follow the inference with clas"
D15-1068,S15-2037,0,0.0777175,"on to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread (Hou et al., 2015). Similarly, our participation, which achieved the third-best postition, used features that try to describe a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolution neural networks to recognize good comments (Zhou et al., 2015b). In a follow-up work, Zhou et al. (2015a) included a long-short term memory in their convolution neural network to learn the classification sequence for the thread. In parallel, in our recent work (Barr´on-Cede˜no et al., 2015), we tried to exploit the dependencies between the thread comments to tackle the same task. We did it by designing features that look globally at the thread and by applying structured prediction models, such as Conditional Random Fields (Lafferty et al., 2001). Community question answering, a recent evolution of question answering in the Web context, allows a user to"
D15-1068,D07-1002,0,\N,Missing
D15-1068,N10-1145,0,\N,Missing
D15-1068,P07-1098,1,\N,Missing
D15-1068,C10-1131,0,\N,Missing
D15-1068,N13-1106,0,\N,Missing
D15-1068,S15-2038,0,\N,Missing
D15-1068,P08-1082,0,\N,Missing
D15-1068,W13-3509,1,\N,Missing
D15-1068,D13-1044,1,\N,Missing
D16-1165,S16-1138,0,0.0817602,"Missing"
D16-1165,J93-2003,0,0.046238,"ceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, which takes as input the original question and two comments together with thei"
D16-1165,P15-2114,0,0.286591,"h q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subtask B, respectively. In this paper, we mainly"
D16-1165,P08-1019,0,0.141574,"mple, q and q 0 are indeed related, and c is a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presente"
D16-1165,P03-1003,0,0.125068,"input components. It does so in a modular kernel function, including stacking from independent subtask A and B classifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of s"
D16-1165,S16-1172,0,0.37884,"provide labeled examples for the so called “subtask A” (q 0 c; appropriateness) and “subtask B” (qq 0 ; relatedness), one could use this supervision to help train the neural network for the primary cQA task. We observed that relatedness has proven quite informative. However, the improvements observed from using appropriateness were more modest. 10 As measured by the relative drop in MAP performance. System MAP AvgRec MRR System MAP AvgRec MRR Full Network Full + appr. preds. 54.51 55.82 60.93 61.63 62.94 62.39 Full Network + subtask A preds. * 1st (Mihaylova et al., 2016) Full Network * 2nd (Filice et al., 2016) * 3rd (Mihaylov and Nakov, 2016b) ... SemEval Average ... SemEval Worst 55.82 55.41 54.51 52.95 51.68 ... 49.30 ... 43.20 Baseline 2 (IR+chron.) 40.36 45.97 45.83 Table 3: Using appropriateness predictions. We present here a stacked experiment in which an additional neural network trained to predict appropriateness is used to inform the full network model. More concretely, we train a feed-forward pairwise neural network for subtask A, which is a simplification of the architecture from Figure 2. The input is reduced to three elements (q 0 , c1 , c2 ), where q 0 is the thread question and c1 an"
D16-1165,P15-1078,1,0.879885,"Missing"
D16-1165,P16-2075,1,0.638784,"Missing"
D16-1165,S16-1137,1,0.835795,"Missing"
D16-1165,S16-1131,0,0.0212185,"od vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, wh"
D16-1165,P11-1143,0,0.142772,"indeed related, and c is a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subta"
D16-1165,P16-2065,1,0.915419,"t the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different way. This search can be h"
D16-1165,S16-1136,1,0.898572,"t the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different way. This search can be h"
D16-1165,K15-1032,1,0.799997,"types are relevant, but the most important ones are the lexical similarity features, the domain-specific features, and the syntactic and semantic embeddings. 1 Web forums try to solve problem (a) in various ways, most often by allowing users to up/downvote answers according to their perceived usefulness, which makes it easier to retrieve useful answers in the future. Unfortunately, this negatively penalizes recent comments, which might be the most relevant and updated ones. This is due to the time it takes for a comment to accumulate votes. Moreover, voting is prone to abuse by forum trolls (Mihaylov et al., 2015; Mihaylov and Nakov, 2016a). Introduction In recent years, community Question Answering (cQA) forums, such as StackOverflow, Quora, Qatar Living, etc., have gained a lot of popularity as a source of knowledge and information. These forums typically organize their content in the form of multiple topic-oriented question–comment threads, where a question posed by a user is followed by a list of other users’ comments, which intend to answer the question. Problem (b) is harder to solve, as it requires that users verify that their question has not been asked before, possibly in a slightly different"
D16-1165,S16-1129,1,0.836098,"Missing"
D16-1165,N13-1090,0,0.0460853,"ion, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply averaging over the embeddings of all words in the text. Features We experiment with three kinds of features: (i) lexical features that measure similarity at a word, word n-gram, and paraphrase level, (ii) distributed representations that measure similarity at a syntactic and semantic level, (iii) domain-specific knowledge features, which capture similarity using thread-level information and other features that have proven valuable to solve similar tasks (Nicosia et al., 2015). 4.1 Lexical similarity features These types of features mea"
D16-1165,S16-1128,0,0.0167536,"of AvgRec and MRR. Note that, even without the Subtask A predictions, our pairwise neural network still produces results that are on par with the state of the art (with improvements slightly over one point in both cases). 6 Related Work Recently, a variety of neural network models have been applied to community question answering tasks such as question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2015) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Feng et al., 2015; Tan et al., 2015; Filice et al., 2016; Barr´on-Cede˜no et al., 2016; Mohtarami et al., 2016). Most of these papers concentrate on constructing advanced neural network architectures in order to model the problem at hand better. For instance, dos Santos et al. (2015) propose a neural network approach combining a convolutional neural network and a bag-of-words representation for modeling question-question similarity. Similarly, Tan et al. (2015) adopt a neural attention mechanism over bidirectional long short-term memory (LSTM) neural network to generate better answer representations given the questions. Similarly, Lei et al. (2015) use a combination of recurrent and convolutional neura"
D16-1165,S15-2047,1,0.902818,"Missing"
D16-1165,S15-2036,1,0.912003,"Missing"
D16-1165,P02-1040,0,0.104411,"ivation at the output is f (q, q10 , c1 , q20 , c2 ) = sig(wvT [φ(q, q10 , c1 , q20 , c2 ), ψ(q, q10 ), ψ(q, q20 ), ψ(q10 , c1 ), ψ(q20 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). We use these feature vectors to encode machine translation evaluation measures, components thereof, cQA task-specific features, etc. The next section gives more detail about these features. MT FEATS We use (as pairwise features) the following six machine translation evaluation features: (i) B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply"
D16-1165,P07-1059,0,0.107973,"stacking from independent subtask A and B classifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, a"
D16-1165,2006.amta-papers.25,0,0.0717448,"ponents thereof, cQA task-specific features, etc. The next section gives more detail about these features. MT FEATS We use (as pairwise features) the following six machine translation evaluation features: (i) B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). (ii) NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). (iii) TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). (iv) M ETEOR: A complex measure, which matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). (v) Unigram P RECISION and R ECALL. 4 G OOGLE VEC We use the pre-trained, 300dimensional embedding vectors from WORD 2 VEC (Mikolov et al., 2013). We compute a vector representation of the text by simply averaging over the embeddings of all words in the text. Features We experiment with three kinds of features: (i) lexical features that measure similarity at a word, word n-gram, and paraphrase level, (ii) distributed representations that measure similari"
D16-1165,P13-1045,0,0.0380397,"of the reference, length ratio between them, and BLEU’s brevity penalty. Again, these are computed over the same six pairs of vectors as before. 4.2 Distributed representations We use the following vector-based embeddings of all input components: q, c1 , c2 , q10 , and q20 . QL VEC We train in-domain word embeddings using WORD 2 VEC on all available QatarLiving data. Again, we use these embeddings to compute 100dimensional vector representations for all input components by averaging over all words in the texts. S YNTAX VEC We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25dimensional vector that is produced internally as a by-product of parsing. Moreover, we use the above vectors to calculate pairwise similarity features, i.e., the cosine between the following six vector pairs: (q, c1 ), (q, c2 ), (q10 , c1 ), (q20 , c2 ), (q, q10 ) and (q, q20 ). 4.3 Domain-specific features We extract various domain-specific features that use thread-level and other useful information known to capture relatedness and appropriateness. S AME AUTHOR We have a thread-level metafeature, which we apply to the pairs (q10 , c1 ), (q20 , c2 ). It checks whether"
D16-1165,J11-2003,0,0.0785057,"sifiers, and it applies SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise f"
D16-1165,S15-2038,0,0.0219871,"SVMs to train a Good vs. Bad classifier (Filice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural"
D16-1165,P15-2116,0,0.0859259,"Missing"
D16-1165,S16-1132,0,0.025907,"ilice et al., 2016). In contrast, our approach here proceeds in a pairwise setting, it is lighter in terms of features engineering, and presents a direct way to combine the relations between the different subtasks in an integrated neural network model. Finally, our model uses lexical features derived from machine translation evaluation. Some previous work also used MT model(s) as a feature(s) (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. 1594 7 Conclusion We presented a neural-based approach to a novel problem in cQA, where given a new question, the task is to rank comments from related questionthreads according to their relevance as answers to the original question. We explored the utility of three types of similarities between the original question, the related question, and the related comment. We adopted a pairwise feed-forward neural network architecture, which takes as input th"
D16-1165,P15-1025,0,0.0858496,"s a good answer for both q 0 and q.1 In the past, the approaches to cQA were focused on using information from the new question q, an existing related question q 0 , and a comment c within the thread of q 0 , to solve different cQA sub-tasks. For example, answer selection, which selects the most appropriate comment c within the thread q 0 , was addressed in SemEval-2015 Task 3 (Nakov et al., 2015). Similarly, question–question similarity, which looks for the most related questions to a given question, was addressed by many authors (Jeon et al., 2005; Duan et al., 2008; Li and Manandhar, 2011; Zhou et al., 2015; dos Santos et al., 2015). In this paper, we solve the cQA task problem2 in a novel way by using the three types of similarities jointly. Our main hypothesis is that relevance, appropriateness, and relatedness are essential to finding the best answer in a community Question Answering setting. Below we present experimental results that support this hypothesis. 1 The essence of this triangle is also described in SemEval 2016 Task 3 to motivate a three-subtask setting for cQA (Nakov et al., 2016). In that evaluation exercise, q 0 c and qq 0 are presented as subtask A and subtask B, respectively."
D18-1131,S16-1081,0,0.0416144,"Missing"
D18-1131,S16-1138,1,0.88442,"Missing"
D18-1131,E17-2115,1,0.903967,"Missing"
D18-1131,S16-1172,1,0.90167,"Missing"
D18-1131,S17-2053,1,0.887,"Missing"
D18-1131,P16-2075,1,0.90714,"Missing"
D18-1131,S16-1137,1,0.908531,"Missing"
D18-1131,D18-1452,1,0.847801,"Missing"
D18-1131,D16-1165,1,0.920046,"Missing"
D18-1131,K17-1024,1,0.866923,"and what properties of the domains are important in this regard; and • we show that adversarial domain adaptation can be efficient even for unseen target domains, given some similarity of the target domain with the source one and with the regularizing adversarial domain. Adversarial domain adaptation (ADA) was proposed by Ganin and Lempitsky (2015), and was then used for NLP tasks such as sentiment analysis and retrieval-based question answering (Chen et al., 2016; Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Yu et al., 2018; Zhang et al., 2017), including cross-language adaptation (Joty et al., 2017) for question-question similarity.2 The rest of this paper is organized as follows: Section 2 presents our model, its components, and the training procedure. Section 3 describes the datasets we used for our experiments, stressing upon their nature and diversity. Section 4 describes our adaptation experiments and discusses the results. Finally, Section 5 concludes with possible directions for future work. 2 Method Our ADA model has three components: (i) question encoder, (ii) similarity function, and (iii) domain adaptation component, as shown in Figure 1. The encoder E maps a sequence of word"
D18-1131,S16-1083,1,0.91625,"Missing"
D18-1131,C18-1181,0,0.0587596,"Missing"
D18-1131,N16-1153,1,0.91865,"Missing"
D18-1131,D14-1162,0,0.0812127,"0 Adaptation — — Classification Wasserstein AskUbuntu source and the target domains. SuperUser Experiments and Evaluation Experimental Setup Baselines We compare our ADA model to the following baselines: (a) direct transfer, which directly applies models learned from the source to the target domain without any adaptation; and (b) the standard unsupervised BM25 (Robertson and Zaragoza, 2009) scoring provided in search engines such as Apache Lucene (McCandless et al., 2010). Models We use a bi-LSTM (Hochreiter and Schmidhuber, 1997) encoder that operates on 300dimensional GloVe word embeddings (Pennington et al., 2014), which we train on the combined data from all domains. We keep word embeddings fixed in our experiments. For the adversarial component, we use a multi-layer perceptron. Evaluation Metrics As our datasets may contain some duplicate question pairs, which were not discovered and thus not annotated, we end up having false negatives. Metrics such as MAP and MRR are not suitable in this situation. Instead, we use AUC (area under the curve) to evaluate how well the model ranks positive pairs vs. negative ones. AUC quantifies how well the true positive rate (tpr) grows at various false positive rates"
D18-1131,P17-1001,0,0.0385569,"ss-entropy loss:   sigmoid W> (v1 v2 ) + b • we study when transfer learning performs well and what properties of the domains are important in this regard; and • we show that adversarial domain adaptation can be efficient even for unseen target domains, given some similarity of the target domain with the source one and with the regularizing adversarial domain. Adversarial domain adaptation (ADA) was proposed by Ganin and Lempitsky (2015), and was then used for NLP tasks such as sentiment analysis and retrieval-based question answering (Chen et al., 2016; Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Yu et al., 2018; Zhang et al., 2017), including cross-language adaptation (Joty et al., 2017) for question-question similarity.2 The rest of this paper is organized as follows: Section 2 presents our model, its components, and the training procedure. Section 3 describes the datasets we used for our experiments, stressing upon their nature and diversity. Section 4 describes our adaptation experiments and discusses the results. Finally, Section 5 concludes with possible directions for future work. 2 Method Our ADA model has three components: (i) question encoder, (ii) similarity function, and"
D18-1131,D15-1166,0,0.0536517,"Missing"
D18-1131,P15-2114,0,0.187747,"Missing"
D18-1131,P18-2046,1,0.894892,"Missing"
D18-1131,P07-1108,0,0.0320584,"Missing"
D18-1131,Q17-1036,0,0.0265715,"v2 ) + b • we study when transfer learning performs well and what properties of the domains are important in this regard; and • we show that adversarial domain adaptation can be efficient even for unseen target domains, given some similarity of the target domain with the source one and with the regularizing adversarial domain. Adversarial domain adaptation (ADA) was proposed by Ganin and Lempitsky (2015), and was then used for NLP tasks such as sentiment analysis and retrieval-based question answering (Chen et al., 2016; Ganin et al., 2016; Li et al., 2017; Liu et al., 2017; Yu et al., 2018; Zhang et al., 2017), including cross-language adaptation (Joty et al., 2017) for question-question similarity.2 The rest of this paper is organized as follows: Section 2 presents our model, its components, and the training procedure. Section 3 describes the datasets we used for our experiments, stressing upon their nature and diversity. Section 4 describes our adaptation experiments and discusses the results. Finally, Section 5 concludes with possible directions for future work. 2 Method Our ADA model has three components: (i) question encoder, (ii) similarity function, and (iii) domain adaptation component, as"
D18-1131,P11-1066,0,0.122868,"Missing"
D18-1389,N18-2004,1,0.801772,"16). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2"
D18-1389,C18-1285,0,0.10102,"Missing"
D18-1389,S17-2006,0,0.117489,"Missing"
D18-1389,C18-1284,0,0.0612131,"increasingly important. For example, the ACM Transactions on Information Systems journal dedicated, in 2016, a special issue on Trust and Veracity of Information in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been"
D18-1389,C18-1158,0,0.0461899,"8; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018; Zubiaga et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018). 2.3 Source Reliability Estimation Unlike stance detection, the problem of source reliability remains largely under-explored. In the case of social media, it concerns modeling the user2 who posted a particular message/tweet, while in the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is our focus in this paper. In previous work, the source"
D18-1389,karadzhov-etal-2017-built,1,0.905758,"ormation in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017;"
D18-1389,karadzhov-etal-2017-fully,1,0.861555,"ormation in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017;"
D18-1389,C18-1288,0,0.0222688,"nt. For example, the ACM Transactions on Information Systems journal dedicated, in 2016, a special issue on Trust and Veracity of Information in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in i"
D18-1389,P17-1066,0,0.0303452,"ing are becoming increasingly important. For example, the ACM Transactions on Information Systems journal dedicated, in 2016, a special issue on Trust and Veracity of Information in Social Media (Papadopoulos et al., 2016). 1 The data and the code are at http://github.mit. edu/CSAIL-SLS/News-Media-Reliability/ Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2015, 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance"
D18-1389,K15-1032,1,0.839773,"ing manual gold annotations specific for the task. Note that estimating the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three small datasets ranging from a couple of hundred to a few thousand articles from a couple of dozen sources, comparing (i) real news vs. (ii) “fake news” vs. (iii) sat"
D18-1389,R15-1058,1,0.866485,"ing manual gold annotations specific for the task. Note that estimating the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three small datasets ranging from a couple of hundred to a few thousand articles from a couple of dozen sources, comparing (i) real news vs. (ii) “fake news” vs. (iii) sat"
D18-1389,P16-2065,1,0.838705,"ting the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three small datasets ranging from a couple of hundred to a few thousand articles from a couple of dozen sources, comparing (i) real news vs. (ii) “fake news” vs. (iii) satire, and found that the latter two have a lot in common across a numbe"
D18-1389,N13-1090,0,0.0157775,"as of a target medium. For example, the absence of a Wikipedia page may indicate that a website is not credible. Also, the content of the page might explicitly mention that a certain website is satirical, left-wing, or has some property related to our task. 3531 • Counts: Statistics about the number of friends, statuses, and favorites. Established media might have higher values for these. Accordingly, we extract the following features: • Has Page: indicates whether the target medium has a Wikipedia page; • Description: A vector representation generated by averaging the Google News embeddings (Mikolov et al., 2013) of all words of the profile description paragraph. These short descriptions might contain an open declaration of partisanship, i.e., left or right political ideology (bias). This could also help predict factuality as extreme partisanship often implies low factuality. In contrast, “fake news” media might just leave this description empty, while high-quality media would want to give some information about who they are. • Vector representation for each of the following segments of the Wikipedia page, whenever applicable: Content, Infobox, Summary, Categories, and Table of Contents. We generate t"
D18-1389,N18-1070,1,0.825569,"b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018; Zubiaga et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018). 2.3 Source Reliability Estimation Unlike stance detection, the problem of source reliability remains largely under-explored. In the case of social media, it concerns modeling the user2 who posted a particular message/tweet, while in the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is our focus in this paper. I"
D18-1389,C18-1287,0,0.234004,"orthy source is one that contains very few false facts. In this paper, we follow a different approach by studying the source reliability as a task in its own right, using manual gold annotations specific for the task. Note that estimating the reliability of a source is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but it also gives an important prior when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017a; De Sarkar et al., 2018; Pan et al., 2018; Pérez-Rosas et al., 2018). 2 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). “Fake News” Detection Most work on “fake news” detection has relied on medium-level labels, which were then assumed to hold for all articles from that source. Horne and Adali (2017) analyzed three sm"
D18-1389,P13-1162,0,0.639986,"given target medium should be critical for assessing the factuality of its reporting, as well as of its potential bias. Towards this goal, we borrow a set of 141 features that were previously proposed for detecting “fake news” articles (Horne et al., 2018b), as we have described above. These features are used to analyze the following article characteristics: • Structure: POS tags, linguistic features based on the use of specific words (function words, pronouns, etc.), and features for clickbait title classification from (Chakraborty et al., 2016); • Sentiment: sentiment scores using lexicons (Recasens et al., 2013; Mitchell et al., 2013) and full systems (Hutto and Gilbert, 2014); Media Bias Detection As we mentioned above, bias was used as a feature for “fake news” detection (Horne et al., 2018b). It has also been the target of classification, e.g., Horne et al. (2018a) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Finally, Rashkin et al. (2017) studied propaganda, which can be seen as extreme bias. See also a recent positi"
D18-1389,S17-2088,1,0.866619,"Missing"
D18-1389,W17-4214,0,0.0904481,"aradzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. 3529 2.2 Stance Detection 2.4 Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018; Zubiaga et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018). 2.3 Source Reliability Estimation Unlike stance detection, the problem of source reliability remains largely under-explored. In the case of social media, it concerns modeling the user2 who posted a particular message/tweet, while in the case of the Web, it is about the trustworthiness of the source (the URL domain, the medium). The latter is ou"
D18-1389,C18-1283,0,0.0694362,"dual (groups of) features. There have also been some related shared tasks such as the SemEval-2017 task 8 on Rumor Detection (Derczynski et al., 2017), the CLEF-2018 lab on Automatic Identification and Verification of Claims in Political Debates (Atanasova et al., 2018; Barrón-Cedeño et al., 2018; Nakov et al., 2018), and the FEVER-2018 task on Fact Extraction and VERification (Thorne et al., 2018). The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey was run by Thorne and Vlachos (2018), which took a fact-checking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), covering truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the process of proliferation of true and false news online. In particular, they analyzed 126K stories tweeted by 3M people more than 4.5M times, and confirmed that “fake news” spread much wider than true news. Veracity of information has been st"
D18-1389,N18-1074,0,0.0869766,"Missing"
D18-1389,P18-1022,0,0.205979,"structural, sentiment, engagement, topicdependent, complexity, bias, and morality. We use this set of features when analyzing news articles. In yet another follow-up work, Horne et al. (2018a) trained a classifier to predict whether a given news article is coming from a reliable or from an unreliable (“fake news” or conspiracy)3 source. Note that they assumed that all news from a given website would share the same reliability class. Such an assumption is fine for training (distant supervision), but we find it problematic for testing, where we believe manual documents-level labels are needed. Potthast et al. (2018) used 1,627 articles from nine sources, whose factuality has been manually verified by professional journalists from BuzzFeed. They applied stylometric analysis, which was originally designed for authorship verification, to predict factuality (fake vs. real). Rashkin et al. (2017) focused on the language used by “fake news” and compared the prevalence of several features in articles coming from trusted sources vs. hoaxes vs. satire vs. propaganda. However, their linguistic analysis and their automatic classification were at the article level and they only covered eight news media sources. 3 We"
D18-1389,D17-1317,0,0.236805,"r from an unreliable (“fake news” or conspiracy)3 source. Note that they assumed that all news from a given website would share the same reliability class. Such an assumption is fine for training (distant supervision), but we find it problematic for testing, where we believe manual documents-level labels are needed. Potthast et al. (2018) used 1,627 articles from nine sources, whose factuality has been manually verified by professional journalists from BuzzFeed. They applied stylometric analysis, which was originally designed for authorship verification, to predict factuality (fake vs. real). Rashkin et al. (2017) focused on the language used by “fake news” and compared the prevalence of several features in articles coming from trusted sources vs. hoaxes vs. satire vs. propaganda. However, their linguistic analysis and their automatic classification were at the article level and they only covered eight news media sources. 3 We show in parentheses, the labels from opensources.co that are used to define a category. 3530 Unlike the above work, (i) we perform classification at the news medium level rather than focusing on an individual article. Thus, (ii) we use reliable manually-annotated labels as oppose"
D18-1452,S16-1130,1,0.8612,"Missing"
D18-1452,P15-2113,1,0.615975,"al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate taskspecific embeddings, and we defer the joint learning with global inference to the structured model. From the perspective of modeling cQA subtasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question–comment thread. This has been done from a feature engineering perspective, by modeling a comment in the context of the entire thread (Barrón-Cedeño et al., 2015), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers collectively. For example, Zhou et al. (2015) treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs. Joty et al. (2015) modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In a follow up work, Joty et al. (2016) also modeled the relations between all pairs of comments in a"
D18-1452,E17-2115,0,0.0635121,"in answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features. In work following the competition, Nakov et al. (2016a) used a triangulation approach to answer ranking in cQA, modeling the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model. Moreover, they focus on one task only, while we use multitask learning. Bonadiman et al. (2017) proposed a multitask neural architecture where the three tasks are trained together with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train taskspecific embeddings, as we do. The general idea of combining DNNs and structured models has been explored recently for other NLP tasks. Collobert et al. (2011) used Viterbi inference to train their DNN models to capture dependencies between word-level tags for a number of sequence labeling tasks: part-of-speech tagging, chunking, named entity recognition, and semantic"
D18-1452,I17-2075,0,0.03107,"een subtasks, providing sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate ta"
D18-1452,S16-1172,0,0.0498933,"erent cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a multitask learning framework. We do this using the 4197 datasets from the SemEval-2016 Task 3 on Community Question Answering (Nakov et al., 2016b), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SU PER TEAM (Mihaylova et al., 2016) and K ELP (Filice et al., 2016), stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features. In work following the competition, Nakov et al. (2016a) used a triangulation approach to answer ranking in cQA, modeling the three types of similarities occurring in the triangle formed by the original question, the related question, and an answer to the related comment. However, theirs is a pairwise ranking model, while we have a joint model."
D18-1452,P15-1078,1,0.900381,"Missing"
D18-1452,P16-2075,1,0.897508,"Missing"
D18-1452,S16-1137,1,0.897878,"Missing"
D18-1452,D15-1068,1,0.814627,"Missing"
D18-1452,P16-1165,1,0.636808,"al inference over arbitrary graph structures accounting for the dependencies between subtasks to provide globally good solutions. The experimental results have proven the suitability of combining the two approaches. The DNNs alone already yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results. In future work, we plan to model text complexity (Mihaylova et al., 2016), veracity (Mihaylova et al., 2018), speech act (Joty and Hoque, 2016), user profile (Mihaylov et al., 2015), trollness (Mihaylov et al., 2018), and goodness polarity (Balchev et al., 2016; Mihaylov et al., 2017). From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain (Shah et al., 2018) or a cross-language setting (Da San Martino et al., 2017; Joty et al., 2017). Trying an ensemble of neural networks with different initial seeds is another possible research direction. Acknowledgments The first aut"
D18-1452,N16-1084,1,0.905394,"Missing"
D18-1452,K17-1024,1,0.89338,"Missing"
D18-1452,C18-1181,0,0.0548546,"tion and Motivation Question answering web forums such as StackOverflow, Quora, and Yahoo! Answers usually organize their content in topically-defined forums containing multiple question–comment threads, where a question posed by a user is often followed by a possibly very long list of comments by other users, supposedly intended to answer the question. Many forums are not moderated, which often results in noisy and redundant content. Within community Question Answering (cQA) forums, two subtasks are of special relevance when a user poses a new question to the website (Hoogeveen et al., 2018; Lai et al., 2018): (i) finding similar questions (question-question relatedness), and (ii) finding relevant answers to the new question, if they already exist (answer selection). ∗ Work conducted while this author was at QCRI, HBKU. Both subtasks have been the focus of recent research as they result in end-user applications. The former is interesting for a user who wants to explore the space of similar questions in the forum and to decide whether to post a new question. It can also be relevant for the forum owners as it can help detect redundancy, eliminate question duplicates, and improve the overall forum st"
D18-1452,P16-1101,0,0.0474628,"er with the same representation. However, they do not model comment-comment interactions in the same question-comment thread nor do they train taskspecific embeddings, as we do. The general idea of combining DNNs and structured models has been explored recently for other NLP tasks. Collobert et al. (2011) used Viterbi inference to train their DNN models to capture dependencies between word-level tags for a number of sequence labeling tasks: part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. Huang et al. (2015) proposed an LSTM-CRF framework for such tasks. Ma and Hovy (2016) included a CNN in the framework to compute word representations from character-level embeddings. While these studies consider tasks related to constituents in a sentence, e.g., words and phrases, we focus on methods to represent comments and to model dependencies between comment-level tags. We also experiment with arbitrary graph structures in our CRF model to model dependencies at different levels. 3 Learning Approach cim Let q be a newly-posed question, and denote the m-th comment (m ∈ {1, 2, . . . , M }) in the answer thread for the i-th potentially related question qi (i ∈ {1, 2, . . . ,"
D18-1452,K15-1032,1,0.833942,"uctures accounting for the dependencies between subtasks to provide globally good solutions. The experimental results have proven the suitability of combining the two approaches. The DNNs alone already yielded competitive results, but the CRF was able to exploit the task-specific embeddings and the dependencies between subtasks to improve the results consistently across a variety of evaluation metrics, yielding state-of-the-art results. In future work, we plan to model text complexity (Mihaylova et al., 2016), veracity (Mihaylova et al., 2018), speech act (Joty and Hoque, 2016), user profile (Mihaylov et al., 2015), trollness (Mihaylov et al., 2018), and goodness polarity (Balchev et al., 2016; Mihaylov et al., 2017). From a modeling perspective, we want to strongly couple CRF and DNN, so that the global errors are backpropagated from the CRF down to the DNN layers. It would be also interesting to extend the framework to a cross-domain (Shah et al., 2018) or a cross-language setting (Da San Martino et al., 2017; Joty et al., 2017). Trying an ensemble of neural networks with different initial seeds is another possible research direction. Acknowledgments The first author would like to thank the funding su"
D18-1452,S16-1136,1,0.878923,"Missing"
D18-1452,N13-1090,0,0.119305,"Missing"
D18-1452,D16-1165,1,0.935991,"he relations between all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Unlike these models, we use DNNs to induce taskspecific embeddings, and, more importantly, we perform multitask learning of three different cQA subtasks, thus enriching the relational structure of the graphical model. We solve the three cQA subtasks jointly, in a multitask learning framework. We do this using the 4197 datasets from the SemEval-2016 Task 3 on Community Question Answering (Nakov et al., 2016b), which are annotated for the three subtasks, and we compare against the systems that participated in that competition. In fact, most of these systems did not try to exploit the interaction between the subtasks or did so only as a pipeline. For example, the top two systems, SU PER TEAM (Mihaylova et al., 2016) and K ELP (Filice et al., 2016), stacked the predicted labels from two subtasks in order to solve the main answer selection subtask using SVMs. In contrast, our approach is neural, it is based on joint learning and task-specific embeddings, and it is also lighter in terms of features."
D18-1452,S16-1083,1,0.914124,"Missing"
D18-1452,P02-1040,0,0.100744,"Missing"
D18-1452,P15-2114,0,0.0698502,"cially answer-goodness and question-question-relatedness influence answerselection significantly; (iii) the CRFs exploit the dependencies between subtasks, providing sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN model"
D18-1452,D18-1131,1,0.84905,"Missing"
D18-1452,2006.amta-papers.25,0,0.0322229,"Missing"
D18-1452,P18-1162,0,0.0171294,"sizeably better results that are on par or above the state of the art. In summary, we demonstrate the effectiveness of this marriage of DNNs and structured conditional models for cQA subtasks, where a feed-forward DNN is first used to build vectors for each individual subtask, which are then “reconciled” in a multitask CRF. Related Work Various neural models have been applied to cQA tasks such as question-question similarity (dos Santos et al., 2015; Lei et al., 2016; Wang et al., 2018) and answer selection (Wang and Nyberg, 2015; Qiu and Huang, 2015; Tan et al., 2015; Chen and Bunescu, 2017; Wu et al., 2018). Most of this work used advanced neural network architectures based on convolutional neural networks (CNN), long short-term memory (LSTM) units, attention mechanism, etc. For instance, dos Santos et al. (2015) combined CNN and bag of words for comparing questions. Tan et al. (2015) adopted an attention mechanism over bidirectional LSTMs to generate better answer representations, and Lei et al. (2016) combined recurrent and CNN models for question representation. In contrast, here we use a simple DNN model, i.e., a feed-forward neural network, which we only use to generate taskspecific embeddi"
D18-1452,P15-2117,0,0.0300461,"taskspecific embeddings, and we defer the joint learning with global inference to the structured model. From the perspective of modeling cQA subtasks as structured learning problems, there is a lot of research trying to exploit the correlations between the comments in a question–comment thread. This has been done from a feature engineering perspective, by modeling a comment in the context of the entire thread (Barrón-Cedeño et al., 2015), but more interestingly by considering a thread as a structured object, where comments are to be classified as good or bad answers collectively. For example, Zhou et al. (2015) treated the answer selection task as a sequence labeling problem and used recurrent convolutional neural networks and LSTMs. Joty et al. (2015) modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In a follow up work, Joty et al. (2016) also modeled the relations between all pairs of comments in a thread, but using a fully-connected pairwise CRF model, which is a joint model that integrates inference within the learning process using global normalization. Unlike these mo"
D18-1452,P13-1045,0,0.0436668,"Missing"
D18-1452,P15-2116,0,0.0365402,"Missing"
D19-1216,N19-1216,1,0.871939,"Missing"
D19-1216,N18-2004,1,0.858007,"Missing"
D19-1216,D18-2029,0,0.0339408,"Missing"
D19-1216,C18-1284,0,0.073254,"in isolation and in various combinations, and further describes some unsuccessful attempts at extracting better features. Finally, Section 7 presents our conclusions and some ideas for future work. 2 2.1 Related Work Fact-checking Claims There has been a lot of research in the last few years in automatic fact-checking of claims and rumors, which can be classified into two general categories. The first approach focuses on the social aspects of the claim and how users in social media react to it (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018). This is reflected by user comments, likes/dislikes, views and other types of reactions, which are collected and used as features. 2100 Other methods use the Web and try to find information that proves or disproves the claim (Mukherjee and Weikum, 2015; Popat et al., 2017; Karadzhov et al., 2017; Mihaylova et al., 2018; Baly et al., 2018b). In either case, what is important is the stance (Riedel et al., 2017; Thorne et al., 2017; Hanselowski et al., 2018; Mohtarami et al., 2018, 2019): whether the opinion expressed in a tweet or in an article by a particular user/source agrees/disagrees with"
D19-1216,C18-1158,0,0.0503396,"im and how users in social media react to it (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018). This is reflected by user comments, likes/dislikes, views and other types of reactions, which are collected and used as features. 2100 Other methods use the Web and try to find information that proves or disproves the claim (Mukherjee and Weikum, 2015; Popat et al., 2017; Karadzhov et al., 2017; Mihaylova et al., 2018; Baly et al., 2018b). In either case, what is important is the stance (Riedel et al., 2017; Thorne et al., 2017; Hanselowski et al., 2018; Mohtarami et al., 2018, 2019): whether the opinion expressed in a tweet or in an article by a particular user/source agrees/disagrees with the claim, and the reliability of the source, i.e., can we trust this source (Baly et al., 2018a, 2019). We should note that all these approaches are limited to textual claims, while we are interested in claims about images. 2.2 Detecting Fake/Manipulated Images The task of detecting fabricated images falls under the area of image forensics. Such tasks are usually solved using traditional statistical methods modeling color, shape, and texture features (Ba"
D19-1216,karadzhov-etal-2017-fully,1,0.833872,"Missing"
D19-1216,P17-1066,0,0.0219245,"l features, both in isolation and in various combinations, and further describes some unsuccessful attempts at extracting better features. Finally, Section 7 presents our conclusions and some ideas for future work. 2 2.1 Related Work Fact-checking Claims There has been a lot of research in the last few years in automatic fact-checking of claims and rumors, which can be classified into two general categories. The first approach focuses on the social aspects of the claim and how users in social media react to it (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018). This is reflected by user comments, likes/dislikes, views and other types of reactions, which are collected and used as features. 2100 Other methods use the Web and try to find information that proves or disproves the claim (Mukherjee and Weikum, 2015; Popat et al., 2017; Karadzhov et al., 2017; Mihaylova et al., 2018; Baly et al., 2018b). In either case, what is important is the stance (Riedel et al., 2017; Thorne et al., 2017; Hanselowski et al., 2018; Mohtarami et al., 2018, 2019): whether the opinion expressed in a tweet or in an article by a particular user/source a"
D19-1216,N18-1070,1,0.904779,"Missing"
D19-1216,D19-1452,1,0.863867,"Missing"
D19-1216,W17-4214,0,0.126821,"al aspects of the claim and how users in social media react to it (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016; Zubiaga et al., 2016; Ma et al., 2017; Dungs et al., 2018). This is reflected by user comments, likes/dislikes, views and other types of reactions, which are collected and used as features. 2100 Other methods use the Web and try to find information that proves or disproves the claim (Mukherjee and Weikum, 2015; Popat et al., 2017; Karadzhov et al., 2017; Mihaylova et al., 2018; Baly et al., 2018b). In either case, what is important is the stance (Riedel et al., 2017; Thorne et al., 2017; Hanselowski et al., 2018; Mohtarami et al., 2018, 2019): whether the opinion expressed in a tweet or in an article by a particular user/source agrees/disagrees with the claim, and the reliability of the source, i.e., can we trust this source (Baly et al., 2018a, 2019). We should note that all these approaches are limited to textual claims, while we are interested in claims about images. 2.2 Detecting Fake/Manipulated Images The task of detecting fabricated images falls under the area of image forensics. Such tasks are usually solved using traditional statistical methods modeling color, shape"
D19-1216,D18-1389,1,\N,Missing
D19-1294,W11-2103,0,0.0224755,"only two candidate translations of the same text as input for comparison: this could be a reference vs. a system translation, or a comparison between two candidate translations (see Section 5.5). 3 Dataset Generation We automatically generated our dataset, which we used to build a pronoun test suite and to train a pronoun evaluation model. In order to avoid generating synthetic data that may not necessarily represent a difficult context (for an MT system to correctly translate the pronouns), we used data from actual system outputs submitted for the WMT translation tasks in 2011–2015 and 2017 (Callison-Burch et al., 2011, 2012; Bojar et al., 2013, 2014, 2015, 2017). Using such data means that what is essentially a conditional language model solution, such as the one used by Bawden et al. (2018), has already failed on these examples. Original French input: Il e´ tait cr´eatif, g´en´ereux, drˆole, affectueux et talentueux, et il va beaucoup me manquer. Reference translation: He was creative, generous, funny, loving and talented, and I will miss him dearly. MT system translation: It was creative, generous, funny, affectionate and talented, and we will greatly miss. Generated noisy example 1: It was creative, gen"
D19-1294,N18-1118,0,0.432704,"sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address inter-sentential phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 201"
D19-1294,D07-1007,0,0.046499,"ntiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et a"
D19-1294,P07-1005,0,0.044586,"onoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is a"
D19-1294,W13-2201,0,0.0312423,"the same text as input for comparison: this could be a reference vs. a system translation, or a comparison between two candidate translations (see Section 5.5). 3 Dataset Generation We automatically generated our dataset, which we used to build a pronoun test suite and to train a pronoun evaluation model. In order to avoid generating synthetic data that may not necessarily represent a difficult context (for an MT system to correctly translate the pronouns), we used data from actual system outputs submitted for the WMT translation tasks in 2011–2015 and 2017 (Callison-Burch et al., 2011, 2012; Bojar et al., 2013, 2014, 2015, 2017). Using such data means that what is essentially a conditional language model solution, such as the one used by Bawden et al. (2018), has already failed on these examples. Original French input: Il e´ tait cr´eatif, g´en´ereux, drˆole, affectueux et talentueux, et il va beaucoup me manquer. Reference translation: He was creative, generous, funny, loving and talented, and I will miss him dearly. MT system translation: It was creative, generous, funny, affectionate and talented, and we will greatly miss. Generated noisy example 1: It was creative, generous, funny, loving and t"
D19-1294,W15-3001,0,0.0618627,"Missing"
D19-1294,J93-2003,0,0.109724,"sive, targeted dataset that can be used as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model m"
D19-1294,W12-3102,0,0.106102,"Missing"
D19-1294,P05-1033,0,0.127023,"ges and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researc"
D19-1294,N19-1423,0,0.0276988,"Missing"
D19-1294,W08-0331,0,0.0463595,"ge Processing, pages 2964–2975, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Here we propose a targeted dataset for machine translation evaluation with a focus on anaphora. We further present a specialized evaluation measure trained on this dataset. The measure performs pairwise evaluations: it learns to distinguish good vs. bad translations of pronouns, without being given specific signals of the errors. It has been argued that pairwise evaluation is useful and sufficient for machine translation evaluation (Guzm´an et al., 2015, 2017). In particular, Duh (2008) has shown that ranking-based evaluation measures can achieve higher correlations with human judgments, as ranking judgments are easier to obtain from human judges and are also easy to use in training, while also directly achieving the purpose of comparing two systems. Note that while it may be possible to rank translations using strong pre-trained conditional language models such as GPT (Radford et al., 2018), all kinds of errors would influence the score, and it would not be targeted towards a specific source of error, such as anaphora here. Our model provides a way to do this, and we demons"
D19-1294,N13-1073,0,0.0119131,"ny, loving and talented, and I will miss him dearly. MT system translation: It was creative, generous, funny, affectionate and talented, and we will greatly miss. Generated noisy example 1: It was creative, generous, funny, loving and talented, and I will miss him dearly. Generated noisy example 2: He was creative, generous, funny, loving and talented, and we will miss him dearly. Figure 1: Noisy examples generated by substituting an MT-generated pronoun in the reference translation. In particular, we aligned the system outputs with the reference translation using an automatic alignment tool (Dyer et al., 2013), and we found examples in which the pronouns did not match the reference translation. This process yielded potentially noisy data, as the alignments are automatic and thus not always perfect. 3.1 User Study In order to ensure that the mismatched pronouns are not equally good translations in the given context, we conducted a user study on a subset of the data. To focus the study on pronouns and to remove the influence of other MT errors, we generated a noisy candidate by replacing the correct pronoun in the reference with the aligned (potentially) incorrect pronoun from the system output. We d"
D19-1294,N04-1035,0,0.120142,"slation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sent"
D19-1294,L16-1100,0,0.0186828,"slation, i.e., this was not a realistic translation task. The 2015 edition of the task also featured a pronoun-focused translation task, which was like a normal MT task except that the evaluation focused on the pronouns only, and was performed manually. In contrast, we have a real MT evaluation setup, and we develop and use a fully automatic evaluation measure. More recently, there has been a move towards using specialized test suites specifically designed to assess system quality for some fine-grained problematic categories, including pronoun translation. For example, the PROTEST test suite (Guillou and Hardmeier, 2016) comprises 250 pronoun tokens, used in a semi-automatic evaluation: the pronouns in the MT output and in the reference are compared automatically, but in case of no matches, manual evaluation was required. Moreover, no final aggregate score over all pronouns was produced. In contrast, we have a much larger test suite with a fully automatic measure. Another semi-automatic system is described in (Guillou et al., 2018). It focused on just two pronouns, it and they, and was applied to a single language pair. In contrast, we have a fully automated evaluation measure, we handle many English pronouns"
D19-1294,D18-1513,0,0.0734255,"to tell apart a human translation from a machine output when going beyond the sentence level (L¨aubli et al., 2018). Overall, it is clear that there is a need for machine translation evaluation measures that look beyond the sentence level, and thus can better appreciate the improvements that a discourseaware MT system could potentially bring. Alternatively, one could use diagnostic test sets that are designed to evaluate how an MT system handles specific discourse phenomena (Bawden et al., 2018; Rios et al., 2018). There have also been proposals to use semi-automatic measures and test suites (Guillou and Hardmeier, 2018). 2964 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2964–2975, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics Here we propose a targeted dataset for machine translation evaluation with a focus on anaphora. We further present a specialized evaluation measure trained on this dataset. The measure performs pairwise evaluations: it learns to distinguish good vs. bad translations of pronouns, without being given specific signals of the err"
D19-1294,W18-6435,0,0.0395801,"evaluation: the pronouns in the MT output and in the reference are compared automatically, but in case of no matches, manual evaluation was required. Moreover, no final aggregate score over all pronouns was produced. In contrast, we have a much larger test suite with a fully automatic measure. Another semi-automatic system is described in (Guillou et al., 2018). It focused on just two pronouns, it and they, and was applied to a single language pair. In contrast, we have a fully automated evaluation measure, we handle many English pronouns, and we cover multiple source languages. Bawden et al. (2018) presented hand-crafted discourse test sets to test a model’s ability to exploit previous source and target sentences, based on 200 contrastive pairs, one with a correct and one with a wrong pronoun translation. This alleviates the need for an automatic evaluation measure as one can just count how many times the MT system has generated the correct pronoun. In contrast, we use texts from pre-existing MT evaluation datasets, we do not require them to be in contrastive pairs, and we have a fully automated evaluation measure; we also use larger datasets. 2965 M¨uller et al. (2018) also used contra"
D19-1294,P15-1078,1,0.896853,"Missing"
D19-1294,2010.iwslt-papers.10,0,0.458807,"Missing"
D19-1294,W15-2501,1,0.850389,"et and the model are based on actual system outputs. • Our evaluation measure achieves high agreement with human judgments. We make both the dataset and the evaluation measure publicly available at https://ntunlpsg.github.io/project/discomt/evalanaphora/. 2 Related Work Previous work on discourse-aware machine translation and MT evaluation has targeted a number of phenomena such as anaphora, gender agreement, lexical consistency, and coherence. In this work, we focus on pronoun translation. Pronoun translation has been the target of a shared task at the DiscoMT and WMT workshops in 2015-2017 (Hardmeier et al., 2015; Guillou et al., 2016; Lo´aiciga et al., 2017). However, the focus was on cross-lingual pronoun prediction, which required choosing the correct pronouns in the context of an existing translation, i.e., this was not a realistic translation task. The 2015 edition of the task also featured a pronoun-focused translation task, which was like a normal MT task except that the evaluation focused on the pronouns only, and was performed manually. In contrast, we have a real MT evaluation setup, and we develop and use a fully automatic evaluation measure. More recently, there has been a move towards usi"
D19-1294,D12-1108,0,0.0206439,"y and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address int"
D19-1294,P82-1020,0,0.802189,"Missing"
D19-1294,N03-1017,0,0.0432782,"as a test suite for pronoun translation, covering multiple source languages and different pronoun errors drawn from real system translations, for English. We further propose an evaluation measure to differentiate good and bad pronoun translations. Finally, we conduct a user study and we report correlation with human judgments. 1 Introduction Traditionally, machine translation (MT) has been performed at the level of individual sentences, i.e., in isolation from the rest of the document. This was due to the nature of the underlying frameworks: word-based (Brown et al., 1993), then phrase-based (Koehn et al., 2003), syntactic (Galley et al., 2004), and hierarchical (Chiang, 2005). While there have been attempts to model the context beyond the sentence level, e.g., looking at neighboring sentences (Carpuat and Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially tra"
D19-1294,D18-1512,0,0.0951009,"Missing"
D19-1294,W17-4801,1,0.88967,"Missing"
D19-1294,W17-4802,0,0.0953603,"existing MT evaluation datasets, we do not require them to be in contrastive pairs, and we have a fully automated evaluation measure; we also use larger datasets. 2965 M¨uller et al. (2018) also used contrastive translation pairs, mined from a parallel corpus using automatic coreference-based mining of context, thus minimizing the risk of producing wrong contrastive examples that are both valid translations. Yet, they did not propose an evaluation measure. Finally, there have been pronoun-focused automatic machine translation evaluation measures. Two important examples include APT (Miculicich Werlen and Popescu-Belis, 2017) and AutoPRF (Hardmeier and Federico, 2010). Both measures require alignments between the source, the reference and the system output texts for evaluating the pronoun translations. However, automatic alignments are noisy; Guillou and Hardmeier (2018) have shown that improvements using heuristics are not statistically significant. They also found low agreement between these measures and human judgments, primarily due to the possibility of many translation choices per pronoun. APT also uses a predetermined list of ‘equivalent pronouns’, obtained for specific pronouns based on a French grammar bo"
D19-1294,W18-6307,0,0.0656857,"Missing"
D19-1294,P02-1040,0,0.118067,"researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address inter-sentential phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 2014). The limitations of BLEU are well-known and have been discussed in detail in a recent study (Reiter, 2018). It has long been argued that as the quality of machine translation improves, there will be a singularity moment when existing evaluation measures would be unable to tell whether a given output was produced by a human or by a machine. Indeed, there have been recent claims that human parity has already been achieved (Hassan et al., 2018), but it has also been shown that it"
D19-1294,D14-1162,0,0.081598,"spect to the correct use of pronouns.3 In Section 3, we described how such datasets can be collected opportunistically without recourse to expensive manual annotation. Figure 4 shows our proposed framework to evaluate MT outputs with respect to pronouns. The inputs to the model are sentences (with or without context Cr and Cs ): R and S. Each input sentence is first mapped into a set of word embedding vectors of dimensionality d by performing a lookup in the shared embedding matrix E ∈ Rv×d with vocabulary size v. E can be initialized randomly or with any pre-trained embeddings such as GloVe (Pennington et al., 2014), or contextualized word vectors such as ELMo (Peters et al., 2018a). In case of initialization with GloVe vectors, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) layer to get a representation of the words that is encoded with contextual information. Let X = (x1 , x2 , . . . , xn ) denote an input sequence, where xt is the tth word embedding vector of the sequence. The LSTM recurrent layer computes a compositional representation kt at every time step t by performing nonlinear transformations of the current input xt and the output of the previous time step kt−1 . 2968 3 Here R (or S) can be"
D19-1294,N18-1202,0,0.488144,"uch datasets can be collected opportunistically without recourse to expensive manual annotation. Figure 4 shows our proposed framework to evaluate MT outputs with respect to pronouns. The inputs to the model are sentences (with or without context Cr and Cs ): R and S. Each input sentence is first mapped into a set of word embedding vectors of dimensionality d by performing a lookup in the shared embedding matrix E ∈ Rv×d with vocabulary size v. E can be initialized randomly or with any pre-trained embeddings such as GloVe (Pennington et al., 2014), or contextualized word vectors such as ELMo (Peters et al., 2018a). In case of initialization with GloVe vectors, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) layer to get a representation of the words that is encoded with contextual information. Let X = (x1 , x2 , . . . , xn ) denote an input sequence, where xt is the tth word embedding vector of the sequence. The LSTM recurrent layer computes a compositional representation kt at every time step t by performing nonlinear transformations of the current input xt and the output of the previous time step kt−1 . 2968 3 Here R (or S) can be a reference or a system translation. Figure 4: Our proposed frame"
D19-1294,D18-1179,0,0.201246,"uch datasets can be collected opportunistically without recourse to expensive manual annotation. Figure 4 shows our proposed framework to evaluate MT outputs with respect to pronouns. The inputs to the model are sentences (with or without context Cr and Cs ): R and S. Each input sentence is first mapped into a set of word embedding vectors of dimensionality d by performing a lookup in the shared embedding matrix E ∈ Rv×d with vocabulary size v. E can be initialized randomly or with any pre-trained embeddings such as GloVe (Pennington et al., 2014), or contextualized word vectors such as ELMo (Peters et al., 2018a). In case of initialization with GloVe vectors, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) layer to get a representation of the words that is encoded with contextual information. Let X = (x1 , x2 , . . . , xn ) denote an input sequence, where xt is the tth word embedding vector of the sequence. The LSTM recurrent layer computes a compositional representation kt at every time step t by performing nonlinear transformations of the current input xt and the output of the previous time step kt−1 . 2968 3 Here R (or S) can be a reference or a system translation. Figure 4: Our proposed frame"
D19-1294,J18-3002,0,0.0185676,"ntial phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 2014). The limitations of BLEU are well-known and have been discussed in detail in a recent study (Reiter, 2018). It has long been argued that as the quality of machine translation improves, there will be a singularity moment when existing evaluation measures would be unable to tell whether a given output was produced by a human or by a machine. Indeed, there have been recent claims that human parity has already been achieved (Hassan et al., 2018), but it has also been shown that it is easy to tell apart a human translation from a machine output when going beyond the sentence level (L¨aubli et al., 2018). Overall, it is clear that there is a need for machine translation evaluation measures that look bey"
D19-1294,W18-6437,0,0.051851,"Missing"
D19-1294,P18-1117,0,0.0597454,"d Wu, 2007; Chan et al., 2007) or even at the entire document (Hardmeier et al., 2012), these approaches were still limited by the underlying framework. Then, along came the neural revolution. Thanks to the attention mechanism, neural translation models such as sequence-to-sequence (Bahdanau et al., 2015) and the Transformer (Vaswani et al., 2017) could model much broader context. While initially translation was still done in a sentence-by-sentence fashion, researchers soon realized that going beyond that has become easier, and recent work has successfully exploited this (Bawden et al., 2018; Voita et al., 2018). This is an exciting research direction as it can help address inter-sentential phenomena such as anaphora, gender agreement, lexical consistency, and text coherence, to mention just a few. Unfortunately, going beyond the sentence level typically yields very few changes in the translation output, and even when these changes are seen as substantial by humans, they remain virtually unnoticed by typical MT evaluation measures such as BLEU (Papineni et al., 2002), which are known to be notoriously problematic for the evaluation of discourse-level aspects in MT (Hardmeier, 2014). The limitations o"
D19-1452,N18-2004,1,0.754681,"Missing"
D19-1452,E17-1024,0,0.0584382,"Missing"
D19-1452,N19-1423,0,0.0364481,"t language). In particular, we used the Fake News Challenge dataset (Hanselowski et al., 2018) as source data and an Arabic benchmark dataset (Baly et al., 2018) as target data. The evaluation results have shown 2.7 and 4.0 absolute improvement in terms of macro-F1 and weighted accuracy for stance detection over the current state-of-the-art monolingual baseline, and 11.4, 14.9, 16.1, 12.9, and 13.1 points of absolute improvement in terms of precision at ranks 1–5 for extracting evidence snippets respectively. Furthermore, a key finding in our investigation is that, in contrast to other tasks (Devlin et al., 2019; Peters et al., 2018), pre-training with large amounts of source data is less effective for cross-lingual stance detection. We show that this is because pre-training can considerably bias the model toward the source language. 2 Method Assume that we are given a training dataset for a source language, Ds , which contains a set of  s s s  N triplets as follows: Ds = (ci , di ), yi i=1 , where N is the number of source samples, (csi , dsi ) is a pair of claim csi and document dsi , and yis ∈ Y , Y = {agree, disagree, discuss, unrelated}, is the corresponding label indicating the stance of the"
D19-1452,C18-1288,0,0.085602,"Missing"
D19-1452,C18-1284,0,0.164896,"Missing"
D19-1452,D19-6603,1,0.828223,"n. Stance Detection. This is an important component for automatic fact-checking systems and veracity inference (Nadeem et al., 2019; Zhang et al., 2019; Atanasova et al., 2019). There have been some nuances in the way researchers have defined the stance detection task. Mohammad et al. (2016) and Zarrella and Marsh (2016) worked on stances regarding target propositions, e.g., entities, concepts or events, as in-favor, against, or neither. Most commonly, stance detection has been defined with respect to a claim as agree, disagree, discuss or unrelated (Hanselowski et al., 2018; Xu et al., 2018; Fang et al., 2019). Previous work mostly developed the models with rich hand-crafted features such as words, word embeddings, and sentiment lexicons (Riedel et al., 2017; Baird et al., 2017; Hanselowski et al., 2018). More recently, Mohtarami et al. (2018) presented a mono-lingual and feature-light memory network for stance detection. In this paper, we built on this work to extend previous efforts in stance detection to a cross-lingual setting, achieving the state-of-the-art result on the target language. 6 Conclusion and Future Work We proposed an effective language adaptation approach to align class labels in"
D19-1452,C18-1158,0,0.261469,"ods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 4442–4452, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics • Our model is able to extract accurate text snippets as evidence to explain its predictions in the target language (results are in Section 4.2). • To the best of our knowledge, this is the first work on cross-lingual stance detection. We conducted our experiments on English (as source language) and Arabic (as target language). In particular, we used the Fake News Challenge dataset (Hanselowski et al., 2018) as source data and an Arabic benchmark dataset (Baly et al., 2018) as target data. The evaluation results have shown 2.7 and 4.0 absolute improvement in terms of macro-F1 and weighted accuracy for stance detection over the current state-of-the-art monolingual baseline, and 11.4, 14.9, 16.1, 12.9, and 13.1 points of absolute improvement in terms of precision at ranks 1–5 for extracting evidence snippets respectively. Furthermore, a key finding in our investigation is that, in contrast to other tasks (Devlin et al., 2019; Peters et al., 2018), pre-training with large amounts of source data is l"
D19-1452,P17-1066,0,0.0478141,"results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance detection which aims to identify the relative perspective of a document with respect to a claim, typi"
D19-1452,S19-2149,1,0.874721,"Missing"
D19-1452,D14-1181,0,0.00235528,"ification tasks. We elaborate on these components below. Input representation component I: It encodes documents and claims into corresponding representations. Each document d is divided into a sequence of paragraphs X = (x1 , . . . , xl ), where each xj is encoded as mj using an LSTM network, and as nj using a CNN; these representations are stored in the memory component M . Note that while LSTMs are designed to capture and memorize their inputs (Tan et al., 2016), CNNs emphasize the local interaction between individual words in sequences, which is important for obtaining good representation (Kim, 2014). 4443 Figure 1: The architecture of our cross-lingual memory network for stance detection. Thus, our I component uses both LSTM and CNN representations. It also uses separate LSTM and CNN with their own parameters to represent each input claim c as clstm and ccnn , respectively. We consider each paragraph as a single piece of evidence because a paragraph usually represents a coherent argument, unified under one or more interrelated topics. We thus use the terms paragraph and evidence interchangeably. Inference component F : Our inference component computes LSTM- and CNN-based similarity betwe"
D19-1452,S16-1003,0,0.151768,"Missing"
D19-1452,N18-1070,1,0.811925,"Missing"
D19-1452,N19-4014,1,0.832314,"rage same-label examples from different domains to map nearby in the embedding space. While supervised approaches perform better than unsupervised ones, recent work (Motiian et al., 2017) has demonstrated superior performance by additionally encouraging class separation, meaning that examples from different domains and with different labels should be projected as far apart as possible in the embedding space. Here, we combined both types of alignments for cross-lingual stance detection. Stance Detection. This is an important component for automatic fact-checking systems and veracity inference (Nadeem et al., 2019; Zhang et al., 2019; Atanasova et al., 2019). There have been some nuances in the way researchers have defined the stance detection task. Mohammad et al. (2016) and Zarrella and Marsh (2016) worked on stances regarding target propositions, e.g., entities, concepts or events, as in-favor, against, or neither. Most commonly, stance detection has been defined with respect to a claim as agree, disagree, discuss or unrelated (Hanselowski et al., 2018; Xu et al., 2018; Fang et al., 2019). Previous work mostly developed the models with rich hand-crafted features such as words, word embeddings, and s"
D19-1452,N18-1202,0,0.0237588,"cular, we used the Fake News Challenge dataset (Hanselowski et al., 2018) as source data and an Arabic benchmark dataset (Baly et al., 2018) as target data. The evaluation results have shown 2.7 and 4.0 absolute improvement in terms of macro-F1 and weighted accuracy for stance detection over the current state-of-the-art monolingual baseline, and 11.4, 14.9, 16.1, 12.9, and 13.1 points of absolute improvement in terms of precision at ranks 1–5 for extracting evidence snippets respectively. Furthermore, a key finding in our investigation is that, in contrast to other tasks (Devlin et al., 2019; Peters et al., 2018), pre-training with large amounts of source data is less effective for cross-lingual stance detection. We show that this is because pre-training can considerably bias the model toward the source language. 2 Method Assume that we are given a training dataset for a source language, Ds , which contains a set of  s s s  N triplets as follows: Ds = (ci , di ), yi i=1 , where N is the number of source samples, (csi , dsi ) is a pair of claim csi and document dsi , and yis ∈ Y , Y = {agree, disagree, discuss, unrelated}, is the corresponding label indicating the stance of the document with respect"
D19-1452,E17-2088,0,0.0825794,"Missing"
D19-1452,P16-1044,0,0.0145071,"n output from the updated memory, and encodes it to a desired format in the response component R using a prediction function, e.g., softmax for classification tasks. We elaborate on these components below. Input representation component I: It encodes documents and claims into corresponding representations. Each document d is divided into a sequence of paragraphs X = (x1 , . . . , xl ), where each xj is encoded as mj using an LSTM network, and as nj using a CNN; these representations are stored in the memory component M . Note that while LSTMs are designed to capture and memorize their inputs (Tan et al., 2016), CNNs emphasize the local interaction between individual words in sequences, which is important for obtaining good representation (Kim, 2014). 4443 Figure 1: The architecture of our cross-lingual memory network for stance detection. Thus, our I component uses both LSTM and CNN representations. It also uses separate LSTM and CNN with their own parameters to represent each input claim c as clstm and ccnn , respectively. We consider each paragraph as a single piece of evidence because a paragraph usually represents a coherent argument, unified under one or more interrelated topics. We thus use t"
D19-1452,N18-1074,0,0.0360876,"atasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance detection which aims to identify the relative perspective of a document with respect to a claim, typically modeled using labels such a"
D19-1452,W14-2508,0,0.0675751,"oach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance detection which aims to identify the relative perspective of a document with respect to a claim, typically modeled using labels such as agree, disagree, discuss, and unrelated. We aim to bridge this gap by proposing a cross-lingual model for stanc"
D19-1452,P17-2067,0,0.0200162,"tively deal with the challenge of limited labeled data in the target language. The evaluation results on public benchmark datasets and comparison against current state-of-the-art approaches demonstrate the effectiveness of our approach. 1 Introduction The rise of social media has enabled the phenomenon of “fake news,” which could target specific individuals and can be used for deceptive purposes (Lazer et al., 2018; Vosoughi et al., 2018). As manual fact-checking is a time-consuming and tedious process, computational approaches have been proposed as a possible alternative (Popat et al., 2017; Wang, 2017; Mihaylova et al., 2018, 2019), based on information sources such as social media (Ma et al., 2017), Wikipedia (Thorne et al., 2018), and knowledge bases (Huynh and Papotti, 2018). Fact-checking is a multi-step process (Vlachos and Riedel, 2014): (i) checking the reliability of media sources, (ii) retrieving potentially relevant documents from reliable sources as evidence for each target claim, (iii) predicting the stance of each document with respect to the target claim, and finally (iv) making a decision based on the stances from (iii) for all documents from (ii). Here, we focus on stance d"
D19-1452,S16-1074,0,0.0484617,"7) has demonstrated superior performance by additionally encouraging class separation, meaning that examples from different domains and with different labels should be projected as far apart as possible in the embedding space. Here, we combined both types of alignments for cross-lingual stance detection. Stance Detection. This is an important component for automatic fact-checking systems and veracity inference (Nadeem et al., 2019; Zhang et al., 2019; Atanasova et al., 2019). There have been some nuances in the way researchers have defined the stance detection task. Mohammad et al. (2016) and Zarrella and Marsh (2016) worked on stances regarding target propositions, e.g., entities, concepts or events, as in-favor, against, or neither. Most commonly, stance detection has been defined with respect to a claim as agree, disagree, discuss or unrelated (Hanselowski et al., 2018; Xu et al., 2018; Fang et al., 2019). Previous work mostly developed the models with rich hand-crafted features such as words, word embeddings, and sentiment lexicons (Riedel et al., 2017; Baird et al., 2017; Hanselowski et al., 2018). More recently, Mohtarami et al. (2018) presented a mono-lingual and feature-light memory network for sta"
D19-1452,D19-3038,1,0.606293,"Missing"
D19-1565,N19-1423,0,0.340062,"T2 TN 1 We define two tasks based on the corpus described in Section 3: (i) SLC (Sentence-level Classification), which asks to predict whether a sentence contains at least one propaganda technique, and (ii) FLC (Fragment-level classification), which asks to identify both the spans and the type of propaganda technique. Note that these two tasks are of different granularities, g1 and g2 , i.e., tokens for FLC and sentences for SLC. We split the corpus into training, development and test, each containing 293, 57, 101 articles and 14,857, 2,108, 4,265 sentences. 5.1 Baselines We depart from BERT (Devlin et al., 2019), as it has achieved state-of-the-art performance on multiple NLP benchmarks, and we design three baselines based on it. BERT. We add a linear layer on top of BERT and we fine-tune it, as suggested in (Devlin et al., 2019). For the FLC task, we feed the final hidden representation for each token to a layer Lg2 that makes a 19-way classification: does this token belong to one of the eighteen propaganda techniques or to none of them (cf. Figure 3-a). For the SLC task, we feed the final hidden representation for the special [CLS] token, which BERT uses to represent the full sentence, to a two-dim"
D19-1565,D17-2002,0,0.111551,". Barr´on-Cede˜no et al. (2019) experimented with a binarized version of the corpus from (Rashkin et al., 2017): propaganda vs. the other three categories. The corpus labels were obtained with distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with cases of ad hominem fallacy identified. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies. A byproduct of Argotario is a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring and irrelevant authority, which directly relate to propaganda techniques (cf. Section 2). Differently from (Habernal et al., 2017, 2018a,b), our corpus has 18 techniques annotated on the same set of news articles. Moreover, our annotations aim at identifying the minimal fragments related to a technique instead of flagging entire arguments. Conclusion and Future Work We have argued for"
D19-1565,L18-1526,0,0.0643642,"propaganda, trusted, hoax, or satire. They included articles from eight sources, two of which are propagandistic. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of the corpus from (Rashkin et al., 2017): propaganda vs. the other three categories. The corpus labels were obtained with distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with cases of ad hominem fallacy identified. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies. A byproduct of Argotario is a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring and irrelevant authority, which directly relate to propaganda techniques (cf. Section 2). Differently from (Habernal et al., 2017, 2018a,b), our corpus has 18 techniques annotated on the same set of news articles. Moreover, our annotations aim at identifying the minimal fra"
D19-1565,N18-1036,0,0.0307582,"propaganda, trusted, hoax, or satire. They included articles from eight sources, two of which are propagandistic. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of the corpus from (Rashkin et al., 2017): propaganda vs. the other three categories. The corpus labels were obtained with distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with cases of ad hominem fallacy identified. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies. A byproduct of Argotario is a corpus with 1.3k arguments annotated with five fallacies, including ad hominem, red herring and irrelevant authority, which directly relate to propaganda techniques (cf. Section 2). Differently from (Habernal et al., 2017, 2018a,b), our corpus has 18 techniques annotated on the same set of news articles. Moreover, our annotations aim at identifying the minimal fra"
D19-1565,J15-3003,0,0.026974,"Missing"
D19-1565,C14-2023,0,0.0434456,"Missing"
D19-1565,C10-2115,1,0.852121,"Missing"
D19-1565,D17-1317,0,0.252149,". To overcome these limitations, we propose a novel task: performing fine-grained analysis of texts by detecting all fragments that contain propaganda techniques as well as their type. In particular, we create a corpus of news articles manually annotated at the fragment level with eighteen propaganda techniques and we propose a suitable evaluation measure. We further design a novel multi-granularity neural network, and we show that it outperforms several strong BERT-based baselines. 1 Introduction Research on detecting propaganda has focused primarily on articles (Barr´on-Cedeno et al., 2019; Rashkin et al., 2017). In many cases, there are no labeled data for individual articles, but there are such labels for entire news outlets. Thus, often all articles from the same news outlet get labeled the way that this outlet is labeled. Yet, it has been observed that propagandistic sources could post objective non-propagandistic articles periodically to increase their credibility (Horne et al., 2018). Similarly, media generally recognized as objective might occasionally post articles that promote a particular editorial agenda and are thus propagandistic. Thus, it is clear that transferring the label of the news"
D19-1565,W02-2024,0,0.298611,"Missing"
D19-1565,W03-0419,0,0.400775,"Missing"
D19-3038,P15-2072,0,0.0737003,"Missing"
D19-3038,D19-1565,1,0.834147,"Missing"
D19-3038,E17-3016,1,0.8158,"ehow (0.4 ≤ p &lt; 0.6), likely (0.6 ≤ p &lt; 0.8), and very likely (p ≥ 0.8). Crawlers and Translation Our crawlers collect articles from a growing list of sources10 , which currently includes 155 RSS feeds, 82 Twitter accounts and two websites. Once a link to an article has been obtained from any of these sources, we rely on the Newspaper3k Python library to extract its contents.11 After deduplication based on both URL and text content, our crawlers currently download 7k-10k articles per day. As of present, we have more than 700k articles stored in our database. We use QCRI’s Machine Translation (Dalvi et al., 2017) to translate English content into Arabic and vice versa. Since translation is performed offline, we select the most accurate system in Dalvi et al. (2017), i.e., the neural-based one. 8 http://kafka.apache.org http://kubernetes.io 10 http://www.tanbih.org/about 11 http://newspaper.readthedocs.io 9 12 http://github.com/several27/ FakeNewsCorpus 224 2.4 Framing Bias Detection We implemented our stance detection model as fine-tuning of BERT on the FNC-1 dataset from the Fake News Challenge13 . Our model outperformed the best submitted system (Hanselowski et al., 2018), obtaining an F1macro of 75"
D19-3038,C18-1158,0,0.133435,"Missing"
D19-3038,D18-1389,1,0.607642,"Missing"
D19-3038,N18-5006,1,0.812156,"Missing"
D19-3038,N19-1216,1,0.55511,"Missing"
D19-3038,P17-1092,0,0.0313814,"Missing"
D19-3038,D18-1483,0,0.0187346,"11): V (u) = tf (u,C0 ) total(C0 ) 2 tf (u,C ) tf (u,C1 ) 0 total(C0 ) + total(C1 ) −1 (1) Figure 2: The Tanbih main page. where tf (u, C0 ) is the number of times (term frequency) item u is cited by group C0 , and total(C0 ) is the sum of the term frequencies of all items cited by C0 . tf (u, C1 ) and total(C1 ) are defined in a similar fashion. We subdivided the range between -1 and 1 into 5 equal size ranges and we assigned the labels far-left, left, center, right, and far-right to those ranges. 2.9 The model achieved state-of-the-art performance on the testing partition of the corpus from Miranda et al. (2018): an F1 of 98.11 and an F1BCubed of 94.41.15 As a comparison, the best model described in (Miranda et al., 2018) achieved an F1 of 94.1. See Staykovski et al. (2019) for further details. Event Identification / Clustering 3 The clustering module aggregates news articles into stories. The pipeline is divided into two stages: (i) local topic identification and (ii) longterm topic matching for story generation. For step (i), we represent each article as a TF.IDF vector, built from the title and the body concatenated. The pre-processing consists of casefolding, lemmatization, punctuation and stopwo"
D19-5024,D19-5021,0,0.0556224,"Char. Emb.           Features Unsup. Tuning    Table 2: Overview of the approaches for the fragment-level classification task. Team NSIT CUNLP JUSTDeep Tha3aroon LIACC MIC-CIS CAUnLP YMJA jinfen ProperGander BERT LSTM      logreg USE CNN Embeddings Features          Context             Table 3: Overview of the approaches used for the sentence-level classification task. 9.2 Team Tha3aroon (Fadel and Al-Ayyoub, 2019) implemented an ensemble of three classifiers: two based on BERT and one based on a universal sentence encoder (Cer et al., 2018). Team NSIT (Aggarwal and Sadana, 2019) explored three of the most popular transfer learning models: various versions of ELMo, BERT, and RoBERTa (Liu et al., 2019). Team Mindcoders (Vlad et al., 2019) combined BERT, Bi-LSTM and Capsule networks (Sabour et al., 2017) into a single deep neural network and pre-trained the resulting network on corpora used for related tasks, e.g., emotion classification. Finally, team ltuorp (Mapes et al., 2019) used an attention transformer using BERT trained on Wikipedia and BookCorpus. Teams Participating in the Sentence-Level Classification Only Team CAUnLP (Hou and Chen, 2019) used two context-awa"
D19-5024,D19-5016,0,0.0895208,"evel Classification Only Team CAUnLP (Hou and Chen, 2019) used two context-aware representations based on BERT. In the first representation, the target sentence is followed by the title of the article. In the second representation, the previous sentence is also added. They performed subsampling in order to deal with class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They also observed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, a"
D19-5024,Q17-1010,0,0.00764997,"on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et al., 2019) participated in both tasks. For the sentence-level classification, they used a voting ensemble including logistic regression, convolutional neural networks, and BERT, in all cases using FastText embeddings (Bojanowski et al., 2017) and pre-trained BERT models. Beside these representations, multiple features of readability, sentiment and emotions were considered. For the fragment-level task, they used a multi-task neural sequence tagger, based on LSTM-CRF (Huang et al., 2015), in conjunction with linguistic features. Finally, they applied sentence- and fragment-level models jointly. 166 SLC Task: Test Set (Official Results) Rank Team F1 Precision Recall 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ltuorp ProperGander YMJA MIC-CIS CUNLP Tha3aroon JUSTDeep CAUnLP LIPN LIACC aschern MindCoders jinfen"
D19-5024,D19-5012,0,0.155995,"523 0.7212 0.6860 0.6197 0.7902 0.6684 0.6075 0.7280 0.6603 0.5155 0.4749 0.4695 0.4627 0.5074 0.5074 0.5074 0.3518 0.2286 Table 5: Results for the SLC task on the development set at the end of phase 1 (see Section 6). Team CUNLP (Alhindi et al., 2019) considered two approaches for the sentence-level task. The first approach was based on fine-tuning BERT. The second approach complemented the fine-tuned BERT approach by feeding its decision into a logistic regressor, together with features from the Linguistic Inquiry and Word Count (LIWC)2 lexicon and punctuation-derived features. Similarly to Gupta et al. (2019), for the fragment-level problem they used a Bi-LSTM-CRF architecture, combining both character- and word-level embeddings. Team ProperGander (Madabushi et al., 2019) also used BERT, but they paid special attention to the imbalance of the data, as well as to the differences between training and testing. They showed that augmenting the training data by oversampling yielded improvements when testing on data that is temporally far from the training (by increasing recall). In order to deal with the imbalance, they performed cost-sensitive classification, i.e., the errors on the smaller positive cl"
D19-5024,D17-2002,0,0.139704,") annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with instances of ad hominem fallacy. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies, a by-product of which is a corpus with 1.3k arguments annotated with five fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda. Introduction Propaganda aims at influencing people’s mindset with the purpose of advancing a specific agenda. In the Internet era, thanks to the mechanism of sharing in social networks, propaganda campaigns have the potential of reaching very large audiences (Glowacki et al., 2018; Muller, 2018; Tard´aguila et al., 2018). Propag"
D19-5024,D19-1565,1,0.818121,"Missing"
D19-5024,L18-1526,0,0.0144225,"a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with instances of ad hominem fallacy. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies, a by-product of which is a corpus with 1.3k arguments annotated with five fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda. Introduction Propaganda aims at influencing people’s mindset with the purpose of advancing a specific agenda. In the Internet era, thanks to the mechanism of sharing in social networks, propaganda campaigns have the potentia"
D19-5024,N18-1036,0,0.0158899,"a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies related to propaganda. Habernal et al. (2018b) presented a corpus of Web forum discussions with instances of ad hominem fallacy. Habernal et al. (2017, 2018a) introduced Argotario, a game to educate people to recognize and create fallacies, a by-product of which is a corpus with 1.3k arguments annotated with five fallacies such as ad hominem, red herring and irrelevant authority, which directly relate to propaganda. Introduction Propaganda aims at influencing people’s mindset with the purpose of advancing a specific agenda. In the Internet era, thanks to the mechanism of sharing in social networks, propaganda campaigns have the potentia"
D19-5024,N19-1423,0,0.0167189,"18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables 6 and 7. 5 (0.59) 10 (1.18) 9 (1.07) 101 (11.95) 26 (3.08) 2 (0.24) 10 (1.18) 10 (1.18) 9 Setup The shared task had two phases: In the development phase, the participants were provided labeled training and development datasets; in the testing phase, testing input was further provided. 9.1 Teams Participating in the Fragment-Level Classification Only Team newspeak (Yoosuf and Yang, 2019) achieved the best results on the test set for the FLC task using 20-way word-level classification based on BERT (Devlin et al., 2019): a word could belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class. The team fed one sentence at a time in order to reduce the workload. In addition to experimenting with an out-of-the-box BERT, they also tried unsupervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million parameters. Moreover, oversampling of the least represented classes proved to be crucial for the final performance. Finally, careful ana"
D19-5024,D19-5023,0,0.090948,"ary (token-derived) class. The team fed one sentence at a time in order to reduce the workload. In addition to experimenting with an out-of-the-box BERT, they also tried unsupervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million parameters. Moreover, oversampling of the least represented classes proved to be crucial for the final performance. Finally, careful analysis has shown that the model pays special attention to adjectives and adverbs. Team Stalin (Ek and Ghanimifard, 2019) focused on data augmentation to address the relatively small size of the data for fine-tuning contextual embedding representations based on ELMo (Peters et al., 2018), BERT, and Grover (Zellers et al., 2019). The balancing of the embedding space was carried out by means of synthetic minority class over-sampling. Then, the learned representations were fed into an LSTM. Phase 1. The participants tried to achieve the best performance on the development set. A live leaderboard kept track of the submissions. Phase 2. The test set was released and the participants had few days to make final predict"
D19-5024,D19-5020,0,0.105337,"Missing"
D19-5024,D19-5010,0,0.315396,"detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at http://propaganda.qcri. org/nlp4if-shared-task/. 1 2 Related Work Propaganda has been tackled mostly at the article level. Rashkin et al. (2017) created a corpus of news articles labelled as propaganda, trusted, hoax, or satire. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies rela"
D19-5024,D17-1317,0,0.273697,"Missing"
D19-5024,D19-5019,0,0.092582,"ith class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They also observed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et al., 2019) participated in both tasks. For the sentence-level classification, they used a voting ensemble including logistic regression, convolutional neural networks, and BERT, in all cases u"
D19-5024,D19-5017,0,0.0948953,"ed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et al., 2019) participated in both tasks. For the sentence-level classification, they used a voting ensemble including logistic regression, convolutional neural networks, and BERT, in all cases using FastText embeddings (Bojanowski et al., 2017) and pre-trained BERT models. Beside these representations, multiple features of readability, sentiment and emotions were considered. For"
D19-5024,2021.ccl-1.108,0,0.0792132,"Missing"
D19-5024,D19-5018,0,0.0333779,"e development set at the end of phase 1 (see Section 6). Team CUNLP (Alhindi et al., 2019) considered two approaches for the sentence-level task. The first approach was based on fine-tuning BERT. The second approach complemented the fine-tuned BERT approach by feeding its decision into a logistic regressor, together with features from the Linguistic Inquiry and Word Count (LIWC)2 lexicon and punctuation-derived features. Similarly to Gupta et al. (2019), for the fragment-level problem they used a Bi-LSTM-CRF architecture, combining both character- and word-level embeddings. Team ProperGander (Madabushi et al., 2019) also used BERT, but they paid special attention to the imbalance of the data, as well as to the differences between training and testing. They showed that augmenting the training data by oversampling yielded improvements when testing on data that is temporally far from the training (by increasing recall). In order to deal with the imbalance, they performed cost-sensitive classification, i.e., the errors on the smaller positive class were more costly. For the fragment-level classification, inspired by named entity recognition, they used a model based on BERT using Continuous Random Field stack"
D19-5024,D19-5014,0,0.053654,".2 Team Tha3aroon (Fadel and Al-Ayyoub, 2019) implemented an ensemble of three classifiers: two based on BERT and one based on a universal sentence encoder (Cer et al., 2018). Team NSIT (Aggarwal and Sadana, 2019) explored three of the most popular transfer learning models: various versions of ELMo, BERT, and RoBERTa (Liu et al., 2019). Team Mindcoders (Vlad et al., 2019) combined BERT, Bi-LSTM and Capsule networks (Sabour et al., 2017) into a single deep neural network and pre-trained the resulting network on corpora used for related tasks, e.g., emotion classification. Finally, team ltuorp (Mapes et al., 2019) used an attention transformer using BERT trained on Wikipedia and BookCorpus. Teams Participating in the Sentence-Level Classification Only Team CAUnLP (Hou and Chen, 2019) used two context-aware representations based on BERT. In the first representation, the target sentence is followed by the title of the article. In the second representation, the previous sentence is also added. They performed subsampling in order to deal with class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They"
D19-5024,D19-5022,0,0.136973,"detect the sentences that contain propaganda. A total of 12 teams submitted systems for the FLC task, 25 teams did so for the SLC task, and 14 teams eventually submitted a system description paper. For both subtasks, most systems managed to beat the baseline by a sizable margin. The leaderboard and the data from the competition are available at http://propaganda.qcri. org/nlp4if-shared-task/. 1 2 Related Work Propaganda has been tackled mostly at the article level. Rashkin et al. (2017) created a corpus of news articles labelled as propaganda, trusted, hoax, or satire. Barr´on-Cede˜no et al. (2019) experimented with a binarized version of that corpus: propaganda vs. the other three categories. Barr´on-Cedeno et al. (2019) annotated a large binary corpus of propagandist vs. non-propagandist articles and proposed a feature-based system for discriminating between them. In all these cases, the labels were obtained using distant supervision, assuming that all articles from a given news outlet share the label of that outlet, which inevitably introduces noise (Horne et al., 2018). A related field is that of computational argumentation which, among others, deals with some logical fallacies rela"
D19-5024,D14-1162,0,0.0840148,"RT. In the first representation, the target sentence is followed by the title of the article. In the second representation, the previous sentence is also added. They performed subsampling in order to deal with class imbalance, and experimented with BERTBASE and BERTLARGE Team LIACC (Ferreira Cruz et al., 2019) used hand-crafted features and pre-trained ELMo embeddings. They also observed a boost in performance when balancing the dataset by dropping some negative examples. Team JUSTDeep (Al-Omari et al., 2019) used a combination of models and features, including word embeddings based on GloVe (Pennington et al., 2014) concatenated with vectors representing affection and lexical features. These were combined in an ensemble of supervised models: bi-LSTM, XGBoost, and variations of BERT. Team YMJA (Hua, 2019) also based their approach on fine-tuned BERT. Inspired by kaggle competitions on sentiment analysis, they created an ensemble of models via cross-validation. Team jinfen (Li et al., 2019) used a logistic regression model fed with a manifold of representations, including TF.IDF and BERT vectors, as well as vocabularies and readability measures. 9.3 Teams Participating in Both Tasks Team MIC-CIS (Gupta et"
D19-5024,N18-1202,0,0.024846,"supervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million parameters. Moreover, oversampling of the least represented classes proved to be crucial for the final performance. Finally, careful analysis has shown that the model pays special attention to adjectives and adverbs. Team Stalin (Ek and Ghanimifard, 2019) focused on data augmentation to address the relatively small size of the data for fine-tuning contextual embedding representations based on ELMo (Peters et al., 2018), BERT, and Grover (Zellers et al., 2019). The balancing of the embedding space was carried out by means of synthetic minority class over-sampling. Then, the learned representations were fed into an LSTM. Phase 1. The participants tried to achieve the best performance on the development set. A live leaderboard kept track of the submissions. Phase 2. The test set was released and the participants had few days to make final predictions. In phase 2, no immediate feedback on the submissions was provided. The winner was determined based on the performance on the test set. 7 Participants and Approac"
D19-5024,D19-5011,0,0.0628514,"mance of this baseline on the SLC task is shown in Tables 4 and 5. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly. The inefficacy of such a simple random baseline is illustrated in Tables 6 and 7. 5 (0.59) 10 (1.18) 9 (1.07) 101 (11.95) 26 (3.08) 2 (0.24) 10 (1.18) 10 (1.18) 9 Setup The shared task had two phases: In the development phase, the participants were provided labeled training and development datasets; in the testing phase, testing input was further provided. 9.1 Teams Participating in the Fragment-Level Classification Only Team newspeak (Yoosuf and Yang, 2019) achieved the best results on the test set for the FLC task using 20-way word-level classification based on BERT (Devlin et al., 2019): a word could belong to one of the 18 propaganda techniques, to none of them, or to an auxiliary (token-derived) class. The team fed one sentence at a time in order to reduce the workload. In addition to experimenting with an out-of-the-box BERT, they also tried unsupervised fine-tuning both on the 1M news dataset and on Wikipedia. Their best model was based on the uncased base model of BERT, with 12 Transformer layers (Vaswani et al., 2017), and 110 million pa"
D19-5024,D19-5015,0,\N,Missing
E12-1050,P11-1070,0,0.0268914,"Missing"
E12-1050,A00-1031,0,0.691468,"Missing"
E12-1050,J95-4004,0,0.812378,"Missing"
E12-1050,P04-1015,0,0.118221,"Missing"
E12-1050,W02-1001,0,0.374605,"Missing"
E12-1050,W96-0102,0,0.115256,"Missing"
E12-1050,P08-2009,0,0.152721,"came in happy.’ (veselo is an adjective) and ‘The child came in happily.’ (it is an adverb); however, the latter is much more likely. Overall, the following factors should be taken into account when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguo"
E12-1050,W09-4105,1,0.912873,"Missing"
E12-1050,gimenez-marquez-2004-svmtool,0,0.644047,"Missing"
E12-1050,P05-1071,0,0.11926,"Missing"
E12-1050,P01-1035,0,0.0209176,"Missing"
E12-1050,J93-2004,0,0.046523,"his is not trivial since words can play different syntactic roles in different contexts, e.g., can is a noun in “I opened a can of coke.” but a verb in “I can write.” Traditionally, linguists have classified English words into the following eight basic POS categories: noun, pronoun, adjective, verb, adverb, preposition, conjunction, and interjection; this list is often extended a bit, e.g., with determiners, particles, participles, etc., but the number of categories considered is rarely more than 15. Computational linguistics works with a larger inventory of POS tags, e.g., the Penn Treebank (Marcus et al., 1993) uses 48 tags: 36 for partof-speech, and 12 for punctuation and currency symbols. This increase in the number of tags is partially due to finer granularity, e.g., there are special tags for determiners, particles, modal verbs, cardinal numbers, foreign words, existential there, etc., but also to the desire to encode morphological information as part of the tags. For example, there are six tags for verbs in the Penn Treebank: VB (verb, base form; e.g., sing), VBD (verb, past tense; e.g., sang), VBG (verb, gerund or present participle; e.g., singing), VBN (verb, past participle; e.g., sung) VBP"
E12-1050,W96-0213,0,0.407514,"count when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguous cases: JJ|NN (adjective or noun as prenominal modifier), JJ|VBG (adjective or gerund/present participle), JJ|VBN (adjective or past participle), NN|VBG (noun or gerund), and R"
E12-1050,P07-1096,0,0.0328539,".’, where it is a neuter pronoun. Finally, there are ambiguities that are very hard or even impossible1 to resolve, e.g., “Deteto vleze veselo.” can mean both ‘The child came in happy.’ (veselo is an adjective) and ‘The child came in happily.’ (it is an adverb); however, the latter is much more likely. Overall, the following factors should be taken into account when modeling Bulgarian morphosyntax: (1) locality vs. non-locality of grammatical features, (2) interdependence of grammatical features, and (3) domain-specific preferences. 4 Method We used the guided learning framework described in (Shen et al., 2007), which has yielded state-ofthe-art results for English and has been successfully applied to other morphologically complex languages such as Icelandic (Dredze and Wallenberg, 2008); we found it quite suitable for Bulgarian as well. We used the feature set defined in (Shen et al., 2007), which includes the following: 1. The feature set of Ratnaparkhi (1996), including prefix, suffix and lexical, as well as some bigram and trigram context features; 2. Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexi"
E12-1050,E03-2015,1,0.844935,"Missing"
E12-1050,H05-1060,0,0.0405124,"Missing"
E12-1050,P11-2009,0,0.153555,"Missing"
E12-1050,C02-1027,0,0.475658,"Missing"
E12-1050,N03-1033,0,0.150525,"Feature templates as in (Ratnaparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguous cases: JJ|NN (adjective or noun as prenominal modifier), JJ|VBG (adjective or gerund/present participle), JJ|VBN (adjective or past participle), NN|VBG (noun or gerund), and RB|RP (adverb or particle). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an ex"
E12-1050,H05-1059,0,0.0154815,"aparkhi, 1996), which have been shown helpful in bidirectional search; 3. More bigram and trigram features and bilexical features as in (Shen et al., 2007). 1 The problem also exists for English, e.g., the annotators of the Penn Treebank were allowed to use tag combinations for inherently ambiguous cases: JJ|NN (adjective or noun as prenominal modifier), JJ|VBG (adjective or gerund/present participle), JJ|VBN (adjective or past participle), NN|VBG (noun or gerund), and RB|RP (adverb or particle). Note that we allowed prefixes and suffixes of length up to 9, as in (Toutanova et al., 2003) and (Tsuruoka and Tsujii, 2005). 494 We further extended the set of features with the tags proposed for the current word token by a morphological lexicon, which maps words to possible tags; it is exhaustive, i.e., the correct tag is always among the suggested ones for each token. We also used 70 linguistically-motivated, highprecision rules in order to further reduce the number of possible tags suggested by the lexicon. The rules are similar to those proposed by Hinrichs and Trushkina (2004) for German; we implemented them as constraints in the CLaRK system (Simov et al., 2003). Here is an example of a rule: If a wordform i"
E12-1050,W11-0328,0,0.0497215,"Missing"
E12-1050,C10-2146,0,0.0238712,"Missing"
gencheva-etal-2017-context,J15-3002,0,\N,Missing
gencheva-etal-2017-context,nakov-etal-2017-trust,1,\N,Missing
gencheva-etal-2017-context,W02-0109,0,\N,Missing
H05-1105,P92-1003,0,0.540421,"ctions can link two words, two constituents (e.g., NPs), two clauses or even two sentences. Thus, the first challenge is to identify the boundaries of the conjuncts of each coordination. The next problem comes from the interaction of the coordinations with other constituents that attach to its conjuncts (most often prepositional phrases). In the example above we need to decide between [health and [quality of life]] and [[health and qual839 ity] of life]. From a semantic point of view, we need to determine whether the or in chronic diseases or disabilities really means or or is used as an and (Agarwal and Boggess, 1992). Finally, we need to choose between a non-elided and an elided reading: [[chronic diseases] or disabilities] vs. [chronic [diseases or disabilities]]. Below we focus on a special case of the latter problem: noun compound (NC) coordination. Consider the NC car and truck production. Its real meaning is car production and truck production. However, due to the principle of economy of expression, the first instance of production has been compressed out by means of ellipsis. By contrast, in president and chief executive, president is simply linked to chief executive. There is also an all-way coordi"
H05-1105,P01-1005,0,0.087387,"Introduction Resolution of structural ambiguity problems such as noun compound bracketing, prepositional phrase (PP) attachment, and noun phrase coordination requires using information about lexical items and their cooccurrences. This in turn leads to the data sparseness problem, since algorithms that rely on making decisions based on individual lexical items must have statistics about every word that may be encountered. Past approaches have dealt with the data sparseness problem by attempting to generalize from semantic classes, either manually built or automatically derived. More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. They demonstrate the idea on a lexical disambiguation problem for which labeled examples are available “for free”. The problem is to choose which of 2-3 commonly confused In a related strand of work, Lapata and Keller (2004) show that computing n-gram statistics over very large corpora yields results that are competitive with if not better than the best supervised and knowledge-based approaches on a wide range of NLP tasks. For example, they show that for"
H05-1105,C94-2195,0,0.0230055,"Missing"
H05-1105,J82-3004,0,0.168565,"Missing"
H05-1105,W95-0103,0,0.077544,"Missing"
H05-1105,P97-1003,0,0.016244,"ans of ellipsis. By contrast, in president and chief executive, president is simply linked to chief executive. There is also an all-way coordination, where the conjunct is part of the whole, as in Securities and Exchange Commission. More formally, we consider configurations of the kind n1 c n2 h, where n1 and n2 are nouns, c is a coordination (and or or) and h is the head noun4 . The task is to decide whether there is an ellipsis or not, independently of the local context. Syntactically, this can be expressed by the following bracketings: [[n1 c n2 ] h] versus [n1 c [n2 h]]. (Collins’ parser (Collins, 1997) always predicts a flat NP for such configurations.) In order to make the task more 4 The configurations of the kind n h1 c h2 (e.g., company/n cars/h1 and/c trucks/h2 ) can be handled in a similar way. realistic (from a parser’s perspective), we ignore the option of all-way coordination and try to predict the bracketing in Penn Treebank (Marcus et al., 1994) for configurations of this kind. The Penn Treebank brackets NCs with ellipsis as, e.g., (NP car/NN and/CC truck/NN production/NN). and without ellipsis as (NP (NP president/NN) and/CC (NP chief/NN executive/NN)) The NPs with ellipsis are"
H05-1105,P99-1081,0,0.813486,"Missing"
H05-1105,J93-1005,0,0.56583,"els; (i) Pr(p|n1 ) vs. Pr(p|v) (ii) Pr(p, n2 |n1 ) vs. Pr(p, n2 |v). Each of these was computed two different ways: using Pr (probabilities) and # (frequencies). We estimate the n-gram counts using exact phrase queries (with inflections, derived from WordNet 2.0) using the MSN Search Engine. We also allow for determiners, where appropriate, e.g., between the preposition and the noun when querying for #(p, n2 ). We add up the frequencies for all possible variations. Web frequencies were reliable enough and did not need smoothing for (i), but for (ii), smoothing using the technique described in Hindle and Rooth (1993) led to better recall. We also tried back-off from (ii) to (i), as well as back-off plus smoothing, but did not find improvements over smoothing alone. We found n-gram counts to be unreliable when pronouns appear in the test set rather than nouns, and disabled them in these cases. Such examples can still be handled by paraphrases or surface features (see below). 837 2.2.2 Web-Derived Surface Features Authors sometimes (consciously or not) disambiguate the words they write by using surface-level markers to suggest the correct meaning. We have found that exploiting these markers, when they occur"
H05-1105,C92-1029,0,0.051725,"Missing"
H05-1105,N04-1016,0,0.22874,"must have statistics about every word that may be encountered. Past approaches have dealt with the data sparseness problem by attempting to generalize from semantic classes, either manually built or automatically derived. More recently, Banko and Brill (2001) have advocated for the creative use of very large text collections as an alternative to sophisticated algorithms and hand-built resources. They demonstrate the idea on a lexical disambiguation problem for which labeled examples are available “for free”. The problem is to choose which of 2-3 commonly confused In a related strand of work, Lapata and Keller (2004) show that computing n-gram statistics over very large corpora yields results that are competitive with if not better than the best supervised and knowledge-based approaches on a wide range of NLP tasks. For example, they show that for the problem of noun compound bracketing, the performance of an n-gram based model computed using search engine statistics was not significantly different from the best supervised algorithm whose parameters were tuned and which used a taxonomy. They find however that these approaches generally fail to outperform supervised state-of-the-art models that are trained"
H05-1105,W05-0603,1,0.331429,"on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 835–842, Vancouver, October 2005. 2005 Association for Computational Linguistics of individual lexical items. The trick is to figure out how to use information that is latent in the web as a corpus, and web search engines as query interfaces to that corpus. In this paper we describe two techniques – surface features and paraphrases – that push the ideas of Banko and Brill (2001) and Lapata and Keller (2004) farther, enabling the use of statistics gathered from very large corpora in an unsupervised manner. In recent work (Nakov and Hearst, 2005) we showed that a variation of the techniques, when applied to the problem of noun compound bracketing, produces higher accuracy than Lapata and Keller (2004) and the best supervised results. In this paper we adapt the techniques to the structural disambiguation problems of prepositional phrase attachment and noun compound coordination. 2 Prepositional Phrase Attachment A long-standing challenge for syntactic parsers is the attachment decision for prepositional phrases. In a configuration where a verb takes a noun complement that is followed by a PP, the problem arises of whether the PP attach"
H05-1105,P00-1014,0,0.0612618,"Missing"
H05-1105,H94-1048,0,0.0846923,"Missing"
H05-1105,P98-2177,0,0.035961,"Missing"
H05-1105,W97-0109,0,0.0763764,"Missing"
H05-1105,J93-2004,0,\N,Missing
H05-1105,C98-2172,0,\N,Missing
J16-2004,P06-2005,0,0.0376243,"networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. Sajjad, Darwish, and Belinkov (2013) first normalized a dialectal Egyptian Arabic to look like Modern Standard Arabic, and then translated the transformed text to English. In fact, this is a more general problem, which arises with informal sources such as SMS messages and Tweets for just any language (Aw et al. 2006; Han and Baldwin 2011; Wang and Ng 2013; Bojja, Nedunchezhian, and Wang 2015). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language to another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian P"
J16-2004,baldwin-awab-2006-open,0,0.0335019,"Missing"
J16-2004,W05-0909,0,0.0268591,"xt. This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points. 6.2 Isolated Experiments Table 4 shows the results for the isolated experiments. We can see that word-level paraphrasing (CN:*) improves by up to 5.56 and 1.39 BLEU points over the two baselines (both results are statistically significant). Compared with ML2EN, CN:word yields an absolute improvement of 4.41 BLEU points, CN:word0 adds another 0.59, and CN:word0 +morph adds 0.56 more. The scores for TER (v. 0.7.25) (Snover et al. 2006) and METEOR (v. 1.3) (Banerjee and Lavie 2005) are on par with those for BLEU (NIST v. 13). Table 4 further shows that the optimal parameters for the word-level systems involve a very low probability cut-off, and a high number of n-best sentences. This indicates that they are robust to noise, probably because bad source-side phrases are Table 4 Isolated experiments. The subscript shows the parameters found on IN2EN-dev and used for IN2EN-test. The superscript shows the absolute test improvement over the ML2EN and the IN2EN baselines. Scores that are statistically significantly better than ML2EN and IN2EN (p &lt; 0.01, Collins’ sign test) are"
J16-2004,W07-0702,0,0.0758696,"Missing"
J16-2004,2015.mtsummit-users.2,1,0.821723,"Missing"
J16-2004,J93-2003,0,0.136161,"Missing"
J16-2004,N06-1003,0,0.125458,"Missing"
J16-2004,P07-1092,0,0.124127,"did not attempt language adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bitext, it makes sense to try to combine it further with the small bitext; thus, in the following we will directly compare and combine these two approaches. One alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bitext, which we do not have. Pivoting 279 Computational Linguistics Volume 42, Number 2 over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bitext)"
J16-2004,P05-1066,0,0.154994,"Missing"
J16-2004,D10-1041,0,0.0471951,"Missing"
J16-2004,2007.iwslt-1.28,0,0.0181645,"xt to get closer to Indonesian. We use monotone translation, that is, we allow no phrase reordering. We tune the parameters of the log-linear model on a development set using minimum error rate training (MERT) (Och 2003). 4.2.2 Cross-Lingual Morphological Variants. Although phrase-level paraphrasing models context better, it remains limited in the size of its Indonesian vocabulary by the small Indonesian–English bitext, just like word-level paraphrasing. We address this by transforming the Indonesian sentences in the development and the test Indonesian–English bitexts into confusion networks (Dyer 2007; Du, Jiang, and Way 2010), where we add Malay morphological variants for the Indonesian words, weighting them based on Equation (2). Note that we do not alter the training bitext; we just transform the source side of the development and the test data sets into confusion networks. 4.3 Text Rewriting with a Specialized Decoder In this section, we introduce a third approach to source language adaptation, which uses a text rewriting decoder to iteratively find the best adaptation for an input sentence. We first discuss the differences between traditional left-to-right decoders and the text rewrit"
J16-2004,2013.mtsummit-papers.23,0,0.0880309,"Missing"
J16-2004,W07-0717,0,0.0260779,"the adapted bitexts. This work leaves several interesting directions for future research: r r 302 One direction is to add more word editing operations, for example, word deletion, insertion, splitting, and concatenation (because we mainly focused on word substitution in this study). Another promising direction is to add more sentence-level feature functions to the text rewriting decoder to further improve language adaptation. Wang, Nakov, and Ng r r r r r Source Language Adaptation for Resource-Poor MT Future work could also experiment with other phrase table combination methods, for example, Foster and Kuhn (2007) proposed a mixture model whose weights are learned with an EM algorithm (Foster, Chen, and Kuhn 2013). Another direction is to add word reordering. In the current work, we assume no word reordering is necessary (apart from what can be achieved within a phrase), but there actually can exist word-order differences between closely related languages. A further direction is to utilize the relationships between the source and the target sides of the input resource-rich bitext to perform language adaptation, since only the source side was used in our current work. For example, Malay–Indonesian adapt"
J16-2004,A00-1002,0,0.179914,"Missing"
J16-2004,P11-1038,0,0.011923,"forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other things, normalizing informal text to a formal form. Sajjad, Darwish, and Belinkov (2013) first normalized a dialectal Egyptian Arabic to look like Modern Standard Arabic, and then translated the transformed text to English. In fact, this is a more general problem, which arises with informal sources such as SMS messages and Tweets for just any language (Aw et al. 2006; Han and Baldwin 2011; Wang and Ng 2013; Bojja, Nedunchezhian, and Wang 2015). Here the main focus is on coping with spelling errors, abbreviations, and slang, which are typically addressed using string edit distance, while also taking pronunciation into account. This is different from our task, where we try to adapt good, formal text from one language to another. A second relevant line of research is on language adaptation and normalization, when done specifically for improving SMT into another language. For example, Marujo et al. (2011) described a rule-based system for adapting Brazilian Portuguese (BP) to Euro"
J16-2004,P13-4033,0,0.0293805,"Missing"
J16-2004,D11-1125,0,0.019765,"cal mapping, that is, the summation of the logarithms of all morphological variant mapping scores (see Equation (2)) used so far 4.3.5 Model. We use a log-linear model, which combines all features to obtain the score for a hypothesis h as follows: score(h) = X λi fi (h) (3) i where fi is the ith feature function with weight λi . The text rewriting decoder prunes bad hypotheses based on score(h); it also selects the best hypothesis as the one with the highest score(h) across all beams. We tune the weights of the feature functions on a development set using pairwise ranking optimization or PRO (Hopkins and May 2011). We optimize BLEU+1 (Liang et al. 2006), a sentence-level approximation of BLEU, as is standard with PRO. 4.4 Combining Bitexts We have presented our source language adaptation approaches in Sections 4.1, 4.2, and 4.3. Now we explain how we combine the Indonesian–English bitext with the synthetic “Indonesian”–English bitext we have generated. We consider the following three bitext combination approaches: Simple concatenation. Assuming the two bitexts are of comparable quality, we simply train an SMT system on their concatenation. Balanced concatenation with repetitions. The two bitexts are no"
J16-2004,2011.eamt-1.19,0,0.834667,"Missing"
J16-2004,N04-1034,0,0.0985059,"Missing"
J16-2004,D09-1141,1,0.872483,"Missing"
J16-2004,P12-2059,1,0.877922,"Missing"
J16-2004,P03-1021,0,0.195581,"English bitexts. We then pivot over the English phrases to generate Indonesian– Malay phrase pairs. As in the case of word-level pivoting, we derive the paraphrase probabilities from the corresponding probabilities in the two phrase tables, again using Equation (1). We then use the Moses phrase-based SMT decoder (Koehn et al. 2007) to “translate” the Malay side of the Malay–English bitext to get closer to Indonesian. We use monotone translation, that is, we allow no phrase reordering. We tune the parameters of the log-linear model on a development set using minimum error rate training (MERT) (Och 2003). 4.2.2 Cross-Lingual Morphological Variants. Although phrase-level paraphrasing models context better, it remains limited in the size of its Indonesian vocabulary by the small Indonesian–English bitext, just like word-level paraphrasing. We address this by transforming the Indonesian sentences in the development and the test Indonesian–English bitexts into confusion networks (Dyer 2007; Du, Jiang, and Way 2010), where we add Malay morphological variants for the Indonesian words, weighting them based on Equation (2). Note that we do not alter the training bitext; we just transform the source s"
J16-2004,J03-1002,0,0.0396448,"adapted “Indonesian” sentences. In the following we first describe how we generate the word-level Indonesian options and the corresponding weights for the Malay words. Then, we explain how we build, decode, and improve the confusion network. 4.1.1 Inducing Word-Level Paraphrases. We use pivoting over English to induce potential Indonesian word translations for a given Malay word. First, we build separate directed word alignments for the Malay–English bitext and for the Indonesian–English bitext using IBM model 4 (Brown et al. 1993), and then we combine them using the intersect+grow heuristic (Och and Ney 2003). We then induce Malay–Indonesian word translation pairs assuming that if an Indonesian word i and a Malay word m are aligned to the same English word e, they could be mutual translations. Each translation pair is associated with a conditional probability, estimated by pivoting over English: Pr(i|m) = X Pr(i|e)Pr(e|m) (1) e Pr(i|e) and Pr(e|m) are estimated using maximum likelihood from the word alignments. Following Callison-Burch, Koehn, and Osborne (2006), we further assume that i is conditionally independent of m given e. 282 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor"
J16-2004,P02-1040,0,0.0957287,"res (Koehn 2013): forward and reverse translation probabilities, forward and reverse lexicalized phrase translation probabilities, and a phrase penalty. We further used a 5-gram language model trained using the SRILM toolkit (Stolcke 2002) with modified Kneser-Ney smoothing (Kneser and Ney 1995). We combined all features in a log-linear model, namely: (1) the five features in the phrase table, (2) a language model score, (3) a word penalty, that is, the number of words in the output translation, and (4) distance-based reordering cost. We tuned the weights of these features by optimizing BLEU (Papineni et al. 2002) on the development set IN2EN-dev using MERT (Och 2003), and we used them for translation with the phrase-based SMT decoder of Moses. We evaluated all systems on the same test set, IN2EN-test. 3 http://translate.google.com/. 292 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT 5.3 Isolated Experiments In the isolated experiments, we train the SMT system on the adapted “Indonesian”– English bitext only, which allows for a direct comparison to using ML2EN or IN2EN only. 5.3.1 Using Word-Level Paraphrases. In our word-level paraphrasing experiments, we adapted Malay to Indonesi"
J16-2004,P13-2001,0,0.0300217,"Missing"
J16-2004,W11-2602,0,0.0155359,"st, we have a different objective: We do not carry out full 278 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT translation but rather adaptation (since our ultimate goal is to translate into a third language X). A special case of this same line of research is the translation between dialects of the same language, for example, between Cantonese and Mandarin (Zhang 1998), or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and Belinkov 2013). Here again, manual rules and/or language-specific tools and resources are typically used. In the case of Arabic dialects, a further complication arises due to the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online media such as social networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic requires, among other thing"
J16-2004,2010.amta-papers.5,0,0.021765,"). In contrast, we have a different objective: We do not carry out full 278 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT translation but rather adaptation (since our ultimate goal is to translate into a third language X). A special case of this same line of research is the translation between dialects of the same language, for example, between Cantonese and Mandarin (Zhang 1998), or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and Belinkov 2013). Here again, manual rules and/or language-specific tools and resources are typically used. In the case of Arabic dialects, a further complication arises due to the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online media such as social networks, chats, forums, Twitter, and SMS messages, though the Egyptian Wikipedia is one notable exception. This causes further mismatch in domain and genre. Thus, translating from Arabic dialects to Modern Standard Arabic re"
J16-2004,D08-1090,0,0.0327916,"Missing"
J16-2004,2006.amta-papers.25,0,0.0219,"N2EN a proper weight in the combined bitext. This is further reconfirmed by the sophisticated phrase table combination, which yields an additional absolute gain of 0.31 BLEU points. 6.2 Isolated Experiments Table 4 shows the results for the isolated experiments. We can see that word-level paraphrasing (CN:*) improves by up to 5.56 and 1.39 BLEU points over the two baselines (both results are statistically significant). Compared with ML2EN, CN:word yields an absolute improvement of 4.41 BLEU points, CN:word0 adds another 0.59, and CN:word0 +morph adds 0.56 more. The scores for TER (v. 0.7.25) (Snover et al. 2006) and METEOR (v. 1.3) (Banerjee and Lavie 2005) are on par with those for BLEU (NIST v. 13). Table 4 further shows that the optimal parameters for the word-level systems involve a very low probability cut-off, and a high number of n-best sentences. This indicates that they are robust to noise, probably because bad source-side phrases are Table 4 Isolated experiments. The subscript shows the parameters found on IN2EN-dev and used for IN2EN-test. The superscript shows the absolute test improvement over the ML2EN and the IN2EN baselines. Scores that are statistically significantly better than ML2E"
J16-2004,W13-5634,0,0.0471894,"Missing"
J16-2004,N07-1061,0,0.109504,"ge adaptation, except for very simple transliteration for Portuguese–Spanish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bitext, it makes sense to try to combine it further with the small bitext; thus, in the following we will directly compare and combine these two approaches. One alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bitext, which we do not have. Pivoting 279 Computational Linguistics Volume 42, Number 2 over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bitext). Yet another alternative"
J16-2004,D12-1027,1,0.658828,"Missing"
J16-2004,N13-1050,1,0.890749,"Missing"
J16-2004,P09-1018,0,0.0204545,"very simple transliteration for Portuguese–Spanish that ignored context entirely; because it does not substitute a word with a completely different word, transliteration did not help much for Malay–Indonesian, which use unified spelling. Still, once we have language-adapted the large bitext, it makes sense to try to combine it further with the small bitext; thus, in the following we will directly compare and combine these two approaches. One alternative, which we do not explore in this work, is to use cascaded translation using a pivot language (Cohn and Lapata 2007; Utiyama and Isahara 2007; Wu and Wang 2009). Unfortunately, using the resource-rich language as a pivot (poor→rich→X) would require an additional parallel poor–rich bitext, which we do not have. Pivoting 279 Computational Linguistics Volume 42, Number 2 over the target X (rich→X→poor) for the purpose of language adaptation, on the other hand, would miss the opportunity to exploit the relationship between the resource-poor and the resource-rich language; this would also be circular since the first step would ask an SMT system to translate its own training data (we only have one rich–X bitext). Yet another alternative approach for improv"
J16-2004,P98-2238,0,0.100485,"age pairs including Czech– Slovak (Hajiˇc, Hric, and Kubonˇ 2000), Turkish–Crimean Tatar (Altintas and Cicekli 2002), Irish–Scottish Gaelic (Scannell 2006), and Macedonian–Bulgarian (Nakov and Tiedemann 2012). In contrast, we have a different objective: We do not carry out full 278 Wang, Nakov, and Ng Source Language Adaptation for Resource-Poor MT translation but rather adaptation (since our ultimate goal is to translate into a third language X). A special case of this same line of research is the translation between dialects of the same language, for example, between Cantonese and Mandarin (Zhang 1998), or between a dialect of a language and a standard version of that language, for example, between some Arabic dialect (e.g., Egyptian) and Modern Standard Arabic (Bakr, Shaalan, and Ziedan 2008; Sawaf 2010; Salloum and Habash 2011; Sajjad, Darwish, and Belinkov 2013). Here again, manual rules and/or language-specific tools and resources are typically used. In the case of Arabic dialects, a further complication arises due to the informal status of the dialects, which are not standardized and not used in formal contexts but rather only in informal online media such as social networks, chats, fo"
J16-2004,C98-2233,0,\N,Missing
J16-2004,P07-2045,0,\N,Missing
J16-2004,P06-1096,0,\N,Missing
J16-2004,D12-1108,0,\N,Missing
J17-4001,D14-1188,0,0.0242496,"et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, a"
J17-4001,P13-2068,0,0.126108,"ot until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gr"
J17-4001,W16-2302,0,0.0340023,"Missing"
J17-4001,J93-2003,0,0.0452676,"Missing"
J17-4001,W07-0718,0,0.047654,"d cohesion. In Section 4, we have suggested some simple ways to create such metrics, and we have also shown that they yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metri"
J17-4001,W08-0309,0,0.0358878,"y yield better correlation with human judgments. Indeed, we have shown that using linguistic knowledge related to discourse structures can improve existing MT evaluation metrics. Moreover, we have further proposed a state-of-theart evaluation metric that incorporates discourse information as one of its information sources. Research in automatic evaluation for MT is very active, and new metrics are constantly being proposed, especially in the context of the MT metric comparisons (Callison-Burch et al. 2007) and metric shared tasks that ran as part of the Workshop on Machine Translation or WMT (Callison-Burch et al. 2008, 2009, 2010, 2011, 2012; Mach´acˇ ek and Bojar 2013, 2014; Stanojevi´c et al. 2015; Bojar et al. 2016), and the NIST Metrics for Machine Translation Challenge, or MetricsMATR.18 For example, at WMT15, 11 research teams submitted 46 metrics to be compared (Stanojevi´c et al. 2015). Many metrics at these evaluation campaigns explore ways to incorporate syntactic and semantic knowledge. This reflects the general trend in the field. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez 2007; Popovic"
J17-4001,W10-1703,0,0.0860047,"Missing"
J17-4001,W12-3102,0,0.454393,"tion on the training data set.5 Note that our approach to learn the interpolation weights is similar to the one used by PRO for tuning the relative weights of the components of a log-linear SMT model (Hopkins and May 2011). Unlike PRO, (i) we used human judgments, not automatic scores, and (ii) we trained on all pairs, not on a subsample. 3.3 Correlation Measures In our experiments, we only considered translation into English (as we had a discourse parser for English only), and we used the data described in Table 1. For evaluation, we followed the standard set-up of the Metrics task of WMT12 (Callison-Burch et al. 2012). For segment-level evaluation, we used Kendall’s τ (Kendall 1938), which can be 5 When fitting the model, we did not include a bias term, as this was harmful. 692 Joty et al. Discourse Structure in Machine Translation Evaluation calculated directly from the human pairwise judgments. For system-level evaluation, we used Spearman’s rank correlation (Spearman 1904) and, in some cases, also Pearson correlation (Pearson 1895), which are appropriate correlation measures as here we have vectors of scores. We measured the correlation of the evaluation metrics with the human judgments provided by the"
J17-4001,W09-0401,0,0.0707378,"Missing"
J17-4001,W11-2103,0,0.0547186,"Missing"
J17-4001,E06-1032,0,0.0857549,"Missing"
J17-4001,W09-2404,0,0.0325022,"Discourse in Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2"
J17-4001,W12-3156,0,0.0221975,"Machine Translation, DiscoMT (Webber et al. 2013, 2015; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational L"
J17-4001,D07-1007,0,0.126996,"Missing"
J17-4001,W11-1211,0,0.0252185,"010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despit"
J17-4001,P07-1005,0,0.124229,"Missing"
J17-4001,P05-1033,0,0.0907183,"systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-senten"
J17-4001,D08-1024,0,0.100033,"Missing"
J17-4001,2003.mtsummit-papers.9,0,0.0284114,"mmunity. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm"
J17-4001,2003.mtsummit-papers.10,0,0.151421,"iciently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b;"
J17-4001,W11-2107,0,0.0349766,"ally significant improvements are marked with ∗∗ for p-value < 0.01. System D ISCO TKparty Best at WMT 14 FR - EN DE - EN HI - EN CS - EN RU - EN Overall 0.433∗∗ 0.417 +0.016 0.380∗∗ 0.345 +0.035 0.434 0.438 −0.004 0.328∗∗ 0.284 +0.044 0.355∗∗ 0.336 +0.019 0.386∗∗ 0.364 +0.024 the best performing metric both at the system level and at the segment level at the WMT08 and WMT09 metrics tasks. From the original ULC, we replaced M ETEOR by the four newer variants M ETEOR-ex (exact match), M ETEOR-st (+stemming), M ETEOR-sy (+synonymy lookup), and M ETEOR-pa (+paraphrasing) in A SIYA’s terminology (Denkowski and Lavie 2011). We also added to the mix TERp-A (a variant of TER with paraphrasing), BLEU, NIST, and R OUGE-W, for a total of 18 individual metrics. The metrics in this set use diverse linguistic information, including lexical-, syntactic-, and semantic-oriented individual metrics. Regarding the discourse metrics, we used five variants, including DR and DR-LEX described in Section 2, and three more constrained variants oriented to match words between trees only if they occur under the same substructure types (e.g., the same nuclearity type). These variants are designed by introducing structural modificatio"
J17-4001,2012.amta-papers.6,0,0.0150667,"a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovi"
J17-4001,N04-1035,0,0.0647715,"nformation (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual informa"
J17-4001,W07-0738,1,0.707228,"Missing"
J17-4001,W09-0440,1,0.876974,"Missing"
J17-4001,H93-1040,0,0.672723,"Missing"
J17-4001,W10-1750,1,0.808317,"Missing"
J17-4001,D11-1084,0,0.0630017,"Missing"
J17-4001,P12-3024,1,0.890983,"Missing"
J17-4001,E12-3001,0,0.0251663,"istency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. th"
J17-4001,W13-3302,0,0.0210552,"; Webber, PopescuBelis, and Tiedemann 2017). The 2015 edition also started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling"
J17-4001,W16-2345,1,0.883757,"Missing"
J17-4001,D14-1027,1,0.899067,"Missing"
J17-4001,P14-1065,1,0.865351,"Missing"
J17-4001,P15-1078,1,0.877827,"Missing"
J17-4001,W13-2252,0,0.0241376,"ch is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 2013). Thus, specialized evaluation measures have been proposed, for example, for the translation of discourse connectives (Hajlaoui and Popescu-Belis 2012; Meyer et al. 2012; Hajlaoui 2013) and for pronominal anaphora (Hardmeier and Federico 2010), among others. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few previous attempts to incorporate discourse information. One example includes the semantics-aware metrics of Gim´enez and M`arquez (2009) and Gim´enez et al. (2010), which used the Discourse Representation Theory (Kamp and Reyle 1993) and tree-based discourse representation structures (DRS) produced by a semantic parser. They calculated the similarity between the MT output and the references based on DRS subtree matching as defi"
J17-4001,2012.amta-caas14.1,0,0.0723287,"gt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT"
J17-4001,2010.iwslt-papers.10,0,0.146919,"esearch problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Har"
J17-4001,W15-2501,1,0.905816,"Missing"
J17-4001,D12-1108,0,0.0554427,"Missing"
J17-4001,D11-1125,0,0.347504,"es 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently tha"
J17-4001,D12-1083,1,0.888624,"Missing"
J17-4001,J15-3002,1,0.897197,"Missing"
J17-4001,W14-3352,1,0.889983,"Missing"
J17-4001,N03-1017,0,0.0235221,"Missing"
J17-4001,W07-0734,0,0.0592291,"the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of li"
J17-4001,W10-1737,0,0.0859654,"Missing"
J17-4001,D14-1220,0,0.0691207,"Missing"
J17-4001,P14-2047,0,0.0404142,"Missing"
J17-4001,W04-1013,0,0.011816,"ation groups: Group I contains our discourse-based evaluation metrics, DR, and DR-LEX. Group II includes the publicly available MT evaluation metrics that participated in the WMT12 metrics task, excluding those that did not have results for all language pairs (Callison-Burch et al. 2012). More precisely, they are SPEDE 07 P P, AMBER, M ETEOR, T ERROR C AT, SIMPBLEU, XE N E RR C ATS, W ORD B LOCK EC, B LOCK E RR C ATS, and POS F. Group III contains other important individual evaluation metrics that are commonly used in MT evaluation: BLEU (Papineni et al. 2002), NIST (Doddington 2002), R OUGE (Lin 2004), and TER (Snover et al. 2006). We calculated the metrics in this group using Asiya. In particular, we used the following Asiya versions of TER and R OUGE: TER P -A and ROUGE- W.8 For each metric in groups II and III, we present the system-level and segment-level results for the original metric as well as for the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results y"
J17-4001,W05-0904,0,0.34905,"post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sen"
J17-4001,W12-3129,0,0.0531251,"Missing"
J17-4001,P11-1023,0,0.0237776,"between constituency trees (Liu and Gildea 2005). In the semantic case, there are metrics that 16 A notable exception is the work of Tu, Zhou, and Zong (2013), who report up to 2.3 BLEU points of improvement for Chinese-to-English translation using an RST-based MT framework. 17 http://www.isi.edu/natural-language/mteval/. 18 http://www.itl.nist.gov/iad/mig/tests/metricsmatr/. 712 Joty et al. Discourse Structure in Machine Translation Evaluation exploit the similarity over named entities, predicate–argument structures (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), or semantic frames (Lo and Wu 2011). Finally, there are metrics that combine several lexico-semantic aspects (Gim´enez and M`arquez 2010b). As we mentioned earlier, one problem with discourse-related MT research is that it might need specialized evaluation metrics to measure progress. This is especially true for research focusing on relatively rare discourse-specific phenomena, as getting them right or wrong might be virtually “invisible” to standard MT evaluation measures such as BLEU, even when manual evaluation does show improvements (Meyer et al. 2012; ˇ Taira, Sudoh, and Nagata 2012; Nov´ak, Nedoluzhko, and Zabokrtsk y´ 20"
J17-4001,E14-1017,0,0.0578767,"that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion and consistency at the document level (Hardmeier, Nivre, and Tiedemann 2012; Ben et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015). Research in this direction has also been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the"
J17-4001,W13-2202,0,0.0551203,"Missing"
J17-4001,W14-3336,0,0.0496545,"Missing"
J17-4001,A00-2002,0,0.288346,"Missing"
J17-4001,P11-3009,0,0.0273654,"ang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the resear"
J17-4001,W13-3306,0,0.032096,"Missing"
J17-4001,W12-0117,0,0.023287,"Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate"
J17-4001,2012.amta-papers.20,0,0.112933,"u 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far"
J17-4001,W13-3303,0,0.0173794,"Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that c"
J17-4001,W13-2221,0,0.0481696,"Missing"
J17-4001,W13-3307,0,0.04636,"Missing"
J17-4001,P03-1021,0,0.159662,"Missing"
J17-4001,P02-1040,0,0.0997549,"been the focus of the two editions of the DiscoMT workshop, in 2013 and 2015 (Webber et al. 2013, 2015; Hardmeier et al. 2015). Automatic MT evaluation is an integral part of the process of developing and tuning an SMT system. Reference-based evaluation measures compare the output of a system to one or more human translations (called references) and produce a similarity score indicating the quality of the translation. The first metrics approached similarity as a shallow word n-gram matching between the translation and one or more references, with a limited use of linguistic information. BLEU (Papineni et al. 2002) is the bestknown metric in this family, and has been used for years as the evaluation standard in the MT community. BLEU can be efficiently calculated and has shown good correlation with human assessments when evaluating systems on large quantities of text. However, it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has"
J17-4001,P09-2004,0,0.0245932,"al cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Mac"
J17-4001,popescu-belis-etal-2012-discourse,0,0.0540549,"Missing"
J17-4001,W07-0707,0,0.0583959,"2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are usually evaluated by computing translation quality on individual sentences and performing some simple aggregation to produce the system-level evaluation scores. To the best of our knowledge, semantic relations between clauses in a sentence and between sentences in a text have not been"
J17-4001,prasad-etal-2008-penn,0,0.0680612,"man assessments; (ii) Different levels of discourse structure and relations provide different information, which shows smooth accumulative contribution to the final correlation score; (iii) Both discourse relations and nuclearity labels have sizeable impact on the evaluation metric, the latter being more important than the former. The last point emphasizes the appropriateness of the RST theory as a formalism for the discourse structure of texts. Contrary to other discourse theories (e.g., the Discourse Lexicalized Tree Adjoining Grammar [Webber 2004] used to build the Penn Discourse Treebank [Prasad et al. 2008]), RST accounts for the nuclearity as an important element of the discourse structure. 5.3 Qualitative Analysis of Good and Bad Translations In the previous two sections we provided a quantitative analysis of which discourse information has the biggest impact on the performance of our discourse-based measure (DR-LEX) and also which parts of the discourse trees help in distinguishing good from bad translations. In this section, we present some qualitative analysis by inspecting a 707 Computational Linguistics Volume 43, Number 4 WMT_2011 WMT_2012 WMT_2013 F1 measure 0.825 0.800 good bad 0.775"
J17-4001,P05-1034,0,0.192756,"Missing"
J17-4001,W03-0402,0,0.0123992,"urces of information in a more direct way. In that paper, we proposed a pairwise setting for learning MT evaluation metrics with preference tree kernels. The setting can incorporate syntactic and discourse information encapsulated in tree-based structures and the objective is to learn to differentiate better from worse translations by using all subtree structures as implicit features. The discourse parser we used is the same used in this article. The syntactic tree is mainly constructed using the Illinois chunker (Punyakanok and Roth 2001). The kernel used for learning is a preference kernel (Shen and Joshi 2003; Moschitti 2008), which decomposes into Partial Tree Kernel (Moschitti 2006) applications between pairs of enriched tree structures. Word unigram matching is also included in the kernel computation, thus being quite similar to DR-LEX. Table 8 shows the results obtained on the same WMT12 data set by using only discourse structures, only syntactic structures or both structures together. As we can see, the τ scores of the syntactic and the discourse variants are not very different (with a general advantage for syntax), but when put together there is a sizeable improvement in correlation for all"
J17-4001,2006.amta-papers.25,0,0.422386,"it is also known that BLEU and similar metrics are unreliable for high-quality translation output (Doddington 2002; Lavie and Agarwal 2007), and they cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie 2012). Moreover, lexical-matching similarity has been shown to be both insufficient and not strictly necessary for two sentences to convey the same meaning (Coughlin 2003; Culy and Riehemann 2003; Callison-Burch, Osborne, and Koehn 2006). Several alternatives emerged to overcome these limitations, most notably TER (Snover et al. 2006) and METEOR (Lavie and Denkowski 2009). Researchers have explored, with good results, the addition of other levels of linguistic information, including synonymy and paraphrasing (Lavie and Denkowski 2009), syntax (Liu and Gildea 2005; Gim´enez and M`arquez 2007; Popovic and Ney 2007), semantic roles (Gim´enez and M`arquez 2007; Lo, Tumuluru, and Wu 2012), and, most recently, discourse (Gim´enez et al. 2010; Wong and Kit 2012; Guzm´an et al. 2014a, 2014b; Joty et al. 2014). 684 Joty et al. Discourse Structure in Machine Translation Evaluation Beyond all previous considerations, MT systems are u"
J17-4001,W15-3031,0,0.0560843,"Missing"
J17-4001,N15-2015,0,0.019606,"d Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and Watanabe 2000; Tu, Zhou, and Zong 2013). More details on discourse-related research for MT can be found in the survey (Hardmeier 2012), as well as in the Ph.D. thesis Discourse in Statistical Machine Translation (Hardmeier 2014), which received the EAMT Best Thesis Award in 2014. 6.2 Discourse in Machine Translation Evaluation Despite the research interest, so far most attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best.16 A common argument is that current automatic evaluation metrics such as B"
J17-4001,W12-4213,0,0.0584238,"Missing"
J17-4001,W10-2602,0,0.0143272,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,W10-1728,0,0.0292312,"lso started a shared task on crosslingual pronoun translation (Hardmeier et al. 2015), which had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Car"
J17-4001,P13-2066,0,0.0534853,"Missing"
J17-4001,P14-1080,0,0.0440631,"Missing"
J17-4001,N12-1046,0,0.0614062,"Missing"
J17-4001,H05-1097,0,0.0561496,"creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´akov´a 2013; Meyer and Webber 2013; Li, Carpuat, and Nenkova 2014; Steele 2015); full discourse-enabled MT (Marcu, Carlson, and"
J17-4001,W12-2503,0,0.0257687,"ch had a continuation at WMT 2016 (Guillou et al. 2016), and which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu"
J17-4001,D07-1080,0,0.245176,"Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license Computational Linguistics Volume 43, Number 4 1. Introduction From its foundations, Statistical Machine Translation (SMT) as a field had two defining characteristics. First, translation was modeled as a generative process at the sentence level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information (Brown et al. 1993; Koehn, Och, and Marcu 2003). Although modern SMT systems switched to a discriminative log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan"
J17-4001,D12-1097,0,0.0277845,"Missing"
J17-4001,N09-2004,0,0.0308979,"ive log-linear framework (Och 2003; Watanabe et al. 2007; Chiang, Marton, and Resnik 2008; Hopkins and May 2011), which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically rich models. One of the fruitful research directions for improving SMT has been the usage of more structured linguistic information. For instance, in SMT we find systems based on syntax (Galley et al. 2004; Quirk, Menezes, and Cherry 2005), hierarchical structures (Chiang 2005), and semantic roles (Wu and Fung 2009; Lo, Tumuluru, and Wu 2012; Bazrafshan and Gildea 2014). However, it was not until recently that syntaxbased SMT systems started to outperform their phrase-based counterparts, especially for language pairs that need long-distance reordering such as Chinese–English and German–English (Nadejde, Williams, and Koehn 2013). Another less-explored way consists of going beyond the sentence-level; for example, translating at the document level or taking into account broader contextual information. The idea is to obtain adequate translations respecting cross-sentence relations and enforcing cohesion an"
J17-4001,D13-1163,0,0.01503,"which is now being featured also at DiscoMT 2017. These shared tasks have the goals of establishing the state of the art and creating common data sets that would help future research in this area. At this point, several discourse-related research problems have been explored in MT: r r r r consistency in translation (Carpuat 2009; Carpuat and Simard 2012; Ture, Oard, and Resnik 2012; Guillou 2013); lexical and grammatical cohesion and coherence (Tiedemann 2010a, 2010b; Gong, Zhang, and Zhou 2011; Hardmeier, Nivre, and Tiedemann 2012; Voigt and Jurafsky 2012; Wong and Kit 2012; Ben et al. 2013; Xiong et al. 2013; Louis and Webber 2014; Tu, Zhou, and Zong 2014; Xiong, Zhang, and Wang 2015); word sense disambiguation (Vickrey et al. 2005; Carpuat and Wu 2007; Chan, Ng, and Chiang 2007); anaphora resolution and pronoun translation (Hardmeier and Federico 2010; Le Nagard and Koehn 2010; Guillou 2012; Popescu-Belis et al. 2012); 711 Computational Linguistics r r Volume 43, Number 4 handling discourse connectives (Pitler and Nenkova 2009; Becher 2011; Cartoni et al. 2011; Meyer 2011; Meyer et al. 2012; Meyer and Popescu-Belis 2012; Hajlaoui and Popescu-Belis 2012; Popescu-Belis et al. 2012; Meyer and Pol´a"
J17-4001,C00-2137,0,0.0572188,"r the linear interpolation of that metric with DR and with DR-LEX. The combinations with DR and DR-LEX that improve over the original metrics are shown in bold, and those that yield degradation are in italic. For the segment-level evaluation, we further indicate which interpolated results yield statistically significant improvement over the original metric. Note that testing statistical significance is not trivial in our case because we have a complex correlation score for which the assumptions that standard tests make are not met. We thus resorted to a non-parametric randomization framework (Yeh 2000), which is commonly used in NLP research.9 4.1 System-Level Results Table 2 shows the system-level experimental results for WMT12. We can see that DR is already competitive by itself: On average, it has a correlation of 0.807, which is very close to the BLEU and the TER scores from group II (0.810 and 0.812, respectively). Moreover, DR yields improvements when combined with 13 of the 15 metrics, with a resulting correlation higher than those of the two individual metrics being combined. This fact suggests that DR contains information that is complementary to that used by most of the other metr"
J17-4001,W14-3302,0,\N,Missing
K15-1007,N12-1062,0,0.16668,"SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and"
K15-1007,N15-1106,0,0.0232565,"lar include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emer"
K15-1007,W08-0304,0,0.0236305,", 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as we"
K15-1007,P10-4002,0,0.0542157,"Missing"
K15-1007,2011.mtsummit-papers.1,0,0.0267411,"rsion of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence len"
K15-1007,N12-1047,0,0.0364323,"), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et"
K15-1007,N04-1035,0,0.0444567,"Missing"
K15-1007,D08-1024,0,0.400262,"Missing"
K15-1007,N12-1023,0,0.102217,"MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been"
K15-1007,N09-1025,0,0.112274,"Missing"
K15-1007,W11-2123,0,0.0394859,"ll WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ensure convergence, we allowed both MERT and PRO to run for up to 25 iterations (default: 16); we further used 1000best lists (default: 1"
K15-1007,D11-1125,0,0.0434425,"Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains"
K15-1007,P05-1033,0,0.371943,"Missing"
K15-1007,N03-1017,0,0.0487953,"rences makes both MERT and PRO appear more stable, allowing them to generate hypotheses that are less spread, and closer to 1. This can be attributed to the best match reference length, which naturally dampens the effect of verbosity during optimization by selecting the reference that is closest to the respective hypothesis. Overall, we can conclude that MERT learns the tuning set’s verbosity more accurately than PRO. PRO learns verbosity that is more dependent on the source side length of the sentences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothi"
K15-1007,P11-2031,0,0.395785,"(Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of"
K15-1007,2005.iwslt-1.8,0,0.117546,"Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al., 2013), which solves instability issues when tuning on the long sentences; we will discuss our PRO fix and the reasons it is needed in Section 5 below. In order to ens"
K15-1007,P05-1066,0,0.065072,"Missing"
K15-1007,P03-1021,0,0.0297129,"anslator; it is often a stylistic choice. and not necessarily related to fluency or adequacy. This aspect is beyond the scope of the present work. 62 Proceedings of the 19th Conference on Computational Language Learning, pages 62–72, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Method For the following analysis, we need to define the following four quantities: Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (C"
K15-1007,P07-2045,0,0.00687606,"able, allowing them to generate hypotheses that are less spread, and closer to 1. This can be attributed to the best match reference length, which naturally dampens the effect of verbosity during optimization by selecting the reference that is closest to the respective hypothesis. Overall, we can conclude that MERT learns the tuning set’s verbosity more accurately than PRO. PRO learns verbosity that is more dependent on the source side length of the sentences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering mo"
K15-1007,P02-1040,0,0.0928671,"ved in singlereference sets (we used the first reference), and to a lesser extent in multiple-reference sets (five references for MT04 and MT05, and four for MT06 and MT09). For Spanish-English, the story is different: here the English sentences tend to be shorter than the Spanish ones, and the verbosity decreases as the sentence length increases. Overall, in all three cases, the verbosity appears to be length-dependent. 2 For multi-reference sets, we use the length of the reference that is closest to the length of the hypothesis. This is the best match length from the original paper on BLEU (Papineni et al., 2002); it is default in the NIST scoring tool v13a, which we use in our experiments. 3 When dealing with multi-reference sets, we use the average reference length. 4 The datasets we experiment with are described in more detail in Section 4 below. 63 Source length vs. avg. verbosity Arabic−English Spanish−English 1.025 set ● Ar−En−multi Ar−En−single Average verbosity Average verbosity 1.175 1.150 1.125 ● 1.100 ● ● ● ● ● ● ● ●● ●●●● ● ● ●●● ●● ● ●● ●● ● ●●●● ● ●●●● ●● ●● ● ●● ● ● ● ●● ●●● ● ●● ● ●● ● ●● ●●● ●● ● ● ● ● ●● ●● ● ● ●● ● ●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● 1.075 1.000 0.975 ● 0.950 ● ● ● ● ● ●"
K15-1007,P05-1034,0,0.111977,"Missing"
K15-1007,W09-0424,0,0.0539164,"Missing"
K15-1007,P08-2030,0,0.0896902,"ntences in the tuning dataset. Experimental Setup We experimented with the phrase-based SMT model (Koehn et al., 2003) as implemented in Moses (Koehn et al., 2007). For Arabic-English, we trained on all data that was allowed for use in the NIST 2012 except for the UN corpus. For Spanish-English, we used all WMT12 data, again except for the UN data. We tokenized and truecased the English and the Spanish side of all bi-texts and also the monolingual data for language modeling using the standard tokenizer of Moses. We segmented the words on the Arabic side using the MADA ATB segmentation scheme (Roth et al., 2008). We built our phrase tables using the Moses pipeline with maxphrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on GigaWord v.5 with Kneser-Ney smoothing using KenLM (Heafield, 2011). On tuning and testing, we dropped the unknown words for Arabic-English, and we used monotoneat-punctuation decoding for Spanish-English. We tuned using MERT and PRO. We used the standard implementation of MERT from the Moses toolkit, and a fixed version of PRO, as we recommended in (Nakov et al"
K15-1007,C10-1075,0,0.228135,"log-linear framework, and their values are optimized to maximize some automatic metric, typically BLEU, on a tuning dataset. Given this setup, it is clear that the choice of a tuning set and its characteristics, can have significant impact on the SMT system’s performance: if the experimental framework (training data, tuning set, and test set) is highly consistent, i.e., there is close similarity in terms of genre, domain and verbosity,1 then translation quality can be improved by careful selection of tuning sentences that exhibit high degree of similarity to the test set (Zheng et al., 2010; Li et al., 2010). In our recent work (Nakov et al., 2012), we have studied the relationship between optimizers such as MERT, PRO and MIRA, and we have pointed out that PRO tends to generate relatively shorter translations, which could lead to lower BLEU scores on testing. Our solution there was to fix the objective function being optimized: PRO uses sentence-level smoothed BLEU+1, as opposed to the standard dataset-level BLEU. Here we are interested in a related but different question: the relationship between properties of the tuning dataset and the optimizer’s performance. More specifically, we study how th"
K15-1007,P12-1002,0,0.0634812,"rk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer et al., 2012). However, we are not aware of any previous studies of the impact of sentence length and dataset verbosity across optimizers. • source-side length: the number of words in the source sentence; • length ratio: the ratio of the number of words in the output hypothesis to those in the reference;2 • verbosity: the ratio of the number of words in the reference to those in the source;3 • hypothesis verbosity: the ratio of the number of words in the hypothesis to those in the source. Naturally, the verbosity varies across different tuning/testing datasets, e.g., because of style, translator choice, et"
K15-1007,D12-1037,0,0.0142596,"ere has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and"
K15-1007,2006.amta-papers.25,0,0.103422,"Missing"
K15-1007,W10-1738,0,0.02313,"Missing"
K15-1007,C08-1074,0,0.0259005,"al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We refer the interested reader to three recent overviews on parameter optimization for SMT: (McAllester and Keshet, 2011; Cherry and Foster, 2012; Gimpel and Smith, 2012). Still, MERT remains the de-facto standard in the statistical machine translation community. Its stability has been of concern, and is widely studied. Suggestions to improve it include using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques there have been also studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA–PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity was reported when using MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011), as well as large variance with MIRA (Simianer"
K15-1007,D07-1080,0,0.0742595,"–72, c Beijing, China, July 30-31, 2015. 2015 Association for Computational Linguistics 2 3 Related Work Method For the following analysis, we need to define the following four quantities: Tuning the parameters of a log-linear model for statistical machine translation is an active area of research. The standard approach is to use minimum error rate training, or MERT, (Och, 2003), which optimizes BLEU directly. Recently, there has been a surge in new optimization techniques for SMT. Two parameter optimizers that have recently become popular include the margin-infused relaxed algorithm or MIRA (Watanabe et al., 2007; Chiang et al., 2008; Chiang et al., 2009), which is an on-line sentence-level perceptron-like passive-aggressive optimizer, and pairwise ranking optimization or PRO (Hopkins and May, 2011), which operates in batch mode and sees tuning as ranking. A number of improved versions thereof have been proposed in the literature including a batch version of MIRA (Cherry and Foster, 2012), with local updates (Liu et al., 2012), a linear regression version of PRO (Bazrafshan et al., 2012), and a non-sampling version of PRO (Dreyer and Dong, 2015); another example is Rampeon (Gimpel and Smith, 2012). We"
K15-1007,C12-1121,1,0.888761,"Missing"
K15-1007,P13-2003,1,0.728346,"Missing"
K17-1024,abdelali-etal-2014-amara,0,0.035599,"Missing"
K17-1024,S16-1081,0,0.0323627,"els. More relevant to our work is the work of Ganin et al. (2016), who proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target. Here, we use adversarial training to learn task-specific representations in a cross-language setting, which is novel for this task, to the best of our knowledge. Question-question similarity was part of Task 3 on cQA at SemEval-2016/2017 (Nakov et al., 2016b, 2017); there was also a similar subtask as part of SemEval-2016 Task 1 on Semantic Textual Similarity (Agirre et al., 2016). Question-question similarity is an important problem with application to question recommendation, question duplicate detection, community question answering, and question answering in general. Typically, it has been addressed using a variety of textual similarity measures. Some work has paid attention to modeling the question topic, which can be done explicitly, e.g., using a graph of topic terms (Cao et al., 2008), or implicitly, e.g., using LDA-based topic language model that matches the questions not only at the term level but also at the topic level (Zhang et al., 2014). Another importan"
K17-1024,P15-2114,0,0.0838445,"rk has paid attention to modeling the question topic, which can be done explicitly, e.g., using a graph of topic terms (Cao et al., 2008), or implicitly, e.g., using LDA-based topic language model that matches the questions not only at the term level but also at the topic level (Zhang et al., 2014). Another important aspect is syntactic structure, e.g., Wang et al. (2009) proposed a retrieval model for finding similar questions based on the similarity of syntactic trees, and Da San Martino et al. (2016) used syntactic kernels. Yet another emerging approach is to use neural networks, e.g., dos Santos et al. (2015) used convolutional neural networks (CNNs), Romeo et al. (2016) used long short-term memory (LSTMs) networks with neural attention to select the important part when comparing two questions, and Lei et al. (2016) used a combined recurrent–convolutional model to map questions to continuous semantic representations. Finally, translation models have been popular for question-question similarity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) us"
K17-1024,P03-1003,0,0.017696,"ity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English)"
K17-1024,P15-1078,1,0.559126,"Missing"
K17-1024,P16-2075,1,0.644004,"Missing"
K17-1024,D15-1068,1,0.173955,"Missing"
K17-1024,N16-1084,1,0.911664,"Missing"
K17-1024,N16-1153,1,0.473014,"Missing"
K17-1024,W15-1521,0,0.0266254,"of related information is only in English. Here, we adapt the idea for adversarial training for domain adaptation as proposed by Ganin et al. (2016). Figure 2 shows the architecture of our crosslanguage adversarial neural network (CLANN) model. The input to the network is a pair (q, q 0 ), which is first mapped to fixed-length vectors (zq , zq0 ). To generate these word embeddings, one can use existing tools such as word2vec (Mikolov et al., 2013) and monolingual data from the respective languages. Alternatively, one can use crosslanguage word embeddings, e.g., trained using the bivec model (Luong et al., 2015). The latter can yield better initialization, which could be potentially crucial when the labeled data is too small to train the input representations with the end-to-end system. 228 ! ! (new;&ar/en)& ! q ! zq ! ! zq’ ! ! (related;&en)& ! q’ BP& h f ! ! ! ! ! ! c hl gradient& reversal& /λ(grad(Ll)& φ""( q ,q’ ) ! ! input& interaction&& embeddings& layer& features& class&label& ! ! l ! language&label& λ(grad(Ll)& loss$Ll& Figure 2: Architecture of CLANN for the question to question similarity problem in cQA. The network then models the interactions between the input embeddings by passing them th"
K17-1024,S16-1136,1,0.400568,"dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained on the TED and on the OPUS bi-texts; for language modeling, it also used the English Gigaword corpus. We further used as features the cosine similarity between question embeddings. In particular, we used (i) 300-dimensional pre-trained Google News embeddings from (Mikolov et al., 2013), (ii) 100dimensional embeddings trained on the entire Qatar Living forum (Mihaylov and Nakov, 2016), and (iii) 25-dimensional Stanford neural parser embeddings (Socher et al., 2013). The latter are produced by the parser internally, as a by-product. Furthermore, we computed various task-specific features, most of them introduced in the 2015 edition of the SemEval task by (Nicosia et al., 2015; Joty et al., 2015). This includes some question-level features: (1) number of URLs/images/emails/phone numbers; (2) number of tokens/sentences; (3) average number of tokens; (4) type/token ratio; (5) number of nouns/verbs/adjectives/adverbs/ pronouns; (6) number of positive/negative smileys; (7) numbe"
K17-1024,N13-1090,0,0.139962,"ify any test example {qn , qn,k n in Arabic. This scenario is of practical importance, e.g., when an Arabic speaker wants to query the system in Arabic, and the database of related information is only in English. Here, we adapt the idea for adversarial training for domain adaptation as proposed by Ganin et al. (2016). Figure 2 shows the architecture of our crosslanguage adversarial neural network (CLANN) model. The input to the network is a pair (q, q 0 ), which is first mapped to fixed-length vectors (zq , zq0 ). To generate these word embeddings, one can use existing tools such as word2vec (Mikolov et al., 2013) and monolingual data from the respective languages. Alternatively, one can use crosslanguage word embeddings, e.g., trained using the bivec model (Luong et al., 2015). The latter can yield better initialization, which could be potentially crucial when the labeled data is too small to train the input representations with the end-to-end system. 228 ! ! (new;&ar/en)& ! q ! zq ! ! zq’ ! ! (related;&en)& ! q’ BP& h f ! ! ! ! ! ! c hl gradient& reversal& /λ(grad(Ll)& φ""( q ,q’ ) ! ! input& interaction&& embeddings& layer& features& class&label& ! ! l ! language&label& λ(grad(Ll)& loss$Ll& Figure 2:"
K17-1024,S17-2003,1,0.046657,"Missing"
K17-1024,D16-1165,1,0.895205,"Missing"
K17-1024,S16-1083,1,0.894404,"Missing"
K17-1024,S15-2036,1,0.775796,"Missing"
K17-1024,P02-1040,0,0.117239,"to English potentially related questions qi0 . 4.3 Pairwise Features In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation between the input question and the potentially related questions.3 These features were proposed in the previous literature for the question– question similarity problem, and they are necessary to obtain state-of-the-art results. In particular, we calculated the similarity between the two questions using machine translation evaluation metrics, as suggested in (Guzm´an et al., 2016). In particular, we used B LEU (Papineni et al., 2002); NIST (Doddington, 2002); TER v0.7.25 (Snover et al., 2006); M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; Unigram P RECISION; Unigram R ECALL. We also used features that model various components of B LEU, as proposed in (Guzm´an et al., 2015): n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), hypothesis and reference length, length ratio, and brevity penalty. 2 Our cross-language dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English p"
K17-1024,W16-1403,0,0.021532,"etup is useful for many real-world applications. One expensive solution is to annotate data for each input language and then to train a separate system for each one. Another option, which can be also costly, is to translate the input, e.g., using machine translation (MT), and then to work monolingually in the target language (Hartrumpf et al., 2008; Lin and Kuo, 2010; Ture and Boschee, 2016). However, the machine-translated text can be of low quality, might lose some input signal, e.g., it can alter sentiment (Mohammad et al., 2016), or may not be really needed (Bouma et al., 2008; Pouran Ben Veyseh, 2016). Using a unified cross-language representation of the input is a third, less costly option, which allows any combination of input languages during both training and testing. In this paper, we take this last approach, i.e., combining languages during both training and testing, and we study the problem of question-question similarity reranking in community Question Answering (cQA), when the input question can be either in English or in Arabic, and the questions it is compared to are always in English. We start with a simple language-independent representation based on cross-language word embedd"
K17-1024,P07-1059,0,0.0110691,"that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English). The goal is to rerank a set of K retrieved qu"
K17-1024,P13-1045,0,0.0193858,"nslating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained on the TED and on the OPUS bi-texts; for language modeling, it also used the English Gigaword corpus. We further used as features the cosine similarity between question embeddings. In particular, we used (i) 300-dimensional pre-trained Google News embeddings from (Mikolov et al., 2013), (ii) 100dimensional embeddings trained on the entire Qatar Living forum (Mihaylov and Nakov, 2016), and (iii) 25-dimensional Stanford neural parser embeddings (Socher et al., 2013). The latter are produced by the parser internally, as a by-product. Furthermore, we computed various task-specific features, most of them introduced in the 2015 edition of the SemEval task by (Nicosia et al., 2015; Joty et al., 2015). This includes some question-level features: (1) number of URLs/images/emails/phone numbers; (2) number of tokens/sentences; (3) average number of tokens; (4) type/token ratio; (5) number of nouns/verbs/adjectives/adverbs/ pronouns; (6) number of positive/negative smileys; (7) number of single/double/ triple exclamation/interrogation symbols; (8) number of interr"
K17-1024,J11-2003,0,0.0117402,"r question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Brill, 2006; Riezler et al., 2007; Hartrumpf et al., 2008; Lin and Kuo, 2010; Surdeanu et al., 2011; Ture and Boschee, 2016). They can also map terms across languages using Wikipedia links or BabelNet (Bouma et al., 2008; Pouran Ben Veyseh, 2016). However, adversarial training has not been tried in that setting. 227 q: give tips? did you do with it; if the answer is yes, then what the magnitude of what you avoid it? In our country, we leave a 15-20 percent. In our case, the input question q is in a different language (Arabic) than the language of the retrieved questions (English). The goal is to rerank a set of K retrieved questions {qk0 }K k=1 written in a source language (e.g., English) a"
K17-1024,tiedemann-2012-parallel,0,0.00637833,"ith the goal to rerank the ten related English questions. As an example, this is the Arabic translation of the original English question from Figure 1:    K Éë ; à AË@ @ YîE. àñÊª®K @ XAÓ ? HAJ Ó@Q»B@ àñ¢ª . Jj . K AÓ èñ¯ ñë AÓ , ÑªK éK . Ag. B@ I KA¿ @ X@ ? éKñJ . ú¯ QK , AKXCK . éJÖ Ï AK. 20 úÍ@ 15 áÓ We further collected 221 additional original questions and 1,863 related questions as unlabeled data, and we got the 221 English questions translated to Arabic.2 4.2 Cross-language Embeddings We used the TED (Abdelali et al., 2014) and the OPUS parallel Arabic–English bi-texts (Tiedemann, 2012) to extract a bilingual dictionary, and to learn cross-language embeddings. We chose these bi-texts as they are conversational (TED talks and movie subtitles, respectively), and thus informal, which is close to the style of our community question answering forum. We trained Arabic-English cross-language word embeddings from the concatenation of these bitexts using bivec (Luong et al., 2015), a bilingual extension of word2vec, which has achieved excellent results on semantic tasks close to ours (Upadhyay et al., 2016). In particular, we trained 200dimensional vectors using the parameters descri"
K17-1024,D16-1055,0,0.100723,"neural network (CLANN) model over a strong nonadversarial system. 1 Introduction Developing natural language processing (NLP) systems that can work indistinctly with different input languages is a challenging task; yet, such a setup is useful for many real-world applications. One expensive solution is to annotate data for each input language and then to train a separate system for each one. Another option, which can be also costly, is to translate the input, e.g., using machine translation (MT), and then to work monolingually in the target language (Hartrumpf et al., 2008; Lin and Kuo, 2010; Ture and Boschee, 2016). However, the machine-translated text can be of low quality, might lose some input signal, e.g., it can alter sentiment (Mohammad et al., 2016), or may not be really needed (Bouma et al., 2008; Pouran Ben Veyseh, 2016). Using a unified cross-language representation of the input is a third, less costly option, which allows any combination of input languages during both training and testing. In this paper, we take this last approach, i.e., combining languages during both training and testing, and we study the problem of question-question similarity reranking in community Question Answering (cQA"
K17-1024,P16-1157,0,0.018411,"used the TED (Abdelali et al., 2014) and the OPUS parallel Arabic–English bi-texts (Tiedemann, 2012) to extract a bilingual dictionary, and to learn cross-language embeddings. We chose these bi-texts as they are conversational (TED talks and movie subtitles, respectively), and thus informal, which is close to the style of our community question answering forum. We trained Arabic-English cross-language word embeddings from the concatenation of these bitexts using bivec (Luong et al., 2015), a bilingual extension of word2vec, which has achieved excellent results on semantic tasks close to ours (Upadhyay et al., 2016). In particular, we trained 200dimensional vectors using the parameters described in (Upadhyay et al., 2016), with a context window of size 5 and iterating for 5 epochs. We then compute the representation for a question by averaging the embedding vectors of the words it contains. Using these cross-language embeddings allows us to compare directly representations of an Arabic or an English input question q to English potentially related questions qi0 . 4.3 Pairwise Features In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation betwee"
K17-1024,2006.amta-papers.25,0,0.0426642,"atures In addition to the embeddings, we also used some pairwise features that model the similarity or some other relation between the input question and the potentially related questions.3 These features were proposed in the previous literature for the question– question similarity problem, and they are necessary to obtain state-of-the-art results. In particular, we calculated the similarity between the two questions using machine translation evaluation metrics, as suggested in (Guzm´an et al., 2016). In particular, we used B LEU (Papineni et al., 2002); NIST (Doddington, 2002); TER v0.7.25 (Snover et al., 2006); M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; Unigram P RECISION; Unigram R ECALL. We also used features that model various components of B LEU, as proposed in (Guzm´an et al., 2015): n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), hypothesis and reference length, length ratio, and brevity penalty. 2 Our cross-language dataset and code are available at https://github.com/qcri/CLANN 231 3 This required translating the Arabic input question to English. For this, we used an in-house Arabic–English phrasebased statistical machine translation system, trained o"
K17-1024,P11-1066,0,0.013245,"sed on the similarity of syntactic trees, and Da San Martino et al. (2016) used syntactic kernels. Yet another emerging approach is to use neural networks, e.g., dos Santos et al. (2015) used convolutional neural networks (CNNs), Romeo et al. (2016) used long short-term memory (LSTMs) networks with neural attention to select the important part when comparing two questions, and Lei et al. (2016) used a combined recurrent–convolutional model to map questions to continuous semantic representations. Finally, translation models have been popular for question-question similarity (Jeon et al., 2005; Zhou et al., 2011). Unlike that work, here we are interested in cross-language adaptation for question-question similarity reranking. The problem was studied in (Martino et al., 2017) using cross-language kernels and deep neural networks; however, they used no adversarial training. Cross-language Question Answering was the topic of several challenges, e.g., at CLEF 2008 (Forner et al., 2008), at NTCIR-8 (Mitamura et al., 2010), and at BOLT (Soboroff et al., 2016). Cross-language QA methods typically use machine translation directly or adapt MT models to the QA setting (Echihabi and Marcu, 2003; Soricut and Bril"
K19-1096,N19-1423,0,0.0128334,"tween two nodes if user u mentions user v in their tweets (i.e., u has authored a tweet that contains “@v” ). We run node2vec on this graph and we extract the embeddings for the users. As we are interested only in the troll users, we ignore the embeddings of users who are only mentioned by other trolls. We use 128 dimensions for the output embeddings. The embeddings extracted from this graph capture how similar troll users are according to the targets of their discussions on the social network. 1026 3.1.3 BERT BERT offers state-of-the-art text embeddings based on the Transformer architecture (Devlin et al., 2019). We use the pre-trained BERT-large, uncased model, which has 24-layers, 1024-hidden, 16-heads, and 340M parameters, which yields output embeddings with 768 dimensions. Given a tweet, we generate an embedding for it by averaging the representations of the BERT tokens from the penultimate layer of the neural network. To obtain a representation for a user, we average the embeddings of all their tweets. The embeddings extracted from the text capture how similar users are according to their use of language. 3.2 Fully Supervised Learning (T1) Given a set of troll users for which we have labels, we"
K19-1096,N18-5006,1,0.836111,"Missing"
K19-1096,karadzhov-etal-2017-built,1,0.91879,"011), and seminar users (Darwish et al., 2017). Several studies have shown that trust is an important factor in online relationships (Ho et al., 2012; Ku, 2012; Hsu et al., 2014; Elbeltagi and Agag, 2016; Ha et al., 2016), but building trust is a longterm process and our understanding of it is still in its infancy (Salo and Karjaluoto, 2007). This makes it easy for politicians and companies to manipulate user opinions in community forums (Dellarocas, 2006; Li et al., 2016; Zhuang et al., 2018). Trolls. Social media have seen the proliferation of fake news and clickbait (Hardalov et al., 2016; Karadzhov et al., 2017a), aggressiveness (Moore et al., 2012), and trolling (Cole, 2015). The latter often is understood to concern malicious online behavior that is intended to disrupt interactions, to aggravate interacting partners, and to lure them into fruitless argumentation in order to disrupt online interactions and communication (Chen et al., 2013). Here we are interested in studying not just any trolls, but those that engage in opinion manipulation (Mihaylov et al., 2015a,b, 2018). This latter definition of troll has also become prominent in the general public discourse recently. Del Vicario et al. (2016)"
K19-1096,karadzhov-etal-2017-fully,1,0.896349,"Missing"
K19-1096,P15-2085,0,0.0181561,"al., 2017) and political debates (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), as well as stance classification (Mohtarami et al., 2018). For example, Castillo et al. (2011) leverage user reputation, author writing style, and various timebased features, Canini et al. (2011) analyze the interaction of content and social network structure, and Morris et al. (2012) studied how Twitter users judge truthfulness. Zubiaga et al. (2016) study how people handle rumors in social media, and found that users with higher reputation are more trusted, and thus can spread rumors easily. Lukasik et al. (2015) use temporal patterns to detect rumors and to predict their frequency, and Zubiaga et al. (2016) focus on conversational threads. More recent work has focused on the credibility and the factuality in community forums (Nakov et al., 2017; Mihaylova et al., 2018, 2019; Mihaylov et al., 2018). 2.2 Understanding the Role of Political Trolls None of the above work has focused on understanding the role of political trolls. The only closely relevant work is that of Kim et al. (2019), who predict the roles of the Russian trolls on Twitter by leveraging social theory and Actor-Network Theory approache"
K19-1096,K15-1032,1,0.726866,"et al., 2016; Zhuang et al., 2018). Trolls. Social media have seen the proliferation of fake news and clickbait (Hardalov et al., 2016; Karadzhov et al., 2017a), aggressiveness (Moore et al., 2012), and trolling (Cole, 2015). The latter often is understood to concern malicious online behavior that is intended to disrupt interactions, to aggravate interacting partners, and to lure them into fruitless argumentation in order to disrupt online interactions and communication (Chen et al., 2013). Here we are interested in studying not just any trolls, but those that engage in opinion manipulation (Mihaylov et al., 2015a,b, 2018). This latter definition of troll has also become prominent in the general public discourse recently. Del Vicario et al. (2016) have also suggested that the spreading of misinformation online is fostered by the presence of polarization and echo chambers in social media (Garimella et al., 2016, 2017, 2018). Trolling behavior is present and has been studied in all kinds of online media: online magazines (Binns, 2012), social networking sites (Cole, 2015), online computer games (Thacker and Griffiths, 2012), online encyclopedia (Shachaf and Hara, 2010), and online newspapers (Ruiz et al"
K19-1096,R15-1058,1,0.711912,"et al., 2016; Zhuang et al., 2018). Trolls. Social media have seen the proliferation of fake news and clickbait (Hardalov et al., 2016; Karadzhov et al., 2017a), aggressiveness (Moore et al., 2012), and trolling (Cole, 2015). The latter often is understood to concern malicious online behavior that is intended to disrupt interactions, to aggravate interacting partners, and to lure them into fruitless argumentation in order to disrupt online interactions and communication (Chen et al., 2013). Here we are interested in studying not just any trolls, but those that engage in opinion manipulation (Mihaylov et al., 2015a,b, 2018). This latter definition of troll has also become prominent in the general public discourse recently. Del Vicario et al. (2016) have also suggested that the spreading of misinformation online is fostered by the presence of polarization and echo chambers in social media (Garimella et al., 2016, 2017, 2018). Trolling behavior is present and has been studied in all kinds of online media: online magazines (Binns, 2012), social networking sites (Cole, 2015), online computer games (Thacker and Griffiths, 2012), online encyclopedia (Shachaf and Hara, 2010), and online newspapers (Ruiz et al"
K19-1096,P16-2065,1,0.900996,"the presence of polarization and echo chambers in social media (Garimella et al., 2016, 2017, 2018). Trolling behavior is present and has been studied in all kinds of online media: online magazines (Binns, 2012), social networking sites (Cole, 2015), online computer games (Thacker and Griffiths, 2012), online encyclopedia (Shachaf and Hara, 2010), and online newspapers (Ruiz et al., 2011), among others. Troll detection has been addressed by using domain-adapted sentiment analysis (Seah et al., 2015), various lexico-syntactic features about user writing style and structure (Chen et al., 2012; Mihaylov and Nakov, 2016), and graph-based approaches over signed social networks (Kumar et al., 2014). 1024 Sockpuppet is a related notion, and refers to a person who assumes a false identity in an Internet community and then speaks to or about themselves while pretending to be another person. The term has also been used to refer to opinion manipulation, e.g., in Wikipedia (Solorio et al., 2014). Sockpuppets have been identified by using authorship-identification techniques and link analysis (Bu et al., 2013). It has been also shown that sockpuppets differ from ordinary users in their posting behavior, linguistic tra"
K19-1096,S19-2149,1,0.863029,"Missing"
K19-1096,N18-1070,1,0.880841,"Missing"
K19-1096,nakov-etal-2017-trust,1,0.897734,"Missing"
K19-1096,solorio-etal-2014-sockpuppet,0,0.0315272,"et al., 2011), among others. Troll detection has been addressed by using domain-adapted sentiment analysis (Seah et al., 2015), various lexico-syntactic features about user writing style and structure (Chen et al., 2012; Mihaylov and Nakov, 2016), and graph-based approaches over signed social networks (Kumar et al., 2014). 1024 Sockpuppet is a related notion, and refers to a person who assumes a false identity in an Internet community and then speaks to or about themselves while pretending to be another person. The term has also been used to refer to opinion manipulation, e.g., in Wikipedia (Solorio et al., 2014). Sockpuppets have been identified by using authorship-identification techniques and link analysis (Bu et al., 2013). It has been also shown that sockpuppets differ from ordinary users in their posting behavior, linguistic traits, and social network structure (Kumar et al., 2017). Internet Water Army is a literal translation of the Chinese term wangluo shuijun, which is a metaphor for a large number of people who are well organized to flood the Internet with purposeful comments and articles. Internet water army has been allegedly used in China by the government (also known as 50 Cent Party) as"
karadzhov-etal-2017-built,W14-4012,0,\N,Missing
karadzhov-etal-2017-built,P15-2085,0,\N,Missing
karadzhov-etal-2017-built,W16-5004,0,\N,Missing
karadzhov-etal-2017-built,karadzhov-etal-2017-fully,1,\N,Missing
karadzhov-etal-2017-fully,W01-0515,0,\N,Missing
karadzhov-etal-2017-fully,N09-2040,0,\N,Missing
karadzhov-etal-2017-fully,D14-1162,0,\N,Missing
karadzhov-etal-2017-fully,P16-2065,1,\N,Missing
karadzhov-etal-2017-fully,Y10-1062,0,\N,Missing
N16-1084,P11-1151,0,0.145112,"ize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored"
N16-1084,W02-1001,0,0.414237,"inference is intractable for general graphs, i.e., graphs with loops. Despite this, it has been advocated by Pearl (1988) to use BP in loopy graphs as an approximation scheme; see also (Murphy, 2012), page 768. The algorithm is then called “loopy” BP, or LBP. Although LBP gives approximate solutions for general graphs, it often works well in practice (Murphy et al., 1999), outperforming other methods such as mean field (Weiss, 2001) and graph-cut (Burfoot et al., 2011). It is important to mention that the approach presented above (i.e., subsection 3.1) is similar in spirit to the approach of Collins (2002), Carreras and M`arquez (2003) and Punyakanok et al. (2005). The main difference is that they use a Perceptron-like online algorithm, where the updates are done based on the best label configuration (i.e., argmaxy p(y|x, θ)) rather than the marginals. One can use graph-cut (applicable only for binary output variables) or max-product LBP for the decoding task. However, this yields a discontinuous estimate (even with averaged perceptron) for the gradient (see Section 5). For the same reason, we use sum-product LBP rather than max-product LBP. 707 3.2 A Joint Model with Global Normalization Altho"
N16-1084,P15-2114,0,0.0916561,"hibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 )."
N16-1084,P08-1019,0,0.0307759,", in your talking baout airsoft i think its prohibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) fo"
N16-1084,S15-2035,0,0.0824082,"of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classification in cQA, we find some work exploiting threadlevel information. Hou et al. (2015) used features about the position of the comment in the thread. Barr´on-Cede˜no et al. (2015) developed more elaborated global features to model thread structure and the interaction among users. Other work exploited global inference algorithms at the thread-level. For instance, (Zhou et al., 2015c; Zhou et al., 2015b; Barr´on-Cede˜no et al., 2015) treated the task as sequential classification, using a variety of machine learning algorithms to label the sequence of timesorted comments: LSTMs, CRFs, SVMhmm , etc. Finally, Joty et al. (2015) showed that exploiting the pairwise relations between c"
N16-1084,D15-1068,1,0.636014,"Missing"
N16-1084,P11-1143,0,0.118538,"aout airsoft i think its prohibited here. good luck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments"
N16-1084,S15-2047,1,0.844443,"Missing"
N16-1084,S15-2036,1,0.602545,"Missing"
N16-1084,P04-1035,0,0.0140909,"CRF model improves results significantly over all rivaling models, yielding the best results on the task to date. In the remainder of this paper, after discussing related work in Section 2, we introduce our joint models in Section 3. We then describe our experimental settings in Section 4. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to"
N16-1084,W00-0721,0,0.0961383,"ify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classi"
N16-1084,C04-1197,0,0.0586523,"a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a perceptron-like algorithm that gets feedback after inference. Punyakanok et al. (2005) studied empirically and theoretically the cases in which this inference-based learning strategy is superior to the decoupled approach. On the particular problem of comment classification in cQA, we find some work exploiting thre"
N16-1084,W04-2401,0,0.0624102,"-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction problems, including shallow parsing (Punyakanok and Roth, 2000), semantic role labeling (Punyakanok et al., 2004), and joint learning of entities and relations (Roth and Yih, 2004). Further work explored the possibility of coupling learning and inference in the previous setting. For instance, Carreras et al. (2005) presented a model for parsing that jointly trains several local decisions with a percept"
N16-1084,W06-1639,0,0.0569981,"Section 2, we introduce our joint models in Section 3. We then describe our experimental settings in Section 4. The experiments and analysis of results are presented in Section 5. Finally, we summarize our contributions with future directions in Section 6. 2 Related Work The idea of using global inference based on locally learned classifiers has been tried in various settings. In the family of graph-based inference, Pang and Lee (2004) used local classification scores with proximity information as edge weights in a graph-cut inference to collectively identify subjective sentences in a review. Thomas et al. (2006) used the same framework to classify congressional transcribed speeches. They applied a classifier to provide edge weights that reflect the degree of agreement between speakers. Burfoot et al. (2011) extended the framework by including other inference algorithms such as loopy belief propagation and mean-field. “Learning and inference under structural and linguistic constraints” (Roth and Yih, 2004) is a framework to combine the predictions of local classifiers in a global decision process solved by Integer Linear Programming. The framework has been applied to many NLP structure prediction prob"
N16-1084,P15-1025,0,0.200763,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N16-1084,P15-2117,0,0.206056,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N16-1084,S15-2037,0,0.113962,"ck Local: Good, Human: Good Figure 1: Example answer-thread with human annotations and automatic predictions by a local classifier at the comment level. http://stackexchange.com/ 703 Proceedings of NAACL-HLT 2016, pages 703–713, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics Thus, the creation of automatic systems for Community Question Answering (cQA), which could provide efficient and effective ways to find good answers in a forum, has received a lot of research attention recently (Duan et al., 2008; Li and Manandhar, 2011; dos Santos et al., 2015; Zhou et al., 2015a; Wang and Ittycheriah, 2015; Tan et al., 2015; Feng et al., 2015; Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015). There have been also related shared tasks at SemEval-20152 and SemEval20163 (Nakov et al., 2015; Nakov et al., 2016). In this paper, we focus on the particular problem of classifying comments in the answer-thread of a given question as good or bad answers. Figure 1 presents an excerpt of a real example from the QatarLiving dataset from SemEval-2015 Task 3. There is a question on top (Q) followed by eight comments (A1 , A2 , · · · , A8 ). According to the h"
N18-1070,N18-2004,1,0.808385,"ores into a single snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 20"
N18-1070,D17-1317,0,0.0837272,"its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on t"
N18-1070,gencheva-etal-2017-context,1,0.834798,"aluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to so"
N18-1070,N18-5006,1,0.775642,"and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition,"
N18-1070,P16-1044,0,0.326197,"n. T imeDistributed(LST M ) (X, W, E) −−−−−−−−−−−−−−−−→ {m1 , ..., mn } (2) 768 Figure 2: The architecture of our memory network model for stance detection. Furthermore, we convert each input claim s into its representation using the corresponding LSTM and CNN networks as follows: where mj is the LSTM representation of xj , and TimeDistributed() indicates a wrapper that enables training the LSTM over all pieces of evidence by applying the same LSTM model to each time-step of a 3D input tensor, i.e., (X, W, E). While LSTM networks were designed to effectively capture and memorize their inputs (Tan et al., 2016), Convolutional Neural Networks (CNNs) emphasize the local interaction between the individual words in the input word sequence, which is important for obtaining an effective representation. Here, we use a CNN in order to encode each xj into its representation cj as shown below (see line 13 in Table 1). LST M,CN N s −−−−−−−−→ slstm , scnn (4) where slstm and scnn are the representations of s computed using LST M and CN N networks, respectively. Note that these are separate networks with different parameters from those used to encode the pieces of evidence. Lines 10–14 of Table 1 describe the ab"
N18-1070,karadzhov-etal-2017-fully,1,0.797992,"be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model"
N18-1070,N18-1074,0,0.0508096,"snippet, the precision for these new snippets will improve to 0.40, 0.38, 0.42, 0.38, and 0.42 at ranks 1–5, as Figure 5(b) shows. If we further extend the evaluation to the sentence level, the precision will jump to 0.60, 0.58, 0.55, 0.62, and 0.57 at ranks 1–5, as we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers t"
N18-1070,K15-1032,1,0.705898,"xtension of the general architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stan"
N18-1070,W14-2508,0,0.13385,"s we can see in Figure 5(c). 3 4 In 76 cases, our model correctly classified the agree/disagree examples when the evaluation was conducted, and it further provided arguably adequate snippets. Some other recent datasets, to be presented at this same HLT-NAACL’2018 conference, do have such gold evidence annotations (Baly et al., 2018; Thorne et al., 2018). 774 5 Related Work 6 Conclusion While stance detection is an interesting task in its own right, e.g., for media monitoring, it is also an important component for fact checking and veracity inference.5 Automatic fact checking was envisioned by Vlachos and Riedel (2014) as a multi-step process that (i) identifies check-worthy statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions to be asked about these statements (Karadzhov et al., 2017), (iii) retrieves relevant information to create a knowledge base (Shiralkar et al., 2017), and (iv) infers the veracity of these statements, e.g., using text analysis (Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Mihaylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have d"
N18-1070,P16-2065,1,0.613587,"architecture based on a similarity matrix, which we use at inference time, and we show that this extension offers sizable performance gains. (iii) Finally, we show that our model is capable of extracting meaningful snippets from a given text document, which is useful not only for stance detection, but more importantly can support human experts who need to decide on the factuality of a given claim. Introduction Recently, an unprecedented amount of false information has been flooding the Internet with aims ranging from affecting individual people’s beliefs and decisions (Mihaylov et al., 2015; Mihaylov and Nakov, 2016) to influencing major events such as political elections (Vosoughi et al., 2018). Consequently, manual fact checking has emerged with the promise to support accurate and unbiased analysis of rumors spreading in social medias, as well as of claims made by public figures or news media. As manual fact checking is a very tedious task, automatic fact checking has been proposed as a possible alternative. This is often broken into intermediate steps in order to alleviate the task complexity. One such step is stance detection, which is also useful for human experts as a stand-alone task. The task aims"
N18-1070,S16-1074,0,0.0172524,"ylova et al., 2018; Karadzhov et al., 2017; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the"
N18-1070,S16-1003,0,0.260879,"Missing"
N18-1070,C16-1230,0,0.034862,"7; Popat et al., 2017). There have been some nuances in the way researchers have defined the stance detection task. SemEval-2016 Task 6 (Mohammad et al., 2016) targets stances with respect to some target proposition, e.g., entities, concepts or events, as infavor, against, or neither. The winning model in the task was based on transfer learning: a Recurrent Neural Network trained on a large Twitter corpus was used to predict task-relevant hashtags and to initialize a second recurrent neural network trained on the provided dataset for stance prediction (Zarrella and Marsh, 2016). Subsequently, Zubiaga et al. (2016) detected the stance of tweets toward rumors and hot topics using linear-chain conditional random fields (CRFs) and tree CRFs that analyze tweets based on their position in treelike conversational threads. Most commonly, stance detection is defined with respect to a claim, e.g., as in the 2017 Fake News Challenge. The best system in the challenge was an ensemble of gradient-boosted decision trees with rich features (e.g., sentiment, word2vec, singular value decomposition (SVD) and TF.IDF features, etc.) and a deep convolutional neural network to address the stance detection problem (Baird et a"
N18-1070,D14-1162,0,0.0795601,"Missing"
N18-1070,N09-2040,0,\N,Missing
N18-2004,karadzhov-etal-2017-built,1,0.798906,"st of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised"
N18-2004,N09-2040,0,0.0986266,"news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Po"
N18-2004,karadzhov-etal-2017-fully,1,0.855688,"Missing"
N18-2004,D16-1011,0,0.191672,"c.). Despite the interdependency between fact checking and stance detection, research on these two problems has not been previously supported by an integrated corpus. This is a gap we aim to bridge by retrieving documents for each claim and annotating them for stance, thus ensuring a natural distribution of the stance labels. Moreover, in order to be trusted by users, a factchecking system should be able to explain the reasoning that led to its decisions. This is best supported by showing extracts (such as sentences or phrases) from the retrieved documents that illustrate the detected stance (Lei et al., 2016). Unfortunately, existing datasets do not offer manual annotation of sentence- or phrase-level supporting evidence. While deep neural networks with attention mechanisms can infer and extract such evidence automatically in an unsupervised way (Parikh et al., 2016), potentially better results can be achieved when having the target sentence provided in advance, which enables supervised or semi-supervised training of the attention. This would allow not only more reliable evidence extraction, but also better stance prediction, and ultimately better factuality prediction. Following this idea, our co"
N18-2004,gencheva-etal-2017-context,1,0.869644,"Missing"
N18-2004,P15-2085,0,0.0248319,"unifies stance detection, stance rationale, relevant document retrieval, and fact checking. This is the first corpus to offer such a combination, not only for Arabic but in general. We further demonstrated experimentally that these unified annotations, and the gold rationales in particular, are beneficial both for stance detection and for fact checking. In future work, we plan to extend the annotations to cover other important aspects of fact checking such as source reliability, language style, and temporal information, which have been shown useful in previous research (Castillo et al., 2011; Lukasik et al., 2015; Ma et al., 2016; Mukherjee and Weikum, 2015; Popat et al., 2017). Overall, the above experiments demonstrate that having a gold rationale can enable better learning. However, the results should be considered as a kind of upper bound on the expected performance improvement, since here we used gold rationales at test time, which would not be available in a real-world scenario. Still, we believe that sizable improvements would still be possible when using the gold rationales for training only. Acknowledgment This research was carried out in collaboration between the MIT Computer Science and Art"
N18-2004,W01-0515,0,0.0674273,"as in (Karadzhov et al., 2017b), we transformed each claim into sub-queries by selecting named entities, adjectives, nouns and verbs with the highest TF.DF score, calculated on a collection of documents from the claims’ sources. Then, we used these sub-queries with the claim itself as input to the search API and retrieved the first 20 returned links, from which we excluded those directing to V ERIFY and R EUTERS, and social media websites that are mostly opinionated. Finally, we calculated two similarity measures between the links’ content (documents) and the claims: the tri-gram containment (Lyon et al., 2001) and the cosine distance between average word embeddings of both texts.6 . We only kept documents with non-zero values for both measures, yielding 3,042 documents: 1,239 for false claims and 1,803 for true claims. (2a) (original false claim) FIFA intends to investigate the game between Syria and Australia.  JË@ Ð Q ª K A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j AJË@Q @ð AK Pñ (2b) (corrected claim in V ERIFY) FIFA does not intend to investigate the game between Syria and Australia, as pro-regime pages claim.  JË@ Ð Q ª K B A ®J ®Ë@ á  K. è@PA J . ÖÏ @ ú¯ J ®j    j® ú«Y K A"
N18-2004,K15-1032,1,0.76343,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,N18-5006,1,0.869083,"Missing"
N18-2004,R15-1058,1,0.833945,"up is not directly supported by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New"
N18-2004,P16-2065,1,0.789602,"ed by existing datasets, which treat fact checking, document retrieval, source credibility, stance detection and rationale extraction as independent tasks. In this paper, we support the interdependencies between these tasks as annotations in the same corpus. We implement this setup on an Arabic fact checking corpus, the first of its kind. 1 Introduction Fact checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 -"
N18-2004,D16-1264,0,0.0505697,"traction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detection or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels"
N18-2004,S16-1003,0,0.170265,"Missing"
N18-2004,D17-1317,0,0.384567,"t checking has recently emerged as an important research topic due to the unprecedented amount of fake news and rumors that are flooding the Internet in order to manipulate people’s opinions (Darwish et al., 2017a; Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016) or to influence the outcome of major events such as political elections (Lazer et al., 2018; Vosoughi et al., 2018). While the number of organizations performing fact checking is growing, these efforts cannot keep up with the pace at which false claims are being produced, including also clickbait (Karadzhov et al., 2017a), hoaxes (Rashkin et al., 2017), and satire (Hardalov et al., 2016). Hence, there is need for automatic fact checking. ∗ This work was carried out when the authors were scientists at QCRI, HBKU. 21 Proceedings of NAACL-HLT 2018, pages 21–27 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Related Work Evidence extraction. Finally, an important characteristic of our dataset is that it provides evidence, in terms of text fragments, for the agree and disagree labels. Having such supporting evidence annotated enables both better learning for supervised systems performing stance detect"
N18-2004,N18-1070,1,0.860201,"Missing"
N18-2004,D16-1244,0,0.101792,"Missing"
N18-2004,N18-1074,0,0.0626421,"tion or fact checking, and also the ability for such systems to learn to explain their decisions to users. Having this latter ability has been recognized in previous work on rationalizing neural predictions (Lei et al., 2016). This is also at the core of recent research on machine comprehension, e.g., using the SQuAD dataset (Rajpurkar et al., 2016). However, such annotations have not been done for stance detection or fact checking before. Finally, while preparing the camera-ready version of the present paper, we came to know about a new dataset for Fact Extraction and VERification, or FEVER (Thorne et al., 2018), which is somewhat similar to ours as it it about both factuality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection betwe"
N18-2004,W14-2508,0,0.248156,"ality and stance, and it has annotation for evidence. Yet, it is also different as (i) the claims are artificially generated by manually altering Wikipedia text, (ii) the knowledge base is restricted to Wikipedia articles, and (iii) the stance and the factuality labels are identical, assuming that Wikipedia articles are reliable to be able to decide a claim’s veracity. In contrast, we use real claims from news outlets, we retrieve articles from the entire Web, and we keep stance and factuality as separate labels. The connection between fact checking and stance detection has been argued for by Vlachos and Riedel (2014), who envisioned a system that (i) identifies factual statements (Hassan et al., 2015; Gencheva et al., 2017; Jaradat et al., 2018), (ii) generates questions or queries (Karadzhov et al., 2017b), (iii) creates a knowledge base using information extraction and question answering (Ba et al., 2016; Shiralkar et al., 2017), and (iv) infers the statements’ veracity using text analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in pract"
N18-2004,pasha-etal-2014-madamira,0,0.0837142,"Missing"
N18-2004,P17-2067,0,0.173053,"ext analysis (Banerjee and Han, 2009; Castillo et al., 2011; Rashkin et al., 2017) or information from external sources (Popat et al., 2016; Karadzhov et al., 2017b; Popat et al., 2017). This connection has been also used in practice, e.g., by Popat et al. (2017); however, different datasets had to be used for stance detection vs. fact checking, as no dataset so far has targeted both. Fact checking is very time-consuming, and thus most datasets focus on claims that have been already checked by experts on specialized sites such as Snopes (Ma et al., 2016; Popat et al., 2016, 2017), PolitiFact (Wang, 2017), or Wikipedia hoaxes (Popat et al., 2016).1 As fact checking is mainly done for English, non-English datasets are rare and often unnatural, e.g., translated from English, and focusing on US politics.2 In contrast, we start with claims that are not only relevant to the Arab world, but that were also originally made in Arabic, thus producing the first publicly available Arabic fact-checking dataset. Stance detection has been studied so far disjointly from fact checking. While there exist some datasets for Arabic (Darwish et al., 2017b), the most popular ones are for English, e.g., from SemEval-"
N18-5006,N16-3003,0,0.160717,"opriate sentence tokenizer for each language. For English, NLTK’s (Loper and Bird, 2002) sent_tokenize handles splitting the text into sentences. However, for Arabic it can only split text based on the presence of the period (.) character. This is because other sentence endings — such as question marks— are different characters (e.g., the Arabic question mark is ‘?’, and not ‘?’). Hence, we used our custom regular expressions to split the Arabic text into sentences. Next comes tokenization. For English, we used NLTK’s tokenizer (Bird et al., 2009), while for Arabic we used Farasa’s segmenter (Abdelali et al., 2016). For Arabic, tokenization is not enough; we also need word segmentation since conjunctions and clitics are commonly attached to the main word, e.g., Â ¢þ + Âþtya + Á¤ (‘and his house’, lit. “and house his”). This causes explosion in the vocabulary size and data sparseness. Features Here we do not propose new features, but rather reuse features that have been previously shown to work well for check-worthiness (Hassan et al., 2015; Gencheva et al., 2017). From (Hassan et al., 2015), we include TF.IDFweighted bag of words, part-of-speech tags, named entities as recognized by Alchemy API, sentim"
N18-5006,P17-1042,0,0.0134211,"n (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse features, as we do not have a discourse parser for Arabic. One of the most important components of the system that we had to port across languages were the word embeddings. We experimented with the following cross-language embeddings: – VecMap: we used a parallel English-Arabic corpus of TED talks1 (Cettolo et al., 2012) to generate monolingual embeddings (Arabic and English) using word2vec (Mikolov et al., 2013). Then we projected these embeddings into a joint vector space using VecMap (Artetxe et al., 2017). 2 Note that these results are not comparable to those in (Gencheva et al., 2017) as we use a different evaluation setup: train/test split vs. cross-validation, debates that involve not only Hillary Clinton and Donald Trump, and we also disable the metadata and the discourse parse features. 1 We used TED talks as they are conversational large corpora, which is somewhat close to the debates we train on. 28 Figure 3: Screenshot of ClaimRank’s output for an Arabic news article, sorted by score. System word2vec VecMap MUSE Attract-Repel Random MAP 0.323 0.298 0.319 0.342 0.161 English R-Pr P@5 P@"
N18-5006,W02-0109,0,0.043495,"eir corresponding color codes. Scores are also stored in the session object along with the sentence list as parallel arrays. In case the user wants the sentences sorted by their scores, or wants to mimic one of the annotation sources strategy in sentence selection, the server gets the text from the session, and re-scores/orders it and sends it back to the client. 3.2 Model 3.4 Adaptation to Arabic To handle Arabic along with English, we integrated some new tools. First, we had to add a language detector in order to use the appropriate sentence tokenizer for each language. For English, NLTK’s (Loper and Bird, 2002) sent_tokenize handles splitting the text into sentences. However, for Arabic it can only split text based on the presence of the period (.) character. This is because other sentence endings — such as question marks— are different characters (e.g., the Arabic question mark is ‘?’, and not ‘?’). Hence, we used our custom regular expressions to split the Arabic text into sentences. Next comes tokenization. For English, we used NLTK’s tokenizer (Bird et al., 2009), while for Arabic we used Farasa’s segmenter (Abdelali et al., 2016). For Arabic, tokenization is not enough; we also need word segmen"
N18-5006,J93-2004,0,0.0701683,"se (Vuli´c et al., 2017), thus yielding better vectors, even for English. The overall MAP results for Arabic are competitive, compared to English. The best model is MUSE, while Attract-Repel is way behind, probably because, unlike VecMap and MUSE, its word embeddings are trained on unsegmented Arabic, which causes severe data sparseness issues. We further needed a part-of-speech (POS) tagger for Arabic, for which we used Farasa (Abdelali et al., 2016), while we used NLTK’s POS tagger for English (Bird et al., 2009). This yields different tagsets: for English, this is the Penn Treebank tagset (Marcus et al., 1993), while for Arabic this the Farasa tagset. Thus, we had to further map all POS tags to the same tagset: the Universal tagset (Petrov et al., 2012). 3.5 Evaluation We train the system on five English political debates, and we test on two debates: either English or their Arabic translations. Note that, compared to our original model (Gencheva et al., 2017), here we use more debates: seven instead of four. Moreover, here we exclude some of the features, namely some debate-specific information (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse"
N18-5006,2012.eamt-1.60,0,0.018724,"compared to our original model (Gencheva et al., 2017), here we use more debates: seven instead of four. Moreover, here we exclude some of the features, namely some debate-specific information (e.g., speaker, system messages), in order to be able to process any free text, and also discourse parse features, as we do not have a discourse parser for Arabic. One of the most important components of the system that we had to port across languages were the word embeddings. We experimented with the following cross-language embeddings: – VecMap: we used a parallel English-Arabic corpus of TED talks1 (Cettolo et al., 2012) to generate monolingual embeddings (Arabic and English) using word2vec (Mikolov et al., 2013). Then we projected these embeddings into a joint vector space using VecMap (Artetxe et al., 2017). 2 Note that these results are not comparable to those in (Gencheva et al., 2017) as we use a different evaluation setup: train/test split vs. cross-validation, debates that involve not only Hillary Clinton and Donald Trump, and we also disable the metadata and the discourse parse features. 1 We used TED talks as they are conversational large corpora, which is somewhat close to the debates we train on. 2"
N18-5006,Q17-1022,0,0.0427226,"Missing"
N18-5006,gencheva-etal-2017-context,1,0.907961,"n et al., 2015). It is trained on data annotated by students, professors, and journalists, and uses features such as sentiment, TF.IDF-weighted words, part-of-speech tags, and named entities. In contrast, (i) we have much richer features, (ii) we support English and Arabic, (iii) we learn from choices made by nine reputable fact-checking organizations, and (iv) we can mimic the selection strategy of each of them. In our previous work, we focused on debates from the US 2016 Presidential Campaign and we used pre-existing annotations from online fact-checking reports by professional journalists (Gencheva et al., 2017). Here we use roughly the same features, with some differences (see below). However, (i) we train on more debates (seven instead of four for English, and also Arabic translations for two debates), (ii) we add support for Arabic, and (iii) we deploy a working system. Patwari et al. (2017) focused on the 2016 US Election campaign as well and independently obtained their data in a similar way. However, they used less features, they did not mimic any specific website, nor did they deploy a working system. Introduction The proliferation of fake news demands the attention of both investigative journ"
N18-5006,petrov-etal-2012-universal,0,0.0725666,"Missing"
N18-5006,P13-1162,0,0.0541594,"ttached to the main word, e.g., Â ¢þ + Âþtya + Á¤ (‘and his house’, lit. “and house his”). This causes explosion in the vocabulary size and data sparseness. Features Here we do not propose new features, but rather reuse features that have been previously shown to work well for check-worthiness (Hassan et al., 2015; Gencheva et al., 2017). From (Hassan et al., 2015), we include TF.IDFweighted bag of words, part-of-speech tags, named entities as recognized by Alchemy API, sentiment scores, and sentence length (in tokens). From (Gencheva et al., 2017), we adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and also for subjectivity. 27 Figure 2: Screenshot of ClaimRank’s output for an English presidential debate, in natural order. – MUSE embeddings: In a similar fashion, we generated cross-language embeddings from the same TED talks using Facebook’s supervised MUSE model (Lample et al., 2017) to project the Arabic and the English monolingual embeddings into a joint vector space. – Attract-Repel embeddings: we used the pretrained English-Arabic embeddings from AttractRepel (Mrkši´c et al., 2017). Table 1 shows the system perfor"
N18-5006,J15-3002,0,0.0693925,"Missing"
N18-5006,P17-1006,0,0.0251397,"Missing"
N19-1144,N12-1084,0,0.430083,"are the performance of different machine learning models on OLID. 1 Introduction Offensive content has become pervasive in social media and thus a serious concern for government organizations, online communities, and social media platforms. One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Rec"
N19-1144,W18-3901,1,0.892109,"Missing"
N19-1144,S19-2010,1,0.442727,"Missing"
N19-1144,D14-1181,0,0.011836,"i) an input embedding layer, (ii) a bidirectional LSTM layer, and (iii) an average pooling layer of input features. The concatenation of the LSTM layer and the average pooling layer is further passed through a dense layer, whose output is ultimately passed through a softmax to produce the final prediction. We set two input channels for the input embedding layers: pre-trained FastText embeddings (Bojanowski et al., 2017), as well as updatable embeddings learned by the model during training. CNN Finally, we experiment with a Convolutional Neural Network (CNN) model based on the architecture of (Kim, 2014), and using the same multi-channel inputs as the above BiLSTM. 4.1 Offensive Language Detection The performance on discriminating between offensive (OFF) and non-offensive (NOT) posts is reported in Table 4. We can see that all models perform significantly better than chance, with the neural models performing substantially better than the SVM. The CNN outperforms the RNN model, achieving a macro-F1 score of 0.80. 4.2 Categorization of Offensive Language In this set of experiments, the models were trained to discriminate between targeted insults and threats (TIN) and untargeted (UNT) offenses,"
N19-1144,W18-4401,1,0.535969,"edia and thus a serious concern for government organizations, online communities, and social media platforms. One of the most common strategies to tackle the problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was"
N19-1144,malmasi-zampieri-2017-detecting,1,0.80182,"h this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalized group, and whether the abusive content is explicit or implicit. Wiegand et al. (2018) further applied this idea to German tweets. They experimented with a task on detecting offensive vs. nonoffensive tweets, and also with a second task on further"
N19-1144,W17-3008,0,0.10368,"he problem is to train systems capable of recognizing offensive content, which can then be deleted or set aside for human moderation. In the last few years, there have been several studies on the application of computational methods to deal with this problem. Prior work has studied offensive language in Twitter (Xu et al., 2012; Burnap and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalize"
N19-1144,W17-3012,0,0.142505,"p and Williams, 2015; Davidson et al., 2017; Wiegand et al., 2018), Wikipedia comments,1 and Facebook posts (Kumar et al., 2018). 1 Previous studies have looked into different aspects of offensive language such as the use of abusive language (Nobata et al., 2016; Mubarak et al., 2017), (cyber-)aggression (Kumar et al., 2018), (cyber-)bullying (Xu et al., 2012; Dadvar et al., 2013), toxic comments1 , hate speech (Kwok and Wang, 2013; Djuric et al., 2015; Burnap and Williams, 2015; Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), and offensive language (Wiegand et al., 2018). Recently, Waseem et al. (2017) analyzed the similarities between different approaches proposed in previous work and argued that there was a need for a typology that differentiates between whether the (abusive) language is directed towards a specific individual or entity, or towards a generalized group, and whether the abusive content is explicit or implicit. Wiegand et al. (2018) further applied this idea to German tweets. They experimented with a task on detecting offensive vs. nonoffensive tweets, and also with a second task on further sub-classifying the offensive tweets as profanity, insult, or abuse. However, to the b"
N19-1144,Q17-1010,0,\N,Missing
N19-1154,E17-2039,0,0.0379246,"Missing"
N19-1154,P17-1080,1,0.899859,"r for modeling non-local syntactic and semantic dependencies, character-based ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Resear"
N19-1154,Q19-1004,1,0.851128,"ysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address th"
N19-1154,I17-1001,1,0.895397,"Missing"
N19-1154,C16-1333,0,0.0223916,"U (Papineni et al., 2002). We trained the morphological classifiers and we tested them on a concatenation of the NEWS and the TED testsets, which were automatically tagged as described in the next paragraph. We trained and evaluated the semantic and the syntactic classifiers on existing annotated corpora. See Table 3 for details about the datasets. 1507 Taggers We used RDRPOST (Nguyen et al., 2014) to annotate data for the classifier. For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The tags are grouped into coarse categories such as events, names, time, and logical expressions. There is enough data for English (≈42K), and we randomly sampled the same amount of data we used to train our morphological classifiers to train the semantic classifiers. Yet, only 1,863 annotated sentences (12,783 tokens) were available for German. Thus, in the experiments, we performed 5-fold cross-validation. For CCG supertagging, we used the English CCGBank (Hockenmaier and Steedman, 2007), which contains 41,586/2,407 train/test sentences.4 See Table 3 for more detailed statistics about the"
N19-1154,W16-2308,0,0.171762,"embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Lee et al., 2017, among others), but the focus has been exclusively on the quality of the resulting translation output. However, it remains unclear what input and output units should be chosen if we are primarily interested in representation learning. Here, we aim at bridging this gap by evaluating the quality of NMT-derived embeddings originating from units of different granularity when used for modeling morphology, s"
N19-1154,P16-1160,0,0.0282665,"st NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Lee et al., 2017, among others), but the focus has been exclusively on the quality of the resulting translation output. However, it remains unclear what input and output units should be chosen if we are primarily intere"
N19-1154,P18-1198,0,0.0296093,"Missing"
N19-1154,P16-2058,0,0.042031,"Missing"
N19-1154,I17-1015,1,0.857534,"ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentati"
N19-1154,N19-1423,0,0.121144,"modeling (LM) using long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997). It has been further argued that complex auxiliary tasks such as neural machine translation (NMT) are better tailored for representation learning, as the internal understanding of the input language that needs to be built by the network to be able to translate from one language to another needs to be much more comprehensive compared to what would be needed for a simple word prediction task. This idea is implemented in the seq2seqbased CoVe model (McCann et al., 2017). More recently, the BERT model (Devlin et al., 2019) proposed to use representation from another NMT model, the Transformer, while optimizing for two LM-related auxiliary tasks: (i) masked language model and (ii) next sentence prediction. Another important aspect of representation learning is the basic unit the model operates on. In word2vec-style embeddings, it is the word, but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle voca"
N19-1154,E14-4029,1,0.793179,"al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address the OOV word problem in MT. The choice of translation unit impacts what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been s"
N19-1154,P18-2006,0,0.0202511,"rns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 2018; Gao et al., 2018), and character-based NMT in particular (Heigold et al., 2018; Belinkov and Bisk, 2018). Unlike this work, we compare robustness to noise for units of different granularity. Moreover, we focus on representation learning rather than on the quality of the translation output. 3 Methodology Our methodology is inspired by research on interpreting neural network (NN) models. A typical framework involves extracting feature representations from different components (e.g., encoder/decoder) of a trained model and then training a classifier to make predictions for an auxiliary task. Th"
N19-1154,W18-1807,0,0.473957,"sus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 2018; Gao et al., 2018), and character-based NMT in particular (Heigold et al., 2018; Belinkov and Bisk, 2018). Unlike this work, we compare robustness to noise for units of different granularity. Moreover, we focus on representation learning rather than on the quality of the translation output. 3 Methodology Our methodology is inspired by research on interpreting neural network (NN) models. A typical framework involves extracting feature representations from different components (e.g., encoder/decoder) of a trained model and then training a classifier to make predictions for an auxiliary task. The performance of the trained classifier is considered to be a proxy for judging"
N19-1154,P06-1064,0,0.0101294,"ed the source side with word/BPE/Morfessor/character units. Similarly, when analyzing the representations from the decoder side, we trained the encoder representation with BPE units, and we varied the decoder side using word/BPE/char units. Our motivation for this setup is that we wanted to analyze the encoder/decoder side representations in isolation, keeping the other half of the network (i.e., the decoder/encoder) static across different settings.6 6 4 There are no available CCG banks for the other languages we experiment with, except for a German CCG bank, which is not publicly available (Hockenmaier, 2006). 5 The decoder has to be unidirectional as, at decoding time, the future is unknown. 6 Heigold et al. (2018) used a similar setup. Results We now present the evaluation results for using representations learned from different input units to predict morphology, semantics, and syntax. For subword and character units, we found the activation of the last subword/character unit of a word to be consistently better than using the average of all activations (See Table 4). Therefore, we report only the results using the Last method, for the remainder of the paper. de Last Avg cs ru sub char sub char s"
N19-1154,J07-3004,0,0.00913541,"ed semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The tags are grouped into coarse categories such as events, names, time, and logical expressions. There is enough data for English (≈42K), and we randomly sampled the same amount of data we used to train our morphological classifiers to train the semantic classifiers. Yet, only 1,863 annotated sentences (12,783 tokens) were available for German. Thus, in the experiments, we performed 5-fold cross-validation. For CCG supertagging, we used the English CCGBank (Hockenmaier and Steedman, 2007), which contains 41,586/2,407 train/test sentences.4 See Table 3 for more detailed statistics about the train/dev/test datasets we used. In our experiments, we used 50k BPE operations and we limited the vocabulary of all systems to 50k. Moreover, we trained the word, BPE, Morfessor, and character-based systems with maximum sentence lengths of 80, 100, 100, and 400 units, respectively. For the classification tasks, we used a logistic regression classifier whose input is either the hidden states in the case of the word-based models, or the Last or the Average representations in the case of chara"
N19-1154,D17-1215,0,0.019313,"what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 2018; Gao et al., 2018), and character-based NMT in particular (Heigold et al., 2018; Belinkov and Bisk, 2018). Unlike this work, we compare robustness to noise for units of different granularity. Moreover, we focus on representation learning rather than on the quality of the translation output. 3 Methodology Our methodology is inspired by research on interpreting neural network (NN) models. A typical framework involves extracting feature representations from different components (e.g., encoder/decoder) of a trained model and then training a classifier to make predictions fo"
N19-1154,N19-1002,0,0.0260977,"016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address the OOV word problem in MT. The choice of translation unit impacts what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subwo"
N19-1154,Q17-1026,0,0.182884,"need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung et al., 2016; Lee et al., 2017, among others), but the focus has been exclusively on the quality of the resulting translation output. However, it remains unclear what input and output units should be chosen if we are primarily interested in representat"
N19-1154,Q16-1037,0,0.0359514,"n combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and h"
N19-1154,L18-1008,0,0.0277678,"y tasks: (i) masked language model and (ii) next sentence prediction. Another important aspect of representation learning is the basic unit the model operates on. In word2vec-style embeddings, it is the word, but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morph"
N19-1154,E14-2005,0,0.0123355,"7) and IWSLT (Cettolo et al., 2016). We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU (Papineni et al., 2002). We trained the morphological classifiers and we tested them on a concatenation of the NEWS and the TED testsets, which were automatically tagged as described in the next paragraph. We trained and evaluated the semantic and the syntactic classifiers on existing annotated corpora. See Table 3 for details about the datasets. 1507 Taggers We used RDRPOST (Nguyen et al., 2014) to annotate data for the classifier. For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The tags are grouped into coarse categories such as events, names, time, and logical expressions. There is enough data for English (≈42K), and we randomly sampled the same amount of data we used to train our morphological classifiers to train the semantic classifiers. Yet, only 1,863 annotated sentences (12,783 tokens) were available for German. Thus, in the experiments, we pe"
N19-1154,P02-1040,0,0.105423,"er training data for English (en), German (de), Russian (ru), and Czech (cs). Here, CV stands for cross-validation. 5 Experimental Setup Data and Languages We trained NMT systems for four language pairs: German-English, CzechEnglish, Russian-English, and English-German, using data made available through two popular machine translation campaigns, namely, WMT (Bojar et al., 2017) and IWSLT (Cettolo et al., 2016). We trained the MT models using a concatenation of the NEWS and the TED training datasets, and we tested on official TED test sets (testsets-11-13) to perform the evaluation using BLEU (Papineni et al., 2002). We trained the morphological classifiers and we tested them on a concatenation of the NEWS and the TED testsets, which were automatically tagged as described in the next paragraph. We trained and evaluated the semantic and the syntactic classifiers on existing annotated corpora. See Table 3 for details about the datasets. 1507 Taggers We used RDRPOST (Nguyen et al., 2014) to annotate data for the classifier. For semantic tagging, we used the gold-annotated semantic tags from the Groningen Parallel Meaning Bank (Abzianidze et al., 2017), which were made available by (Bjerva et al., 2016). The"
N19-1154,D14-1162,0,0.0879568,"optimizing for two LM-related auxiliary tasks: (i) masked language model and (ii) next sentence prediction. Another important aspect of representation learning is the basic unit the model operates on. In word2vec-style embeddings, it is the word, but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabul"
N19-1154,N18-1202,0,0.211861,"NLP task could be useful for other tasks as well. For example, word embeddings learned for a simple word prediction task in context, word2vec-style (Mikolov et al., 2013b), have now become almost obligatory in state-of-the-art NLP models. One issue with such word embeddings is that the resulting representation is context-independent. Recently, it has been shown that huge performance gains can be achieved by contextualizing the representations, so that the same word could have a different embedding in different contexts. This is best achieved by changing the auxiliary task. For example, ELMo (Peters et al., 2018) learns contextualized word embeddings from language modeling (LM) using long short-term memory networks (LSTMs) (Hochreiter and Schmidhuber, 1997). It has been further argued that complex auxiliary tasks such as neural machine translation (NMT) are better tailored for representation learning, as the internal understanding of the input language that needs to be built by the network to be able to translate from one language to another needs to be much more comprehensive compared to what would be needed for a simple word prediction task. This idea is implemented in the seq2seqbased CoVe model (M"
N19-1154,P17-2095,1,0.861481,"rent word.2 Word Representation Units We consider four representation units: words, byte-pair encoding (BPE) units, morphological units, and characters. Table 2 shows an example of each representation unit. BPE splits words into symbols (a symbol is a sequence of characters) and then iteratively replaces the most frequent sequences of symbols with a new merged symbol. In essence, frequent character n-grams merge to form one symbol. The number of merge operations is controlled by a hyper-parameter OP; a high value of OP means coarse segmentation and a low value means fine-grained segmentation (Sajjad et al., 2017). For morphologically segmented units, we use an unsupervised morphological segmenter, Morfessor (Smit et al., 2014). Note that although BPE and Morfessor segment words at a similar level of granularity, the segmentation generated by Morfessor is linguistically motivated. For example, it splits the gerund verb shooting into root shoot and the suffix ing. Compare this to the BPE segmentation sho + oting, which has no linguistic connotation. On the extreme, the fully character-level units treat each word as a sequence of characters. Extracting Activations for Subword and Character Units (ii) Las"
N19-1154,E17-2060,0,0.0280466,"MT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), morphological segmentation (Bradbury and Socher, 2016), characters (Durrani et al., 2014; Lee et al., 2017), and hybrid units (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016) to address the OOV word problem in MT. The choice of translation unit impacts what the network learns. Sennrich (2017) carried a systematic error analysis by comparing subword versus character units and found the latter to be better at handling OOV and transliterations, whereas BPEbased subword units were better at capturing syntactic dependencies. In contrast, here we focus on representation learning, not translation quality. Robustness to noise is an important aspect in machine learning. It has been studied for various models (Szegedy et al., 2014; Goodfellow et al., 2015), including NLP in general (Papernot et al., 2016; Samanta and Mehta, 2017; Liang et al., 2018; Jia and Liang, 2017; Ebrahimi et al., 201"
N19-1154,P16-1162,0,0.661548,"but this does not hold for NMT-based models, as computational and memory limitations, as of present, prevent NMT from using a large vocabulary, typically limiting it to 30-50k words (Wu et al., 2016). This is a severe limitation, as most NLP applications need to handle vocabularies of millions of words, e.g., word2vec (Mikolov et al., 2013b), GloVe (Pennington et al., 2014) and FastText (Mikolov et al., 2018) offer pre-trained embeddings for 3M, 2M, and 2.5M words/phrases. The problem is typically addressed using byte-pair encoding (BPE), where words are segmented into pseudo-word sequences (Sennrich et al., 2016). 1504 Proceedings of NAACL-HLT 2019, pages 1504–1516 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics A less popular solution is to use characters as the basic unit (Chung et al., 2016; Lee et al., 2017), and in the case of morphologically complex languages, yet another alternative is to reduce the vocabulary size by using unsupervised morpheme segmentation (Bradbury and Socher, 2016). The impact of using different units of representation in NMT models has been studied in previous work (Ling et al., 2015; Costa-juss`a and Fonollosa, 2016; Chung e"
N19-1154,D16-1159,0,0.0278884,"respect to noise. We found that while representations derived from morphological segments are better for modeling non-local syntactic and semantic dependencies, character-based ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. S"
N19-1154,E14-2006,0,0.157575,"Missing"
N19-1154,D16-1079,0,0.0226064,"Missing"
N19-1154,D18-1503,0,0.0447618,"Missing"
N19-1154,W17-4115,0,0.0210815,"encies, character-based ones are superior for morphology and are also more robust to noise. There is also value in combining different representations. 2 Related Work Representation analysis aims at demystifying what is learned inside the neural network blackbox. This includes analyzing word and sentence embeddings (Adi et al., 2017; Qian et al., 2016b; Ganesh et al., 2017; Conneau et al., 2018, among others), RNN states (Qian et al., 2016a; Shi et al., 2016; Wu and King, 2016; Wang et al., 2017), and NMT representations (Shi et al., 2016; Belinkov et al., 2017a), as applied to morphological (Vylomova et al., 2017; Dalvi et al., 2017), semantic (Qian et al., 2016b; Belinkov et al., 2017b) and syntactic (Linzen et al., 2016; Tran et al., 2018; Conneau et al., 2018) tasks. See Belinkov and Glass (2019) for a recent survey. Other studies carried a more fine-grained neuronlevel analysis for NMT and LM (Dalvi et al., 2019; Bau et al., 2019; Lakretz et al., 2019). While previous work focused on words, here we compare units of different granularities. Subword translation units aim at reducing the vocabulary size and the out-of-vocabulary (OOV) rate. Researchers have used BPE units (Sennrich et al., 2016), mor"
N19-1154,D16-1181,0,0.0642653,"Missing"
N19-1216,D18-1389,1,0.852475,"treme left/right media tend to be propagandistic, while center media are more factual, and thus generally more trustworthy. This connection can be clearly seen in Figure 1. Figure 1: Correlation between bias and factuality for the news outlets in the Media Bias/Fact Check website. 2109 Proceedings of NAACL-HLT 2019, pages 2109–2116 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Despite the connection between factuality and bias, previous research has addressed them as independent tasks, even when the underlying dataset had annotations for both (Baly et al., 2018). In contrast, here we solve them jointly. Our contributions can be summarized as follows: • We study an under-explored but arguably important problem: predicting the factuality of reporting of news media. Moreover, unlike previous work, we do this jointly with the task of predicting political bias. • As factuality and bias are naturally defined on an ordinal scale (factuality: from low to high, and bias: from extreme-left to extreme-right), we address them as ordinal regression. Using multi-task ordinal regression is novel for these tasks, and it is also an under-explored direction in machine"
N19-1216,S16-1039,0,0.0212248,"atures extracted from its corresponding Wikipedia page and Twitter profile, as well as analysis of its URL structure and traffic information about it from Alexa rank. In the present work, we use a similar set of features, but we treat the problem as one of ordinal regression. Moreover, we model the political ideology and the factuality of reporting jointly in a multitask learning setup, using several auxiliary tasks. Multitask Ordinal Regression Ordinal regression is well-studied and is commonly used for text classification on an ordinal scale, e.g., for sentiment analysis on a 5-point scale (He et al., 2016; Rosenthal et al., 2017a). However, multi-task ordinal regression remains an understudied problem. Yu et al. (2006) proposed a Bayesian framework for collaborative ordinal regression, and demonstrated that modeling multiple ordinal regression tasks outperforms single-task models. 2110 Walecki et al. (2016) were interested in jointly predicting facial action units and their intensity level. They argued that, due to the high number of classes, modeling these tasks independently would be inefficient. Thus, they proposed the copula ordinal regression model for multi-task learning and demonstrated"
N19-1216,P14-1105,0,0.23407,"for “fake news” detection (Horne et al., 2018a). It has also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates, 2018). Unlike the above work, here we focus on predicting the political ideology of news media outlets. In our previous work (Baly et al., 2018), we did target the politic"
N19-1216,C18-1285,0,0.0857451,"Missing"
N19-1216,karadzhov-etal-2017-built,1,0.776933,"d have reached millions of users, and the harm caused could hardly be undone. An arguably more promising direction is to focus on fact-checking entire news outlets, which can be done in advance. Then, we could fact-check the news before they were even written: by checking how trustworthy the outlets that published them are. Knowing the reliability of a medium is important not only when fact-checking a claim (Popat et al., 2017; Nguyen et al., 2018), but also when solving article-level tasks such as “fake news” and click-bait detection (Brill, 2001; Finberg et al., 2002; Hardalov et al., 2016; Karadzhov et al., 2017; De Sarkar et al., 2018; Pan et al., 2018; P´erez-Rosas et al., 2018) Political ideology (or left/right bias) is a related characteristic, e.g., extreme left/right media tend to be propagandistic, while center media are more factual, and thus generally more trustworthy. This connection can be clearly seen in Figure 1. Figure 1: Correlation between bias and factuality for the news outlets in the Media Bias/Fact Check website. 2109 Proceedings of NAACL-HLT 2019, pages 2109–2116 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Despite the connection"
N19-1216,D18-1388,0,0.286431,"multi-task learning setup, using several auxiliary tasks. Predicting Political Ideology In previous work, political ideology, also known as media bias, was used as a feature for “fake news” detection (Horne et al., 2018a). It has also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates,"
N19-1216,P16-2065,1,0.857213,"k has modeled the factuality of reporting at the medium level by checking the general stance of the target medium with respect to known manually factchecked claims, without access to gold labels about the overall medium-level factuality of reporting (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017, 2018). The trustworthiness of Web sources has also been studied from a Data Analytics perspective, e.g., Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims. In social media, there has been research targeting the user, e.g., finding malicious users (Mihaylov and Nakov, 2016; Mihaylova et al., 2018; Mihaylov et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017). Unlike the above work, here we study source reliability as a task in its own right, using manual gold annotations specific for the task and assigned by independent fact-checking journalists. Moreover, we address the problem as one of ordinal regression on a three-point scale, and we solve it jointly with political ideology prediction in a multi-task learning setup, using several auxiliary tasks. Predicting Political Ideology In p"
N19-1216,C18-1287,0,0.129334,"Missing"
N19-1216,P18-1022,0,0.0920354,"for the task and assigned by independent fact-checking journalists. Moreover, we address the problem as one of ordinal regression on a three-point scale, and we solve it jointly with political ideology prediction in a multi-task learning setup, using several auxiliary tasks. Predicting Political Ideology In previous work, political ideology, also known as media bias, was used as a feature for “fake news” detection (Horne et al., 2018a). It has also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on prop"
N19-1216,P17-1068,0,0.165651,"Missing"
N19-1216,D17-1317,0,0.141066,"vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates, 2018). Unlike the above work, here we focus on predicting the political ideology of news media outlets. In our previous work (Baly et al., 2018), we did target the political bias of entire news outlets, as opposed to working at the article level (we also modeled factuality of reporting, but as a separate task without trying multi-task learning). In addition to the text of the articles published by the target news medium, we used features extracted fr"
N19-1216,S17-2088,1,0.917538,"from its corresponding Wikipedia page and Twitter profile, as well as analysis of its URL structure and traffic information about it from Alexa rank. In the present work, we use a similar set of features, but we treat the problem as one of ordinal regression. Moreover, we model the political ideology and the factuality of reporting jointly in a multitask learning setup, using several auxiliary tasks. Multitask Ordinal Regression Ordinal regression is well-studied and is commonly used for text classification on an ordinal scale, e.g., for sentiment analysis on a 5-point scale (He et al., 2016; Rosenthal et al., 2017a). However, multi-task ordinal regression remains an understudied problem. Yu et al. (2006) proposed a Bayesian framework for collaborative ordinal regression, and demonstrated that modeling multiple ordinal regression tasks outperforms single-task models. 2110 Walecki et al. (2016) were interested in jointly predicting facial action units and their intensity level. They argued that, due to the high number of classes, modeling these tasks independently would be inefficient. Thus, they proposed the copula ordinal regression model for multi-task learning and demonstrated that it can outperform"
N19-1216,D13-1010,0,0.070197,"as also been the target of classification, e.g., Horne et al. (2018b) predicted whether an article is biased (political or bias) vs. unbiased. Similarly, Potthast et al. (2018) classified the bias in a target article as (i) left vs. right vs. mainstream, or as (ii) hyper-partisan vs. mainstream. Left-vs-right bias classification at the article level was also explored by Kulkarni et al. (2018), who modeled both the textual and the URL contents of the target article. There has been also work targeting bias at the phrase or the sentence level (Iyyer et al., 2014), focusing on political speeches (Sim et al., 2013) or legislative documents (Gerrish and Blei, 2011), or targeting users in Twitter (Preot¸iuc-Pietro et al., 2017). Another line of related work focuses on propaganda, which can be seen as a form of extreme bias (Rashkin et al., 2017; Barr´on-Cede˜no et al., 2019a,b). See also a recent position paper (Pitoura et al., 2018) and an overview paper on bias on the Web (BaezaYates, 2018). Unlike the above work, here we focus on predicting the political ideology of news media outlets. In our previous work (Baly et al., 2018), we did target the political bias of entire news outlets, as opposed to worki"
nakov-vogel-2017-robust,D09-1141,1,\N,Missing
nakov-vogel-2017-robust,P02-1040,0,\N,Missing
nakov-vogel-2017-robust,D08-1024,0,\N,Missing
nakov-vogel-2017-robust,P13-2003,1,\N,Missing
nakov-vogel-2017-robust,W12-3136,1,\N,Missing
nakov-vogel-2017-robust,P07-2045,0,\N,Missing
nakov-vogel-2017-robust,P08-2030,0,\N,Missing
nakov-vogel-2017-robust,N09-1025,0,\N,Missing
nakov-vogel-2017-robust,P11-2031,0,\N,Missing
nakov-vogel-2017-robust,N06-1003,0,\N,Missing
nakov-vogel-2017-robust,R13-1066,1,\N,Missing
nakov-vogel-2017-robust,N03-1017,0,\N,Missing
nakov-vogel-2017-robust,K15-1007,1,\N,Missing
nakov-vogel-2017-robust,2005.iwslt-1.8,0,\N,Missing
nakov-vogel-2017-robust,D07-1080,0,\N,Missing
nakov-vogel-2017-robust,C10-1075,0,\N,Missing
nakov-vogel-2017-robust,W07-0716,0,\N,Missing
nakov-vogel-2017-robust,P12-1002,0,\N,Missing
nakov-vogel-2017-robust,W11-2123,0,\N,Missing
nakov-vogel-2017-robust,P03-1021,0,\N,Missing
nakov-vogel-2017-robust,N12-1047,0,\N,Missing
nakov-vogel-2017-robust,D11-1125,0,\N,Missing
nakov-vogel-2017-robust,N12-1062,0,\N,Missing
nakov-vogel-2017-robust,C12-1121,1,\N,Missing
nakov-vogel-2017-robust,N12-1023,0,\N,Missing
P05-3017,X96-1039,0,0.048854,"’ {NO ORDER, ALLOW GAPS} [layer=’shallow_parse’ && tag_type=’NP’ [layer=’chemicals’] AS chem $ ] [layer=’shallow_parse’ && tag_type=’NP’ [layer=’MeSH’ && label BELOW ""C""] AS dis $ ] ] AS sent SELECT chem.content,dis.content,sent.content This looks for sentences containing two NPs in any order without overlaps (NO ORDER) and separated by any number of intervening elements. We further require one of the NPs to end (ensured by the $ symbol) with a chemical, and the other (the disease) to end with a MeSH term from the C subtree. 4 System Architecture Our basic model is similar to that of TIPSTER (Grishman, 1996): each annotation is stored as a record, which specifies the character-level beginning and ending positions, the layer and the type. The basic table9 contains the following columns: (1) annotation id; (2) doc id; (3) section: title, abstract or body; (4) layer id: layer identifier (word, POS, shallow parse, sentence, etc.); (5) start char pos: beginning character position, relative to section and doc id; (6) end char pos: ending character position; (7) tag type: a layer-specific token identifier. After evaluating various different extensions of the structure above, we have arrived at one with"
P05-3017,U04-1019,0,0.0772853,"oolean expressions over nodes and regular expressions inside nodes. Matching uses a binary index and is performed recursively starting at the top node in the query. TIGERSearch2 is associated with the German syntactic corpus TIGER. The tool is more typed than TGrep2 and allows search over discontinuous constituents that are common in German. TIGERSearch stores the corpus in a Prolog-like logical form and searches using unification matching. LPath is an extension of XPath with three features: immediate precedence, subtree scoping and edge alignment. The queries are executed in an SQL database (Lai and Bird, 2004). Other tree query languages include CorpusSearch, Gsearch, Linguist’s Search Engine, Netgraph, TIQL, VIQTORYA etc. Some tools go beyond the tree model and allow multiple intersecting hierarchies. Emu (Cassidy and Harrington, 2001) supports sequential levels of annotations over speech datasets. Hierarchical relations may exist between tokens in different levels, but precedence is defined only between elements within the same level. The queries cannot 1 2 http://tedlab.mit.edu/∼dr/Tgrep2/ http://www.ims.uni-stuttgart.de/projekte/TIGER/TIGERSearch/ express immediate precedence and are executed u"
P05-3017,ma-etal-2002-models,0,\N,Missing
P05-3017,bird-etal-2000-towards,0,\N,Missing
P05-3017,P02-1032,1,\N,Missing
P08-1052,J94-4005,0,0.0190161,"ama et al. (2002), who use named entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 2.4 Freq. 2205 1923 771 382 189 189 169 148 106 81 78 77 66 66 58 48 47 45 ... 4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The"
P08-1052,P98-1015,0,0.119469,"Missing"
P08-1052,J06-1003,0,0.00331044,"translation, word sense disambiguation, and information extraction. For example, a relational search engine like TextRunner, which serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). Introduction Despite the tremendous amount of work on word similarity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach o"
P08-1052,S07-1003,1,0.387472,"Missing"
P08-1052,C94-2119,0,0.093553,"amed entity recognizers and look for anchors belonging to matching semantic classes, e.g., LOCATION, ORGANIZATION. The idea is further extended by Nakov et al. (2004), who apply it in the biomedical domain, imposing the additional restriction that the sentences from which the paraphrases are extracted cite the same target paper. 2.4 Freq. 2205 1923 771 382 189 189 169 148 106 81 78 77 66 66 58 48 47 45 ... 4 Word Similarity Another important group of related work is on using syntactic dependency features in a vector-space model for measuring word similarity, e.g., (Alshawi and Carter, 1994), (Grishman and Sterling, 1994), (Ruge, 1992), and (Lin, 1998). For example, given a noun, Lin (1998) extracts verbs that have that noun as a subject or object, and adjectives that modify it. 3 Method Given a pair of nouns, we try to characterize the semantic relation between them by leveraging the vast size of the Web to build linguistically-motivated lexically-specific features. We mine the Web for sentences containing the target nouns, and we extract the connecting verbs, prepositions, and coordinating conjunctions, which we use in a vector-space model to measure relational similarity. The process of extraction starts wi"
P08-1052,P06-2064,0,0.45477,"Missing"
P08-1052,J07-2002,0,0.0179384,"Missing"
P08-1052,W01-0511,1,0.855213,"Missing"
P08-1052,P02-1032,1,0.865229,"Missing"
P08-1052,H05-1047,0,0.00788142,"l similarity problems, including but not limited to question answering, information retrieval, machine translation, word sense disambiguation, and information extraction. For example, a relational search engine like TextRunner, which serves queries like “find all X such that X causes wrinkles”, asking for all entities that are in a particular relation with a given entity (Cafarella et al., 2006), needs to recognize that laugh wrinkles is an instance of CAUSE-EFFECT. While there are not many success stories so far, measuring semantic similarity has proven its advantages for textual entailment (Tatu and Moldovan, 2005). Introduction Despite the tremendous amount of work on word similarity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achie"
P08-1052,N03-1033,0,0.0203025,"n the query changes the set of returned results and their ranking. For each query, we collect the text snippets from the result set (up to 1,000 per query). We split them into sentences, and we filter out all incomplete ones and those that do not contain the target nouns. We further make sure that the word sequence following the second mentioned target noun is nonempty and contains at least one nonnoun, thus ensuring the snippet includes the entire noun phrase: snippets representing incomplete sentences often end with a period anyway. We then perform POS tagging using the Stanford POS tagger (Toutanova et al., 2003) 1 454 POS P V V V V V V V V V V P C V V V V V ... V Direction 2→1 1→2 1→2 2→1 2→1 1→2 1→2 1→2 2→1 1→2 1→2 2→1 1→2 1→2 1→2 2→1 1→2 2→1 ... 2→1 Table 1: The most frequent Web-derived features for committee member. Here V stands for verb (possibly +preposition and/or +particle), P for preposition and C for coordinating conjunction; 1 → 2 means committee precedes the feature and member follows it; 2 → 1 means member precedes the feature and committee follows it. and shallow parsing with the OpenNLP tools2 , and we extract the following types of features: Verb: We extract a verb if the subject NP"
P08-1052,P06-1040,0,0.101838,"rity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, pages 452–460, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 2.1 Related Work Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words"
P08-1052,J06-3003,0,0.213011,"rity (see (Budanitsky and Hirst, 2006) for an overview), there is surprisingly little research on the important related problem of relational similarity – semantic similarity between pairs of words. Students who took the SAT test before 2005 or who ∗ After January 2008 at the Linguistic Modeling Department, Institute for Parallel Processing, Bulgarian Academy of Sciences, nakov@lml.bas.bg In this paper, we introduce a novel linguisticallymotivated Web-based approach to relational similarity, which, despite its simplicity, achieves stateof-the-art performance on a number of problems. Following Turney (2006b), we test our approach on SAT verbal analogy questions and on mapping noun-modifier pairs to abstract relations like TIME, LOCATION and CONTAINER. We further apply it to (1) characterizing noun-noun compounds using abstract linguistic predicates like CAUSE, USE, and FROM, and (2) classifying the relation between pairs of nominals in context. 452 Proceedings of ACL-08: HLT, pages 452–460, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics 2 2.1 Related Work Characterizing Semantic Relations Turney and Littman (2005) characterize the relationship between two words"
P08-1052,C98-1015,0,\N,Missing
P11-1130,baldwin-awab-2006-open,0,0.30741,",ganang)=gnng, and thus, the ratio is 4/6=0.67 (≥ 0.5) for gunung-ganang. However, for aceh-nias, it is 1/4=0.25, and thus (f) is applicable. As an illustration, here are the wordforms we generate for adik-beradiknya/‘his siblings’: adik, adik-beradiknya, adik-beradik nya, adik-beradik, beradiknya, beradik nya, adik nya, and beradik. And for berpelajaran/‘is educated’, we build the list: berpelajaran, pelajaran, pelajar, ajaran, and ajar. Note that the lists do include the original word. To generate the above wordforms, we used two morphological analyzers: a freely available Malay lemmatizer (Baldwin and Awab, 2006), and an inhouse re-implementation of the Indonesian stemmer described in (Adriani et al., 2007). Note that these tools’ objective is to return a single lemma/stem, e.g., they would return adik for adik-beradiknya, and ajar for berpelajaran. However, it was straightforward to modify them to also output the above intermediary wordforms, which the tools were generating internally anyway when looking for the final lemma/stem. Finally, since the two modified analyzers had different strengths and weaknesses, we combined their outputs to increase recall. 3.2 Word-Level Paraphrasing We perform word-l"
P11-1130,J93-2003,0,0.0711408,"ond the capabilities of statistical machine translation systems. Our experiments translating from Malay, whose morphology is mostly derivational, into English show significant improvements over rivaling approaches based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, w"
P11-1130,N06-1003,0,0.434639,"obability that w′ is a good paraphrase of w. Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr(w′ |w) to the first arc, and 1 to each subsequent arc. We calculate the probability Pr(w′ |w) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w′ are aligned to. More precisely, we use pivoting to estimate the probability Pr(w′ |w) as follows: P Pr(w′ |w) = i Pr(w′ |w, ei )Pr(ei |w) 1301 Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assumption that w′ is conditionally independent of w given ei , thus obtaining the following expression: P Pr(w′ |w) = i Pr(w′ |ei )Pr(ei |w) We estimate the probability Pr(ei |w) directly from the word-aligned training bi-text as follows: Pr(ei |w) = P#(w,ei ) j #(w,ej ) where #(x, e) is the number of times the Malay word x is aligned to the English word e. Estimating Pr(w′ |ei ) cannot be done directly since w′ might not be present on the Malay side of the training bi-text, e.g., because it is a multi-token sequence generated by clitic segmentati"
P11-1130,P05-1033,0,0.0391683,"measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with various kinds of morphological wor"
P11-1130,P05-1066,0,0.132449,"Missing"
P11-1130,P08-1115,0,0.391255,"ically simpler than the original word. entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignment"
P11-1130,P10-4002,0,0.0236514,"ove general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as alternative wordforms; it uses no sentence-level or phrase-level paraphrases. 6 We also tried the word segmentation model of Dyer (2009) as implemented in the cdec decoder (Dyer et al., 2010), which learns word segmentation lattices from raw text in an unsupervised manner. Unfortunately, it could not learn meaningful word segmentations for Malay, and thus we do not compare against it. We believe this may be due to its focus on word segmentation, which is of limited use for Malay. sent. 1k 2k 5k 10k 20k 40k 80k 160k 320k system baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases baseline paraphrases 1-gram 59.78 62.23 64.20 66.38 68.12 70.41 70.13 72.04 73.19 73.28 74"
P11-1130,W07-0729,0,0.0984925,"re morphologically simpler than the original word. entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for"
P11-1130,N09-1046,0,0.124504,"the original word. entries would be hard to estimate. For example, the clitics, and even many of the intermediate morphological forms, would not exist as individual words in the training bi-text, which means that there would be no word alignments or lexical probabilities available for them. Another option would be to generate separate word alignments for the original training bi-text and for a version of it where the source (Malay) side has been paraphrased. Then, the two bi-texts and their word alignments would be concatenated and used to build a phrase table (Dyer, 2007; Dyer et al., 2008; Dyer, 2009). This would solve the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both par"
P11-1130,N04-1035,0,0.0273084,"rd tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with various kinds of morphological wordforms, including derivational ones. We demonstrate its potential o"
P11-1130,H05-1085,0,0.0282473,"dup (‘life/living’), respectively. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish a"
P11-1130,N06-2013,0,0.0562633,"to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose s"
P11-1130,D07-1091,0,0.0246854,"T for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead"
P11-1130,E03-1076,0,0.039781,"ers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general line"
P11-1130,N03-1017,0,0.0231554,"based on five automatic evaluation measures (for 320,000 sentence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with vario"
P11-1130,P07-2045,0,0.0131534,"ed with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. 1303 We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’ channel model.6 This is the model of Dyer (2007). It uses 0-1 weights in the lattice and only allows lemmata as"
P11-1130,N04-4015,0,0.0618989,"ne of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word (Habash and Sadat, 2006). For languages with 1305 more or less regular inflectional morphology like Arabic or Turkish, another good idea is to segment words into morpheme sequences, e.g., prefix(es)stem-suffix(es), which can be used instead of the original words (Lee, 2004) or in addition to them. This can be achieved using a lattice input to the translation system (Dyer et al., 2008; Dyer, 2009). Unfortunately, none of these general lines of research suits Malay well, whose compounds are rarely concatenated, clitics are not so frequent, and morphology is mostly derivational, and thus likely to generate words whose semantics substantially differs from the semantics of the original word. Therefore, we cannot expect the existence of equivalence classes: it is only occasionally that two derivationally related wordforms would share the same target language translati"
P11-1130,W10-1754,1,0.877222,"Missing"
P11-1130,D09-1141,1,0.870794,"the problems with the word alignments and the phrase pair probabilities estimations in a principled manner, but it would require choosing for each word only one of the paraphrases available to it, while we would prefer to have a way to allow all options. Moreover, the paraphrased and the original versions of the corpus would be given equal weights, which might not be desirable. Finally, since the two versions of the bitext would be word-aligned separately, there would be no interaction between them, which might lead to missed opportunities for improved alignments in both parts of the bi-text (Nakov and Ng, 2009). We avoid the above issues by adopting a sentencelevel paraphrasing approach. Following the general framework proposed in (Nakov, 2008), we first create multiple paraphrased versions of the sourceside sentences of the training bi-text. Then, each paraphrased source sentence is paired with its original translation. This augmented bi-text is wordaligned and a phrase table T ′ is built from it, which is merged with a phrase table T for the original bitext. The merged table contains all phrase entries from T , and the entries for the phrase pairs from T ′ that are not in T . Following Nakov and N"
P11-1130,J03-1002,0,0.00563438,"used 1,420 sentences with 28.8K Malay word tokens, which were translated by three human translators, yielding translations of 32.8K, 32.4K, and 32.9K English word tokens, respectively. For development, we used 2,000 sentence pairs of 63.4K English and 58.5K Malay word tokens. 4.2 General Experimental Setup First, we tokenized and lowercased all datasets: training, development, and testing. We then built directed word-level alignments for the training bitext for English→Malay and for Malay→English using IBM model 4 (Brown et al., 1993), which we symmetrized using the intersect+grow heuristic (Och and Ney, 2003). Next, we extracted phraselevel translation pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. 1303 We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on t"
P11-1130,P03-1021,0,0.15835,"i-text is wordaligned and a phrase table T ′ is built from it, which is merged with a phrase table T for the original bitext. The merged table contains all phrase entries from T , and the entries for the phrase pairs from T ′ that are not in T . Following Nakov and Ng (2009), we add up to three additional indicator features (taking the values 0.5 and 1) to each entry in the merged phrase table, showing whether the entry came from (1) T only, (2) T ′ only, or (3) both T and T ′ . We also try using the first one or two features only. We set all feature weights using minimum error rate training (Och, 2003), and we optimize their number (one, two, or three) on the development dataset.5 5 In theory, we should re-normalize the probabilities; in practice, this is not strictly required by the log-linear SMT model. 1302 Each of our paraphrased sentences differs from its original sentence by a single word, which prevents combinatorial explosions: on average, we generate 14 paraphrased versions per input sentence. It further ensures that the paraphrased parts of the sentences will not dominate the word alignments or the phrase pairs, and that there would be sufficient interaction at word alignment time"
P11-1130,P02-1040,0,0.09307,"n pairs of maximum length seven, which we scored and used to build a phrase table where each phrase pair is associated with the following five standard feature functions: forward and reverse phrase translation probabilities, forward and reverse lexicalized phrase translation probabilities, and phrase penalty. 1303 We trained a log-linear model using the following standard SMT feature functions: trigram language model probability, word penalty, distance-based distortion cost, and the five feature functions from the phrase table. We set all weights on the development dataset by optimizing BLEU (Papineni et al., 2002) using minimum error rate training (Och, 2003), and we plugged them in a beam search decoder (Koehn et al., 2007) to translate the Malay test sentences to English. Finally, we detokenized the output, and we evaluated it against the three reference translations. 4.3 Systems Using the above general experimental setup, we implemented the following baseline systems: • baseline. This is the default system, which uses no morphological processing. • lemmatize all. This is the second baseline that uses lemmatized versions of the Malay side of the training, development and testing datasets. • ‘noisier’"
P11-1130,P05-1034,0,0.024385,"ntence pairs; 9.5 million English word tokens). 1 Introduction Traditionally, statistical machine translation (SMT) models have assumed that the word should be the basic token-unit of translation, thus ignoring any wordinternal morphological structure. This assumption can be traced back to the first word-based models of IBM (Brown et al., 1993), which were initially proposed for two languages with limited morphology: French and English. While several significantly improved models have been developed since then, including phrase-based (Koehn et al., 2003), hierarchical (Chiang, 2005), treelet (Quirk et al., 2005), and syntactic (Galley et al., 2004) models, they all preserved the assumption that words should be atomic. In this paper, we propose a paraphrase-based approach to translating from a morphologically complex language. Unlike previous research, we focus on the pairwise relationship between morphologically related wordforms, which we treat as potential paraphrases, and which we handle using paraphrasing techniques at various levels: word, phrase, and sentence level. An important advantage of this framework is that it can cope with various kinds of morphological wordforms, including derivational"
P11-1130,2006.amta-papers.25,0,0.110224,"Missing"
P11-1130,P06-1122,0,0.0142902,"em, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006), or like Arabic, Turkish, Finnish, and, to a lesser extent, Spanish and Italian, where clitics often attach to the preceding word ("
P11-1130,P07-1108,0,0.0137854,"araphrase of w. Note that multi-word paraphrases, e.g., resulting from clitic segmentation, are encoded using a sequence of arcs; in such cases, we assign Pr(w′ |w) to the first arc, and 1 to each subsequent arc. We calculate the probability Pr(w′ |w) using the training Malay-English bi-text, which we align at the word level using IBM model 4 (Brown et al., 1993), and we observe which English words w and w′ are aligned to. More precisely, we use pivoting to estimate the probability Pr(w′ |w) as follows: P Pr(w′ |w) = i Pr(w′ |w, ei )Pr(ei |w) 1301 Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007), we make the simplifying assumption that w′ is conditionally independent of w given ei , thus obtaining the following expression: P Pr(w′ |w) = i Pr(w′ |ei )Pr(ei |w) We estimate the probability Pr(ei |w) directly from the word-aligned training bi-text as follows: Pr(ei |w) = P#(w,ei ) j #(w,ej ) where #(x, e) is the number of times the Malay word x is aligned to the English word e. Estimating Pr(w′ |ei ) cannot be done directly since w′ might not be present on the Malay side of the training bi-text, e.g., because it is a multi-token sequence generated by clitic segmentation. Thus, we think o"
P11-1130,E06-1006,0,0.0585011,"an (‘life/existence’) are derivational forms of jalan (‘go’) and hidup (‘life/living’), respectively. Thus, in the paraphrasing system, they were involved in sentence-level paraphrasing, where the alignments were improved. While the wrong phrase pair was still available, the system chose a better one from the paraphrased training bi-text. 6 Related Work Most research in SMT for a morphologically rich source language has focused on inflected forms of the same word. The assumption is that they would have similar semantics and thus could have the same translation. Researchers have used stemming (Yang and Kirchhoff, 2006), lemmatization (Al-Onaizan et al., 1999; Goldwater and McClosky, 2005; Dyer, 2007), or direct clustering (Talbot and Osborne, 2006) to identify such groups of words and use them as equivalence classes or as possible alternatives in translation. Frameworks for the simultaneous use of different word-level representations have been proposed as well (Koehn and Hoang, 2007). A second important line of research has focused on word segmentation, which is useful for languages like German, which are rich in compound words that are spelled concatenated (Koehn and Knight, 2003; Yang and Kirchhoff, 2006)"
P12-2059,J93-2003,0,0.0293793,"rs character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate"
P12-2059,N06-1003,0,0.0452181,"cted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) are estimated using maximum likelihood from MK–EN and EN–BG word alignments. Then, we induced translation probability estimations for the reverse direction Pr(b|m) and we calculated the quantity Piv(m, b) = Pr(m|b) Pr(b|m). We calculated a similar quantity Dir(m, b), where the probabilities Pr(m|b) and Pr(b|m) are estimated using maximum likelihood from the MK–BG bitext directly. Finally, we calculated the similarity score S(m, b)"
P12-2059,P05-1066,0,0.106808,"Missing"
P12-2059,I08-8003,0,0.0091741,"like vreden, which means ‘valuable’ in Macedonian but ‘harmful’ in Bulgarian. Finally, competitive linking helps prevent issues related to word inflection that cannot be handled using the semantic component alone. 3.2 Transliteration Training For each pair in the list of cognate pairs, we added spaces between any two adjacent letters for both words, and we further appended special start and end characters. We split the resulting list into training, development and testing parts and we trained and tuned a character-level MacedonianBulgarian phrase-based monotone SMT system similar to that in (Finch and Sumita, 2008; Tiedemann and Nabende, 2009; Nakov and Ng, 2009; Nakov and Ng, 2012). The system used a character-level Bulgarian language model trained on words. We set the maximum phrase length and the language model order to 10, and we tuned the system using MERT. 3.3 Transliteration Lattice Generation Given a Macedonian sentence, we generated a lattice where each input Macedonian word of length three or longer was augmented with Bulgarian alternatives: n-best transliterations generated by the above character-level Macedonian-Bulgarian SMT system (after the characters were concatenated to form a word and"
P12-2059,N07-1047,0,0.0292559,"s in each tuning step to calculate the usual word-based BLEU score. character bigrams: MK: na av vi is st ti in na a ? ? BG: na ai is st ti in na a ? ? Figure 1: Preparing the training corpus for alignment. Statistical word alignment models heavily rely on context-independent lexical translation parameters and, therefore, are unable to properly distinguish character mapping differences in various contexts. The alignment models used in the transliteration literature have the same problem as they are usually based on edit distance operations and finite-state automata without contextual history (Jiampojamarn et al., 2007; Damper et al., 2005; Ristad and Yianilos, 1998). We, thus, transformed the input to sequences of character n-grams as suggested by Tiedemann (2012); examples are shown in Figure 1. This artificially increases the vocabulary as shown in Table 2, making standard alignment models and their lexical translation parameters more expressive. single characters character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams consti"
P12-2059,N03-1017,0,0.0578816,"be different, and they may diverge more substantially at the level of morphology. However, the differences often constitute consistent regularities that can be generalized when translating. The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation (Vilar et al., 2007; Tiedemann, 2009a) and transliteration (Matthews, 2007). Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages. However, the strength of phrasebased SMT (Koehn et al., 2003) is that it can support rather large sequences (phrases) that capture translations of entire chunks. This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration. Table 1 shows how character-level phrase tables can cover mappings spanning over multi-word units. Thus, character-level phrase-based SMT models combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, as well as to various combinations thereof. 2 Training"
P12-2059,P07-2045,0,0.021785,"01 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spellin"
P12-2059,N03-2016,0,0.052787,"inear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) a"
P12-2059,J00-2004,0,0.0232873,"give equal weights to the translational similarity (the sum of the first two terms) and to the spelling similarity (twice LCSR). We excluded all words of length less than three, as well as all Macedonian-Bulgarian word pairs (m, b) for which Piv(m, b) + Dir(m, b) < 0.01, and those for which LCSR(m, b) was below 0.58, a value found by Kondrak et al. (2003) to work well for a number of European language pairs. Finally, using S(m, b), we induced a weighted bipartite graph, and we performed a greedy approximation to the maximum weighted bipartite matching in that graph using competitive linking (Melamed, 2000), to produce the final list of cognate pairs. Note that the above-described cognate extraction algorithm has three important components: (1) orthographic, based on LCSR, (2) semantic, based on word alignments and pivoting over English, and (3) competitive linking. The orthographic component is essential when looking for cognates since they must have similar spelling by definition, while the semantic component prevents the extraction of false friends like vreden, which means ‘valuable’ in Macedonian but ‘harmful’ in Bulgarian. Finally, competitive linking helps prevent issues related to word in"
P12-2059,D09-1141,1,0.654294,"t ‘harmful’ in Bulgarian. Finally, competitive linking helps prevent issues related to word inflection that cannot be handled using the semantic component alone. 3.2 Transliteration Training For each pair in the list of cognate pairs, we added spaces between any two adjacent letters for both words, and we further appended special start and end characters. We split the resulting list into training, development and testing parts and we trained and tuned a character-level MacedonianBulgarian phrase-based monotone SMT system similar to that in (Finch and Sumita, 2008; Tiedemann and Nabende, 2009; Nakov and Ng, 2009; Nakov and Ng, 2012). The system used a character-level Bulgarian language model trained on words. We set the maximum phrase length and the language model order to 10, and we tuned the system using MERT. 3.3 Transliteration Lattice Generation Given a Macedonian sentence, we generated a lattice where each input Macedonian word of length three or longer was augmented with Bulgarian alternatives: n-best transliterations generated by the above character-level Macedonian-Bulgarian SMT system (after the characters were concatenated to form a word and the special symbols were removed). 303 In the la"
P12-2059,J03-1002,0,0.0216691,"ses the vocabulary as shown in Table 2, making standard alignment models and their lexical translation parameters more expressive. single characters character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a char"
P12-2059,P03-1021,0,0.0418781,"d-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext"
P12-2059,P02-1040,0,0.0953179,"hat bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phrase-based translation. In our experiments, we used GIZA++ (Och and Ney, 2003) with standard settings and the growdiagonal-final-and heuristics to symmetrize the final IBM-model-4-based Viterbi alignments (Brown et al., 1993). The phrases were extracted and scored using the Moses training tools (Koehn et al., 2007).1 We tuned the parameters of the log-linear SMT model using minimum error rate training (Och, 2003), optimizing BLEU (Papineni et al., 2002). 1 Note that the extracted phrase table does not include sequences of character n-grams. We map character n-gram alignments to links between single characters before extraction. 302 3 Transliteration We also built a character-level SMT system for word-level transliteration, which we trained on a list of automatically extracted pairs of likely cognates. 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–E"
P12-2059,2006.amta-papers.25,0,0.137489,"Missing"
P12-2059,2009.eamt-1.3,1,0.799268,"vel. Closely-related languages such as Macedonian and Bulgarian exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities. Spelling conventions in such related languages can still be different, and they may diverge more substantially at the level of morphology. However, the differences often constitute consistent regularities that can be generalized when translating. The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation (Vilar et al., 2007; Tiedemann, 2009a) and transliteration (Matthews, 2007). Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages. However, the strength of phrasebased SMT (Koehn et al., 2003) is that it can support rather large sequences (phrases) that capture translations of entire chunks. This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration. Table 1 shows how character-level phrase tables can cover mappings spanning over multi-word units. Thus, character-level phrase-based SMT models"
P12-2059,E12-1015,1,0.649421,"1: Preparing the training corpus for alignment. Statistical word alignment models heavily rely on context-independent lexical translation parameters and, therefore, are unable to properly distinguish character mapping differences in various contexts. The alignment models used in the transliteration literature have the same problem as they are usually based on edit distance operations and finite-state automata without contextual history (Jiampojamarn et al., 2007; Damper et al., 2005; Ristad and Yianilos, 1998). We, thus, transformed the input to sequences of character n-grams as suggested by Tiedemann (2012); examples are shown in Figure 1. This artificially increases the vocabulary as shown in Table 2, making standard alignment models and their lexical translation parameters more expressive. single characters character bigrams character trigrams words Macedonian 99 1,851 13,794 41,816 Bulgarian 101 1,893 14,305 30,927 Table 2: Vocabulary size of character-level alignment models and the corresponding word-level model. It turns out that bigrams constitute a good compromise between generality and contextual specificity, which yields useful character alignments with good performance in terms of phra"
P12-2059,N07-1061,0,0.059458,"ction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) are estimated using maximum likelihood from MK–EN and EN–BG word alignments. Then, we induced translation probability estimations for the reverse direction Pr(b|m) and we calculated the quantity Piv(m, b) = Pr(m|b) Pr(b|m). We calculated a similar quantity Dir(m, b), where the probabilities Pr(m|b) and Pr(b|m) are estimated using maximum likelihood from the MK–BG bitext directly. Finally, we calculated the similarity score S(m, b) = Piv(m, b)+Dir(m, b)+2×LCSR(m, b), where LCSR"
P12-2059,W07-0705,0,0.507999,"ns below the word level. Closely-related languages such as Macedonian and Bulgarian exhibit a large overlap in their vocabulary and strong syntactic and lexical similarities. Spelling conventions in such related languages can still be different, and they may diverge more substantially at the level of morphology. However, the differences often constitute consistent regularities that can be generalized when translating. The language similarities and the regularities in morphological variation and spelling motivate the use of character-level translation models, which were applied to translation (Vilar et al., 2007; Tiedemann, 2009a) and transliteration (Matthews, 2007). Certainly, translation cannot be adequately modeled as simple transliteration, even for closelyrelated languages. However, the strength of phrasebased SMT (Koehn et al., 2003) is that it can support rather large sequences (phrases) that capture translations of entire chunks. This makes it possible to include mappings that go far beyond the edit-distancebased string operations usually modeled in transliteration. Table 1 shows how character-level phrase tables can cover mappings spanning over multi-word units. Thus, character-level phrase"
P12-2059,P07-1108,0,0.0492959,". 3.1 Cognate Extraction Classic NLP approaches to cognate extraction look for words with similar spelling that co-occur in parallel sentences (Kondrak et al., 2003). Since our Macedonian-Bulgarian bitext (MK–BG) was small, we further used a MK–EN and an EN–BG bitext. First, we induced IBM-model-4 word alignments for MK–EN and EN–BG, from which we extracted four conditional lexical translation probabilities: Pr(m|e) and Pr(e|m) for MK–EN, and Pr(b|e) and Pr(e|b) for EN–BG, where m, e, and b stand for a Macedonian, an English, and a Bulgarian word. Then, following (Callison-Burch et al., 2006; Wu and Wang, 2007; Utiyama and Isahara, 2007), we induced conditional P lexical translation probabilities as Pr(m|b) = e Pr(m|e) Pr(e|b), where Pr(m|e) and Pr(e|b) are estimated using maximum likelihood from MK–EN and EN–BG word alignments. Then, we induced translation probability estimations for the reverse direction Pr(b|m) and we calculated the quantity Piv(m, b) = Pr(m|b) Pr(b|m). We calculated a similar quantity Dir(m, b), where the probabilities Pr(m|b) and Pr(b|m) are estimated using maximum likelihood from the MK–BG bitext directly. Finally, we calculated the similarity score S(m, b) = Piv(m, b)+Dir(m,"
P13-2003,W08-0304,0,0.162532,"8 Lengths Neg Ref 229.0 52.5 48.5 48.70 48.4 48.9 47.6 48.4 47.8 48.6 48.0 48.7 48.0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Larg"
P13-2003,2005.iwslt-1.8,0,0.132545,"Missing"
P13-2003,2011.mtsummit-papers.1,0,0.292832,"3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focuse"
P13-2003,P07-2045,0,0.0178806,"Missing"
P13-2003,N12-1047,0,0.420009,"e of Figure 2: Example reference translation and hyto the affect that the of some is the with ] us our to the affect that the with ] us our of the in baker , the cook , the on and the , the we know , pothesis translations after iterations 1, 3 and 4. has are in the heaven of to the affect that the of weakness of @-@ Ittihad @-@ Al the force , to The last two hypotheses are monsters. Figure 2 shows the translations after iterations 1, 3 and 4; the last two are monsters. The monster at iteration 3 is potentially useful, but that at iteration 4 is clearly unsuitable as a negative example. 1 See (Cherry and Foster, 2012) for details on objectives. Also, using PRO to initialize MERT, as implemented in Moses, yields 46.52 BLEU and monsters, but using MERT to initialize PRO yields 47.55 and no monsters. 2 13 3 Slaying Monsters: Theory Cut-offs. A cut-off is a deterministic rule that filters out pairs that do not comply with some criteria. We experiment with a maximal cut-off on (a) the difference in BLEU+1 scores and (b) the difference in lengths. These are relative cut-offs because they refer to the pair, but absolute cut-offs that apply to each of the elements in the pair are also possible (not explored here)."
P13-2003,D08-1024,0,0.694161,"itable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+1, and they have addressed the problem using more sensible smoothing. We wondered whether the issue could be partially relieved simply by tuning on longer sentences, for w"
P13-2003,N04-1022,0,0.20397,"Missing"
P13-2003,N09-1025,0,0.176433,"47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focused on monsters. Reasons (i) and (ii) arguably also apply to stochastic sampling of differentials (for BLEU+1 or for length), which fails to kill the monsters, m"
P13-2003,C08-1074,0,0.152708,".70 48.4 48.9 47.6 48.4 47.8 48.6 48.0 48.7 48.0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained w"
P13-2003,P11-2031,0,0.0884574,"0 48.7 47.9 48.6 TEST(tune:full) BLEU+1 Avg. for 3 reruns Pos Neg BLEU StdDev 52.2 2.8 47.80 0.052 47.7 42.9 47.59 0.114 47.5 43.6 47.62 0.091 47.8 43.6 47.44 0.070 47.9 43.6 47.48 0.046 47.7 43.1 47.64 0.090 47.8 43.5 47.67 0.096 47.8 43.6 47.65 0.097 Table 4: More fixes to PRO (with random acceptance, no minimum BLEU+1). The (†† ) indicates that random acceptance kills monsters. The asterisk (∗ ) indicates improved stability over random acceptance. The stability of MERT has been improved using regularization (Cer et al., 2008), random restarts (Moore and Quirk, 2008), multiple replications (Clark et al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et"
P13-2003,C12-1121,1,0.824561,"Missing"
P13-2003,P03-1021,0,0.107668,"examples that are comparable to the positive ones. Instead, tuning on long sentences quickly introduces monsters, i.e., corrupted negative examples that are unsuitable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+"
P13-2003,W09-0439,0,0.0977443,"Missing"
P13-2003,P08-2030,0,0.146958,"Missing"
P13-2003,N12-1023,0,0.695369,"Missing"
P13-2003,P12-1002,0,0.292614,"al., 2011), and parameter aggregation (Cettolo et al., 2011). With the emergence of new optimization techniques, there have been studies that compare stability between MIRA–MERT (Chiang et al., 2008; Chiang et al., 2009; Cherry and Foster, 2012), PRO–MERT (Hopkins and May, 2011), MIRA– PRO–MERT (Cherry and Foster, 2012; Gimpel and Smith, 2012; Nakov et al., 2012). Pathological verbosity can be an issue when tuning MERT on recall-oriented metrics such as METEOR (Lavie and Denkowski, 2009; Denkowski and Lavie, 2011). Large variance between the results obtained with MIRA has also been reported (Simianer et al., 2012). However, none of this work has focused on monsters. Reasons (i) and (ii) arguably also apply to stochastic sampling of differentials (for BLEU+1 or for length), which fails to kill the monsters, maybe because it gives them some probability of being selected by design. To alleviate this, we test the above settings with random acceptance. 4.3 Random Acceptance Table 4 shows the results for accepting training pairs for PRO uniformly at random. To eliminate possible biases, we also removed the min=0.05 BLEU+1 selection criterion. Surprisingly, this setup effectively eliminated the monster proble"
P13-2003,W11-2123,0,0.0476342,"Missing"
P13-2003,D11-1125,0,0.655498,"and can cause testing BLEU to drop by several points absolute. We propose several effective ways to address the problem, using length- and BLEU+1based cut-offs, outlier filters, stochastic sampling, and random acceptance. The best of these fixes not only slay and protect against monsters, but also yield higher stability for PRO as well as improved testtime BLEU scores. Thus, we recommend them to anybody using PRO, monsterbeliever or not. 1 2 Monsters, Inc. PRO uses pairwise ranking optimization, where the learning task is to classify pairs of hypotheses into correctly or incorrectly ordered (Hopkins and May, 2011). It searches for a vector of weights w such that higher evaluation metric scores correspond to higher model scores and vice versa. More formally, PRO looks for weights w such that g(i, j) &gt; g(i, j 0 ) ⇔ hw (i, j) &gt; hw (i, j 0 ), where g is a local scoring function (typically, sentencelevel BLEU+1) and hw are the model scores for a given input sentence i and two candidate hypotheses j and j 0 that were obtained using w. If g(i, j) &gt; g(i, j 0 ), we will refer to j and j 0 as the positive and the negative example in the pair. Learning good parameter values requires negative examples that are com"
P13-2003,D07-1080,0,0.412307,"examples that are unsuitable for learning: they are (i) much longer than the respective positive examples and the references, and (ii) have very low BLEU+1 scores compared to the positive examples and in absolute terms. The low BLEU+1 means that PRO effectively has to learn from positive examples only. Once Upon a Time... For years, the standard way to do statistical machine translation parameter tuning has been to use minimum error-rate training, or MERT (Och, 2003). However, as researchers started using models with thousands of parameters, new scalable optimization algorithms such as MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011) have emerged. As these algorithms are relatively new, they are still not quite well understood, and studying their properties is an active area of research. For example, Nakov et al. (2012) have pointed out that PRO tends to generate translations that are consistently shorter than desired. They have blamed this on inadequate smoothing in PRO’s optimization objective, namely sentencelevel BLEU+1, and they have addressed the problem using more sensible smoothing. We wondered whether the issue could be partially relieved simply by tuning on l"
P13-2003,N03-1017,0,0.0496158,"Missing"
P14-1065,W11-2103,0,0.0360688,"herence relations in the source language when generating target-language translations. In this paper, rather than proposing yet another MT evaluation metric, we show that discourse information is complementary to many existing evaluation metrics, and thus should not be ignored. We first design two discourse-aware similarity measures, which use DTs generated by a publiclyavailable discourse parser (Joty et al., 2012); then, we show that they can help improve a number of MT evaluation metrics at the segment- and at the system-level in the context of the WMT11 and the WMT12 metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanis"
P14-1065,N04-1035,0,0.0377299,"ve process at the sentence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, an"
P14-1065,W12-3102,0,0.0260244,"Missing"
P14-1065,W07-0738,1,0.784963,"Missing"
P14-1065,W09-0440,1,0.927179,"Missing"
P14-1065,W11-1211,0,0.0329553,"sed discourse representation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a diffe"
P14-1065,D08-1024,0,0.0729502,"Missing"
P14-1065,P05-1033,0,0.102259,"over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this"
P14-1065,2010.iwslt-papers.10,0,0.0454846,"ial of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR"
P14-1065,D12-1108,0,0.0251193,"ramework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is also within reach, and that SMT systems would benefit from preserving the coherence relations in the source la"
P14-1065,W10-1750,1,0.883167,"Missing"
P14-1065,W11-2107,0,0.0510915,"the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judges 8 20 15 18 498 924 570 708 171 303 207 249 20 31 18 32"
P14-1065,D11-1125,0,0.120375,"et al., 2012). These metrics tasks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield be"
P14-1065,D12-1083,1,0.558373,"is demonstrated by the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus sho"
P14-1065,P02-1040,0,0.0916457,"html SPAN NUC EDU Attribution SPAN Satellite NUC Nucleus SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear"
P14-1065,P13-1048,1,0.222411,"the establishment of a recent workshop dedicated to Discourse in Machine Translation (Webber et al., 2013), collocated with the 2013 annual meeting of the Association of Computational Linguistics. The area of discourse analysis for SMT is still nascent and, to the best of our knowledge, no previous research has attempted to use rhetorical structure for SMT or machine translation evaluation. One possible reason could be the unavailability of accurate discourse parsers. However, this situation is likely to change given the most recent advances in automatic discourse analysis (Joty et al., 2012; Joty et al., 2013). We present experiments in using discourse structure for improving machine translation evaluation. We first design two discourse-aware similarity measures, which use all-subtree kernels to compare discourse parse trees in accordance with the Rhetorical Structure Theory. Then, we show that these measures can help improve a number of existing machine translation evaluation metrics both at the segment- and at the system-level. Rather than proposing a single new metric, we show that discourse information is complementary to the state-of-the-art evaluation metrics, and thus should be taken into ac"
P14-1065,W07-0707,0,0.0299954,"g proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse info"
P14-1065,P05-1034,0,0.0192094,"tence-level. Second, it was purely statistical over words or word sequences and made little to no use of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield bett"
P14-1065,W04-1013,0,0.0173282,"NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 systs ranks sents judge"
P14-1065,2006.amta-papers.25,0,0.149639,"us SPAN EDU Nucleus Attribution NGRAM NUC Satellite Nucleus REL Nucleus the ECB should be lender of the last resort . suggest (a) DT for DR NGRAM (b) DT for DR-LEX Figure 2: Two different DT representations for the highlighted subtree shown in Figure 1b. WMT12 systs ranks sents judges CS - EN DE - EN ES - EN FR - EN 6 16 12 15 1,294 1,427 1,141 1,395 951 975 923 949 45 47 45 44 To complement the set of individual metrics that participated at the WMT12 metrics task, we also computed the scores of other commonlyused evaluation metrics: BLEU (Papineni et al., 2002), NIST (Doddington, 2002), TER (Snover et al., 2006), ROUGE-W (Lin, 2004), and three M ETEOR variants (Denkowski and Lavie, 2011): M ETEOR-ex (exact match), M ETEOR-st (+stemming) and M ETEOR-sy (+synonyms). The uniform linear combination of the previous 7 individual metrics plus the 12 from Asiya-0809 is reported as Asiya-ALL in the experimental section. The individual metrics combined in Asiya-ALL can be naturally categorized according to the type of linguistic information they use to compute the quality scores. We grouped them in the following four families and calculated the uniform linear combination of the metrics in each group:5 WMT11 sy"
P14-1065,W05-0904,0,0.154446,"n campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For example, at WMT12, 12 metrics were compared (Callison-Burch et al., 2012), most of them new. There have been several attempts to incorporate syntactic and semantic linguistic knowledge into MT evaluation. For instance, at the syntactic level, we find metrics that measure the structural similarity between shallow syntactic sequences (Gim´enez and M`arquez, 2007; Popovic and Ney, 2007) or between constituency trees (Liu and Gildea, 2005). In the semantic case, there are metrics that exploit the similarity over named entities and predicate-argument structures (Gim´enez and M`arquez, 2007; Lo et al., 2012). In this work, instead of proposing a new metric, we focus on enriching current MT evaluation metrics with discourse information. Our experiments show that many existing metrics can benefit from additional knowledge about discourse structure. In comparison to the syntactic and semantic extensions of MT metrics, there have been very few attempts to incorporate discourse information so far. One example are the semantics-aware m"
P14-1065,W12-3129,0,0.298188,"e of linguistic information. Although modern SMT systems have switched to a discriminative log-linear framework, which allows for additional sources as features, it is generally hard to incorporate dependencies beyond a small window of adjacent words, thus making it difficult to use linguistically-rich models. Recently, there have been two promising research directions for improving SMT and its evaluation: (a) by using more structured linguistic information, such as syntax (Galley et al., 2004; Quirk et al., 2005), hierarchical structures (Chiang, 2005), and semantic roles (Wu and Fung, 2009; Lo et al., 2012), and (b) by going beyond the sentence-level, e.g., translating at the document level (Hardmeier et al., 2012). 687 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 687–698, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics We believe that the semantic and pragmatic information captured in the form of DTs (i) can help develop discourse-aware SMT systems that produce coherent translations, and (ii) can yield better MT evaluation metrics. While in this work we focus on the latter, we think that the former is"
P14-1065,D07-1080,0,0.0300512,"ks are based on sentence-level evaluation, which arguably can limit the benefits of using global discourse properties. Fortunately, several sentences are long and complex enough to present rich discourse structures connecting their basic clauses. Thus, although limited, this setting is able to demonstrate the potential of discourselevel information for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judg"
P14-1065,2012.amta-papers.20,0,0.0666078,"ion for MT evaluation. Furthermore, sentence-level scoring (i) is compatible with most translation systems, which work on a sentence-bysentence basis, (ii) could be beneficial to modern MT tuning mechanisms such as PRO (Hopkins and May, 2011) and MIRA (Watanabe et al., 2007; Chiang et al., 2008), which also work at the sentence-level, and (iii) could be used for reranking n-best lists of translation hypotheses. 2 A common argument, is that current automatic evaluation metrics such as BLEU are inadequate to capture discourse-related aspects of translation quality (Hardmeier and Federico, 2010; Meyer et al., 2012). Thus, there is consensus that discourseinformed MT evaluation metrics are needed in order to advance research in this direction. Here we suggest some simple ways to create such metrics, and we also show that they yield better correlation with human judgments. The field of automatic evaluation metrics for MT is very active, and new metrics are continuously being proposed, especially in the context of the evaluation campaigns that run as part of the Workshops on Statistical Machine Translation (WMT 2008-2012), and NIST Metrics for Machine Translation Challenge (MetricsMATR), among others. For"
P14-1065,P11-3009,0,0.0426777,"tation structures (DRS) produced by a semantic parser. They calculate the similarity between the MT output and references based on DRS subtree matching, as defined in (Liu and Gildea, 2005), DRS lexical overlap, and DRS morpho-syntactic overlap. However, they could not improve correlation with human judgments, as evaluated on the MetricsMATR dataset. Related Work Addressing discourse-level phenomena in machine translation is relatively new as a research direction. Some recent work has looked at anaphora resolution (Hardmeier and Federico, 2010) and discourse connectives (Cartoni et al., 2011; Meyer, 2011), to mention two examples.1 However, so far the attempts to incorporate discourse-related knowledge in MT have been only moderately successful, at best. 1 We refer the reader to (Hardmeier, 2012) for an in-depth overview of discourse-related research for MT. 688 In order to develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse"
P14-1065,moschitti-basili-2006-tree,0,0.00987264,"of the relation while satellites are supportive ones. Note that the nuclearity and relation labels in the reference translation are also realized in the system translation in (b), but not in (c), which makes (b) a better translation compared to (c), according to our hypothesis. We argue that existing metrics that only use lexical and syntactic information cannot distinguish well between (b) and (c). 3.2 Measuring Similarity A number of metrics have been proposed to measure the similarity between two labeled trees, e.g., Tree Edit Distance (Tai, 1979) and Tree Kernels (Collins and Duffy, 2001; Moschitti and Basili, 2006). Tree kernels (TKs) provide an effective way to integrate arbitrary tree structures in kernelbased machine learning algorithms like SVMs. In the present work, we use the convolution TK defined in (Collins and Duffy, 2001), which efficiently calculates the number of common subtrees in two trees. Note that this kernel was originally designed for syntactic parsing, where the subtrees are subject to the constraint that their nodes are taken with either all or none of the children. This constraint of the TK imposes some limitations on the type of substructures that can be compared. 2 The discourse"
P14-1065,D12-1097,0,0.146948,"o develop a discourse-aware evaluation metric, we first generate discourse trees for the reference and the system-translated sentences using a discourse parser, and then we measure the similarity between the two discourse trees. We describe these two steps below. Compared to the previous work, (i) we use a different discourse representation (RST), (ii) we compare discourse parses using all-subtree kernels (Collins and Duffy, 2001), (iii) we evaluate on much larger datasets, for several language pairs and for multiple metrics, and (iv) we do demonstrate better correlation with human judgments. Wong and Kit (2012) recently proposed an extension of MT metrics with a measure of document-level lexical cohesion (Halliday and Hasan, 1976). Lexical cohesion is achieved using word repetitions and semantically similar words such as synonyms, hypernyms, and hyponyms. For BLEU and TER, they observed improved correlation with human judgments on the MTC4 dataset when linearly interpolating these metrics with their lexical cohesion score. Unlike their work, which measures lexical cohesion at the document-level, here we are concerned with coherence (rhetorical) structure, primarily at the sentence-level. 3 3.1 Gener"
P14-1065,P07-1098,0,0.0569476,"ive partial credit to subtrees that differ in labels but match in their skeletons. More specifically, it uses the tags SPAN and EDU to build the skeleton of the tree, and considers the nuclearity and/or the relation labels as properties, added as children, of these tags. For example, a SPAN has two properties (its nuclearity and its relation), and an EDU has one property (its nuclearity). The words of an EDU are placed under the predefined children NGRAM. In order to allow the tree kernel to find subtree matches at the word level, we include an additional layer of dummy leaves as was done in (Moschitti et al., 2007); not shown in Figure 2, for simplicity. Experimental Setup In our experiments, we used the data available for the WMT12 and the WMT11 metrics shared tasks for translations into English.3 This included the output from the systems that participated in the WMT12 and the WMT11 MT evaluation campaigns, both consisting of 3,003 sentences, for four different language pairs: Czech-English (CS EN ), French-English ( FR - EN), German-English (DE - EN), and Spanish-English (ES - EN); as well as a dataset with the English references. We measured the correlation of the metrics with the human judgments pro"
P14-1065,N09-2004,0,\N,Missing
P14-1065,W13-3300,0,\N,Missing
P15-1078,W10-1750,1,0.923317,"Missing"
P15-1078,P14-1023,0,0.016965,"q. (3), can be rewritten as follows: 4.1 Syntactic vectors. We generate a syntactic vector for each sentence using the Stanford neural parser (Socher et al., 2013a), which generates a 25dimensional vector as a by-product of syntactic parsing using a recursive NN. Below we will refer to these vectors as SYNTAX 25. Semantic vectors. We compose a semantic vector for a given sentence using the average of the embedding vectors for the words it contains (Mitchell and Lapata, 2010). We use pre-trained, fixedlength word embedding vectors produced by (i) GloVe (Pennington et al., 2014), (ii) COMPOSES (Baroni et al., 2014), and (iii) word2vec (Mikolov et al., 2013b). Our primary representation is based on 50dimensional GloVe vectors, trained on Wikipedia 2014+Gigaword 5 (6B tokens), to which below we will refer as W IKI -GW25. Furthermore, we experiment with W IKI GW300, the 300-dimensional GloVe vectors trained on the same data, as well as with the CC300-42B and CC-300-840B, 300-dimensional GloVe vectors trained on 42B and on 840B tokens from Common Crawl. Network Training The negative log likelihood of the training data for the model parameters θ = (W12 , W1r , W2r , wv , b12 , b1r , b2r , bv ) can be written"
P15-1078,W11-2107,0,0.0115221,"100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as will be discussed below, their"
P15-1078,P14-1129,0,0.0611925,"Missing"
P15-1078,W08-0331,0,0.313473,"that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce"
P15-1078,W07-0718,0,0.0607111,"(SMT) parameter tuning, for system comparison, and for assessing the progress during MT system development. The quality of automatic MT evaluation metrics is usually assessed by computing their correlation with human judgments. To that end, quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In"
P15-1078,W07-0738,1,0.52667,"Missing"
P15-1078,W11-2103,0,0.0331433,"r improvements: +1.5 and +2.0 points absolute when adding SYNTAX 25 and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at"
P15-1078,D14-1027,1,0.819258,"Missing"
P15-1078,W12-3102,0,0.0577153,"Missing"
P15-1078,P14-1065,1,0.735686,"Missing"
P15-1078,W14-3352,1,0.565452,"Missing"
P15-1078,P02-1040,0,0.100737,"ans that rivals the state of the art. 1 Introduction Automatic machine translation (MT) evaluation is a necessary step when developing or comparing MT systems. Reference-based MT evaluation, i.e., comparing the system output to one or more human reference translations, is the most common approach. Existing MT evaluation measures typically output an absolute quality score by computing the similarity between the machine and the human translations. In the simplest case, the similarity is computed by counting word n-gram matches between the translation and the reference. This is the case of BLEU (Papineni et al., 2002), which has been the standard for MT evaluation for years. Nonetheless, more recent evaluation measures take into account various aspects of linguistic similarity, and achieve better correlation with human judgments. 1 We do not argue that the pairwise approach is better than the direct estimation of human quality scores. Both approaches have pros and cons; we see them as complementary. 805 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 805–814, c Beijing, China, July 26-31"
P15-1078,2004.tmi-1.8,0,0.0676924,"ased approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it both in terms of the feature representation and of the learning framework. For instance, we integrate several layers of linguistic information, while Duh (2008) only used lexical and POS matches as features. Secondly, we use information about both the reference and the two alternative translations simultaneously in a neural-based learning framework capable of modeling complex interactions between the features. Another related work is that of Kulesza and Shieber (2004), in which lexical and syntactic features, together with other metrics, e.g., BLEU and NIST, are used in an SVM classifier to discriminate good from bad translations. However, their setting is not pairwise comparison, but a classification task to distinguish human- from machineproduced translations. Moreover, in their work, using syntactic features decreased the correlation with human judgments dramatically (although classification accuracy improved), while in our case the effect is positive. In our previous work (Guzm´an et al., 2014a), we introduced a learning framework for the pairwise sett"
P15-1078,D14-1162,0,0.0794828,"h application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3 Learning Task Given two translation h"
P15-1078,W07-0707,0,0.0221202,"Missing"
P15-1078,W05-0904,0,0.150588,"ecause the use of kernels requires that the SVM operate in the much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to repr"
P15-1078,2006.amta-papers.25,0,0.187957,"2 VEC 300, trained on 100B words from Google News. Finally, we use C OMPOSES 400, the 400-dimensional COMPOSES vectors trained on 2.8 billion tokens from ukWaC, the English Wikipedia, and the British National Corpus. 4.2 The main findings of our experiments are shown in Table 1. Section I of Table 1 shows the results for four commonly-used metrics for MT evaluation that compare a translation hypothesis to the reference(s) using primarily lexical information like word and n-gram overlap (even though some allow paraphrases): BLEU, NIST, TER, and M ETEOR (Papineni et al., 2002; Doddington, 2002; Snover et al., 2006; Denkowski and Lavie, 2011). We will refer to the set of these four metrics as 4 METRICS. These metrics are not tuned and achieve Kendall’s τ between 18.5 and 23.5. Section II of Table 1 shows the results for multilayer neural networks trained on vectors from word embeddings only: SYNTAX 25 and W IKI GW25. These networks achieve modest τ values around 10, which should not be surprising: they use very general vector representations and have no access to word or n-gram overlap or to length information, which are very important features to compute similarity against the reference. However, as wi"
P15-1078,W12-3129,0,0.0269611,"much slower dual space. Thus, some simplification is needed to make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our se"
P15-1078,P13-1045,0,0.14241,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W13-2202,0,0.0122239,"and W IKI -GW25, respectively. Finally, adding both yields even further improvements close to τ of 30 (+2.64 τ points), showing that lexical semantics and syntactic representations are complementary. Section IV of Table 1 puts these numbers in perspective: it lists the τ for the top three systems that participated at WMT12, whose scores ranged between 22.9 and 25.4. Tuning and Evaluation Datasets We experiment with datasets of segment-level human rankings of system outputs from the WMT11, WMT12 and WMT13 Metrics shared tasks (Callison-Burch et al., 2011; Callison-Burch et al., 2012; Mach´acˇ ek and Bojar, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and B"
P15-1078,D13-1170,0,0.00357626,"ructured neural embeddings and a neural network learning architecture for MT evaluation is completely novel. This is despite the growing interest in recent years for deep neural nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes bey"
P15-1078,W14-3336,0,0.0290128,"r, 2013). We focus on translating into English, for which the WMT11 and WMT12 datasets can be split by source language: Czech (cs), German (de), Spanish (es), and French (fr); WMT13 also has Russian (ru). 4.3 Evaluation Score We evaluate our metrics in terms of correlation with human judgments measured using Kendall’s τ . We report τ for the individual languages as well as macro-averaged across all languages. Note that there were different versions of τ at WMT over the years. Prior to 2013, WMT used a strict version, which was later relaxed at WMT13 and further revised at WMT14. See (Mach´acˇ ek and Bojar, 2014) for a discussion. Here we use the strict version used at WMT11 and WMT12. 4.4 Experiments and Results Experimental Settings Datasets: We train our neural models on WMT11 and we evaluate them on WMT12. We further use a random subset of 5,000 examples from WMT13 as a validation set to implement early stopping. Early stopping: We train on WMT11 for up to 10,000 epochs, and we calculate Kendall’s τ on the development set after each epoch. We then select the model that achieves the highest τ on the validation set; in case of ties for the best τ , we select the latest epoch that achieved the highes"
P15-1078,W11-2113,0,0.253095,"quality rankings of alternative translations have been created by human judges. It is known that assigning an absolute score to a translation is a difficult task for humans. Hence, ranking-based evaluations, where judges are asked to rank the output of 2 to 5 systems, have been used in recent years, which has yielded much higher inter-annotator agreement (Callison-Burch et al., 2007). These human quality judgments can be used to train automatic metrics. This supervised learning can be oriented to predict absolute scores, e.g., using regression (Albrecht and Hwa, 2008), or rankings (Duh, 2008; Song and Cohn, 2011). A particular case of the latter is used to learn in a pairwise setting, i.e., given a reference and two alternative translations (or hypotheses), the task is to decide which one is better. This setting emulates closely how human judges perform evaluation assessments in reality, and can be used to produce rankings for an arbitrarily large number of hypotheses. In this pairwise setting, the challenge is to learn, from a pair of hypotheses, which are the features that help to discriminate the better from the worse translation. Although the pairwise setting does not produce absolute quality scor"
P15-1078,D12-1097,0,0.0127587,"o make it practical. While there are some solutions in the kernel-based learning framework to alleviate the computational burden, in this paper we explore an entirely different direction. Related Work Contemporary MT evaluation measures have evolved beyond simple lexical matching, and now take into account various aspects of linguistic structures, including synonymy and paraphrasing (Lavie and Denkowski, 2009), syntax (Gim´enez and M`arquez, 2007; Popovi´c and Ney, 2007; Liu and Gildea, 2005), semantics (Gim´enez and M`arquez, 2007; Lo et al., 2012), and even discourse (Comelles et al., 2010; Wong and Kit, 2012; Guzm´an et al., 2014b; Joty et al., 2014). The combination of several of these aspects has led to improved results in metric evaluation campaigns, such as the WMT metrics task (Bojar et al., 2014). In this paper, we present a general framework for learning to rank translations in the pairwise setting, using information from several linguistic representations of the translations and references. This work has connections with the ranking-based approaches for learning to reproduce human judgments of MT quality. In particular, our setting is similar to that of Duh (2008), but differs from it bot"
P15-1078,N13-1090,0,0.0630498,"nets (NNs) and word embeddings with application to a myriad of NLP problems. For example, in SMT we have observed an increased use of neural nets for language modeling (Bengio et al., 2003; Mikolov et al., 2010) as well as for improving the translation model (Devlin et al., 2014; Sutskever et al., 2014). Deep learning has spread beyond language modeling. For example, recursive NNs have been used for syntactic parsing (Socher et al., 2013a) and sentiment analysis (Socher et al., 2013b). The increased use of NNs by the NLP community is in part due to (i) the emergence of tools such as word2vec (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), which have enabled NLP researchers to learn word embeddings, and (ii) unified learning frameworks, e.g., (Collobert et al., 2011), which cover a variety of NLP tasks such as part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. While in this work we make use of widely available pre-computed structured embeddings, the novelty of our work goes beyond the type of information considered as input, and resides on the way it is integrated to a neural network architecture that is inspired by our intuitions about MT evaluation. 3"
P15-1078,W11-0329,0,0.0126675,"Missing"
P15-1078,W14-3302,0,\N,Missing
P15-2113,P07-1098,1,0.451732,"al problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer s"
P15-2113,N10-1145,0,0.0528465,"answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve high"
P15-2113,S15-2047,1,0.84994,"Missing"
P15-2113,S15-2035,0,0.0819635,"1.45 65.57±1.54 76.23±0.45 76.43±0.92 75.05±0.70 75.61±0.63 75.71±0.71 Table 3: Precision, Recall, F1 , Accuracy computed at the comment level; F1,ta and Ata are averaged at the thread level. Precision, Recall, F1 , F1,ta are computed with respect to the good classifier on 5-fold cross-validation (mean±stand. dev.). 4.2 Experimental Setup As in the competition, the results are macroaveraged at class level. The results of the top 3 Our local classifiers are support vector machines systems are reported for comparison: JAIST (Tran (SVM) with C = 1 (Joachims, 1999), logistic et al., 2015), HITSZ (Hou et al., 2015) and regression with a Gaussian prior with variance 10, QCRI (Nicosia et al., 2015), where the latter refers and logistic ordinal regression (McCullagh, 1980). to our old system that we used for the competition. In order to capture long-range sequential depenThe two main observations are (i) using threaddencies, we use a second-order SVMhmm (Yu level features helps significantly; and (ii) the ordiand Joachims, 2008) (with C = 500 and nal regression model, which captures the idea that epsilon = 0.01) and a second-order linear-chain potential lies between good and bad, achieves at CRF, which con"
P15-2113,S15-2036,1,0.620958,"Missing"
P15-2113,D13-1044,1,0.903656,"Missing"
P15-2113,W01-0515,0,0.0237286,"can affect the label of the current answer, this dependency is too loose to have impact on the selection accuracy. In other words, labels should be used together with answers’ content to account for stronger and more effective dependencies. 2 Basic and Thread-Level Features 3.1 Baseline Features We measure lexical and syntactic similarity between q and c. We compute the similarity between word n-grams (n = [1, . . . , 4]), after stopword removal, using greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We also apply partial tree kernels (Moschitti, 2006) on shallow syntactic trees. We designed a set of heuristic features that might suggest whether c is good or not. Forty-four Boolean features express whether c (i) includes URLs or emails (2 feats.); (ii) contains the word “yes”, “sure”, “no”, “can”, “neither”, “okay”, and “sorry”, as well as symbols ‘?’ and ‘@’ (9 feats.); (iii) starts with “yes” (1 feat.); (iv) includes a sequence of three or more repeated characters or a word longer than fifteen characters (2 feats.); (v) belongs to one of the categories of the for"
P15-2113,W13-3509,1,0.800933,"g the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should be adopted to achieve higher accuracy. To test our hypothesis about the usefulness of thread-level information, we used"
P15-2113,D07-1002,0,0.0974152,"a Abstract This is a real problem, as a question can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and"
P15-2113,P08-1082,0,0.0417245,"n can have hundreds of answers, the vast majority of which would not satisfy the users’ information needs. Thus, finding the desired information in a long list of answers might be very time-consuming. The problem of selecting the relevant text passages (i.e., those containing good answers) has been tackled in QA research, either for non-factoid QA or for passage reranking. Usually, automatic classifiers are applied to the answer passages retrieved by a search engine to derive a relative order; see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008; Heilman and Smith, 2010; Wang and Manning, 2010; Severyn and Moschitti, 2012; Yao et al., 2013; Severyn et al., 2013; Severyn and Moschitti, 2013) for detail. To the best of our knowledge, there is no QA work that effectively identifies good answers based on the selection of the other answers retrieved for a question. This is mainly due to the loose dependencies between the different answer passages in standard QA. In contrast, we postulate that in a cQA setting, the answers from different users in a common thread are strongly interconnected and, thus, a joint answer selection model should b"
P15-2113,S15-2038,0,0.16032,"Missing"
P15-2113,C10-1131,0,0.223512,"Missing"
P15-2113,N13-1106,0,0.0861461,"Missing"
P16-2065,P02-1022,0,0.0500412,"eech (POS). We tag using GATE (Cunningham et al., 2011) with a simplified model trained on a transformed version of the BulTreeBank-DP (Simov et al., 2002). For each POS tag type, we take the number of occurrences in the text divided by the total number of tokens. We use both fine-grained and course-grained POS tags, e.g., from the POS tag Npmsi, we generate three tags: Npmsi, N and Np. Named entities. We also use the occurrence of named entities as features. For extracting named entities such as location, country, person name, date unit, etc., we use the lexicons that come with Gate’s ANNIE (Cunningham et al., 2002) pipeline, which we translated to Bulgarian. In future work, we plan to use a better named entity recognizer based on CRF (Georgiev et al., 2009). 5 Experiments and Evaluation We train and evaluate an L2-regularized Logistic Regression with LIBLINEAR (Fan et al., 2008) as implemented in SCIKIT- LEARN (Pedregosa et al., 2011), using scaled and normalized features to the [0;1] interval. As we have perfectly balanced sets of 650 positive and 650 negative examples for paid troll vs. non-trolls and 578 positive and 578 negative examples for mentioned troll vs. non-trolls, the baseline accuracy is 5"
P16-2065,K15-1032,1,0.416421,"r relevant research direction (Rowe and Butters, 2009). Detecting untruthful and deceptive information has been studied using both psychology and computational linguistics (Ott et al., 2011). A related problem is Web spam detection, which has been addressed using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. In our previous work, we focused on finding opinion manipulation troll users (Mihaylov et al., 2015a) and on modeling the behavior of exposed vs. paid trolls (Mihaylov et al., 2015b). Here, we go beyond user profile and we try to detect individual troll vs. non-troll comments in a news community forum based on both text and metadata. 3 Accusation: “To comment from “Prorok Ilia”: I can see that you are a red troll by the words that you are using” Accused troll’s comment: This Boyko3 is always in your mind! You only think of him. We like Boko the Potato (the favorite of the Lamb), the way we like the Karlies. Paid troll’s comment: in the previous protests, the entire country participated, but"
P16-2065,R15-1058,1,0.621543,"r relevant research direction (Rowe and Butters, 2009). Detecting untruthful and deceptive information has been studied using both psychology and computational linguistics (Ott et al., 2011). A related problem is Web spam detection, which has been addressed using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. In our previous work, we focused on finding opinion manipulation troll users (Mihaylov et al., 2015a) and on modeling the behavior of exposed vs. paid trolls (Mihaylov et al., 2015b). Here, we go beyond user profile and we try to detect individual troll vs. non-troll comments in a news community forum based on both text and metadata. 3 Accusation: “To comment from “Prorok Ilia”: I can see that you are a red troll by the words that you are using” Accused troll’s comment: This Boyko3 is always in your mind! You only think of him. We like Boko the Potato (the favorite of the Lamb), the way we like the Karlies. Paid troll’s comment: in the previous protests, the entire country participated, but"
P16-2065,P11-1032,0,0.0110363,"nnotator agreement was substantial: Cohen’s Kappa of 0.82. Moreover, a simple bag-of-words classifier could find these 578 accusations with an F1 -score of 0.85. Here are some examples (translated): Table 1: Statistics about our dataset. Label Paid troll comments Mentioned troll comments Non-troll comments Comments 650 578 650+578 Table 2: Comments selected for experiments. Trustworthiness of statements on the Web is another relevant research direction (Rowe and Butters, 2009). Detecting untruthful and deceptive information has been studied using both psychology and computational linguistics (Ott et al., 2011). A related problem is Web spam detection, which has been addressed using spam keyword spotting (Dave et al., 2003), lexical affinity of arbitrary words to spam content (Hu and Liu, 2004), frequency of punctuation and word co-occurrence (Li et al., 2006). See (Castillo and Davison, 2011) for an overview on adversarial web search. In our previous work, we focused on finding opinion manipulation troll users (Mihaylov et al., 2015a) and on modeling the behavior of exposed vs. paid trolls (Mihaylov et al., 2015b). Here, we go beyond user profile and we try to detect individual troll vs. non-troll"
P16-2065,S14-2103,1,0.840185,"ead. Word2Vec clusters. We trained word2vec on 80M words from 34,514 publications and 1,930,818 comments in our forum, obtaining 268,617 word vectors, which we grouped into 5,372 clusters using K-Means clustering, and then we use these clusters as features. Sentiment. We use features derived from MPQA Subjectivity Lexicon (Wilson et al., 2005) and NRC Emotion Lexicon (Mohammad and Turney, 2013) and the lexicon of Hu and Liu (2004). Originally these lexicons were built for English, but we translated them to Bulgarian using Google Translate. Then, we reused the sentiment analysis pipeline from (Velichkov et al., 2014), which we adapted for Bulgarian. Bad words. We use the number of bad words in the comment as a feature. The words come from the Bad words list v2.0, which contains 458 bad words collected for a filter of forum or IRC channels in English.6 We translated this list to Bulgarian using Google Translate and we removed duplicates to obtain Bad Words Bg 1. We further used the above word2vec model to find the three most similar words for each bad word in Bad Words Bg 1, and we constructed another lexicon: Bad Words Bg 3.7 Finally, we generate two features: one for each lexicon. Mentions. We noted that"
P16-2065,H05-1044,0,0.0138158,"number of words, and the number of ALL CAPS words. Metadata. We use the time of comment posting (worktime: 9:00-19:00h vs. night: 21:00-6:00h), part of the week (workdays: Mon-Fri vs. weekend: Sat-Sun), and the rank of the comment divided by the number of comments in the thread. Word2Vec clusters. We trained word2vec on 80M words from 34,514 publications and 1,930,818 comments in our forum, obtaining 268,617 word vectors, which we grouped into 5,372 clusters using K-Means clustering, and then we use these clusters as features. Sentiment. We use features derived from MPQA Subjectivity Lexicon (Wilson et al., 2005) and NRC Emotion Lexicon (Mohammad and Turney, 2013) and the lexicon of Hu and Liu (2004). Originally these lexicons were built for English, but we translated them to Bulgarian using Google Translate. Then, we reused the sentiment analysis pipeline from (Velichkov et al., 2014), which we adapted for Bulgarian. Bad words. We use the number of bad words in the comment as a feature. The words come from the Bad words list v2.0, which contains 458 bad words collected for a filter of forum or IRC channels in English.6 We translated this list to Bulgarian using Google Translate and we removed duplica"
P16-2065,R09-1022,1,\N,Missing
P16-2065,simov-etal-2002-building,0,\N,Missing
P16-2075,S16-1172,0,0.151144,"Missing"
P16-2075,S16-1138,0,0.225785,"Missing"
P16-2075,P15-1078,1,0.661142,"Missing"
P16-2075,S16-1137,1,0.558982,"Missing"
P16-2075,N16-1153,1,0.859816,"Missing"
P16-2075,P15-2114,0,0.110685,". Furthermore, in MTE we can expect shorter texts, which are typically much more similar. In contrast, in cQA, the question and the intended answers might differ significantly both in terms of length and in lexical content. Thus, it is not clear a priori whether the MTE network can work well to address the cQA problem. Here, we show that the analogy is not only convenient, but also that using it can yield state-of-the-art results for the cQA task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and"
P16-2075,P11-1143,0,0.168069,", 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely abo"
P16-2075,S16-1136,1,0.71134,"Missing"
P16-2075,P03-1003,0,0.0648629,"plied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the"
P16-2075,N13-1090,0,0.12755,"feature sets as ψ(q, c1 ) and ψ(q, c2 ). When including the external features, the activation at the output is f (q, c1 , c2 ) = sig(wvT [φ(q, c1 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). 4 Features We experiment with three kinds of features: (i) input embeddings, (ii) features from MTE (Guzm´an et al., 2015) and (iii) task-specific features from SemEval-2015 Task 3 (Nicosia et al., 2015). A. Embedding Features We used two types of vector-based embeddings to encode the input texts q, c1 and c2 : (1) G OOGLE VECTORS: 300dimensional embedding vectors, trained on 100 billion words from Google News (Mikolov et al., 2013). The encoding of the full text is just the average of the word embeddings. (2) S YNTAX: We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. Also, we compute cosine similarity features with the above vectors: cos(q, c1 ) and cos(q, c2 ). B. MTE features We use the following MTE metrics (MT FEATS), which compare the similarity between the question and a candidate answer: (1) B LEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover"
P16-2075,J11-2003,0,0.0747787,") and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely above the task baselines."
P16-2075,S16-1083,1,0.652512,"Missing"
P16-2075,S15-2036,1,0.343646,"Missing"
P16-2075,S15-2038,0,0.165347,"(Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields results that are largely above the task baselines. Furthermore, by adap"
P16-2075,P15-2116,0,0.0936834,"Missing"
P16-2075,P02-1040,0,0.0977685,"in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better, which is appropriate for a ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships between them; (iii) it uses a number of machine translation evaluation measures that have not been explored for the cQA task before, e.g., T ER (Snover et al., 2006), M ETEOR (Lavie and Denkowski, 2009), and B LEU (Papineni et al., 2002). The analogy we apply to adapt the neural MTE architecture to the cQA problem is the following: given two comments c1 and c2 from the question thread—which play the role of the two competing translation hypotheses—we have to decide whether c1 is a better answer than c2 to question q—which plays the role of the translation reference. If we have a function f (q, c1 , c2 ) to make this decision, then we can rank the finite list of comments in the thread by comparing all possible pairs and by accumulating for each comment the scores for it given by f . From a general point of view, MTE and the cQ"
P16-2075,P15-1025,0,0.0694515,"answer to the question. Furthermore, in MTE we can expect shorter texts, which are typically much more similar. In contrast, in cQA, the question and the intended answers might differ significantly both in terms of length and in lexical content. Thus, it is not clear a priori whether the MTE network can work well to address the cQA problem. Here, we show that the analogy is not only convenient, but also that using it can yield state-of-the-art results for the cQA task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Rie"
P16-2075,P07-1059,0,0.0901484,"015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Most of these papers concentrate on providing advanced neural architectures in order to better model the problem at hand. However, our goal here is different: we extend and reuse an existing pairwise NN framework from a different but related problem. There is also work that uses machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015) e.g., a variation of IBM model 1, to compute the probability that the question is a possible “translation” of the candidate answer. Unlike that work, here we port an entire MTE framework to the cQA problem. A preliminary version of this work was presented in (Guzm´an et al., 2016). To validate our intuition, we present series of experiments using the publicly available SemEval2016 Task 3 datasets, with focus on subtask A. We show that a na¨ıve application of the MTE architecture and features on the cQA task already yields resu"
P16-2075,2006.amta-papers.25,0,0.555241,"ural network is interesting for the cQA problem because: (i) it works in a pairwise fashion, i.e., given two translation hypotheses and a reference translation to compare to, the network decides which translation hypothesis is better, which is appropriate for a ranking problem; (ii) it allows for an easy incorporation of rich syntactic and semantic embedded representations of the input texts, and it efficiently models complex non-linear relationships between them; (iii) it uses a number of machine translation evaluation measures that have not been explored for the cQA task before, e.g., T ER (Snover et al., 2006), M ETEOR (Lavie and Denkowski, 2009), and B LEU (Papineni et al., 2002). The analogy we apply to adapt the neural MTE architecture to the cQA problem is the following: given two comments c1 and c2 from the question thread—which play the role of the two competing translation hypotheses—we have to decide whether c1 is a better answer than c2 to question q—which plays the role of the translation reference. If we have a function f (q, c1 , c2 ) to make this decision, then we can rank the finite list of comments in the thread by comparing all possible pairs and by accumulating for each comment the"
P16-2075,P13-1045,0,0.0205596,"). 4 Features We experiment with three kinds of features: (i) input embeddings, (ii) features from MTE (Guzm´an et al., 2015) and (iii) task-specific features from SemEval-2015 Task 3 (Nicosia et al., 2015). A. Embedding Features We used two types of vector-based embeddings to encode the input texts q, c1 and c2 : (1) G OOGLE VECTORS: 300dimensional embedding vectors, trained on 100 billion words from Google News (Mikolov et al., 2013). The encoding of the full text is just the average of the word embeddings. (2) S YNTAX: We parse the entire question/comment using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. Also, we compute cosine similarity features with the above vectors: cos(q, c1 ) and cos(q, c2 ). B. MTE features We use the following MTE metrics (MT FEATS), which compare the similarity between the question and a candidate answer: (1) B LEU (Papineni et al., 2002); (2) NIST (Doddington, 2002); (3) TER v0.7.25 (Snover et al., 2006). (4) M ETEOR v1.4 (Lavie and Denkowski, 2009) with paraphrases; (5) Unigram P RECISION; (6) Unigram R ECALL. BLEU COMP. We further use as features various components"
Q19-1008,C18-1250,0,0.0423055,"Missing"
Q19-1008,N18-1150,0,0.0549137,"Missing"
Q19-1008,P18-1063,0,0.0327679,"Missing"
Q19-1008,D14-1179,0,0.0313026,"Missing"
Q19-1008,P82-1020,0,0.722017,"Missing"
Q19-1008,N18-1158,0,0.0398823,"Missing"
Q19-1008,N03-1020,0,0.272513,"ointer-generator network architecture to be a good testbed for experiments with a new RNN unit because it enables both abstractive and extractive summarization. We adopt the model from See et al. (2017) as our LEAD baseline. This model uses a bi-directional LSTM encoder (400 steps) with attention distribution and an LSTM decoder (100 steps for training and 120 steps for testing), with all hidden states being 256-dimensional, and 128-dimensional word embeddings trained from scratch during training. For training, we use the cross-entropy loss for the seq2seq model. For evaluation, we use ROUGE (Lin and Hovy, 2003). We also allow the coverage mechanism proposed in the original paper, which penalizes repetitions and improves the quality of the summaries (marked as ‘‘cov.’’ in Table 4). Following the original paper, we train LEAD for 270k iterations and we turn on the coverage for about 3k iterations at the end to get LEAD cov. We use an Adagrad optimizer with a learning rate of 0.15, an accumulator value of 0.1, and a batch size of 16. For decoding, we use a beam of size 4. The only component in LEAD that our proposed models change is the type of the RNN unit for the Model 1 2 3 4 5 6 7 8 9 10 11 12 13 1"
Q19-1008,D15-1166,0,0.0154989,"ext (at the word and at the sentence level), and then we concatenate the story and the question. For the word level, we embed the words into dense vectors, and we feed them into the RNN. Hence, the input sequence can be labeled as (s) (s) (q ) (q ) {x1 , . . . , xn , x1 , . . . , xm }, where the story has n words and the question has m words. For the sentence level, we generate sentence embeddings by averaging word embeddings. Thus, the input sequence for a story with n sentences (s) (s) is {x1 , . . . , xn , x(q) }. Attention mechanism for sentence level. We use simple dot-product attention (Luong et al., 2015): ( s) {pt }0≤t≤n := softmax({h(q) · ht }0≤t≤n ). The Pn (s) context vector c := t=0 pt ht is then passed, together with the query vector, to a dense layer. Models. We compare uRNN, EURNN, LSTM, GRU, GORU, and RUM (with η = N/A in all experiments). The RNN model outputs the prediction at the end of the question through a softmax layer. We use a batch size of 32 for all 20 subtasks. We train the model using Adam optimizer with a learning rate of 0.001 (Kingma and Ba, 2015). All embeddings (word and sentence) are 64-dimensional. For each subset, we train until convergence on the dev set, without"
Q19-1008,E17-1001,0,0.0701759,"Missing"
Q19-1008,J93-2004,0,0.0642854,"ne 14) outperforms all models except for attnLSTM. Furthermore, LSTM and GRU benefit the most from adding attention (lines 10–11), while the phase-coded models (lines 9, 12–15) obtain only a small boost in performance or even a decrease (e.g., in line 13). Although RUM (line 14) shares the best accuracy with LSTM (line 10), we hypothesize that a ‘‘phaseinspired’’ attention might further boost RUM’s performance.5 Language modeling [character-level] (D) is an important testbed for RNNs (Graves, 2013). Data. The Penn Treebank (PTB) corpus is a collection of articles from The Wall Street Journal (Marcus et al., 1993), with a vocabulary of 10k words (using 50 different characters). We use a train/dev/test split of 5.1M/400k/450k tokens, and we replace rare words with <unk&gt;. We feed 150 tokens at a time, and we use a batch size of 128. Models. We incorporate RUM into a recent high-level model: Fast-Slow RNN (FS-RNN) (Mujika et al., 2017). The FS-RNN-k architecture 5 RUM’s associative memory, Equation (2), is similar to attention because it accumulates phase (i.e., forms a context). We plan to investigate phase-coded attention in future work. 128 consists of two hierarchical layers: one of them is a ‘‘fast’’"
Q19-1008,P17-1099,0,0.0919691,"Missing"
Q19-1008,J02-4002,0,0.00758407,"follow the Science Daily-style of reporting. The pointergenerator framework also allows for copying scientific terminology, which allows it to handle simultaneously domains ranging from computer science, to physics, to medicine. Interestingly, the words cancer and diseases are not mentioned in the research paper’s title or abstract, not even on the entire first page; yet, our models manage to extract them. See a demo and more examples in the link at footnote 1. 6 http://www.sciencedaily.com/releases/ 2017/07/170724142035.htm. 7 Other summarization work preserved the original scientific style (Teufel and Moens, 2002; Nikolov et al., 2018). 8 As the full text for research papers is typically only available in PDF format (sometimes also in HTML and/or XML), it is generally hard to convert to text format. Thus, we focus on publications by just a few well-known publishers, which cover a sizable proportion of the research papers discussed in Science Daily, and for which we developed parsers: American Association for the Advancement of Science (AAAS), Elsevier, Public Library of Science (PLOS), Proceedings of the National Academy of Sciences (PNAS), Springer, and Wiley. Ultimately, we ended up with 50,308 full"
R09-1022,D08-1030,0,0.0545769,"Missing"
R09-1022,M98-1001,0,0.0436201,"Missing"
R09-1022,W99-0613,0,0.0554257,"Missing"
R09-1022,W09-3538,0,0.0469039,"Missing"
R09-1022,P98-2206,0,0.109459,"Missing"
R09-1022,W03-0430,0,0.151189,"Missing"
R09-1022,N04-1043,0,0.0688611,"Missing"
R09-1022,popov-etal-2004-creation,0,0.37681,"Missing"
R09-1022,W95-0107,0,0.0643128,"Missing"
R09-1022,W02-2024,0,0.17217,"Missing"
R09-1022,W00-1219,0,0.0559088,"Missing"
R09-1022,J92-4003,0,\N,Missing
R09-1022,P02-1060,0,\N,Missing
R09-1022,C98-2201,0,\N,Missing
R09-1022,W03-0419,0,\N,Missing
R09-1022,P02-1062,0,\N,Missing
R09-1054,P07-1083,0,0.0276899,"Missing"
R09-1054,J93-2003,0,0.0438312,"Missing"
R09-1054,W02-0908,0,0.0455816,"Missing"
R09-1054,N04-1016,0,0.0706882,"Missing"
R09-1054,J93-1004,0,0.491767,"Missing"
R09-1054,N07-1045,0,0.0274462,"Missing"
R09-1054,J03-3005,0,0.109269,"Missing"
R09-1054,J03-3001,0,0.112283,"Missing"
R09-1054,W02-0902,0,0.0852312,"Missing"
R09-1054,P98-2127,0,0.27581,"Missing"
R09-1054,N01-1020,0,0.0863608,"Missing"
R09-1054,J99-1003,0,0.104448,"Missing"
R09-1054,mulloni-pekar-2006-automatic,0,0.0348167,"Missing"
R09-1054,J03-1002,0,0.00751101,"Missing"
R09-1054,C04-1031,0,0.0554894,"Missing"
R09-1054,C98-2122,0,\N,Missing
R09-1065,P04-1035,0,0.170884,"Missing"
R09-1065,W02-1011,0,0.0178689,"external resources. Keywords Sentiment analysis, subjectivity identification, polarity classification, text categorization. 1 Introduction Recently, there has been growing research interest in determining the polarity, positive or negative, of the author’s opinion on a specific topic in natural language texts. Such analysis has various potential applications ranging from components for web sites to business and government intelligence [6]. Previous research on document sentiment classification has shown that machine learning based classifiers perform much better compared to rule-based systems [7]. However, the task remains challenging since opinions are typically expressed in a specific manner, using many rare words and language expressions. As previous research has shown [8], even words with a single occurrence on training can turn out to be good predictors on testing. As a result, the classification accuracy for sentiment analysis using machine learning approaches tends to be much lower compared to that for other text classification tasks like topic identification. ∗ Also: Department of Computer Science, National University of Singapore, 13 Computing Drive, Singapore 117417, nakov@c"
R09-1065,J04-3002,0,0.709292,"Missing"
R11-1045,N04-1016,0,0.0254641,"using weights that are tuned separately for each individual relation. While semantic relations can hold between different parts of speech, e.g., between a verb and a noun, we focus on relations between nominals.2 The most relevant related publication is that of ´ S´eaghdha and Copestake (2009), who combine O attributional and relational features using kernels. However, they are interested in a special kind of relations: between the nouns in a noun-noun compounds like steel knife. Moreover, they use the British National Corpus instead of the Web, which is known to cause data sparseness issues (Lapata and Keller, 2004), they do not focus on linguistically motivated relational features such as verbs and prepositions explicitly, they use co-hyponyms but not hypernyms to generalize the relation arguments, and they give equal weights to the similarities between heads and between modifiers. The remainder of the paper is organized as follows: Section 2 introduces our Web mining methods for argument and relation modeling, Section 3 presents our experimental setup, Section 4 discusses the results, and Section 5 concludes and points to some directions for future work. 2 Method 2.1 Overview As we said above, we combi"
R11-1045,P08-1027,0,0.0188986,"attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approaches is that they can benefit f"
R11-1045,P08-1052,1,0.911974,"chmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approaches is that they can benefit from many preexisting lexical resources"
R11-1045,S07-1003,1,0.900621,"Missing"
R11-1045,E09-1071,0,0.129319,"Missing"
R11-1045,C92-2082,0,0.444973,"example is the task of extracting semantic relations between nominals from text, which has attracted a lot of research attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are mor"
R11-1045,N09-2060,0,0.0412172,"Missing"
R11-1045,S10-1006,1,0.887905,"Missing"
R11-1045,P06-1015,0,0.163697,"has attracted a lot of research attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approach"
R11-1045,D09-1099,1,0.854304,"g verbs and prepositions and the coordinating conjunctions are extracted from the Web, and there is a frequency of extraction associated with each of them, which we incorporate into a relational vector and use to measure relational similarity between training and testing examples. 2.2 Argument Modeling We model the arguments using a distribution over Web-derived hypernyms and co-hyponyms. Multiple knowledge harvesting procedures have been proposed in the literature for the automatic acquisition of hyponyms (Hearst, 1992; Pas¸ca, 2007; Kozareva et al., 2008) and hypernyms (Ritter et al., 2009; Hovy et al., 2009). While we could have used any of them for our experiments, we chose the method of Kozareva et al. (2008), which (i) can extract hypernyms and hyponyms simultaneously, (ii) has been shown to achieve higher accuracy than the methods described in (Pas¸ca, 2007; Ritter et al., 2009), and also (iii) is easy to implement. It uses a doublyanchored pattern (DAP) of the following general form: Hyp./co-hyp. for arg. 1/2 cohyp arg1:tea hyper arg1:beverage hyper arg1:drink hyper arg1:item hyper arg1:product cohyp arg1:chocolate cohyp arg1:cocoa cohyp arg1:soda hyper arg1:crop hyper arg1:food cohyp arg1:s"
R11-1045,P02-1032,0,0.034789,"wo general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advantage of argument modeling approaches is that they can benefit from many preexisting lexical resources. For example, systems using WordNet (Fellbaum, 1998) had sizable performance gains for SemEval-1 Task 4. However, this advantage was mainly due to manually annotated WordNet senses for the relation arguments being provided for this task. There was a restricted track where using them was not allowed"
R11-1045,P06-2064,0,0.0182905,"inals from text, which has attracted a lot of research attention following the creation of two benchmark datasets as part of SemEval-1 Task 4 (Girju et al., 2007) and SemEval-2 Task 8 (Hendrickx et al., 2010). 323 Proceedings of Recent Advances in Natural Language Processing, pages 323–330, Hissar, Bulgaria, 12-14 September 2011. Going back to semantic relations, there is a similar split between two general lines of research. The first one learns the relation directly, e.g., using suitable patterns that can connect the arguments (Hearst, 1992; Turney and Littman, 2005; Nakov and Hearst, 2006; Kim and Baldwin, 2006; Pantel and Pennacchiotti, 2006; Davidov and Rappoport, 2008; Nakov, 2008b; Nakov and Hearst, 2008; Katrenko et al., 2010). This is useful for context-dependent relations like C AUSE -E FFECT, which are dynamic and often episodic in nature, e.g., My Friday’s exam causes me anxiety. The second line focuses on the arguments, e.g., by generalizing them over a lexical hierarchy (Rosario et al., 2002; Girju et al., 2005; Kim and Baldwin, ´ S´eaghdha, 2009). This works well for 2007; O relations like PART-W HOLE, which are more permanent and context-independent, e.g., door-car. An important advanta"
R11-1045,N03-1033,0,0.00552802,"“noun2 THAT? * noun1” where noun1 and noun2 are inflected variants of the head nouns in the relation arguments, THAT? stands for that, which, who or the empty string, and * stands for up to eight instances3 of the search engine’s star operator. We instantiate these generalized patterns and we submit them to Google as exact phrase queries. We then collect the snippets for all returned results (up to 1,000). We split the extracted snippets into sentences, and we filter out all incomplete ones and those that do not contain the target nouns. We POS tag the sentences using the Stanford POS tagger (Toutanova et al., 2003) and we make sure that the word sequence following the second mentioned target noun is non-empty and contains at least one non-noun, i.e., that the snippet includes the entire noun phrase of the second noun in the pattern instantiation. This is because we want the second noun in the pattern instantiation to be the head of an NP: if the NP in incomplete, the second noun could be a modifier in that partial NP. “sem-class such as term1 and term2 ” where sem-class stands for a semantic class, and term1 and term2 are members of this class. In our experiments, we use the following twoplaceholder for"
R11-1045,P08-1119,1,0.831503,"coffee of the guy who makes coffee. Again, the paraphrasing verbs and prepositions and the coordinating conjunctions are extracted from the Web, and there is a frequency of extraction associated with each of them, which we incorporate into a relational vector and use to measure relational similarity between training and testing examples. 2.2 Argument Modeling We model the arguments using a distribution over Web-derived hypernyms and co-hyponyms. Multiple knowledge harvesting procedures have been proposed in the literature for the automatic acquisition of hyponyms (Hearst, 1992; Pas¸ca, 2007; Kozareva et al., 2008) and hypernyms (Ritter et al., 2009; Hovy et al., 2009). While we could have used any of them for our experiments, we chose the method of Kozareva et al. (2008), which (i) can extract hypernyms and hyponyms simultaneously, (ii) has been shown to achieve higher accuracy than the methods described in (Pas¸ca, 2007; Ritter et al., 2009), and also (iii) is easy to implement. It uses a doublyanchored pattern (DAP) of the following general form: Hyp./co-hyp. for arg. 1/2 cohyp arg1:tea hyper arg1:beverage hyper arg1:drink hyper arg1:item hyper arg1:product cohyp arg1:chocolate cohyp arg1:cocoa cohyp"
R11-1045,J06-3003,0,0.0288624,"of the arguments, which we use in instance-based classifiers. The evaluation on the dataset of SemEval-1 Task 4 shows an improvement over the state-ofthe-art for the case where using manually annotated WordNet senses is not allowed. 1 The task of identifying semantic relations in text is complicated by their heterogeneous nature. Thus, it is often addressed using non-parametric instance-based classifiers like the k nearest neighbors (kNN), which effectively reduce it to measuring the relational similarity between a testing and each of the training examples. The latter is studied in detail by Turney (2006), who distinguishes between attributional similarity or correspondence between attributes, and relational similarity or correspondence between relations. Attributional similarity is interested in the similarity between two words (or nominals, noun phrases), A and B. In contrast, relational similarity focuses on the relationship between two pairs of words (or nominals, noun phrases), i.e., it asks how similar the relations A:B and C:D are. Measuring relational similarity directly is hard, and thus it is rarely done directly. Instead, relational similarity is typically modeled as a function of t"
R13-1066,C04-1072,0,0.0239716,"nces are often available for the English (target) side of the tuning and the evaluation dataset, but not for the source language, e.g., Arabic, Chinese. 1 One could hire translators, but this would be costly. 504 Proceedings of Recent Advances in Natural Language Processing, pages 504–510, Hissar, Bulgaria, 7-13 September 2013. 3 3.1 Method We will do the selection with respect to some English reference, e.g., backtranslation of the Arabic reference generated by our own system or by Google translate. Below, we present the similarity measures that we use for the selection. BLEU+1 (B1). BLEU+1 (Lin and Och, 2004) is a smoothed version of BLEU (Papineni et al., 2002) used to address sparseness problems with n-gram matches when comparing sentences. BLEU+1 BP smooth (B1-BP). The BLEU+1 approximation of BLEU smooths the n-gram counts but not the brevity penalty, thus destroying the balance between the two; it also assigns a non-zero precision to cases with zero matches. Thus, we experiment with a version of BLEU+1 from (Nakov et al., 2012) that smooths the brevity penalty and also uses a “grounding” factor. BLEU+1 Sigmoid LP (B1-SG). Note that the brevity penalty of BLEU/BLEU+1 penalizes shorter but not l"
R13-1066,D12-1037,0,0.0156699,"inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the available resources for training, tuning and evaluation. These resources are hard to ob"
R13-1066,D11-1033,0,0.021347,"given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus crit"
R13-1066,D09-1074,0,0.0212759,"s of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the avai"
R13-1066,W09-0439,0,0.0212717,"-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.991 30.03 0.983 30.14 0.986 AVG, no self BLEU len 30.30 1.020 30.18 0.993 30.14 0.990 29.36 0.981 29.32 0.982 Table 2: Tuning and testing on MT04. We tune on the English input in the first column, then we translate all MT04x inputs. We report BLEU and hyp/ref length ratios averaged over (a) all MT04 datasets, and (b) all but the one used for tuning. Table 1 shows the results when tuning on MT04 and testing on MT05. There are several interesting observations we can make. First, the choice of test dataset has a"
R13-1066,D10-1044,0,0.0220994,"on, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SM"
R13-1066,E06-1005,0,0.0303122,"ields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions"
R13-1066,P10-2041,0,0.0333882,"multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical m"
R13-1066,W11-2123,0,0.0332866,"we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.991 30.03 0.983 30.14 0.986 AVG, no self BLEU len 30.30 1.020 30.18 0.993 30.14 0.990 29.36 0.981 29.32 0.982 Table 2: Tuning and testing"
R13-1066,C12-1121,1,0.90772,"Missing"
R13-1066,2005.eamt-1.19,1,0.791718,"nd Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at tuning time of multiple input versions of the same tuning sentence and a single reference translation for it. Previous English–Arabic SMT has used the first input (AlHaj and Lavie, 2012; Kholy and Habash, 2012). Introduction Nowadays, statistical machine translation (SMT) systems are data-driven, and thus critically depend on the available resources for training, tuning and evalua"
R13-1066,2001.mtsummit-papers.46,0,0.0699721,"puts: (a) select one of the datasets, (b) select the best input for each sentence, and (c) synthesize an input for each sentence by fusing the available inputs. Surprisingly, we find out that it is best to tune on the hardest available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et"
R13-1066,W12-5611,0,0.0298896,"Missing"
R13-1066,P02-1040,0,0.092353,"side of the tuning and the evaluation dataset, but not for the source language, e.g., Arabic, Chinese. 1 One could hire translators, but this would be costly. 504 Proceedings of Recent Advances in Natural Language Processing, pages 504–510, Hissar, Bulgaria, 7-13 September 2013. 3 3.1 Method We will do the selection with respect to some English reference, e.g., backtranslation of the Arabic reference generated by our own system or by Google translate. Below, we present the similarity measures that we use for the selection. BLEU+1 (B1). BLEU+1 (Lin and Och, 2004) is a smoothed version of BLEU (Papineni et al., 2002) used to address sparseness problems with n-gram matches when comparing sentences. BLEU+1 BP smooth (B1-BP). The BLEU+1 approximation of BLEU smooths the n-gram counts but not the brevity penalty, thus destroying the balance between the two; it also assigns a non-zero precision to cases with zero matches. Thus, we experiment with a version of BLEU+1 from (Nakov et al., 2012) that smooths the brevity penalty and also uses a “grounding” factor. BLEU+1 Sigmoid LP (B1-SG). Note that the brevity penalty of BLEU/BLEU+1 penalizes shorter but not longer sentences. Thus, we also experiment with a versi"
R13-1066,N03-1017,0,0.023922,".001 0.51 MT054 BLEU len 35.46 0.988 35.31 0.972 35.12 0.970 34.81 0.960 34.82 0.965 35.15 0.973 0.65 AVERAGE BLEU len 34.24 0.989 34.12 0.972 33.95 0.970 33.74 0.960 33.67 0.964 34.03 0.974 0.57 Table 1: Tuning on MT04 and testing on MT05. Shown are BLEU scores and hypothesis/reference length ratios. The best and the worst BLEU scores for each test MT05 dataset are in bold and stroke out, respectively; the last row shows the absolute difference between them. 4 Experiments and Evaluation 4.1 4.2 Experimental Setup TEST ⇒ TUNE ⇓ MT040 MT041 MT042 MT043 MT044 We used the phrase-based SMT model (Koehn et al., 2003), as implemented in the Moses toolkit (Koehn et al., 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arab"
R13-1066,P08-2030,0,0.123937,", 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring"
R13-1066,2005.iwslt-1.8,0,0.156577,"tem (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et al., 2008), fixing automatically all wrong instances of alef, ta marbuta and alef maqsura. We segmented the Arabic words by splitting out conjunctions (MADA scheme D1). For English, we converted all words to lowercase. We built our phrase tables using the standard Moses pipeline with max-phrase-length 7 and Kneser-Ney smoothing. We also built a lexicalized reordering model (Koehn et al., 2005): msd-bidirectional-fe. We used a 5-gram language model trained on the GigaWord v.5 with KneserNey smoothing using KenLM (Heafield, 2011). For optimization, we used MERT. For evaluation, we used NIST’s BLEU scoring tool v13a, which we ran on a desegmented Arabic output, where conjunctions are attached to the following word. In order to ensure stability, we performed three reruns of MERT for each experiment, and we report evaluation results averaged over the three reruns, as suggested by Foster and Kuhn (2009). 2 Tuning on MT04, testing on MT05 AVERAGE BLEU len 29.41 1.014 30.13 0.993 30.07 0.9"
R13-1066,E09-1082,0,0.0210994,"t available input, not on the one that yields the highest BLEU score. This finding has implications on how to pick good translators and how to select useful data for parameter optimization in SMT. 1 2 Related Work One relevant line of research is on multi-source translation, which generates a single translation given multiple versions of the input. This line was started by Och and Ney (2001), who translated the different inputs in isolation and then selected one of them. It has been further extended with various strategies for generating a consensus translation by combining either the inputs (Schroeder et al., 2009) or the outputs (Matusov et al., 2006) of the SMT system. In contrast, we assume having multiple sources at tuning but not at testing time. A related line focused on data selection. For training data, this includes filtering (Moore and Lewis, 2010; Foster et al., 2010), instanceweighting (Axelrod et al., 2011; Matsoukas et al., 2009) and model adaptation (Hildebrand et al., 2005). For tuning data, Liu et al. (2012) built a separate tuning dataset for each test sentence, which is too costly for real-world translation. To the best of our knowledge, ours is the first attempt to make best use at t"
R13-1066,P07-2045,0,0.00956357,"70 34.81 0.960 34.82 0.965 35.15 0.973 0.65 AVERAGE BLEU len 34.24 0.989 34.12 0.972 33.95 0.970 33.74 0.960 33.67 0.964 34.03 0.974 0.57 Table 1: Tuning on MT04 and testing on MT05. Shown are BLEU scores and hypothesis/reference length ratios. The best and the worst BLEU scores for each test MT05 dataset are in bold and stroke out, respectively; the last row shows the absolute difference between them. 4 Experiments and Evaluation 4.1 4.2 Experimental Setup TEST ⇒ TUNE ⇓ MT040 MT041 MT042 MT043 MT044 We used the phrase-based SMT model (Koehn et al., 2003), as implemented in the Moses toolkit (Koehn et al., 2007), to train an SMT system translating from English to Arabic. For tuning and evaluation, we used two multireference datasets, MT04 and MT05, from the NIST 2012 OpenMT Evaluation,2 each with a single Arabic input and five English reference translations, which we inverted, ending up with five English inputs and one Arabic reference for each one. We trained the English-Arabic system (translation, reordering, and language models) on all training data from NIST 2012 except for UN data. Following Kholy and Habash (2012), we normalized the Arabic training, development and test data using MADA (Roth et"
R13-1066,2006.amta-papers.25,0,0.0233586,"e easiest). We believe that this finding has implications on how we should pick good translators and how we should select useful data for parameter optimization. On the other hand, it might also indicate a problem with BLEU as an evaluation measure. In future work, we plan to test our methods on other Arabic-English datasets that have multiple English references. We further plan experiments with other language pairs, e.g., ChineseEnglish, which are available from NIST and IWSLT. We also want to study the effect of the tuning dataset selection on evaluation measures other than BLEU, e.g., TER (Snover et al., 2006) and METEOR (Lavie and Denkowski, 2009). Looking at tuning dataset selection that takes the test data into account is another promising direction for future work. Features from quality estimation (Specia et al., 2010) might be also helpful to determine the best input to tune on. Another related, but different, research direction is about how to best evaluate (as opposed to tune, which we have explored above) an SMT system in case multiple possible versions of the input sentences are available. Choosing the hardest dataset A closer look at the strategies for backtranslate and X-vs-all-but-X rev"
R13-1088,A00-1002,0,0.055846,"Missing"
R13-1088,D11-1125,0,0.0197306,"est BLEU scores are achieved for n = 2. The numbers in the table imply that the alignments become noisier for n-grams longer than two characters; look at the increasing number of phrases that can be extracted from the aligned corpus, many of which do not survive the filtering. 4.2 One possibility is to just apply the models tuned for the individual translation tasks, which is suboptimal. Therefore, we also introduce a global tuning approach, in which we generate k-best lists for the combined cascaded translation model and we tune corresponding end-to-end weights using MERT (Och, 2003) or PRO (Hopkins and May, 2011). We chose to set the size of the k-best lists to 20 in both steps to keep the size manageable, with 400 hypotheses for each tuning sentence. Another option is to combine (i) the direct translation model, (ii) the word-level pivot model, and (iii) the character-level pivot model. Throwing them all in one k-best reranking system does not work well when using the unnormalized model scores. However, global tuning helps reassign weights such that the interactions between the various components can be covered. We use the same global tuning model introduced above using a combined system as the black"
R13-1088,P05-1074,0,0.0454208,"684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine wo"
R13-1088,N07-1047,0,0.035904,"g characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and avoiding generating non-word sequences: we opted for models of order 10, both for the language model and for the maximal phrase length (normally, 5 and 7, respectively). One difficulty is that training these models requires the alignment of characters in bitexts. Specialized character-level alignment algorithms do exist, e.g., those developed for character-tophoneme translations (Damper et al., 2005; Jiampojamarn et al., 2007). However, Tiedemann (2012a) has demonstrated that standard tools for word alignment are in fact also very effective for character-level alignment, especially when extended with local context. Using character ngrams instead of single characters improves the expressive power of lexical translation parameters, which are one of the most important factors in standard word alignment models. For example, using character n-grams increases the vocabulary size of a 1.3M tokens-long Bulgarian text as follows: 101 single characters, 1,893 character bigrams, and 14,305 character trigrams; compared to 30,9"
R13-1088,2008.iwslt-papers.1,0,0.0151556,"translations, called bitexts, which are not available for most language pairs and textual domains. As a result, building an SMT system to translate directly between two languages is often not possible. A common solution to this problem is to use an intermediate, or pivot language to bridge the gap in training such a system. 2 Related Work SMT using pivot languages has been studied for several years. Cohn and Lapata (2007) used triangulation techniques for the combination of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 20"
R13-1088,D07-1103,0,0.0301062,"using character n-grams proposed by Tiedemann (2012a). Another direction we explore is the possibility of reducing the noise in the phrase table. Treating even closely related languages by transliteration techniques is only a rough approximation to the translation task at hand. Furthermore, during training we observe many example translations that are not literally translated from one language to another. Hence, the character-level phrase table will be filled with many noisy and unintuitive translation options. We, therefore, applied phrase table pruning techniques based on relative entropy (Johnson et al., 2007) to remove unreliable pairs. Table 1: Size of the datasets. The original data from OPUS is contributed by on-line users with little quality control and is thus quite noisy. Subtitles in OPUS are checked using automatic language identifiers and aligned using time information (Tiedemann, 2009b; Tiedemann, 2012b). However, we identified many misaligned files and, therefore, we realigned the corpus using hunalign (Varga et al., 2005). We also found several Bulgarian files misclassified as Macedonian and vice versa, which we addressed by filtering out any document pair for which the BLEU score exce"
R13-1088,J93-2003,0,0.0252105,"sely related languages largely overlap in vocabulary and exhibit strong syntactic and lexical similarities. Most words have common roots and express concepts with similar linguistic constructions. Spelling conventions and morphology can still differ, but these differences are typically regular and thus can easily be generalized. These similarities and regularities motivate the use of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed the character-level n-best lists in each tuning step to ca"
R13-1088,W12-3102,0,0.0581772,"Missing"
R13-1088,P07-2045,0,0.0113947,"fer, but these differences are typically regular and thus can easily be generalized. These similarities and regularities motivate the use of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed the character-level n-best lists in each tuning step to calculate word-level BLEU. Thus, we optimized word-level BLEU, while performing character-level translation. 4 We further used the Macedonian–English and the Bulgarian–English movie subtitles datasets from OPUS, which we split into dev/test (10K se"
R13-1088,P07-1092,0,0.0511864,"translation today, are easy to build and offer competitive performance in terms of translation quality. Unfortunately, training such systems requires large parallel corpora of sentences and their translations, called bitexts, which are not available for most language pairs and textual domains. As a result, building an SMT system to translate directly between two languages is often not possible. A common solution to this problem is to use an intermediate, or pivot language to bridge the gap in training such a system. 2 Related Work SMT using pivot languages has been studied for several years. Cohn and Lapata (2007) used triangulation techniques for the combination of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by var"
R13-1088,2009.mtsummit-papers.7,0,0.0171824,"on of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish"
R13-1088,C10-1027,0,0.0123116,"ber 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-lev"
R13-1088,D10-1015,1,0.858695,"first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-level models, a relevant line of research is on combining SMT models of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tie"
R13-1088,2011.eamt-1.19,0,0.0253963,"Missing"
R13-1088,D09-1141,1,0.895904,"rish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-level models, a relevant line of research is on combining SMT models of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is"
R13-1088,E12-1015,1,0.884559,"10) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and a"
R13-1088,tiedemann-2012-parallel,1,0.770407,"10) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and a"
R13-1088,P12-2059,1,0.830649,"et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus capturing larger context and a"
R13-1088,N07-1061,0,0.0401329,"d by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006)"
R13-1088,C12-1121,1,0.893166,"Missing"
R13-1088,J03-1002,0,0.0125279,"contextual specificity. Character-level SMT Models Closely related languages largely overlap in vocabulary and exhibit strong syntactic and lexical similarities. Most words have common roots and express concepts with similar linguistic constructions. Spelling conventions and morphology can still differ, but these differences are typically regular and thus can easily be generalized. These similarities and regularities motivate the use of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed th"
R13-1088,W07-0705,0,0.511628,"of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of hig"
R13-1088,P03-1021,0,0.019644,"ut filtering, the best BLEU scores are achieved for n = 2. The numbers in the table imply that the alignments become noisier for n-grams longer than two characters; look at the increasing number of phrases that can be extracted from the aligned corpus, many of which do not survive the filtering. 4.2 One possibility is to just apply the models tuned for the individual translation tasks, which is suboptimal. Therefore, we also introduce a global tuning approach, in which we generate k-best lists for the combined cascaded translation model and we tune corresponding end-to-end weights using MERT (Och, 2003) or PRO (Hopkins and May, 2011). We chose to set the size of the k-best lists to 20 in both steps to keep the size manageable, with 400 hypotheses for each tuning sentence. Another option is to combine (i) the direct translation model, (ii) the word-level pivot model, and (iii) the character-level pivot model. Throwing them all in one k-best reranking system does not work well when using the unnormalized model scores. However, global tuning helps reassign weights such that the interactions between the various components can be covered. We use the same global tuning model introduced above using"
R13-1088,D12-1027,1,0.907259,"Missing"
R13-1088,P02-1040,0,0.0876252,"of character-level SMT models, which can operate at the sub-word level, but also cover mappings spanning over words and multi-word units. Hence, we used GIZA++ (Och and Ney, 2003) to generate IBM model 4 alignments (Brown et al., 1993) for character n-grams, which we symmetrized using the grow-diag-final-and heuristics. We then converted the result to character alignments by dropping all characters behind the initial one. Finally, we used the Moses toolkit (Koehn et al., 2007) to build a character-level phrase table. 677 We tuned the parameters of the log-linear SMT model by optimizing BLEU (Papineni et al., 2002). Computing BLEU scores over character sequences does not make much sense, especially for small n-gram sizes (usually, n ≤ 4). Therefore, we post-processed the character-level n-best lists in each tuning step to calculate word-level BLEU. Thus, we optimized word-level BLEU, while performing character-level translation. 4 We further used the Macedonian–English and the Bulgarian–English movie subtitles datasets from OPUS, which we split into dev/test (10K sentence pairs for each) and train datasets. We made sure that the dev/test datasets for MK-BG, MK-EN and BG-EN do not overlap, and that all d"
R13-1088,P07-1108,0,0.0556458,"sentences and their translations, called bitexts, which are not available for most language pairs and textual domains. As a result, building an SMT system to translate directly between two languages is often not possible. A common solution to this problem is to use an intermediate, or pivot language to bridge the gap in training such a system. 2 Related Work SMT using pivot languages has been studied for several years. Cohn and Lapata (2007) used triangulation techniques for the combination of phrase tables. The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜n"
R13-1088,P09-1018,0,0.0136668,"The lexical weights in such an approach can be estimated by bridging word alignments (Wu and Wang, 2007; Bertoldi et al., 2008). 676 Proceedings of Recent Advances in Natural Language Processing, pages 676–684, Hissar, Bulgaria, 7-13 September 2013. Character-level SMT models, thus combine the generality of character-by-character transliteration and lexical mappings of larger units that could possibly refer to morphemes, words or phrases, to various combinations thereof. Cascaded translation via pivot languages is used by various researchers (de Gispert and Mari˜no, 2006; Koehn et al., 2009; Wu and Wang, 2009). Several techniques are compared in (Utiyama and Isahara, 2007; de Gispert and Mari˜no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Alti"
R13-1088,2009.eamt-1.3,1,0.939893,"rity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and Ng, 2012). However, that work did not use character-level models. Character-level models were used for transliteration (Matthews, 2007; Tiedemann and Nabende, 2009) and for SMT between closely related languages (Vilar et al., 2007; Tiedemann, 2009a; Nakov and Tiedemann, 2012). Tiedemann (2012a) used pivoting with character-level SMT. 3 One drawback of character-level models is their inability to model long-distance word reorderings. However, we do not assume very large syntactic differences between closely related languages. Another issue is that sentences become longer, which causes an overhead in decoding time. In our experiments below, we use phrase-based SMT, treating characters as words, and using a special character for the original space character. Due to the reduced vocabulary, we can easily train models of higher order, thus c"
R13-1088,P98-2238,0,0.0514886,"no, 2006; Wu and Wang, 2009). Pivot languages can also be used for paraphrasing and lexical adaptation (Bannard and Callison-Burch, 2005; Crego et al., 2010). None of this work exploits the similarity between the pivot and the source/target language. The first step in our pivoting experiments involves SMT between closely related languages, which has been handled using word-for-word translation and manual rules for a number of language pairs, e.g., Czech–Slovak (Hajiˇc et al., 2000), Turkish–Crimean Tatar (Altintas and Cicekli, 2002), Irish–Scottish Gaelic (Scannell, 2006), Cantonese–Mandarin (Zhang, 1998). In contrast, we explore statistical approaches that are potentially applicable to many language pairs. Since we combine word- and character-level models, a relevant line of research is on combining SMT models of different granularity, e.g., Luong et al. (2010) combine word- and morphemelevel representations for English–Finnish. However, they did not assume similarity between the two languages, neither did they use pivoting. Another relevant research combines bitexts between related languages with little or no adaptation (Nakov and Ng, 2009; Marujo et al., 2011; Wang et al., 2012; Nakov and N"
R13-1088,C98-2233,0,\N,Missing
R15-1034,baccianella-etal-2010-sentiwordnet,0,0.336878,"iani, 2002), and that it crucially needed external knowledge in the form of suitable sentiment polarity lexicons. For further detail, see the surveys by Pang and Lee (2008) and Liu and Zhang (2012). Until recently, such sentiment polarity lexicons have been manually crafted, and were of small to moderate size, e.g., LIWC (Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis in Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). The remainder of the paper is organized as follows: Section 2 presents some related work. Sections 3 and 4 describe the datasets and the various lexicons we created for Macedonian. Section 5 gives detail about our system, including the preprocessing steps and the features used."
R15-1034,P15-2113,1,0.726279,"Missing"
R15-1034,esuli-sebastiani-2006-sentiwordnet,0,0.0758621,"ument classification (Sebastiani, 2002), and that it crucially needed external knowledge in the form of suitable sentiment polarity lexicons. For further detail, see the surveys by Pang and Lee (2008) and Liu and Zhang (2012). Until recently, such sentiment polarity lexicons have been manually crafted, and were of small to moderate size, e.g., LIWC (Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis in Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). The remainder of the paper is organized as follows: Section 2 presents some related work. Sections 3 and 4 describe the datasets and the various lexicons we created for Macedonian. Section 5 gives detail about our system, including the preprocessing s"
R15-1034,S15-2080,0,0.0426301,"e information and, as a result, to have influence on the content distributed via these services. The ease of sharing, e.g., directly from a laptop, a tablet or a smart phone, have contributed to the tremendous growth of the content that users share on a daily basis, to the extent that nowadays social networks have no choice but to filter part of the information stream even when it comes from our closest friends. 1 Other related tasks were the Aspect-Based Sentiment Analysis task (Pontiki et al., 2014; Pontiki et al., 2015), and the task on Sentiment Analysis of Figurative Language in Twitter (Ghosh et al., 2015). 249 Proceedings of Recent Advances in Natural Language Processing, pages 249–257, Hissar, Bulgaria, Sep 7–9 2015. In our experiments below, we focus on Macedonian, for which we only know two publications on sentiment analysis, none of which is about Twitter. Gajduk and Kocarev (2014) experimented with 800 posts from the Kajgana forum (260 positive, 260 negative, and 280 objective), using SVM and Naïve Bayes classifiers, and features such as bag of words, rules for negation, and stemming. Uzunova and Kulakov (2015) experimented with 400 movie reviews2 (200 positive, and 200 negative; no objec"
R15-1034,D15-1068,1,0.783246,"Missing"
R15-1034,P05-1015,0,0.185673,"present, word position and part-of-speech tagging. Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations (Wiebe et al., 2004), the role of context in determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it is crucial to know not just the polariy of the sentiment, but also the topic towards which this sentiment is expressed (Stoyanov and Cardie, 2008). Early sentiment analysis research focused on customer reviews of movies, and later of hotels, phones, laptops, etc. Later, with the emergence of social media, sentiment analysis in Twitter became a hot research topic. The earliest Twitter sentiment datasets were both small and proprietary, such as the i-sieve corpus (Kouloumpis et al., 201"
R15-1034,R15-1036,1,0.499582,"t Twitter sentiment datasets were both small and proprietary, such as the i-sieve corpus (Kouloumpis et al., 2011), or relied on noisy labels obtained from emoticons or hashtags. This situation changed with the emergence of the SemEval task on Sentiment Analysis in Twitter, which ran in 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). The task created standard datasets of several thousand tweets annotated for sentiment polarity. Our work here is inspired by that task. 2 There have been also experiments on movie reviews for the closely related Bulgarian language (Kapukaranov and Nakov, 2015), but there the objective was to predict user rating, which was addressed as an ordinal regression problem. 250 Dataset Train Test Similar observations were made in the AspectBased Sentiment Analysis task, which ran at SemEval 2014-2015 (Pontiki et al., 2014; Pontiki et al., 2015). In both tasks, the winning systems benefited from building and using massive sentiment polarity lexicons (Mohammad et al., 2013; Zhu et al., 2014). These large-scale automatic lexicons were typically built using bootstrapping, starting with a small seed of, e.g., 50-60 words (Mohammad et al., 2013), and sometimes ev"
R15-1034,W02-1011,0,0.0392313,"2014; Rosenthal et al., 2015). The remainder of the paper is organized as follows: Section 2 presents some related work. Sections 3 and 4 describe the datasets and the various lexicons we created for Macedonian. Section 5 gives detail about our system, including the preprocessing steps and the features used. Section 6 describes our experiments and discusses the results. Section 7 concludes with possible directions for future work. 2 Related Work Research in sentiment analysis started in the early 2000s. Initially, the problem was regarded as standard document classification into topics, e.g., Pang et al. (2002) experimented with various classifiers such as maximum entropy, Naïve Bayes and SVM, using standard features such as unigram/bigrams, word counts/present, word position and part-of-speech tagging. Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations (Wiebe et al., 2004), the role of context in determining the sentiment orientatio"
R15-1034,N10-1087,0,0.0393172,"d in recent SemEval competitions. In future work, we are interested in studying the impact of the raw corpus size, e.g., we could only collect half a million tweets for creating lexicons and analyzing/evaluating the system, while Kiritchenko et al. (2014) built their lexicon on million tweets and evaluated their system on 135 million English tweets. Moreover, we are interested not only in quantity but also in quality, i.e., in studying the quality of the individual words and phrases used as seeds. An interesting work in that direction, even though in a different domain and context, is that of Kozareva and Hovy (2010). We are further interested in finding alternative ways for defining the sentiment polarity, including degree of positive or negative sentiment, and in evaluating them by constructing polarity lexicons in new ways (Severyn and Moschitti, 2015). More ambitiously, we would like to extend our system to detecting sentiment over a period of time for the purpose of finding trends towards a topic (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015), e.g., predicting whether the sentiment is strongly negative, weakly negative, strongly positive, etc. We further plan application to othe"
R15-1034,S14-2004,0,0.0510844,"social media services such as Facebook, Twitter and Google+, and the advance of Web 2.0 have enabled users to share information and, as a result, to have influence on the content distributed via these services. The ease of sharing, e.g., directly from a laptop, a tablet or a smart phone, have contributed to the tremendous growth of the content that users share on a daily basis, to the extent that nowadays social networks have no choice but to filter part of the information stream even when it comes from our closest friends. 1 Other related tasks were the Aspect-Based Sentiment Analysis task (Pontiki et al., 2014; Pontiki et al., 2015), and the task on Sentiment Analysis of Figurative Language in Twitter (Ghosh et al., 2015). 249 Proceedings of Recent Advances in Natural Language Processing, pages 249–257, Hissar, Bulgaria, Sep 7–9 2015. In our experiments below, we focus on Macedonian, for which we only know two publications on sentiment analysis, none of which is about Twitter. Gajduk and Kocarev (2014) experimented with 800 posts from the Kajgana forum (260 positive, 260 negative, and 280 objective), using SVM and Naïve Bayes classifiers, and features such as bag of words, rules for negation, and s"
R15-1034,S15-2082,0,0.0667065,"Missing"
R15-1034,K15-1032,1,0.816949,"Missing"
R15-1034,R15-1058,1,0.892641,"Missing"
R15-1034,R09-1065,1,0.674602,"g. Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations (Wiebe et al., 2004), the role of context in determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it is crucial to know not just the polariy of the sentiment, but also the topic towards which this sentiment is expressed (Stoyanov and Cardie, 2008). Early sentiment analysis research focused on customer reviews of movies, and later of hotels, phones, laptops, etc. Later, with the emergence of social media, sentiment analysis in Twitter became a hot research topic. The earliest Twitter sentiment datasets were both small and proprietary, such as the i-sieve corpus (Kouloumpis et al., 2011), or relied on noisy labels obtained from emoticons"
R15-1034,S13-2053,0,0.0432563,"ty for w, and its magnitude shows the corresponding sentiment strength. (w,pos) In turn, P M I(w, pos) = PP(w)P (pos) , where P (w, pos) is the probability to see w with any of the seed positive words in the same tweet,8 P (w) is the probability to see w in any tweet, and P (pos) is the probability to see any of the seed positive words in a tweet; P M I(w, neg) is defined similarly. Turney’s PMI-based approach further serves as the basis for two popular large-scale automatic lexicons for English sentiment analysis in Twitter, initially developed by NRC for their participation in SemEval-2013 (Mohammad et al., 2013). The Hashtag Sentiment Lexicon uses as seeds hashtags containing 32 positive and 36 negative words, e.g., #happy and #sad; it then uses PMI and extracts 775,000 sentiment words from 135 million tweets. Similarly, the Sentiment140 lexicon contains 1.6 million sentiment words and phrases, extracted from the same 135 million tweets, but this time using smileys as seed indicators for positive and negative sentiment, e.g., :), :-) and :)) serve as positive seeds, and :( and :-( as negative ones. In our experiments, we used all words from our manually-crafted Macedonian sentiment polarity lexicon a"
R15-1034,S14-2009,1,0.672969,"lications such as mining opinions from product reviews, detecting inappropriate content, and many others. Below we describe the creation of data and the development of a system for sentiment polarity classification in Twitter for Macedonian: positive, negative, neutral. We are inspired by a similar task at SemEval, which is an ongoing series of evaluations of computational semantic analysis systems, composed by multiple challenges such as text similarity, word sense disambiguation, etc. One of the challenges there was on Sentiment Analysis in Twitter, at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2015), where over 40 teams participated three years in a row.1 Here we follow a similar setup, focusing on message-level sentiment analysis of tweets, but for Macedonian instead of English. Moreover, while at SemEval the task organizers used Mechanical Turk to do the annotations, where the control for quality is hard (everybody can pretend to know English), our annotations are done by native speakers of Macedonian. We present work on sentiment analysis in Twitter for Macedonian. As this is pioneering work for this combination of language and genre, we cr"
R15-1034,S15-2078,1,0.868535,"opinions from product reviews, detecting inappropriate content, and many others. Below we describe the creation of data and the development of a system for sentiment polarity classification in Twitter for Macedonian: positive, negative, neutral. We are inspired by a similar task at SemEval, which is an ongoing series of evaluations of computational semantic analysis systems, composed by multiple challenges such as text similarity, word sense disambiguation, etc. One of the challenges there was on Sentiment Analysis in Twitter, at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2015), where over 40 teams participated three years in a row.1 Here we follow a similar setup, focusing on message-level sentiment analysis of tweets, but for Macedonian instead of English. Moreover, while at SemEval the task organizers used Mechanical Turk to do the annotations, where the control for quality is hard (everybody can pretend to know English), our annotations are done by native speakers of Macedonian. We present work on sentiment analysis in Twitter for Macedonian. As this is pioneering work for this combination of language and genre, we created suitable resources"
R15-1034,S13-2052,1,0.705872,"needs of various applications such as mining opinions from product reviews, detecting inappropriate content, and many others. Below we describe the creation of data and the development of a system for sentiment polarity classification in Twitter for Macedonian: positive, negative, neutral. We are inspired by a similar task at SemEval, which is an ongoing series of evaluations of computational semantic analysis systems, composed by multiple challenges such as text similarity, word sense disambiguation, etc. One of the challenges there was on Sentiment Analysis in Twitter, at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2015), where over 40 teams participated three years in a row.1 Here we follow a similar setup, focusing on message-level sentiment analysis of tweets, but for Macedonian instead of English. Moreover, while at SemEval the task organizers used Mechanical Turk to do the annotations, where the control for quality is hard (everybody can pretend to know English), our annotations are done by native speakers of Macedonian. We present work on sentiment analysis in Twitter for Macedonian. As this is pioneering work for this combination of l"
R15-1034,N15-1159,0,0.0614469,"ko et al. (2014) built their lexicon on million tweets and evaluated their system on 135 million English tweets. Moreover, we are interested not only in quantity but also in quality, i.e., in studying the quality of the individual words and phrases used as seeds. An interesting work in that direction, even though in a different domain and context, is that of Kozareva and Hovy (2010). We are further interested in finding alternative ways for defining the sentiment polarity, including degree of positive or negative sentiment, and in evaluating them by constructing polarity lexicons in new ways (Severyn and Moschitti, 2015). More ambitiously, we would like to extend our system to detecting sentiment over a period of time for the purpose of finding trends towards a topic (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015), e.g., predicting whether the sentiment is strongly negative, weakly negative, strongly positive, etc. We further plan application to other social media services, with the idea of analyzing the sentiment of an online conversation. We would like to see the impact of earlier messages on the sentiment of newer messages, e.g., as in (Vanzo et al., 2014; Barrón-Cedeño et al., 2015; J"
R15-1034,C08-1103,0,0.333491,"es: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations (Wiebe et al., 2004), the role of context in determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it is crucial to know not just the polariy of the sentiment, but also the topic towards which this sentiment is expressed (Stoyanov and Cardie, 2008). Early sentiment analysis research focused on customer reviews of movies, and later of hotels, phones, laptops, etc. Later, with the emergence of social media, sentiment analysis in Twitter became a hot research topic. The earliest Twitter sentiment datasets were both small and proprietary, such as the i-sieve corpus (Kouloumpis et al., 2011), or relied on noisy labels obtained from emoticons or hashtags. This situation changed with the emergence of the SemEval task on Sentiment Analysis in Twitter, which ran in 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). T"
R15-1034,P02-1053,0,0.101564,"6 describes our experiments and discusses the results. Section 7 concludes with possible directions for future work. 2 Related Work Research in sentiment analysis started in the early 2000s. Initially, the problem was regarded as standard document classification into topics, e.g., Pang et al. (2002) experimented with various classifiers such as maximum entropy, Naïve Bayes and SVM, using standard features such as unigram/bigrams, word counts/present, word position and part-of-speech tagging. Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations (Wiebe et al., 2004), the role of context in determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it is crucial to know not just the pola"
R15-1034,C14-1221,0,0.12718,"Missing"
R15-1034,J04-3002,0,0.313669,"garded as standard document classification into topics, e.g., Pang et al. (2002) experimented with various classifiers such as maximum entropy, Naïve Bayes and SVM, using standard features such as unigram/bigrams, word counts/present, word position and part-of-speech tagging. Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations (Wiebe et al., 2004), the role of context in determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it is crucial to know not just the polariy of the sentiment, but also the topic towards which this sentiment is expressed (Stoyanov and Cardie, 2008). Early sentiment analysis research focused on customer reviews of movies, and later of hotels, phones, laptops, etc."
R15-1034,H05-1044,0,0.526527,"we mentioned above, since the very beginning, researchers have realized that sentiment analysis was quite different from standard document classification (Sebastiani, 2002), and that it crucially needed external knowledge in the form of suitable sentiment polarity lexicons. For further detail, see the surveys by Pang and Lee (2008) and Liu and Zhang (2012). Until recently, such sentiment polarity lexicons have been manually crafted, and were of small to moderate size, e.g., LIWC (Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis in Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). The remainder of the paper is organized as follows: Section 2 presents some related work. Sections 3 and 4 desc"
R15-1034,S14-2076,0,0.128003,"Missing"
R15-1036,D15-1068,1,0.806175,"Missing"
R15-1036,baccianella-etal-2010-sentiwordnet,0,0.25856,"that sentiment analysis crucially needed external knowledge in the form of suitable sentiment polarity lexicons. For further detail, see the surveys by Pang and Lee (2008) and Liu and Zhang (2012). Until recently, such sentiment polarity lexicons were manually crafted, and were thus of small to moderate size, e.g., LIWC (Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems benefited from building and u"
R15-1036,P15-2113,1,0.734155,"Missing"
R15-1036,R15-1034,1,0.403308,"an. Gajduk and Kocarev (2014) experimented with 800 posts from the Kajgana forum (260 positive, 260 negative, and 280 objective), using Support Vector Machines (SVM) and Na¨ıve Bayes classifiers, and features such as bag of words, rules for negation, and stemming. More closely related to our work, Uzunova and Kulakov (2015) experimented with 400 movie reviews (200 positive + 200 negative), and a Na¨ıve Bayes classifier, using a small manually annotated sentiment lexicon of unknown size, and various preprocessing techniques such as negation handling and spelling/character translation. Finally, Jovanoski et al. (2015) presented work on sentiment analysis of Macedonian tweets (8,583 for training + 1,139 for testing) using a 3-way tweet-level sentiment polarity classification scheme: positive, negative, and neutral/objective. They used standard features but variety of preprocessing steps, including morphological processing and POS tagging for Macedonian, negation handling, text standardization, tweet-specific processing, etc. More imporantly, they made use of several lexicons, some translated from other languages,3 which they augmented with bootstrapping, ultimately achieving results that are on par with the"
R15-1036,esuli-sebastiani-2006-sentiwordnet,0,0.302391,"ss, sport, and politics, and that sentiment analysis crucially needed external knowledge in the form of suitable sentiment polarity lexicons. For further detail, see the surveys by Pang and Lee (2008) and Liu and Zhang (2012). Until recently, such sentiment polarity lexicons were manually crafted, and were thus of small to moderate size, e.g., LIWC (Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems be"
R15-1036,K15-1032,1,0.789603,"Missing"
R15-1036,R09-1022,1,0.910366,"Missing"
R15-1036,R15-1058,1,0.891676,"Missing"
R15-1036,E12-1050,1,0.900018,"Missing"
R15-1036,S13-2053,0,0.0430415,"automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems benefited from building and using massive sentiment polarity lexicons (Mohammad et al., 2013; Zhu et al., 2014). 3 Table 1: Statistics about our dataset. 0.0 35 0.5 200 1.0 602 1.5 48 2.0 425 2.5 132 3.0 812 3.5 306 4.0 1447 4.5 526 5.0 5665 0 6000 Figure 1: User rating distribution in our dataset. Figure 1 shows a distribution of the user ratings in our movie reviews dataset. We can see that the distribution is generally skewed towards full scores, while scores with halves are much less frequent: people seem to prefer a 5-point scale, and would not take full advantage of an 11-point one. Moreover, the distribution is also skewed towards high scores, and quite heavily towards a 5-sta"
R15-1036,S13-2052,1,0.856924,"e thus of small to moderate size, e.g., LIWC (Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems benefited from building and using massive sentiment polarity lexicons (Mohammad et al., 2013; Zhu et al., 2014). 3 Table 1: Statistics about our dataset. 0.0 35 0.5 200 1.0 602 1.5 48 2.0 425 2.5 132 3.0 812 3.5 306 4.0 1447 4.5 526 5.0 5665 0 6000 Figure 1: User rating distribution in our dataset."
R15-1036,W06-3808,0,0.041341,"11. Regression. For regression, we used the same SVM tool and the same features and parameters as for classification, but we predicted a numerical value; this is known as support vector regression (Smola and Sch¨olkopf, 2004) Ordinal Regression. For this scenario, we used ordinal logistic regression. This model is also known as proportional odds and was introduced by McCullagh (1980).13 The use of ordinal regression for sentiment analysis, is not very common, mostly because the ordinal formulation of the task is not very common, even though it was used by some researchers (Pang and Lee, 2005; Goldberg and Zhu, 2006; Baccianella et al., 2009). Yet, it makes a lot of sense to use it as it tries to fit the data into thresholded regions as a classification task would do, and at the same time tries to predict values with an established order and position in the label space. This makes it interesting especially in the 5-class setup, where we have a small number of labels and there is ordering between them. Contextual Features In addition to the above textual features, we further added some contextual (metadata) features: • movie length: numeric feature indicating the run-length of the movie; • country: binary"
R15-1036,S14-2009,1,0.87087,"oderate size, e.g., LIWC (Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems benefited from building and using massive sentiment polarity lexicons (Mohammad et al., 2013; Zhu et al., 2014). 3 Table 1: Statistics about our dataset. 0.0 35 0.5 200 1.0 602 1.5 48 2.0 425 2.5 132 3.0 812 3.5 306 4.0 1447 4.5 526 5.0 5665 0 6000 Figure 1: User rating distribution in our dataset. Figure 1 shows a distrib"
R15-1036,S15-2078,1,0.859849,"(Pennebaker et al., 2001), General Inquirer (Stone et al., 1966), Bing Liu’s lexicon (Hu and Liu, 2004), and MPQA (Wilson et al., 2005), all have 2000-8000 words. Early efforts in building them automatically also yielded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems benefited from building and using massive sentiment polarity lexicons (Mohammad et al., 2013; Zhu et al., 2014). 3 Table 1: Statistics about our dataset. 0.0 35 0.5 200 1.0 602 1.5 48 2.0 425 2.5 132 3.0 812 3.5 306 4.0 1447 4.5 526 5.0 5665 0 6000 Figure 1: User rating distribution in our dataset. Figure 1 shows a distribution of the user ratings"
R15-1036,P04-1035,0,0.0449849,"arch domains for sentiment analysis as they (i) have the properties of a short message, and (ii) are already manually annotated by the author, as the score generally reflects sentiment polarity. Popular features for score/sentiment prediction include POS tags, word n-grams, word lemmata, and various context features based on the distance from a topic word. The challenge with movie reviews is that only some of the words are relevant for sentiment analysis. In fact, often the review is just a short narrative of the movie plot. One way to approach the problem is to use a subjectivity classifier (Pang and Lee, 2004), which can be used to filter out objective sentences from the reviews, thus allowing the classifier then to focus on the subjective sentences only. Early researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work looked into the linguistic aspects of how opinions, evaluations, and speculations are expressed in text (Wiebe et al., 2004), into the role of context for determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic"
R15-1036,C08-1103,0,0.312202,"vs. negative. Later work looked into the linguistic aspects of how opinions, evaluations, and speculations are expressed in text (Wiebe et al., 2004), into the role of context for determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it was crucial to know not just the sentiment, but also the topic towards which this sentiment was expressed (Stoyanov and Cardie, 2008). 2 Some linguists consider Macedonian a dialect of Bulgarian; this is also the position of the Bulgarian government. 3 In fact, they used, without translation, the Bulgarian lexicon that we present in this work. 267 Characteristic unique words unique users unique movie genres unique movie countries unique movie actors unique movie directors Given the lack of previously developed datasets or sentiment polarity lexicons for Bulgarian, we had to create them ourselves. In addition to preparing a dataset of annotated movies, we further focused on building a sentiment polarity lexicon for Bulgarian"
R15-1036,P05-1015,0,0.968079,"movies in Bulgarian,1 where each review is associated with an 11-scale star rating: 0, 0.5, 1, ..., 4.5, 5. • We prepare a new sentiment lexicon for Bulgarian, which is also freely available. • Most importantly, we present the first work for Bulgarian on predicting fine-grained sentiment. 1 The dataset is freely available for research purposes at http://bkapukaranov.github.io/ 266 Proceedings of Recent Advances in Natural Language Processing, pages 266–274, Hissar, Bulgaria, Sep 7–9 2015. Fine-grained sentiment analysis tries to predict sentiment in a text using a finer scale, e.g., 5-stars; Pang and Lee (2005) pioneered this sub-field. In their work, they looked at the problem from two perspectives: as one vs. all classification, and as a regression by putting the 5-star ratings on a metric scale. An interesting observation in their research is that humans are not very good at doing such kinds of highly granular judgments and are often off the target mark by a full star. Naturally, most research in sentiment analysis was done for English, and very little efforts were devoted to other languages. We are not aware of other work on fine-grained sentiment analysis for Bulgarian. There is work on sentime"
R15-1036,P02-1053,0,0.0516928,"de POS tags, word n-grams, word lemmata, and various context features based on the distance from a topic word. The challenge with movie reviews is that only some of the words are relevant for sentiment analysis. In fact, often the review is just a short narrative of the movie plot. One way to approach the problem is to use a subjectivity classifier (Pang and Lee, 2004), which can be used to filter out objective sentences from the reviews, thus allowing the classifier then to focus on the subjective sentences only. Early researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work looked into the linguistic aspects of how opinions, evaluations, and speculations are expressed in text (Wiebe et al., 2004), into the role of context for determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it was crucial"
R15-1036,W02-1011,0,0.0341869,"ng, etc. More imporantly, they made use of several lexicons, some translated from other languages,3 which they augmented with bootstrapping, ultimately achieving results that are on par with the state of the art for English. The remainder of this paper is organized as follows: Section 2 introduces related work, Section 3 describes the dataset and teh lexicon we prepared, Section 4 presents the features we experiment with, Section 5 describes our experiments, and Section 6 discusses the results. Finally, Section 7 concludes and points to some possible directions for future work. 2 Related Work Pang et al. (2002) were the first to look into text classification not in terms of topics, but focusing on how sentiment polarity is distributed in a document. They tried several machine learning algorithms on an English movie reviews dataset, and evaluated the performance of basic features such as n-grams and part of speech (POS) tags. Movie reviews were one of the first research domains for sentiment analysis as they (i) have the properties of a short message, and (ii) are already manually annotated by the author, as the score generally reflects sentiment polarity. Popular features for score/sentiment predict"
R15-1036,C14-1221,0,0.15207,"Missing"
R15-1036,J04-3002,0,0.256461,"just a short narrative of the movie plot. One way to approach the problem is to use a subjectivity classifier (Pang and Lee, 2004), which can be used to filter out objective sentences from the reviews, thus allowing the classifier then to focus on the subjective sentences only. Early researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work looked into the linguistic aspects of how opinions, evaluations, and speculations are expressed in text (Wiebe et al., 2004), into the role of context for determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it was crucial to know not just the sentiment, but also the topic towards which this sentiment was expressed (Stoyanov and Cardie, 2008). 2 Some linguists consider Macedonian a dialect of Bulgarian; this is also the position of the Bulgarian government. 3 In f"
R15-1036,S14-2004,0,0.0531866,"elded lexicons of moderate sizes (Esuli and Sebastiani, 2006; Baccianella et al., 2010). However, recent results have shown that automatically extracted large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems benefited from building and using massive sentiment polarity lexicons (Mohammad et al., 2013; Zhu et al., 2014). 3 Table 1: Statistics about our dataset. 0.0 35 0.5 200 1.0 602 1.5 48 2.0 425 2.5 132 3.0 812 3.5 306 4.0 1447 4.5 526 5.0 5665 0 6000 Figure 1: User rating distribution in our dataset. Figure 1 shows a distribution of the user ratings in our movie reviews dataset. We can see that the distribution is generally skewed towards full scores, while scores with halves are much less frequent: people seem to prefer a 5-point scale, and would not take fu"
R15-1036,H05-1044,0,0.618523,"bjectivity classifier (Pang and Lee, 2004), which can be used to filter out objective sentences from the reviews, thus allowing the classifier then to focus on the subjective sentences only. Early researchers realized the importance of external sentiment lexicons, e.g., Turney (2002) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive vs. negative. Later work looked into the linguistic aspects of how opinions, evaluations, and speculations are expressed in text (Wiebe et al., 2004), into the role of context for determining the sentiment orientation (Wilson et al., 2005), of deeper linguistic processing such as negation handling (Pang and Lee, 2008), of finer-grained sentiment distinctions (Pang and Lee, 2005), of positional information (Raychev and Nakov, 2009), etc. Moreover, it was recognized that in many cases, it was crucial to know not just the sentiment, but also the topic towards which this sentiment was expressed (Stoyanov and Cardie, 2008). 2 Some linguists consider Macedonian a dialect of Bulgarian; this is also the position of the Bulgarian government. 3 In fact, they used, without translation, the Bulgarian lexicon that we present in this work. 2"
R15-1036,R09-1065,1,0.73768,"oked at the problem from two perspectives: as one vs. all classification, and as a regression by putting the 5-star ratings on a metric scale. An interesting observation in their research is that humans are not very good at doing such kinds of highly granular judgments and are often off the target mark by a full star. Naturally, most research in sentiment analysis was done for English, and very little efforts were devoted to other languages. We are not aware of other work on fine-grained sentiment analysis for Bulgarian. There is work on sentiment analysis by Bulgarian scolars (Raychev, 2009; Raychev and Nakov, 2009; Kraychev and Koychev, 2012; Kraychev, 2014). We are aware of three publications for the closely-related Macedonian language,2 which is mutually intelligible with Bulgarian. Gajduk and Kocarev (2014) experimented with 800 posts from the Kajgana forum (260 positive, 260 negative, and 280 objective), using Support Vector Machines (SVM) and Na¨ıve Bayes classifiers, and features such as bag of words, rules for negation, and stemming. More closely related to our work, Uzunova and Kulakov (2015) experimented with 400 movie reviews (200 positive + 200 negative), and a Na¨ıve Bayes classifier, using"
R15-1036,S14-2076,0,0.0715904,"large-scale lexicons (e.g., up to a million words and phrases) offer important performance advantages, as confirmed at shared tasks on Sentiment Analysis on Twitter at SemEval 2013-2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). These lexicons were crucial for the top-performing teams in the competition in all three years. Similar observations were made in the AspectBased Sentiment Analysis task at SemEval 20142015 (Pontiki et al., 2014). In both tasks, the winning systems benefited from building and using massive sentiment polarity lexicons (Mohammad et al., 2013; Zhu et al., 2014). 3 Table 1: Statistics about our dataset. 0.0 35 0.5 200 1.0 602 1.5 48 2.0 425 2.5 132 3.0 812 3.5 306 4.0 1447 4.5 526 5.0 5665 0 6000 Figure 1: User rating distribution in our dataset. Figure 1 shows a distribution of the user ratings in our movie reviews dataset. We can see that the distribution is generally skewed towards full scores, while scores with halves are much less frequent: people seem to prefer a 5-point scale, and would not take full advantage of an 11-point one. Moreover, the distribution is also skewed towards high scores, and quite heavily towards a 5-star rating in particu"
R15-1058,R09-1022,1,0.84095,"eplies, and the time of commenting. Overall, paid trolls looked roughly like the “mentioned” trolls, except that they were posting most of their comments on working days and during working hours. Unfortunately, our features only worked well for trolls with high number of posts. Thus, in future work, we plan to add keywords, topics, named entities, sentiment analysis (Kapukaranov and Nakov, 2015; Jovanoski et al., 2015), etc, in order to be able to detect “fresh” trolls; this would require stemming (Nakov, 2003b; Nakov, 2003a), POS tagging (Georgiev et al., 2012), and named entity recognition (Georgiev et al., 2009). We also plan to analyze the comment threads as a whole (Barr´on-Cede˜no et al., 2015; Joty et al., 2015). Figure 2: “Mentioned” trolls vs. paid trolls vs. non-trolls based on average feature values. (2 - Comments per active day) shows that paid trolls and “mentioned” trolls write twice as many comments as non-trolls per day. (3 - Avg comments per publication) shows that both paid and “mentioned” trolls post more comments per publication than non-trolls. (4 - Neg voted by other users), (6 - High neg voted by other users), (7 - Med neg voted by other users) show that both paid and “mentioned”"
R15-1058,P15-2113,1,0.423617,"Missing"
R15-1058,E12-1050,1,0.80735,"mments, of positive and of negative votes, of posted replies, and the time of commenting. Overall, paid trolls looked roughly like the “mentioned” trolls, except that they were posting most of their comments on working days and during working hours. Unfortunately, our features only worked well for trolls with high number of posts. Thus, in future work, we plan to add keywords, topics, named entities, sentiment analysis (Kapukaranov and Nakov, 2015; Jovanoski et al., 2015), etc, in order to be able to detect “fresh” trolls; this would require stemming (Nakov, 2003b; Nakov, 2003a), POS tagging (Georgiev et al., 2012), and named entity recognition (Georgiev et al., 2009). We also plan to analyze the comment threads as a whole (Barr´on-Cede˜no et al., 2015; Joty et al., 2015). Figure 2: “Mentioned” trolls vs. paid trolls vs. non-trolls based on average feature values. (2 - Comments per active day) shows that paid trolls and “mentioned” trolls write twice as many comments as non-trolls per day. (3 - Avg comments per publication) shows that both paid and “mentioned” trolls post more comments per publication than non-trolls. (4 - Neg voted by other users), (6 - High neg voted by other users), (7 - Med neg vote"
R15-1058,D15-1068,1,0.492479,"Missing"
R15-1058,R15-1034,1,0.842759,"m (iii) does quite well also at telling apart (i) from (iii). Our further analysis has shown that the most important features were the number of comments, of positive and of negative votes, of posted replies, and the time of commenting. Overall, paid trolls looked roughly like the “mentioned” trolls, except that they were posting most of their comments on working days and during working hours. Unfortunately, our features only worked well for trolls with high number of posts. Thus, in future work, we plan to add keywords, topics, named entities, sentiment analysis (Kapukaranov and Nakov, 2015; Jovanoski et al., 2015), etc, in order to be able to detect “fresh” trolls; this would require stemming (Nakov, 2003b; Nakov, 2003a), POS tagging (Georgiev et al., 2012), and named entity recognition (Georgiev et al., 2009). We also plan to analyze the comment threads as a whole (Barr´on-Cede˜no et al., 2015; Joty et al., 2015). Figure 2: “Mentioned” trolls vs. paid trolls vs. non-trolls based on average feature values. (2 - Comments per active day) shows that paid trolls and “mentioned” trolls write twice as many comments as non-trolls per day. (3 - Avg comments per publication) shows that both paid and “mentioned”"
R15-1058,R15-1036,1,0.842966,"ained to distinguish (ii) from (iii) does quite well also at telling apart (i) from (iii). Our further analysis has shown that the most important features were the number of comments, of positive and of negative votes, of posted replies, and the time of commenting. Overall, paid trolls looked roughly like the “mentioned” trolls, except that they were posting most of their comments on working days and during working hours. Unfortunately, our features only worked well for trolls with high number of posts. Thus, in future work, we plan to add keywords, topics, named entities, sentiment analysis (Kapukaranov and Nakov, 2015; Jovanoski et al., 2015), etc, in order to be able to detect “fresh” trolls; this would require stemming (Nakov, 2003b; Nakov, 2003a), POS tagging (Georgiev et al., 2012), and named entity recognition (Georgiev et al., 2009). We also plan to analyze the comment threads as a whole (Barr´on-Cede˜no et al., 2015; Joty et al., 2015). Figure 2: “Mentioned” trolls vs. paid trolls vs. non-trolls based on average feature values. (2 - Comments per active day) shows that paid trolls and “mentioned” trolls write twice as many comments as non-trolls per day. (3 - Avg comments per publication) shows that"
R15-1058,K15-1032,1,0.384797,"that one comment, etc. For example, one non-scaled feature is the number of times a comment by the target user was voted negatively, i.e., as thumbs down, by other users. As a non-scaled feature, we would use this number directly, while above we would scale it by dividing it by the total number of user’s comments, by the total number of publications the user has commented on, etc. Obviously, there is a danger in using non-scaled features: older users are likely to have higher values for them compared to recentlyregistered users. Yet, we found unscaled features useful in previous experiments (Mihaylov et al., 2015), so we included them here as well. 3 There were six known paid trolls with more than 40 comments, and the remaining nine known paid trolls from Bivol had less than 40 comments. 445 Features All Scaled (AS) AS - comment order (Scaled - S) AS - is reply (S) AS - is reply to has reply (S) AS - similarity (S) AS - similarity top (S) AS - topl oved hated (S) AS - total comments (S) AS - triggered replies range (S) AS - triggered replies total (S) AS - vote updown total (S) AS - time (S) AS - time hours (S) AS - vote up/down reply status (S) AS - time day of week (S) AS + Non Scaled (NS) AS - vote"
R15-1058,P11-1032,0,0.0562887,"ly to be such, and one who has never been called a troll is unlikely to be such. We compare the profiles of (i) paid trolls vs. (ii) “mentioned” trolls vs. (iii) non-trolls, and we further show that a classifier trained to distinguish (ii) from (iii) does quite well also at telling apart (i) from (iii). 1 2 Introduction Related Work A popular way to manipulate public opinion in Intternet is by making controversial posts on a specific topic that aim to win the argument at any cost, usually accompanied by untruthful and deceptive information. The problem of deceptive opinion spam is studied in (Ott et al., 2011), where the authors integrated work from both psychology and computational linguistics trying to detect fake opinions that were written to sound authentic. Malicious troll users posting misinformation posts have also been studied using graph-based approaches over signed social networks (Ortega et al., 2012; Kumar et al., 2014). A related problem is that of trustworthiness of statements on the Web (Rowe and Butters, 2009). During the 2013-2014 Bulgarian protests against the Oresharski cabinet, social networks and news community forums became the main “battle grounds” between supporters and oppo"
R19-1029,D18-1389,1,0.603551,"hat we allow multiple of these labels simultaneously. We further add a non-toxic label for articles that represent good news. 2 2.1 Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016, 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018b; Zlatkova et al., 2019). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Bal"
R19-1029,N19-1216,1,0.84568,"Missing"
R19-1029,N18-2004,1,0.865959,"Missing"
R19-1029,D19-1565,1,0.821097,"d temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018a, 2019; Dinkov et al., 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015a; Kulkarni et al., 2018; Baly et al., 2018a; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answer"
R19-1029,C18-1285,0,0.0621054,"Missing"
R19-1029,C18-1158,0,0.027807,"on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. 2.2 Stance Detection Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018b; Mohtarami et al., 2019). 248 2.3 Source Reliability Estimation They found that “fake news” pack a lot of information in the title (as the focus is on users who do not read beyond the title), and use shorter, simpler, and repetitive content in the body (as writing fake information takes a lot of effort). Thus, they argued that the title and the body should be analyzed separately. In follow-up work, Horne et al. (2018"
R19-1029,S17-2006,0,0.0687829,"Missing"
R19-1029,N19-1423,0,0.0252461,"er expected more or less semantic and valid HTML to be able to process it. Thus, we manually verified the data, fixed any issues we could find and added any missing information. We ended up with a little over 200 articles with some kind of toxicity. 3 LSA We trained a Latent Semantic Analysis (LSA) model on our data. We first built TF.IDF vectors for the title and the body. Then, we applied singular value decomposition (SVD) to generate vectors of 15 dimensions for the titles and of 200 dimensions for the article bodies. Figure 1: Label distribution in the dataset. 3 Method BERT We used BERT (Devlin et al., 2019) for sentence representation, which has achieved very strong results on eleven natural language processing tasks including GLUE, MultiNLI, and SQuAD. Since then, it was used to improve over the state of the art for a number of NLP tasks. The original model was trained on English Wikipedia articles (2500M words). Due to the model complexity and to its size, it is hard to find enough data that represents a specific domain for a specific language. We used BERT-as-a-service, which generates a vector of 768 numerical values for a given text. In its original form, this is a sentence representation t"
R19-1029,C18-1284,0,0.0895367,"use this information by performing multi-class classification over the toxicity labels: fake news, sensations, hate speech, conspiracies, anti-democratic, pro-authoritarian, defamation, delusion. Note that we allow multiple of these labels simultaneously. We further add a non-toxic label for articles that represent good news. 2 2.1 Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016, 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018b; Zlatkova et al., 2019). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation o"
R19-1029,S19-2147,0,0.0229432,"ews (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018a, 2019; Dinkov et al., 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015a; Kulkarni et al., 2018; Baly et al., 2018a; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. 2.2 Stance Detection Stance detection has been addressed as a task in its own right, where models have been developed b"
R19-1029,karadzhov-etal-2017-built,1,0.935048,", pro-authoritarian, defamation, delusion. Note that we allow multiple of these labels simultaneously. We further add a non-toxic label for articles that represent good news. 2 2.1 Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016, 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018b; Zlatkova et al., 2019). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016;"
R19-1029,karadzhov-etal-2017-fully,1,0.901729,"Missing"
R19-1029,K15-1032,1,0.898145,"tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018a, 2019; Dinkov et al., 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015a; Kulkarni et al., 2018; Baly et al., 2018a; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on F"
R19-1029,C18-1288,0,0.0291537,"by performing multi-class classification over the toxicity labels: fake news, sensations, hate speech, conspiracies, anti-democratic, pro-authoritarian, defamation, delusion. Note that we allow multiple of these labels simultaneously. We further add a non-toxic label for articles that represent good news. 2 2.1 Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016, 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018b; Zlatkova et al., 2019). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has"
R19-1029,R15-1058,1,0.92051,"tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018a, 2019; Dinkov et al., 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015a; Kulkarni et al., 2018; Baly et al., 2018a; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on F"
R19-1029,D18-1388,0,0.0266865,"t published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018a, 2019; Dinkov et al., 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015a; Kulkarni et al., 2018; Baly et al., 2018a; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERif"
R19-1029,P16-2065,1,0.860619,"lgarian that targets toxicity. In particular, (Karadzhov et al., 2017a) built a fake news and click-bait detector for Bulgarian based on data from a hackaton. While most of the above research has focused on isolated and specific task (such as trustworthiness, fake news, fact-checking), here we try to create a holistic approach by exploring several toxic and non-toxic labels simultaneously. 1 User modeling in social media and news community forums has focused on finding malicious users such as opinion manipulation trolls, paid (Mihaylov et al., 2015b) or just perceived (Mihaylov et al., 2015a; Mihaylov and Nakov, 2016; Mihaylov et al., 2018; Mihaylova et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017a). 2 We show in parentheses, the labels from opensources.co that are used to define a category. 249 Characteristic In addition to this dataset of only toxic articles, we added some “non-toxic” articles, fetched from media without toxicity examples in Media Scan: we added a total of 96 articles from 25 media. Table 1 shows statistics about the dataset, and Figure 1 shows the distribution of the labels. Value Toxic articles Non-toxic"
R19-1029,S19-2149,1,0.838315,"l., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. 2.2 Stance Detection Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018b; Mohtarami et al., 2019). 248 2.3 Source Reliability Estimation They found that “fake news” pack a lot of information in the title (as the focus is on"
R19-1029,N18-1070,1,0.874937,"Missing"
R19-1029,P17-1066,0,0.0352547,"Missing"
R19-1029,D19-1452,1,0.787323,"ion and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. 2.2 Stance Detection Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018b; Mohtarami et al., 2019). 248 2.3 Source Reliability Estimation They found that “fake news” pack a lot of information in the title (as the focus is on users who do not read beyond the title), and use shorter, simpler, and repetitive content in the body (as writing fake information takes a lot of effort). Thus, they argued that the title and the body should be analyzed separately. In follow-up work, Horne et al. (2018b) created a large-scale dataset covering 136K articles from 92 sources from opensources.co, which they characterize based on 130 features from seven categories: structural, sentiment, engagement, topicde"
R19-1029,P13-1162,0,0.0360697,"251 4.7 5 ElMo Next, we use deep contextualized word representations from ElMo, which uses generative bidirectional language model pre-training (Peters et al., 2018). The model yields 1024-dimensional representation, which we generate separately for the article title and for its body. 4.8 5.1 Finally, we use features from the NELA toolkit (Horne et al., 2018a), which were previously shown useful for detecting fake news, political bias, etc. The toolkit implements 129 features, which we extract separately for the article title and for its body: 5.2 • Sentiment: sentiment scores using lexicons (Recasens et al., 2013; Mitchell et al., 2013) and full systems (Hutto and Gilbert, 2014); • Topic: lexicon features to differentiate between science topics and personal concerns; • Complexity: type-token ratio, readability, number of cognitive process words (identifying discrepancy, insight, certainty, etc.); • Bias: features modeling bias (Recasens et al., 2013; Mukherjee and Weikum, 2015) and subjectivity (Horne et al., 2017); • Morality: features based on the Moral Foundation Theory (Graham et al., 2009) and lexicons (Lin et al., 2018) A summary of all features is shown in Table 2. Body BERT ElMO LSA NELA Stylo"
R19-1029,C18-1287,0,0.0597881,"Missing"
R19-1029,W17-4214,0,0.0314962,"sayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. 2.2 Stance Detection Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018b; Mohtarami et al., 2019). 248 2.3 Source Reliability Estimation They found that “fake news” pack a lot of information in the title (as the focus is on users who do not read beyond the title), and use shorter, simpler, and repetitive content in the body (as writing fake information takes a lot of effort). Thus, they argued that the title and the body should be analyze"
R19-1029,N18-1202,0,0.0403443,"We passed the model the first 300 tokens for each title or body to generate 512-dimensional vectors. • sentence count text: number of sentences in the article; • avg sentence length char text: average length of the sentences in the article body, in terms of characters; • avg sentence length word text: average length of the sentences in the article body, in terms of words; 5 http://www.alexa.com/ http://tfhub.dev/google/ universal-sentence-encoder/2 6 251 4.7 5 ElMo Next, we use deep contextualized word representations from ElMo, which uses generative bidirectional language model pre-training (Peters et al., 2018). The model yields 1024-dimensional representation, which we generate separately for the article title and for its body. 4.8 5.1 Finally, we use features from the NELA toolkit (Horne et al., 2018a), which were previously shown useful for detecting fake news, political bias, etc. The toolkit implements 129 features, which we extract separately for the article title and for its body: 5.2 • Sentiment: sentiment scores using lexicons (Recasens et al., 2013; Mitchell et al., 2013) and full systems (Hutto and Gilbert, 2014); • Topic: lexicon features to differentiate between science topics and perso"
R19-1029,C18-1283,0,0.0134021,"basis, especially in social media. As people have limited time to spend reading news, capturing people’s attention is getting ever harder. Media have to use various techniques to get their readers back such as bigger advertisement and better services. 247 Proceedings of Recent Advances in Natural Language Processing, pages 247–258, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_029 The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey (Thorne and Vlachos, 2018) took a fact-checking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), and it covered truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the proliferation of true and false news online. The veracity of information has been studied at different levels: (i) claim (e.g., factchecking), (ii) article (e.g., “fake news” detection), (iii) user (e.g., hunting for trolls), and (iv) medium"
R19-1029,N18-1074,0,0.0181519,"al., 2018a; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. 2.2 Stance Detection Stance detection has been addressed as a task in its own right, where models have been developed based on data from the Fake News Challenge (Riedel et al., 2017; Thorne et al., 2017; Mohtarami et al., 2018; Hanselowski et al., 2018), or from SemEval-2017 Task 8 (Derczynski et al., 2017; Dungs et al., 2018). It has also been studied for other languages such as Arabic (Darwish et al., 2017b; Baly et al., 2018b; Mohtarami et al., 2019). 248 2.3 Source Re"
R19-1029,D19-3038,1,0.88919,"Missing"
R19-1029,P18-1022,0,0.153854,"b; Mihaylova et al., 2018; Baly et al., 2018b; Zlatkova et al., 2019). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018a, 2019; Dinkov et al., 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015a; Kulkarni et al., 2018; Baly et al., 2018a; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs ("
R19-1029,D19-1216,1,0.757584,"e of these labels simultaneously. We further add a non-toxic label for articles that represent good news. 2 2.1 Fact-Checking At the claim-level, fact-checking and rumor detection have been primarily addressed using information extracted from social media, i.e., based on how users comment on the target claim (Canini et al., 2011; Castillo et al., 2011; Ma et al., 2016, 2017; Dungs et al., 2018; Kochkina et al., 2018). The Web has also been used as a source of information (Mukherjee and Weikum, 2015; Popat et al., 2016, 2017; Karadzhov et al., 2017b; Mihaylova et al., 2018; Baly et al., 2018b; Zlatkova et al., 2019). In both cases, the most important information sources are stance (does a tweet or a news article agree or disagree with the claim?), and source reliability (do we trust the user who posted the tweet or the medium that published the news article?). Other important sources are linguistic expression, meta information, and temporal dynamics. Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018a, 2019; Di"
R19-1029,D17-1317,0,\N,Missing
R19-1053,P17-1171,0,0.42646,"ter understanding the question, we typically depend on our background knowledge, and on relevant information from external sources. 447 Proceedings of Recent Advances in Natural Language Processing, pages 447–459, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_053 Finally, we address the resource scarceness in low-resource languages and the absence of question contexts in our dataset by extracting relevant passages from Wikipedia articles. These datasets brought a variety of models and approaches. The usage of external knowledge has been an interesting topic, e.g., Chen et al. (2017a) used Wikipedia knowledge for answering opendomain questions, Pan et al. (2018) applied entity discovery and linking as a source of prior knowledge. Sun et al. (2019b) explored different reading strategies such as back and forth reading, highlighting, and self-assessment. Ni et al. (2019) focused on finding essential terms and removing distraction words, followed by reformulation of the question, in order to find better evidence before sending a query to the MRC system. A simpler approach was presented by Clark et al. (2016), who leveraged information retrieval, corpus statistics, and simple"
R19-1053,P17-1176,0,0.111093,"ter understanding the question, we typically depend on our background knowledge, and on relevant information from external sources. 447 Proceedings of Recent Advances in Natural Language Processing, pages 447–459, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_053 Finally, we address the resource scarceness in low-resource languages and the absence of question contexts in our dataset by extracting relevant passages from Wikipedia articles. These datasets brought a variety of models and approaches. The usage of external knowledge has been an interesting topic, e.g., Chen et al. (2017a) used Wikipedia knowledge for answering opendomain questions, Pan et al. (2018) applied entity discovery and linking as a source of prior knowledge. Sun et al. (2019b) explored different reading strategies such as back and forth reading, highlighting, and self-assessment. Ni et al. (2019) focused on finding essential terms and removing distraction words, followed by reformulation of the question, in order to find better evidence before sending a query to the MRC system. A simpler approach was presented by Clark et al. (2016), who leveraged information retrieval, corpus statistics, and simple"
R19-1053,P17-1147,0,0.204168,"r for Bulgarian Momchil Hardalov1 Ivan Koychev1 Preslav Nakov2 1 Sofia University “St. Kliment Ohridski”, Bulgaria, 2 Qatar Computing Research Institute, HBKU, Qatar, {hardalov, koychev}@fmi.uni-sofia.bg pnakov@hbku.edu.qa Abstract Machines do not have the reasoning ability of humans, but they are still able to learn concepts. The growing interest in teaching machines to answer questions posed in natural language has led to the introduction of various new datasets for different tasks such as reading comprehension, both extractive, e.g., span-based (Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019), and non-extractive, e.g., multiple-choice questions (Richardson et al., 2013; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019a). Recent advances in neural network architectures, especially the raise of the Transformer (Vaswani et al., 2017), and better contextualization of language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; Grave et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019b; Dai et al., 2019) offered new opportunities to advance the field. Here, we investi"
R19-1053,K17-1024,1,0.843279,"Models Multilingual embeddings helped researchers to achieve new state-of-the-art results on many NLP tasks. While many pre-trained model (Grave et al., 2018; Devlin et al., 2019; Lample and Conneau, 2019) are available, the need for task-specific data in the target language still remains. Learning such models is language-independent, and representations for common words remain close in the latent vector space for a single language, albeit unrelated for different languages. A possible approach to overcome this effect is to learn an alignment function between spaces (Artetxe and Schwenk, 2018; Joty et al., 2017). 1 The dataset and the source code are available at http: //github.com/mhardalov/bg-reason-BERT 448 3.1 Moreover, zero-shot application of fine-tuned multilingual language models (Devlin et al., 2019; Lample and Conneau, 2019) on XNLI (Conneau et al., 2018), a corpus containing sentence pairs annotated with textual entailment and translated into 14 languages, has shown very close results to such by a language-specific model. Zero-shot transfer and multilingual models had been a hot topic in (neural) machine translation (MT) in the past several years. Johnson et al. (2017) introduced a simple"
R19-1053,D17-1082,0,0.408728,"Institute, HBKU, Qatar, {hardalov, koychev}@fmi.uni-sofia.bg pnakov@hbku.edu.qa Abstract Machines do not have the reasoning ability of humans, but they are still able to learn concepts. The growing interest in teaching machines to answer questions posed in natural language has led to the introduction of various new datasets for different tasks such as reading comprehension, both extractive, e.g., span-based (Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019), and non-extractive, e.g., multiple-choice questions (Richardson et al., 2013; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019a). Recent advances in neural network architectures, especially the raise of the Transformer (Vaswani et al., 2017), and better contextualization of language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; Grave et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019b; Dai et al., 2019) offered new opportunities to advance the field. Here, we investigate skill transfer from a highresource language, i.e., English, to a low-resource one, i.e., Bulgarian, for the task of multiple-choice read"
R19-1053,D18-1269,0,0.187367,"ment. Ni et al. (2019) focused on finding essential terms and removing distraction words, followed by reformulation of the question, in order to find better evidence before sending a query to the MRC system. A simpler approach was presented by Clark et al. (2016), who leveraged information retrieval, corpus statistics, and simple inference over a semi-automatically constructed knowledge base for answering fourthgrade science questions. Current state-of-the-art approaches in machine reading comprehension are grounded on transfer learning and fine-tuning of language models (Peters et al., 2018; Conneau et al., 2018; Devlin et al., 2019). Yang et al. (2019a) presented an opendomain extractive reader based on BERT (Devlin et al., 2019). Radford et al. (2018) used generative pre-training of a Transformer (Vaswani et al., 2017) as a language model, transferring it to downstream tasks such as natural language understanding, reading comprehension, etc. Finally, there has been a Bulgarian MRC dataset (Peñas et al., 2012). It was used by Simov et al. (2012), who converted the question-answer pairs to declarative sentences, and measured their similarity to the context, transforming both to a bag of linguistic un"
R19-1053,P19-1285,0,0.055792,"Missing"
R19-1053,D18-1260,0,0.341166,"chev}@fmi.uni-sofia.bg pnakov@hbku.edu.qa Abstract Machines do not have the reasoning ability of humans, but they are still able to learn concepts. The growing interest in teaching machines to answer questions posed in natural language has led to the introduction of various new datasets for different tasks such as reading comprehension, both extractive, e.g., span-based (Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019), and non-extractive, e.g., multiple-choice questions (Richardson et al., 2013; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019a). Recent advances in neural network architectures, especially the raise of the Transformer (Vaswani et al., 2017), and better contextualization of language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; Grave et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019b; Dai et al., 2019) offered new opportunities to advance the field. Here, we investigate skill transfer from a highresource language, i.e., English, to a low-resource one, i.e., Bulgarian, for the task of multiple-choice reading comprehension. Most previous work (Pan"
R19-1053,N19-1423,0,0.455799,"language has led to the introduction of various new datasets for different tasks such as reading comprehension, both extractive, e.g., span-based (Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019), and non-extractive, e.g., multiple-choice questions (Richardson et al., 2013; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019a). Recent advances in neural network architectures, especially the raise of the Transformer (Vaswani et al., 2017), and better contextualization of language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; Grave et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019b; Dai et al., 2019) offered new opportunities to advance the field. Here, we investigate skill transfer from a highresource language, i.e., English, to a low-resource one, i.e., Bulgarian, for the task of multiple-choice reading comprehension. Most previous work (Pan et al., 2018; Radford et al., 2018; Tay et al., 2018; Sun et al., 2019b) was monolingual, and a relevant context for each question was available a priori. We take the task a step further by exploring the capability of a neur"
R19-1053,Q19-1014,0,0.342608,"pnakov@hbku.edu.qa Abstract Machines do not have the reasoning ability of humans, but they are still able to learn concepts. The growing interest in teaching machines to answer questions posed in natural language has led to the introduction of various new datasets for different tasks such as reading comprehension, both extractive, e.g., span-based (Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019), and non-extractive, e.g., multiple-choice questions (Richardson et al., 2013; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019a). Recent advances in neural network architectures, especially the raise of the Transformer (Vaswani et al., 2017), and better contextualization of language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; Grave et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019b; Dai et al., 2019) offered new opportunities to advance the field. Here, we investigate skill transfer from a highresource language, i.e., English, to a low-resource one, i.e., Bulgarian, for the task of multiple-choice reading comprehension. Most previous work (Pan et al., 2018; Radf"
R19-1053,N19-1270,0,0.0359852,"Missing"
R19-1053,N18-1202,0,0.140014,"ions posed in natural language has led to the introduction of various new datasets for different tasks such as reading comprehension, both extractive, e.g., span-based (Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019), and non-extractive, e.g., multiple-choice questions (Richardson et al., 2013; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019a). Recent advances in neural network architectures, especially the raise of the Transformer (Vaswani et al., 2017), and better contextualization of language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; Grave et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019b; Dai et al., 2019) offered new opportunities to advance the field. Here, we investigate skill transfer from a highresource language, i.e., English, to a low-resource one, i.e., Bulgarian, for the task of multiple-choice reading comprehension. Most previous work (Pan et al., 2018; Radford et al., 2018; Tay et al., 2018; Sun et al., 2019b) was monolingual, and a relevant context for each question was available a priori. We take the task a step further by exploring the"
R19-1053,W17-2623,0,0.289803,"hot Multilingual Transfer for Bulgarian Momchil Hardalov1 Ivan Koychev1 Preslav Nakov2 1 Sofia University “St. Kliment Ohridski”, Bulgaria, 2 Qatar Computing Research Institute, HBKU, Qatar, {hardalov, koychev}@fmi.uni-sofia.bg pnakov@hbku.edu.qa Abstract Machines do not have the reasoning ability of humans, but they are still able to learn concepts. The growing interest in teaching machines to answer questions posed in natural language has led to the introduction of various new datasets for different tasks such as reading comprehension, both extractive, e.g., span-based (Nguyen et al., 2016; Trischler et al., 2017; Joshi et al., 2017; Rajpurkar et al., 2018; Reddy et al., 2019), and non-extractive, e.g., multiple-choice questions (Richardson et al., 2013; Lai et al., 2017; Clark et al., 2018; Mihaylov et al., 2018; Sun et al., 2019a). Recent advances in neural network architectures, especially the raise of the Transformer (Vaswani et al., 2017), and better contextualization of language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2018; Grave et al., 2018; Howard and Ruder, 2018; Radford et al., 2019; Yang et al., 2019b; Dai et al., 2019) offered new opportunities to advance the fie"
R19-1053,N19-4013,0,0.0943229,"sential terms and removing distraction words, followed by reformulation of the question, in order to find better evidence before sending a query to the MRC system. A simpler approach was presented by Clark et al. (2016), who leveraged information retrieval, corpus statistics, and simple inference over a semi-automatically constructed knowledge base for answering fourthgrade science questions. Current state-of-the-art approaches in machine reading comprehension are grounded on transfer learning and fine-tuning of language models (Peters et al., 2018; Conneau et al., 2018; Devlin et al., 2019). Yang et al. (2019a) presented an opendomain extractive reader based on BERT (Devlin et al., 2019). Radford et al. (2018) used generative pre-training of a Transformer (Vaswani et al., 2017) as a language model, transferring it to downstream tasks such as natural language understanding, reading comprehension, etc. Finally, there has been a Bulgarian MRC dataset (Peñas et al., 2012). It was used by Simov et al. (2012), who converted the question-answer pairs to declarative sentences, and measured their similarity to the context, transforming both to a bag of linguistic units: lemmata, POS tags, and dependency re"
R19-1053,D13-1020,0,\N,Missing
R19-1053,D19-5804,0,\N,Missing
R19-1053,N19-1388,0,\N,Missing
R19-1053,N19-1030,0,\N,Missing
R19-1053,Q19-1016,0,\N,Missing
R19-1127,N19-1078,0,0.113306,"Missing"
R19-1127,Q17-1010,0,0.0765556,"e such as Bulgarian using POS and grammatical information can improve the results. Thus, we mix automatically learned features — the word and the character embeddings —, with hand-crafted features encoded as a grammatical vector. In the rest of this section, we describe the different components of our system. LSTM-CRF Implementation For the implementation of the general LSTM-CRF architecture, we use Tensorflow (Sak et al., 2014). Word Embedding Nowadays there are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to produce character embeddings, we us"
R19-1127,Q16-1026,0,0.0296897,"nguage and per entity types have been expected. Such a task, however, is also good motivation for improving the NER systems for Slavic languages, including Bulgarian. There is some previous work on NER for Bulgarian. Georgiev et al. (2009) presented a model using Conditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relevant lexicons. For German, Gillick et al. (2016) achieved an F1 score of 82.84, which shows that the rich morphology causes a drop in the performance. Currently, the prevalent paradigm in NLP is to use neural networks, typically based on LSTMs or CNNs. As we have mentioned above, Lample et al. (2016) proposed an LSTM-CRF model for NER.3 The model uses a bi-directional LSTM to encode"
R19-1127,N19-1423,0,0.165751,"are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to produce character embeddings, we use a bidirectional LSTM over the character representation of the text. For each character in the text, each of the two LSTMs produces an hidden vector. For each word, the hidden vector for the last character produced by the left-to-right LSTM models information about the suffix of the word. Similarly, the hidden vector for the first character produced by the right-to-left LSTM models information about the prefix of the word. Following the approach, used in"
R19-1127,doddington-etal-2004-automatic,0,0.0193656,"12). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embeddings for the numerous word forms involved in multiword names that require additional grammatical knowledge in order to be processed properly. Here we incorporate such information as additional input to our neural model. 1 Other schemata such as ACE (Doddington et al., 2004) used a richer inventory of entity types. 1104 Proceedings of Recent Advances in Natural Language Processing, pages 1104–1113, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_127 Our contributions are as follows: Our work is based on Bulgarian, but we claim that it is appropriate also for other languages with rich morphological systems like Slavic and Romance languages, for example. For that reason, we present first the best results for NER in other Slavic languages having in mind that they are synthetic, while Bulgarian is a predominantly analytic language whose mor"
R19-1127,R09-1022,1,0.791678,"Slavic languages (Bulgarian, Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian), their normalization/lemmatization as well as cross-lingual linking. Our evaluation on NER in this paper is more similar to the relaxed evaluation parameter where the string is detected and classified, not the invariant. Considering the complexity of the task, the drop of the results per language and per entity types have been expected. Such a task, however, is also good motivation for improving the NER systems for Slavic languages, including Bulgarian. There is some previous work on NER for Bulgarian. Georgiev et al. (2009) presented a model using Conditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relev"
R19-1127,N16-1155,0,0.0254207,"onditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relevant lexicons. For German, Gillick et al. (2016) achieved an F1 score of 82.84, which shows that the rich morphology causes a drop in the performance. Currently, the prevalent paradigm in NLP is to use neural networks, typically based on LSTMs or CNNs. As we have mentioned above, Lample et al. (2016) proposed an LSTM-CRF model for NER.3 The model uses a bi-directional LSTM to encode the left and the right context of the current input word. Then it passes the concatenation of the two hidden vectors (one produced by the left LSTM and one by the right LSTM) to a CRF model. Its task is to ensure the global consistency of the NER tags. 2 http://"
R19-1127,C96-1079,0,0.665641,"al role in the processing of texts with application to many real-world Natural Language Processing (NLP) tasks such as Question Answering, Information Extraction, Machine Translation, Dialog Systems, and chatbots, where it is sometimes called Concept Segmentation and Labeling (Saleh et al., 2014). Preslav Nakov Qatar Computing Research Institute, HBKU Qatar pnakov@qf.org.qa Traditionally, NER has focused on recognizing entities such as person (PER), organization (ORG), location (LOC), and miscellaneous (MISC). This tradition goes back to the Message Understanding Conference (MUC) for English (Grishman and Sundheim, 1996), and the subsequent CoNLL 2002/2003 shared tasks, which also targeted other European Languages such as Spanish, Dutch, and German (Tjong Kim Sang and De Meulder, 2003).1 This same setup was followed in more recent work for a number of other languages, and we also follow it in the present work. Early systems relied on hand-crafted rules with pattern-matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatica"
R19-1127,N16-1030,0,0.578157,"matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatically (Popescu and Etzioni, 2005). Adding such resources required special approaches to resolve the ambiguity between names and common words. Such problems were solved using models such as Hidden Markov Models (Zhou and Su, 2002) and Conditional Random Fields (Sutton and McCallum, 2012). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embeddings for the numerous word forms involved in multiword names that require additional grammatical knowledge in order to be processed properly. Here we incorporate such information as additional input to our neural model. 1 Other schemata such as ACE (Doddington et al., 2004) used a richer inventory of entity types. 1104 Proceedings of Rece"
R19-1127,N10-1000,0,0.104846,"Missing"
R19-1127,D15-1176,0,0.0991376,"Missing"
R19-1127,P19-1441,0,0.0262885,"elp identify loan words in Bulgarian. Another promising research direction is to compare the differences in the graphical representation of named entities in Bulgarian and English. For example, in English all components of a named entity are capitalized (except for the functional words). In order to have comparable data, we envision to pre-transform the Bulgarian dataset to which to apply the English capitalization rule for the phrasal named entities. Finally, we plan to experiment with different monolingual representations from ElMo (Peters et al., 2018), BERT (Devlin et al., 2019), ROBERTa (Liu et al., 2019c), XLNet (Yang et al., 2019), and Ernie 2.0 (Sun et al., 2019), pooled representations from Flair (Akbik et al., 2019), distilled representations from MT-DNN (Liu et al., 2019a,b) or cross-language representations from XLM (Lample and Conneau, 2019). 8 Acknowledgements This research was partially supported by the Bulgarian National Interdisciplinary Research eInfrastructure for Resources and Technologies in favor of the Bulgarian Language and Cultural Heritage, part of the EU infrastructures CLARIN and DARIAH – CLaDA-BG, Grant number DO01164/28.08.2018 We would like to thank the anonymous rev"
R19-1127,2021.ccl-1.108,0,0.0979666,"Missing"
R19-1127,W14-1609,0,0.0301426,"ving the NER systems for Slavic languages, including Bulgarian. There is some previous work on NER for Bulgarian. Georgiev et al. (2009) presented a model using Conditional Random Fields with several hand-crafted features. They combined wellestablished features used for other languages with language-specific lexical, syntactic, and morphological information. Their result is the previous state-of-the-art for Bulgarian. So far, the highest reported results for NER are for English. For example, Chiu and Nichols (2016) reported an F1 score of 91.20 using BiLSTM + CNN + gazetteers + linking, while Passos et al. (2014) achieved an F1 score of 90.90 using a new form of learning word embeddings that can leverage information from relevant lexicons. For German, Gillick et al. (2016) achieved an F1 score of 82.84, which shows that the rich morphology causes a drop in the performance. Currently, the prevalent paradigm in NLP is to use neural networks, typically based on LSTMs or CNNs. As we have mentioned above, Lample et al. (2016) proposed an LSTM-CRF model for NER.3 The model uses a bi-directional LSTM to encode the left and the right context of the current input word. Then it passes the concatenation of the t"
R19-1127,D14-1162,0,0.0883647,"t for a morphologically rich language such as Bulgarian using POS and grammatical information can improve the results. Thus, we mix automatically learned features — the word and the character embeddings —, with hand-crafted features encoded as a grammatical vector. In the rest of this section, we describe the different components of our system. LSTM-CRF Implementation For the implementation of the general LSTM-CRF architecture, we use Tensorflow (Sak et al., 2014). Word Embedding Nowadays there are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to"
R19-1127,N18-1202,0,0.300472,". Word Embedding Nowadays there are many different approaches to train word vectors such as Word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), FastText (Bojanowski et al., 2017), and many more. In our experiments, we use the pre-trained Bulgarian word embeddings from FastText (Bojanowski et al., 2017).4 This choice was motivated by the fact that FastText uses the structure of the words by taking into consideration character n-grams, thus modeling morphology and many out-of-vocabulary words. 4 In this work, we do not use any contextualization of the word embeddings such as ElMo (Peters et al., 2018) and BERT (Devlin et al., 2019), as our Bi-LSTM architecture already performs contextualization. Character Bi-LSTM Embedding In order to produce character embeddings, we use a bidirectional LSTM over the character representation of the text. For each character in the text, each of the two LSTMs produces an hidden vector. For each word, the hidden vector for the last character produced by the left-to-right LSTM models information about the suffix of the word. Similarly, the hidden vector for the first character produced by the right-to-left LSTM models information about the prefix of the word."
R19-1127,D17-1283,0,0.0130415,"s are trained in an unsupervised manner on external data, while the characterbased LSTM embeddings are trained on the training data as part of the end-to-end training of the full LSTM-CRF model. This model does not need any explicit feature engineering nor does it need manual gazetteers; yet, it achieved state-of-the-art performance for four languages: English, German, Dutch, and Spanish. Here we take this model as a basis, and we augment it to model part-of-speech (POS) and grammatical information, which turns out to be very important for a morphologically complex language such as Bulgarian. Strubell et al. (2017) extended the above model by substituting the LSTM with Iterated Dilated Convolutional Neural Networks, a variant of CNN, which permit fixed-depth convolutions to run in parallel across entire documents, thus making use of GPUs, which yields up to 20-fold speed up, while retaining performance comparable to that of the LSTM-CRF model. They further aggregated context from the entire input document, which they found to be helpful. In our preliminary monolingual experiments, this model performed very similarly, but slightly worse, than the LSTMCRF model, and thus we chose LSTM-CRF for our experime"
R19-1127,W19-3709,0,0.54809,"Missing"
R19-1127,W17-1412,0,0.013574,"Markov Model. The feature modeling also proved to be working in Czech, as their best results used features based on morphological analysis, two-stage prediction, word clustering, and gazetteers. For Polish, Piskorski et al. (2004) achieved precision of 91.0, recall of 77.5, and F1 score of 82.4. They used the SProUT system, which is an NLP platform, consisting of pattern/action rules. In the last years, the interest in NER for Slavic languages grew. Two shared tasks were organized —- the first and the second Multilingual Named Entity Challenge in Slavic Languages. They have been descibed in (Piskorski et al., 2017) and (Piskorski et al., 2019). The challenges included several tasks: recognition of mentions of named entities in Web documents in seven Slavic languages (Bulgarian, Croatian, Czech, Polish, Russian, Slovak, Slovene, Ukrainian), their normalization/lemmatization as well as cross-lingual linking. Our evaluation on NER in this paper is more similar to the relaxed evaluation parameter where the string is detected and classified, not the invariant. Considering the complexity of the task, the drop of the results per language and per entity types have been expected. Such a task, however, is also go"
R19-1127,W03-0419,0,0.829024,"Missing"
R19-1127,H05-1043,0,0.109539,"the subsequent CoNLL 2002/2003 shared tasks, which also targeted other European Languages such as Spanish, Dutch, and German (Tjong Kim Sang and De Meulder, 2003).1 This same setup was followed in more recent work for a number of other languages, and we also follow it in the present work. Early systems relied on hand-crafted rules with pattern-matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatically (Popescu and Etzioni, 2005). Adding such resources required special approaches to resolve the ambiguity between names and common words. Such problems were solved using models such as Hidden Markov Models (Zhou and Su, 2002) and Conditional Random Fields (Sutton and McCallum, 2012). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embed"
R19-1127,C14-1020,1,0.88617,"Missing"
R19-1127,P02-1060,0,0.180519,"ecent work for a number of other languages, and we also follow it in the present work. Early systems relied on hand-crafted rules with pattern-matching (Appelt et al., 1995). Unfortunately, this required an large pre-annotated datasets, collecting which was time-consuming and error-prone. The next step was to add gazetteers and lexicons that were generated automatically or semi-automatically (Popescu and Etzioni, 2005). Adding such resources required special approaches to resolve the ambiguity between names and common words. Such problems were solved using models such as Hidden Markov Models (Zhou and Su, 2002) and Conditional Random Fields (Sutton and McCallum, 2012). In our work here, we use deep neural networks for Bulgarian NER. Lample et al. (2016) have shown remarkable results for English, using a combination of Bi-LSTMs (Bi-directional Long Short-Term Memory) and CRF. However, the approach is problematic for morphologically rich languages. The main problem is the missing information within word embeddings for the numerous word forms involved in multiword names that require additional grammatical knowledge in order to be processed properly. Here we incorporate such information as additional in"
R19-1127,N03-1031,0,\N,Missing
R19-1127,H05-2017,0,\N,Missing
R19-1127,W02-2024,0,\N,Missing
R19-1141,D18-1389,1,0.867559,"d dataset. Section 4 describes our method and features. Section 5 presents the experiments and the evaluation results. Finally, Section 7 concludes and points to some possible directions for future work. 1229 Proceedings of Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 20"
R19-1141,N19-1216,1,0.885016,"Missing"
R19-1141,D19-1565,1,0.88107,"78-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answer"
R19-1141,P08-1118,0,0.0399865,"Missing"
R19-1141,S17-2006,0,0.0513316,"Missing"
R19-1141,P15-2139,0,0.0249286,"tweets and temporal information (Ma et al., 2016). We also want to explore other multi-task learning options, e.g., as described in (Ruder, 2017). Figure 2: Ablation experiment with the multi model. Each row is an experiment removing one target. Each column is the MAP difference with respect to the multi model for the corresponding target. It would be interesting to investigate the reasons why the NYT source does not benefit from the multi-task architecture. In order to adapt to this situation with a single model, we plan to experiment with a network with soft parameter sharing, e.g., as in (Duong et al., 2015). For example, we could create a chain of layers that back-propagate to the input using only single task targets and then add an auxiliary layer that is shared between the tasks on the side. In this way, the model would be able to turn off the multi-task learning completely for some of the sources. However, training such kind of model might require significantly more training data; semi-supervised training might be a possible solution. Acknowledgments We would like to thank the anonymous reviewer, whose constructive feedback has helped us improve the quality of this paper. This work is part of"
R19-1141,W09-0439,0,0.0126282,"CW-USPD-2016 corpus contains four debates, we perform 4-fold cross-validation, where each time we leave one debate out for testing, and we train on the remaining three debates. Moreover, in order to stabilize the results, we repeat each experiment three times with different random seeds and we report the average over these three reruns of the system.4 4 Having multiple reruns is a standard procedure to stabilize an optimization algorithm that is sensitive to the random seed, e.g., this strategy has been argued for when using MERT for tuning hyper-parameters in Statistical Machine Translation (Foster and Kuhn, 2009). In our neural model, we used ReLU units and a shared layer of size 300. For training, we used Stochastic Gradient Descent with Nesterov momentum,5 iterating for 100 epochs. Recall that our main objective is to prioritize the claims that should be selected for manual factchecking, which is best achieved by proposing a ranked list of claims. Thus, we have a ranking task, for which we use suitable information retrieval evaluation measures. In particular, we adopt Mean Average Precision (MAP) as our primary evaluation measure. We further report RPrecision, or R-Pr, and precision at k, or P@k,6 f"
R19-1141,gencheva-etal-2017-context,1,0.605665,"Missing"
R19-1141,S19-2147,0,0.0306601,"news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data min"
R19-1141,N18-5006,1,0.895733,"Missing"
R19-1141,J15-3002,0,0.0278902,"IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden layers. During training, in the process of backpropagation, each task modifies the weights of its own task-specific layer and also of the shared layer. 1231 Figure 1: The architecture of our neural multi-task learning model, predicting whether each of the nine individual fact-checking organizations (tasks) would consider this sentence check-worthy and one cumulati"
R19-1141,D18-1388,0,0.0308285,"tural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERifi"
R19-1141,W16-2117,0,0.0319884,"been collected. Thus, the task can be reduced to recognizing textual entailment (Dagan et al., 2009). 1230 de Marneffe et al. (2008) also looked for contradictions in text. They tried to classify the contradictions that can be found in a piece of text in two categories —those occurring via antonymy, negation, and date/number mismatch, and those arising from different world knowledge and lexical contrasts. The features that are selected for the task of contradiction detection include polarity, numbers, dates and time, antonymy, factivity, modality, structural, and relational features. Finally, Le et al. (2016) used deep learning. They argued that the top terms in claim vs. nonclaim sentences are highly overlapping in content, which is a problem for bag-of-words approaches. Thus, they used a Convolutional Neural Network, where each word is represented by its embedding and each named entity is replaced by its tag, e.g., person, organization, location. Unlike the above work, we mimic the selection strategy of one specific fact-checking organization by learning to jointly predict the selection choices by multiple such organizations. 3 Data In our experiments, we used the CW-USPD2016 dataset from our pr"
R19-1141,K15-1032,1,0.805395,"f Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fac"
R19-1141,S19-2149,1,0.839641,"l., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey (Thorne and Vlachos, 2018) took a factchecking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), and it covered truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the proliferation o"
R19-1141,N13-1090,0,0.0171527,"sk of checkworthiness prediction. In particular, from (Hassan et al., 2015b), we adopt TF.IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden layers. During training, in the process of backpropagation, each task modifies the weights of its own task-specific layer and also of the shared layer. 1231 Figure 1: The architecture of our neural multi-task learning model, predicting whether each of the nine individual fact-ch"
R19-1141,P18-1022,0,0.0483181,"related work. Section 3 describes the used dataset. Section 4 describes our method and features. Section 5 presents the experiments and the evaluation results. Finally, Section 7 concludes and points to some possible directions for future work. 1229 Proceedings of Recent Advances in Natural Language Processing, pages 1229–1239, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_141 2 Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Na"
R19-1141,P13-1162,0,0.0835321,"hether each of the nine individual sources (tasks) would have selected it, and whether at least one of them would, which is the special task ANY. The input to our neural network consists of various domain-specific features that have been previously shown to work well for the task of checkworthiness prediction. In particular, from (Hassan et al., 2015b), we adopt TF.IDF-weighted bag of words, part-of-speech tags, the presence of named entities, sentiment scores, and sentence length (in number of tokens). Moreover, from (Gencheva et al., 2017), we further adopt lexicon features, e.g., for bias (Recasens et al., 2013), for sentiment (Liu et al., 2005), for assertiveness (Hooper, 1974), and for subjectivity; structural features, e.g., for location of the sentence within the debate/intervention; LDA topics (Blei et al., 2003); word embeddings, pretrained on Google News (Mikolov et al., 2013); and discourse relations with respect to the neighboring sentences (Joty et al., 2015). See (Hassan et al., 2015b; Gencheva et al., 2017) for more details about each of these feature types. After the input layer, comes a hidden layer that is shared between all tasks. It is followed by ten parallel task-specific hidden la"
R19-1141,C18-1283,0,0.0892411,"d et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova et al., 2019), among others. The interested reader can learn more about “fake news” from the overview by Shu et al. (2017), which adopted a data mining perspective and focused on social media. Another recent survey (Thorne and Vlachos, 2018) took a factchecking perspective on “fake news” and related problems. Yet another survey was performed by Li et al. (2016), and it covered truth discovery in general. Moreover, there were two recent articles in Science: Lazer et al. (2018) offered a general overview and discussion on the science of “fake news”, while Vosoughi et al. (2018) focused on the proliferation of true and false news online. The first work to target check-worthiness estimation, i.e., predicting which sentences in a given input text should be prioritized for factchecking, was the ClaimBuster system (Hassan et al., 2015b)"
R19-1141,N18-1074,0,0.0902678,"Missing"
R19-1141,W14-2508,0,0.076833,"oint that attracted wide public attention to the problem. By then, a number of organizations, e.g., FactCheck1 and Snopes2 among many others, launched factchecking initiatives. Yet, this proved to be a very demanding manual effort, and only a relatively small number of claims could be fact-checked. Thus, it is important to prioritize what to check. 1 2 http://www.factcheck.org/ http://www.snopes.com/ Llu´ıs M`arquez Amazon Core ML lluismv@amazon.com The task of detecting check-worthy claims has been recognized as an important stage in the process of fully automatic fact-checking. According to Vlachos and Riedel (2014) this is a multistep process that (i) extracts statements to be fact-checked, (ii) constructs appropriate questions, (iii) obtains the answers from relevant sources, and (iv) reaches a verdict using these answers. Hassan et al. (2015a) presented a similar vision, and in a follow up work they made check-worthiness an integral part of an end-to-end fact-checking system Hassan et al. (2017). Here, we approach the problem of mimicking the selection strategy of several renowned fact-checking organizations such as PolitiFact, FactCheck, ABC, CNN, NPR, NYT, Chicago Tribune, The Guardian, and The Wash"
R19-1141,D19-3038,1,0.810868,"Related Work The proliferation of false information has attracted a lot of research interest recently. This includes challenging the truthiness of news (Brill, 2001; Hardalov et al., 2016; Potthast et al., 2018), of news sources (Baly et al., 2018, 2019), and of social media posts (Canini et al., 2011; Castillo et al., 2011; Zubiaga et al., 2016), as well as studying credibility, influence, bias, and propaganda (Ba et al., 2016; Chen et al., 2013; Mihaylov et al., 2015; Kulkarni et al., 2018; Baly et al., 2018; Mihaylov et al., 2018; Barr´on-Cede˜no et al., 2019; Da San Martino et al., 2019; Zhang et al., 2019). Research was facilitated by shared tasks such as the SemEval 2017 and 2019 tasks on Rumor Detection (Derczynski et al., 2017; Gorrell et al., 2019), the CLEF 2018 and 2019 CheckThat! labs (Nakov et al., 2018; Elsayed et al., 2019b,a), which featured tasks on automatic identification (Atanasova et al., 2018, 2019) and verification (Barr´on-Cede˜no et al., 2018; Hasanain et al., 2019) of claims in political debates, the FEVER 2018 and 2019 task on Fact Extraction and VERification (Thorne et al., 2018), and the SemEval 2019 task on Fact-Checking in Community Question Answering Forums (Mihaylova"
S07-1003,W04-3205,0,0.0491922,"ierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We hav"
S07-1003,C92-2082,0,0.0345238,"a variety of methods (since we work with relations between nominals, the part of speech is always noun). We have used WordNet 3.0 on the Web and sense index tags. We chose the following semantic relations: Cause-Effect, Content-Container, InstrumentAgency, Origin-Entity, Part-Whole, ProductProducer and Theme-Tool. We wrote seven detailed definitions, including restrictions and conventions, plus prototypical positive and near-miss negative examples. For each relation separately, we based data collection on wild-card search patterns that Google allows. We built the patterns manually, following Hearst (1992) and Nakov and Hearst (2006). Instances of the relation Content-Container, for example, come up in response to queries such as “* contains *”, “* holds *”, “the * in the *”. Following the model of the Senseval-3 English Lexical Sample Task, we set out to collect 140 training and at least 70 test examples per relation, so we had a number of different patterns to ensure variety. We also aimed to collect a balanced number of positive and negative examples. The use of heuristic patterns to search for both positive and negative examples 14 should naturally result in negative examples that are near"
S07-1003,J02-3004,0,0.0275234,"oun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work for all applications. For example, the gene-gene relation scheme of Stephens et al. (2001), with relations like X phosphorylates Y, is unlikely to be transferred easily to general text. We have created a benchmark data set to allow the evaluation of different semantic relation classification algorithms. We do not presume to propose a single classification scheme, however allurin"
S07-1003,W04-2609,1,0.951787,"cine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Chklovski and Pantel (2004) introduce a 5-class set, designed specifically for characterizing verb-verb semantic relations. Stephens et al. (2001) propose 17 classes targeted to relations between genes. Lapata (2002) presents a binary classification of relations in nominalizations. There is little consensus on the relation sets and algorithms for analyzing semantic relations, and it seems unlikely that any single scheme could work f"
S07-1003,W01-0511,0,0.263052,"cer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2"
S07-1003,P02-1032,0,0.0943661,"traction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Nastase and Szpakowicz (2003) present a two-level hierarchy for classifying noun-modifier relations in base noun phrases from general text, with 5 classes at the top and 30 classes at the bottom; other researchers (Turney and Littman, 2005; Turney, 2005; Nastase et al., 2006) have used their class scheme and data set. Moldovan et al. (2004) propose a 35class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and"
S07-1003,H05-1047,0,0.0359726,"their results. There were 14 teams who submitted 15 systems. 1 Task Description and Related Work The theme of Task 4 is the classification of semantic relations between simple nominals (nouns or base noun phrases) other than named entities – honey bee, for example, shows an instance of the ProductProducer relation. The classification occurs in the context of a sentence in a written English text. Algorithms for classifying semantic relations can be applied in information retrieval, information extraction, text summarization, question answering and so on. The recognition of textual entailment (Tatu and Moldovan, 2005) is an example of successful use of this type of deeper analysis in high-end NLP applications. The literature shows a wide variety of methods of nominal relation classification. They depend as much on the training data as on the domain of application and the available resources. Rosario and Hearst (2001) classify noun compounds from the domain of medicine, using 13 classes that describe the semantic relation between the head noun and the modifier in a given noun compound. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, w"
S07-1003,H91-1061,0,\N,Missing
S07-1080,P06-2064,0,0.0116933,"7. 2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compound can be characterized by the set of intervening verbs extracted from the Web. 3 Method Given an entity-annotated example sentence, we reduce the target entities e1 and e2 to single nouns noun1 and noun2 , by keeping their last nouns only, which we assume to be the heads. We then mine the Web for sentences containing both noun1 and noun2 , from which"
S07-1080,P02-1032,1,0.836987,"Missing"
S07-1080,P06-1040,0,0.0385558,"cattering and iterative seman366 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 366–369, c Prague, June 2007. 2007 Association for Computational Linguistics tic specialization, using WordNet, word sense disambiguation, and linguistic features. Barker and Szpakowicz (1998) propose a two-level hierarchy with 5 classes at the upper level and 30 at the lower level. Turney (2005) introduces latent relational analysis, which uses the Web, synonyms, patterns like “X for Y ”, “X such as Y ”, etc., and singular value decomposition to smooth the frequencies. Turney (2006) induces patterns from the Web, e.g. CAUSE is best characterized by “Y * causes X”, and “Y in * early X” is the best pattern for TEMPORAL. Kim and Baldwin (2006) propose to use a predefined set of seed verbs and multiple resources: WordNet, CoreLex, and Moby’s thesaurus. Finally, in a previous publication (Nakov and Hearst, 2006), we make the claim that the relation between the nouns in a noun-noun compound can be characterized by the set of intervening verbs extracted from the Web. 3 Method Given an entity-annotated example sentence, we reduce the target entities e1 and e2 to single nouns nou"
S07-1080,P98-1015,0,\N,Missing
S07-1080,C98-1015,0,\N,Missing
S10-1006,W09-1401,0,0.0287746,"for each semantic relation. Here, we describe the general guidelines, which delineate the scope of the data to be collected and state general principles relevant to the annotation of all relations.1 Our objective is to annotate instances of semantic relations which are true in the sense of holding in the most plausible truth-conditional interpretation of the sentence. This is in the tradition of the Textual Entailment or Information Validation paradigm (Dagan et al., 2009), and in contrast to “aboutness” annotation such as semantic roles (Carreras and M`arquez, 2004) or the BioNLP 2009 task (Kim et al., 2009) where negated relations are also labelled as positive. Similarly, we exclude instances of semantic relations which hold only in speculative or counterfactural scenarios. In practice, this means disallowing annotations within the scope of modals or negations, e.g., “Smoking may/may not have caused cancer in this case.” We accept as relation arguments only noun phrases with common-noun heads. This distinguishes our task from much work in Information Extraction, which tends to focus on specific classes of named entities and on considerably more finegrained relations than we do. Named entities ar"
S10-1006,W05-0620,0,\N,Missing
S10-1007,P07-1072,0,0.0565426,"ited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose a"
S10-1007,I05-1082,1,0.151111,"a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected dire"
S10-1007,P06-2064,1,0.272527,"(MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This pa"
S10-1007,E03-1073,0,0.0115181,"ample, malaria mosquito is a ‘mosquito that carries malaria’. Evaluating the quality of such paraphrases is the theme of Task 9 at SemEval-2010. This paper describes some background, the task definition, the process of data collection and the task results. We also venture a few general conclusions before the participating teams present their systems at the SemEval-2010 workshop. There were 5 teams who submitted 7 systems. 1 Introduction Noun compounds (NCs) are sequences of two or more nouns that act as a single noun,1 e.g., stem cell, stem cell research, stem cell research organization, etc. Lapata and Lascarides (2003) observe that NCs pose syntactic and semantic challenges for three basic reasons: (1) the compounding process is extremely productive in English; (2) the semantic relation between the head and the modifier is implicit; (3) the interpretation can be influenced by contextual and pragmatic factors. Corpus studies have shown that while NCs are very common in English, their frequency distribution follows a Zipfian or power-law distribution and the majority of NCs encountered will be rare types (Tanaka and Baldwin, 2003; Lapata and Lascarides, 2003; Bald´ S´eaghdha, 2008). As a win and Tanaka, 2004;"
S10-1007,C94-2125,0,0.271982,"Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edit"
S10-1007,W04-2609,0,0.051487,"ing, NC semantics plays a central role in complex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popular"
S10-1007,P08-1052,1,0.415224,"e, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-based NC semantics has encouraged us to propose an evaluation exercise for the 2010 edition of SemEval. This paper gives a bird’s-eye view of the task. Section 2"
S10-1007,W06-3813,1,0.819134,"lex knowledge discovery and applications, including but not limited to Question Answering (QA), Machine Translation (MT), and Information Retrieval (IR). For example, knowing the (implicit) semantic relation between the NC components can help rank and refine queries in QA and IR, or select promising translation pairs in MT (Nakov, 2008a). Thus, robust semantic interpretation of NCs should be of much help in broad-coverage semantic processing. Proposed approaches to modelling NC semantics have used semantic similarity (Nastase and Szpakowicz, 2003; Moldovan et al., 2004; Kim and Baldwin, 2005; Nastase and Szpakowicz, 2006; ´ S´eaghdha and Copestake, 2007) Girju, 2007; O and paraphrasing (Vanderwende, 1994; Kim and Baldwin, 2006; Butnariu and Veale, 2008; Nakov and Hearst, 2008). The former body of work seeks to measure the similarity between known and unseen NCs by considering various features, usually context-related. In contrast, the latter group uses verb semantics to interpret NCs directly, e.g., olive oil as ‘oil that is extracted from olive(s)’, drug death as ‘death that is caused by drug(s)’, flu shot as a ‘shot that prevents flu’. The growing popularity – and expected direct utility – of paraphrase-bas"
S10-1007,W07-1108,1,0.889705,"Missing"
S10-1007,D08-1027,0,0.00993176,"Missing"
S10-1007,C08-1011,1,\N,Missing
S10-1007,W03-1803,0,\N,Missing
S10-1007,W04-0404,0,\N,Missing
S10-1007,P84-1109,0,\N,Missing
S10-1007,W01-0511,0,\N,Missing
S13-2025,W04-0404,0,0.0532624,"Missing"
S13-2025,C08-1011,1,0.662917,"ages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve rich and specific meanings."
S13-2025,W09-2416,1,0.899683,"Missing"
S13-2025,P07-1072,0,0.203668,"Missing"
S13-2025,P06-2064,0,0.395087,"Missing"
S13-2025,W04-2609,0,0.0835979,"located in Geneva. 138 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on Semantic c Evaluation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE)"
S13-2025,P08-1052,1,0.483189,"uation (SemEval 2013), pages 138–143, Atlanta, Georgia, June 14-15, 2013. 2013 Association for Computational Linguistics Research on the automatic interpretation of NCs has focused mainly on common two-word NCs. The usual task is to classify the semantic relation underlying a compound with either one of a small number of predefined relation labels or a paraphrase from an open vocabulary. Examples of the former take on classification include (Moldovan et al., 2004; Girju, ´ S´eaghdha and Copestake, 2008; Tratz and 2007; O Hovy, 2010). Examples of the latter include (Nakov, 2008b; Nakov, 2008a; Nakov and Hearst, 2008; Butnariu and Veale, 2008) and a previous NC paraphrasing task at SemEval-2010 (Butnariu et al., 2010), upon which the task described here builds. The assumption of a small inventory of predefined relations has some advantages – parsimony and generalization – but at the same time there are limitations on expressivity and coverage. For example, the NCs headache pills and fertility pills would be assigned the same semantic relation (PURPOSE) in most inventories, but their relational semantics are quite different (Downing, 1977). Furthermore, the definitions given by human subjects can involve r"
S13-2025,E09-1071,1,0.900906,"Missing"
S13-2025,C08-1082,1,0.890902,"Missing"
S13-2025,W03-1803,0,0.161511,"Missing"
S13-2025,P10-1070,0,0.829879,"Missing"
S13-2052,baccianella-etal-2010-sentiwordnet,0,0.68035,",131 471 648 430 57 2,734 1,541 160 1,071 1,104 159 Vocabulary Size 20,012 4,426 11,736 3,562 Table 2: Statistics for Subtask A. We then identified popular topics as those named entities that are frequently mentioned in association with a specific date (Ritter et al., 2012). Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from training and spanned later periods. To identify messages that express sentiment towards these topics, we filtered the tweets using SentiWordNet (Baccianella et al., 2010). We removed messages that contained no sentimentbearing words, keeping only those with at least one word with positive or negative sentiment score that is greater than 0.3 in SentiWordNet for at least one sense of the words. Without filtering, we found class imbalance to be too high.3 Twitter messages are rich in social media features, including out-of-vocabulary (OOV) words, emoticons, and acronyms; see Table 1. A large portion of the OOV words are hashtags (e.g., #sheenroast) and mentions (e.g., @tash jade). Corpus Twitter - Training Twitter - Dev Twitter - Test SMS - Test Filtering based o"
S13-2052,C10-2005,0,0.783288,"sing beyond those encountered when working with more traditional text genres such as newswire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is c"
S13-2052,W10-2914,0,0.0363732,"ith more traditional text genres such as newswire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is conveyed, e.g., we can study the properties"
S13-2052,S10-1097,0,0.598998,"wire. Tweets and SMS messages are short in length: a sentence or a headline rather than a document. The language they use is very informal, with creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags.1 How to handle such challenges so as to automatically mine and understand the opinions and sentiments that people are communicating has only very recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Another aspect of social media data, such as Twitter messages, is that they include rich structured information about the individuals involved in the communication. For example, Twitter maintains information about who follows whom. Re-tweets (reshares of a tweet) and tags inside of tweets provide discourse information. Modeling such structured information is important because it provides means for empirically studying social interactions where opinion is conveyed, e.g., we can study the properties of persuasive language or those associated wit"
S13-2052,D11-1141,1,0.104433,"ity Classification Given a message, decide whether it is of positive, negative, or neutral sentiment. For messages conveying both a positive and a negative sentiment, whichever is the stronger one was to be chosen. http://www.daedalus.es/TASS/corpus.php 313 Dataset Creation In the following sections we describe the collection and annotation of the Twitter and SMS datasets. 3.1 Data Collection Twitter is the most common micro-blogging site on the Web, and we used it to gather tweets that express sentiment about popular topics. We first extracted named entities using a Twitter-tuned NER system (Ritter et al., 2011) from millions of tweets, which we collected over a one-year period spanning from January 2012 to January 2013; we used the public streaming Twitter API to download tweets. Instructions: Subjective words are ones which convey an opinion. Given a sentence, identify whether it is objective, positive, negative, or neutral. Then, identify each subjective word or phrase in the context of the sentence and mark the position of its start and end in the text boxes below. The number above each word indicates its position. The word/phrase will be generated in the adjacent textbox so that you can confirm"
S14-2009,baccianella-etal-2010-sentiwordnet,0,0.393694,"luding the test data) was used to develop a sentiment lexicon, and this lexicon was used to automatically label additional Tweet/SMS messages and then used with the original data to train the classifier, then such a system would be considered unconstrained. 3 Positive Positive Negative 3,662 575 1,572 492 982 33 427 1,466 340 601 394 202 40 304 Objective / Neutral 4,600 739 1,640 1,207 669 13 411 Table 2: Dataset statistics for Subtask B. 3.2 Annotation We annotated the new tweets as in 2013: by identifying tweets from popular topics that contain sentiment-bearing words by using SentiWordNet (Baccianella et al., 2010) as a filter. We altered the annotation task for the sarcastic tweets, displaying them to the Mechanical Turk annotators without the #sarcasm hashtag; the Turkers had to determine whether the tweet is sarcastic on their own. Moreover, we asked Turkers to indicate the degree of sarcasm as (a) definitely sarcastic, (b) probably sarcastic, and (c) not sarcastic. As in 2013, we combined the annotations using intersection, where a word had to appear in 2/3 of the annotations to be accepted. An annotated example from each source is shown in Table 3. Datasets In this section, we describe the process"
S14-2009,S13-2053,0,0.592137,"sometimes it was worse, sometimes it performed the same. Thus, we decided to produce a single ranking, including both constrained and unconstrained systems, where we mark the latter accordingly. 5.1 The features used were quite varied, including word-based (e.g., word and character ngrams, word shapes, and lemmata), syntactic, and Twitter-specific such as emoticons and abbreviations. The participants still relied heavily on lexicons of opinion words, the most popular ones being the same as in 2013: MPQA, SentiWordNet and Bing Liu’s opinion lexicon. Popular this year was also the NRC lexicon (Mohammad et al., 2013), created by the best-performing team in 2013, which is top-performing this year as well. Subtask A Table 4 shows the results for subtask A, which attracted 27 submissions from 21 teams. There were seven unconstrained submissions: five teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only. The best systems were constrained. All participating systems outperformed the majority class baseline by a sizable margin. 5.2 Preprocessing of tweets was still a popular technique. In addition to standard NLP steps such as tokenization, stemming, lemm"
S14-2009,C10-2005,0,0.376895,"Missing"
S14-2009,S13-2052,1,0.554296,"resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment. Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media. Toward that goal, we created the SemEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013). It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity. This year, we extended the corpus by adding new tweets and LiveJournal sentences. Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). In fact, sarcasm indicates that the message polarity should be flipped. With this in mind, this year, we also evaluate on sarcastic tweets. We describe the Sentiment Analysi"
S14-2009,W10-2914,0,0.0317872,"14 Task 9: Sentiment Analysis in Twitter Sara Rosenthal Columbia University Alan Ritter Carnegie Mellon University sara@cs.columbia.edu rittera@cs.cmu.edu Preslav Nakov Qatar Computing Research Institute Veselin Stoyanov Johns Hopkins University pnakov@qf.org.qa ves@cs.jhu.edu Abstract Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document. How to handle such challenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created"
S14-2009,S10-1097,0,0.139604,"Rosenthal Columbia University Alan Ritter Carnegie Mellon University sara@cs.columbia.edu rittera@cs.cmu.edu Preslav Nakov Qatar Computing Research Institute Veselin Stoyanov Johns Hopkins University pnakov@qf.org.qa ves@cs.jhu.edu Abstract Moreover, tweets and SMS messages are short: a sentence or a headline rather than a document. How to handle such challenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small a"
S14-2009,P11-2008,0,0.0644631,"Missing"
S14-2009,W02-1011,0,0.0331504,"er, tweets and SMS messages are short: a sentence or a headline rather than a document. How to handle such challenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment. Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conve"
S14-2009,S14-2004,0,0.0177631,"llenges so as to automatically mine and understand people’s opinions and sentiments has only recently been the subject of research (Jansen et al., 2009; Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; O’Connor et al., 2010; Pak and Paroubek, 2010; Tumasjan et al., 2010; Kouloumpis et al., 2011). Several corpora with detailed opinion and sentiment annotation have been made freely available, e.g., the MPQA newswire corpus (Wiebe et al., 2005), the movie reviews corpus (Pang et al., 2002), or the restaurant and laptop reviews corpora that are part of this year’s SemEval Task 4 (Pontiki et al., 2014). These corpora have proved very valuable as resources for learning about the language of sentiment in general, but they do not focus on tweets. While some Twitter sentiment datasets were created prior to SemEval-2013, they were either small and proprietary, such as the isieve corpus (Kouloumpis et al., 2011) or focused solely on message-level sentiment. Thus, the primary goal of our SemEval task is to promote research that will lead to better understanding of how sentiment is conveyed in Social Media. Toward that goal, we created the SemEval Tweet corpus as part of our inaugural Sentiment Ana"
S14-2009,P11-2102,0,0.0688453,"understanding of how sentiment is conveyed in Social Media. Toward that goal, we created the SemEval Tweet corpus as part of our inaugural Sentiment Analysis in Twitter Task, SemEval-2013 Task 2 (Nakov et al., 2013). It contains tweets and SMS messages with sentiment expressions annotated with contextual phrase-level and messagelevel polarity. This year, we extended the corpus by adding new tweets and LiveJournal sentences. Another interesting phenomenon that has been studied in Twitter is the use of the #sarcasm hashtag to indicate that a tweet should not be taken literally (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). In fact, sarcasm indicates that the message polarity should be flipped. With this in mind, this year, we also evaluate on sarcastic tweets. We describe the Sentiment Analysis in Twitter task, ran as part of SemEval-2014. It is a continuation of the last year’s task that ran successfully as part of SemEval2013. As in 2013, this was the most popular SemEval task; a total of 46 teams contributed 27 submissions for subtask A (21 teams) and 50 submissions for subtask B (44 teams). This year, we introduced three new test sets: (i) regular tweets, (ii) sarcastic tweets, and"
S14-2009,D11-1141,1,0.0497869,"were constrained. All participating systems outperformed the majority class baseline by a sizable margin. 5.2 Preprocessing of tweets was still a popular technique. In addition to standard NLP steps such as tokenization, stemming, lemmatization, stopword removal and POS tagging, most teams applied some kind of Twitter-specific processing such as substitution/removal of URLs, substitution of emoticons, word normalization, abbreviation lookup, and punctuation removal. Finally, several of the teams used Twitter-tuned NLP tools such as part of speech and named entity taggers (Gimpel et al., 2011; Ritter et al., 2011). Subtask B The results for subtask B are shown in Table 5. The subtask attracted 50 submissions from 44 teams. There were eight unconstrained submissions: six teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only. As for subtask A, the best systems were constrained. Again, all participating systems outperformed the majority class baseline; however, some systems were very close to it. 6 The similarity of preprocessing techniques, NLP tools, classifiers and features used in 2013 and this year is probably partially due to many teams partic"
S14-2009,W13-1605,0,0.0809238,"Missing"
S14-2103,S14-2009,1,0.818482,"t. We trained an SVM classifier with a linear kernel using a variety of features. We used publicly available resources only, and thus our results should be easily replicable. Overall, our system is ranked 20th out of 50 submissions (by 44 teams) based on the average of the three 2014 evaluation data scores, with an F1-score of 63.62 on general tweets, 48.37 on sarcastic tweets, and 68.24 on LiveJournal messages. 1 Introduction We describe the submission of the team of the Sofia University, Faculty of Mathematics and Informatics (SU-FMI) to SemEval-2014 Task 9 on Sentiment Analysis in Twitter (Rosenthal et al., 2014). 2 Method Our approach is inspired by the highest scoring team in 2013, NRC Canada (Mohammad et al., 2013). We reused many of their resources.1 Our system consists of two main submodules, (i) feature extraction in the framework of GATE (Cunningham et al., 2011), and (ii) machine learning using SVM with linear kernels as implemented in LIBLINEAR2 (Fan et al., 2008). ∗ Sofia University, bobby.velichkov@gmail.com † Sofia University, b.kapukaranov@gmail.com ‡ Sofia University, iigrozev@gmail.com § Sofia University, j.karanesheva@gmail.com ¶ Sofia University, tbmihailov@gmail.com k Sofia Universit"
S14-2103,R13-1011,0,0.175951,"mmons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 1 http://www.umiacs.umd.edu/ ˜saif/WebPages/Abstracts/ NRC-SentimentAnalysis.htm 2 http://www.csie.ntu.edu.tw/˜cjlin/ liblinear/ 590 Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 590–595, Dublin, Ireland, August 23-24, 2014. 2.1 Preprocessing We further used the following lexicons: We integrated a pipeline of various resources for tweet analysis that are already available in GATE (Bontcheva et al., 2013) such as a Twitter tokenizer, a sentence splitter, a hashtag tokenizer, a Twitter POS tagger, a morphological analyzer, and the Snowball3 stemmer. We further implemented in GATE some shallow text processing components in order to handle negation contexts, emoticons, elongated words, all-caps words and punctuation. We also added components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 • NRC Hashtag Sentiment Lex"
S14-2103,P10-1040,0,0.0535375,"l. We then calculated four features: number of positive and negative emoticons in the tweet, and whether the last token is a positive or a negative emoticon. • Positive terms count; • Positive negated terms count; • Positive/negative terms count ratio; • Sentiment of the last token; • Overall sentiment terms count. 3 http://snowball.tartarus.org/ http://www.ark.cs.cmu.edu/TweetNLP/ cluster_viewer.html 5 http://github.com/percyliang/ brown-cluster 4 591 2.2.2 Tweet-level features • Word and word bigram clusters: word clusters have been shown to improve the performance of supervised NLP models (Turian et al., 2010). We use the word clusters built by CMU’s NLP toolkit, which were produced over a collection of 56 million English tweets (Owoputi et al., 2012) and built using the Percy Liang’s HMM-based implementation6 of Brown clustering (Liang, 2005; Brown et al., 1992), which group the words into 1,000 hierarchical clusters. We use two features based on these clusters: We use the following tweet-level features: • All caps: the number of words with all characters in upper case; • Hashtags: the number ot hashtags in the tweet; • Elongated words: the number of words with character repetitions. 2.2.3 Term-le"
S14-2103,J92-4003,0,0.645586,"ources for tweet analysis that are already available in GATE (Bontcheva et al., 2013) such as a Twitter tokenizer, a sentence splitter, a hashtag tokenizer, a Twitter POS tagger, a morphological analyzer, and the Snowball3 stemmer. We further implemented in GATE some shallow text processing components in order to handle negation contexts, emoticons, elongated words, all-caps words and punctuation. We also added components to find words and phrases contained in sentiment lexicons, as well as to annotate words with word cluster IDs using the lexicon built at CMU,4 which uses the Brown clusters (Brown et al., 1992) as implemented5 by (Liang, 2005). 2.2 • NRC Hashtag Sentiment Lexicon: list of words and their associations with positive and negative sentiment (Mohammad et al., 2013): 54,129 unigrams, 316,531 bigrams, 480,010 pairs, and 78 high-quality positive and negative hashtag terms; • Sentiment140 Lexicon: list of words with associations to positive and negative sentiments (Mohammad et al., 2013): 62,468 unigrams, 677,698 bigrams, 480,010 pairs; • Stanford Sentiment Treebank: contains 239,231 evaluated words and phrases. If a word or a phrase was found in the tweet, we took the given sentiment label."
S14-2103,H05-1044,0,0.0816822,"ns 239,231 evaluated words and phrases. If a word or a phrase was found in the tweet, we took the given sentiment label. Features 2.2.1 Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: For the NRC Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, we calculated the following features for unigrams, bigrams and pairs: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • Sum of positive terms’ sentiment; • Sum of negative terms’ sentiment; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Sum of the sentiment for all terms in the tweet; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • Sum of negated positive terms’ sentiment; • Negative/positive terms ratio; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. • Max positive sentiment; For each lexicon, we find in the tweet the terms that are listed in it, and then we calculate the following features: • Min negative sentiment; • Max sentiment of a term. • Negative terms count; We used different features for the t"
S14-2103,D09-1063,0,0.0247878,"Sentiment lexicon features We used several preexisting lexicons, both manually designed and automatically generated: For the NRC Hashtag Sentiment Lexicon and the Sentiment140 Lexicon, we calculated the following features for unigrams, bigrams and pairs: • Minqing Hu and Bing Liu opinion lexicon (Hu and Liu, 2004): 4,783 positive and 2,006 negative terms; • Sum of positive terms’ sentiment; • Sum of negative terms’ sentiment; • MPQA Subjectivity Cues Lexicon (Wilson et al., 2005): 8,222 terms; • Sum of the sentiment for all terms in the tweet; • Macquarie Semantic Orientation Lexicon (MSOL) (Mohammad et al., 2009): 30,458 positive and 45,942 negative terms; • Sum of negated positive terms’ sentiment; • Negative/positive terms ratio; • NRC Emotion Lexicon (Mohammad et al., 2013): 14,181 terms with specified emotion. • Max positive sentiment; For each lexicon, we find in the tweet the terms that are listed in it, and then we calculate the following features: • Min negative sentiment; • Max sentiment of a term. • Negative terms count; We used different features for the two lexicon groups because their contents differ. The first four lexicons provide a discrete sentiment value for each word. In contrast, t"
S14-2103,S13-2053,0,0.261674,"resources only, and thus our results should be easily replicable. Overall, our system is ranked 20th out of 50 submissions (by 44 teams) based on the average of the three 2014 evaluation data scores, with an F1-score of 63.62 on general tweets, 48.37 on sarcastic tweets, and 68.24 on LiveJournal messages. 1 Introduction We describe the submission of the team of the Sofia University, Faculty of Mathematics and Informatics (SU-FMI) to SemEval-2014 Task 9 on Sentiment Analysis in Twitter (Rosenthal et al., 2014). 2 Method Our approach is inspired by the highest scoring team in 2013, NRC Canada (Mohammad et al., 2013). We reused many of their resources.1 Our system consists of two main submodules, (i) feature extraction in the framework of GATE (Cunningham et al., 2011), and (ii) machine learning using SVM with linear kernels as implemented in LIBLINEAR2 (Fan et al., 2008). ∗ Sofia University, bobby.velichkov@gmail.com † Sofia University, b.kapukaranov@gmail.com ‡ Sofia University, iigrozev@gmail.com § Sofia University, j.karanesheva@gmail.com ¶ Sofia University, tbmihailov@gmail.com k Sofia University, yasen.kiprov@gmail.com ∗∗ Ontotext, g.d.georgiev@gmail.com †† Sofia University, koychev@fmi.uni-sofia.bg"
S14-2103,S13-2052,1,0.67562,"why our results differ so much from those of the NRC-Canada team in 2013 since our features are quite similar. We attribute the difference to the fact that some of the lexicons we use actually hurt our score as we mentioned above. Another difference could be that last year’s NRC system uses n-grams, which we have disabled as they lowered our scores. Last but not least, there could be bugs lurking in our feature representation that additionally lower our results. Experimental setup At development time, we trained on train-2013, tuned the C value of SVM on dev-2013, and evaluated on test-2013 (Nakov et al., 2013). For our submission, we trained on train-2013+dev-2013, and we evaluated on the 2014 test dataset provided by the organizers. This dataset contains two parts and a total of five datasets: (a) progress test (the Twitter and SMS test datasets for 2013), and (b) new test datasets (from Twitter, from Twitter with sarcasm, and from LiveJournal). We used C=0.012, which was best on development. 3.2 Official results Due to our very late entering in the competition, we have only managed to perform a small number of experiments, and we only participated in subtask B. We were ranked 20th out of 50 submi"
S15-2036,P14-1023,0,0.00431346,"entence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of th"
S15-2036,W10-2802,0,0.0170168,"tial tree kernel (Moschitti, 2006) to calculate the similarity between the question and the comment based on their corresponding shallow syntactic trees. These trees have word lemmata as leaves, then there is a POS tag node parent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimen"
S15-2036,W01-0515,0,0.415394,"rule-based. 2.1 Similarity Measures The similarity features measure the similarity sim(q, c) between the question and a target comment, assuming that high similarity signals a GOOD answer. We consider three kinds of similarity measures, which we describe below. 2.1.1 Lexical Similarity We compute the similarity between word n-gram representations (n = [1, . . . , 4]) of q and c, using the following lexical similarity measures (after stopword removal): greedy string tiling (Wise, 1996), longest common subsequences (Allison and Dix, 1986), Jaccard coefficient (Jaccard, 1901), word containment (Lyon et al., 2001), and cosine similarity. We further compute cosine on lemmata and POS tags, either including stopwords or not. We also use similarity measures, which weigh the terms using the following three formulæ: X sim(q, c) = idf (t) (1) t∈q∩c sim(q, c) = X t∈q∩c sim(q, c) = X t∈q∩c log(idf (t)) (2)   |C| log 1 + tf (t) (3) where idf (t) is the inverse document frequency (Sparck Jones, 1972) of term t in the entire Qatar Living dataset, C is the number of comments in this collection, and tf (t) is the term frequency of the term in the comment. Equations 2 and 3 are variations of idf; cf. Nallapati (200"
S15-2036,S15-2047,1,0.437926,"Missing"
S15-2036,N13-1090,0,0.0152512,"tar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help us on top of all other features we had. 2.2 Context Comments are organized sequentially according to the time line of the comment thread. Whether a question includes further comments by the person who asked the original question or just several comments by the same user, or whether it belongs to a category in which a given kind of answer is expected, are all important factors. Therefore, we consider a set of featur"
S15-2036,S13-2053,0,0.0143676,"omments suggested visiting a Web site or contained an email address. Therefore, we included two boolean features to verify the presence of URLs or emails in c. Another feature captures the length of c, as longer (GOOD ) comments usually contain detailed information to answer a question. 2.5 Polarity These features, which we used for subtask B only, try to determine whether a comment is positive or negative, which could be associated with YES or NO answers. The polarity of a comment c is X pol(w) (5) pol(c) = w∈c where pol(w) is the polarity of word w in the NRC Hashtag Sentiment Lexicon v0.1 (Mohammad et al., 2013). We disregarded pol(w) if its absolute value was less than 1. We further use boolean features that check the existence of some keywords in the comment. Their values are set to true if c contains words like (i) yes, can, sure, wish, would, or (ii) no, not, neither. 2.6 User Profile With this set of features, we aim to model the behavior of the different participants in previous queries. Given comment c by user u, we consider the number of GOOD , BAD , POTENTIAL , and DIALOGUE comments u has produced before.4 We also consider the average word length of GOOD , BAD , POTENTIAL , and DIALOGUE comm"
S15-2036,D14-1162,0,0.0928882,"arent for each lemma leaf, and POS tag nodes are in turn grouped under shallow parsing chunks, which are linked to a root sentence node; finally, all root sentence nodes are linked to a super root for all sentences in the question/comment. 2.1.3 Semantic Similarity We apply three approaches to build wordembedding vector representations, using (i) latent semantic analysis (Croce and Previtali, 2010), trained on the Qatar Living corpus with a word co-occurrence window of size ±3 and producing a vector of 250 dimensions with SVD (we produced a vector for each noun in the vocabulary); (ii) GloVe (Pennington et al., 2014), using a model pre-trained on Common Crawl (42B tokens), with 300 dimensions; and (iii) COMPOSES (Baroni et al., 2014), using previously-estimated predict vectors of 400 dimensions.3 We represent both q and c as a sum of the vectors corresponding to the words within them (neglecting the subject of c). We compute the cosine similarity to estimate sim(q, c). We also experimented with word2vec (Mikolov et al., 2013) vectors pre-trained with both cbow and skipgram on news data, and also with both word2vec and GloVe vectors trained on Qatar Living data, but we discarded them as they did not help u"
S15-2047,S15-2048,0,0.127647,"Missing"
S15-2047,N10-1145,0,0.0124059,"A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment,"
S15-2047,S15-2039,0,0.0505919,"Missing"
S15-2047,S15-2035,0,0.132649,"Missing"
S15-2047,S15-2040,0,0.0642528,"Missing"
S15-2047,P07-1098,1,0.123047,"are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with featu"
S15-2047,S15-2036,1,0.483036,"Missing"
S15-2047,D14-1162,0,0.0890449,"tures above can be binary, integer, or real-valued, e.g., can be calculated using various weighting schemes such as TF.IDF for words/lemmata/stems. Although most participants focused on engineering features to be used with a standard classifier such as SVM or a decision tree, some also used more advanced techniques. For example, some teams used sequence or partial tree kernels (Moschitti, 2006). Another popular technique was to use word embeddings, e.g., modeled using convolution or recurrent neural networks, or with latent semantic analysis, and also vectors trained using word2vec and GloVe (Pennington et al., 2014), as pre-trained on Google News or Wikipedia, or trained on the provided Qatar Living data. Less popular techniques included dialog modeling for the list of comments for a given question, e.g., using conditional random fields to model the sequence of comment labels (Good, Bad, Potential, Dialog), mapping the question and the comment to a graph structure and performing graph traversal, using word alignments between the question and the comment, time modeling, and sentiment analysis. Finally, for Arabic, some participants translated the Arabic data to English, and then extracted features from bo"
S15-2047,S15-2044,0,0.0450275,"Missing"
S15-2047,D13-1044,1,0.0589439,"n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. For Arabic, we also made use of a real cQA portal, the Fatwa website,3 where questions about Islam are posed by regular users and are answered by knowledgeable scholars. For subtask A, we used a setup similar to that for English, but this time each question had exactly one correct answer among the candidate answers (see Section 3 for detail); we did not offer subtask B for Arabic. Overall for the task, we needed manual annotations in two different languages an"
S15-2047,D07-1002,0,0.0113579,"focus on aspects that are relevant for the SemEval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied line"
S15-2047,P08-1082,0,0.294043,"Eval community, namely on learning the relationship between two pieces of text. 1 http://www.qatarliving.com/forum/ 269 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to"
S15-2047,S15-2038,0,0.257423,"Missing"
S15-2047,S15-2041,0,0.062348,"Missing"
S15-2047,C10-1131,0,0.028049,"Missing"
S15-2047,D07-1003,0,0.0108685,"hop on Semantic Evaluation (SemEval 2015), pages 269–281, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Linguistics Subtask A goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work.2 For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; t"
S15-2047,N13-1106,0,0.0203816,"Missing"
S15-2047,S15-2042,0,0.0485548,"Missing"
S15-2047,S15-2043,0,0.0966631,"Missing"
S15-2047,S15-2037,0,0.0693424,"Missing"
S15-2078,baccianella-etal-2010-sentiwordnet,0,0.572805,"rs to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other"
S15-2078,C10-2005,0,0.0833505,"ion are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we dis"
S15-2078,W10-2914,0,0.0273404,"iu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of"
S15-2078,P11-2102,0,0.0395462,"dia such as Weblogs, microblogs, and discussion forums are used daily to express personal thoughts, which allows researchers to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, speci"
S15-2078,N03-2012,0,0.0438636,"of prior polarity of a phrase. 1 Svetlana Kiritchenko Introduction Social media such as Weblogs, microblogs, and discussion forums are used daily to express personal thoughts, which allows researchers to gain valuable insight into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et"
S15-2078,S12-1047,1,0.517008,"ator agreement is low, and annotators struggle even to remain self-consistent. In contrast, it is much easier to make relative judgments, e.g., to say whether one word is more positive than another. Moreover, it is possible to derive an absolute score from pairwise judgments, but this requires a much larger number of annotations. Fortunately, there are schemes that allow to infer more pairwise annotations from less judgments. 455 One such annotation scheme is MaxDiff (Louviere, 1991), which is widely used in market surveys (Almquist and Lee, 2009); it was also used in a previous SemEval task (Jurgens et al., 2012). In MaxDiff, the annotator is presented with four terms and asked which term is most positive and which is least positive. By answering just these two questions, five out of six pairwise rankings become known. Consider a set in which a judge evaluates A, B, C, and D. If she says that A and D are the most and the least positive, we can infer the following: A &gt; B, A &gt; C, A &gt; D, B &gt; D, C &gt; D. The responses to the MaxDiff questions can then be easily translated into a ranking for all the terms and also into a real-valued score for each term. We crowdsourced the MaxDiff questions on CrowdFlower, r"
S15-2078,W13-1605,0,0.0482134,"Missing"
S15-2078,S13-2053,1,0.407706,"of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five subtasks, with most teams participating in more than one subtask. This year the task included reruns of two legacy subtasks, which asked to detect the sentiment expressed in a tweet or by a particular phrase in a tweet. The task further added three new subtasks. The first two focused on the sentiment towards a given topic in a single tweet or in a set of tweets, respect"
S15-2078,S13-2052,1,0.689604,"ing sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five sub"
S15-2078,S10-1097,0,0.0524882,"ion (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter ha"
S15-2078,W02-1011,0,0.0472385,"t into the opinions of a very large number of individuals, i.e., at a scale that was simply not possible a few years ago. As a result, nowadays, sentiment analysis is commonly used to study the public opinion towards persons, objects, and events. In particular, opinion mining and opinion detection are applied to product reviews (Hu and Liu, 2004), for agreement detection (Hillard et al., 2003), and even for sarcasm identification (Gonz´alez-Ib´an˜ ez et al., 2011; Liebrecht et al., 2013). Early work on detecting sentiment focused on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled en"
S15-2078,D11-1141,1,0.0366066,"ive. • Subtask E. Determining Strength of Association of Twitter Terms with Positive Sentiment (Degree of Prior Polarity): Given a word/phrase, propose a score between 0 (lowest) and 1 (highest) that is indicative of the strength of association of that word/phrase with positive sentiment. If a word/phrase is more positive than another one, it should be assigned a relatively higher score. 452 Data Collection Subtasks A–D First, we gathered tweets that express sentiment about popular topics. For this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned NER system (Ritter et al., 2011). Our initial training set was collected over a one-year period spanning from January 2012 to January 2013. Each subsequent Twitter test set was collected a few months prior to the corresponding evaluation. We used the public streaming Twitter API to download the tweets. We then identified popular topics as those named entities that are frequently mentioned in association with a specific date (Ritter et al., 2012). Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from tr"
S15-2078,S14-2009,1,0.518377,"d on newswire text (Wiebe et al., 2005; Baccianella et al., 2010; Pang et al., 2002; Hu and Liu, 2004). As later research turned towards social media, people realized this presented a number of new challenges. Misspellings, poor grammatical structure, emoticons, acronyms, and slang were common in these new media, and were explored by a number of researchers (Barbosa and Feng, 2010; Bifet et al., 2011; Davidov et al., 2010; Jansen et al., 2009; Kouloumpis et al., 2011; O’Connor et al., 2010; Pak and Paroubek, 2010). Later, specialized shared tasks emerged, e.g., at SemEval (Nakov et al., 2013; Rosenthal et al., 2014), which compared teams against each other in a controlled environment using the same training and testing datasets. These shared tasks had the side effect to foster the emergence of a number of new resources, which eventually spread well beyond SemEval, e.g., NRC’s Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al., 2013).1 Below, we discuss the public evaluation done as part of SemEval-2015 Task 10. In its third year, the SemEval task on Sentiment Analysis in Twitter has once again attracted a large number of participants: 41 teams across five subtasks, with most teams pa"
S15-2078,H05-1044,0,0.415097,"s much smaller: 30.3 for B vs. 26.7 for C. Finally, the last column in the table reports the results for the 75 sarcastic 2015 tweets. The winner here is KLUEless with an F1 of 39.26, followed by TwitterHawk with F1 =31.30, and then by UMDuluth-CS8761 with F1 =29.91. 460 Subtask D: Trend Towards a Topic Subtask E: Degree of Prior Polarity Ten teams participated in subtask E. Many chose an unsupervised approach and leveraged newlycreated and pre-existing sentiment lexicons such as the Hashtag Sentiment Lexicon, the Sentiment140 Lexicon (Kiritchenko et al., 2014), the MPQA Subjectivity Lexicon (Wilson et al., 2005), and SentiWordNet (Baccianella et al., 2010), among others. Several participants further automatically created their own sentiment lexicons from large collections of tweets. Three teams, including the winner INESC-ID, adopted a supervised approach and used word embeddings (supplemented with lexicon features) to train a regression model. The results are presented in Table 14. The last row shows the performance of a lexicon-based baseline. For this baseline, we chose the two most frequently used existing, publicly available, and automatically generated sentiment lexicons: Hashtag Sentiment Lexi"
S15-2078,P14-1029,1,0.129095,"tag Sentiment tweet corpora (Kiritchenko et al., 2014). In order to reduce the skewness towards the neutral class, we selected terms from different ranges of automatically determined sentiment values as provided by the corresponding Sentiment140 and Hashtag Sentiment lexicons. The term set comprised regular English words, hashtagged words (e.g., #loveumom), misspelled or creatively spelled words (e.g., parlament or happeeee), abbreviations, shortenings, and slang. Some terms were negated expressions such as no fun. (It is known that negation impacts the sentiment of its scope in complex ways (Zhu et al., 2014).) We annotated these terms for degree of sentiment manually. Further details about the data collection and the annotation process can be found in Section 3.2.2 as well as in (Kiritchenko et al., 2014). The trial dataset consisted of 200 instances, and no training dataset was provided. Note, however, that the trial data was large enough to be used as a development set, or even as a training set. Moreover, the participants were free to use any additional manually or automatically generated resources when building their systems for subtask E. The testset included 1,315 instances. 453 Annotation"
S16-1001,S16-1024,0,0.0330359,"Missing"
S16-1001,S16-1036,0,0.0182689,"Missing"
S16-1001,S16-1033,0,0.0180935,"Missing"
S16-1001,S16-1010,0,0.0389367,"Missing"
S16-1001,S16-1032,0,0.0327633,"Missing"
S16-1001,S16-1015,0,0.0357065,"Missing"
S16-1001,S16-1019,0,0.0273999,"Missing"
S16-1001,S16-1173,0,0.130145,"Missing"
S16-1001,S16-1017,0,0.0167507,"Missing"
S16-1001,S16-1011,0,0.0306781,"Missing"
S16-1001,S16-1037,0,0.0231103,"Missing"
S16-1001,S16-1034,0,0.0319013,"Missing"
S16-1001,S16-1020,0,0.0329578,"Missing"
S16-1001,S16-1021,0,0.0169336,"Missing"
S16-1001,S16-1028,0,0.0313488,"Missing"
S16-1001,S16-1039,0,0.0328294,"Missing"
S16-1001,S16-1014,0,0.0235931,"Missing"
S16-1001,S16-1018,0,0.0272029,"Missing"
S16-1001,E12-1062,0,0.0190165,"16, pages 1–18, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 1.2 2 Quantification We replace classification with quantification, i.e., supervised class prevalence estimation. With regard to Twitter, hardly anyone is interested in whether a specific person has a positive or a negative view of the topic. Rather, applications look at estimating the prevalence of positive and negative tweets about a given topic. Most (if not all) tweet sentiment classification studies conducted within political science (Borge-Holthoefer et al., 2015; Kaya et al., 2013; Marchetti-Bowick and Chambers, 2012), economics (Bollen et al., 2011; O’Connor et al., 2010), social science (Dodds et al., 2011), and market research (Burton and Soboleva, 2011; Qureshi et al., 2013), use Twitter with an interest in aggregate data and not in individual classifications. Estimating prevalences (more generally, estimating the distribution of the classes in a set of unlabelled items) by leveraging training data is called quantification in data mining and related fields. Previous work has argued that quantification is not a mere byproduct of classification, since (a) a good classifier is not necessarily a good quant"
S16-1001,N13-1090,0,0.00624088,"t frequently used by the participants are Theano and Keras. Conversely, kernel machines seem to be less frequently used than in the past, and the use of learning methods other than the ones mentioned above is scarce. The use of distant supervision is ubiquitous; this is natural, since there is an abundance of freely available tweets labelled according to sentiment (possibly with silver labels only, e.g., emoticons), and it is intuitive that their use as additional training data could be helpful. Another ubiquitous technique is the use of word embeddings, usually generated via either word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014); most authors seem to use general-purpose, pre-trained embeddings, while some authors also use customized word embeddings, trained either on the Tweet 2016 dataset or on tweet datasets of some sort. Nothing radically new seems to have emerged with respect to text preprocessing; as in previous editions of this task, participants use a mix of by now obvious techniques, such as negation scope detection, elongation normalization, detection of amplifiers and diminishers, plus the usual extraction of word n-grams, character n-grams, and POS ngrams. The use of sent"
S16-1001,S16-1029,0,0.0228175,"Missing"
S16-1001,S16-1005,0,0.0353255,"Missing"
S16-1001,S13-2052,1,0.676646,"s. Subtask E is similar to SemEval-2015 Task 10 Subtask D, which consisted of the following problem: Given a set of messages on a given topic from the same period of time, classify the overall sentiment towards the topic in these messages as strongly positive, weakly positive, neutral, weakly negative, or strongly negative. Note that in SemEval-2015 Task 10 Subtask D, exactly one of the five classes had to be chosen, while in our Subtask E, a distribution across the five classes has to be estimated. 2 Note that we retired the expression-level subtask A, which was present in SemEval 2013–2015 (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b). Goal As per the above discussion, Subtasks B to E are new. Conceptually, they form a 2×2 matrix, as shown in Table 1, where the rows indicate the goal of the task (classification vs. quantification) and the columns indicate the granularity of the task (twovs. five-point scale). Granularity Two-point Five-point (binary) (ordinal) Subtask B Subtask C Subtask D Subtask E Classification Quantification Table 1: A 2×2 matrix summarizing the similarities and the differences between Subtasks B-E. 3 Datasets In this section, we des"
S16-1001,S16-1023,0,0.035765,"Missing"
S16-1001,D14-1162,0,0.101908,"Missing"
S16-1001,S16-1007,0,0.0359431,"Missing"
S16-1001,D11-1141,1,0.122667,"ask (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b) for training and development. In addition we created new training and testing datasets. 3,662 575 1,572 492 982 33 427 1,040 1,466 340 601 394 202 40 304 365 4,600 739 1,640 1,207 669 13 411 987 9,728 1,654 3,813 2,093 1,853 86 1,142 2,392 We employed the following annotation procedure. As in previous years, we first gathered tweets that express sentiment about popular topics. For this purpose, we extracted named entities from millions of tweets, using a Twitter-tuned named entity recognition system (Ritter et al., 2011). The collected tweets were greatly skewed towards the neutral class. In order to reduce the class imbalance, we removed those that contained no sentiment-bearing words. We used SentiWordNet 3.0 (Baccianella et al., 2010) as a repository of sentiment words. Any word listed in SentiWordNet 3.0 with at least one sense having a positive or a negative sentiment score greater than 0.3 was considered sentiment-bearing.4 The training and development tweets were collected from July to October 2015. The test tweets were collected from October to December 2015. We used the public streaming Twitter API t"
S16-1001,S14-2009,1,0.830416,"Eval-2016 Task 4: Sentiment Analysis in Twitter Preslav Nakov♣ , Alan Ritter♦ , Sara Rosenthal♥ , Fabrizio Sebastiani♣∗, Veselin Stoyanov♠ ♣ Qatar Computing Research Institute, Hamad bin Khalifa University, Qatar ♦ Department of Computer Science and Engineering, The Ohio State University, USA ♥ IBM Watson Health Research, USA ♠ Johns Hopkins University, USA Abstract As a testament to the prominence of research on sentiment analysis in Twitter, the tweet sentiment classification (TSC) task has attracted the highest number of participants in the last three SemEval campaigns (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b). Previous editions of the SemEval task involved binary (P OSITIVE vs. N EGATIVE) or single-label multi-class classification (SLMC) when a N EU TRAL 1 class is added (P OSITIVE vs. N EGATIVE vs. N EUTRAL). SemEval-2016 Task 4 represents a significant departure from these previous editions. Although two of the subtasks (Subtasks A and B) are reincarnations of previous editions (SLMC classification for Subtask A, binary classification for Subtask B), SemEval-2016 Task 4 introduces two completely new problems, taken individually (Subtasks C and D) and"
S16-1001,S15-2078,1,0.882833,"ent Analysis in Twitter Preslav Nakov♣ , Alan Ritter♦ , Sara Rosenthal♥ , Fabrizio Sebastiani♣∗, Veselin Stoyanov♠ ♣ Qatar Computing Research Institute, Hamad bin Khalifa University, Qatar ♦ Department of Computer Science and Engineering, The Ohio State University, USA ♥ IBM Watson Health Research, USA ♠ Johns Hopkins University, USA Abstract As a testament to the prominence of research on sentiment analysis in Twitter, the tweet sentiment classification (TSC) task has attracted the highest number of participants in the last three SemEval campaigns (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015; Nakov et al., 2016b). Previous editions of the SemEval task involved binary (P OSITIVE vs. N EGATIVE) or single-label multi-class classification (SLMC) when a N EU TRAL 1 class is added (P OSITIVE vs. N EGATIVE vs. N EUTRAL). SemEval-2016 Task 4 represents a significant departure from these previous editions. Although two of the subtasks (Subtasks A and B) are reincarnations of previous editions (SLMC classification for Subtask A, binary classification for Subtask B), SemEval-2016 Task 4 introduces two completely new problems, taken individually (Subtasks C and D) and in combination (Subtask"
S16-1001,S16-1030,0,0.0918744,"Missing"
S16-1001,S16-1026,0,0.030281,"Missing"
S16-1001,S16-1031,0,0.0361636,"Missing"
S16-1001,S16-1035,0,0.0311617,"Missing"
S16-1001,S16-1022,0,0.0439887,"Missing"
S16-1001,S16-1009,0,0.0306813,"Missing"
S16-1001,S16-1027,0,0.0572505,"Missing"
S16-1001,S16-1013,0,0.043193,"Missing"
S16-1001,S16-1008,0,0.0255744,"Missing"
S16-1001,S16-1040,0,0.0464329,"Missing"
S16-1001,baccianella-etal-2010-sentiwordnet,1,\N,Missing
S16-1001,S16-1016,0,\N,Missing
S16-1001,S16-1041,0,\N,Missing
S16-1001,S16-1012,0,\N,Missing
S16-1001,S16-1006,1,\N,Missing
S16-1083,S16-1128,1,0.911614,"2 MAP points over the IR baseline). They use distributed representations of words, knowledge graphs generated with BabelNet, and frames from FrameNet. Their contrastive2 run is even better, with MAP of 77.33. The second best system is that of ConvKN (Barr´on-Cede˜no et al., 2016) with MAP of 76.02; they are also first on MRR, second on AvgRec and F1 , and third on Accuracy. The third best system is KeLP (Filice et al., 2016) with MAP of 75.83; they are also first on AvgRec, F1 , and Accuracy. They have a contrastive run with MAP of 76.28, which would have ranked second. The fourth best, SLS (Mohtarami et al., 2016) is very close, with MAP of 75.55; it is also first on MRR and Accuracy, and third on AvgRec. It uses a bag-of-vectors approach with various vector- and text-based features, and different neural network approaches including CNNs and LSTMs to capture the semantic similarity between questions and answers. 6.3 Subtask C, English (Question-External Comment Similarity) The results for subtask C, English are shown in Table 5. This subtask attracted 10 teams, and 28 runs: 10 primary and 18 contrastive. Here the teams performed much better than they did for subtask B. The first three baselines were al"
S16-1083,P07-1098,1,0.303795,"cess of their creation. Section 5 explains the evaluation measures. Section 6 presents the results for all subtasks and for all participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree"
S16-1083,S15-2047,1,0.907588,"Missing"
S16-1083,S15-2036,1,0.813812,"Missing"
S16-1083,D13-1044,1,0.636695,"n syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to automatically learn associations between questions and candidate answers. One interesting aspect of the above research is the need for syntactic structures; this is also corroborated in (Severyn and Moschitti, 2012; Severyn and Moschitti, 2013). Note that answer selection can use models for textual entailment, semantic similarity, and for natural language inference in general. Using information about the thread is another important direction. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to the usual local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, whether the answer is first, whether the answer is last (Hou et al., 2015). Similarly, the third-best team, QCRI,"
S16-1083,P08-1082,0,0.0152363,"participating systems. Section 7 summarizes the main approaches and features used by these systems. Finally, Section 8 offers some further discussion and presents the main conclusions. 2 Related Work Our task goes in the direction of passage reranking, where automatic classifiers are normally applied to pairs of questions and answer passages to derive a relative order between passages, e.g., see (Radlinski and Joachims, 2005; Jeon et al., 2005; Shen and Lapata, 2007; Moschitti et al., 2007; Severyn and Moschitti, 2015; Moschitti, 2008; Tymoshenko and Moschitti, 2015; Tymoshenko et al., 2016; Surdeanu et al., 2008). In recent years, many advanced models have been developed for automating answer selection, producing a large body of work. For instance, Wang et al. (2007) proposed a probabilistic quasisynchronous grammar to learn syntactic transformations from the question to the candidate answers; Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs; Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees; and Yao et al. (2013) applied linear chain CRFs with features derived from TED to a"
S16-1083,D07-1002,0,\N,Missing
S16-1083,N10-1145,0,\N,Missing
S16-1083,S15-2035,0,\N,Missing
S16-1083,C10-1131,0,\N,Missing
S16-1083,N13-1106,0,\N,Missing
S16-1083,P15-1078,1,\N,Missing
S16-1083,P15-2113,1,\N,Missing
S16-1083,R15-1058,1,\N,Missing
S16-1083,S16-1130,1,\N,Missing
S16-1083,S16-1138,1,\N,Missing
S16-1083,S16-1126,0,\N,Missing
S16-1083,S16-1132,0,\N,Missing
S16-1083,S16-1134,0,\N,Missing
S16-1083,S16-1136,1,\N,Missing
S16-1083,S16-1133,0,\N,Missing
S16-1083,S16-1172,1,\N,Missing
S16-1083,S16-1137,1,\N,Missing
S16-1083,S16-1131,0,\N,Missing
S16-1083,N16-1084,1,\N,Missing
S16-1083,S16-1135,0,\N,Missing
S16-1083,N16-1152,1,\N,Missing
S16-1083,P16-2075,1,\N,Missing
S16-1083,P16-2065,1,\N,Missing
S16-1083,S15-2037,0,\N,Missing
S16-1083,D15-1068,1,\N,Missing
S16-1083,K15-1032,1,\N,Missing
S16-1129,W10-1001,0,0.0803764,"Missing"
S16-1129,S16-1130,1,0.83936,"Missing"
S16-1129,P15-2113,1,0.869381,"Missing"
S16-1129,S15-2048,0,0.0204916,"ng (Nakov et al., 2015). The task in 2015 was to classify comments in a thread as relevant, potentially useful, or bad with respect to the thread question. This year’s Community Question Answering subtask A is similar to subtask A in 2015, but now it is a ranking task, asking to rank the answers in a thread based on their relevance with respect to the thread’s question. Given this similarity, most of the techniques used by participants in the 2015 subtask A are potentially valuable for this year’s subtask A as well. Below we mention just the few most relevant among them. In their 2015 system, Belinkov et al. (2015) used vectors of the question and of the comment, metadata features, and text-based similarities. Nicosia et al. (2015) used similarity measures, URLs in the comment text and statistics about the user profile: number of good, bad, and potentially useful comments. Similarly, we use the number of posts by the same user in the thread, the ID of the question’s author, topic model-based feature, special words, etc. Determing the overall sentiment of the question can also be useful, and it was used in 2015 (Nicosia et al., 2015). One way to do it is to build a sufficiently large question taxonomy as"
S16-1129,P02-1022,0,0.249955,"Missing"
S16-1129,P15-1078,1,0.814073,"Missing"
S16-1129,P16-2075,1,0.875492,"Missing"
S16-1129,S16-1137,1,0.884854,"Missing"
S16-1129,D15-1068,1,0.875716,"Missing"
S16-1129,N16-1084,1,0.88503,"Missing"
S16-1129,P16-2065,1,0.922017,"nce last activity in the forum, time of the day in which the user was active, etc. We also added as user characteristics the number of good and bad comments from the annotated training data. However, the user features did not improve the results. We noticed that over time, the number of both good and bad comments for a user in the forum grew, and the number of good and bad comments for most of the users was similar. We also used troll user features, e.g., number of mentions of the user as troll and troll behavior characteristics as described in (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a). Features All All − semantic vectors All − metadata All − comment characteristics All − distances All − URLs All − User stats All − Wh-words in Q and C All − Wh-words in Q All − Wh-words in C All − Loc/Org in Comment All − POS count in Q All − POS count in C All − POS and Wh-words in Q Primary Contrastive-1 Contrastive-2 Dev-2016 as test set MAP Accuracy 69.89 76.60 65.93 73.11 65.51 74.96 69.30 75.49 68.22 76.19 69.96 76.27 70.08 76.48 69.55 76.56 69.73 76.97 69.98 76.48 69.95 76.56 69.85 76.07 69.61 76.02 70.02 76.43 70.67 77.62 70.06 76.84 —– —– Test-2016 as test set MAP Accuracy 77.83 7"
S16-1129,S16-1136,1,0.736581,"nce last activity in the forum, time of the day in which the user was active, etc. We also added as user characteristics the number of good and bad comments from the annotated training data. However, the user features did not improve the results. We noticed that over time, the number of both good and bad comments for a user in the forum grew, and the number of good and bad comments for most of the users was similar. We also used troll user features, e.g., number of mentions of the user as troll and troll behavior characteristics as described in (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a). Features All All − semantic vectors All − metadata All − comment characteristics All − distances All − URLs All − User stats All − Wh-words in Q and C All − Wh-words in Q All − Wh-words in C All − Loc/Org in Comment All − POS count in Q All − POS count in C All − POS and Wh-words in Q Primary Contrastive-1 Contrastive-2 Dev-2016 as test set MAP Accuracy 69.89 76.60 65.93 73.11 65.51 74.96 69.30 75.49 68.22 76.19 69.96 76.27 70.08 76.48 69.55 76.56 69.73 76.97 69.98 76.48 69.95 76.56 69.85 76.07 69.61 76.02 70.02 76.43 70.67 77.62 70.06 76.84 —– —– Test-2016 as test set MAP Accuracy 77.83 7"
S16-1129,K15-1032,1,0.889083,"the original question. A good ranking is one where the PerfectMatch and the Relevant questions (without distinguishing between them) are both ranked above the Irrelevant ones. We also used semantic vectors (Mikolov et al., 2013a) pretrained on Google News data: 300dimensional vectors, available for three million words and phrases. For all subtasks, we further trained semantic vecˇ uˇrek and Sojka, 2010) on tors using Gensim (Reh˚ 200,000 questions and two million comments from the Qatar Living Forum,2 , which were provided by the task organizers. Finally, using this same data, and following (Mihaylov et al., 2015a; Mihaylov et al., 2015b), we scraped information about the users from the forum and we extracted for each of them the time in the forum, the active period, the number of questions, the comments in the forum, etc. 4 Method We build our system on top of the framework developed by our colleagues (Zamanov et al., 2015). In particular, we approach the task as a classification problem similarly to the approach we took for SemEval 2015 Task 3 (Nakov et al., 2015). However, unlike 2015, this year we have a ranking problem for all subtasks, e.g., for subtask A we have to rank the comments depending o"
S16-1129,R15-1058,1,0.893615,"the original question. A good ranking is one where the PerfectMatch and the Relevant questions (without distinguishing between them) are both ranked above the Irrelevant ones. We also used semantic vectors (Mikolov et al., 2013a) pretrained on Google News data: 300dimensional vectors, available for three million words and phrases. For all subtasks, we further trained semantic vecˇ uˇrek and Sojka, 2010) on tors using Gensim (Reh˚ 200,000 questions and two million comments from the Qatar Living Forum,2 , which were provided by the task organizers. Finally, using this same data, and following (Mihaylov et al., 2015a; Mihaylov et al., 2015b), we scraped information about the users from the forum and we extracted for each of them the time in the forum, the active period, the number of questions, the comments in the forum, etc. 4 Method We build our system on top of the framework developed by our colleagues (Zamanov et al., 2015). In particular, we approach the task as a classification problem similarly to the approach we took for SemEval 2015 Task 3 (Nakov et al., 2015). However, unlike 2015, this year we have a ranking problem for all subtasks, e.g., for subtask A we have to rank the comments depending o"
S16-1129,N13-1090,0,0.0604322,",690 comments for Subtask C. For subtask A, the comments in a question-answer thread are annotated as Good, PotentiallyUseful and Bad. A good ranking is one that ranks all Good comments above PotentiallyUseful and Bad ones (without distinguishing between the latter two). For subtask B, the potentially relevant questions are annotated as PerfectMatch, Relevant and Irrelevant with respect to the original question. A good ranking is one where the PerfectMatch and the Relevant questions (without distinguishing between them) are both ranked above the Irrelevant ones. We also used semantic vectors (Mikolov et al., 2013a) pretrained on Google News data: 300dimensional vectors, available for three million words and phrases. For all subtasks, we further trained semantic vecˇ uˇrek and Sojka, 2010) on tors using Gensim (Reh˚ 200,000 questions and two million comments from the Qatar Living Forum,2 , which were provided by the task organizers. Finally, using this same data, and following (Mihaylov et al., 2015a; Mihaylov et al., 2015b), we scraped information about the users from the forum and we extracted for each of them the time in the forum, the active period, the number of questions, the comments in the foru"
S16-1129,S15-2047,1,0.915309,"Missing"
S16-1129,S15-2036,1,0.907652,"Missing"
S16-1129,S15-2043,1,0.930659,"ng similar questions and ranking their answers with respect to the new question. Two additional supporting subtasks are defined: Subtask A (Question-Comment Similarity): Given a question from a question-comment thread, rank the comments within the thread based on their relevance with respect to the question. Subtask B (Question-Question Similarity): Given a new question, re-rank the similar questions retrieved by a search engine with respect to that question. 1 http://alt.qcri.org/semeval2016/task3/ Related Work We build our preprocessing and feature extraction pipeline based on the system of Zamanov et al. (2015), which was developed by a subset of our 2016 team for SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Nakov et al., 2015). The task in 2015 was to classify comments in a thread as relevant, potentially useful, or bad with respect to the thread question. This year’s Community Question Answering subtask A is similar to subtask A in 2015, but now it is a ranking task, asking to rank the answers in a thread based on their relevance with respect to the thread’s question. Given this similarity, most of the techniques used by participants in the 2015 subtask A are potentiall"
S16-1130,P15-2113,1,0.34436,"Missing"
S16-1130,J90-1003,0,0.414417,"ational Linguistics At test time, we generate regression scores for each answer in a question-answer thread and we rerank the answers accordingly. Before exploring our features, we will first introduce PMI and how we use it to generate goodness polarity lexicons. 3 Pointwise Mutual Information and Strength of Association The pointwise mutual information (PMI) is a notion from the theory of information: given two random variables A and B, the mutual information of A and B is the “amount of information” (in units such as bits) obtained about the random variable A, through the random variable B (Church and Hanks, 1990). Let a and b be two values from the sample space of A and B, respectively. The pointwise mutual information between a and b is defined as follows: P (A = a, B = b) P (A = a) · P (B = b) P (A = a|B = b) = log P (A = a) pmi(a; b) = log (1) (2) pmi(a; b) takes values between −∞, which is when P (A = a, B = b) = 0, and min {− log P (A = a), − log P (B = b)}, when P (A = a|B = b) = P (B = b|A = a) = 1. The mutual information between A and B is the expected value of pmi(a; b): M I(A, B) = XX pmi(a; b) (3) a∈A b∈B PMI is central to a popular approach for bootstrapping sentiment lexicons proposed by"
S16-1130,P15-1078,1,0.367345,"Missing"
S16-1130,P16-2075,1,0.640597,"Missing"
S16-1130,S16-1137,1,0.873543,"Missing"
S16-1130,D15-1068,1,0.501097,"Missing"
S16-1130,N16-1084,1,0.47077,"Missing"
S16-1130,S16-1004,0,0.0328479,"chitti (2015), who proposed an approach to lexicon induction, which, instead of using PMI for SO, assigns positive/negative labels to the unlabeled tweets (based on the seeds), and then trains an SVM classifier on them, using word n-grams as features. These n-grams are then used as lexicon entries with the learned classifier weights as polarity scores. While this is an interesting approach, in our experiments below, we will stick to PMI as a more established method to estimate SO. Finally, there is a related task at SemEval-2016 on predicting the out-of-context sentiment intensity of phrases (Kiritchenko et al., 2016), but there the focus is on multiword phrases. 4 Building Goodness Polarity Lexicons We use SO to build goodness polarity lexicons for good/bad comments in the forum. Instead of using positive and negative sentiment words as seeds, we start with seed words that are associated with good or bad comments. Unlike the work above, we do not do pure bootstrapping, but rather we use a semisupervised approach, which works in two steps. Step 1: In order to come up with a list of words that signal a good/bad comment (which is not as easy as it is to come up with such words manually), we look for words th"
S16-1130,P16-2065,1,0.688768,"ent ranking. 848 While our PMI-cool system did not perform very well at the competition as it lacked important features and as we had a bug at submission time, our goodness polarity lexicons proved useful and contributed to the strong performance of another topperforming system at SemEval-2016 Task 3: SUper team (Mihaylova et al., 2016). In future work, we plan to strengthen our system with more features. In particular, we would like to incorporate rich knowledge sources, e.g., semantic similarity features based on fine-tuned word embeddings and topics similarities as in the SemanticZ system (Mihaylov and Nakov, 2016b). There are also plenty of interesting features to borrow from the SUper Team system (Mihaylova et al., 2016), including veracity, text complexity, and troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a). It would be interesting to combine these in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We further plan to use information from"
S16-1130,S16-1136,1,0.831115,"ent ranking. 848 While our PMI-cool system did not perform very well at the competition as it lacked important features and as we had a bug at submission time, our goodness polarity lexicons proved useful and contributed to the strong performance of another topperforming system at SemEval-2016 Task 3: SUper team (Mihaylova et al., 2016). In future work, we plan to strengthen our system with more features. In particular, we would like to incorporate rich knowledge sources, e.g., semantic similarity features based on fine-tuned word embeddings and topics similarities as in the SemanticZ system (Mihaylov and Nakov, 2016b). There are also plenty of interesting features to borrow from the SUper Team system (Mihaylova et al., 2016), including veracity, text complexity, and troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a). It would be interesting to combine these in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We further plan to use information from"
S16-1130,K15-1032,1,0.507639,"ibuted to the strong performance of another topperforming system at SemEval-2016 Task 3: SUper team (Mihaylova et al., 2016). In future work, we plan to strengthen our system with more features. In particular, we would like to incorporate rich knowledge sources, e.g., semantic similarity features based on fine-tuned word embeddings and topics similarities as in the SemanticZ system (Mihaylov and Nakov, 2016b). There are also plenty of interesting features to borrow from the SUper Team system (Mihaylova et al., 2016), including veracity, text complexity, and troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a). It would be interesting to combine these in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We further plan to use information from entire threads to make better predictions, as using threadlevel information for answer classification has already been shown useful for SemEval-2015 Task 3, subtask A, e.g., by using features modeling the threa"
S16-1130,R15-1058,1,0.589029,"ibuted to the strong performance of another topperforming system at SemEval-2016 Task 3: SUper team (Mihaylova et al., 2016). In future work, we plan to strengthen our system with more features. In particular, we would like to incorporate rich knowledge sources, e.g., semantic similarity features based on fine-tuned word embeddings and topics similarities as in the SemanticZ system (Mihaylov and Nakov, 2016b). There are also plenty of interesting features to borrow from the SUper Team system (Mihaylova et al., 2016), including veracity, text complexity, and troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a). It would be interesting to combine these in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We further plan to use information from entire threads to make better predictions, as using threadlevel information for answer classification has already been shown useful for SemEval-2015 Task 3, subtask A, e.g., by using features modeling the threa"
S16-1130,S13-2053,0,0.0374564,"these words to induce sentiment polarity orientation for new words in a large unannotated set of texts (in his case, product reviews). The idea is that words that co-occur in the same text with positive seed words are likely to be positive, while those that tend to co-occur with negative words are likely to be negative. To quantify this intuition, Turney defines the notion of semantic orientation (SO) for a term w as follows: SO(w) = pmi(w, pos) − pmi(w, neg) where pos and neg stand for any positive and negative seed word, respectively. 845 The idea was later used by other researchers, e.g., Mohammad et al. (2013) built several lexicons based on PMI between words and tweet categories. Here the categories (positive and negative) were defined by a seed set of emotional hashtags, e.g., #happy, #sad, #angry, etc. or by simple positive and negative smileys, e.g., ;), :), ;(, :(. In this case, the resulting lexicons included not only words, but also bigrams and discontinuous pairs of words. Another related work is that of Severyn and Moschitti (2015), who proposed an approach to lexicon induction, which, instead of using PMI for SO, assigns positive/negative labels to the unlabeled tweets (based on the seeds"
S16-1130,S15-2047,1,0.796217,"Missing"
S16-1130,S15-2036,1,0.846065,"Missing"
S16-1130,N15-1159,0,0.0488621,"(w) = pmi(w, pos) − pmi(w, neg) where pos and neg stand for any positive and negative seed word, respectively. 845 The idea was later used by other researchers, e.g., Mohammad et al. (2013) built several lexicons based on PMI between words and tweet categories. Here the categories (positive and negative) were defined by a seed set of emotional hashtags, e.g., #happy, #sad, #angry, etc. or by simple positive and negative smileys, e.g., ;), :), ;(, :(. In this case, the resulting lexicons included not only words, but also bigrams and discontinuous pairs of words. Another related work is that of Severyn and Moschitti (2015), who proposed an approach to lexicon induction, which, instead of using PMI for SO, assigns positive/negative labels to the unlabeled tweets (based on the seeds), and then trains an SVM classifier on them, using word n-grams as features. These n-grams are then used as lexicon entries with the learned classifier weights as polarity scores. While this is an interesting approach, in our experiments below, we will stick to PMI as a more established method to estimate SO. Finally, there is a related task at SemEval-2016 on predicting the out-of-context sentiment intensity of phrases (Kiritchenko e"
S16-1130,P02-1053,0,0.0371245,". Let a and b be two values from the sample space of A and B, respectively. The pointwise mutual information between a and b is defined as follows: P (A = a, B = b) P (A = a) · P (B = b) P (A = a|B = b) = log P (A = a) pmi(a; b) = log (1) (2) pmi(a; b) takes values between −∞, which is when P (A = a, B = b) = 0, and min {− log P (A = a), − log P (B = b)}, when P (A = a|B = b) = P (B = b|A = a) = 1. The mutual information between A and B is the expected value of pmi(a; b): M I(A, B) = XX pmi(a; b) (3) a∈A b∈B PMI is central to a popular approach for bootstrapping sentiment lexicons proposed by Turney (2002). The idea is to start with a small set of seed positive (e.g., excellent) and negative words (bad), and then to use these words to induce sentiment polarity orientation for new words in a large unannotated set of texts (in his case, product reviews). The idea is that words that co-occur in the same text with positive seed words are likely to be positive, while those that tend to co-occur with negative words are likely to be negative. To quantify this intuition, Turney defines the notion of semantic orientation (SO) for a term w as follows: SO(w) = pmi(w, pos) − pmi(w, neg) where pos and neg s"
S16-1136,S16-1130,1,0.829827,"g word embeddings models and features in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We also want to incorporate several rich knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016), and PMIbased goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016), as well as sentiment polarity features (Nicosia et al., 2015). We further plan to use information from entire threads to make better predictions, as using threadlevel information for answer classification has already been shown useful for SemEval-2015 Task 3, subtask A, e.g., by using features modeling the thread structure and dialogue (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015), or by applying threadlevel inference using the predictions of local classifiers (Joty et al., 2015; Joty et al., 2016). How to use such models efficiently in the ranking setup of 2016 is an interesting rese"
S16-1136,P15-2113,1,0.367524,"Missing"
S16-1136,S15-2048,0,0.0329003,"sks A and C. 1 http://alt.qcri.org/semeval2016/task3/ Related Work This year’s SemEval-2016 Task 3 is a follow up of SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Nakov et al., 2015). The 2015 subtask A asked to determine whether an answer was relevant, potentially useful, or bad, while this year this is about ranking. Here we focus on features that use semantic knowledge such as word embeddings, various features extracted from word embeddings, and topic models. Word embeddings and word embeddings similarities have been used by teams in the 2015 edition of the task (Belinkov et al., 2015; Zamanov et al., 2015; Tran et al., 2015; Nicosia et al., 2015). LDA topic have also been used (Tran et al., 2015). Many other features have been tried for the task. For example, Tran et al. (2015) used metadata about the question and the comment. User profile statistics such as number of Good, Bad and Potentially Useful comments by a given user have been used to model user likelihood of posting different types of comment (Nicosia et al., 2015). Vo et al. (2015) and Nicosia et al. (2015) used syntactic tree similarities to compare questions to comments. The problem of selecting relevant answe"
S16-1136,P04-3031,0,0.0979496,"res are semantic similarity based on word embeddings and topics, but we also use some metadata features. 4.1 Preprocessing Before extracting features, we preprocessed the input text using several steps. We first replaced URLs in text with TOKEN URL, numbers with TOKEN NUM, images with TOKEN IMG, and emoticons with TOKEN EMO. We then tokenized the text by matching only continuous alphabet characters including (underscore). Next, we lowercased the result. For the training, the development, and the test datasets, we removed the stopwords using the English stopwords lexicon from the NLTK toolkit (Bird and Loper, 2004). 4.2 Features We used several semantic vector similarity and metadata feature groups. For the similarity measures mentioned below, we used cosine similarity: 1− u.v kuk . kvk (1) Semantic Word Embeddings. We used semantic word embeddings obtained from Word2Vec models trained on different unannotated data sources including the QatarLiving and DohaNews. We also used a model pre-trained on Google News text. For each piece of text such as comment text, question body and question subject, we constructed the centroid vector from the vectors of all words in that text (excluding stopwords). n P i=1 w"
S16-1136,P15-1078,1,0.306188,"Missing"
S16-1136,P16-2075,1,0.713942,"Missing"
S16-1136,S16-1137,1,0.8117,"Missing"
S16-1136,N16-1084,1,0.485647,"Missing"
S16-1136,P16-2065,1,0.0950629,"btask C, which would rank our system second. In future work, we plan to use our best performing word embeddings models and features in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We also want to incorporate several rich knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016), and PMIbased goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016), as well as sentiment polarity features (Nicosia et al., 2015). We further plan to use information from entire threads to make better predictions, as using threadlevel information for answer classification has already been shown useful for SemEval-2015 Task 3, subtask A, e.g., by using features modeling the thread structure and dialogue (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015), or by applying threadlevel inference using the predictions of local classifiers (Joty et al., 2015; Joty et al., 201"
S16-1136,K15-1032,1,0.367762,"core to 78.52 for Subtask A, and to 53.39 for Subtask C, which would rank our system second. In future work, we plan to use our best performing word embeddings models and features in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We also want to incorporate several rich knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016), and PMIbased goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016), as well as sentiment polarity features (Nicosia et al., 2015). We further plan to use information from entire threads to make better predictions, as using threadlevel information for answer classification has already been shown useful for SemEval-2015 Task 3, subtask A, e.g., by using features modeling the thread structure and dialogue (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015), or by applying threadlevel inference using the predictions of lo"
S16-1136,R15-1058,1,0.584005,"core to 78.52 for Subtask A, and to 53.39 for Subtask C, which would rank our system second. In future work, we plan to use our best performing word embeddings models and features in a deep learning architecture, e.g., as in the MTE-NN system (Guzm´an et al., 2016a; Guzm´an et al., 2016b), which borrowed an entire neural network framework and achitecture from previous work on machine translation evaluation (Guzm´an et al., 2015). We also want to incorporate several rich knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016), and PMIbased goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016), as well as sentiment polarity features (Nicosia et al., 2015). We further plan to use information from entire threads to make better predictions, as using threadlevel information for answer classification has already been shown useful for SemEval-2015 Task 3, subtask A, e.g., by using features modeling the thread structure and dialogue (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015), or by applying threadlevel inference using the predictions of lo"
S16-1136,N13-1090,0,0.134014,"iously asked question. The SemEval-2016 Task 3 on Community Question Answering1 (Nakov et al., 2016) aims to solve this real-life problem. The main subtask (Subtask C) asks to find an answer that already exists in the forum and will be appropriate as a response to a newly-posted question. There is also a secondary, Subtask A, which focuses on QuestionComment Similarity and asks to rank the comments within a question-comment thread based on their relevance with respect to the thread’s question. Here, we examine the performance of using different word embeddings obtained with the Word2Vec tool (Mikolov et al., 2013), which we use to build vectors for the questions and the answers. We train classifiers using features derived from these embeddings to solve subtasks A and C. 1 http://alt.qcri.org/semeval2016/task3/ Related Work This year’s SemEval-2016 Task 3 is a follow up of SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Nakov et al., 2015). The 2015 subtask A asked to determine whether an answer was relevant, potentially useful, or bad, while this year this is about ranking. Here we focus on features that use semantic knowledge such as word embeddings, various features extracted"
S16-1136,S15-2047,1,0.393585,"Missing"
S16-1136,S15-2036,1,0.694172,"Missing"
S16-1136,N03-1033,0,0.0506028,"imilarity and we took the average similarity of the top N words. We took the top 1,2,3 and 5 words similarities as features. The assumption here is that if the average similarity for the top N most similar words is high, then the answer might be relevant. Aligned similarity. For each word in the question body, we chose the most similar word from the comment text and we took the average of all best word pair similarities as suggested in (Tran et al., 2015). centroid(w1..n ) = 881 Part of speech (POS) based word vector similarities. We performed part of speech tagging using the Stanford tagger (Toutanova et al., 2003), and we took similarities between centroid vectors of words with a specific tag from the comment text and the centroid vector of the words with a specific tag from the question body text. The assumption is that some parts of speech between the question and the comment might be closer than other parts of speech. Word clusters (WC) similarity. We clustered the word vectors from the Word2Vec vocabulary in 1,000 clusters (with 200 words per cluster on average) using K-Means clustering. We then calculated the cluster similarity between the question body word clusters and the answer text word clust"
S16-1136,S15-2038,0,0.305631,"6/task3/ Related Work This year’s SemEval-2016 Task 3 is a follow up of SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Nakov et al., 2015). The 2015 subtask A asked to determine whether an answer was relevant, potentially useful, or bad, while this year this is about ranking. Here we focus on features that use semantic knowledge such as word embeddings, various features extracted from word embeddings, and topic models. Word embeddings and word embeddings similarities have been used by teams in the 2015 edition of the task (Belinkov et al., 2015; Zamanov et al., 2015; Tran et al., 2015; Nicosia et al., 2015). LDA topic have also been used (Tran et al., 2015). Many other features have been tried for the task. For example, Tran et al. (2015) used metadata about the question and the comment. User profile statistics such as number of Good, Bad and Potentially Useful comments by a given user have been used to model user likelihood of posting different types of comment (Nicosia et al., 2015). Vo et al. (2015) and Nicosia et al. (2015) used syntactic tree similarities to compare questions to comments. The problem of selecting relevant answers has even been approached as a spam fil"
S16-1136,S15-2041,0,0.0205057,"gs, and topic models. Word embeddings and word embeddings similarities have been used by teams in the 2015 edition of the task (Belinkov et al., 2015; Zamanov et al., 2015; Tran et al., 2015; Nicosia et al., 2015). LDA topic have also been used (Tran et al., 2015). Many other features have been tried for the task. For example, Tran et al. (2015) used metadata about the question and the comment. User profile statistics such as number of Good, Bad and Potentially Useful comments by a given user have been used to model user likelihood of posting different types of comment (Nicosia et al., 2015). Vo et al. (2015) and Nicosia et al. (2015) used syntactic tree similarities to compare questions to comments. The problem of selecting relevant answers has even been approached as a spam filtering task (Vo et al., 2015). 2 https://github.com/tbmihailov/ semeval2016-task3-cqa 879 Proceedings of SemEval-2016, pages 879–886, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics 3 Features Qatar Living Forum Qatar Living Forum+Ext Google News Doha News Data In our experiments, we used annotated training, development and testing datasets, as well as a large unannotated dataset,"
S16-1136,S15-2043,0,0.061201,"lt.qcri.org/semeval2016/task3/ Related Work This year’s SemEval-2016 Task 3 is a follow up of SemEval-2015 Task 3 on Answer Selection in Community Question Answering (Nakov et al., 2015). The 2015 subtask A asked to determine whether an answer was relevant, potentially useful, or bad, while this year this is about ranking. Here we focus on features that use semantic knowledge such as word embeddings, various features extracted from word embeddings, and topic models. Word embeddings and word embeddings similarities have been used by teams in the 2015 edition of the task (Belinkov et al., 2015; Zamanov et al., 2015; Tran et al., 2015; Nicosia et al., 2015). LDA topic have also been used (Tran et al., 2015). Many other features have been tried for the task. For example, Tran et al. (2015) used metadata about the question and the comment. User profile statistics such as number of Good, Bad and Potentially Useful comments by a given user have been used to model user likelihood of posting different types of comment (Nicosia et al., 2015). Vo et al. (2015) and Nicosia et al. (2015) used syntactic tree similarities to compare questions to comments. The problem of selecting relevant answers has even been appro"
S16-1136,P06-4018,0,\N,Missing
S16-1136,W02-0109,0,\N,Missing
S16-1136,D15-1068,1,\N,Missing
S16-1137,S16-1130,1,0.829487,"uld work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HBKU, part of Qatar Foundation. It is part of the Interactive sYstems for Answer Search (Iyas) project, which is developed in collaboration with M"
S16-1137,P15-2113,1,0.3401,"Missing"
S16-1137,2012.eamt-1.60,0,0.0222671,"of the hypotheses and of the reference, length ratio between them, and BLEU’s brevity penalty. We will refer to the set of these features as BLEU COMP. 4.3 Task-specific features QL VEC (in MTE-NN-improved only). Similarly to the G OOGLE VEC, but on task-specific data, we train word vectors using WORD 2 VEC on all available cQA training data (Qatar Living) and use them as input to the NN. QL+IWSLT VEC (in MTE-NN-{primary, contrastive1/2} only). We also use trained word vectors on the concatenation of the cQA training data and the English portion of the IWSLT data, which consists of TED talks (Cettolo et al., 2012) and is thus informal and somewhat similar to cQA data. TASK FEAT. We further extract various taskspecific skip-arc features, most of them proposed for the 2015 edition of the task (Nakov et al., 2015). This includes some comment-specific features: • number of URLs/images/emails/phone numbers; • number of occurrences of the string thank;3 • number of tokens/sentences; • average number of tokens; • type/token ratio; • number of nouns/verbs/adjectives/adverbs/pronouns; • number of positive/negative smileys; • number of single/double/triple exclamation/interrogation symbols; • number of interroga"
S16-1137,P15-2114,0,0.102811,"we used our system for subtask A to solve subtask C, which asks to find good answers to a new question that was not asked before in the forum by reranking the answers to related questions. For the purpose, we weighted the subtask A scores by the reciprocal rank of the related questions (following the order given by the organizers, i.e., the ranking by Google). Without any subtask C specific addition, we achieved the fourth best result in the task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al."
S16-1137,P03-1003,0,0.0981864,"asks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who app"
S16-1137,P15-1078,1,0.448296,"Missing"
S16-1137,P16-2075,1,0.558646,"Missing"
S16-1137,D15-1068,1,0.265429,"Missing"
S16-1137,N16-1084,1,0.557796,"Missing"
S16-1137,P11-1143,0,0.386837,"al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a differ"
S16-1137,N12-1019,0,0.0191253,"2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreover, instead of using MTE metrics as features, we port an entire MTE framework to the cQA problem. 3 Neural Model for Answer Ranking The NN model we use for answer ranking is depicted in Figure 1. It is a direct adaptation of the feed-forward NN for MTE described in (Guzm´an et al., 2015). Technically, we have a binary classification task with input (q, c1 , c2 ), which should output 1 if c1 is a better answer to q than c2 , and 0 otherwise.2 The network computes a sigmoid"
S16-1137,P16-2065,1,0.435928,"we have adopted a pairwise neural network architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings of the input texts that are non-linearly combined in the hidden layer. 892 Our post-competition improvements have shown state-of-the-art performance (Guzm´an et al., 2016), with sizeable contribution from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions a"
S16-1137,S16-1136,1,0.763699,"we have adopted a pairwise neural network architecture, which incorporates MTE features, as well as rich syntactic and semantic embeddings of the input texts that are non-linearly combined in the hidden layer. 892 Our post-competition improvements have shown state-of-the-art performance (Guzm´an et al., 2016), with sizeable contribution from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions a"
S16-1137,K15-1032,1,0.168613,"from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HB"
S16-1137,R15-1058,1,0.13714,"from both the MTE features and from the network architecture. This is an encouraging result as it was not a priori clear that an MTE approach would work well for cQA. In future work, we plan to incorporate fine-tuned word embeddings as in the SemanticZ system (Mihaylov and Nakov, 2016b), and information from entire threads (Nicosia et al., 2015; Barr´on-Cede˜no et al., 2015; Joty et al., 2015; Joty et al., 2016). We also want to add more knowledge sources, e.g., as in the SUper Team system (Mihaylova et al., 2016), including veracity, sentiment, complexity, troll user features as inspired by (Mihaylov et al., 2015a; Mihaylov et al., 2015b; Mihaylov and Nakov, 2016a), and PMI-based goodness polarity lexicons as in the PMI-cool system (Balchev et al., 2016). We further plan to explore the application of our NN architecture to subtasks B and C, and to study the interactions among the three subtasks in order to solve the primary subtask C. Furthermore, we would like to try a similar neural network for other semantic similarity problems, such as textual entailment. Acknowledgments This research was performed by the Arabic Language Technologies (ALT) group at the Qatar Computing Research Institute (QCRI), HB"
S16-1137,N13-1090,0,0.0509505,"(q, c1 , c2 ), ψ(q, c1 ), ψ(q, c2 )] + bv ). 4 Learning Features We experiment with three kinds of features: (i) input embeddings, (ii) features motivated by previous work on Machine Translation Evaluation (MTE) (Guzm´an et al., 2015) and (iii) task-specific features, mostly proposed by participants in the 2015 edition of the task (Nakov et al., 2015). 4.1 Embedding Features We use the following vector-based embeddings of (q, c1 , c2 ) as input to the NN: • G OOGLE VEC: We use the pre-trained, 300dimensional embedding vectors, which Tomas Mikolov trained on 100 billion words from Google News (Mikolov et al., 2013). • S YNTAX VEC: We parse the entire question/comment text using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. 889 Moreover, we use the above vectors to calculate pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evalu"
S16-1137,S15-2047,1,0.355744,"Missing"
S16-1137,S15-2036,1,0.557554,"Missing"
S16-1137,P02-1040,0,0.113293,"late pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candidate answer: • B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). • NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). • TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). • M ETEOR: A measure that matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). • P RECISION: measure, originating in information retrieval. • R ECALL: another measure coming from information retrieval. BLEU COMP. Following (Guzm´an et al., 2015), we further use as features various components that are"
S16-1137,P07-1059,0,0.170705,"s et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. Howeve"
S16-1137,2006.amta-papers.25,0,0.118347,"only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candidate answer: • B LEU: This is the most commonly used measure for machine translation evaluation, which is based on n-gram overlap and length ratios (Papineni et al., 2002). • NIST: This measure is similar to B LEU, and is used at evaluation campaigns run by NIST (Doddington, 2002). • TER: Translation error rate; it is based on the edit distance between a translation hypothesis and the reference (Snover et al., 2006). • M ETEOR: A measure that matches the hypothesis and the reference using synonyms and paraphrases (Lavie and Denkowski, 2009). • P RECISION: measure, originating in information retrieval. • R ECALL: another measure coming from information retrieval. BLEU COMP. Following (Guzm´an et al., 2015), we further use as features various components that are involved in the computation of B LEU: n-gram precisions, n-gram matches, total number of n-grams (n=1,2,3,4), lengths of the hypotheses and of the reference, length ratio between them, and BLEU’s brevity penalty. We will refer to the set of these f"
S16-1137,P13-1045,0,0.067615,"nput embeddings, (ii) features motivated by previous work on Machine Translation Evaluation (MTE) (Guzm´an et al., 2015) and (iii) task-specific features, mostly proposed by participants in the 2015 edition of the task (Nakov et al., 2015). 4.1 Embedding Features We use the following vector-based embeddings of (q, c1 , c2 ) as input to the NN: • G OOGLE VEC: We use the pre-trained, 300dimensional embedding vectors, which Tomas Mikolov trained on 100 billion words from Google News (Mikolov et al., 2013). • S YNTAX VEC: We parse the entire question/comment text using the Stanford neural parser (Socher et al., 2013), and we use the final 25-dimensional vector that is produced internally as a by-product of parsing. 889 Moreover, we use the above vectors to calculate pairwise similarity features. More specifically, given a question q and a pair of comments c1 and c2 for it, we calculate the following features: ψ(q, c1 ) = cos(q, c1 ) and ψ(q, c2 ) = cos(q, c2 ). 4.2 MTE features MT FEATS (in MTE-NN-improved only). We use (as skip-arc pairwise features) the following six machine translation evaluation features, to which we refer as MT FEATS, and which measure the similarity between the question and a candid"
S16-1137,J11-2003,0,0.0693038,"election (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreo"
S16-1137,S15-2038,0,0.178473,"oschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015), e.g., a variation of IBM model 1, to compute the probability that the question is a “translation” of the candidate answer. Unlike that work, here we use machine translation evaluation (MTE) instead of machine translation models. 888 sentences embeddings xq pairwise nodes pairwise features hq1 ψ(q,c1) ψ(q,c2) q xc1 hq2 v f(q,c1,c2) c1 xc2 h12 output layer c2 Figure 1: Overall architecture of the NN. Another relevant work is that of Madnani et al. (2012), who applied MTE metrics as features for paraphrase identification. However, here we have a different problem: cQA. Moreover, instead of usin"
S16-1137,P15-2116,0,0.0572579,"Missing"
S16-1137,P15-1025,0,0.0622365,"the network. Finally, we used our system for subtask A to solve subtask C, which asks to find good answers to a new question that was not asked before in the forum by reranking the answers to related questions. For the purpose, we weighted the subtask A scores by the reciprocal rank of the related questions (following the order given by the organizers, i.e., the ranking by Google). Without any subtask C specific addition, we achieved the fourth best result in the task. 2 Related Work Recently, many neural network (NN) models have been applied to cQA tasks: e.g., question-question similarity (Zhou et al., 2015; dos Santos et al., 2015; Lei et al., 2016) and answer selection (Severyn and Moschitti, 2015; Wang and Nyberg, 2015; Shen et al., 2015; Feng et al., 2015; Tan et al., 2015). Also, other participants in the SemEval 2016 Task 3 applied NNs to solve some of the subtasks (Nakov et al., 2016). However, our goal was different: we were interested in extending an existing pairwise NN framework from a different but related problem. There is also work that uses scores from machine translation models as a features for cQA (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Br"
S17-2003,S17-2044,0,0.0542572,"Missing"
S17-2003,N10-1145,0,0.016031,"es used by these systems and provides further discussion. Finally, Section 6 presents the main conclusions. 2 Question-answer similarity has been a subtask (subtask A) of our task in its two previous editions (Nakov et al., 2015, 2016b). This is a wellresearched problem in the context of general question answering. One research direction has been to try to match the syntactic structure of the question to that of the candidate answer. For example, Wang et al. (2007) proposed a probabilistic quasi-synchronous grammar to learn syntactic transformations from the question to the candidate answers. Heilman and Smith (2010) used an algorithm based on Tree Edit Distance (TED) to learn tree transformations in pairs. Wang and Manning (2010) developed a probabilistic model to learn tree-edit operations on dependency parse trees. Yao et al. (2013) applied linear chain conditional random fields (CRFs) with features derived from TED to learn associations between questions and candidate answers. Moreover, syntactic structure was central for some of the top systems that participated in SemEval-2016 Task 3 (Filice et al., 2016; Barr´on-Cede˜no et al., 2016). Related Work The first step to automatically answer questions on"
S17-2003,C16-2001,1,0.881977,"Missing"
S17-2003,S15-2035,0,0.0226369,"andidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features that only look at the question–answer pair. For example, the second-best team, HITSZ-ICRC, used as a feature the position of the comment in the thread, such as whether the answer is first or last (Hou et al., 2015). Similarly, the third-best team, QCRI, used features to model a comment in the context of the entire comment thread, focusing on user interaction (Nicosia et al., 2015). Finally, the fifth-best team, ICRC-HIT, treated the answer selection task as a sequence labeling problem and proposed recurrent convolutional neural networks to recognize good comments (Zhou et al., 2015b). In follow-up work, Zhou et al. (2015a) included long-short term memory (LSTM) units in their convolutional neural network to model the classification sequence for the thread, and Barr´on-Cede˜no et al. (2015) exploited the"
S17-2003,K15-1032,1,0.0248911,"to make more consistent global decisions about the goodness of the answers in the thread. They modeled the relations between pairs of comments at any distance in the thread, and combined the predictions of local classifiers using graph-cut and Integer Linear Programming. In follow up work, Joty et al. (2016) proposed joint learning models that integrate inference within the learning process using global normalization and an Ising-like edge potential. 5 https://github.com/tbmihailov/ semeval2016-task3-cqa 6 Using a heuristic that if several users call somebody a troll, then s/he should be one (Mihaylov et al., 2015a,b; Mihaylov and Nakov, 2016a; Mihaylov et al., 2017b). 30 Category Original Questions Train+Dev+Test from SemEval-2015 – Train(1,2)+Dev+Test from SemEval-2016 (200+67)+50+70 2,480+291+319 – – – (1,999+670)+500+700 (181+54)+59+81 (606+242)+155+152 (1,212+374)+286+467 880 24 139 717 – (19,990+6,700)+5,000+7,000 8,800 – – – (1,988+849)+345+654 (16,319+5,154)+4,061+5,943 (1,683+697)+594+403 246 8,291 263 (14,110+3,790)+2,440+3,270 2,930 (5,287+1,364)+818+1,329 (6,362+1,777)+1,209+1,485 (2,461+649)+413+456 1,523 1,407 0 Related Questions – Perfect Match – Relevant – Irrelevant Related Comments (w"
S17-2003,S17-2009,0,0.0610799,"Missing"
S17-2003,S15-2036,1,0.824235,"Missing"
S17-2003,J11-2003,0,0.0485113,"ting systems across all three subtasks. This includes fine-tuned word embeddings5 (Mihaylov and Nakov, 2016b); features modeling text complexity, veracity, and user trollness6 (Mihaylova et al., 2016); sentiment polarity features (Nicosia et al., 2015); and PMI-based goodness polarity lexicons (Balchev et al., 2016; Mihaylov et al., 2017a). Yet another research direction has been on using machine translation models as features for question-answer similarity (Berger et al., 2000; Echihabi and Marcu, 2003; Jeon et al., 2005; Soricut and Brill, 2006; Riezler et al., 2007; Li and Manandhar, 2011; Surdeanu et al., 2011; Tran et al., 2015; Hoogeveen et al., 2016a; Wu and Zhang, 2016), e.g., a variation of IBM model 1 (Brown et al., 1993), to compute the probability that the question is a “translation” of the candidate answer. Similarly, (Guzm´an et al., 2016a,b) ported an entire machine translation evaluation framework (Guzm´an et al., 2015) to the CQA problem. Using information about the answer thread is another important direction, which has been explored mainly to address Subtask A. In the 2015 edition of the task, the top participating systems used thread-level features, in addition to local features tha"
S17-2003,D16-1244,0,0.0146703,"Missing"
S17-2003,S17-2059,0,0.0505879,"Missing"
S17-2088,W11-0413,0,0.00529951,"esearch community, not only for general multilingual sentiment analysis, but also for multilingual sentiment analysis towards a topic, which is still a largely unexplored research direction for many languages and in particular for morphologically complex languages such as Arabic. Arabic has become an emergent language for sentiment analysis, especially as more resources and tools for it have recently become available. It is also both interesting and challenging due to its rich morphology and abundance of dialectal use in Twitter. Early Arabic studies focused on sentiment analysis in newswire (Abdul-Mageed and Diab, 2011; Elarnaoty et al., 2012), but recently there has been a lot more work on social media, especially Twitter (Mourad and Darwish, 2013; AbdulMageed et al., 2014; Refaee and Rieser, 2014; Salameh et al., 2015), where the challenges of sentiment analysis are compounded by the presence of multiple dialects and orthographical variants, which are frequently used in conjunction with the formal written language. 2 Task Definition SemEval-2017 Task 4 consists of five subtasks, each offered for both Arabic and English: 1. Subtask A: Given a tweet, decide whether it expresses P OSITIVE, N EGATIVE or N EU"
S17-2088,S17-2136,0,0.032979,"Missing"
S17-2088,S17-2100,0,0.0311852,"Missing"
S17-2088,S17-2094,0,0.0934299,"Missing"
S17-2088,S17-2131,0,0.0271268,"Missing"
S17-2088,S17-2118,0,0.0359005,"Missing"
S17-2088,S17-2135,0,0.0337418,"Missing"
S17-2088,S17-2127,0,0.025875,"Missing"
S17-2088,S17-2106,0,0.0349085,"Missing"
S17-2088,S17-2126,0,0.0773318,"Missing"
S17-2088,S17-2133,0,0.033901,"Missing"
S17-2088,S17-2122,0,0.0294111,"Missing"
S17-2088,P15-1073,0,0.0305592,"d in individual tweets. We should also note that quantification is not a mere byproduct of classification, as it can be addressed using different approaches and it also needs different evaluation measures (Forman, 2008; Esuli and Sebastiani, 2015). User Information Demographic information in Twitter has been studied and analyzed using network analysis and natural language processing (NLP) techniques (Mislove et al., 2011; Nguyen et al., 2013; Rosenthal and McKeown, 2016). Recent work has shown that user information and information from the network can help sentiment analysis in other corpora (Hovy, 2015) and in Twitter (Volkova et al., 2013; Yang and Eisenstein, 2015). Thus, this year we encouraged participants to use information from the public profiles of Twitter users such as demographics (e.g., age, location) as well as information from the rest of the social network (e.g., sentiment of the tweets of friends), with the goal of analyzing the impact of this information on improving sentiment analysis. The rest of this paper is organized as follows. Section 2 presents in more detail the five subtasks of SemEval-2017 Task 4. Section 3 describes the English and the Arabic datasets and how we c"
S17-2088,E17-1094,1,0.708391,"cale to an ordered five-point scale means moving from binary to ordinal classification (aka ordinal regression). 502 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 502–518, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Some work studied the utility of machine translation for sentiment analysis of Arabic texts (Salameh et al., 2015; Mohammad et al., 2016b; Refaee and Rieser, 2015), identification of sentiment holders (Elarnaoty et al., 2012), and sentiment targets (Al-Smadi et al., 2015; Farra et al., 2015; Farra and McKeown, 2017). We believe that the development of a standard Arabic Twitter dataset for sentiment, and particularly with respect to topics, will encourage further research in this regard. Tweet Quantification SemEval-2017 Task 4 includes tweet quantification tasks along with tweet classification tasks, also on 2-point and 5-point scales. While the tweet classification task is concerned with whether a specific tweet expresses a given sentiment towards a topic, the tweet quantification task looks at estimating the distribution of tweets about a given topic across the different sentiment classes. Most (if not"
S17-2088,S17-2120,0,0.029874,"Missing"
S17-2088,W15-3210,1,0.71717,"egorical two-point scale to an ordered five-point scale means moving from binary to ordinal classification (aka ordinal regression). 502 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 502–518, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Some work studied the utility of machine translation for sentiment analysis of Arabic texts (Salameh et al., 2015; Mohammad et al., 2016b; Refaee and Rieser, 2015), identification of sentiment holders (Elarnaoty et al., 2012), and sentiment targets (Al-Smadi et al., 2015; Farra et al., 2015; Farra and McKeown, 2017). We believe that the development of a standard Arabic Twitter dataset for sentiment, and particularly with respect to topics, will encourage further research in this regard. Tweet Quantification SemEval-2017 Task 4 includes tweet quantification tasks along with tweet classification tasks, also on 2-point and 5-point scales. While the tweet classification task is concerned with whether a specific tweet expresses a given sentiment towards a topic, the tweet quantification task looks at estimating the distribution of tweets about a given topic across the different senti"
S17-2088,S17-2115,0,0.0475427,"Missing"
S17-2088,S17-2104,0,0.034982,"Missing"
S17-2088,S15-2080,0,0.00753509,"bin Khalifa University, Qatar ♦ Department of Computer Science, Columbia University ♣ IBM Research, USA Abstract SemEval is the International Workshop on Semantic Evaluation, formerly SensEval. It is an ongoing series of evaluations of computational semantic analysis systems, organized under the umbrella of SIGLEX, the Special Interest Group on the Lexicon of the Association for Computational Linguistics. Other related tasks at SemEval have explored sentiment analysis of product review and their aspects (Pontiki et al., 2014, 2015, 2016), sentiment analysis of figurative language on Twitter (Ghosh et al., 2015), implicit event polarity (Russo et al., 2015), detecting stance in tweets (Mohammad et al., 2016a), out-of-context sentiment intensity of words and phrases (Kiritchenko et al., 2016), and emotion detection (Strapparava and Mihalcea, 2007). Some of these tasks featured languages other than English, such as Arabic (Pontiki et al., 2016; Mohammad et al., 2016a); however, they did not target tweets, nor did they focus on sentiment towards a topic. This year, we performed a re-run of the subtasks in SemEval-2016 Task 4, which, in addition to the overall sentiment of a tweet, featured classificatio"
S17-2088,S17-2113,0,0.036781,"Missing"
S17-2088,S17-2121,0,0.0267073,"Missing"
S17-2088,S16-1004,0,0.017382,"ormerly SensEval. It is an ongoing series of evaluations of computational semantic analysis systems, organized under the umbrella of SIGLEX, the Special Interest Group on the Lexicon of the Association for Computational Linguistics. Other related tasks at SemEval have explored sentiment analysis of product review and their aspects (Pontiki et al., 2014, 2015, 2016), sentiment analysis of figurative language on Twitter (Ghosh et al., 2015), implicit event polarity (Russo et al., 2015), detecting stance in tweets (Mohammad et al., 2016a), out-of-context sentiment intensity of words and phrases (Kiritchenko et al., 2016), and emotion detection (Strapparava and Mihalcea, 2007). Some of these tasks featured languages other than English, such as Arabic (Pontiki et al., 2016; Mohammad et al., 2016a); however, they did not target tweets, nor did they focus on sentiment towards a topic. This year, we performed a re-run of the subtasks in SemEval-2016 Task 4, which, in addition to the overall sentiment of a tweet, featured classification, ordinal regression, and quantification with respect to a topic. Furthermore, we introduced a new language, Arabic. Finally, we made available to the participants demographic inform"
S17-2088,S17-2103,0,0.0431188,"Missing"
S17-2088,S17-2112,0,0.0291584,"Missing"
S17-2088,S17-2110,0,0.0344685,"Missing"
S17-2088,S17-2125,0,0.0336735,"Missing"
S17-2088,S16-1001,1,0.184969,"cipating this year. 1 Introduction The identification of sentiment in text is an important field of study, with social media platforms such as Twitter garnering the interest of researchers in language processing as well as in political and social sciences. The task usually involves detecting whether a piece of text expresses a P OSITIVE, a N EGATIVE, or a N EUTRAL sentiment; the sentiment can be general or about a specific topic, e.g., a person, a product, or an event. The Sentiment Analysis in Twitter task has been run yearly at SemEval since 2013 (Nakov et al., 2013; Rosenthal et al., 2014; Nakov et al., 2016b), with the 2015 task introducing sentiment towards a topic (Rosenthal et al., 2015) and the 2016 task introducing tweet quantification and five-point ordinal classification (Nakov et al., 2016a). Ordinal Classification As last year, SemEval2017 Task 4 includes sentiment analysis on a fivepoint scale {H IGHLY P OSITIVE, P OSITIVE, N EU TRAL , N EGATIVE , H IGHLY N EGATIVE }, which is in line with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp. In machine learning terms, moving from a categorical two-point scale to an ordered five-point scale means moving"
S17-2088,S17-2132,0,0.034206,"Missing"
S17-2088,S17-2124,0,0.034501,"Missing"
S17-2088,S13-2052,1,0.370524,"very popular, with a total of 48 teams participating this year. 1 Introduction The identification of sentiment in text is an important field of study, with social media platforms such as Twitter garnering the interest of researchers in language processing as well as in political and social sciences. The task usually involves detecting whether a piece of text expresses a P OSITIVE, a N EGATIVE, or a N EUTRAL sentiment; the sentiment can be general or about a specific topic, e.g., a person, a product, or an event. The Sentiment Analysis in Twitter task has been run yearly at SemEval since 2013 (Nakov et al., 2013; Rosenthal et al., 2014; Nakov et al., 2016b), with the 2015 task introducing sentiment towards a topic (Rosenthal et al., 2015) and the 2016 task introducing tweet quantification and five-point ordinal classification (Nakov et al., 2016a). Ordinal Classification As last year, SemEval2017 Task 4 includes sentiment analysis on a fivepoint scale {H IGHLY P OSITIVE, P OSITIVE, N EU TRAL , N EGATIVE , H IGHLY N EGATIVE }, which is in line with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp. In machine learning terms, moving from a categorical two-point scale"
S17-2088,E12-1062,0,0.0160791,"pics, will encourage further research in this regard. Tweet Quantification SemEval-2017 Task 4 includes tweet quantification tasks along with tweet classification tasks, also on 2-point and 5-point scales. While the tweet classification task is concerned with whether a specific tweet expresses a given sentiment towards a topic, the tweet quantification task looks at estimating the distribution of tweets about a given topic across the different sentiment classes. Most (if not all) tweet sentiment classification studies within political science (Borge-Holthoefer et al., 2015; Kaya et al., 2013; Marchetti-Bowick and Chambers, 2012), economics (Bollen et al., 2011; O’Connor et al., 2010), social science (Dodds et al., 2011), and market research (Burton and Soboleva, 2011; Qureshi et al., 2013), study Twitter with an interest in aggregate statistics about sentiment and are not interested in the sentiment expressed in individual tweets. We should also note that quantification is not a mere byproduct of classification, as it can be addressed using different approaches and it also needs different evaluation measures (Forman, 2008; Esuli and Sebastiani, 2015). User Information Demographic information in Twitter has been studi"
S17-2088,S17-2111,0,0.0479537,"Missing"
S17-2088,S16-1003,0,0.114327,"Missing"
S17-2088,W13-1608,0,0.0100057,"which is still a largely unexplored research direction for many languages and in particular for morphologically complex languages such as Arabic. Arabic has become an emergent language for sentiment analysis, especially as more resources and tools for it have recently become available. It is also both interesting and challenging due to its rich morphology and abundance of dialectal use in Twitter. Early Arabic studies focused on sentiment analysis in newswire (Abdul-Mageed and Diab, 2011; Elarnaoty et al., 2012), but recently there has been a lot more work on social media, especially Twitter (Mourad and Darwish, 2013; AbdulMageed et al., 2014; Refaee and Rieser, 2014; Salameh et al., 2015), where the challenges of sentiment analysis are compounded by the presence of multiple dialects and orthographical variants, which are frequently used in conjunction with the formal written language. 2 Task Definition SemEval-2017 Task 4 consists of five subtasks, each offered for both Arabic and English: 1. Subtask A: Given a tweet, decide whether it expresses P OSITIVE, N EGATIVE or N EU TRAL sentiment. 2. Subtask B: Given a tweet and a topic, classify the sentiment conveyed towards that topic on a two-point scale: P"
S17-2088,S14-2004,0,0.0421543,"Sara Rosenthal♣ , Noura Farra♦ , Preslav Nakov♥ Qatar Computing Research Institute, Hamad bin Khalifa University, Qatar ♦ Department of Computer Science, Columbia University ♣ IBM Research, USA Abstract SemEval is the International Workshop on Semantic Evaluation, formerly SensEval. It is an ongoing series of evaluations of computational semantic analysis systems, organized under the umbrella of SIGLEX, the Special Interest Group on the Lexicon of the Association for Computational Linguistics. Other related tasks at SemEval have explored sentiment analysis of product review and their aspects (Pontiki et al., 2014, 2015, 2016), sentiment analysis of figurative language on Twitter (Ghosh et al., 2015), implicit event polarity (Russo et al., 2015), detecting stance in tweets (Mohammad et al., 2016a), out-of-context sentiment intensity of words and phrases (Kiritchenko et al., 2016), and emotion detection (Strapparava and Mihalcea, 2007). Some of these tasks featured languages other than English, such as Arabic (Pontiki et al., 2016; Mohammad et al., 2016a); however, they did not target tweets, nor did they focus on sentiment towards a topic. This year, we performed a re-run of the subtasks in SemEval-201"
S17-2088,S15-2077,0,0.0084312,"Computer Science, Columbia University ♣ IBM Research, USA Abstract SemEval is the International Workshop on Semantic Evaluation, formerly SensEval. It is an ongoing series of evaluations of computational semantic analysis systems, organized under the umbrella of SIGLEX, the Special Interest Group on the Lexicon of the Association for Computational Linguistics. Other related tasks at SemEval have explored sentiment analysis of product review and their aspects (Pontiki et al., 2014, 2015, 2016), sentiment analysis of figurative language on Twitter (Ghosh et al., 2015), implicit event polarity (Russo et al., 2015), detecting stance in tweets (Mohammad et al., 2016a), out-of-context sentiment intensity of words and phrases (Kiritchenko et al., 2016), and emotion detection (Strapparava and Mihalcea, 2007). Some of these tasks featured languages other than English, such as Arabic (Pontiki et al., 2016; Mohammad et al., 2016a); however, they did not target tweets, nor did they focus on sentiment towards a topic. This year, we performed a re-run of the subtasks in SemEval-2016 Task 4, which, in addition to the overall sentiment of a tweet, featured classification, ordinal regression, and quantification with"
S17-2088,N15-1078,0,0.210418,"IVE , H IGHLY N EGATIVE }, which is in line with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp. In machine learning terms, moving from a categorical two-point scale to an ordered five-point scale means moving from binary to ordinal classification (aka ordinal regression). 502 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 502–518, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Some work studied the utility of machine translation for sentiment analysis of Arabic texts (Salameh et al., 2015; Mohammad et al., 2016b; Refaee and Rieser, 2015), identification of sentiment holders (Elarnaoty et al., 2012), and sentiment targets (Al-Smadi et al., 2015; Farra et al., 2015; Farra and McKeown, 2017). We believe that the development of a standard Arabic Twitter dataset for sentiment, and particularly with respect to topics, will encourage further research in this regard. Tweet Quantification SemEval-2017 Task 4 includes tweet quantification tasks along with tweet classification tasks, also on 2-point and 5-point scales. While the tweet classification task is concerned with whether a speci"
S17-2088,N15-2010,0,0.0142604,"th product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp. In machine learning terms, moving from a categorical two-point scale to an ordered five-point scale means moving from binary to ordinal classification (aka ordinal regression). 502 Proceedings of the 11th International Workshop on Semantic Evaluations (SemEval-2017), pages 502–518, c Vancouver, Canada, August 3 - 4, 2017. 2017 Association for Computational Linguistics Some work studied the utility of machine translation for sentiment analysis of Arabic texts (Salameh et al., 2015; Mohammad et al., 2016b; Refaee and Rieser, 2015), identification of sentiment holders (Elarnaoty et al., 2012), and sentiment targets (Al-Smadi et al., 2015; Farra et al., 2015; Farra and McKeown, 2017). We believe that the development of a standard Arabic Twitter dataset for sentiment, and particularly with respect to topics, will encourage further research in this regard. Tweet Quantification SemEval-2017 Task 4 includes tweet quantification tasks along with tweet classification tasks, also on 2-point and 5-point scales. While the tweet classification task is concerned with whether a specific tweet expresses a given sentiment towards a to"
S17-2088,S17-2105,0,0.0384855,"Missing"
S17-2088,W16-5604,1,0.687692,"eva, 2011; Qureshi et al., 2013), study Twitter with an interest in aggregate statistics about sentiment and are not interested in the sentiment expressed in individual tweets. We should also note that quantification is not a mere byproduct of classification, as it can be addressed using different approaches and it also needs different evaluation measures (Forman, 2008; Esuli and Sebastiani, 2015). User Information Demographic information in Twitter has been studied and analyzed using network analysis and natural language processing (NLP) techniques (Mislove et al., 2011; Nguyen et al., 2013; Rosenthal and McKeown, 2016). Recent work has shown that user information and information from the network can help sentiment analysis in other corpora (Hovy, 2015) and in Twitter (Volkova et al., 2013; Yang and Eisenstein, 2015). Thus, this year we encouraged participants to use information from the public profiles of Twitter users such as demographics (e.g., age, location) as well as information from the rest of the social network (e.g., sentiment of the tweets of friends), with the goal of analyzing the impact of this information on improving sentiment analysis. The rest of this paper is organized as follows. Section"
S17-2088,S15-2078,1,0.432234,"important field of study, with social media platforms such as Twitter garnering the interest of researchers in language processing as well as in political and social sciences. The task usually involves detecting whether a piece of text expresses a P OSITIVE, a N EGATIVE, or a N EUTRAL sentiment; the sentiment can be general or about a specific topic, e.g., a person, a product, or an event. The Sentiment Analysis in Twitter task has been run yearly at SemEval since 2013 (Nakov et al., 2013; Rosenthal et al., 2014; Nakov et al., 2016b), with the 2015 task introducing sentiment towards a topic (Rosenthal et al., 2015) and the 2016 task introducing tweet quantification and five-point ordinal classification (Nakov et al., 2016a). Ordinal Classification As last year, SemEval2017 Task 4 includes sentiment analysis on a fivepoint scale {H IGHLY P OSITIVE, P OSITIVE, N EU TRAL , N EGATIVE , H IGHLY N EGATIVE }, which is in line with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp. In machine learning terms, moving from a categorical two-point scale to an ordered five-point scale means moving from binary to ordinal classification (aka ordinal regression). 502 Proceedings of t"
S17-2088,S07-1013,0,0.111401,"Missing"
S17-2088,S14-2009,1,0.462302,"total of 48 teams participating this year. 1 Introduction The identification of sentiment in text is an important field of study, with social media platforms such as Twitter garnering the interest of researchers in language processing as well as in political and social sciences. The task usually involves detecting whether a piece of text expresses a P OSITIVE, a N EGATIVE, or a N EUTRAL sentiment; the sentiment can be general or about a specific topic, e.g., a person, a product, or an event. The Sentiment Analysis in Twitter task has been run yearly at SemEval since 2013 (Nakov et al., 2013; Rosenthal et al., 2014; Nakov et al., 2016b), with the 2015 task introducing sentiment towards a topic (Rosenthal et al., 2015) and the 2016 task introducing tweet quantification and five-point ordinal classification (Nakov et al., 2016a). Ordinal Classification As last year, SemEval2017 Task 4 includes sentiment analysis on a fivepoint scale {H IGHLY P OSITIVE, P OSITIVE, N EU TRAL , N EGATIVE , H IGHLY N EGATIVE }, which is in line with product ratings occurring in the corporate world, e.g., Amazon, TripAdvisor, and Yelp. In machine learning terms, moving from a categorical two-point scale to an ordered five-poin"
S17-2088,S17-2128,0,0.0487754,"Missing"
S17-2088,S17-2107,0,0.0332387,"Missing"
S17-2088,S17-2108,0,0.0331549,"Missing"
S17-2088,D13-1187,0,0.0131323,"Missing"
S17-2088,S17-2119,0,0.0318597,"Missing"
S17-2088,S17-2101,0,0.0415145,"Missing"
S17-2088,S17-2102,0,0.0532706,"Missing"
S17-2088,S17-2123,0,0.0335859,"Missing"
S17-2088,S17-2114,0,0.0210194,"Missing"
S17-2088,S17-2137,0,0.0286355,"Missing"
S19-2010,S19-2121,0,0.0487207,"Missing"
S19-2010,S19-2100,0,0.0543071,"Missing"
S19-2010,S19-2120,0,0.040833,"Missing"
S19-2010,S19-2110,0,0.0445018,"Missing"
S19-2010,S19-2129,0,0.043829,"Missing"
S19-2010,S19-2144,0,0.044782,"Missing"
S19-2010,W18-4411,0,0.0794205,"Aggression identification: The TRAC shared task on Aggression Identification (Kumar et al., 2018) provided participants with a dataset containing 15,000 annotated Facebook posts and comments in English and Hindi for training and validation. For testing, two different sets, one from Facebook and one from Twitter, were used. The goal was to discriminate between three classes: non-aggressive, covertly aggressive, and overtly aggressive. The best-performing systems in this competition used deep learning approaches based on convolutional neural networks (CNN), recurrent neural networks, and LSTM (Aroyehun and Gelbukh, 2018; Majumder et al., 2018). While each of the above tasks tackles a particular type of abuse or offense, there are many commonalities. For example, an insult targeted at an individual is commonly known as cyberbulling and insults targeted at a group are known as hate speech. The hierarchical annotation model proposed in OLID (Zampieri et al., 2019) and used in OffensEval aims to capture this. We hope that the OLID’s dataset would become a useful resource for various offensive language identification tasks. 3 The training and testing material for OffensEval is the aforementioned Offensive Languag"
S19-2010,W18-4416,0,0.0526243,"ings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 75–86 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics 2 Related Work Toxic comments: The Toxic Comment Classification Challenge5 was an open competition at Kaggle, which provided participants with comments from Wikipedia organized in six classes: toxic, severe toxic, obscene, threat, insult, identity hate. The dataset was also used outside of the competition (Georgakopoulos et al., 2018), including as additional training material for the aforementioned TRAC shared (Fortuna et al., 2018). Different abusive and offense language identification problems have been explored in the literature ranging from aggression to cyber bullying, hate speech, toxic comments, and offensive language. Below we discuss each of them briefly. Aggression identification: The TRAC shared task on Aggression Identification (Kumar et al., 2018) provided participants with a dataset containing 15,000 annotated Facebook posts and comments in English and Hindi for training and validation. For testing, two different sets, one from Facebook and one from Twitter, were used. The goal was to discriminate between t"
S19-2010,S19-2111,0,0.0516978,"Missing"
S19-2010,S19-2131,0,0.076679,"Missing"
S19-2010,S19-2107,0,0.0613142,"Missing"
S19-2010,P11-2008,0,0.060614,"Missing"
S19-2010,S19-2098,1,0.882958,"Missing"
S19-2010,W15-4322,0,0.0652014,"Missing"
S19-2010,W18-4401,1,0.628306,"Facebook and Twitter. As manual filtering is very time consuming, and as it can cause post-traumatic stress disorder-like symptoms to human annotators, there have been many research efforts aiming at automating the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval. Section 4 includes a brief description of OLID based on (Zampieri et al., 2019)."
S19-2010,S19-2139,0,0.0394818,"Missing"
S19-2010,S19-2011,0,0.137461,"Missing"
S19-2010,S19-2115,0,0.0421739,"Missing"
S19-2010,W18-4423,0,0.178724,"Missing"
S19-2010,S19-2116,0,0.0624146,"Missing"
S19-2010,malmasi-zampieri-2017-detecting,1,0.577817,"ears have seen the proliferation of offensive language in social media platforms such as Facebook and Twitter. As manual filtering is very time consuming, and as it can cause post-traumatic stress disorder-like symptoms to human annotators, there have been many research efforts aiming at automating the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval."
S19-2010,S19-2109,0,0.0646835,"Missing"
S19-2010,S19-2134,0,0.0461997,"Missing"
S19-2010,P14-5010,0,0.00482749,"Missing"
S19-2010,S19-2105,0,0.0325559,"Missing"
S19-2010,D14-1162,0,0.0847332,"Missing"
S19-2010,S19-2127,0,0.0671146,"Missing"
S19-2010,S19-2103,0,0.0392423,"Missing"
S19-2010,N18-1202,0,0.0384382,"tators on the platform and by using test questions to discard annotators who did not achieve a certain threshold. All the tweets were annotated by two people. In case of disagreement, a third annotation was requested, and ultimately we used a majority vote. Examples of tweets from the dataset with their annotation labels are shown in Table 1. 5 Results The models used in the task submissions ranged from traditional machine learning, e.g., SVM and logistic regression, to deep learning, e.g., CNN, RNN, BiLSTM, including attention mechanism, to state-of-the-art deep learning models such as ELMo (Peters et al., 2018) and BERT (Devlin et al.). Figure 2 shows a pie chart indicating the breakdown by model type for all participating systems in sub-task A. Deep learning was clearly the most popular approach, as were also ensemble models. Similar trends were observed for subtasks B and C. Table 2: The teams that participated in OffensEval and submitted system description papers. 7 78 https://www.figure-eight.com/ The results for each of the sub-tasks are shown in Table 4. Due to the large number of submissions, we only show the F1-score for the top-10 teams, followed by result ranges for the rest of the teams."
S19-2010,S19-2118,0,0.0364634,"Missing"
S19-2010,S19-2104,0,0.0457917,"Missing"
S19-2010,S19-2123,0,0.1488,"Missing"
S19-2010,S19-2136,0,0.0650507,"Missing"
S19-2010,S19-2112,0,0.0343921,"Missing"
S19-2010,S19-2141,0,0.0614093,"Missing"
S19-2010,S19-2140,0,0.0351825,"Missing"
S19-2010,S19-2119,0,0.0413788,"Missing"
S19-2010,S19-2113,0,0.0437079,"Missing"
S19-2010,S19-2066,0,0.0526442,"Missing"
S19-2010,S19-2102,0,0.0607101,"Missing"
S19-2010,S19-2125,0,0.0321167,"Missing"
S19-2010,S19-2106,0,0.0611208,"Missing"
S19-2010,S19-2132,0,0.0476906,"Missing"
S19-2010,S19-2108,0,0.131809,"Missing"
S19-2010,W17-3012,0,0.152463,"ing the process. The task is usually modeled as a supervised classification problem, where systems are trained on posts annotated with respect to the presence of some form of abusive or offensive content. Examples of offensive content studied in previous work include hate speech (Davidson et al., 2017; Malmasi and Zampieri, 2017, 2018), cyberbulling (Dinakar et al., 2011), and aggression (Kumar et al., 2018). Moreover, given the multitude of terms and definitions used in the literature, some recent studies have investigated the common aspects of different abusive language detection sub-tasks (Waseem et al., 2017; Wiegand et al., 2018). Sub-task C: Offense target identification (66 participating teams) The remainder of this paper is organized as follows: Section 2 discusses prior work, including shared tasks related to OffensEval. Section 3 presents the shared task description and the subtasks included in OffensEval. Section 4 includes a brief description of OLID based on (Zampieri et al., 2019). Section 5 discusses the participating systems and their results in the shared task. Finally, Section 6 concludes and suggests directions for future work. 1 http://competitions.codalab.org/ competitions/20011"
S19-2010,S19-2126,0,0.0759606,"Missing"
S19-2010,S19-2137,0,0.0528763,"Missing"
S19-2010,S19-2135,0,0.0463278,"Missing"
S19-2010,S19-2128,0,0.0505463,"Missing"
S19-2010,S19-2097,0,0.0448575,"Missing"
S19-2010,N12-1084,0,0.417325,"nsive language identification tasks. 3 The training and testing material for OffensEval is the aforementioned Offensive Language Identification Dataset (OLID) dataset, which was built specifically for this task. OLID was annotated using a hierarchical three-level annotation model introduced in Zampieri et al. (2019). Four examples of annotated instances from the dataset are presented in Table 1. We use the annotation of each of the three layers in OLID for a sub-task in OffensEval as described below. Bullying detection: There have been several studies on cyber bullying detection. For example, Xu et al. (2012) used sentiment analysis and topic models to identify relevant topics, and Dadvar et al. (2013) used user-related features such as the frequency of profanity in previous messages. Hate speech identification: This is the most studied abusive language detection task (Kwok and Wang, 2013; Burnap and Williams, 2015; Djuric et al., 2015). More recently, Davidson et al. (2017) presented the hate speech detection dataset with over 24,000 English tweets labeled as non offensive, hate speech, and profanity. 3.1 Sub-task A: Offensive language identification In this sub-task, the goal is to discriminate"
S19-2010,S19-2124,0,0.0512483,"Missing"
S19-2010,S19-2101,0,0.0376161,"Missing"
S19-2010,N19-1144,1,0.39554,"orizing Offensive Language in Social Media (OffensEval) Marcos Zampieri,1 Shervin Malmasi,2 Preslav Nakov,3 Sara Rosenthal,4 Noura Farra,5 Ritesh Kumar6 1 University of Wolverhampton, UK, 2 Amazon Research, USA 3 Qatar Computing Research Institute, HBKU, Qatar 4 IBM Research, USA, 5 Columbia University, USA, 6 Bhim Rao Ambedkar University, India m.zampieri@wlv.ac.uk Abstract Interestingly, none of this previous work has studied both the type and the target of the offensive language, which is our approach here. Our task, OffensEval1 , uses the Offensive Language Identification Dataset (OLID)2 (Zampieri et al., 2019), which we created specifically for this task. OLID is annotated following a hierarchical three-level annotation schema that takes both the target and the type of offensive content into account. Thus, it can relate to phenomena captured by previous datasets such as the one by Davidson et al. (2017). Hate speech, for example, is commonly understood as an insult targeted at a group, whereas cyberbulling is typically targeted at an individual. We defined three sub-tasks, corresponding to the three levels in our annotation schema:3 We present the results and the main findings of SemEval-2019 Task"
S19-2010,S19-2130,0,0.0564906,"Missing"
S19-2010,S19-2142,0,0.0349466,"Missing"
S19-2010,S19-2117,0,0.0415284,"Missing"
S19-2010,S19-2138,0,0.094085,"Missing"
S19-2010,S19-2143,0,0.05351,"Missing"
S19-2149,D18-2029,0,0.0392084,"rity Class Baseline Accuracy F1 AvgRec MAP 0.815 0.5112 0.5122 0.1557 0.791 0.5241 0.6351 0.1348 0.718 0.4023 0.4453 0.2673 0.686 0.654 0.3754 0.3255 0.4034 0.3265 0.3332 0.1566 0.611 0.2966 0.3176 0.2224 0.548 0.548 0.527 0.439 0.439 0.2717 0.2717 0.2608 0.1339 0.1339 0.3417 0.3417 0.3478 0.2419 0.2419 0.1219 0.1219 0.5711 0.2085 0.2085 0.830 0.285 0.333 0.156 Table 4: Subtask B: Results for answer classification based on the official submissions, evaluated on the test set. The best system for Subtask A was by team Fermi (IIIT Hyderabad). They used Google’s Universal Sentence representation (Cer et al., 2018), and XGBoost (Chen and Guestrin, 2016). The best system for Subtask B was by team AUTOHOME-ORCA (Autohome Inc. and Beijing University of Posts and Telecommunications), who used BERT (Devlin et al., 2019). 865 Fermi performed evaluation of different embedding models - InferSent, Concatenated Power Mean Word Embedding, Lexical Vectors, ELMo and The Universal Sentence Encoder, used in subtask A to feed an XGBoost classifier. ColumbiaNLP used ULMFiT, but performed additional unsupervised tuning of the language model on questions, answers and question-answer pairs from the Qatar Living Forum. TMLa"
S19-2149,S19-2199,0,0.0611199,"Missing"
S19-2149,S19-2200,0,0.0329929,"Missing"
S19-2149,S19-2202,0,0.0398586,"Missing"
S19-2149,D18-1389,1,0.942232,"tics, e.g., during the 2016 presidential campaign in the USA, which was dominated by fake news in social media and by false claims. Investigative journalists and volunteers have been working hard to get to the root of a claim and to present solid evidence in favor or against it. Manual fact-checking is very time-consuming, and thus automatic methods have been proposed to speed-up the process, e.g., there has been work on checking the factuality/credibility of a claim, of a news article, or of an information source (Ba et al., 2016; Zubiaga et al., 2016; Ma et al., 2016; Castillo et al., 2011; Baly et al., 2018). 1 As of present, fully automatic methods for fact checking still lag behind in terms of quality, and thus also of credibility in the eyes of the users, compared to what high-quality manual checking by reputable sources can achieve, which means that a final double-checking by a human expert is needed. 860 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 860–869 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics Q: HI ;; IF WIFE IS UNDER HER HUSBAND’S SPONSORSHIP AND IS WILLING TO COME QATAR ON VISIT; HOW LONG"
S19-2149,N19-1216,1,0.88891,"Missing"
S19-2149,N19-1423,0,0.0287417,"224 0.548 0.548 0.527 0.439 0.439 0.2717 0.2717 0.2608 0.1339 0.1339 0.3417 0.3417 0.3478 0.2419 0.2419 0.1219 0.1219 0.5711 0.2085 0.2085 0.830 0.285 0.333 0.156 Table 4: Subtask B: Results for answer classification based on the official submissions, evaluated on the test set. The best system for Subtask A was by team Fermi (IIIT Hyderabad). They used Google’s Universal Sentence representation (Cer et al., 2018), and XGBoost (Chen and Guestrin, 2016). The best system for Subtask B was by team AUTOHOME-ORCA (Autohome Inc. and Beijing University of Posts and Telecommunications), who used BERT (Devlin et al., 2019). 865 Fermi performed evaluation of different embedding models - InferSent, Concatenated Power Mean Word Embedding, Lexical Vectors, ELMo and The Universal Sentence Encoder, used in subtask A to feed an XGBoost classifier. ColumbiaNLP used ULMFiT, but performed additional unsupervised tuning of the language model on questions, answers and question-answer pairs from the Qatar Living Forum. TMLab’s system used the Universal Sentence Encoder. A common neural network architecture was LSTM, where YNU-HPCC combined LSTM with an attention mechanism. TueFact used comment chain embeddings. Other machin"
S19-2149,N09-2040,0,0.0358647,"S? CAN U PLEASE ANSWER ME.. THANKZZZ... Jurczyk and Agichtein (2007) modelled author authority using link analysis. Agichtein et al. (2008) looked for high-quality answers using PageRank and HITS, in addition to intrinsic content quality, e.g., punctuation and typos, syntactic and semantic complexity, and grammaticality. Lita et al. (2005) studied three qualitative dimensions for answers: source credibility (e.g., does the document come from a government website), sentiment analysis, and contradiction compared to other answers. Su et al. (2010) looked for verbs and adjectives that cast doubt. Banerjee and Han (2009) used language modelling to validate the reliability of an answer’s source. Jeon et al. (2006) focused on non-textual features such as click counts, answer activity level, and copy counts. Pelleg et al. (2016) curated social media content using syntactic, semantic, and social signals. Unlike this research, we (i) target factuality rather than credibility, (ii) address it as a task in its own right, and on a specialised dataset. Information credibility was also studied in social computing. Castillo et al. (2011) modeledd user reputation. Canini et al. (2011) analyzed the interaction of content"
S19-2149,S19-2204,0,0.0363163,"Missing"
S19-2149,S19-2150,0,0.035584,"Missing"
S19-2149,karadzhov-etal-2017-built,1,0.917789,"uality rather than credibility, (ii) address it as a task in its own right, and on a specialised dataset. Information credibility was also studied in social computing. Castillo et al. (2011) modeledd user reputation. Canini et al. (2011) analyzed the interaction of content and social network structure. Morris et al. (2012) studied how Twitter users judge truthfulness. Lukasik et al. (2015) used temporal patterns to detect rumors, and Zubiaga et al. (2016) focused on conversations. Other authors have been querying the Web to gather support for accepting or refuting a claim (Popat et al., 2016; Karadzhov et al., 2017b). In social media, there has been research targeting the user, e.g., finding malicious users (Mihaylov and Nakov, 2016; Mihaylova et al., 2018; Mihaylov et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017). Finally, there has been work on credibility, trust, and expertise in news communities (Mukherjee and Weikum, 2015). Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims. Recent work has also focused on evaluating the factuality of reporting of entire news outlets (Baly"
S19-2149,P16-2065,1,0.854437,"credibility was also studied in social computing. Castillo et al. (2011) modeledd user reputation. Canini et al. (2011) analyzed the interaction of content and social network structure. Morris et al. (2012) studied how Twitter users judge truthfulness. Lukasik et al. (2015) used temporal patterns to detect rumors, and Zubiaga et al. (2016) focused on conversations. Other authors have been querying the Web to gather support for accepting or refuting a claim (Popat et al., 2016; Karadzhov et al., 2017b). In social media, there has been research targeting the user, e.g., finding malicious users (Mihaylov and Nakov, 2016; Mihaylova et al., 2018; Mihaylov et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017). Finally, there has been work on credibility, trust, and expertise in news communities (Mukherjee and Weikum, 2015). Dong et al. (2015) proposed that a trustworthy source is one that contains very few false claims. Recent work has also focused on evaluating the factuality of reporting of entire news outlets (Baly et al., 2018, 2019).3 However, none of this work was about QA or cQA. a1 : Maximum period is 9 Months.... a2 : 6 months"
S19-2149,karadzhov-etal-2017-fully,1,0.918401,"Missing"
S19-2149,P15-2085,0,0.0502703,"cused on non-textual features such as click counts, answer activity level, and copy counts. Pelleg et al. (2016) curated social media content using syntactic, semantic, and social signals. Unlike this research, we (i) target factuality rather than credibility, (ii) address it as a task in its own right, and on a specialised dataset. Information credibility was also studied in social computing. Castillo et al. (2011) modeledd user reputation. Canini et al. (2011) analyzed the interaction of content and social network structure. Morris et al. (2012) studied how Twitter users judge truthfulness. Lukasik et al. (2015) used temporal patterns to detect rumors, and Zubiaga et al. (2016) focused on conversations. Other authors have been querying the Web to gather support for accepting or refuting a claim (Popat et al., 2016; Karadzhov et al., 2017b). In social media, there has been research targeting the user, e.g., finding malicious users (Mihaylov and Nakov, 2016; Mihaylova et al., 2018; Mihaylov et al., 2018), sockpuppets (Maity et al., 2017), Internet water army (Chen et al., 2013), and seminar users (Darwish et al., 2017). Finally, there has been work on credibility, trust, and expertise in news communiti"
S19-2149,S17-2003,1,0.81888,"Missing"
S19-2149,C18-1287,0,0.0937832,"Missing"
S19-2149,S15-2047,1,0.810091,"Missing"
S19-2149,S16-1083,1,0.718374,"Missing"
S19-2149,nakov-etal-2017-trust,1,0.844326,"Missing"
S19-2149,S17-2088,1,0.889353,"Missing"
S19-2149,S19-2201,0,0.0517401,"Missing"
S19-2149,S16-1001,1,0.942922,"ntains very few false claims. Recent work has also focused on evaluating the factuality of reporting of entire news outlets (Baly et al., 2018, 2019).3 However, none of this work was about QA or cQA. a1 : Maximum period is 9 Months.... a2 : 6 months maximum a3 : This has been answered in QL so many times. Please do search for information regarding this. BTW answer is 6 months. Figure 1: Example from the Qatar Living forum. Figure 1 presents an excerpt of an example from the Qatar Living Forum, with one question and three answers selected from a longer thread. According to SemEval-2016 Task 3 (Nakov et al., 2016a), all three answers would be considered G OOD since they are formally answering the question. Nevertheless, a1 contains false information, while a2 and a3 are correct, as can be established from an official government website.2 Checking the veracity of answers in a cQA forum is a hard problem, which requires putting together aspects of language understanding, modelling the context, integrating several information sources, uisng world knowledge and complex inference, among others. Moreover, high-quality automatic fact-checking would offer better experience to users of cQA systems, e.g., the u"
S19-2149,Y10-1062,0,0.0232269,"HAVE HEARD ITS NOT POSSIBLE TO EXTEND VISIT VISA MORE THAN 6 MONTHS? CAN U PLEASE ANSWER ME.. THANKZZZ... Jurczyk and Agichtein (2007) modelled author authority using link analysis. Agichtein et al. (2008) looked for high-quality answers using PageRank and HITS, in addition to intrinsic content quality, e.g., punctuation and typos, syntactic and semantic complexity, and grammaticality. Lita et al. (2005) studied three qualitative dimensions for answers: source credibility (e.g., does the document come from a government website), sentiment analysis, and contradiction compared to other answers. Su et al. (2010) looked for verbs and adjectives that cast doubt. Banerjee and Han (2009) used language modelling to validate the reliability of an answer’s source. Jeon et al. (2006) focused on non-textual features such as click counts, answer activity level, and copy counts. Pelleg et al. (2016) curated social media content using syntactic, semantic, and social signals. Unlike this research, we (i) target factuality rather than credibility, (ii) address it as a task in its own right, and on a specialised dataset. Information credibility was also studied in social computing. Castillo et al. (2011) modeledd u"
S19-2149,S19-2205,0,0.0359528,"Missing"
S19-2149,S19-2203,0,0.0676244,"Missing"
S19-2149,S19-2198,0,0.0551512,"Missing"
S19-2149,S19-2206,0,\N,Missing
S19-2176,D18-1389,1,0.901408,"Missing"
S19-2176,N19-1216,1,0.837505,"Missing"
S19-2176,N19-1423,0,0.0692818,"Missing"
S19-2176,S19-2145,0,0.0296054,"Jack Ryder team to SemEval-2019 Task 4 on Hyperpartisan News Detection. The task asked participants to predict whether a given article is hyperpartisan, i.e., extreme-left or extremeright. We propose an approach based on BERT with fine-tuning, which was ranked 7th out 28 teams on the distantly supervised dataset, where all articles from a hyperpartisan/nonhyperpartisan news outlet are considered to be hyperpartisan/non-hyperpartisan. On a manually annotated test dataset, where human annotators double-checked the labels, we were ranked 29th out of 42 teams. 1 Introduction SemEval-2019 Task 4 (Kiesel et al., 2019) asks to distinguish between articles that are extremely one-sided, i.e., extreme-left or extreme-right, and such that are not. The organizers provided two datasets: 1. By article: A small dataset of 645 manually annotated articles (BA in the following). 2. By publisher: A large dataset of 750,000 articles annotated using distant supervision, where an article is considered hyperpartisan if its source is labeled as such (BP in the following). The set is separated into 600,000 articles for training (BP-train) and 150,000 articles for validation (BP-val). Furthermore, two test sets, one annotated"
S19-2176,D18-1388,0,0.0217847,"Missing"
S19-2176,P18-1022,0,0.0439102,"Missing"
S19-2176,P17-1068,0,0.0567789,"Missing"
S19-2176,D17-1317,0,0.0383943,"Missing"
S19-2176,D13-1010,0,0.0818406,"Missing"
S19-2176,P14-1105,0,0.0706,"Missing"
S19-2182,D18-1389,1,0.904156,"Missing"
S19-2182,N19-1216,1,0.832379,"Missing"
S19-2182,N18-2004,1,0.896708,"Missing"
S19-2182,C18-1158,0,0.0258872,"al election (Brill, 2001; Finberg et al., 2002; Castillo et al., 2011; Baly et al., 2018a; Kulkarni et al., 2018; Mihaylov et al., 2018; Baly et al., 2019). Most approaches have focused on predicting credibility, bias or stance. Stance detection was considered as an intermediate step for detecting fake claims, where the veracity of a claim is checked by aggregating the stances of the retrieved relevant articles (Baly et al., 2018b; Nakov et al., 2019). Several stance detection models have been proposed including deep convolutional neural networks (Baird et al., 2017), multi-layer perceptrons (Hanselowski et al., 2018), and end-to-end memory networks (Mohtarami et al., 2018). 1041 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1041–1046 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from"
S19-2182,S19-2145,0,0.0557502,"distant supervision. Additional experiments showed that significant performance gains can be achieved with better feature pre-processing.1 1 Introduction The rise of social media has enabled people to easily share information with a large audience without regulations or quality control. This has allowed malicious users to spread disinformation and misinformation (a.k.a. “fake news”) at an unprecedented rate. Fake news is typically characterized as being hyperpartisan (one-sided), emotional and riddled with lies (Potthast et al., 2018). The SemEval-2019 Task 4 on Hyperpartisan News Detection (Kiesel et al., 2019) focused on the challenge of automatically identifying whether a text is hyperpartisan or not. While hyperpartisanship is defined as “exhibiting one or more of blind, prejudiced, or unreasoning allegiance to one party, faction, cause, or person”, we model this task as a binary document classification problem. Scholars have argued that all biased messages can be considered propagandistic, regardless of whether the bias was intentional or not (Ellul, 1965, p. XV). 1 Our system is available at https://github.com/ AbdulSaleh/QCRI-MIT-SemEval2019-Task4 Thus, we approached the task departing from an"
S19-2182,D18-1388,0,0.165557,"Missing"
S19-2182,N18-1070,1,0.913935,"Missing"
S19-2182,H05-1044,0,0.0505862,"1042 X biasi (Dj ) = count(cue, Dj ) cue∈BLi X wk ∈Dj (3) count(wk , Dj ) Lexicon-based Features. Rashkin et al. (2017) studied the occurrence of specific types of words in different kinds of articles, and showed that words from certain lexicons (e.g., negation and swear words) appear more frequently in propaganda, satire, and hoax articles than in trustworthy articles. We capture this by extracting features that reflect the frequency of words from particular lexicons. We use 18 lexicons from Wiktionary, Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001), Wilson’s subjectives (Wilson et al., 2005), Hyland’s hedges (Hyland, 2015), and Hooper’s assertives (Hooper, 1975). For each lexicon, we count the total number of words in the article that appear in the lexicon. This resulted in 18 features, one for each lexicon. Vocabulary Richness Potthast et al. (2018) showed that hyperpartisan outlets tend to use a writing style that is different from mainstream outlets. Different topic-independent features have been proposed to characterize the vocabulary richness, style and complexity of a text. For this task, we used the following vocabulary richness features: (i) type–token ratio (TTR), or the"
S19-2182,P18-1022,0,0.211987,"d by aggregating the stances of the retrieved relevant articles (Baly et al., 2018b; Nakov et al., 2019). Several stance detection models have been proposed including deep convolutional neural networks (Baird et al., 2017), multi-layer perceptrons (Hanselowski et al., 2018), and end-to-end memory networks (Mohtarami et al., 2018). 1041 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 1041–1046 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from fake news and satire. They used features such as the number of occurrences of different part-of-speech tags, swearing and slang words, stop words, punctuation, and negation as stylistic markers. They also used a number of readability measures. Rashkin et al. (2017) focused on a multi-class setting (real news vs. satire vs. hoax vs. propaganda) a"
S19-2182,D17-1317,0,0.206809,"l Linguistics The stylometric analysis model of Koppel et al. (2007) was used by Potthast et al. (2018) to address hyperpartisanship. They used articles from nine news sources whose factuality has been manually verified by professional journalists. Writing style and complexity were also considered by Horne and Adal (2017) to differentiate real news from fake news and satire. They used features such as the number of occurrences of different part-of-speech tags, swearing and slang words, stop words, punctuation, and negation as stylistic markers. They also used a number of readability measures. Rashkin et al. (2017) focused on a multi-class setting (real news vs. satire vs. hoax vs. propaganda) and relied on word n-grams. Similarly to Potthast et al. (2018), we believe that there is an inherent style in propaganda, regardless of the source publishing it. Many stylistic features were proposed for authorship identification, i.e., the task of predicting whether a piece of text has been written by a particular author. One of the most successful representations for such a task are character-level n-grams (Stamatatos, 2009), and they turn out to represent some of our most important stylistic features. More det"
S19-2182,C18-1283,0,0.0315537,"e believe that there is an inherent style in propaganda, regardless of the source publishing it. Many stylistic features were proposed for authorship identification, i.e., the task of predicting whether a piece of text has been written by a particular author. One of the most successful representations for such a task are character-level n-grams (Stamatatos, 2009), and they turn out to represent some of our most important stylistic features. More details about research on fact-checking and the spread of fake news online can be found in recent surveys (Lazer et al., 2018; Vosoughi et al., 2018; Thorne and Vlachos, 2018). 3 System Description We developed our system for detecting hyperpartisanship in news articles by training a logistic regression classifier using features such as character and word n-grams, lexicon-based indicators, and readability and vocabulary richness measures. Below, we describe these features in detail. Character 3-grams. Stamatatos (2009) argued that, for tasks where the topic is irrelevant, character-level representations are more sensitive than token-level ones. We hypothesize that this applies to hyperpartisan news detection, since articles on both sides of the political spectrum m"
S19-2182,P02-1053,0,0.0234741,"= α + i:yi =0 xi , and we set the smoothing parameter α to 1. Finally, we calculate the vector:  r = log p/ k p k q/ k q k  (1) which is used to scale the TF.IDF features to create the NB-TF.IDF features as follows: x0i = r ◦ xi , ∀i (2) Bias Analysis We analyze the bias in the language used in the documents by (i) creating bias lexicons that contain left and right bias cues, and (ii) using these lexicons to compute two scores for each document, indicating the intensity of bias towards each ideology. To generate the list of cues that signal biased language, we use Semantic Orientation (SO) (Turney, 2002) to identify the words that are strongly associated with each of the left and right documents in the training dataset. Those SO values can be either positive or negative, indicating association with right or left biases, respectively. Then, we select words whose absolute SO value is ≥ 0.4 to create two bias lexicons: BLlef t and BLright . Finally, we use these lexicons to compute two bias scores per document according to Equation (3), where for each document Dj , the frequency of cues in the lexicon BLi that are present in Dj is normalized by the total number of words in Dj : 1042 X biasi (Dj"
S19-2182,P12-2018,0,0.0995675,"Missing"
W04-2010,P00-1035,0,0.0605695,"Missing"
W04-2010,A92-1018,0,0.0842329,"Missing"
W04-2010,W98-1239,0,0.0840999,"Missing"
W04-2010,W99-0904,0,0.0458185,"Missing"
W04-2010,N04-1016,0,0.061132,"Missing"
W04-2010,J97-3003,0,0.0715867,"Missing"
W04-2010,W00-0722,0,0.0335834,"Missing"
W04-2010,W00-0712,0,0.0396639,"Missing"
W04-2010,P99-1037,0,0.0715021,"Missing"
W04-2010,P00-1027,0,0.056029,"Missing"
W04-2010,J93-2006,0,\N,Missing
W04-2010,W97-0124,0,\N,Missing
W05-0603,J03-3005,0,0.249046,"are kept together in the expanded version. However, this NC is ambiguous, and can also be paraphrased as cells from the brain stem, implying a left bracketing. Some NCs’ meaning cannot be readily expressed with a prepositional paraphrase (Warren, 1978). An alternative is the copula paraphrase, as in office building that/which is a skyscraper (right bracketing), or a verbal paraphrase such as pain associated with arthritis migraine (left). Other researchers have used prepositional paraphrases as a proxy for determining the semantic relations that hold between nouns in a compound (Lauer, 1995; Keller and Lapata, 2003; Girju et al., 2005). Since most NCs have a prepositional paraphrase, Lauer builds a model trying to choose between the most likely candidate prepositions: of, for, in, at, on, from, with and about (excluding like which is mentioned by Warren). This could be problematic 21 though, since as a study by Downing (1977) shows, when no context is provided, people often come up with incompatible interpretations. In contrast, we use paraphrases in order to make syntactic bracketing assignments. Instead of trying to manually decide the correct paraphrases, we can issue queries using paraphrase pattern"
W05-0603,N04-1016,0,0.584491,"es that are used in science fiction. 4 4.1 Evaluation Lauer’s Dataset We experimented with the dataset from (Lauer, 1995), in order to produce results comparable to those of Lauer and Keller & Lapata. The set consists of 244 unambiguous 3-noun NCs extracted from Grolier’s encyclopedia; however, only 216 of these NCs are unique. Lauer (1995) derived n-gram frequencies from the Grolier’s corpus and tested the dependency and the adjacency models using this text. To help combat data sparseness issues he also incorporated a taxonomy and some additional information (see Related Work section above). Lapata and Keller (2004) derived their statistics from the Web and achieved results close to Lauer’s using simple lexical models. 4.2 Biomedical Dataset We constructed a new set of noun compounds from the biomedical literature. Using the Open NLP 6 In addition to the articles (a, an, the), we also used quantifiers (e.g. some, every) and pronouns (e.g. this, his). tools,7 we sentence splitted, tokenized, POS tagged and shallow parsed a set of 1.4 million MEDLINE abstracts (citations between 1994 and 2003). Then we extracted all 3-noun sequences falling in the last three positions of noun phrases (NPs) found in the sha"
W05-0603,J93-2005,0,0.351437,"Missing"
W07-0730,W06-3123,0,0.0196787,"Table 2: Summary of our submissions. All runs are for the News Commentary test data. The official submissions are marked with a star. applied rule (8), since its Spanish/French equivalent que (as well as the German daß) is always obligatory. These transformations affected 927 out of the 2007 test sentences. We also used this transformed data set when translating to German (however, German uses NCs as much as English does). 3.4 Other Non-standard Settings Below we discuss some non-standard settings that differ from the ones suggested by the organizers in their baseline system. First, following Birch et al. (2006), who found that higher-order LMs give better results2 , we used a 5-gram LM for News Commentary, and 7-gram LM for Europarl (as opposed to 3-gram, as done normally). Second, for all runs we trained our systems on all sentences of length up to 100 (rather than 40, as suggested in the baseline system). Third, we used a maximum phrase length limit of 10 (rather than 7, as typically done). Fourth, we used both a lexicalized and distance-based reordering models (as opposed to lexicalized only, as in the baseline system). Finally, while we did not use any resources other than the ones provided by t"
W07-0730,P03-1054,0,0.00530773,", but syntactically different from the phrases learned on training, and the potential for a high-quality translation is missed. We address this problem by using nearly equivalent syntactic paraphrases of the original sentences. Each paraphrased sentence is paired with the foreign translation that is associated with the original sentence in the training data. This augmented training corpus can then be used to train an SMT system. Alternatively, we can paraphrase the test sentences making them closer to the target language syntax. Given an English sentence, we parse it with the Stanford parser (Klein and Manning, 2003) and then generate paraphrases using the following syntactic transformations: 1. [NP NP1 P NP2 ] ⇒ [NP NP2 NP1 ]. inequality in income ⇒ income inequality. 2. [NP NP1 of NP2 ] ⇒ [NP NP2 poss NP1 ]. inequality of income ⇒ income’s inequality. 3. NPposs ⇒ NP. income’s inequality ⇒ income inequality. 4. NPposs ⇒ NPP Pof . income’s inequality ⇒ inequality of income. 5. NPN C ⇒ NPposs . income inequality ⇒ income’s inequality. 6. NPN C ⇒ NPP P . income inequality ⇒ inequality in incomes. 212 Proceedings of the Second Workshop on Statistical Machine Translation, pages 212–215, c Prague, June 2007. 2"
W07-0730,W06-3114,0,0.0700457,"California at Berkeley Berkeley, CA 94720 hearst@ischool.berkeley.edu 2 Introduction Modern Statistical Machine Translation (SMT) systems are trained on aligned sentences of bilingual corpora, typically from one domain. When tested on text from that same domain, such systems demonstrate state-of-the art performance; however, on out-of-domain text the results can get significantly worse. For example, on the WMT 2006 Shared Task evaluation, the French to English translation BLEU scores dropped from about 30 to about 20 for nearly all systems, when tested on News Commentary rather than Europarl (Koehn and Monz, 2006). Therefore, this year the shared task organizers have provided 1M words of bilingual News Commentary training data in addition to the Europarl data (about 30M words), thus challenging the participants to experiment with domain adaptation. Below we describe our domain adaptation experiments, trying to achieve better results on the News Monolingual Syntactic Paraphrasing In many cases, the testing text contains “phrases” that are equivalent, but syntactically different from the phrases learned on training, and the potential for a high-quality translation is missed. We address this problem by us"
W07-0730,N03-2016,0,0.0852024,"ems were trained on both corpora. • Language models. We used two language models (LM) – a small in-domain one (trained on News Commentary) and a big out-of-domain one (trained on Europarl). For example, for EN → ES (from English to Spanish), on the lowercased tuning data set, using in-domain LM only achieved a BLEU of 0.332910, while using both LMs yielded 0.354927, a significant effect. • Cognates. Previous research has found that using cognates can help get better word alignments (and ultimately better MT results), especially in case of a small training set. We used the method described in (Kondrak et al., 2003) in order to extract cognates from the two data sets. We then added them as sentence pairs to the News Commentary corpus before training the word alignment models1 for ucb3, ucb4 and ucb5. 1 Following (Kondrak et al., 2003), we considered words of length 4 or more, we required the length ratio to be between 7 and 10 , and we accepted as potential cognates all pairs for 10 7 which the longest common subsequence ratio (LCSR) was 0.58 or more. We repeated 3 times the cognate pairs extracted from the Europarl, and 4 times the ones from News Commentary. 214 • Phrases. The ucb5 system uses the Europ"
W07-0730,N04-1016,0,0.0242204,"ith internal possessive marker; NP that is a Noun Compound. While the first four and the last two transformations are purely syntactic, (5) and (6) are not. The algorithm must determine whether a possessive marker is feasible for (5) and must choose the correct preposition for (6). In either case, for noun compounds (NCs) of length 3 or more, it also needs to choose the position to modify, e.g., inquiry’s committee chairman vs. inquiry committee’s chairman. In order to ensure accuracy of the paraphrases, we use statistics gathered from the Web, using a variation of the approaches presented in Lapata and Keller (2004) and Nakov and Hearst (2005). We use patterns to generate possible prepositional or copula paraphrases in the context of the preceding and the following word in the sentence, First we split the NC into two parts N1 and N2 in all possible ways, e.g., beef import ban lifting would be split as: (a) 213 &quot;lt &quot;lt &quot;lt &quot;lt N1 N2 N2 N2 poss prep that that N2 rt&quot; det N10 rt&quot; be det N10 rt&quot; be prep det N10 rt&quot; where: lt is the word preceding N1 in the original sentence or empty if none, rt is the word following N2 in the original sentence or empty if none, poss is a possessive marker (’s or ’), that is t"
W07-0730,P95-1007,0,0.0772654,"Missing"
W07-0730,W05-0603,1,0.85274,"r; NP that is a Noun Compound. While the first four and the last two transformations are purely syntactic, (5) and (6) are not. The algorithm must determine whether a possessive marker is feasible for (5) and must choose the correct preposition for (6). In either case, for noun compounds (NCs) of length 3 or more, it also needs to choose the position to modify, e.g., inquiry’s committee chairman vs. inquiry committee’s chairman. In order to ensure accuracy of the paraphrases, we use statistics gathered from the Web, using a variation of the approaches presented in Lapata and Keller (2004) and Nakov and Hearst (2005). We use patterns to generate possible prepositional or copula paraphrases in the context of the preceding and the following word in the sentence, First we split the NC into two parts N1 and N2 in all possible ways, e.g., beef import ban lifting would be split as: (a) 213 &quot;lt &quot;lt &quot;lt &quot;lt N1 N2 N2 N2 poss prep that that N2 rt&quot; det N10 rt&quot; be det N10 rt&quot; be prep det N10 rt&quot; where: lt is the word preceding N1 in the original sentence or empty if none, rt is the word following N2 in the original sentence or empty if none, poss is a possessive marker (’s or ’), that is that, which or who, be is is"
W07-0730,P03-1021,0,0.0129077,"sing parameters were filled with 1e-40. The ucb5 system is also trained on Europarl, yielding a third lexicalized re-ordering model and adding 4 new parameters to the phrase table entries. Unfortunately, longer sentences (up to 100 tokens, rather than 40), longer phrases (up to 10 tokens, rather than 7), two LMs (rather than just one), higher-order LMs (order 7, rather than 3), multiple higher-order lexicalized re-ordering models (up to 3), etc. all contributed to increased system’s complexity, and, as a result, time limitations prevented us from performing minimum-error-rate training (MERT) (Och, 2003) for ucb3, ucb4 and ucb5. Therefore, we used the MERT parameter values from ucb1 instead, e.g. the first 4 phrase weights of ucb1 were divided by two, copied twice and used in ucb3 as the first 8 phrase-table parameters. The extra 4 parameters of ucb5 came from training a separate MT system on the Europarl data (scaled accordingly). 3.3 Paraphrasing the Test Set In some of our experiments (ucb2 and ucb4), given a test sentence, we generated the single most-likely paraphrase, which makes it syntactically closer to Spanish and French. Unlike English, which makes extensive use of noun compounds,"
W07-0730,2006.amta-papers.2,0,\N,Missing
W08-0320,W06-3114,0,0.0613808,"Missing"
W08-0320,W07-0733,0,0.235908,", 2007), and is described in more detail in (Nakov, 2007). Unfortunately, using multiple paraphrased versions of the same sentence changes the word frequencies in the training bi-text, thus causing worse maximum likelihood estimates, which results in bad system performance. However, real improvements can still be achieved by merging the phrase tables of the two systems, giving priority to the original. 2.2 Domain Adaptation In our previous findings (Nakov and Hearst, 2007), we found that using in-domain and out-of-domain language models is the best way to perform domain adaptation. Following (Koehn and Schroeder, 2007), we further used two phrase tables. 2.3 Improving the Recaser One problem we noticed with the default recasing is that unknown words are left in lowercase. However, many unknown words are in fact named entities (persons, organization, or locations), which should be spelled capitalized. Therefore, we prepared a new recasing script, which makes sure that all unknown words keep their original case. 148 Changing Tokenization/Detokenization We found the default tokenizer problematic: it keeps complex adjectives as one word, e.g., wellrehearsed, self-assured, Arab-Israeli. While linguistically corr"
W08-0320,P03-1021,0,0.0097688,"milar manner: we first kept all phrases from Rnews , then we added those from Reuro which were not present in Rnews , and finally those from Rpar which were not in Rnews nor in Reuro . We used two language models with Kneser-Ney smoothing: a 3-gram model trained on News Commentary, and a 5-gram model trained on Europarl. We then trained a log-linear model using the following feature functions: language model probabilities, word penalty, distortion cost, and the parameters from the phrase table. We set the feature weights by optimizing the Bleu score directly using minimum error rate training (Och, 2003) on the development set. We used these weights in a beam search decoder to produce translations for the test sentences, which we compared to the WMT’07 gold standard using Bleu (Papineni et al., 2002). 4 Results and Discussion Table 1 shows the evaluation results using the WMT’07 News Commentary test data. Our best English→Spanish system news10 ≺euro10 ≺par10 (see the table caption for explanation of the notation), which is also our submission, achieved 35.09 Bleu score with the improved recaser; with the default recaser, the score drops to 34.85. Due to space limitations, our Spanish→English"
W08-0320,P02-1040,0,0.107514,"used two language models with Kneser-Ney smoothing: a 3-gram model trained on News Commentary, and a 5-gram model trained on Europarl. We then trained a log-linear model using the following feature functions: language model probabilities, word penalty, distortion cost, and the parameters from the phrase table. We set the feature weights by optimizing the Bleu score directly using minimum error rate training (Och, 2003) on the development set. We used these weights in a beam search decoder to produce translations for the test sentences, which we compared to the WMT’07 gold standard using Bleu (Papineni et al., 2002). 4 Results and Discussion Table 1 shows the evaluation results using the WMT’07 News Commentary test data. Our best English→Spanish system news10 ≺euro10 ≺par10 (see the table caption for explanation of the notation), which is also our submission, achieved 35.09 Bleu score with the improved recaser; with the default recaser, the score drops to 34.85. Due to space limitations, our Spanish→English results are not in Table 1. This time, we did not use paraphrases, and our best system news10 ≺euro10 achieved 35.78 and 35.17 Bleu score with the improved and the default recaser, respectively. As th"
W08-0320,W07-0730,1,\N,Missing
W09-0412,N03-2016,0,0.464734,"up with seven parameters for each entry in the merged phrase table. Merging Two Lexicalized Reordering Tables. When building the two phrase tables, we also built two lexicalized reordering tables (Koehn et al., 2005) for them, Rnews and Reuro , which we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Mela"
W09-0412,N01-1020,0,0.0624677,"st kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length"
W09-0412,P07-1083,0,0.0541762,"e merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments"
W09-0412,W95-0115,0,0.249911,"003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Oc"
W09-0412,J99-1003,0,0.0358046,"Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous researchers in computational linguistics (Bergsma and Kondrak, 2007; Mann and Yarowsky, 2001; Melamed, 1999), however, we adopted a simplified definition which ignores origin, defining cognates as words in different languages that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and"
W09-0412,J93-2003,0,0.0101214,"s that are mutual translations and have a similar orthography. We extracted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distanc"
W09-0412,J00-2004,0,0.0976046,"ining News Commentary bi-text) and an out-ofdomain one (trained on the provided monolingual Spanish Europarl data). For both LMs, we used 5-gram models with Kneser-Ney smoothing. Merging Two Phrase Tables. Following Nakov (2008), we trained and merged two phrasebased SMT systems: a small in-domain one using the News Commentary bi-text, and a large out-ofLCSR (s1 , s2 ) = |LCS(s1 ,s2 )| max(|s1 |,|s2 |) where LCS(s1 , s2 ) is the longest common subsequence of s1 and s2 , and |s |is the length of s. Following Nakov et al. (2007), we combined the LCSR similarity measure with competitive linking (Melamed, 2000) in order to extract potential cog76 nates from the training bi-text. Competitive linking assumes that, given a source English sentence and its Spanish translation, a source word is either translated with a single target word or is not translated at all. Given an English-Spanish sentence pair, we calculated LCSR for all cross-lingual word pairs (excluding stopwords and words of length 3 or less), which induced a fully-connected weighted bipartite graph. Then, we performed a greedy approximation to the maximum weighted bipartite matching in that graph (competitive linking) as follows: First, we"
W09-0412,W07-0718,0,0.0553417,"Missing"
W09-0412,W08-0309,0,0.0509475,"Missing"
W09-0412,W06-3114,0,0.0532889,"on at the shared task. 1 Introduction Modern Statistical Machine Translation (SMT) systems are typically trained on sentence-aligned parallel texts (bi-texts) from a particular domain. When tested on text from that domain, they demonstrate state-of-the art performance, but on out-of-domain test data the results can deteriorate significantly. For example, on the WMT06 Shared Translation Task, the scores for French-to-English translation dropped from about 30 to about 20 Bleu points for nearly all systems when tested on News Commentary instead of the Europarl1 text, which was used for training (Koehn and Monz, 2006). 1 2 The NUS System Below we describe separately the standard and the nonstandard settings of our system. 2.1 Standard Settings In our baseline experiments, we used the following general setup: First, we tokenized the par2 The task organizers invited submissions translating forward and/or backward between English and five other European languages (French, Spanish, German, Czech and Hungarian), but we only participated in English→Spanish, due to time limitations. See (Koehn, 2005) for details about the Europarl corpus. Proceedings of the Fourth Workshop on Statistical Machine Translation , pag"
W09-0412,W08-0320,1,0.884628,"Missing"
W09-0412,2005.iwslt-1.8,0,0.0516195,"tion probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We further added two new features, Fnews and Feuro , which show the source of each phrase. Their values are 1 and 0.5 when the phrase was extracted from the News Commentary bi-text, 0.5 and 1 when it was extracted from the Europarl bi-text, and 1 and 1 when it was extracted from both. As a result, we ended up with seven parameters for each entry in the merged phrase table. Merging Two Lexicalized Reordering Tables. When building the two phrase tables, we also built two lexicalized reordering tables (Koehn et al., 2005) for them, Rnews and Reuro , which we merged as follows: We first kept all phrases from Rnews, then we added those from Reuro which were not present in Rnews . This resulting lexicalized reordering table was used together with the above-described merged phrase table. Cognates. Previous research has shown that using cognates can yield better word alignments (AlOnaizan et al., 1999; Kondrak et al., 2003), which in turn often means higher-quality phrase pairs and better SMT systems. Linguists define cognates as words derived from a common root (Bickford and Tuggy, 2002). Following previous resear"
W09-0412,J03-1002,0,0.00684256,"cted and used such potential cognates in order to bias the training of the IBM word alignment models. Following Melamed (1995), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from th"
W09-0412,J04-4002,0,0.0932008,"5), we measured the orthographic similarity using longest common subsequence ratio (LCSR), which is defined as follows: allel bi-text, converted it to lowercase, and filtered out the overly-long training sentences, which complicate word alignments (we tried maximum length limits of 40 and 100). We then built separate English-to-Spanish and Spanish-to-English directed word alignments using IBM model 4 (Brown et al., 1993), combined them using the intersect+grow heuristic (Och and Ney, 2003), and extracted phrase-level translation pairs of maximum length 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate trai"
W09-0412,P03-1021,0,0.0163364,"ained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features of our system can be summarized as follows: Two Language Model"
W09-0412,P02-1040,0,0.082241,"h 7 using the alignment template approach (Och and Ney, 2004). We thus obtained a phrase table where each phrase translation pair is associated with the following five standard parameters: forward and reverse phrase translation probabilities, forward and reverse lexical translation probabilities, and phrase penalty. We then trained a log-linear model using the standard feature functions: language model probability, word penalty, distortion costs (we tried distance based and lexicalized reordering models), and the parameters from the phrase table. We set all feature weights by optimizing Bleu (Papineni et al., 2002) directly using minimum error rate training (MERT) (Och, 2003) on the tuning part of the development set (dev-test2009a). We used these weights in a beam search decoder (Koehn et al., 2007) to translate the test sentences (the English part of dev-test2009b, tokenized and lowercased). We then recased the output using a monotone model that translates from lowercase to uppercase Spanish, we post-cased it using a simple heuristic, de-tokenized the result, and compared it to the gold standard (the Spanish part of dev-test2009b) using Bleu and NIST. 2.2 Nonstandard Settings The nonstandard features"
W09-0412,2005.mtsummit-papers.11,0,0.0734173,"Missing"
W09-0412,W07-0730,1,\N,Missing
W09-0412,P07-2045,0,\N,Missing
W09-1412,W03-0430,0,0.144326,"Missing"
W09-1412,P05-1012,0,0.109715,"each pair of a sentence xi and a labeling y, we compute a vector-valued feature representation f (xi , y). Given a weight vector w, the dot-product w · f (x, y) ranks the possible labelings y of x; we will denote the top scoring labeling as yw (x). As with hidden Markov models (Rabiner, 1989), yw (x) can be computed efficiently for suitable feature functions using dynamic programming. The learning portion of our method requires finding a weight vector w that scores the correct labeling of the training data higher than any incorrect labeling. We used a one-best version of MIRA (Crammer, 2004; McDonald et al., 2005) to choose w. MIRA is an online learning algorithm that updates the weight vector w for each training sentence xi according to the following rule: Proceedings of the Workshop on BioNLP: Shared Task, pages 95–98, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Feature Set Baseline (current word) + POS & char 3-gram + previous POS tag + lexicon (final tagger) wnew = arg min kw − wold k w s.t. w · f (xi , yi ) − w · f (x, yˆ) ≥ L(yi , yˆ) where L(yi , y) is a measure of the loss of using y instead of the correct labeling yi , and yˆ is a shorthand for ywold (xi ). I"
W09-1412,W95-0107,0,0.102512,"Missing"
W09-2415,W04-3205,0,0.0639941,"unrelated semantic roles. There is a rudimentary frame hierarchy that defines mappings between roles of individual frames,5 but it is far from complete. The situation is similar in PropBank. PropBank does use a small number of semantic roles, but these are again to be interpreted at the level of individual predicates, with little cross-predicate generalization. In contrast, all of the semantic relation inventories discussed in Section 1 contain fewer than 50 types of semantic relations. More generally, semantic relation inventories attempt to generalize relations across wide groups of verbs (Chklovski and Pantel, 2004) and include relations that are not verbcentered (Nastase and Szpakowicz, 2003; Moldovan et al., 2004). Using the same labels for similar semantic relations facilitates supervised learning. For example, a model trained with examples of sell relations should be able to transfer what it has learned to give relations. This has the potential of adding 5 For example, it relates the B UYER role of the C OM frame (verb sell ) to the R ECIPIENT role of the G IVING frame (verb give). MERCE SELL 97 1. People in Hawaii might be feeling &lt;e1>aftershocks&lt;/e1> from that powerful &lt;e2>earthquake&lt;/e2> for weeks"
W09-2415,P08-1027,0,0.0924452,"tical NLP settings, where any relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, som"
W09-2415,J02-3001,0,0.0386032,"This is motivated by modelling considerations. Presumably, the data for OTHER will be very nonhomogeneous. By including it, we force any model of the complete data set to correctly identify the decision boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of mu"
W09-2415,S07-1003,1,0.384989,"Missing"
W09-2415,C92-2082,0,0.060649,"tion will take place in two rounds. In the first round, we will do a coarse-grained search for positive examples for each relation. We will collect data from the Web using a semi-automatic, pattern-based search procedure. In order to ensure a wide variety of example sentences, we will use several dozen patterns per relation. We will also ensure that patterns retrieve both positive and negative example sentences; the latter will help populate the OTHER relation with realistic near-miss negative examples of the other relations. The patterns will be manually constructed following the approach of Hearst (1992) and Nakov and Hearst (2008).6 The example collection for each relation R will be passed to two independent annotators. In order to maintain exclusivity of relations, only examples that are negative for all relations but R will be included as positive and only examples that are negative for all nine relations will be included as OTHER. Next, the annotators will compare their decisions and assess inter-annotator agreement. Consensus will be sought; if the annotators cannot agree on an example it will not be included in the data set, but it will be recorded for future analysis. Finally, two othe"
W09-2415,P08-2047,0,0.0143881,"relation can hold between a pair of nominals which occur in a sentence or a discourse. 2.4 Summary While there is a substantial amount of work on relation extraction, the lack of standardization makes it difficult to compare different approaches. It is known from other fields that the availability of standard benchmark data sets can provide a boost to the advancement of a field. As a first step, SemEval2007 Task 4 offered many useful insights into the performance of different approaches to semantic relation classification; it has also motivated followup research (Davidov and Rappoport, 2008; Katrenko and Adriaans, 2008; Nakov and Hearst, 2008; ´ S´eaghdha and Copestake, 2008). O Our objective is to build on the achievements of SemEval-2007 Task 4 while addressing its shortcomings. In particular, we consider a larger set of semantic relations (9 instead of 7), we assume a proper multi-class classification setting, we emulate the effect of an “open” relation inventory by means of a tenth class OTHER, and we will release to the research community a data set with a considerably 1 http://www.itl.nist.gov/iad/mig/tests/ ace/ 2 Although it was not designed for a multi-class set-up, some subsequent publications tri"
W09-2415,I05-1082,1,0.187851,"nds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relat"
W09-2415,W04-2609,0,0.0615549,"fy noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes c"
W09-2415,P08-1052,1,0.611835,"medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is wh"
W09-2415,C08-1082,1,0.339838,"Missing"
W09-2415,J05-1004,0,0.0280101,"boundaries between the individual relations and “everything else”. This encourages good generalization behaviour to larger, noisier data sets commonly seen in real-world applications. 3.1 Semantic Relations versus Semantic Roles There are three main differences between our task (classification of semantic relations between nominals) and the related task of automatic labeling of semantic roles (Gildea and Jurafsky, 2002). The first difference is to do with the linguistic phenomena described. Lexical resources for theories of semantic roles such as FrameNet (Fillmore et al., 2003) and PropBank (Palmer et al., 2005) have been developed to describe the linguistic realization patterns of events and states. Thus, they target primarily verbs (or event nominalizations) and their dependents, which are typically nouns. In contrast, semantic relations may occur between all parts of speech, although we limit our attention to nominals in this task. Also, semantic role descriptions typically relate an event to a set of multiple participants and props, while semantic relations are in practice (although not necessarily) binary. The second major difference is the syntactic context. Theories of semantic roles usually d"
W09-2415,P06-1015,1,0.178213,"m of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and data set. Moldovan et al. (2004) propose a 35-class scheme to classify relations in various phrases; the same scheme has been applied to noun compounds and other noun phrases (Girju et al., 2005). Lapata (2002) presents a binary classification of relations in nominalizations. Pantel and Pennacchiotti (2006) concentrate on five relations in an IE-style setting. In short, there is little agreement on relation inventories. 2.2 The Role of Context A fundamental question in relation classification is whether the relations between nominals should be considered out of context or in context. When one looks at real data, it becomes clear that context does indeed play a role. Consider, for example, the noun compound wood shed : it may refer either to a shed made of wood, or to a shed of any material used to store wood. This ambiguity is likely to be resolved in particular contexts. In fact, most NLP appli"
W09-2415,D07-1075,0,0.0368294,"annotation, we define a nominal as a noun or a base noun phrase. A base noun phrase is a noun and its pre-modifiers (e.g., nouns, adjectives, determiners). We do not include complex noun phrases (e.g., noun phrases with attached prepositional phrases or relative clauses). For example, lawn is a noun, lawn mower is a base noun phrase, and the engine of the lawn mower is a complex noun phrase. We focus on heads that are common nouns. This emphasis distinguishes our task from much work in IE, which focuses on named entities and on considerably more fine-grained relations than we do. For example, Patwardhan and Riloff (2007) identify categories like Terrorist organization as participants in terror-related semantic relations, which consists predominantly of named entities. We feel that named entities are a specific category of nominal expressions best dealt with using techniques which do not apply to common nouns; for example, they do not lend themselves well to semantic generalization. Figure 1 shows two examples of annotated sentences. The XML tags &lt;e1> and &lt;e2> mark the target nominals. Since all nine proper semantic relations in this task are asymmetric, the ordering of the two nominals must be taken into acco"
W09-2415,W01-0511,0,0.0250487,"CL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 94–99, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one"
W09-2415,P02-1032,0,0.00907258,"stics 2 Semantic Relation Classification: Issues 2.1 Defining the Relation Inventory A wide variety of relation classification schemes exist in the literature, reflecting the needs and granularities of various applications. Some researchers only investigate relations between named entities or internal to noun-noun compounds, while others have a more general focus. Some schemes are specific to a domain such as biomedical text. Rosario and Hearst (2001) classify noun compounds from the domain of medicine into 13 classes that describe the semantic relation between the head noun and the modifier. Rosario et al. (2002) classify noun compounds using the MeSH hierarchy and a multi-level hierarchy of semantic relations, with 15 classes at the top level. Stephens et al. (2001) propose 17 very specific classes targeting relations between genes. Nastase and Szpakowicz (2003) address the problem of classifying noun-modifier relations in general text. They propose a two-level hierarchy, with 5 classes at the first level and 30 classes at the second one; other researchers (Kim and Baldwin, 2005; Nakov and Hearst, 2008; Nastase et al., 2006; Turney, 2005; Turney and Littman, 2005) have used their class scheme and dat"
W09-2415,W09-1401,0,\N,Missing
W09-2415,J02-3004,0,\N,Missing
W09-2415,S10-1006,1,\N,Missing
W09-2415,W04-2412,0,\N,Missing
W09-2416,W04-0404,0,0.485888,"Missing"
W09-2416,C08-1011,1,0.300292,"he problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability distribution over a range of candidates. For example, highly probable paraphrases for chocolate bar are bar made of chocolate and bar that tastes like chocolate, while bar that eats chocolate is very unlikely. As described in Section 3.3, a set of goldstandard paraphrase distributions can be constructed by collating responses from a large number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we"
W09-2416,P07-1072,0,0.421183,"Missing"
W09-2416,P84-1109,0,0.450496,"araphrasing models, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a gi"
W09-2416,W96-0309,0,0.0699356,"ls, but it is exacerbated by the restricted nature of prepositions. Furthermore, many NCs cannot be paraphrased adequately with prepositions, e.g., woman driver, honey bee. A richer, more flexible paraphrasing model is afforded by the use of verbs. In such a model, a honey 102 bee is a bee that produces honey, a sleeping pill is a pill that induces sleeping and a headache pill is a pill that relieves headaches. In some previous computational work on NC interpretation, manually constructed dictionaries provided typical activities or functions associated with nouns (Finin, 1980; Isabelle, 1984; Johnston and Busa, 1996). It is, however, impractical to build large structured lexicons for broad-coverage systems; these methods can only be applied to specialized domains. On the other hand, we expect that the ready availability of large text corpora should facilitate the automatic mining of rich paraphrase information. The SemEval-2010 task we present here builds on the work of Nakov (Nakov and Hearst, 2006; Nakov, 2007; Nakov, 2008b), where NCs are paraphrased by combinations of verbs and prepositions. Given the problem of synonymy, we do not provide a single correct paraphrase for a given NC but a probability d"
W09-2416,P06-2064,1,0.371927,"rge number of human subjects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for detai"
W09-2416,E03-1073,0,0.0568415,"Missing"
W09-2416,P08-1052,1,0.883976,"jects. In this framework, the task of interpretation becomes one of identifying the most likely paraphrases for an NC. Nakov (2008b) and Butnariu and Veale (2008) have demonstrated that paraphrasing information can be collected from corpora in an unsupervised fashion; we expect that participants in SemEval-2010 Task 9 will further develop suitable techniques for this problem. Paraphrases of this kind have been shown to be useful in applications such as machine translation (Nakov, 2008a) and as an intermediate step in inventory-based classification of abstract relations (Kim and Baldwin, 2006; Nakov and Hearst, 2008). Progress in paraphrasing is therefore likely to have follow-on benefits in many areas. 3 Task Description The description of the task we present below is preliminary. We invite the interested reader to visit the official Website of SemEval-2010 Task 9, where upto-date information will be published; there is also a discussion group and a mailing list.2 3.1 Preliminary Study In a preliminary study, we asked 25-30 human subjects to paraphrase 250 noun-noun compounds using suitable paraphrasing verbs. This is the Levi250 dataset (Levi, 1978); see (Nakov, 2008b) for details.3 The most popular par"
W09-2416,W07-1108,1,0.886065,"Missing"
W09-2416,W01-0511,0,0.105028,"ONSTITUTE into SOURCE-RESULT, RESULT-SOURCE and COPULA; COPULA is then further subdivided at two additional levels. 101 In computational linguistics, popular inventories of semantic relations have been proposed by Nastase and Szpakowicz (2003) and Girju et al. (2005), among others. The former groups 30 finegrained relations into five coarse-grained supercategories, while the latter is a flat list of 21 relations. Both schemes are intended to be suitable for broad-coverage analysis of text. For specialized applications, however, it is often useful to use domain-specific relations. For example, Rosario and Hearst (2001) propose 18 abstract relations for interpreting NCs in biomedical text, e.g., DEFECT, MATERIAL, PERSON AFFILIATED, ATTRIBUTE OF CLINICAL STUDY. Inventory-based analyses offer significant advantages. Abstract relations such as ‘location’ and ‘possession’ capture valuable generalizations about NC semantics in a parsimonious framework. Unlike paraphrase-based analyses (Section 2.2), they are not tied to specific lexical items, which may themselves be semantically ambiguous. They also lend themselves particularly well to automatic interpretation methods based on multi-class classification. On the"
W09-2416,D08-1027,0,0.0137166,"Missing"
W09-2416,W03-1803,0,0.0693929,"Missing"
W09-4105,J96-1002,0,0.0116859,"Missing"
W09-4105,P05-1022,0,0.0960431,"Missing"
W09-4105,J93-2004,0,0.0365081,"both timeconsuming and error-prone. Still, using machine learning has one major limitation: it requires manually annotated corpora as training data, which can be quite costly to create. Fortunately, for Bulgarian such a rich resource already exists – the BulTreeBank1 , an HPSGbased Syntactic Treebank with rich annotations at various linguistic levels. The existence of such a resource makes it possible to adapt to Bulgarian various nlp tools that have been originally developed for other languages, e.g., English, and that have been trained on similar kinds of resources, e.g., the Penn Treebank [4]. 1 (1) The features used in the OpenNLP framework combine heterogeneous contextual information such as words around the end of a sentence for the English sentence splitter, or word, character ?-grams and partof-speech tag alone and in various combinations for the English chunker. These features are based on the publications of Sha and Pereira [7] for the chunker, and on the dissertation of Ratnaparkhi [6] for the POS tagger and the syntactic parser. The remainder of this paper is organized as follows: Section 2 describes the process of converting the BulTreeBank XML data to Penn Treebank-styl"
W09-4105,N03-1028,0,0.0610098,"vels. The existence of such a resource makes it possible to adapt to Bulgarian various nlp tools that have been originally developed for other languages, e.g., English, and that have been trained on similar kinds of resources, e.g., the Penn Treebank [4]. 1 (1) The features used in the OpenNLP framework combine heterogeneous contextual information such as words around the end of a sentence for the English sentence splitter, or word, character ?-grams and partof-speech tag alone and in various combinations for the English chunker. These features are based on the publications of Sha and Pereira [7] for the chunker, and on the dissertation of Ratnaparkhi [6] for the POS tagger and the syntactic parser. The remainder of this paper is organized as follows: Section 2 describes the process of converting the BulTreeBank XML data to Penn Treebank-style bracketing, Section 3 describes the experiments and discusses the results, and Section 4 concludes and suggests directions for future work. Created at the Linguistic Modelling Laboratory (LML), Institute for Parallel Processing, Bulgarian Academy of Sciences. See http://www.bultreebank.org for details. 2 3 http://opennlp.sourceforge.net http://m"
W09-4105,P07-1096,0,0.128102,"Missing"
W09-4503,W06-3306,0,\N,Missing
W09-4503,J96-1002,0,\N,Missing
W09-4503,P05-1012,0,\N,Missing
W09-4503,W02-0303,0,\N,Missing
W09-4503,W05-1301,0,\N,Missing
W11-4205,W07-1509,0,\N,Missing
W11-4205,W07-1017,0,\N,Missing
W11-4205,D11-1133,0,\N,Missing
W11-4205,D11-1136,0,\N,Missing
W11-4205,R09-1022,1,\N,Missing
W12-3136,J93-2003,0,0.0307065,"nted with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order becomes first @-@ order; • Maximum sentence length: 100 tokens; • Truecasing: convert sentence-initial words to their most frequent case in the training dataset; • Word alignments: directed IBM model 4 (Brown et al., 1993) alignments in both directions, then grow-diag-final-and heuristics; • Maximum phrase length: 7 tokens; 1 The WMT12 organizers invited systems translating between English and four other European languages, in both directions: French, Spanish, German, and Czech. However, we only participated in Spanish→English and German→English. • Phrase table scores: forward & reverse phrase translation probabilities, forward & reverse lexical translation probabilities, phrase penalty; 298 Proceedings of the 7th Workshop on Statistical Machine Translation, pages 298–303, c Montr´eal, Canada, June 7-8, 2012. 2"
W12-3136,N03-1017,0,0.0392872,"g Research Institute for the WMT12 Shared Translation Task. We used a phrase-based statistical machine translation model with several non-standard settings, most notably tuning data selection and phrase table combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline sy"
W12-3136,P07-2045,0,0.0110187,"2008): when a phrase pair appeared in both tables, they only kept the entry from the first table, while we keep the entries from both tables. 299 30.94 31.36 Table 2: Phrase table merging. 2.3 Language Models We built the language models (LM) for our systems using a probabilistic 5-gram model with KneserNey (KN) smoothing. We experimented with LMs trained on different training datasets. We used the SRILM toolkit (Stolcke, 2002) for training the language models, and the KenLM toolkit (Heafield and Lavie, 2010) for binarizing the resulting ARPA models for faster loading with the Moses decoder (Koehn et al., 2007). 2.3.1 Using WMT12 Corpora Only We trained 5-gram LMs on datasets provided by the task organizers. The results are presented in Table 3. The first line reports the baseline BLEU scores using a language model trained on the target side of the News Commentary + Europarl training bi-texts. The second line shows the results when using an interpolation (minimizing the perplexity on the news2010 tuning dataset) of different language models, trained on the following corpora: • the monolingual News Commentary corpus plus the English sides of all training News Commentary v.7 bi-texts (for French-Engli"
W12-3136,N04-1022,0,0.0436485,"t. This means that our selected source-side sentences tended to be shorter than in the baseline. Moreover, the standard deviation of the sentence lengths was smaller for our samples as well, which means that there were fewer long sentences; this is good since long sentences can take very long to translate. As a result, we observed sizable speedup in parameter tuning when running MERT on our selected tuning datasets. Decoding and Hypothesis Reranking We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmann and Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). The results are shown in Table 7. We can see that both yield improvements in BLEU, even if small. 2.6 Baseline (es:#2,de:#3) +MP 29.83 29.98 21.72 22.03 Baseline (es:#4,de:#5) +MBR 30.16 30.31 22.30 22.48 Table 7: Decoding parameters. Experiments with monotone at punctuation (MP) reordering, and minimum Bayes risk (MBR) decoding. The results for the actual news2012 testset are shown in Table 8: the system combination results are our primary submission. We can see that system combination yielded 0.4 BLEU points of improvement for Spanish-English and 0.2-0.3 BLEU points for German-English. 3 T"
W12-3136,W08-0320,1,0.751604,"g MERT.3 Table 2 shows that this improves by +0.42 BLEU points. 2 In theory, we should also re-normalize the conditional probabilities (forward/reverse phrase translation probability, and forward/reverse lexicalized phrase translation probability) since they may not sum to one anymore. In practice, this is not that important since the log-linear phrase-based SMT model does not require that the phrase table features be probabilities (e.g., F1 , F2 , F3 , and the phrase penalty are not probabilities); moreover, we have extra features whose impact is bigger. 3 This is similar but different from (Nakov, 2008): when a phrase pair appeared in both tables, they only kept the entry from the first table, while we keep the entries from both tables. 299 30.94 31.36 Table 2: Phrase table merging. 2.3 Language Models We built the language models (LM) for our systems using a probabilistic 5-gram model with KneserNey (KN) smoothing. We experimented with LMs trained on different training datasets. We used the SRILM toolkit (Stolcke, 2002) for training the language models, and the KenLM toolkit (Heafield and Lavie, 2010) for binarizing the resulting ARPA models for faster loading with the Moses decoder (Koehn"
W12-3136,P02-1040,0,0.0922053,"le combination. The evaluation results show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order become"
W12-3136,2006.amta-papers.25,0,0.0155937,"esults show that we rank second in BLEU and TER for Spanish-English, and in the top tier for German-English. 1 System Description Introduction The team of the Qatar Computing Research Institute (QCRI) participated in the Shared Translation Task of WMT12 for two language pairs:1 SpanishEnglish and German-English. We used the state-ofthe-art phrase-based model (Koehn et al., 2003) for statistical machine translation (SMT) with several non-standard settings, e.g., data selection and phrase table combination. The evaluation results show that we rank second in BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) for Spanish-English, and in the top tier for German-English. In Section 2, we describe the parameters of our baseline system and the non-standard settings we experimented with. In Section 3, we discuss our primary and secondary submissions for the two language pairs. Finally, in Section 4, we provide a short summary. 2.1 Initial Configuration Our baseline system can be summarized as follows: • Training: News Commentary + Europarl training bi-texts; • Tuning: news2010; • Testing: news2011; • Tokenization: splitting words containing a dash, e.g., first-order becomes first @-@ order; • Maximum s"
W12-3136,J03-1005,0,0.0353527,"smaller than in our baseline, the news2011 development dataset. This means that our selected source-side sentences tended to be shorter than in the baseline. Moreover, the standard deviation of the sentence lengths was smaller for our samples as well, which means that there were fewer long sentences; this is good since long sentences can take very long to translate. As a result, we observed sizable speedup in parameter tuning when running MERT on our selected tuning datasets. Decoding and Hypothesis Reranking We experimented with two decoding settings: (1) monotone at punctuation reordering (Tillmann and Ney, 2003), and (2) minimum Bayes risk decoding (Kumar and Byrne, 2004). The results are shown in Table 7. We can see that both yield improvements in BLEU, even if small. 2.6 Baseline (es:#2,de:#3) +MP 29.83 29.98 21.72 22.03 Baseline (es:#4,de:#5) +MBR 30.16 30.31 22.30 22.48 Table 7: Decoding parameters. Experiments with monotone at punctuation (MP) reordering, and minimum Bayes risk (MBR) decoding. The results for the actual news2012 testset are shown in Table 8: the system combination results are our primary submission. We can see that system combination yielded 0.4 BLEU points of improvement for Sp"
W13-5301,W13-5300,0,0.244973,"Missing"
W14-3352,E06-1032,0,0.104635,"translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to the combination other metrics fr"
W14-3352,W07-0734,0,0.111099,"Missing"
W14-3352,D08-1024,0,0.0117943,"se years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous"
W14-3352,W14-3336,0,0.123967,"Missing"
W14-3352,2003.mtsummit-papers.9,0,0.0555949,"part raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Next, we add to th"
W14-3352,P07-1098,0,0.0527917,"words in an Elementary Discourse Unit (EDU) are grouped under a predefined tag EDU, to which the nuclearity status of the EDU is attached: nucleus vs. satellite. Coherence relations, such as Attribution, Elaboration, and Enablement, between adjacent text spans constitute the internal nodes of the tree. Like the EDUs, the nuclearity statuses of the larger discourse units are attached to the relation labels. Notice that with this representation the tree kernel can easily be extended to find subtree matches at the word level, i.e., by including an additional layer of dummy leaves as was done in (Moschitti et al., 2007). We applied the same solution in our representations. Discourse-Based Metrics In our recent work (Guzm´an et al., 2014), we used the information embedded in the discourse-trees (DTs) to compare the output of an MT system to a human reference. More specifically, we used a state-of-the-art sentence-level discourse parser (Joty et al., 2012) to generate discourse trees for the sentences in accordance with the Rhetorical Structure Theory (RST) of discourse (Mann and Thompson, 1988). Then, we computed the similarity between DTs of the human references and the system translations using a convolutio"
W14-3352,2003.mtsummit-papers.10,0,0.311329,"trics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design better evaluation metrics. The Metrics task at the Workshop on Machine Translation (WMT) has been instrumental in this quest. Below we present QCRI’s submission to the Metrics task of WMT14, which consists of the DiscoTK family of discourse-based metrics. In particular, we experiment with five different transformations and augmentations of a discourse tree representation, and we combine the kernel scores for each of them into a single score which we call D ISCOTKlight . Ne"
W14-3352,2012.amta-papers.6,0,0.0421525,"@qf.org.qa Abstract Recently, there has been steady increase in BLEU scores for well-resourced language pairs such as Spanish-English and Arabic-English. However, it was also observed that BLEU-like ngram matching metrics are unreliable for highquality translation output (Doddington, 2002; Lavie and Agarwal, 2007). In fact, researchers already worry that BLEU will soon be unable to distinguish automatic from human translations.1 This is a problem for most present-day metrics, which cannot tell apart raw machine translation output from a fully fluent professionally post-edited version thereof (Denkowski and Lavie, 2012). Another concern is that BLEU-like n-gram matching metrics tend to favor phrase-based SMT systems over rule-based systems and other SMT paradigms. In particular, they are unable to capture the syntactic and semantic structure of sentences, and are thus insensitive to improvement in these aspects. Furthermore, it has been shown that lexical similarity is both insufficient and not strictly necessary for two sentences to convey the same meaning (Culy and Riehemann, 2003; Coughlin, 2003; Callison-Burch et al., 2006). The above issues have motivated a large amount of work dedicated to design bette"
W14-3352,P03-1021,0,0.00737295,"orms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Associatio"
W14-3352,P02-1040,0,0.100492,"nally, we add other metrics from the A SIYA MT evaluation toolkit, and we tune the weights of the combination on actual human judgments. Experiments on the WMT12 and WMT13 metrics shared task datasets show correlation with human judgments that outperforms what the best systems that participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mea"
W14-3352,2006.amta-papers.25,0,0.163958,"ent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that are based on new representations of the DTs. In the remainder of this section, we will focus on the individual DT representations that"
W14-3352,P14-1065,1,0.494914,"Missing"
W14-3352,D07-1080,0,0.0139657,"hat participated in these years achieved, both at the segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, w"
W14-3352,D11-1125,0,0.0213356,"segment and at the system level. 1 Introduction The rapid development of statistical machine translation (SMT) that we have seen in recent years would not have been possible without automatic metrics for measuring SMT quality. In particular, the development of BLEU (Papineni et al., 2002) revolutionized the SMT field, allowing not only to compare two systems in a way that strongly correlates with human judgments, but it also enabled the rise of discriminative log-linear models, which use optimizers such as MERT (Och, 2003), and later MIRA (Watanabe et al., 2007; Chiang et al., 2008) and PRO (Hopkins and May, 2011), to optimize BLEU, or an approximation thereof, directly. While over the years other strong metrics such as TER (Snover et al., 2006) and Meteor (Lavie and Denkowski, 2009) have emerged, BLEU remains the de-facto standard, despite its simplicity. 1 This would not mean that computers have achieved human proficiency; it would rather show BLEU’s inadequacy. 402 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 402–408, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Here, we extend our previous work by developing metrics that"
W14-3352,D12-1083,1,\N,Missing
W14-3628,bouamor-etal-2014-multidialectal,0,0.0565805,"a for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor, and its MAP model (Creutz and Lagus, 2007), using different variations of the collected Qatari data. We optimize the single hyp"
W14-3628,J93-2003,0,0.0394221,"Missing"
W14-3628,2012.eamt-1.60,0,0.0204419,"n help to better adapt resources for dialects and MSA for SMT. This section describes our experimental setup. to Qº, and we reduce character elongations to be just two characters long. In order to maintain consistency among different resources, we re  move supplementary diacritics, e.g., Y®« ‘knots’ Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor.  /Euqad/ is normal"
W14-3628,E06-1047,0,0.0771248,"so explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA consonant h. /j/ can be used to distinguish between different dialects, particularly Gulf subdialects. h. /j/ is pronounced as ø /y/ i"
W14-3628,P11-1105,0,0.0332246,"Missing"
W14-3628,elmahdy-etal-2014-development,0,0.0277134,"More detailed description follows below. Lexical variations are among the most obvious differences between Arabic dialects. For exam ple, the MSA word @ XAÓ ‘what’ /mA*A/ would be 3.4 QCA Table 1: Statistics about the collected parallel corpora (in thousands). AVIAO shows the statistics about the AVIA corpus excluding Qatari data. /taEal˜am/ becomes ÕÎªK @ /AitEalim/ in EGY, while the MSA form is preserved in QA. 3.3 Corpus Bilingual corpora: – The QCA speech corpus, comprises 14.7k sentences that are phonetically transcribed from TV broadcasts in Qatari Arabic and translated to English; see (Elmahdy et al., 2014) for more detail. The corpus was designed for speech recognition and we faced several normalization-related issues that we had to resolve before it could be used for machine translation and language modeling. One example is the usage of five Persian characters to represent some sounds in Arabic words. Moreover, the English side had some grammatical and spelling errors. We normalized the Arabic side and corrected the English side of the corpus as described in Section 4.2. The corpus can be found at http://sprosig.isle. illinois.edu/corpora/1. – The AVIA corpus1 is designed as a reference source"
W14-3628,P12-1016,0,0.0171831,"/V/ to ¬ H /P/ to H. /b/, and P and h /J/ to h. /j/. /f/, For the English texts, the orthographic variations were already normalized. However, the English side of the QCA corpus had some spelling and grammatical errors, which we corrected manually. On the grammatical side, we only corrected a subset of the data, which we used for tuning and testing our SMT system (see Section 5). 4.3 Morphological Decomposition There is no general Arabic morphological segmenter that works for all variations of Arabic. The most commonly used segmenters for Arabic were designed for MSA (Habash et al., 2009; Green and DeNero, 2012). Due to the lexical and morphological differences between dialects and MSA, these MSA-based morphological tools do not work well for dialects. 5 Experimental Setup Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6 This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. This issue relates to the QCA corpus. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et a"
W14-3628,abdelali-etal-2014-amara,1,0.75838,"s and MSA for SMT. This section describes our experimental setup. to Qº, and we reduce character elongations to be just two characters long. In order to maintain consistency among different resources, we re  move supplementary diacritics, e.g., Y®« ‘knots’ Datasets: We divided the QCA corpus into 1k sentences each for development and testing, and we used the remaining 12k for training. We adapted parallel corpora for Egyptian, Levantine and MSA to English to be used for Qatari Arabic to English SMT. For MSA, we used parallel corpora of TED talks (Cettolo et al., 2012) and the AMARA corpus (Abdelali et al., 2014), which consists of educational videos. Since the QCA corpus is in the speech domain, we believe that an MSA corpus of spoken domain would be more helpful than a text domain such as News. For Egyptian and Levantine, we used the parallel corpus provided by Zbib et al. (2012). There is no Gulf–English parallel data available in the literature. The data that we found was a very small collection of subdialects of Gulf Arabic; we did not use it for MT experiments. However, we used the Qatari part of the AVIA corpus to train Morfessor.  /Euqad/ is normalized to Y®«, and we map Persian letters to th"
W14-3628,P06-1086,0,0.176067,"uses morphological segmentation to combine resources for other Arabic dialects in a QA-EN SMT system effectively (Section 4.3). We also explain our experimental setup and we present the results (Section 5). We then discuss translating in the reverse direction, i.e., into Qatari Arabic (Section 6). Finally, we point to possible directions for future work and we conclude the paper (Section 7). Building morphological segmenters for the Arabic dialects: Researchers have already focused efforts on crafting and extending existing MSA tools to DA by mainly using a set of rules (Habash et al., 2012). Habash and Rambow (2006) presented MAGEAD, a knowledge-based morphological analyzer and generator for Egyptian and Levantine Arabic. Chiang et al. (2006) developed a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA conso"
W14-3628,al-sabbagh-girju-2010-mining,0,0.0365102,"me additional monolingual data for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such a"
W14-3628,W14-3601,0,0.0778929,"ages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor, and its MAP model (Creutz and Lagus, 2007), using different variations of the collected Qatari data. We optimize the single hyperparameter of the MAP model by maximizing the translation quality of the QA-EN SMT system in terms of BLEU. Our experimental results demo"
W14-3628,D09-1141,1,0.822284,"ith the QCA bitext for Qatari Arabic to English machine translation. We explored three segmentation options for the Arabic side of the data: (i) no segmentation, (ii) ATB segmentation, and (iii) unsupervised segmentation using Morfessor. The QCA corpus is of much smaller size compared to other Arabic variants, say MSA. It is possible that in the training of the machine translation models, the large corpus dominates the QCA corpus. In order to avoid that, we balanced the two corpora by replicating the smaller corpus X number of times in order to make it approximately equal to the large corpus (Nakov and Ng, 2009).8 The complete procedure is described below. In a nutshell, for building a machine translation system using the MSA plus Qatari corpus, we first balanced the Qatari corpus to make it approximately equal to MSA and concatenated them. For training Morfessor, the Qatari Arabic data consisted of QCA, Novels and AVIAQA , while for SMT, it consisted of QCA only. In both cases, we balanced it to be approximately equal to MSA. We then trained Morfessor on the balanced (QCA, Novels, AVIAQA ) plus MSA data and we segmented the Arabic side of the balanced QCA plus MSA training data for machine translati"
W14-3628,C12-1121,1,0.888804,"Missing"
W14-3628,W11-2123,0,0.0741014,"Missing"
W14-3628,P02-1040,0,0.0966622,"Missing"
W14-3628,D11-1125,0,0.0296435,"Missing"
W14-3628,W14-3627,0,0.108906,"ialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The rest of the paper is organized as follows: Fi"
W14-3628,N09-1024,0,0.0276176,"s the first collection of monolingual corpora for Gulf Arabic subdialects. It can be helpful for, e.g., language modeling when translating into Arabic, for learning the similarities and differences between Gulf subdialects, etc. Table 2 shows some statistics about the data after punctuation tokenization. In this work, we used an unsupervised morphological segmenter, Morfessor-categories MAP6 , an unsupervised model with a single hyperparameter (Creutz and Lagus, 2007). We chose Morfessor because of its superior performance on Arabic compared to other unsupervised models (Siivola et al., 2007; Poon et al., 2009). The model has a single hyperparameter, the perplexity threshold parameter B, which controls the granularity of segmentation. The recommended value ranges from 1 to 400 where 1 means maximum fine-grained segmentation, and 400 restricts it to the least segmented output. We set the threshold empirically to 70, as shown in Section 5.1. 4.2 5 Corpus Tokens Types AE BH Novel KW OM QA SA Forum QA 573 43 244 22 178 27 412 43 614 71 69 15 372 27 Table 2: Statistics about the collected monolingual corpora (in thousands of words). Orthographic Normalization The inconsistency in the orthographic spellin"
W14-3628,2006.amta-papers.21,0,0.0275349,"oped a Levantine morphological analyzer on top of an existing MSA analyzer using an explicit knowledge base. 208 Meanwhile, the MSA consonant /Z/ is realized as /D/ in EGY. For example, the MSA ‘luck’ is maintained pronunciation /HaZ/ of ¡k in QA and transformed to /HaD/ in EGY. This change is consistent in all words within each dialect. However, such phonological variations between dialects have the potential to add ambiguity to dialectal Arabic. The MSA consonant h. /j/ can be used to distinguish between different dialects, particularly Gulf subdialects. h. /j/ is pronounced as ø /y/ in KW, Riesa and Yarowsky (2006) trained a supervised trie-based model using a small lexicon of dialectal affixes. In our work, we eliminate the need for linguistic knowledge by training an unsupervised model using available resources. The unsupervised mode of learning allowed us to develop a multi-dialectal morphological segmenter. 3 Arabic Dialects In this section, we highlight some of the linguistic differences between Arabic dialects and MSA, with a focus on the Qatari dialect. 3.1 BH, QA, AE,  /q/ in OM, much like in EGY, h. /j/ in SA, much like in LEV. For example, the MSA word Yj.Ó ‘mosque’ /masjid/ is  pronounced"
W14-3628,N03-1017,0,0.00752633,"these MSA-based morphological tools do not work well for dialects. 5 Experimental Setup Machine translation system settings: We used a phrase-based statistical machine translation model as implemented in the Moses toolkit (Koehn et al., 2007) for machine translation. 6 This is an extension of the basic Morfessor method and is based on a Maximum a Posteriori model. This issue relates to the QCA corpus. 211 We built separate directed word alignments for source-to-target and target-to-source using IBM model 4 (Brown et al., 1993), and we symmetrized them using the grow-diag-final-and heuristics (Koehn et al., 2003). We then extracted phrase pairs with a maximum length of seven, and we scored them using maximum likelihood estimation with Kneser-Ney smoothing (Kneser and Ney, 1995). We also built a lexicalized reordering model, msd-bidirectional-fe. We built a 5-gram language model on the English side of QCA-train using KenLM (Heafield, 2011). Finally, we built a log-linear model using the above features. We tuned the model weights by optimizing BLEU (Papineni et al., 2002) on the tuning set, using PRO (Hopkins and May, 2011) with sentencelevel BLEU+1 optimization (Nakov et al., 2012). In testing, we used"
W14-3628,P13-2001,1,0.937556,"ic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The rest of the paper is o"
W14-3628,N04-1022,0,0.0213042,"Missing"
W14-3628,salama-etal-2014-youdacc,0,0.1266,"ic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentat"
W14-3628,maamouri-etal-2006-developing,0,0.115294,"l, we also collected some additional monolingual data for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dia"
W14-3628,W11-2602,0,0.0453054,"which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap between dialects and MSA. The r"
W14-3628,2010.amta-papers.5,0,0.355818,"ation model, which we train on the Arabic side of the multi-dialectal training data, which consists of Qatari Arabic, Egyptian Arabic (EGY), Levantine Arabic (LEV) and MSA to English, i.e., a scaled combination of all the available parallel data. We train a QA-EN SMT system using the segmented multi-dialectal data, and we show an absolute gain of 1.5 BLEU points compared to a baseline that uses no segmentation. Adapting SMT resources for other Arabic dialects: Many researchers have explored the potential of using MSA as a pivot language for improving SMT of Arabic dialects (Bakr et al., 2008; Sawaf, 2010; Salloum and Habash, 2011; Sajjad et al., 2013a; Jeblee et al., 2014). This often involves DA-MSA conversion schemes as an alternative in the absence of DA-MSA parallel resources. In contrast, limited work has been done on leveraging available resources for other dialects. Recently, Zbib et al. (2012) have shown that using a small amount of dialectal data could yield great improvements for SMT. Here, we investigate the potential of improving the resource adaptability of Arabic dialects. Our work is different as we use an unsupervised segmenter that helps in improving the lexical overlap betwe"
W14-3628,P11-2007,0,0.145365,"ta for Qatari Arabic. Qatari Arabic is a subdialect of the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an"
W14-3628,N12-1006,0,0.715837,"the more general Gulf dialect, among with Saudi, Kuwaiti, Emirati, Bahraini, and Omani; we collected additional monologual data for each of these subdialects, and we release this data to the research community. Related Work NLP for DA is still in its early stages of development and many challenges need to be overcomed such as the lack of suitable tools and resources. Collecting resources for dialectal Arabic: Several researchers have directed efforts to develop DA computational resources (Maamouri et al., 2006; Al-Sabbagh and Girju, 2010; Zaidan and Callison-Burch, 2011; Salama et al., 2014). Zbib et al. (2012) built two dialectal Arabic-English parallel corpora for Egyptian and Levantine Arabic using crowdsourcing. Bouamor et al. (2014) presented a multi-dialectal Arabic parallel corpus, which covers five Arabic dialects besides MSA and English. Mubarak and Darwish (2014) collected a multi-dialectal corpus using Twitter. Unlike previous work, we focus on Gulf subdialects, particularly Qatari Arabic. The monolingual data that we collected is a high-quality dialectal resource and originates from dialect-specific sources such as novels and forums. We train an unsupervised segmentation tool, Morphessor"
W14-3628,N06-2051,1,0.790504,"word forms of a root word may not be always possible. Considering the different variants of Arabic, the problem is exacerabated as dialects could use different choices of affixes for the same function. For example, the MSA . ªÊK /yalEabuwn/, meaning ‘they are playword àñJ ing’, could be found as . ªÊK /ylEbuwn/ in Gulf, àñJ @ñJ.ªÊK Ñ« /Eam yilEabuA/ in Levantine, and as @ñJ.ªÊJ K. /biylEabwA/ in Egyptian Arabic. as Introduction One possible solution is to use a morphological segmenter that segments words into simpler units such as stems and affixes, which might be covered in the training set (Zollmann et al., 2006; Tsai et al., 2010). When applied to dialects, this may reduce the lexical gap between dialects and MSA by matching the common stems. Unfortunately, there are no standard morphological segmentation tools for dialects. Due to the difference in morphology, tools designed for MSA do not work well for dialects. Developing rule-based segmenters for each dialect might appear to be the ideal solution, but, as the orthography of dialects is not standardized, crafting linguistic rules for them is very hard. The Arabic language has many varieties, where the Modern Standard Arabic (MSA) coexists with va"
W14-3628,P07-2045,0,\N,Missing
W14-3628,W12-2301,0,\N,Missing
W14-3628,2013.iwslt-evaluation.8,1,\N,Missing
W15-2501,W15-2508,1,0.700494,"all submissions, both primary and secondary, in terms of macro-averaged F-score, several systems performed better in terms of accuracy. The other systems did not use explicit anaphora resolution, but attempted to gather relevant information about possible antecedents by considering a certain number of preceding, or preceding and following, noun phrases. They differed in the type of classifier and in the information sources used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED U"
W15-2501,E06-1032,0,0.027114,"itself achieves the best scores, but considering the inadequacy of BLEU for pronoun evaluation, we do not see this as a major concern in itself. The other submissions fall behind in terms of automatic MT metrics. The UU - HARDMEIER system is similar to the other SMT systems, but uses different language and translation models, which evidently do not yield the same level of raw MT performance as the baseline system. ITS 2 is a rule-based system. Since it is well known that n-gram-based evaluation metrics do not always do full justice to rule-based MT approaches not using n-gram language models (Callison-Burch et al., 2006), it is difficult to draw definite conclusions from this system’s lower scores. Finally, the extremely low scores for the A 3-108 system indicate serious problems with translation quality, an impression that we easily confirmed by examining the system output. 8 5 The low scores for the ITS 2 system were partly due to a design decision. The anaphora prediction component of ITS 2 only generated the personal pronouns il, elle, ils and elles; this led to zero recall for ce and ça/cela and, as a consequence, to a large number of misses that would have been comparatively easy to predict with an n-gr"
W15-2501,P14-1065,1,0.749966,"Missing"
W15-2501,2012.eamt-1.60,1,0.298652,"est data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare"
W15-2501,2010.iwslt-papers.10,1,0.800853,"g such a setup makes it possible to explore a variety of approaches for solving the problem at hand since the participating groups independently come up with various ways to address it. All of this is highly beneficial for continued research as it creates a well-defined benchmark with a low entry barrier, a set of results to compare to, and a collection of properly evaluated ideas to start from. We decided to base this shared task on the problem of pronoun translation. Historically, this was one of the first discourse problems to be considered in the context of SMT (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010); yet, it is still far from being solved. For an overview of the existing work on pronoun translation, we refer the reader to Hardmeier (2014, Section 2.3.1). The typical case is an anaphoric pronoun – one that refers to an entity mentioned earlier in the discourse, its antecedent. Many languages have agreement constraints between pronouns and their antecedents. In translation, these constraints must be satisfied in the target language. Note that source language information is not enough for this task. To see why, consider the following example for English– French:1 We describe the design, the"
W15-2501,P13-4033,1,0.934301,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,chrupala-etal-2008-learning,0,0.0467164,"Missing"
W15-2501,W11-2107,0,0.0222412,"Missing"
W15-2501,D13-1037,1,0.793439,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,N13-1073,0,0.0272151,"inting characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoot"
W15-2501,W15-2510,1,0.750725,"hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), they do not specifically focus on pronoun translation. 5 Machine Translation Evaluation http://stp.lingfil.uu.se/~ch/DiscoMT2015.maneval/index.php Machine Translation Eva"
W15-2501,J07-3002,0,0.00746111,"n order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all three models, the results on pronoun links are better than those on all links. More"
W15-2501,P13-2121,0,0.00705082,"Missing"
W15-2501,D07-1103,0,0.0077006,"00 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200"
W15-2501,guillou-etal-2014-parcor,1,0.687529,"g requirements: 1. The talks have been transcribed (in English) and translated into French. 2. They were not included in the training, development, and test datasets of any IWSLT evaluation campaign, so DiscoMT.tst2015 can be used as held-out data with respect to those. 3. They contain a sufficient number of tokens of the English pronouns it and they translated into the French pronouns listed in Table 1. 4. They amount to a total number of words suitable for evaluation purposes (e.g., tens of thousands). 2 http://www.ted.com 3 The following overview of text characteristics is based on work by Guillou et al. (2014). 3 To meet requirement 3, we selected talks for which the combined count of the rarer classes ça, cela, elle, elles and on was high. The resulting distribution of pronoun classes, according to the extraction procedure described in Section 5.1, can be found in Table 8 further below. We aimed to have at least one pair of talks given by the same speaker and at least one pair translated by the same translator. These two features are not required by the DiscoMT shared task, but could be useful for further linguistic analysis, such as the influence of speakers and translators on the use of pronouns"
W15-2501,W15-2509,0,0.0482925,"gy of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T sy"
W15-2501,W14-3352,1,0.899484,"Missing"
W15-2501,D14-1027,1,0.902972,"Missing"
W15-2501,P07-2045,0,0.00942963,"2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million w"
W15-2501,2005.mtsummit-papers.11,0,0.0282203,"e resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For compl"
W15-2501,W15-2514,0,0.067491,"Missing"
W15-2501,W10-1737,0,0.740286,"Missing"
W15-2501,W11-1902,0,0.185739,"sions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with"
W15-2501,W15-2512,0,0.121441,"glish-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For completeness, we also provided a recasing model that was trained on the same dataset to render it straightforward to produce case-sensitive output, which we required as the final submission. 4.2 ITS 2 (Loáiciga and Wehrli, 2015) was a rulebased machine translation system using syntaxbased transfer. For the shared task, it was extended with an anaphora resolution component influenced by Binding Theory (Chomsky, 1981). For the sixth submission, A 3-108, no system description paper was submitted. Its output seemed to have been affected by problems at the basic MT level, yielding very bad translation quality. 4.3 Evaluation Methods Evaluating machine translations for pronoun correctness automatically is difficult because standard assumptions fail. In particular, it is incorrect to assume that a pronoun is translated corr"
W15-2501,2006.amta-papers.25,0,0.144804,"Missing"
W15-2501,W15-2511,0,0.0430542,"s used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED UEDIN MALTA 2 MALTA WHATELLES UEDIN 2 UU - TIED 2 GENEVA GENEVA 2 IDIAP IDIAP 2 A 3-108 ( WITHDRAWN ) Macro-F Accuracy ce cela elle elles F-score il 0.584 0.579 0.571 0.565 0.561 0.553 0.550 0.539 0.437 0.421 0.206 0.164 0.129 0.122 0.663 0.742 0.723 0.740 0.732 0.721 0.714 0.734 0.592 0.579 0.307 0.407 0.240 0.325 0.817 0.862 0.823 0.875 0.853 0.862 0.823 0.849 0.647 0.611 0.282 0.152 0.225 0.220 0.346 0.235 0.213 0.1"
W15-2501,W14-3334,1,0.85254,"ed the same bitext as for the MT baseline in the first task (Section 4.1); we pre-processed it like before, except for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignme"
W15-2501,W15-2513,0,0.169961,"ompared to the perhaps more obvious methodology of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline"
W15-2501,tiedemann-2012-parallel,1,0.765503,"ic or human processing. Table 3 shows some statistics and metadata about the TED talks that are part of the DiscoMT.tst2015 set. talk id segs 205 1756 1819 1825 1894 1935 1938 1950 1953 1979 2043 2053 189 186 147 120 237 139 107 243 246 160 175 144 4,188 4,320 2,976 2,754 5,827 3,135 2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment"
W15-2501,W15-2515,1,0.722711,"and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphor"
W15-2501,C96-2141,0,0.0650665,"for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . Fo"
W15-2501,P00-1056,0,0.0244007,"reated automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all t"
W15-2501,W15-2516,0,0.0553089,"air, and (ii) the sums for each row/column; • accuracy; All six groups with system description papers used some form of machine learning. The main difference was whether or not they explicitly attempted to resolve pronominal coreference. Two systems relied on explicit anaphora resolution: UEDIN and MALTA. They both applied the Stanford coreference resolver (Lee et al., 2011) on the source language text, then projected the antecedents to the target language through the word alignments, and finally obtained morphological tags with the Morfette software (Chrupała et al., 2008). The UEDIN system (Wetzel et al., 2015) was built around a maximum entropy classifier. In addition to local context and antecedent information, it used the NADA tool (Bergsma and Yarowsky, 2011) to identify nonreferring pronouns and included predictions by a standard n-gram language model as a feature. The MALTA system (Pham and van der Plas, 2015) was based on a feed-forward neural network combined with word2vec continuous-space word embeddings (Mikolov et al., 2013). It used local context and antecedent information. • precision (P), recall (R), and F-score for each label; • micro-averaged P, R, F-score (note that in our setup, mi"
W15-2501,J03-1002,0,0.00752171,"ing two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and s"
W15-2501,P03-1021,0,0.00636797,"filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version"
W15-2501,P02-1040,0,\N,Missing
W15-2501,W14-3302,0,\N,Missing
W15-5401,Y08-1042,0,0.153038,"iscusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and t"
W15-5401,W15-5408,0,0.229488,"Missing"
W15-5401,W15-5410,0,0.0986295,"sk track, we further made available DSLCC v2.1, which extended DSLCC v2.0 with Mexican Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Sub"
W15-5401,D14-1069,0,0.183053,"Missing"
W15-5401,W14-5317,0,0.204271,"Missing"
W15-5401,W15-5409,0,0.310823,"Spanish and Macanese Portuguese data. 6 The script we used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submissi"
W15-5401,W15-5403,0,0.148019,"used to substitute named entities with placeholders is available here: https://github.com/ Simdiva/DSL-Task/blob/master/blindNE.py 4 Date May 20, 2015 June 22, 2015 June 24, 2015 June 26, 2015 July 20, 2015 Table 3: The DSL 2015 Shared Task schedule. 4 Team BOICEV BRUNIBP INRIA MAC MMS* NLEL NRC OSEVAL PRHLT SUKI Total Closed (Normal) X X X X X X X X X 9 Closed (No NEs) X X X X X X X 7 Open (Normal) X X X 3 Open (No NEs) X X X 3 System Description Paper (Bobicev, 2015) ´ et al., 2015) (Acs (Malmasi and Dras, 2015b) (Zampieri et al., 2015) (Fabra-Boluda et al., 2015) (Goutte and L´eger, 2015) (Franco-Salvador et al., 2015) (Jauhiainen et al., 2015a) 8 Table 4: The participating teams in the DSL 2015 Shared Task. 4 Rank 1 2-3 2-3 4 5 6 7 8 9 Results In this section, we present the results of the 2nd edition of the DSL shared task.7 Most of the participating teams used DSLCC v2.0 only, and thus took part in the closed submission track. Yet, three of the teams collected additional data or used DSLCC v1.0, and thereby participated in the open submission. 4.1 Submitted Runs Accuracy 95.54 95.24 95.24 94.67 94.14 93.66 92.74 83.91 64.04 Table 5: Closed submission results for test set A. A total of 24 teams subscribed"
W15-5401,I11-1062,0,0.0379892,"Missing"
W15-5401,P12-3005,0,0.0283147,"Missing"
W15-5401,W13-1728,1,0.611887,"e group, and then chooses between languages or varieties within this group. The team achieved very strong results this year, ranking second in the closed submission on test set A, third on test set B, and first in the open submission on both test sets A and B. Two other participants used two-stage classification: NLEL (Fabra-Boluda et al., 2015) and ´ et al., 2015). BRUniBP (Acs The MMS team experimented with three approaches (Zampieri et al., 2015), and their best run combined TF.IDF weighting and an SVM classifier, which was previously successfully applied to native language identification (Gebre et al., 2013). The SUKI team (Jauhiainen et al., 2015a) used token-based backoff, which was previously applied to general-purpose language identification (Jauhiainen et al., 2015b). The BOBICEV team applied prediction by partial matching, which had not been used for this task before (Bobicev, 2015). Finally, the PRHLT team (Franco-Salvador et al., 2015) used word and sentence vectors, which is to our knowledge the first attempt to apply them to discriminating between similar languages. Table 7: Open submission results for test set A. This could be related to the availability of DSLCC v1.0 as an obvious add"
W15-5401,U13-1003,0,0.339119,"e general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC v.1.0, which included excerpts from journalistic texts from sources such as the SETimes Corpus1 (Tyers and Alperen, 2010), HC Corpora2 and the Leipzig Corpora Collection (Biemann et al"
W15-5401,W15-5413,0,0.538185,"Missing"
W15-5401,W14-5315,0,0.370837,"Missing"
W15-5401,W14-5316,0,0.374689,"Missing"
W15-5401,W15-5407,0,0.1816,"ke the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the general setup of the task, Section 4 presents the results of the competition, Section 5 summarizes the approaches used by the participants, and Section 6 offers conclusions. 2 Related Work 2.2 Language identification has attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as Malay and Indonesian (RanaivoMalanc¸on, 2006), Persian and Dari (Malmasi and Dras, 2015a), Brazilian and European Portuguese (Zampieri and Gebre, 2012), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), and English varieties (Lui and Cook, 2013), among others. This interest has eventually given rise to special shared tasks, which allowed researchers to compare and benchmark various approaches on common standard datasets. Below we will describe some of these shared tasks, including the first edition of the DSL task. 2.1 The First Edition of the DSL Task For the first edition of the task, we compiled the DSL Corpus Collection (Tan et al., 2014), or DSLCC"
W15-5401,C12-1160,1,0.783877,"Missing"
W15-5401,tiedemann-2012-parallel,1,0.851577,"Missing"
W15-5401,W14-5314,0,0.282427,"Missing"
W15-5401,xia-etal-2010-problems,0,0.0651996,"Missing"
W15-5401,W14-5904,0,0.0750336,"Missing"
W15-5401,W14-5307,1,0.737281,"Missing"
W15-5401,W15-5411,1,0.87912,"Missing"
W15-5401,W14-2505,0,0.0277162,"raining and development subsets, and we further prepared two test sets, as described in Section 3.3 below. As in 2014, teams could make two types of submissions (for each team, we allowed up to three runs per submission type; in the official ranking, we included the run with the highest score only): • Closed submission: Using only the DSLCC v2.0 for training. • Open submission: Using any dataset other than DSLCC v2.0 for training.3 3.2 The Unshared Task Track Along with the Shared Task, this year we proposed an Unshared Task track inspired by the unshared task in PoliInformatics held in 2014 (Smith et al., 2014). For this track, teams were allowed to use any version of DSLCC to investigate differences between similar languages and language varieties using NLP methods. We were interested in studying questions like these: • Are there fundamental grammatical differences in a language group? • What are the most distinctive lexical choices for each language? • Which text representation is most suitable to investigate language variation? • What is the impact of lexical and grammatical variation on NLP applications? Although eleven teams subscribed for the Unshared Task track, none of them ended up submitin"
W15-5401,W14-3907,0,0.0457835,"2015. 2015 Association for Computational Linguistics Another popular research direction has been on language identification on Twitter, which was driven by interest in geolocation prediction for end-user applications (Ljubeˇsi´c and Kranjˇci´c, 2015). This interest has given rise to the TweetLID shared task (Zubiaga et al., 2014), which asked participants to recognize the language of tweet messages, focusing on English and on languages spoken on the Iberian peninsula such as Basque, Catalan, Spanish, and Portuguese. The Shared Task on Language Identification in CodeSwitched Data held in 2014 (Solorio et al., 2014) is another related competition, where the focus was on tweets in which users were mixing two or more languages in the same tweet. First, in order to simulate a real-world language identification scenario, we included in the testing dataset some languages that were not present in the training dataset. Moreover, we included a second test set, where we substituted the named entities with placeholders to make the task more challenging and less dependent on the text topic and domain. The remainder of this paper is organized as follows: Section 2 discusses related work, Section 3 describes the gene"
W15-5401,U10-1003,0,\N,Missing
W15-5401,W14-5318,0,\N,Missing
W16-0427,S16-1003,0,0.0429727,"Missing"
W16-0427,S12-1033,0,0.0277781,"ing especially popular for research due to its scale, representativeness, variety of topics discussed, as well as ease of public access to its messages. Unfortunately, research in that direction was hindered by the unavailability of suitable datasets and lexicons for system training, development and testing. While some Twitter-specific resources were developed, initially they were either small and proprietary, such as the i-sieve corpus (Kouloumpis et al., 2011), were created only for Spanish like the TASS corpus (Villena-Rom´an et al., 2013), or relied on noisy labels obtained automatically (Mohammad, 2012; Pang et al., 2002). This situation changed with the shared task on Sentiment Analysis on Twitter, which was organized at SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013, 2014, 2015 and 2016, attracting over 40+ of participating teams in all four editions. While the focus was on general tweets, the task also featured out-ofdomain testing on SMS messages, LiveJournal messages, as well as on sarcastic tweets. SemEval-2013 task 2 (Nakov et al., 2013) and SemEval-2014 Task 9 (Rosenthal et al., 2014) had an"
W16-0427,S13-2052,1,0.778359,"al., 2013), or relied on noisy labels obtained automatically (Mohammad, 2012; Pang et al., 2002). This situation changed with the shared task on Sentiment Analysis on Twitter, which was organized at SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013, 2014, 2015 and 2016, attracting over 40+ of participating teams in all four editions. While the focus was on general tweets, the task also featured out-ofdomain testing on SMS messages, LiveJournal messages, as well as on sarcastic tweets. SemEval-2013 task 2 (Nakov et al., 2013) and SemEval-2014 Task 9 (Rosenthal et al., 2014) had an expression-level and a message-level polarity subtasks. 171 SemEval-2015 Task 10 (Rosenthal et al., 2015; Nakov et al., 2016b) further added subtasks on topicbased message polarity classification, on detecting trends towards a topic, and on determining the outof-context (a priori) strength of association of Twitter terms with positive sentiment. SemEval-2016 Task 4 (Nakov et al., 2016a) dropped the phrase-level subtask, and focused on sentiment with respect to a topic. It further introduced a 5-point scale, which is used for human review"
W16-0427,S16-1001,1,0.874012,"Missing"
W16-0427,W02-1011,0,0.0156812,"opular for research due to its scale, representativeness, variety of topics discussed, as well as ease of public access to its messages. Unfortunately, research in that direction was hindered by the unavailability of suitable datasets and lexicons for system training, development and testing. While some Twitter-specific resources were developed, initially they were either small and proprietary, such as the i-sieve corpus (Kouloumpis et al., 2011), were created only for Spanish like the TASS corpus (Villena-Rom´an et al., 2013), or relied on noisy labels obtained automatically (Mohammad, 2012; Pang et al., 2002). This situation changed with the shared task on Sentiment Analysis on Twitter, which was organized at SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013, 2014, 2015 and 2016, attracting over 40+ of participating teams in all four editions. While the focus was on general tweets, the task also featured out-ofdomain testing on SMS messages, LiveJournal messages, as well as on sarcastic tweets. SemEval-2013 task 2 (Nakov et al., 2013) and SemEval-2014 Task 9 (Rosenthal et al., 2014) had an expression-level and"
W16-0427,S14-2004,0,0.0254788,"nd focused on sentiment with respect to a topic. It further introduced a 5-point scale, which is used for human review ratings on popular websites such as Amazon, TripAdvisor, Yelp, etc.; from a research perspective, this meant moving from classification to ordinal regression. Moreover, it focused on quantification, i.e., determining what proportion of a set of tweets on a given topic are positive/negative about it. It also featured a 5-point scale ordinal quantification subtask (Gao and Sebastiani, 2015). Other related (mostly non-Twitter) tasks have explored aspect-based sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016), sentiment analysis of figurative language on Twitter (Ghosh et al., 2015), implicit event polarity (Russo et al., 2015), stance in tweets (Mohammad et al., 2016), out-of-context sentiment intensity of phrases (Kiritchenko et al., 2016), and emotion detection (Strapparava and Mihalcea, 2007). Some of these tasks featured languages other than English. We expect the quest for more interesting formulations of the general sentiment analysis task to continue. We see SemEval as the engine of this innovation, as it not only does head-to-head comparisons,"
W16-0427,S15-2082,0,0.0567236,"Missing"
W16-0427,S16-1002,0,0.0352884,"Missing"
W16-0427,S14-2009,1,0.839788,"d automatically (Mohammad, 2012; Pang et al., 2002). This situation changed with the shared task on Sentiment Analysis on Twitter, which was organized at SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013, 2014, 2015 and 2016, attracting over 40+ of participating teams in all four editions. While the focus was on general tweets, the task also featured out-ofdomain testing on SMS messages, LiveJournal messages, as well as on sarcastic tweets. SemEval-2013 task 2 (Nakov et al., 2013) and SemEval-2014 Task 9 (Rosenthal et al., 2014) had an expression-level and a message-level polarity subtasks. 171 SemEval-2015 Task 10 (Rosenthal et al., 2015; Nakov et al., 2016b) further added subtasks on topicbased message polarity classification, on detecting trends towards a topic, and on determining the outof-context (a priori) strength of association of Twitter terms with positive sentiment. SemEval-2016 Task 4 (Nakov et al., 2016a) dropped the phrase-level subtask, and focused on sentiment with respect to a topic. It further introduced a 5-point scale, which is used for human review ratings on popular websites such as Amazon, Trip"
W16-0427,S15-2078,1,0.876036,"Missing"
W16-0427,S15-2077,0,0.0133799,"Advisor, Yelp, etc.; from a research perspective, this meant moving from classification to ordinal regression. Moreover, it focused on quantification, i.e., determining what proportion of a set of tweets on a given topic are positive/negative about it. It also featured a 5-point scale ordinal quantification subtask (Gao and Sebastiani, 2015). Other related (mostly non-Twitter) tasks have explored aspect-based sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016), sentiment analysis of figurative language on Twitter (Ghosh et al., 2015), implicit event polarity (Russo et al., 2015), stance in tweets (Mohammad et al., 2016), out-of-context sentiment intensity of phrases (Kiritchenko et al., 2016), and emotion detection (Strapparava and Mihalcea, 2007). Some of these tasks featured languages other than English. We expect the quest for more interesting formulations of the general sentiment analysis task to continue. We see SemEval as the engine of this innovation, as it not only does head-to-head comparisons, but also creates databases and tools that enable follow-up research for many years afterwards. Proceedings of NAACL-HLT 2016, pages 171–172, c San Diego, California,"
W16-0427,S07-1013,0,0.0596631,"ermining what proportion of a set of tweets on a given topic are positive/negative about it. It also featured a 5-point scale ordinal quantification subtask (Gao and Sebastiani, 2015). Other related (mostly non-Twitter) tasks have explored aspect-based sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015; Pontiki et al., 2016), sentiment analysis of figurative language on Twitter (Ghosh et al., 2015), implicit event polarity (Russo et al., 2015), stance in tweets (Mohammad et al., 2016), out-of-context sentiment intensity of phrases (Kiritchenko et al., 2016), and emotion detection (Strapparava and Mihalcea, 2007). Some of these tasks featured languages other than English. We expect the quest for more interesting formulations of the general sentiment analysis task to continue. We see SemEval as the engine of this innovation, as it not only does head-to-head comparisons, but also creates databases and tools that enable follow-up research for many years afterwards. Proceedings of NAACL-HLT 2016, pages 171–172, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics References Wei Gao and Fabrizio Sebastiani. 2015. Tweet sentiment: From classification to quantification. I"
W16-0427,S15-2080,0,\N,Missing
W16-0427,S16-1004,0,\N,Missing
W16-2345,D12-1133,0,0.253709,"set of the provided training data that has well-defined document boundaries in order to allow for meaningful extraction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddi"
W16-2345,W16-2348,0,0.0249438,"urce word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER class. The difference between the primary and contrastive systems is small. In the primary system, the feature val"
W16-2345,P06-1005,0,0.150956,"res based on the target-language model estimates provided by the baseline system, linguistic features concerning the source word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER clas"
W16-2345,2012.eamt-1.60,1,0.85967,"predicting all of the other pronouns, the system relied solely on the scores coming from the proposed PLM model. This target-side PLM model uses a large target-language training dataset to learn a probabilistic relation between each target pronoun and the distribution of the gender-number of its preceding nouns and pronouns. For prediction, given each source pronoun “it” or “they”, the system uses the PLM to score all possible candidates and to select the one with the highest score. In addition to the PoS-tagged lemmatised data that was provided for the shared task, the WIT3 parallel corpus (Cettolo et al., 2012), provided as part of the training data at the DiscoMT 2015 workshop, was used to train the PLM model. Furthermore, a French PoS-tagger, Morfette (Chrupala et al., 2008), was employed for gendernumber extraction. Before extracting the examples as feature vectors, the data is linguistically preprocessed usˇ ing the Treex framework (Popel and Zabokrtsk´ y, 2010). The source-language texts undergo a thorough analysis and are enriched with PoS tags, dependency syntax, as well as semantic roles and coreference for English. On the other hand, only grammatical genders are assigned to nouns in the tar"
W16-2345,chrupala-etal-2008-learning,0,0.0898214,"Missing"
W16-2345,W16-2350,1,0.838182,"on. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trai"
W16-2345,W11-2123,0,0.0192239,"e classifier is trained on a combination of semantic, based on lexical resources such as VerbNet (Schuler, 2005) and WordNet (Miller, 1995), and frequencies computed over the annotated Gigaword corpus (Napoles et al., 2012), syntactic, from the dependency parser in the Mate tools (Bohnet et al., 2013), and contextual features. The event classification results are modest, reaching only 54.2 F-score for the event class. The translation model, into which the classifier is integrated, is a 6-gram language model computed over target lemmata using modified KneserNey smoothing and the KenLM toolkit (Heafield, 2011). In addition to the pure target lemma context, it also has access to the identity of the sourcelanguage pronoun, used as a concatenated label to each REPLACE item. This provides information about the number marking of the pronouns in the source, and also allows for the incorporation of the output of the ‘it’-label classifier. To predict classes for an unseen test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels p"
W16-2345,W16-2349,0,0.0373816,"sing the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun translation decisions. The model performs reasonably well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features"
W16-2345,W10-1737,0,0.434398,"Missing"
W16-2345,guillou-etal-2014-parcor,1,0.910739,"the fact that all talks are originally given in English, which means that French–English translation is in reality a back-translation. • she: feminine singular subject pronoun; 3 1 We explain below in Section 3.3.3 how non-subject pronouns are filtered out from the data. 528 TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. As shown in analysis presented by Guillou et al. (2014), TED talks differ from other text types with respect to pronoun usage. TED speakers frequently use first- and second-person pronouns (singular and plural): first-person to refer to themselves and their colleagues or to themselves and the audience, second-person to refer to the audience, the larger set of viewers, or people in general. TED speakers often use the pronoun “they” without a specific textual antecedent, in sentences such as “This is what they think.” They also use deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audie"
W16-2345,W16-2351,1,0.900928,"Missing"
W16-2345,E12-3001,1,0.880326,"it is required by syntax to fill the subject position. An event reference pronoun may refer to a verb phrase (VP), a clause, an entire sentence, or a longer passage of text. Examples of each of these pronoun functions are provided in Figure 1. It is clear that instances of the English pronoun “it” belonging to each of these functions would have different translation requirements in French and German. Introduction Pronoun translation poses a problem for current state-of-the-art Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). 525 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 525–542, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 The problem of pronouns in machine translation has long been studied. In particular, for SMT systems, the recent previous studies cited above have focused on the translation of anaphoric pronouns. In this case, a well-known constraint of languages with grammatical gender is that agreement must hold between an anaphoric pronoun and the NP with which it corefers, called its antecede"
W16-2345,W16-2352,1,0.881771,"Missing"
W16-2345,W16-2353,0,0.0435664,"s useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed sequences of these embeddings within a certain window to the left and to the right of the target pronoun. The window size used by the system is 50 tokens or until the end of the sentence boundary. All of these inputs are read"
W16-2345,2010.iwslt-papers.10,1,0.888921,"Missing"
W16-2345,D13-1037,1,0.883273,"3 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trained 6gram language model identical to the contrastive system of the UUPPSALA submission described above. The"
W16-2345,S16-1001,1,0.795211,"d 69.76 in macro-averaged recall. This is very much above the performance of baseline0 and baseline-1.5, which are in the low-mid 40s. It is also well above the majority/random baseline (not shown) at 11.11, which is outperformed by far by all systems. Note that the top-3 systems in terms of macro-averaged recall are also the top-3 in terms of accuracy, but in different order. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we adopted macro-averaged recall, which was also recently adopted by some other competitions, e.g., by SemEval-2016 Task 4 (Nakov et al., 2016). Moreover, as in 2015, we also report accuracy as a secondary evaluation measure. Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. 8 If the test data did not have any instances of some of the classes, we excluded these classes from the macro-a"
W16-2345,W15-2501,1,0.657407,"ould replace a placeholder value (represented by the token REPLACE) in the target-language text. It requires no specific Machine Translation (MT) expertise and is interesting as a machine learning task in its own right. Within the context of SMT, one could think of the task of cross-lingual pronoun prediction as a component of an SMT system. This component may take the form of a decoder feature or it may be used to provide “corrected” pronoun translations in a post-editing scenario. The design of the WMT 2016 shared task has been influenced by the design and the results of a 2015 shared task (Hardmeier et al., 2015) organised at the EMNLP workshop on Discourse in MT (DiscoMT). The first intuition about evaluating pronoun translation is to require participants to submit MT systems — possibly with specific strategies for pronoun translation — and to estimate the correctness of the pronouns they output. This estimation, however, cannot be performed with full reliability only by comparing pronouns across candidate and reference translations because this would miss the legitimate variation of certain pronouns, as well as variations in gender or number of the antecedent itself. Human judges are thus required f"
W16-2345,W16-2354,0,0.0469939,"Missing"
W16-2345,H05-1108,0,0.0601982,"Missing"
W16-2345,W14-3334,1,0.800608,"he OTHER class. For the DiscoMT 2015 shared task, we explored this issue for English–French and found that GIZA++ model 4 and HMM with grow-diag-final-and symmetrisation gave the best results. For pronoun– pronoun links, we had an F-score of 0.96, with perfect recall and precision of 0.93 (Hardmeier et al., 2015). This was slightly higher than for other links, which had an F-score of 0.92. For German–English, we explored this issue this year since it is a new language pair. We used an aligned gold standard of 987 sentences from (Pad´o and Lapata, 2005), which has been extensively evaluated by Stymne et al. (2014). We used the same methodology as in 2015, and performed an evaluation on the subset of links between the pronouns we are interested in. We report precision and recall of links both for the pronoun subset and for all links, shown in Table 4. The alignment quality is considerably worse than for French–English both for all links and for pronouns, but again the results for pronouns is better than for all links in both precision and recall. 6 https://github.com/slavpetrov/ universal-pos-tags 530 Alignment Symmetrisation Model 4 fast-align gdfa HMM gd gdf ∪ ∩ All links P R Pronouns P R .75 .69 .80"
W16-2345,W16-2355,1,0.832701,"the test dataset is imbalanced. Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset. In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.). Standard F1 and macro-averaged F1 are also sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail. The UU-S TYMNE systems (Stymne, 2016) use linear SVM classifiers for all language pairs. A number of different features were explored, but anaphora is not explicitly modelled. The features used can be grouped in the following way: source pronouns, local context words/lemmata, preceding nouns, target PoS n-grams with two different PoS tag-sets, dependency heads of pronouns, target LM scores, alignments, and pronoun position. A joint tagger and dependency parser on the source text is used for some of the features. The primary system is a 2-step classifier where a binary classifier is first used to distinguish between the OTHER clas"
W16-2345,petrov-etal-2012-universal,0,0.0937891,"Missing"
W16-2345,W16-2356,1,0.48149,"networks, except for the embedding for the aligned pronoun. All outputs of the recurrent layers are concatenated to a single vector along with the embedding of the aligned pronoun. This vector is then used to make the pronoun prediction by a dense neural network layer. The primary systems are trained to optimise macro-averaged recall and the contrastive systems are optimised without preference towards rare classes. The system is trained only on the shared task data and all parts of the data, in-domain and out-of-domain, are used for training the system. 5.5 5.6 UHELSINKI The UHELSINKI system (Tiedemann, 2016) implements a simple linear classifier based on LibSVM with its L2-loss SVC dual solver. The system applies local source-language and target-language context using the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun tra"
W16-2345,W16-2357,0,0.0259257,"well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features: tokens and their PoS tags are extracted from a context window around source- and targetside pronouns. N -gram combinations of these features are included by concatenating adjacent tokens or PoS tags. Furthermore, the pleonastic use of a pronoun is detected with NADA (Bergsma and Yarowsky, 2011) on the source side. 534 This CRF approach has been applied only to German, but there are plans to extend it to other languages. This indicates that the NN mechanism is quite effective. Th"
W16-2345,sagot-2010-lefff,0,0.0184156,"traction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed seq"
W16-2345,schmid-etal-2004-smor,0,0.0349386,"test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the pr"
W16-2345,W12-3018,0,\N,Missing
W16-2345,2015.iwslt-evaluation.1,1,\N,Missing
W16-2345,W14-6111,0,\N,Missing
W16-4801,W16-4821,0,0.0339271,"Missing"
W16-4801,W16-4826,0,0.0502832,"Missing"
W16-4801,W16-4827,0,0.11257,"Missing"
W16-4801,W16-4819,0,0.0808398,"Missing"
W16-4801,W16-4816,0,0.0637155,"Missing"
W16-4801,W15-5410,0,0.238809,"Missing"
W16-4801,W16-4802,0,0.114498,"Missing"
W16-4801,W16-4831,0,0.0913939,"Missing"
W16-4801,W16-4830,0,0.0438028,"Missing"
W16-4801,D14-1154,0,0.0630846,"otivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification"
W16-4801,W16-4828,0,0.0387334,"Missing"
W16-4801,W15-5409,0,0.149587,"Missing"
W16-4801,W16-4829,0,0.0511602,"Missing"
W16-4801,W15-5403,0,0.27462,"Missing"
W16-4801,W16-4822,0,0.0759257,"Missing"
W16-4801,W15-5413,0,0.562658,"Missing"
W16-4801,W16-4823,0,0.0418563,"Missing"
W16-4801,W14-5316,0,0.385305,"Missing"
W16-4801,L16-1284,1,0.849869,"Missing"
W16-4801,W16-4824,0,0.0430542,"Missing"
W16-4801,W16-4817,0,0.0342647,"Missing"
W16-4801,W16-4815,0,0.0384942,"Missing"
W16-4801,Y08-1042,0,0.0574334,"ticipants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic"
W16-4801,W16-4818,0,0.128184,"Missing"
W16-4801,J16-3005,0,0.112984,"Missing"
W16-4801,W15-5408,0,0.217783,"Missing"
W16-4801,W16-4820,0,0.38437,"Missing"
W16-4801,W14-5317,0,0.0630949,"Missing"
W16-4801,U13-1003,0,0.256503,"size and scope featuring two subtasks and attracting a record number of participants. Below we present the task setup, the evaluation results, and a brief discussion about the features and learning methods that worked best. More detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natur"
W16-4801,W14-5315,0,0.0614151,"Missing"
W16-4801,W15-5407,1,0.493189,"detail about each particular system can be found in the corresponding system description paper, as cited in this report. 2 Related Work Language and dialect identification have attracted a lot of research attention in recent years, covering a number of similar languages and language varieties such as South-Slavic languages (Ljubeˇsi´c et al., 2007), English varieties (Lui and Cook, 2013), varieties of Mandarin in China, Taiwan and Singapore (Huang and Lee, 2008), Malay vs. Indonesian (Ranaivo-Malanc¸on, 2006), Brazilian vs. European Portuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods incl"
W16-4801,W16-4814,1,0.85912,"Missing"
W16-4801,W16-4825,0,0.0362109,"Missing"
W16-4801,W14-5314,0,0.0924571,"Missing"
W16-4801,W15-3205,0,0.0361398,"ortuguese (Zampieri and Gebre, 2012), and Persian vs. Dari (Malmasi and Dras, 2015a), to mention just a few. The interest in this aspect of language identification has motivated the organization of shared tasks such as the DSL challenge, which allowed researchers to compare various approaches using the same dataset. Along with the interest in similar languages and language variety identification, we observed substantial interest in applying natural language processing (NLP) methods for the processing of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with met"
W16-4801,W14-5313,0,0.115089,"of dialectal Arabic with special interest in methods to discriminate between Arabic dialects. Shoufan and Al-Ameri (2015) presented a comprehensive survey on these methods including recent studies on Arabic dialect identification such as (Elfardy and Diab, 2014; Darwish et al., 2014; Zaidan and Callison-Burch, 2014; This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://creativecommons.org/licenses/by/4.0/ 1 Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–14, Osaka, Japan, December 12 2016. Tillmann et al., 2014; Malmasi and Dras, 2015a). Methods for Arabic dialect detection present significant overlap with methods proposed for similar language identification. For this reason, in the 2016 edition of the DSL challenge we offered a subtask on Arabic dialect identification. Below, we discuss some related shared tasks including the first two editions of the DSL challenge. 2.1 Related Shared Tasks Several shared tasks related to the DSL task have been organized in recent years. Two examples are the ALTW language identification shared task (Baldwin and Lui, 2010) on general-purpose language identification,"
W16-4801,P11-1122,0,0.0270219,"Missing"
W16-4801,W14-5307,1,0.744553,"Missing"
W16-4801,W15-5411,1,0.882098,"Missing"
W17-1201,W16-4802,0,0.127115,"character ngrams and a Na”ive Bayes classifier. The system followed the work of the system submitted to the DSL 2016 by Barbaresi (2016). • CECL: The system uses a two-step approach as in (Goutte et al., 2014). The first step identifies the language group using an SVM classifier with a linear kernel trained on character n-grams (1-4) that occur at least 100 times in the dataset weighted by Okapi BM25 (Robertson et al., 1995). The second step discriminates between each language within the group using a set of SVM classifiers trained • tubasfs: Following the success of tubasfs at DSL 2016 (C¸o¨ ltekin and Rama, 2016), which was ranked first in the closed training track, this year’s tubasfs submission used a linear SVM classifier. The system used both characters and words as features, and carefully optimized hyperparameters: n-gram size and margin/regularization parameter for SVM. 5 In 2016 ADI and DSL were organized under the name DSL shared task, and ADI was run as a sub-task. 4 • gauge: This team submitted a total of three runs. Run 1 used an SVM classifier with character n-grams (2–6), run 2 (their best run) used logistic regression trained using character n-grams (1–6), and run 3 used hard voting of t"
W17-1201,W17-1221,0,0.532877,"of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent pu"
W17-1201,W17-1215,0,0.0474398,"Missing"
W17-1201,W17-1213,0,0.0702486,"pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar langua"
W17-1201,W13-1728,1,0.0339111,"rovided lexical features. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL"
W17-1201,W17-1217,0,0.0300428,"Missing"
W17-1201,W15-5413,0,0.101469,"Missing"
W17-1201,W13-1712,0,0.199423,"Missing"
W17-1201,W14-5316,0,0.160565,"Missing"
W17-1201,U13-1003,0,0.160089,"est set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we looked at how systems perform on discriminating between similar languages and language varieties across different domains, an aspect highlighted by Lui and Cook (2013) and Lui (2014). For this purpose, we provided an out-of-domain test set containing manually annotated microblog posts written in Bosnian, Croatian, Serbian, Brazilian and European Portuguese. 2.1 2.2 Dataset The DSLCC v4.04 contains 22,000 short excerpts of news texts for each language or language variety divided into 20,000 texts for training (18,000 texts) and development (2,000 texts), and 2,000 texts for testing. It contains a total of 8.6 million tokens for training and over half a million tokens for testing. The fourteen languages included in the v4.0 grouped by similarity are Bosnian,"
W17-1201,L16-1284,1,0.900242,"Missing"
W17-1201,W16-3928,0,0.0176574,"ranging from 3 for CLP to 11 for DSL. Below we describe the individual tasks. 2 Discriminating between Similar Languages (DSL) Discriminating between similar languages is one of the main challenges faced by language identification systems. Since 2014 the DSL shared task has been organized every year providing scholars and developers with an opportunity to evaluate language identification methods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the"
W17-1201,W15-5407,1,0.88226,"ods using a standard dataset and evaluation methodology. Albeit related to other shared tasks such as the 2014 TweetLID challenge (Zubiaga et al., 2014) and the 2016 shared task on Geolocation Prediction (Han et al., 2016), the DSL shared task continues to be the only shared task focusing on the discrimination between similar languages and language varieties. 1 The MAZA team submitted two separate papers: one for each task they participated in. 2 This number does not include the submissions to the Arabic Dialect Identification subtask of DSL in 2016. 2 At DSL 2015, the four best systems, MAC (Malmasi and Dras, 2015b), MMS (Zampieri et al., 2015a), NRC (Goutte and L´eger, 2015), and SUKI (Jauhiainen et al., 2015) performed similarly on test set B compared to test set A: in the closed training setting, where the systems were trained only using the training data provided by the DSL organizers, their accuracy dropped from 95.54 to 94.01, from 95.24 to 92.78, from 95.24 to 93.01, and from 94.67 to 93.02, respectively.3 Finally, inspired by recent work on language identification of user-generated content (Ljubeˇsi´c and Kranjˇci´c, 2015; Abainia et al., 2016), in the DSL 2016 task (Malmasi et al., 2016), we l"
W17-1201,W17-1222,1,0.800029,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W17-1211,0,0.333885,"Danish, and Norwegian (TL) – Swedish (SL). Note that the latter two pairs include a triple of related languages. 1 Proceedings of the Fourth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the r"
W17-1201,W17-1220,1,0.878577,"milar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al.,"
W17-1201,W16-4801,1,0.679876,"Missing"
W17-1201,W17-1225,0,0.428199,"us Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to the DSL challenge grow from 8 in 2014 to 10 in 2015 and then to 17 in 2016.2 The 2015 and the 2016 editions of the DSL"
W17-1201,W13-1714,0,0.148384,"ures. This year, we added a multi-model aspect to the task by further providing acoustic features. The system description paper of CECL (Bestgen, 2017) provides some interesting insights about the DSL task. First, they found out that BM25 weighting, which was previously applied to native language identification (NLI) (Wang et al., 2016), worked better than using TF.IDF. They further highlighted the similarity between similar language identification and NLI as evidenced by a number of entries in the DSL task that are adaptations of systems used for NLI (Goutte et al., 2013; Gebre et al., 2013; Jarvis et al., 2013). We observe that the variation in performance among the top ten teams is less than four percentage points. The team ranked last (eleventh) approached the task using LSTM and achieved an F1 score of 0.202. Unfortunately, they did not submit a system description paper, and thus we do not have much detail about their system. However, in the DSL 2016 task (Malmasi et al., 2016), neural network-based approaches already proved not to be very competitive for the task. See (Medvedeva et al., 2017) for a comparison between the performance of an SVM and an RNN approach for the DSL task. 2.5 Arabic Dial"
W17-1201,W17-1219,0,0.132166,"cia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and Gall´e, 2016; Castro et al., 2016). We also saw the number of system submissions to th"
W17-1201,W15-5408,0,0.243101,"Missing"
W17-1201,W16-4820,0,0.425079,"Missing"
W17-1201,W17-1212,0,0.183999,"Missing"
W17-1201,W17-1226,0,0.0868204,"Missing"
W17-1201,L16-1641,1,0.367123,"Missing"
W17-1201,N15-1010,0,0.0146702,"Missing"
W17-1201,W15-3040,0,0.0163982,"ams (1–6), and run 3 used hard voting of three systems: SVM, Logistic Regression, and Na”ive Bayes and character ngrams (2–6) as features. • bayesline: This team participated with a Multinomial Na¨ıve Bayes (MNB) classifier similar to that of Tan et al. (2014), with no special parameter tuning, as this system was initially intended to serve as an intelligent baseline for the task (but now it has matured into a competitive system). In their bestperforming run 1, they relied primarily on character 4-grams as features. The feature sets they used were selected by a search strategy as proposed in (Scarton et al., 2015). • cic ualg: This team submitted three runs. Runs 1 and 2 first predict the language group, and then discriminate between the languages within that group. The first step uses an SVM classifier with a combination of character 3– 5-grams, typed character 3-grams, applying the character n-gram categories introduced by Sapkota et al. (2015), and word unigrams using TF-weighting. The second step uses the same features and different classifiers: SVMs + Multinominal Na¨ıve Bayes (MNB) in run 1, and MNB in run 2 (which works best). Run 3 uses a single MNB classifier to discriminate between all fourte"
W17-1201,D10-1112,1,0.910362,"CN i-vector (as in Run 2) with (ii) an SVM model trained on count bag of characters 2–4-grams, which yielded an F1 of 0.612. This year, we introduced a new dialectal area, which focused on German dialects of Switzerland. Indeed, the German-speaking part of Switzerland is characterized by the widespread use of dialects in everyday communication, and by a large number of different dialects and dialectal areas. There have been two major approaches to Swiss German dialect identification in the literature. The corpus-based approach predicts the dialect of any text fragment extracted from a corpus (Scherrer and Rambow, 2010; Hollenstein and Aepli, 2015). The dialectological approach tries to identify a small set of distinguishing dialectal features, which are then elicited interactively from the user in order to identify his or her dialect (Leemann et al., 2016). In this task, we adopt a corpus-based approach, and we develop a new dataset for this. • deepCybErNet: This team submitted two runs. Run 1 adopted a Bi-LSTM architecture using the lexical features, and achieved an F1 score of 0.208, while run 2 used the i-vector features and achieved an F1 of 0.574. 3.3 Results Table 5 shows the evaluation results for t"
W17-1201,W14-5307,1,0.307051,"Missing"
W17-1201,W15-5411,1,0.900323,"Missing"
W17-1201,L16-1680,0,0.0188911,"Missing"
W17-1201,N12-1052,0,0.0102303,"Missing"
W17-1201,W14-1614,1,0.904663,"Missing"
W17-1201,tiedemann-2012-parallel,1,0.0255189,"LP task: parallel training data. Participants were asked not to use the development data with their gold standard annotation of dependency relations for any training purposes. The purpose of the development datasets is entirely for testing model performance during system development. All the knowledge used for parsing should origin in the provided source language data. Other sources (except for target language sources) could also be used in unconstrained submissions, but none of the participants chose that option. For the constrained setup, we also provided parallel datasets coming from OPUS (Tiedemann, 2012) that could be used for training cross-lingual parsers in any way. The datasets included translated movie subtitles and contained quite a bit of noise in terms of alignment, encoding, and translation quality. They were also from a very different domain, which made the setup quite realistic considering that one would used whatever could be found for the task. The sizes of the parallel datasets are given in Table 8. In the setup of the shared task, we also provided simple baselines and an “upper bound” of a model trained on annotated target language data. The cross-lingual baselines included del"
W17-1201,C14-1175,1,0.927054,"nd without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in the literature in particular in connection with dependency parsing (Hwa et al., 2005; McDonald et al., 2013; T¨ackstr¨om et al., 2012; Tiedemann, 2014). The motivation for cross-lingual models is the attempt to bootstrap tools for languages that do not have annotated resources, which are typically necessary for supervised data-driven techniques, using data and resources from other languages. This is especially successful for closely related languages with similar syntactic structures and strong lexical overlap (Agi´c et al., 2012). With this background, it is a natural extension for our shared task to consider cross-lingual parsing as well. We do so by simulating the resource-poor situation by selecting language pairs from the Universal Depe"
W17-1201,W15-2137,1,0.514813,"ad of around 0.7. 4.5 Summary This first edition of the GDI task was a success, given the short time between the 2016 and 2017 editions. In the future, we would like to better control transcriber effects, either by a more thorough selection of training and test data, or by adding transcriber-independent features such as acoustic features, as has been done in the ADI task this year. Further dialectal areas could also be added. 10 5 Cross-lingual Dependency Parsing (CLP) Avoiding gold labels is important here in order to avoid exaggerated results that blur the picture of a more realistic setup (Tiedemann, 2015). The tagger models are trained on the original target language treebanks using UDpipe (Straka et al., 2016) with standard settings and without any optimization of the hyper parameters. The size of the source language data is given in Table 5. We can see that for Czech we have by far the largest corpus, which will also be reflected in the results we obtain. VarDial 2017 featured for the first time a crosslingual parsing task for closely related languages.7 Transfer learning and annotation projection are popular approaches in this field and various techniques and models have been proposed in th"
W17-1201,W17-1216,1,0.921592,"shop on NLP for Similar Languages, Varieties and Dialects, pages 1–15, c Valencia, Spain, April 3, 2017. 2017 Association for Computational Linguistics Team ahaqst bayesline CECL cic ualg Citius Ixa Imaxin CLUZH CUNI deepCybErNet gauge Helsinki-CLP MAZA (ADI) MAZA (GDI) mm lct qcri mit SUKI timeflow tubasfs unibuckernel XAC Bayesline Total DSL X X X X X X ADI X X X X X 11 CLP X X X X X X GDI X X X X X X X X X X 10 6 X X X 3 System Description Paper (Hanani et al., 2017) – (Bestgen, 2017) (G´omez-Adorno et al., 2017) (Gamallo et al., 2017) (Clematide and Makarov, 2017) (Rosa et al., 2017) – – (Tiedemann, 2017) (Malmasi and Zampieri, 2017a) (Malmasi and Zampieri, 2017b) (Medvedeva et al., 2017) – (Jauhiainen et al., 2017) (Criscuolo and Aluisio, 2017) (C ¸ o¨ ltekin and Rama, 2017) (Ionescu and Butnaru, 2017) (Barbaresi, 2017) 15 Table 1: The teams that participated in the VarDial’2017 Evaluation Campaign. 1.2 Participating Teams The fourth edition of the DSL shared task was motivated by the success of the previous editions and by the growing interest of the research community in the identification of dialects and similar languages, as evidenced by recent publications (Xu et al., 2016; Radford and G"
W17-4801,D12-1133,0,0.165034,"16 (Guillou et al., 2016), but the differences in the resulting evaluation scores are actually minor. As we have explained above, the shared task focused primarily on subject pronouns. However, in English and German, some pronouns are ambiguous between subject and object position, e.g., the English it and the German es and sie. In order to address this issue, in 2016 we introduced filtering of object pronouns based on dependency parsing. This filtering removed all pronoun instances that did not have a subject dependency label.6 For joint dependency parsing and POS-tagging, we used Mate Tools (Bohnet and Nivre, 2012), with default models. Since in 2016 we found that this filtering was very accurate, this year we performed only automatic filtering for the training and the development, and also for the test datasets. Note that since only subject pronouns can be realized as prodropped pronouns in Spanish, subject filtering was not necessary. 4 Baseline Systems The baseline system is based on an n-gram language model (LM). The architecture is the same as that used for the WMT 2016 cross-lingual pronoun prediction task.7 In 2016, most systems outperformed this baseline, and for the sake of comparison, we thoug"
W17-4801,W16-2350,1,0.857196,"Missing"
W17-4801,W17-4807,1,0.863641,"Missing"
W17-4801,2010.iwslt-papers.10,1,0.907647,"is is hard as selecting the correct pronoun may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-au"
W17-4801,W15-2501,1,0.855394,"gradually raised interest in the research community for a shared task that would allow to compare various competing proposals and to quantify the extent to which they improve the translation of different pronouns for different language pairs and different translation directions. However, evaluating pronoun translation comes with its own challenges, as reference-based evaluation, which is standard for machine translation in general, cannot easily take into account legitimate variations of translated pronouns or their placement in the sentence. Thus, building upon experience from DiscoMT 2015 (Hardmeier et al., 2015) and WMT 2016 (Guillou et al., 2016), this year’s cross-lingual pronoun prediction shared task has been designed to test the capacity of the participating systems for translating pronouns correctly, in a framework that allows for objective evaluation, as we will explain below. 2 ce OTHER ce|PRON qui|PRON It ’s an idiotic debate . It has to stop . REPLACE 0 eˆ tre|VER un|DET d´ebat|NOM idiot|ADJ REPLACE 6 devoir|VER stopper|VER .|. 0-0 1-1 2-2 3-4 4-3 6-5 7-6 8-6 9-7 10-8 Figure 2: English→French example from the development dataset. First come the gold class labels, followed by the pronouns (t"
W17-4801,N13-1073,0,0.0460377,"raka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED talks, one for each subtask. However, we chose the current setup as using a smaller set of documents r"
W17-4801,W17-4806,0,0.0618776,"ial anaphora) or in different sentences (inter-sentential anaphora). Most MT systems translate sentences in isolation, and thus inter-sentential anaphoric pronouns will be translated without knowledge of their antecedent, and thus pronoun-antecedent agreement cannot be guaranteed. NMT yields generally higher-quality translation, but is harder to analyze, and thus little is known about how well it handles pronoun translation. Yet, it is clear that it has access to larger context compared to phrase-based SMT models, potentially spanning multiple sentences, which can improve pronoun translation (Jean et al., 2017a). Motivated by these challenges, the DiscoMT 2017 workshop on Discourse in Machine Translation offered a shared task on cross-lingual pronoun prediction. This was a classification task, asking the participants to make predictions about which pronoun should replace a placeholder in the target-language text. The task required no MT expertise and was designed to be interesting as a machine learning task on its own right, e.g., for researchers working on co-reference resolution. Source Target POS tags Reference The above constraints start playing a role in pronoun translation in situations where"
W17-4801,E12-3001,0,0.0230275,"may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the sou"
W17-4801,2005.mtsummit-papers.11,0,0.0869283,"teresting research challenges from the perspective of both speech recognition and machine translation. Therefore, both research communities are making increased use of them in building benchmarks. TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. 3.1.2 Europarl and News For training purposes, in addition to TED talks, we further made available the Europarl3 (Koehn, 2005) and News Commentary4 corpora for all language pairs but Spanish-English, for which only TED talks and Europarl were available. We used the alignments provided by OPUS, including the document boundaries from the original sources. For Europarl, we used ver. 7 of the data release, and for News Commentary we used ver. 9. 3.2 Test Set Selection We selected the test data from talks added recently to the TED repository such that: 1. The talks have been transcribed (in English) and translated into both German and French. 2. They were not used in the IWSLT evaluation campaigns, nor in the DiscoMT 2015"
W17-4801,2005.iwslt-1.8,0,0.121419,"OS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English dir"
W17-4801,W10-1737,0,0.13529,"Missing"
W17-4801,J03-1002,0,0.0103978,"rted the TreeTagger’s POS tags to the target coarse POS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction,"
W17-4801,P14-2050,0,0.026726,"that is aligned to the pronoun to be predicted. All input sequences are fed in an embedding layer followed by two layers of GRUs. The values in the last layer form a vector, which is further concatenated to the pronoun alignment embeddings, to form a larger vector, which is then used to make the final prediction using a dense neural network. The pretraining is a modification of the skip-gram model of WORD 2 VEC (Mikolov et al., 2013), in which along with the skip-gram token context, all target sentence pronouns are predicted as well. The process of pretraining is performed using WORD 2 VECF (Levy and Goldberg, 2014). 5.2 NYU The NYU system (Jean et al., 2017b) uses an attention-based neural machine translation model and three variants that incorporate information from the preceding source sentence. The sentence is added as an auxiliary input using additional encoder and attention models. The systems are not specifically designed for pronoun prediction and may be used to generate complete sentence translations. They are trained exclusively on the data provided for the task, using the text only and ignoring the provided POS tags and alignments. 5.4 UU-Hardmeier The UU- HARDMEIER system (Hardmeier, 2017) is"
W17-4801,petrov-etal-2012-universal,0,0.0455296,"Missing"
W17-4801,W16-2351,1,0.891541,"Missing"
W17-4801,S17-2088,1,0.872336,"Missing"
W17-4801,W16-2202,0,0.0276957,"Missing"
W17-4801,D15-1166,0,0.0502002,"nt. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the targetlanguage lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of cl"
W17-4801,W16-2353,0,0.0406223,"/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a different tag: EP. Still, we decided to use the same filtering this year, to keep the task sta"
W17-4801,L16-1680,0,0.054481,"Missing"
W17-4801,W17-4808,0,0.0205662,"mata constructed from news texts, parliament debates, and the TED talks of the training/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a differe"
W17-4801,W16-2355,1,0.796565,"the data is used in each epoch. For the primary system, all classes are sampled equally, as long as there are enough instances for each class. Although this sampling method biases the system towards macro-averaged recall, on the test data the system performed very well in terms of both macro-averaged recall and accuracy. The secondary system uses a sampling method in which the samples are proportional to the class distribution in the development dataset. 5.5 UU-Stymne16 The UU-S TYMNE 16 system uses linear SVM classifiers, and it is the same system that was submitted for the 2016 shared task (Stymne, 2016). It is based mainly on local features, and anaphora is not explicitly modeled. The features used include source pronouns, local context words/lemmata, target POS n-grams with two different POS tagsets, dependency heads of pronouns, alignments, and position of the pronoun. A joint tagger and dependency parser (Bohnet and Nivre, 2012) is used on the source text in order to produce some of the features. Overall, the source pronouns, the local context and the dependency features performed best across all language pairs. 8 7 Stymne (2016) describes several variations of the method, including both"
W17-4801,W17-4805,1,0.882705,"Missing"
W17-4801,S16-1001,1,0.80372,"erforming system here is T URKU NLP with a macro-averaged recall of 58.82. However, it is nearly tied with U PPSALA, and both are somewhat close to NYU. Noteworthy, though, is that the highest-scoring system on macro-average recall is the contrastive system of NYU; NYU also has the second-best accuracy, outperformed only by U PPSALA. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we followed the setup of 2016, where we switched to macroaveraged recall, which was also recently adopted by some other competitions, e.g., by SemEval2016/2017 Task 4 (Nakov et al., 2016; Rosenthal et al., 2017). Moreover, as in 2015 and 2016, we also report accuracy as a secondary evaluation measure (but we abandon F1 altogether). Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. The advantage of macro-averaged recall over acc"
W17-4801,C96-2141,0,0.453266,"c information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED t"
W18-3901,W18-3913,0,0.217545,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3919,0,0.254393,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W18-3932,0,0.129693,"per. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants"
W18-3901,W17-1223,0,0.131933,"data to reduce the transcriber effects seen last year. 7 • The LaMa system is a blend (weighted vote) of eight classifiers being stochastic gradient descent (hinge and modified Huber), multinomial Na¨ıve Bayes, both counts and tf-idf, FastText, and modified Kneser-Ney smoothing. The classifiers were trained using word n-grams (1-6) and character n-grams (1-8). The hyperparameters were determined with cross-validation and searching on the development set. • XAC system is a refined version of the n-gram-based Bayesline system described in last year’s XAC submission to the VarDial shared tasks (Barbaresi, 2017), and previously used as a baseline for the DSL shared task (Tan et al., 2014). The XAC team achieved their best results using a Na¨ıve Bayes classifier. • The GDI classification system is based on an ensemble of multiple SVM classifiers. The system was trained on various word- and character-level features. • The dkosmajac system is based on a normalized Euclidean distance measure. The distances are calculated between a sample and each class profile. The class profiles are generated by selecting the most frequent features for each class, which results in profiles that are of the same length fo"
W18-3901,W18-3918,0,0.0590619,"Missing"
W18-3901,W18-3925,0,0.142653,"Missing"
W18-3901,W17-1214,0,0.138486,"was part of the first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions gen"
W18-3901,W18-3909,0,0.15091,"Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive evaluation exercise with four shared tasks in 2017. This year, the VarDial workshop featured the second edition of the VarDial evaluation campaign w"
W18-3901,W18-3933,1,0.889149,"Missing"
W18-3901,W18-3920,1,0.880795,"Missing"
W18-3901,W18-3926,0,0.0553879,"Missing"
W18-3901,W18-3921,0,0.054375,"Missing"
W18-3901,W17-1225,0,0.134424,"r of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layers, which represents the prob"
W18-3901,W16-4818,0,0.048944,", duration (Dur.), in number of hours. 5.2 Participants and Approaches In this section, we present a short description of the systems that competed in the ADI shared task: • UnibucKernel system (Butnaru and Ionescu, 2018) combines three kernel matrices: one calculated using just the lexical features, another one computed on embeddings, and a combined kernel computed on the phonetic features. The final matrix is the mean of these three matrices. As a classifier, they used Kernel Ridge Regression. The approach is similar to the systems that ranked second and first in the previous two ADI tasks (Ionescu and Popescu, 2016; Ionescu and Butnaru, 2017). • Safina system (Ali, 2018a) accepts a sequence of 256 characters as input in addition to the acoustic embedding vectors. First, the sequence of characters is one-hot encoded, then it is passed to a GRU layer, which is followed by a convolutional layer with different filter sizes ranging from 2 to 7. The convolutional layer is followed by batch normalizations, max-pooling, and dropout layers, and finally a softmax layer. In contrast, the acoustic embedding vectors go directly to another softmax layer. The final output is the average between these two softmax layer"
W18-3901,W18-3915,0,0.0733204,"Missing"
W18-3901,W18-3929,0,0.411409,"Missing"
W18-3901,W18-3907,0,0.178349,"Missing"
W18-3901,W18-3922,0,0.0589219,"Missing"
W18-3901,W18-3928,0,0.0556119,"Missing"
W18-3901,kumar-2012-challenges,1,0.824307,"ed printed stories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Osl"
W18-3901,kumar-2014-developing,1,0.832345,"ories, novels and essays in books, magazines, and newspapers. We scanned the printed materials, then we performed OCR, and finally we asked native speakers of the respective languages to correct the OCR output. Since there are no specific OCR models available for these languages, we used the Google OCR for Hindi, part of the Drive API. Since all the languages used the Devanagari script, we expected the OCR to work reasonably well, and overall it did. We further managed to get some blogs in Magahi and Bhojpuri. There are several corpora already available for Modern Standard Hindi (Kumar, 2012; Kumar, 2014a; Kumar, 2014b; Choudhary and Jha, 2011). However, in order to keep the domain the same as for the other languages, we collected data from blogs that mainly contain stories and novels. Thus, the Modern Standard Hindi data collected for this study is also from the literature domain.5 9.2 Participants and Approaches • The SUKI team used HeLI with adaptive language models based on character n-grams from 1 to 6, as previously described in Section 5. The also used an iterative version of the language model adaptation technique, with three additional adaptation epochs. ¨ • Tubingen-Oslo team submit"
W18-3901,W14-0405,1,0.912975,"Missing"
W18-3901,W18-3917,1,0.897664,"Missing"
W18-3901,L16-1242,1,0.902855,"Missing"
W18-3901,L16-1676,1,0.914666,"Missing"
W18-3901,W16-4814,1,0.86288,"-Oslo system (C ¸ o¨ ltekin et al., 2018) is trained on word and character n-grams using a single SVM classifier, which is fine-tuned using cross-validation. It is similar to the submissions by the same authors to previous VarDial shared tasks (C¸o¨ ltekin and Rama, 2017; C¸o¨ ltekin and Rama, 2016). They also tried an approach based on RNN, which worked worse. • Arabic Identification system is based on an ensemble of SVM classifiers trained on character and word n-grams. The approach is similar to the systems ranked second and first in the previous two ADI tasks (Malmasi and Zampieri, 2017a; Malmasi and Zampieri, 2016). 5 5.3 Results Six teams submitted runs for the ADI shared task and the results are shown in Table 3. The best result, an F1 score of 0.589, was achieved by UnibucKernel,1 followed by safina, with an F1 score of 0.575. The following three teams are tied for the third place as they are not statistically different. Rank 1 2 3 3 3 4 Team F1 (Macro) UnibucKernel safina BZU SYSTRAN T¨ubingen-Oslo Arabic Identification 0.589 0.576 0.534 0.529 0.514 0.500 Table 3: ADI results: ranked taking statistical significance into account. 5.4 Summary We introduced multi-phoneme representation for the dialecta"
W18-3901,W17-1222,1,0.833986,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W17-1220,1,0.80558,"e first VarDial evaluation campaign (Zampieri et al., 2017). It provided manual transcriptions of recorded interviews from four dialect areas of the German-speaking Switzerland, namely Bern, Basel, Lucerne, and Zurich. The training and the test data was extracted from the ArchiMob corpus (Samardˇzi´c et al., 2016). The training data consisted in 3,000–4,000 utterances from 3–5 different speakers per dialect; the test data consisted of about 900 utterances by a single speaker per dialect. A total of ten teams participated in the 2017 GDI task and the two best-performing systems (Bestgen, 2017; Malmasi and Zampieri, 2017b) achieved weighted F1-measure of up to 0.66. Transcribers were shown to affect the performance of the systems, e.g., for the Lucerne dialect, whose test set was transcribed by a different person than the training set, recall figures were only around 0.3. 5 Third Arabic Dialect Identification (ADI) This year’s third edition of the ADI task addressed the multi-dialectal challenge in spoken Arabic in the broadcast domain. Last year, in the second edition of the ADI task (Zampieri et al., 2017), we offered the input represented as (i) automatic text transcriptions generated using large-vocabular"
W18-3901,W16-4801,1,0.733081,"Missing"
W18-3901,W18-3927,0,0.0554913,"Missing"
W18-3901,W18-3914,0,0.175754,"ES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participated in the VarDial’2018 evaluation campaign. 4 Previous Shared Tasks Since the first DSL challenge, the shared tasks organized within the scope of the VarDial workshop have enjoyed substantial increase in the number of participants and in the overall interest from the NLP community. This motivated the organizers to turn the shared tasks at VarDial into a more comprehensive eva"
W18-3901,W18-3924,0,0.0833746,"and the number of submissions varied widely across the tasks, ranging from 6 entries for ADI and MTT to 12 entries for DFS. Table 1 lists the participating teams, the shared tasks they took part in, and a reference to the system description paper. Team ADI Arabic Identification benf BZU CLiPS CEA List DeepLIMA CoAStaL DFSlangid dkosmajac GDI classification ILIdentification JANES JSI LaMa LTL-UDE mmb lct safina STEVENDU2018 SUKI X SYSTRAN Taurus T¨ubingen-Oslo Twist Bytes Meta UH&CU UnibucKernel we are indian XAC Total X DFS GDI ILI MTT System Description Papers X X X X X X X X X X X X X X X (Naser and Hanani, 2018) (Kreutz and Daelemans, 2018) (Meftah and Semmar, 2018) (Ciobanu et al., 2018a) (Ciobanu et al., 2018b) (Ljubeˇsi´c, 2018) (Ljubeˇsi´c, 2018) X X X X X X X X X X X X X X X X X X X 6 X 12 X 8 X X 8 6 (Kroon et al., 2018) (Ali, 2018a; Ali, 2018b; Ali, 2018c) (Du and Wang, 2018) (Jauhiainen et al., 2018a; Jauhiainen et al., 2018b; Jauhiainen et al., 2018c) (Michon et al., 2018) (van Halteren and Oostdijk, 2018) (C ¸ o¨ ltekin et al., 2018) (Benites et al., 2018) (Silfverberg and Drobac, 2018) (Butnaru and Ionescu, 2018) (Gupta et al., 2018) (Barbaresi, 2018) 22 Table 1: The teams that participate"
W18-3901,L16-1641,1,0.509642,"Missing"
W18-3901,W17-1224,1,0.744067,"Missing"
W18-3901,W18-3923,1,0.879634,"Missing"
W18-3901,W14-5307,1,0.857122,"Missing"
W18-3901,W15-5401,1,0.855743,"Missing"
W18-3901,W17-1201,1,0.607978,"Missing"
