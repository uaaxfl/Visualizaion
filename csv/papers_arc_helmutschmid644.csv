2020.wmt-1.131,The {LMU} {M}unich System for the {WMT}20 Very Low Resource Supervised {MT} Task,2020,-1,-1,3,0,13977,jindvrich libovicky,Proceedings of the Fifth Conference on Machine Translation,0,"We present our systems for the WMT20 Very Low Resource MT Task for translation between German and Upper Sorbian. For training our systems, we generate synthetic data by both back- and forward-translation. Additionally, we enrich the training data with German-Czech translated from Czech to Upper Sorbian by an unsupervised statistical MT system incorporating orthographically similar word pairs and transliterations of OOV words. Our best translation system between German and Sorbian is based on transfer learning from a Czech-German system and scores 12 to 13 BLEU higher than a baseline system built using the available parallel data only."
2020.coling-main.488,Automatically Identifying Words That Can Serve as Labels for Few-Shot Text Classification,2020,-1,-1,2,0,3823,timo schick,Proceedings of the 28th International Conference on Computational Linguistics,0,"A recent approach for few-shot text classification is to convert textual inputs to cloze questions that contain some form of task description, process them with a pretrained language model and map the predicted words to labels. Manually defining this mapping between words and labels requires both domain expertise and an understanding of the language model{'}s abilities. To mitigate this issue, we devise an approach that automatically finds such a mapping given small amounts of training data. For a number of tasks, the mapping found by our approach performs almost as well as hand-crafted label-to-word mappings."
J17-2003,"Statistical Models for Unsupervised, Semi-Supervised Supervised Transliteration Mining",2017,66,7,2,0.554699,3156,hassan sajjad,Computational Linguistics,0,"We present a generative model that efficiently mines transliteration pairs in a consistent fashion in three different settings: unsupervised, semi-supervised, and supervised transliteration mining. The model interpolates two sub-models, one for the generation of transliteration pairs and one for the generation of non-transliteration pairs (i.e., noise). The model is trained on noisy unlabeled data using the EM algorithm. During training the transliteration sub-model learns to generate transliteration pairs and the fixed non-transliteration model generates the noise pairs. After training, the unlabeled data is disambiguated based on the posterior probabilities of the two sub-models. We evaluate our transliteration mining system on data from a transliteration mining shared task and on parallel corpora. For three out of four language pairs, our system outperforms all semi-supervised and supervised systems that participated in the NEWS 2010 shared task. On word pairs extracted from parallel corpora with fewer than 2{\%} transliteration pairs, our system achieves up to 86.7{\%} F-measure with 77.9{\%} precision and 97.8{\%} recall."
J15-2001,The Operation Sequence {M}odel{---}{C}ombining N-Gram-Based and Phrase-Based Statistical Machine Translation,2015,48,18,2,0.814466,3159,nadir durrani,Computational Linguistics,0,"In this article, we present a novel machine translation model, the Operation Sequence Model OSM, which combines the benefits of phrase-based and N-gram-based statistical machine translation SMT and remedies their drawbacks. The model represents the translation process as a linear sequence of operations. The sequence includes not only translation operations but also reordering operations. As in N-gram-based SMT, the model is: i based on minimal translation units, ii takes both source and target information into account, iii does not make a phrasal independence assumption, and iv avoids the spurious phrasal segmentation problem. As in phrase-based SMT, the model i has the ability to memorize lexical reordering triggers, ii builds the search graph dynamically, and iii decodes with large translation units during search. The unique properties of the model are i its strong coupling of reordering and translation where translation and reordering decisions are conditioned on n previous translation and reordering decisions, and ii the ability to model local and long-range reorderings consistently. Using BLEU as a metric of translation accuracy, we found that our system performs significantly better than state-of-the-art phrase-based systems Moses and Phrasal and N-gram-based systems Ncode on standard translation tasks. We compare the reordering component of the OSM to the Moses lexical reordering model by integrating it into Moses. Our results show that OSM outperforms lexicalized reordering on all translation tasks. The translation quality is shown to be improved further by learning generalized representations with a POS-based OSM."
D14-1103,Dependency parsing with latent refinements of part-of-speech tags,2014,9,2,4,1,27147,thomas mueller,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,In this paper we propose a method to increase dependency parser performance without using additional labeled or unlabeled data by refining the layer of predicted part-of-speech (POS) tags. We perform experiments on English and German and show significant improvements for both languages. The refinement is based on generative split-merge training for Hidden Markov models (HMMs).
C14-1041,Investigating the Usefulness of Generalized Word Representations in {SMT},2014,48,26,3,0.911458,3159,nadir durrani,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"We investigate the use of generalized representations (POS, morphological analysis and word clusters) in phrase-based models and the N-gram-based Operation Sequence Model (OSM). Our integration enables these models to learn richer lexical and reordering patterns, consider wider contextual information and generalize better in sparse data conditions. When interpolating generalized OSM models on the standard IWSLT and WMT tasks we observed improvements of up to 1.35 on the English-to-German task and 0.63 for the German-to-English task. Using automatically generated word classes in standard phrase-based models and the OSM models yields an average improvement of 0.80 across 8 language pairs on the IWSLT shared task."
W13-2213,{M}unich-{E}dinburgh-{S}tuttgart Submissions of {OSM} Systems at {WMT}13,2013,26,12,3,1,3159,nadir durrani,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes Munich-EdinburghStuttgartxe2x80x99s submissions to the Eighth Workshop on Statistical Machine Translation. We report results of the translation tasks from German, Spanish, Czech and Russian into English and from English to German, Spanish, Czech, French and Russian. The systems described in this paper use OSM (Operation Sequence Model). We explain different pre-/post-processing steps that we carried out for different language pairs. For German-English we used constituent parsing for reordering and compound splitting as preprocessing steps. For Russian-English we transliterated the unknown words. The transliteration system is learned with the help of an unsupervised transliteration mining algorithm."
W13-2228,{QCRI}-{MES} Submission at {WMT}13: Using Transliteration Mining to Improve Statistical Machine Translation,2013,18,9,5,1,3156,hassan sajjad,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes QCRI-MESxe2x80x99s submission on the English-Russian dataset to the Eighth Workshop on Statistical Machine Translation. We generate improved word alignment of the training data by incorporating an unsupervised transliteration mining module to GIZA and build a phrase-based machine translation system. For tuning, we use a variation of PRO which provides better weights by optimizing BLEU1 at corpus-level. We transliterate out-of-vocabulary words in a postprocessing step by using a transliteration system built on the transliteration pairs extracted using an unsupervised transliteration mining system. For the Russian to English translation direction, we apply linguistically motivated pre-processing on the Russian side of the data."
W13-2230,{M}unich-{E}dinburgh-{S}tuttgart Submissions at {WMT}13: Morphological and Syntactic Processing for {SMT},2013,24,7,5,0,36553,marion weller,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We present 5 systems of the MunichEdinburgh-Stuttgart 1 joint submissions to the 2013 SMT Shared Task: FR-EN, ENFR, RU-EN, DE-EN and EN-DE. The first three systems employ inflectional generalization, while the latter two employ parser-based reordering, and DE-EN performs compound splitting. For our experiments, we use standard phrase-based Moses systems and operation sequence models (OSM)."
P13-2071,Can {M}arkov Models Over Minimal Translation Units Help Phrase-Based {SMT}?,2013,29,43,3,1,3159,nadir durrani,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"The phrase-based and N-gram-based SMT frameworks complement each other. While the former is better able to memorize, the latter provides a more principled model that captures dependencies across phrasal boundaries. Some work has been done to combine insights from these two frameworks. A recent successful attempt showed the advantage of using phrasebased search on top of an N-gram-based model. We probe this question in the reverse direction by investigating whether integrating N-gram-based translation and reordering models into a phrase-based decoder helps overcome the problematic phrasal independence assumption. A large scale evaluation over 8 language pairs shows that performance does significantly improve."
N13-1001,"Model With Minimal Translation Units, But Decode With Phrases",2013,23,32,3,1,3159,nadir durrani,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"N-gram-based models co-exist with their phrase-based counterparts as an alternative SMT framework. Both techniques have pros and cons. While the N-gram-based framework provides a better model that captures both source and target contexts and avoids spurious phrasal segmentation, the ability to memorize and produce larger translation units gives an edge to the phrase-based systems during decoding, in terms of better search performance and superior selection of translation units. In this paper we combine N-grambased modeling with phrase-based decoding, and obtain the benefits of both approaches. Our experiments show that using this combination not only improves the search accuracy of the N-gram model but that it also improves the BLEU scores. Our system outperforms state-of-the-art phrase-based systems (Moses and Phrasal) and N-gram-based systems by a significant margin on German, French and Spanish to English translation tasks."
J13-1005,"Knowledge Sources for Constituent Parsing of {G}erman, a Morphologically Rich and Less-Configurational Language",2013,58,14,2,0.45624,3265,alexander fraser,Computational Linguistics,0,"We study constituent parsing of German, a morphologically rich and less-configurational language. We use a probabilistic context-free grammar treebank grammar that has been adapted to the morphologically rich properties of German by markovization and special features added to its productions. We evaluate the impact of adding lexical knowledge. Then we examine both monolingual and bilingual approaches to parse reranking. Our reranking parser is the new state of the art in constituency parsing of the TIGER Treebank. We perform an analysis, concluding with lessons learned, which apply to parsing other morphologically rich and less-configurational languages."
D13-1032,Efficient Higher-Order {CRF}s for Morphological Tagging,2013,23,87,2,1,27147,thomas mueller,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,Training higher-order conditional random fields is prohibitive for huge tag sets. We present an approximated conditional random field using coarse-to-fine decoding and early updating. We show that our implementation yields fast and accurate morphological taggers across six languages with different morphological properties and that across languages higher-order models give significant improvements over 1-order models.
P12-1049,A Statistical Model for Unsupervised and Semi-supervised Transliteration Mining,2012,17,27,3,1,3156,hassan sajjad,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a novel model to automatically extract transliteration pairs from parallel corpora. Our model is efficient, language pair independent and mines transliteration pairs in a consistent fashion in both unsupervised and semi-supervised settings. We model transliteration mining as an interpolation of transliteration and non-transliteration sub-models. We evaluate on NEWS 2010 shared task data and on parallel corpora with competitive results."
N12-1043,A Comparative Investigation of Morphological Language Modeling for the Languages of the {E}uropean {U}nion,2012,20,6,3,1,27147,thomas mueller,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We investigate a language model that combines morphological and shape features with a Kneser-Ney model and test it in a large crosslingual study of European languages. Even though the model is generic and we use the same architecture and features for all languages, the model achieves reductions in perplexity for all 21 languages represented in the Europarl corpus, ranging from 3% to 11%. We show that almost all of this perplexity reduction can be achieved by identifying suffixes by frequency."
E12-1007,Dependency Parsing of {H}ungarian: Baseline Results and Challenges,2012,24,15,3,0.785329,29565,richard farkas,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Hungarian is a stereotype of morphologically rich and non-configurational languages. Here, we introduce results on dependency parsing of Hungarian that employ a 80K, multi-domain, fully manually annotated corpus, the Szeged Dependency Treebank. We show that the results achieved by state-of-the-art data-driven parsers on Hungarian and English (which is at the other end of the configurational-non-configurational spectrum) are quite similar to each other in terms of attachment scores. We reveal the reasons for this and present a systematic and comparative linguistically motivated error analysis on both languages. This analysis highlights that addressing the language-specific phenomena is required for a further remarkable error reduction."
D12-1095,Forest Reranking through Subtree Ranking,2012,19,4,2,0.785329,29565,richard farkas,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"We propose the subtree ranking approach to parse forest reranking which is a generalization of current perceptron-based reranking methods. For the training of the reranker, we extract competing local subtrees, hence the training instances (candidate subtree sets) are very similar to those used during beam-search parsing. This leads to better parameter optimization. Another chief advantage of the framework is that arbitrary learning to rank methods can be applied. We evaluated our reranking approach on German and English phrase structure parsing tasks and compared it to various state-of-the-art reranking approaches such as the perceptron-based forest reranker. The subtree ranking approach with a Maximum Entropy model significantly outperformed the other approaches."
C12-2105,Data-driven Dependency Parsing With Empty Heads,2012,24,8,4,0,17921,wolfgang seeker,Proceedings of {COLING} 2012: Posters,0,"Syntactic dependency structures are based on the assumption that there is exactly one node in the structure for each word in the sentence. However representing elliptical constructions (e.g. missing verbs) is problematic as the question where the dependents of the elided material should be attached to has to be solved. In this paper, we present an in-depth study into the challenges of introducing empty heads into dependency structures during automatic parsing. Structurally, empty heads provide an attachment site for the dependents of the non-overt material and thus preserve the linguistically plausible structure of the sentence. We compare three different (computational) approaches to the introduction of empty heads and evaluate them against German and Hungarian data. We then conduct a fine-grained error analysis on the output of one of the approaches to highlight some of the difficulties of the task. We find that while a clearly defined part of the phenomena can be learned by the parser, more involved elliptical structures are still mostly out of reach of the automatic tools."
W11-2924,Features for Phrase-Structure Reranking from Dependency Parses,2011,22,6,3,0.785329,29565,richard farkas,Proceedings of the 12th International Conference on Parsing Technologies,0,"Radically different approaches have been proved to be effective for phrase-structure and dependency parsers in the last decade. Here, we aim to exploit the divergence in these approaches and show the utility of features extracted from the automatic dependency parses of sentences for a discriminative phrase-structure parser. Our experiments show a significant improvement over the state-of-the-art German discriminative constituent parser."
P11-1044,An Algorithm for Unsupervised Transliteration Mining with an Application to Word Alignment,2011,20,14,3,1,3156,hassan sajjad,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We propose a language-independent method for the automatic extraction of transliteration pairs from parallel corpora. In contrast to previous work, our method uses no form of supervision, and does not require linguistically informed preprocessing. We conduct experiments on data sets from the NEWS 2010 shared task on transliteration mining and achieve an F-measure of up to 92%, outperforming most of the semi-supervised systems that were submitted. We also apply our method to English/Hindi and English/Arabic parallel corpora and compare the results with manually built gold standards which mark transliterated word pairs. Finally, we integrate the transliteration module into the GIZA word aligner and evaluate it on two word alignment tasks achieving improvements in both precision and recall measured against gold standard word alignments."
P11-1105,A Joint Sequence Translation Model with Integrated Reordering,2011,21,75,2,1,3159,nadir durrani,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We present a novel machine translation model which models translation by a linear sequence of operations. In contrast to the N-gram model, this sequence includes not only translation but also reordering operations. Key ideas of our model are (i) a new reordering approach which better restricts the position to which a word or phrase can be moved, and is able to handle short and long distance re-orderings in a unified way, and (ii) a joint sequence model for the translation and reordering probabilities which is more flexible than standard phrase-based MT. We observe statistically significant improvements in BLEU over Moses for German-to-English and Spanish-to-English tasks, and comparable results for a French-to-English task."
I11-1015,Comparing Two Techniques for Learning Transliteration Models Using a Parallel Corpus,2011,17,10,3,1,3156,hassan sajjad,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"We compare the use of an unsupervised transliteration mining method and a rulebased method to automatically extract lists of transliteration word pairs from a parallel corpus of Hindi/Urdu. We build joint source channel models on the automatically aligned orthographic transliteration units of the automatically extracted lists of transliteration pairs resulting in two transliteration systems. We compare our systems with three transliteration systems available on the web, and show that our systems have better performance. We perform an extensive analysis of the results of using both methods and show evidence that the unsupervised transliteration mining method is superior for applications requiring high recall transliteration lists, while the rule-based method is useful for obtaining high precision lists."
P10-1048,{H}indi-to-{U}rdu Machine Translation through Transliteration,2010,20,40,4,1,3159,nadir durrani,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"We present a novel approach to integrate transliteration into Hindi-to-Urdu statistical machine translation. We propose two probabilistic models, based on conditional and joint probability formulations, that are novel solutions to the problem. Our models consider both transliteration and translation when translating a particular Hindi word given the context whereas in previous work transliteration is only used for translating OOV (out-of-vocabulary) words. We use transliteration as a tool for disambiguation of Hindi homonyms which can be both translated or transliterated or transliterated differently based on different contexts. We obtain final BLEU scores of 19.35 (conditional probability model) and 19.00 (joint probability model) as compared to 14.30 for a baseline phrase-based system and 16.25 for a system which transliterates OOV words in the baseline system. This indicates that transliteration is useful for more than only translating OOV words for language pairs like Hindi-Urdu."
faass-etal-2010-design,Design and Application of a Gold Standard for Morphological Analysis: {SMOR} as an Example of Morphological Evaluation,2010,14,15,3,0,45273,gertrud faass,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes general requirements for evaluating and documenting NLP tools with a focus on morphological analysers and the design of a Gold Standard. It is argued that any evaluation must be measurable and documentation thereof must be made accessible for any user of the tool. The documentation must be of a kind that it enables the user to compare different tools offering the same service, hence the descriptions must contain measurable values. A Gold Standard presents a vital part of any measurable evaluation process, therefore, the corpus-based design of a Gold Standard, its creation and problems that occur are reported upon here. Our project concentrates on SMOR, a morphological analyser for German that is to be offered as a web-service. We not only utilize this analyser for designing the Gold Standard, but also evaluate the tool itself at the same time. Note that the project is ongoing, therefore, we cannot present final results."
heid-etal-2010-corpus,A Corpus Representation Format for Linguistic Web Services: The {D}-{SPIN} Text Corpus Format and its Relationship with {ISO} Standards,2010,7,22,2,0,24867,ulrich heid,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"In the framework of the preparation of linguistic web services for corpus processing, the need for a representation format was felt, which supports interoperability between different web services in a corpus processing pipeline, but also provides a well-defined interface to both, legacy tools and their data formats and upcoming international standards. We present the D-SPIN text corpus format, TCF, which was designed for this purpose. It is a stand-off XML format, inspired by the philosophy of the emerging standards LAF (Linguistic Annotation Framework) and its ``instances'' MAF for morpho-syntactic annotation and SynAF for syntactic annotation. Tools for the exchange with existing (best practice) formats are available, and a converter from MAF to TCF is being tested in spring 2010. We describe the usage scenario where TCF is embedded and the properties and architecture of TCF. We also give examples of TCF encoded data and describe the aspects of syntactic and semantic interoperability already addressed."
E09-1079,Tagging {U}rdu Text with Parts of Speech: A Tagger Comparison,2009,15,20,2,1,3156,hassan sajjad,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"In this paper, four state-of-art probabilistic taggers i.e. TnT tagger, TreeTagger, RF tagger and SVM tool, are applied to the Urdu language. For the purpose of the experiment, a syntactic tagset is proposed. A training corpus of 100,000 tokens is used to train the models. Using the lexicon extracted from the training corpus, SVM tool shows the best accuracy of 94.15%. After providing a separate lexicon of 70,568 types, SVM tool again shows the best accuracy of 95.66%."
P08-1057,Combining {EM} Training and the {MDL} Principle for an Automatic Verb Classification Incorporating Selectional Preferences,2008,23,32,4,0,631,sabine walde,Proceedings of ACL-08: HLT,1,"This paper presents an innovative, complex approach to semantic verb classification that relies on selectional preferences as verb properties. The probabilistic verb class model underlying the semantic classes is trained by a combination of the EM algorithm and the MDL principle, providing soft clusters with two dimensions (verb senses and subcategorisation frames with selectional preferences) as a result. A language-model-based evaluation shows that after 10 training iterations the verb class model results are above the baseline results."
C08-1098,Estimation of Conditional Probabilities With Decision Trees and an Application to Fine-Grained {POS} Tagging,2008,15,126,1,1,13978,helmut schmid,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"We present a HMM part-of-speech tagging method which is particularly suited for POS tagsets with a large number of fine-grained tags. It is based on three ideas: (1) splitting of the POS tags into attribute vectors and decomposition of the contextual POS probabilities of the HMM into a product of attribute probabilities, (2) estimation of the contextual probabilities with decision trees, and (3) use of high-order HMMs. In experiments on German and Czech data, our tagger outperformed state-of-the-art POS taggers."
P07-1013,Phonological Constraints and Morphological Preprocessing for Grapheme-to-Phoneme Conversion,2007,19,29,2,0,5404,vera demberg,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Grapheme-to-phoneme conversion (g2p) is a core component of any text-to-speech system. We show that adding simple syllabification and stress assignment constraints, namely xe2x80x98one nucleus per syllablexe2x80x99 and xe2x80x98one main stress per wordxe2x80x99, to a joint n-gram model for g2p conversion leads to a dramatic improvement in conversion accuracy. Secondly, we assessed morphological preprocessing for g2p conversion. While morphological information has been incorporated in some past systems, its contribution has never been quantitatively assessed for German. We compare the relevance of morphological preprocessing with respect to the morphological segmentation method, training set size, the g2p conversion algorithm, and two languages, English and German."
P06-1023,Trace Prediction and Recovery with Unlexicalized {PCFG}s and Slash Features,2006,11,40,1,1,13978,helmut schmid,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes a parser which generates parse trees with empty elements in which traces and fillers are co-indexed. The parser is an unlexicalized PCFG parser which is guaranteed to return the most probable parse. The grammar is extracted from a version of the PENN treebank which was automatically annotated with features in the style of Klein and Manning (2003). The annotation includes GPSG-style slash features which link traces and fillers, and other features which improve the general parsing accuracy. In an evaluation on the PENN treebank (Marcus et al., 1993), the parser outperformed other unlexicalized PCFG parsers in terms of labeled bracketing f-score. Its results for the empty category prediction task and the trace-filler co-indexation task exceed all previously reported results with 84.1% and 77.4% f-score, respectively."
H05-1065,Disambiguation of Morphological Structure using a {PCFG},2005,7,4,1,1,13978,helmut schmid,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"German has a productive morphology and allows the creation of complex words which are often highly ambiguous. This paper reports on the development of a head-lexicalized PCFG for the disambiguation of German morphological analyses. The grammar is trained on unlabeled data using the Inside-Outside algorithm. The parser achieves a precision of more than 68% on difficult test data, which is 23% more than the baseline obtained by randomly choosing one of the simplest analyses. Remarkable is the fact that precision drops to 52% without lexicalization."
schmid-etal-2004-smor,"{SMOR}: A {G}erman Computational Morphology Covering Derivation, Composition and Inflection",2004,5,98,1,1,13978,helmut schmid,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We present a morphological analyser for German inflection and word formation implemented in finite state technology. Unlike purely lexicon-based approaches, it can account for productive word formation like derivation and composition. The implementation is based on the Stuttgart Finite State Transducer Tools (SFST-Tools), a non-commercial FST platform. It is fast and achieves a high coverage."
C04-1024,Efficient Parsing of Highly Ambiguous Context-Free Grammars with Bit Vectors,2004,6,124,1,1,13978,helmut schmid,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,An efficient bit-vector-based CKY-style parser for context-free parsing is presented. The parser computes a compact parse forest representation of the complete set of possible analyses for large treebank grammars and long input sentences. The parser uses bit-vector operations to parallelise the basic parsing operations. The parser is particularly useful when all analyses are needed rather than just the most probable one.
C04-1095,New Statistical Methods for Phrase Break Prediction,2004,16,21,1,1,13978,helmut schmid,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,The paper presents two methods for the prediction of phrase breaks. The first method uses a standard HMM part-of-speech tagger with variable context length. The second method directly encodes the distance from the last phrase break in its states. It combines the probability of a phrase break given the distance from the last phrase break with the probability of a break given the local context consisting of the surrounding words and part of speech tags. The accuracy of the new tagger is 2 percentage points higher than that of Taylor and Black (1998) on similar data.
C02-1029,A Generative Probability Model for Unification-Based Grammars,2002,11,3,1,1,13978,helmut schmid,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,A generative probability model for unification-based grammars is presented in which rule probabilities depend on the feature structure of the expanded constituent. The presented model is the first model which requires no normalization and allows the application of dynamic programming algorithms for disambiguation (Viterbi) and training (Inside-Outside). Another advantage is the small number of parameters.
C02-1108,Lexicalization of Probabilistic Grammars,2002,7,3,1,1,13978,helmut schmid,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"Two general methods for the lexicalization of probabilistic grammars are presented which are modular, powerful and require only a small number of parameters. The first method multiplies the unlexicalized parse tree probability with the exponential of the mutual information terms of all word-governor pairs in the parse. The second lexicalization method accounts for the dependencies between the different arguments of a word. The model is based on a EM clustering model with word classes and selectional restrictions as hidden features. This model is useful for finding word classes, selectional restrictions and word sense probabilities."
P01-1060,Parse Forest Computation of Expected Governors,2001,12,7,1,1,13978,helmut schmid,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"In a headed tree, each terminal word can be uniquely labeled with a governing word and grammatical relation. This labeling is a summary of a syntactic analysis which eliminates detail, reflects aspects of semantics, and for some grammatical relations (such as subject of finite verb) is nearly uncontroversial. We define a notion of expected governor markup, which sums vectors indexed by governors and scaled by probabilistic tree weights. The quantity is computed in a parse forest representation of the set of tree analyses for a given sentence, using vector sums and scaling by inside probability and flow."
C00-2105,Robust {G}erman Noun Chunking With a Probabilistic Context-Free Grammar,2000,17,58,1,1,13978,helmut schmid,{COLING} 2000 Volume 2: The 18th International Conference on Computational Linguistics,0,"We present a noun chunker for German which is based on a head-lexicalised probabilistic context-free grammar. A manually developed grammar was semi-automatically extended with robustness rules in order to allow parsing of unrestricted text. The model parameters were learned from unlabelled training data by a probabilistic context-free parser. For extracting noun chunks, the parser generates all possible noun chunk analyses, scores them with a novel algorithm which maximizes the best chunk sequence criterion, and chooses the most probable chunk sequence. An evaluation of the chunker on 2,140 hand-annotated noun chunks yielded 92% recall and 93% precision."
1997.iwpt-1.21,Parsing by Successive Approximation,1997,-1,-1,1,1,13978,helmut schmid,Proceedings of the Fifth International Workshop on Parsing Technologies,0,"It is proposed to parse feature structure-based grammars in several steps. Each step is aimed to eliminate as many invalid analyses as possible as efficiently as possible. To this end the set of feature constraints is divided into three subsets, a set of context-free constraints, a set of filtering constraints and a set of structure-building constraints, which are solved in that order. The best processing strategy differs: Context-free constraints are solved efficiently with one of the well-known algorithms for context-free parsing. Filtering constraints can be solved using unification algorithms for non-disjunctive feature structures whereas structure-building constraints require special techniques to represent feature structures with embedded disjunctions efficiently. A compilation method and an efficient processing strategy for filtering constraints are presented."
C94-1027,Part-of-Speech Tagging With Neural Networks,1994,5,266,1,1,13978,helmut schmid,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Text corpora which are tagged with part-of-speech information are useful in many areas of linguistic research. In this paper, a new part-of-speech tagging method based on neural networks (Net-Tagger) is presented and its performance is compared to that of a HMM-tagger (Cutting et al., 1992) and a trigram-based tagger (Kempe, 1993). It is shown that the Net-Tagger performs as well as the trigram-based tagger and better than the HMM-tagger."
