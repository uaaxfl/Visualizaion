2007.jeptalnrecital-long.20,2006.jeptalnrecital-long.11,1,0.875034,"Missing"
2007.jeptalnrecital-long.20,sagot-etal-2006-lefff,0,0.065359,"Missing"
2007.jeptalnrecital-long.20,saint-dizier-2006-prepnet,0,0.062192,"Missing"
2008.jeptalnrecital-court.6,2006.jeptalnrecital-long.11,1,0.863432,"Missing"
2008.jeptalnrecital-court.6,2007.jeptalnrecital-poster.15,0,0.0505751,"Missing"
2008.jeptalnrecital-court.6,P07-1115,0,0.0710112,"Missing"
2008.jeptalnrecital-court.6,W04-2104,0,0.0779349,"Missing"
2008.jeptalnrecital-court.6,sagot-etal-2006-lefff,0,0.0633788,"Missing"
2008.jeptalnrecital-court.6,francopoulo-etal-2006-lexical,0,\N,Missing
2009.jeptalnrecital-long.29,S07-1012,0,0.0616536,"Missing"
2009.jeptalnrecital-long.29,W04-1213,0,0.0579772,"Missing"
2009.jeptalnrecital-long.29,W04-3111,0,0.0802436,"Missing"
2009.jeptalnrecital-long.29,M95-1002,0,0.186931,"Missing"
2010.jeptalnrecital-long.35,alex-etal-2006-impact,0,0.0475085,"Missing"
2010.jeptalnrecital-long.35,J08-4004,0,0.15749,"Missing"
2010.jeptalnrecital-long.35,J96-2004,0,0.217543,"Missing"
2010.jeptalnrecital-long.35,2009.jeptalnrecital-long.19,0,0.10515,"Missing"
2010.jeptalnrecital-long.35,J08-3001,0,0.0280564,"Missing"
2011.jeptalnrecital-long.12,W09-3302,0,0.0214641,"Missing"
2011.jeptalnrecital-long.12,W10-1806,0,0.0208006,"Missing"
2011.jeptalnrecital-long.12,P08-1092,0,0.0245517,"Missing"
2011.jeptalnrecital-long.12,W10-0701,0,0.0271508,"Missing"
2011.jeptalnrecital-long.12,W10-0712,0,0.0271393,"Missing"
2011.jeptalnrecital-long.12,mcgraw-etal-2010-collecting,0,0.0673832,"de mise en place des garde-fous est non nul (Callison-Burch & Dredze, 2010). De même, le coût de validation (Kaisser & Lowe, 2008) ou de développement (Xu & Klakow, 2010) post-MTurk permettant de compenser la mauvaise qualité des résultats (voir section 3.4) n’est généralement pas précisément évalué. Or, ces coûts supplémentaires ne sont jamais pris en compte dans le calcul final. De plus, certaines tâches peuvent se révéler plus coûteuses que prévues. Ainsi, si l’on ne trouve pas de Turkers pour faire la tâche, on peut être obligé d’augmenter la rémunération, comme Novotney & Callison-Burch (2010), qui, partant d’un coût très bas (5 dollars de l’heure transcrite), ont été obligés de le multiplier par 7 (37 dollars de l’heure) pour transcrire du coréen, par manque de Turkers qualifiés. 3.4 MTurk permet de produire une qualité équivalente ? 3.4.1 Limitations liées à la non expertise Les Turkers étant des non experts, le Requester (fournisseur de tâches) doit découper les tâches complexes en tâches plus simples (HIT, Human Intelligence Task), afin de les rendre réalisables. Ce faisant, le chercheur est amené à faire des choix qui peuvent biaiser les résultats. Un exemple de ce type de bia"
2011.jeptalnrecital-long.12,U08-1016,0,0.0295311,"Missing"
2011.jeptalnrecital-long.12,N10-1024,0,0.0332135,"Missing"
2011.jeptalnrecital-long.12,pak-paroubek-2010-twitter,0,0.0286786,"Missing"
2011.jeptalnrecital-long.12,P05-1044,0,0.0189206,"Missing"
2011.jeptalnrecital-long.12,D08-1027,0,0.0220232,"Missing"
2011.jeptalnrecital-long.12,W07-1523,0,0.0188492,"Missing"
2011.jeptalnrecital-long.12,P10-1070,0,0.019888,"Missing"
2011.jeptalnrecital-long.12,W07-2203,0,0.0512438,"Missing"
2011.jeptalnrecital-long.12,xu-klakow-2010-paragraph,0,0.0241202,"Missing"
2011.jeptalnrecital-long.12,P95-1026,0,0.332177,"Missing"
2011.jeptalnrecital-long.12,W10-0728,0,0.0291831,"Missing"
2017.jeptalnrecital-court.21,W14-1206,0,0.0405071,"Missing"
2017.jeptalnrecital-court.21,2010.jeptalnrecital-long.3,0,0.0433137,"Missing"
2017.jeptalnrecital-court.21,C12-1055,1,0.828073,"Missing"
2017.jeptalnrecital-court.21,C16-1286,1,0.892863,"Missing"
2017.jeptalnrecital-court.21,W15-2204,1,0.884851,"Missing"
2017.jeptalnrecital-court.21,sagot-2010-lefff,0,0.0261628,"Missing"
2017.jeptalnrecital-court.21,seretan-2012-acquisition,0,0.0371227,"Missing"
2017.jeptalnrecital-court.21,P13-4001,0,0.0623349,"Missing"
2020.acl-tutorials.4,J08-1008,0,0.229852,"Missing"
2020.acl-tutorials.4,J07-4009,0,0.026953,"ot.mieskes@h-da.de neveol@limsi.fr 1 Tutorial Content considerable number of reviewers are junior researchers, who might lack the experience and expertise necessary for high-quality reviews. A tutorial on this topic might increase reviewers’ confidence, as well as the quality of the reviews. Given the importance of conferences in NLP, the reviewing standards should be as high as with journals in other fields. This tutorial will cover the goals, processes, and evaluation of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing—reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—has raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference papers, but conversations in the hallways at *ACL mee"
2020.acl-tutorials.4,J05-4006,0,0.156389,"high as with journals in other fields. This tutorial will cover the goals, processes, and evaluation of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing—reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—has raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference papers, but conversations in the hallways at *ACL meetings suggest that we have a publication bias towards papers that report high performance, with perhaps not much else of interest in them (Manning, 2015). It need not be this way. There is good reason to think that reviewing is a learnable (and teachable) skill (Basford, 1990; Paice, 2001; Benos et al., 2003; Koike et al., 2009; Shukla, 2010; Tandon, 2014; Spyns and Vida"
2020.jeptalnrecital-eternal.4,L18-1619,1,0.865841,"Missing"
2020.jeptalnrecital-eternal.4,L18-1025,0,0.0336268,"Missing"
2020.jeptalnrecital-eternal.4,P13-1166,0,0.0277763,"Missing"
2020.jeptalnrecital-eternal.4,D17-1010,0,0.0437426,"Missing"
2020.jeptalnrecital-eternal.4,P17-2035,0,0.0489453,"Missing"
2020.lrec-1.34,borg-gatt-2014-crowd,1,0.629272,"Missing"
2020.lrec-1.34,W10-0701,0,0.110558,"Missing"
2020.lrec-1.34,D10-1088,0,0.0443179,"Missing"
2020.lrec-1.34,J17-4005,0,0.0379369,"Missing"
2020.lrec-1.34,R11-1057,0,0.0708106,"Missing"
2020.lrec-1.34,W10-0708,0,0.0754337,"Missing"
2020.lrec-1.34,W10-0713,0,0.0138294,"Missing"
2020.lrec-1.34,L18-1074,0,0.0664263,"Missing"
2020.lrec-1.34,C16-1286,1,0.900816,"Missing"
2020.lrec-1.34,L18-1644,1,0.886465,"Missing"
2020.lrec-1.34,W10-0712,0,0.0208388,"Missing"
2020.lrec-1.34,R19-1079,1,0.89228,"Missing"
2020.lrec-1.34,C08-1080,1,0.781798,"Missing"
2020.lrec-1.34,piperidis-2012-meta,0,0.0849343,"Missing"
2020.lrec-1.34,W12-3152,0,0.0793882,"Missing"
2020.lrec-1.34,D11-1141,0,0.0175118,"Missing"
2020.lrec-1.34,2020.lrec-1.38,1,0.686934,"Missing"
2020.lrec-1.34,E17-3021,0,0.0216142,"Missing"
2020.lrec-1.34,varadi-etal-2008-clarin,0,0.127908,"Missing"
2020.lrec-1.34,P11-1122,0,0.0270497,"Missing"
2020.lrec-1.541,W08-2230,0,0.0931812,"Missing"
2020.lrec-1.541,L18-1024,0,0.0277894,"lecting language data as we write these lines. Other GWAPs were then designed, addressing new tasks, like ZombiLingo (still running) for the annotation of dependency relations for French (Guillaume et al., 2016) or Wordrobe (no more active), for various semantic annotation tasks (Bos and Nissim, 2015). Most of the active GWAPs in the domain now appear on the LDC LingoBoingo portal1 . Apart from the already mentioned games, it presents TileAttack (Madge et al., 2017), Wormingo (Kicikoglu et al., 2019), WordClicker (Madge et al., 2019), Name That Language! (described as the Language ID game in (Cieri et al., 2018)) and Know Your Nyms?. Many of these GWAPs were quite successful, both in terms of the quantity of created data and of the obtained quality (Chamberlain et al., 2013). However, despite the wide variety of available GWAPs, there is, to our knowledge, no other active (and open) gamified platform or game dealing with MWE identification. The only related work we found is a gamified interface which was developed as part of the PARSEME COST action2 , allowing selected participants (researchers) to guess the meaning of opaque MWEs in other languages (Krstev and Savary, 2018). 3. 3.1. Rigor Mortis A g"
2020.lrec-1.541,W18-4923,1,0.779561,"Missing"
2020.lrec-1.541,C16-1286,1,0.846595,"h respect to experts. 2. tested for a while now. The first to be developed were JeuxDeMots, a game allowing the creation of a lexical network for French, which is more than ten years old now (Lafourcade, 2007; Lafourcade et al., 2018), closely followed by Phrase Detectives (Chamberlain et al., 2008), in which participants annotate co-reference relations in English corpora. Both games are still running and collecting language data as we write these lines. Other GWAPs were then designed, addressing new tasks, like ZombiLingo (still running) for the annotation of dependency relations for French (Guillaume et al., 2016) or Wordrobe (no more active), for various semantic annotation tasks (Bos and Nissim, 2015). Most of the active GWAPs in the domain now appear on the LDC LingoBoingo portal1 . Apart from the already mentioned games, it presents TileAttack (Madge et al., 2017), Wormingo (Kicikoglu et al., 2019), WordClicker (Madge et al., 2019), Name That Language! (described as the Language ID game in (Cieri et al., 2018)) and Know Your Nyms?. Many of these GWAPs were quite successful, both in terms of the quantity of created data and of the obtained quality (Chamberlain et al., 2013). However, despite the wid"
2020.lrec-1.541,W18-4925,0,0.0477251,"Missing"
2020.lrec-1.541,schneider-etal-2014-comprehensive,0,0.22839,"re used to learn state-of-the-art identification models (Schneider et al., 2016; Ramisch et al., 2018). The construction of such annotated corpora is nonetheless costly. Indeed, they are mainly annotated by experts or linguistics-aware people long-trained by experts in order to guarantee the annotation quality. Indeed, MWEs are known to be hard to identify due to the fuzzy delimitation between compositional and non-compositional combinations of words. This difficulty is demonstrated in the modest average inter-annotator agreement (0.65 in F-score) for comprehensive MWEs annotation in English (Schneider et al., 2014). In this paper, we propose a gamified platform for annotating MWEs. Experiments were carried out for French. The aim of this paper is to assess to what extent one can rely on corpora annotated in MWEs by the participants with respect to experts. 2. tested for a while now. The first to be developed were JeuxDeMots, a game allowing the creation of a lexical network for French, which is more than ten years old now (Lafourcade, 2007; Lafourcade et al., 2018), closely followed by Phrase Detectives (Chamberlain et al., 2008), in which participants annotate co-reference relations in English corpora."
2020.lrec-1.541,S16-1084,0,0.0271908,"ons include multiple linguistic phenomena, as mentioned in (Sag et al., 2001), for example idioms (e.g. add fuel to the fire), phrasal verbs (e.g. give up), complex function words (e.g. as soon as), light-verb constructions (e.g. take a bath), adverbial and nominal open compounds (e.g. by the way, dry run). Handling MWEs is a key challenge for natural language processing (Sag et al., 2001), on which researchers have long been working. Recently, significant advances have been made thanks to the availability of new MWEannotated data that are used to learn state-of-the-art identification models (Schneider et al., 2016; Ramisch et al., 2018). The construction of such annotated corpora is nonetheless costly. Indeed, they are mainly annotated by experts or linguistics-aware people long-trained by experts in order to guarantee the annotation quality. Indeed, MWEs are known to be hard to identify due to the fuzzy delimitation between compositional and non-compositional combinations of words. This difficulty is demonstrated in the modest average inter-annotator agreement (0.65 in F-score) for comprehensive MWEs annotation in English (Schneider et al., 2014). In this paper, we propose a gamified platform for anno"
2020.sltu-1.15,goldhahn-etal-2012-building,0,0.0369981,"ine translation. 3.2. 3.2.1. The Web as a Corpus Crawled Corpora for Less-Resourced Languages One way to address the data bottleneck is to resort to Web crawling. Web crawling for multilingual corpus construction consists in gathering texts from the Web that are further curated and automatically classified by language. For instance, the An Cr´ubad´an project (Scannell, 2007), first initiative of the kind to our knowledge, uses a combination of trigrams, automatically generated lexicons and lists of words specific to a given language, to identify on the Web contents written in 2,228 languages. Goldhahn et al. (2012) combine various techniques, including the bootstrap of corpora through search queries. Whichever the method chosen to crawl the Web, it is necessary to perform language identification to classify the documents. Both works presented above indeed require statistical information on the distribution of characteristic patterns, such as trigrams, for each language. This kind of information is not always available, especially when it comes to multi-variant languages, composed of similar dialects (eg. the dialects of Occitan) or that can be written with competing orthographies (eg. Cornish). What is"
2020.sltu-1.15,L18-1550,0,0.0297416,"s represent a challenge. In fact, they push us to deal with the issues that variation processes imply, either because these productions account for most of the written existence of a non-standardized language, or because they diverge from a standard language in an undeterministic fashion. Yet, for the endangered languages there is an urge to develop tools that match the actual linguistic practice of its end-users to sustain their digital use. Even though less-resourced languages benefit from the current trends in NLP which tend towards less supervision (see, for instance (Lample et al., 2017; Grave et al., 2018)) and seek higher robustness to variation, processing technologies still highly rely on the availability of text corpora. 3. Existing Sources Used for Corpus Collection Although there exist sources of text corpus readily available for numerous languages, these ”opportunistic” corpora (McEnery and Hardie, 2011) present several shortcomings, including: • their nature and license sometimes require operations that result in a loss of information such as the metadata necessary to identify the languages or the structure of the document. • using them requires additional linguistic resources (to perfo"
2020.sltu-1.15,L18-1071,1,0.912046,"multiple participants giving their opinion and possibly debating the solution. The crowdsourced corpus is here composed of the conversations between the participants. The second article presents an online spell and grammar checker for Welsh, used as such by speakers. The corpus collected here is the input to be spellchecked. This strategy appears as particularly efficient to collect diverse data in terms both of form and content. 5.2. Crowdsourcing Cooking Recipes We have focused in previous work on producing corpora collaboratively annotated with part-of-speech for underresourced languages (Millour and Fort, 2018; Millour and Fort, 2019). Our experiments involved Alsatian, a continuum of Alemannic dialects spoken in Alsace, a diglossic French region, and Mauritian Creole, a French-based Creole spoken mostly in Mauritius. A flexible spelling system called Orthal has been developed for Alsatian (Cr´evenatWerner and Zeidler, 2008) and a standardized spelling (Lortograf Kreol Morisien) is promoted by the Mauritian Creole Academy (Akademi Kreol Morisien) (Police-Michel et al., 2012) and supported by the Mauritian government. Although, to our knowledge, there exists no precise statistics on the use of these"
2020.sltu-1.15,N16-1070,0,0.0165375,"ic resources and more specifically text corpora for a non-standardized language. After describing the conditions and setup of this experiment, we present the challenges that were encountered as well as an analysis of their potential causes. 5.1. Eliciting Corpora Crowdsourcing text corpora often resorts to eliciting techniques, such as asking for descriptions to inspire the contributors. In such cases, crowdsourcing can be described as explicit, meaning that the goal of the activity is expressed plainly to the participant. Producing text corpora being a tedious task requiring time and effort, Niculae and Danescu-Niculescu-Mizil (2016) and Prys et al. (2016) have come up with original ideas to crowdource text corpora implicitly. The first article presents Street Crowd, an online game which objective is to identify the location where a picture was taken. This search towards the correct location is done collaboratively, with multiple participants giving their opinion and possibly debating the solution. The crowdsourced corpus is here composed of the conversations between the participants. The second article presents an online spell and grammar checker for Welsh, used as such by speakers. The corpus collected here is the input"
2020.sltu-1.15,L16-1519,0,0.0281164,"for a non-standardized language. After describing the conditions and setup of this experiment, we present the challenges that were encountered as well as an analysis of their potential causes. 5.1. Eliciting Corpora Crowdsourcing text corpora often resorts to eliciting techniques, such as asking for descriptions to inspire the contributors. In such cases, crowdsourcing can be described as explicit, meaning that the goal of the activity is expressed plainly to the participant. Producing text corpora being a tedious task requiring time and effort, Niculae and Danescu-Niculescu-Mizil (2016) and Prys et al. (2016) have come up with original ideas to crowdource text corpora implicitly. The first article presents Street Crowd, an online game which objective is to identify the location where a picture was taken. This search towards the correct location is done collaboratively, with multiple participants giving their opinion and possibly debating the solution. The crowdsourced corpus is here composed of the conversations between the participants. The second article presents an online spell and grammar checker for Welsh, used as such by speakers. The corpus collected here is the input to be spellchecked. Th"
2020.sltu-1.15,L18-1656,0,0.0712189,"f linguistic communities into collaboratively building resources for their languages. We argue that this method is all the more reasonable when it comes to collecting meaningful data for non-standardized languages. In Section 5., we present the existing initiatives to crowdsource text corpora. Based on our own experiments and on the result of a survey regarding the digital use of a nonstandardized language, we explain why collecting this particular type of resource is challenging. 2. increasing number of speakers (see for instance, the studies on specific languages carried by Rivron (2012) or Soria et al. (2018)). Although linguistic communities are being threatened all over the world, this represents a valuable opportunity to observe, document and equip with appropriate tools an increasing number of languages. These new spaces of written conversation have been taken over by linguistic communities which practice had been mainly oral until then (van Esch et al., 2019). When no orthography has been defined for a given language, or when one (or various) conventions exist but are not consistently used by the speakers, spellings may vary from one speaker to another. Indeed, when the spellings are not stan"
2021.eacl-tutorials.4,J05-4006,0,0.112871,"requisites: Proficiency in English Tutorial Content This tutorial will cover the theory and practice of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing— reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—have raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference Table 1 presents a brief outline of the tutorial. Our aim is to provide enough options for hands-on experience and smaller-group activities in breakout rooms. 1.1 Reading List • Kenneth Church. 2005. Last words: Reviewing the reviewers. Computational Linguistics, 31(4):575–578 • Button K. S., Bal L., Clark A., and Shipley T. 2016. Preventing the ends from justifying the means: withholding results to address publication bi"
2021.eacl-tutorials.4,J08-1008,0,0.12302,"Missing"
2021.eacl-tutorials.4,J07-4009,0,0.0747782,"e context of widening the NLP community, researchers joining the field might not have the opportunity to practise reviewing. This tutorial fills in this gap by providing an opportunity to learn the basics of reviewing. Also more experienced researchers might find this tutorial interesting to revise their reviewing procedure. 1 Type: Introductory Structure: see Table 1 Prerequisites: Proficiency in English Tutorial Content This tutorial will cover the theory and practice of reviewing research in natural language processing. As has been pointed out for years by leading figures in our community (Webber, 2007), researchers in the ACL community face a heavy—and growing— reviewing burden. Initiatives to lower this burden have been discussed at the recent ACL general assembly in Florence (ACL 2019)1 . Simultaneously, notable “false negatives”—rejection by our conferences of work that was later shown to be tremendously important after acceptance by other conferences (Church, 2005)—have raised awareness of the fact that our reviewing practices leave something to be desired. . . and we do not often talk about “false positives” with respect to conference Table 1 presents a brief outline of the tutorial. O"
C08-3003,C04-1044,1,0.86677,"rsing process as it is used to control syntactic composition. This principle can also be used to filter lexical selections. For a input sentence, a lexical selection is a choice of an elementary tree from the anchored grammar for each word of the sentence. Indeed, the number of possible lexical selections may present an exponential complexity in the length of the sentence. A way of filtering them consists in abstracting some information from the initial formalism F to a new formalism Fabs . Then, parsing in Fabs allows to eliminate wrong lexical selections at a minimal cost (Boullier, 2003). (Bonfante et al., 2004) shows that polarities allow original methods of abstraction. Following this idea, the lexical disambiguation module checks the global neutrality of every lexical selection for each polarized feature: a set of trees bearing negative and positive polarities can only be reduced to a neutral tree if the sum of the negative polarities for each feature equals the sum of its positive polarities. Counting the sum of positive and negative features can be done in a compact way by using an automaton. This automaton structure allows to share all paths that have the same global polarity balance (Bonfante"
C08-3003,W03-3006,0,0.0195939,"echanism in the parsing process as it is used to control syntactic composition. This principle can also be used to filter lexical selections. For a input sentence, a lexical selection is a choice of an elementary tree from the anchored grammar for each word of the sentence. Indeed, the number of possible lexical selections may present an exponential complexity in the length of the sentence. A way of filtering them consists in abstracting some information from the initial formalism F to a new formalism Fabs . Then, parsing in Fabs allows to eliminate wrong lexical selections at a minimal cost (Boullier, 2003). (Bonfante et al., 2004) shows that polarities allow original methods of abstraction. Following this idea, the lexical disambiguation module checks the global neutrality of every lexical selection for each polarized feature: a set of trees bearing negative and positive polarities can only be reduced to a neutral tree if the sum of the negative polarities for each feature equals the sum of its positive polarities. Counting the sum of positive and negative features can be done in a compact way by using an automaton. This automaton structure allows to share all paths that have the same global po"
C08-3003,P06-1018,0,0.489469,"Missing"
C08-3003,C96-2120,0,0.582231,"Missing"
C12-1055,J11-4004,0,0.141961,"o organize an annotation campaign but not to that of the annotation task per se. Note also that the analysis we propose here applies whatever the level of expertise of the annotators. Besides, the annotation guide is recognized as the keystone of annotation campaigns, as it defines what should be annotated. Here, we consider that the need is clearly defined and known from all the participants. 897 Annotation evaluation Studies concerning the evaluation of the quality of manual annotation allowed to identify some factors influencing inter- and intra-annotator agreements. (Gut and Bayerl, 2004; Bayerl and Paul, 2011) demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated (the larger the number of categories, the lower the inter-annotator agreement) and that the categories prone to confusions are in limited number. This brings out two complexity dimensions related to the number of categories and to the ambiguity between some categories. This ambiguity-related complexity dimension also appears in (Popescu-Belis, 2007). In the study of the inter-annotator agreements, (Krippendorff, 2004) identified a step of identification of the elements to annotate that is"
C12-1055,cook-stevenson-2010-automatically,0,0.0220103,"ion task can be decomposed into two or more EATs if the tagset itself is made of smaller independent ones. 2 This decomposition in EATs is formal in the sense that it is independent of the pragmatic organization of the work: the annotators can handle different EATs as separate steps on the source signal or all at once depending on the specific nature of the work and the tools they use. The decomposition in EATs does not result in a simplification of the original task as it is often the case for the Human Intelligence Tasks (HITs) to be performed by Turkers (workers) in Amazon Mechanical Turk (Cook and Stevenson, 2010). To take a simple example, the annotation of gene renaming relations can be analyzed as a combination of two EATs. The first one identifies the gene names in the source signal. The second one relies on the first level of annotation and indicates which of the genes hold in a renaming relation. Obviously, the annotators can add both types of annotations at the same time, but the tagsets are independent and it is easier, from a formal point of view, to analyze the annotation task as a combination of two EATs than as a unique, but complex one. 3.2 What to annotate? Localizing the units to be anno"
C12-1055,W09-3002,0,0.33341,"plexity. In the simplest case, the choice is boolean and annotating amounts to assigning the discriminated units into two categories:5 sentences may be marked as relevant or not; the occurrences of the it pronoun are marked as anaphoric or impersonal. However, the choice is often more open, for instance for representing the diversity of morphosyntactic units, annotating syntactic dependencies or typing named entities. For the richest annotations, structured labels are often proposed: the annotator adds several labels on a single given unit, the combination of which forms the final annotation (Dandapat et al., 2009). Finally, there are annotation tasks for which the choice of a label is entirely left to the annotator, as in speech transcription, where there may be as many labels as words. In such cases, we consider that we have a huge tagset, even though the annotation effort is probably of a slightly different nature for the annotator. If an annotation A is formed of a sequence of m labels (A = E1 E2 . . . Em ) and each label Ei can take ni different values, the complete tagset theoretically contains n different labels, with n = n1 ∗ n2 ∗ . . . ∗ nm . However, in practice, constraints are defined which"
C12-1055,P03-1068,0,0.0166699,"2.1 Discrimination In some annotation experiments, the question of “what to annotate” is straightforward, for instance when the units to annotate have already been marked in an automatic pre-annotation phase or when all the units are to be annotated, as in a POS-tagging task. However, for the annotators, the corpus is often a haystack within which they must find what to annotate, and discriminating what should be annotated from what should not is a complex task. Identifying the units on which the annotation work should focus is all the more complex as the units to consider are heterogeneous. (Erk et al., 2003) emphasizes that semantic role annotation and discourse annotation mix several levels of segmentation (from less than a word to more than a sentence). As a simple example, it is easier to identify in a text negatively connoted adverbs, in particular, than all the negative expressions, as the latter can be words, phrases, or even entire parts of a text. In the second case several segmentation levels are actually to be considered. This gives a first scale of difficulty. An annotation task is considered difficult to the extent that the discrimination factor, defined by the following formula, is h"
C12-1055,2010.jeptalnrecital-long.35,1,0.839263,"gene names, in our example, are simple tokens. On the contrary, the characterizing factors are low: the tagset is boolean (Dimension=0), a type language is used (Expressiveness=0.25) and ambiguity is very low as only few gene names are also common names (theoretical ambiguity can be approximated at 0.019 and residual ambiguity is on average of 0.04 for two annotators). In this case, the context is the highest factor as it is often necessary to read the whole PubMed abstract to understand the role of a mentioned entity and as annotators sometimes consult external resources (context weights 1) (Fort et al., 2010). This first EAT is represented on the same graph as the pronoun classification, thus enabling the comparison of the two tasks (see Figure 1a). If they both show little complexity on three dimensions (delimitation, expressiveness, tagset dimension), the first one (pronouns) presents a high ambiguity dimension and no discrimination problem, while the second one (gene names) shows high complexity levels on the discrimination and context dimensions and no ambiguity. The solutions to alleviate the costs of these campaigns should therefore be adapted (pre-annotation and easy access to context for g"
C12-1055,W10-1807,1,0.881481,"0.02 for the two annotators), but the context is high, as in the previous case. Figure 1b shows how the two EATs are combined to provide an analysis of the complexity of the whole task, which proves to be focused on discrimination and context. 4 Validation and illustration 4.1 Experimental validation Although, as we showed in section 2, this grid of analysis has never been identified as such, some existing results or experiments confirm its applicability. One first example of this is related to discrimination. In experiments led on the effects of pre-annotation on POS-tagging, the authors of (Fort and Sagot, 2010) showed that if the automatic pre-annotation is of very good quality, i.e. if only few tokens disseminated within the text have to be corrected, very good annotators can end up with less good annotation results, due to lapses in their concentration. This corresponds directly to the discrimination dimension we present here, which tends to get higher if the annotations to perform are submerged within the text. Also, it has been observed that the quality of the annotation decreases with the number of tags involved (Bayerl and Paul, 2011). Structuring the tagset allows to reduce the degree of free"
C12-1055,W11-0411,1,0.885766,"oner in the process. In the context of the gene renaming annotation campaign, we discovered that in some cases, the annotators needed the whole text to make their final decisions, and not only the abstract they had to annotate, as it was initially planned. If this need could have been identified beforehand, it would have been taken into account and the annotation tool would have been parameterized to give an easy access to the whole documents to annotators. 4.2 Example: structured named entities We applied this analysis on a structured named entity annotation campaign for French described in (Grouin et al., 2011). Structured named entities Within the Quaero program10 , a new definition of structured named entities was proposed. This new structure relies on two principles: the entities are both hierarchical and compositional. An entity is then composed of two kinds of elements: the 7 types (and 32 subtypes) which refer to a general segmentation of the world into major categories and the 31 components which allow to tag every word of the entity expression. Following this definition, the annotation campaign can be decomposed into two EATs: types and components. Figure 2 illustrates this definition on a F"
C12-1055,ide-romary-2006-representing,0,0.024176,"y have been, in particular, inherited from corpus linguistics. (Leech, 1993, 2005) present a list of what a specification for annotation should contain. It represents an effort toward the identification of several levels of difficulty encountered when making decisions during the manual annotation of a corpus: segmentation, inclusion of units (words or phrases) within other units (clauses or sentences), assigning categories to some textual fragments. Annotation formats Recommendations were published within the framework of the standardization effort of the annotation formats, in particular in (Ide and Romary, 2006), that finally gave birth to the ISO 24612 standard. In this view, the authors are little concerned by manual annotation difficulties as such but, to represent annotations they identify complex structuring elements which are of interest here : segmentation (identification of continuous or discontinuous sequences of characters), several layers of annotations (for example morpho-syntactic, then syntactic), relations between these layers, overlapping issues. Organization of annotation campaigns An overall schema of the organization of annotation campaigns has also emerged. It involves several act"
C12-1055,W11-1810,1,0.882881,"Missing"
C12-1055,kaplan-etal-2010-annotation,0,0.0361697,"Missing"
C12-1055,J93-2004,0,0.0603872,"e of NLP technologies on some oral or written discourse. This corresponds to a great diversity of phenomena, as these annotations vary in nature (phonetic, morpho-syntactic, semantic or task-oriented labels), in the range they cover (they can concern a couple of characters, a word, a paragraph or a whole text), in their degree of coverage (all the text is annotated or only a part of it) and in their form (atomic value, complex feature structures or relations and even cross-document alignment relations). It has been a long road since the big pioneer annotation campaigns like the Penn Treebank (Marcus et al., 1993), but one problem remains: manual annotation is expensive. Various strategies have been implemented to reduce or control annotation costs. Tools have been developed to assist and guide the work of annotators. Automatic annotation methods, sometimes based on machine learning, have been introduced to relieve the annotator of the most trivial and repetitive work and to allow him/her to focus on the hardest annotation tasks where human interpretation is critical. For simple tasks, the use of crowdsourcing is developed with the idea of dividing up tedious work and exploiting the number of annotator"
C12-1055,J05-1004,0,0.0345415,"ity of the annotation task are correlated (the larger the number of categories, the lower the inter-annotator agreement) and that the categories prone to confusions are in limited number. This brings out two complexity dimensions related to the number of categories and to the ambiguity between some categories. This ambiguity-related complexity dimension also appears in (Popescu-Belis, 2007). In the study of the inter-annotator agreements, (Krippendorff, 2004) identified a step of identification of the elements to annotate that is called “unitizing”. Similarly, in the Proposition Bank project (Palmer et al., 2005), the organizers separated role “identification” from role “classification” to compute the inter-annotator agreement, in order to “isolate the role classification decisions” from the (supposedly easier) identification. 2.3 Insights from Cognitive Science Few publications focus on the difficulties of manual annotation. In (Tomanek et al., 2010), the authors used an eye-tracking device to analyze and model the annotation cognitive complexity. Their experiment was carried out on a simple named-entity annotation task (persons, locations and organizations), with some pre-identification of complex n"
C12-1055,W12-3606,1,0.757456,"necessary to validate a choice by exploring external data (such as Wikipedia). Delimitation Discrimination Expressiveness Context Weight Tagset dimension Ambiguity Figure 3: Synthesis of the complexity for the structured named entities campaign (2 EATs, double scale) This analysis validates the choice of the hierarchical tagset that has been done for the annotation campaign described in(Grouin et al., 2011). Had a flat tagset been chosen, the dimension score would have been 1 (ν = 61). Moreover, this analysis is in line with what was observed concerning the context weight within the campaign(Rosset et al., 2012). Of course, it presents some limits as it has been shown for the ambiguity score computation. Conclusion The grid of analysis we propose here should be used as part of the preparatory work of any annotation campaign, large or small. Obviously, when done a priori, on a small sample of annotations, the results will be approximate, but we believe that the analysis itself helps asking the right questions and finding the appropriate solutions. Obviously, this pre-campaign work should be supported by an appropriate tool, so that this grid of analysis is computed more or less automatically. We are w"
C12-1055,P10-1118,0,0.0263219,"y dimension also appears in (Popescu-Belis, 2007). In the study of the inter-annotator agreements, (Krippendorff, 2004) identified a step of identification of the elements to annotate that is called “unitizing”. Similarly, in the Proposition Bank project (Palmer et al., 2005), the organizers separated role “identification” from role “classification” to compute the inter-annotator agreement, in order to “isolate the role classification decisions” from the (supposedly easier) identification. 2.3 Insights from Cognitive Science Few publications focus on the difficulties of manual annotation. In (Tomanek et al., 2010), the authors used an eye-tracking device to analyze and model the annotation cognitive complexity. Their experiment was carried out on a simple named-entity annotation task (persons, locations and organizations), with some pre-identification of complex noun phrases containing at least one potential named entity. They measured the influence of the syntactic and semantic complexities1 as well as the size of context that was used by the annotators. The results show that the annotation performance tends on average to “correlate with the [semantic] complexity of the annotation phrase” and less so"
C12-2079,J08-4004,0,0.650981,"us (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state"
C12-2079,J11-4004,0,0.0379275,"e obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number of categories, the lower the inter-annotator agreement. However, categories prone to confusion are in limited number. The meta-analysis presented by Bayerl and Paul (2011) extends this research on the factors influencing agreement results, identifying 8 such factors and proposing useful recommendations to improve manual annotation reliability. However, neither of these studies provides a clear picture of the behavior of the agreement coefficients 0 This work has been partially financed by OSEO, the French State Agency for Innovation, under the Quaero program. 810 or of their meanings. The experiments detailed in (Reidsma and Carletta, 2008) constitute an interesting step in this direction, focusing on the effect of annotation errors on machine learning systems"
C12-2079,fort-etal-2012-analyzing,1,0.700025,"present the pros and cons of these methods, from the statistical and mathematical points of view, with some hints about specific issues raised in some annotation campaigns, like the prevalence of one category. A section of their article is dedicated to various attempts at providing an interpretation scale for the Kappa family coefficients and how they failed to converge. Works such as (Gwet, 2012) are also to be mentioned. They present various inter-rater reliability coefficients and insist on benchmarking issues related to their interpretation. Many authors, among whom (Grouin et al., 2011; Fort et al., 2012), tried to obtain a more precise assessment of the quality of the annotation in their campaigns by computing different coefficients and analyzing the obtained results. However, their analyses lack robustness, as they only apply to similar campaigns. Other studies concerning the evaluation of the quality of manual annotation identified some factors that influence inter- and intra-annotator agreements, thereby giving clues on their behavior. Gut and Bayerl (2004) thus demonstrated that the inter-annotator agreement and the complexity of the annotation task are correlated: the larger the number o"
C12-2079,W11-0411,1,0.934731,"nterannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can be of many types and the well-known Kappa-family is described in details in (Artstein and Poesio, 2008). However, as pointed out by the authors of this article, the obtained results are difficult to interpret. Kappa coefficients, for example, are difficult to compare, even within the same annotation task, as they imply a definition of the markables that can vary from one campaign to the other (Grouin et al., 2011). More generally, we lack clues to know if a Kappa of 0.75 is a “good” result, or if a Kappa of 0.8 is twice as good as one of 0.4 or if a result of 0.6 obtained using one coefficient is better than 0.5 with another one, and for which annotation task. We first briefly present the state of the art (Section 2), then detail the principles of our method to benchmark measures (Section 3) and show on some examples how different coefficients can be compared (Section 4). We finally discuss current limitations and point out future developments. 2 State of the art A quite detailed analysis of the most c"
C12-2079,J02-1002,0,0.272818,"on the contrary, a reference element is missing (false negative). All of these error paradigms tend to damage the annotations, so each of them should be taken into account by agreement measures. We propose here to apply each measure to a set of corpora, each of which embeds errors from one or more paradigms, and with a certain magnitude (the higher the magnitude, the higher the number of errors). This experiment should allow us to observe how the measures behave w.r.t. the different paradigms, and with a full range of magnitudes. The idea of creating artificial damaged corpora is inspired by Pevzner and Hearst (2002), then Bestgen (2009) in thematic segmentation, but our goal (giving meaning to measures) and our method (e.g. applying progressive magnitudes) are very different. 3.2 Protocol Reference. A reference annotation set (called reference) is provided to the system: a true Gold Standard or an automatically generated set based on a statistical model. It is assumed to correspond exactly to what annotations should be, with respect to the annotation guidelines. Shuffling. A shuffling process is an algorithm that automatically generates a multi-annotated corpus given three parameters: a reference annotat"
C12-2079,J08-3001,0,0.379365,", and the obtained results are used to model the behavior of these measures and understand their actual meaning. KEYWORDS: inter-annotator agreement, manual corpus annotation, evaluation. Proceedings of COLING 2012: Posters, pages 809–818, COLING 2012, Mumbai, December 2012. 809 1 Introduction The quality of manual annotations has a direct impact on the applications using them. For example, it was demonstrated that machine learning tools learn to make the same mistakes as the human annotators, if these mistakes follow a certain regular pattern and do not correspond to simple annotation noise (Reidsma and Carletta, 2008; Schluter, 2011). Furthermore, errors in a manually annotated reference corpus (a “gold-standard”) can obviously bias an evaluation performed using this corpus as a reference. Finally, a bad quality annotation would lead to misleading clues in a linguistic analysis used to create rule-based systems. However, it is not possible to directly evaluate the validity of manual annotations. Instead, interannotator agreement measures are used: at least two annotators are asked to annotate the same sample of text in parallel, their annotations are compared and a coefficient is computed. The latter can"
C16-1286,W04-3202,0,0.053824,"e Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human annotators. This is the case with microworking crowdsourcing, especially through the use of platforms like Amazon Mechanical Turk, via which the workers are (micro)paid to perform simplified tasks (Snow et al., 2008). Apart from the ethical issues raised by these platforms (detailed in (Fort et al., 2011)), microworking platforms d"
C16-1286,D15-1075,0,0.036927,"o.org/. This work is licenced under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/ 2 See (Church, 2011) for an in-depth reflection on the subject. License details: http:// 3041 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3041–3052, Osaka, Japan, December 11-17 2016. could be simplified by presenting only one pair of sentences and a binary response to a single question, such as “Would most people say that if the first sentence is true, then the second sentence must be true?” (Bowman et al., 2015). To paraphrase (Dandapat et al., 2009), it seems that there is no easy escape from the high cost of complex linguistic annotation. We present here an on-line game that enables the production of quality annotations of complex phenomena (here, dependency syntax), tested on French. The produced corpus is constantly growing and is designed to be (i) completely free and available, and (ii) of sufficient quality. We first examine the existing treebanks for French and their limitations and detail some previous experiments with Games with a Purpose (GWAPs). Then we present the game we developed and t"
C16-1286,F12-2024,0,0.0219807,"resourced, primarily due to legal issues: lexicons and annotated corpora existed but could not be used or redistributed freely. This is in particular the case for the French Treebank (FTB or corpus arbor´e de Paris 7) (Abeill´e et al., 2003), which is available for research purposes only and cannot be freely redistributed.3 Several versions are reported in the literature, with the size varying from 12,351 sentences and 350,947 words (for the FTBUC) to 18,535 sentences and 557,149 words4 (for the FTB-SPRML). To try and circumvent this restriction, Candito and Seddah created the Sequoia corpus (Candito and Seddah, 2012), which is freely available5 under a LGPL-LR license, but is limited to 67,038 tokens. The same authors developed an additional question bank with 23,236 tokens (Seddah and Candito, 2016). Both corpora use the same annotation guide and set of relations as the FTB. A Universal Dependency corpus (McDonald et al., 2013) was created for French and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain d"
C16-1286,carmen-etal-2010-tag,0,0.0255639,"wever, the creation of such resources is notoriously costly, especially when complex annotations, e.g. for dependency syntax, are at issue. For example, the cost of the Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human annotators. This is the case with microworking crowdsourcing, especially through the use of platforms like Amazon Mechanical Turk, via which the workers are ("
C16-1286,W09-3002,0,0.113493,"vailability of manually annotated corpora of high quality (or, at least, reliability) is therefore key to the development of the field in any given language. However, the creation of such resources is notoriously costly, especially when complex annotations, e.g. for dependency syntax, are at issue. For example, the cost of the Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human"
C16-1286,W15-2204,1,0.858222,"Missing"
C16-1286,hana-hladka-2012-getting,0,0.0546123,"Missing"
C16-1286,Q14-1035,0,0.0388699,"Missing"
C16-1286,lacheret-etal-2014-rhapsodie,0,0.0261395,"he same annotation guide and set of relations as the FTB. A Universal Dependency corpus (McDonald et al., 2013) was created for French and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain drawbacks, for example, it does not distinguish between arguments and modifiers for nominal complements of verbs. Other treebanks exist for French, but they either concern spoken language, as with Rhapsodie (Lacheret et al., 2014) or the oral Treebank (Abeill´e and Crabb´e, 2013), or specific language types, like the Social Media Treebank (Seddah et al., 2012). This situation (in which references exist, but a large, fully available, manually annotated corpus is lacking) makes dependency syntax for French an appropriate candidate for testing a new paradigm for complex linguistic resource development: the use of on-line Games with a Purpose. 2.2 Playing to Create Language Resources Games with a Purpose are games in which participants, knowingly or not, create data by playing. They are not serious games as such, as their"
C16-1286,J93-2004,0,0.053289,"d a message revealing the right answer parvient (reaches) are displayed. An advantage of offering a separate T RAINING for each relation is that the player does not have to wait long before starting the game. Once s/he is connected to the game, only a few minutes are required before starting actual play and production of annotations. 3.2.2 Play phase In the general mode, the player chooses a relation from those available and a sequence of ten questions is proposed. The questions vary as follows: 15 Insight concerning the complexity of syntactic annotation for the Penn Treebank is provided in (Marcus et al., 1993), where the learning curve for syntax was estimated to be twice that for part-of-speech (two months vs one). 16 This is much like in an annotation guide but with simpler vocabulary and fewer details. 3044 REFEval Play phase TRAINING CONTROL (feedback) (feedback) EVAL Eval Unannotated corpus (Wikipedia) Ref corpus (Sequoia) REFTrain & Control Training phase (no feedback) Player’s confidence EXPEval Pre annotation with 2 parsers ANNOTATION Raw text (no feedback) EXPGame Figure 1: Organization of the different mechanisms and corpora • For relations where the dependent element tends to be unique f"
C16-1286,L16-1375,0,0.0362327,"or corpus arbor´e de Paris 7) (Abeill´e et al., 2003), which is available for research purposes only and cannot be freely redistributed.3 Several versions are reported in the literature, with the size varying from 12,351 sentences and 350,947 words (for the FTBUC) to 18,535 sentences and 557,149 words4 (for the FTB-SPRML). To try and circumvent this restriction, Candito and Seddah created the Sequoia corpus (Candito and Seddah, 2012), which is freely available5 under a LGPL-LR license, but is limited to 67,038 tokens. The same authors developed an additional question bank with 23,236 tokens (Seddah and Candito, 2016). Both corpora use the same annotation guide and set of relations as the FTB. A Universal Dependency corpus (McDonald et al., 2013) was created for French and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain drawbacks, for example, it does not distinguish between arguments and modifiers for nominal complements of verbs. Other treebanks exist for French, but they either concern spoken language,"
C16-1286,C12-1149,0,0.0223579,"and is freely available under a CC BY-NC-SA license. In version 1.3, released in May 2016, it contains 401,960 tokens, but it ”has not been manually corrected systematically”.6 Moreover, the annotation format for Universal Dependencies suffers from certain drawbacks, for example, it does not distinguish between arguments and modifiers for nominal complements of verbs. Other treebanks exist for French, but they either concern spoken language, as with Rhapsodie (Lacheret et al., 2014) or the oral Treebank (Abeill´e and Crabb´e, 2013), or specific language types, like the Social Media Treebank (Seddah et al., 2012). This situation (in which references exist, but a large, fully available, manually annotated corpus is lacking) makes dependency syntax for French an appropriate candidate for testing a new paradigm for complex linguistic resource development: the use of on-line Games with a Purpose. 2.2 Playing to Create Language Resources Games with a Purpose are games in which participants, knowingly or not, create data by playing. They are not serious games as such, as their main purpose is not to train people, but to produce data (such as annotations, lexicon entries, image labels, etc). GWAPs for NLP ar"
C16-1286,W13-2304,0,0.0226836,"especially when complex annotations, e.g. for dependency syntax, are at issue. For example, the cost of the Prague Dependency Treebank was estimated at $600,000 in (B¨ohmov´a et al., 2001). Over the years, many solutions have been investigated in the attempt to lower manual annotation costs. One obvious avenue is to use an appropriate annotation tool, as shown for example in (Dandapat et al., 2009). As a complementary aid, NLP systems can be used to reduce the annotation burden – either beforehand, for example with tag dictionaries (Carmen et al., 2010) and more generally with preannotation (Skjærholt, 2013), or more iteratively during the annotation process, for instance through active learning (Baldridge and Osborne, 2004). These solutions have proved efficient and have indeed helped reduce the annotation cost, but the creation of a large annotated corpus in the traditional manner remains very expensive. Another way to address the issue is simply to limit the amount paid to the human annotators. This is the case with microworking crowdsourcing, especially through the use of platforms like Amazon Mechanical Turk, via which the workers are (micro)paid to perform simplified tasks (Snow et al., 200"
C16-1286,D08-1027,0,0.14295,"Missing"
C16-1286,W13-0215,0,0.0473477,"Missing"
C16-1286,J11-2010,1,\N,Missing
candito-etal-2014-deep,candito-etal-2010-statistical,1,\N,Missing
candito-etal-2014-deep,de-marneffe-etal-2006-generating,0,\N,Missing
candito-etal-2014-deep,J93-2004,0,\N,Missing
candito-etal-2014-deep,W09-4624,0,\N,Missing
candito-etal-2014-deep,H94-1020,0,\N,Missing
candito-etal-2014-deep,P05-1011,0,\N,Missing
candito-etal-2014-deep,P04-1041,0,\N,Missing
candito-etal-2014-deep,W00-1436,0,\N,Missing
candito-etal-2014-deep,J05-1004,0,\N,Missing
candito-etal-2014-deep,W13-3724,0,\N,Missing
candito-etal-2014-deep,abeille-barrier-2004-enriching,0,\N,Missing
couillault-etal-2014-evaluating,sagot-2010-lefff,0,\N,Missing
couillault-etal-2014-evaluating,J93-2004,0,\N,Missing
couillault-etal-2014-evaluating,2005.mtsummit-papers.11,0,\N,Missing
couillault-etal-2014-evaluating,hajic-etal-2012-announcing,0,\N,Missing
couillault-etal-2014-evaluating,calzolari-etal-2012-lre,0,\N,Missing
couillault-etal-2014-evaluating,choukri-etal-2012-using,0,\N,Missing
F12-2008,2004.jeptalnrecital-recital.6,1,0.748575,"Missing"
F12-2008,Y09-1013,1,0.893627,"Missing"
F12-2008,W10-1807,1,0.883522,"Missing"
F12-2008,J93-2004,0,0.04017,"Missing"
F12-2008,W04-2104,0,0.0666194,"Missing"
F12-2008,sagot-2010-lefff,1,0.897512,"Missing"
F12-2031,J96-2004,0,0.158413,"Missing"
F12-2031,W09-3002,0,0.0277441,"Missing"
F12-2031,fort-claveau-2012-annotating,1,0.809654,"Missing"
F12-2031,W11-0411,1,0.870748,"Missing"
F12-2031,2009.jeptalnrecital-court.23,0,0.0609105,"Missing"
F13-2016,W10-1804,0,0.0378841,"Missing"
F13-2016,F12-2024,0,0.0350345,"Missing"
F13-2016,J08-3001,0,0.0561746,"Missing"
F13-2016,scott-etal-2012-corpus,0,0.0258017,"Missing"
F14-1026,Y09-1013,0,0.0293152,"Missing"
F14-2031,candito-etal-2010-statistical,1,0.895459,"Missing"
F14-2031,candito-etal-2014-deep,1,0.70798,"Missing"
F14-2031,F12-2024,1,0.886133,"Missing"
F14-2031,J05-1004,0,0.189665,"Missing"
fort-claveau-2012-annotating,W09-3025,1,\N,Missing
fort-claveau-2012-annotating,J08-4004,0,\N,Missing
fort-claveau-2012-annotating,J96-2004,0,\N,Missing
fort-claveau-2012-annotating,W11-0411,1,\N,Missing
fort-etal-2012-analyzing,passonneau-2006-measuring,0,\N,Missing
fort-etal-2012-analyzing,J08-3001,0,\N,Missing
fort-etal-2012-analyzing,J08-4004,0,\N,Missing
fort-etal-2012-analyzing,J96-2004,0,\N,Missing
fort-etal-2012-analyzing,galibert-etal-2010-named,1,\N,Missing
fort-etal-2012-analyzing,alex-etal-2006-impact,0,\N,Missing
fort-etal-2012-analyzing,W10-1804,0,\N,Missing
guillaume-etal-2014-mapping,sagot-2010-lefff,0,\N,Missing
guillaume-etal-2014-mapping,2010.jeptalnrecital-long.32,0,\N,Missing
J11-2010,W10-1806,0,0.0141013,"ost signiﬁcant drawback” of MTurk, as, in their context of annotating noun compound relations using a large taxonomy, “it is impossible to force each Turker to label every data point without putting all the terms onto a single Web page, which is highly impractical for a large taxonomy. Some Turkers may label every compound, but most do not.” They also note that ”while we requested that Turkers only work on our task if English was their ﬁrst language, we had no method of enforcing this.” Finally, they note that “Turker annotation quality varies considerably.” Another important point is made in Bhardwaj et al. (2010), where it is shown that, for their task of word sense disambiguation, a small number of trained annotators are superior to a larger number of untrained Turkers. On that point, their results contradict that of Snow et al. (2008), whose task was much simpler (the number of senses per word was 3 for the latter, versus 9.5 for the former). The difﬁculty of having Turkers perform complex tasks also appears in Gillick and Liu (2010, page 148), an article from the proceedings of the NAACL-HLT 2010 Workshop on Amazon Mechanical Turk, in which non-expert evaluation of summarization systems is proved t"
J11-2010,P08-1092,0,0.0254454,"Missing"
J11-2010,W10-0701,0,0.241805,"istics Volume 37, Number 2 Figure 1 Evolution of MTurk usage in NLP publications. MTurk is composed of two populations: the Requesters, who launch the tasks to be completed, and the Turkers, who complete these tasks. Requesters create the so-called “HITs” (Human Intelligence Tasks), which are elementary components of complex tasks. The art of the Requesters is to split complex tasks into basic steps and to ﬁx a reward, usually very low (for instance US$0.05 to translate a sentence). Using the MTurk paradigm, language resources can be produced at a fraction (1/10th at least) of the usual cost (Callison-Burch and Dredze 2010). MTurk should therefore not be considered to be a game. Although it is superﬁcially similar to Phrase Detectives, in that case the gain is not emphasized (only the best contributors gain a prize, which consists of Amazon vouchers). The same applies to the French-language JeuxDeMots (“Play on Words”), which does not offer any prize (Lafourcade 2007), and to Phrase Detectives (Chamberlain, Poesio, and Kruschwitz 2008), in which the gain is not emphasized (only the best contributors gain a prize). MTurk is not a game or a social network, it is an unregulated labor marketplace: a system which del"
J11-2010,W10-0722,0,0.00773373,"as their ﬁrst language, we had no method of enforcing this.” Finally, they note that “Turker annotation quality varies considerably.” Another important point is made in Bhardwaj et al. (2010), where it is shown that, for their task of word sense disambiguation, a small number of trained annotators are superior to a larger number of untrained Turkers. On that point, their results contradict that of Snow et al. (2008), whose task was much simpler (the number of senses per word was 3 for the latter, versus 9.5 for the former). The difﬁculty of having Turkers perform complex tasks also appears in Gillick and Liu (2010, page 148), an article from the proceedings of the NAACL-HLT 2010 Workshop on Amazon Mechanical Turk, in which non-expert evaluation of summarization systems is proved to be “not able to recover system rankings derived from experts.” Even more interestingly, Wais et al. (2010) show that standard machine learning techniques (in their case, a naive Bayes classiﬁer) can outperfom the Turkers on a categorization task (classifying businesses into Automotive, Health, Real Estate, etc.). Therefore, in some cases, NLP tools already do better than MTurk. Finally, as we said earlier, the vast majority"
J11-2010,N10-1024,0,0.00718371,"third of the HITs are performed by Turkers who need MTurk to make basic ends meet, you then have to admit that MTurk is, at least for them, a labor marketplace. Moreover, the frequent assumption that the low rewards are a result of the classical law of supply-and-demand (large numbers of Turkers means more supply of labor and therefore lower acceptable salaries) is false. Firstly, we do not observe that there are too many Turkers. In fact, there are not enough Turkers. This can be observed through the difﬁculty in ﬁnding Turkers with certain abilities (e.g., understanding a speciﬁc language [Novotney and Callison-Burch 2010]), and in the difﬁculty in performing very large HIT groups (Ipeirotis 2010a). This is not surprising, as we have seen that the number of active Turkers is not that large. Secondly, the low cost is a result of the Requesters’ view of the relation between quality and reward: Many articles (e.g., Marge, Banerjee, and Rudnicky 2010) relate that there is no correlation between the reward and the ﬁnal quality. The reason is that increasing the price is believed to attract spammers (i.e., Turkers who cheat, not really performing the job, but using robots or answering randomly), and these are numero"
J11-2010,D08-1027,0,0.241226,"Missing"
J11-2010,P10-1070,0,0.00669572,"urk (35 papers)—the existence of which is, in itself, strong evidence of the importance of the use of MTurk in the domain. A vast majority of papers present small to medium size experiments where the authors have been able to produce linguistic resources or perform evaluations at a very low cost; at least for transcription and translation, the quality is sufﬁcient to train and evaluate statistical translation/transcription systems (Callison-Burch and Dredze 2010; Marge, Banerjee, and Rudnicky 2010). Some of these papers, however, bring to light language resource quality problems. For example, Tratz and Hovy (2010, page 684) note that the user interface limitations constitute “[t]he ﬁrst and most signiﬁcant drawback” of MTurk, as, in their context of annotating noun compound relations using a large taxonomy, “it is impossible to force each Turker to label every data point without putting all the terms onto a single Web page, which is highly impractical for a large taxonomy. Some Turkers may label every compound, but most do not.” They also note that ”while we requested that Turkers only work on our task if English was their ﬁrst language, we had no method of enforcing this.” Finally, they note that “Tu"
L16-1252,couillault-etal-2014-evaluating,1,0.898803,"Missing"
L16-1252,J11-2010,1,0.869278,"ute to our colleague Jean Véronis, who died recently and spent a lot of time and energy making accessible to the public the subtleties of some research issues in NLP through his well-known blog4 , which he created in 2004. 1.2. (Some) Recent Actions Recently, some ethical concerns were raised concerning Amazon Mechanical Turk, in particular regarding the absence of a clear relation between the Requesters (people, including researchers, proposing a task) and the Turkers (workers), preventing the latter from any possibility of legal action from wrongdoings by the former (Adda and Mariani, 2010; Fort et al., 2011; Fort et al., 2014). Another issue is that of the very low wages. In order to try and improve this situation, Chris Callison-Burch proposed a tool5 to help Turkers find higher paying jobs (CallisonBurch, 2014). The reflection started on Amazon Mechanical Turk led some of us to involve private and public bodies in the writing of an Ethics and Big Data Charter6 (Couillault et al., 2014), whose aim was to document as much as possible the building of language resources and, more generally, of data. The charter consists in a form, split into three sections, respectively dedicated to traceability,"
L18-1071,Q16-1022,0,0.044939,"Missing"
L18-1071,W10-1807,1,0.853447,"Missing"
L18-1071,J11-2010,1,0.777987,"y allows to distinguish between transparent, voluntary crowdsourcing (for instance, Wikipedia), microworking platforms which propose rather transparent tasks and a micro-remuneration (such as Amazon Mechanical Turk) and games with a purpose, that more or less hide the task being performed (like, for example, in Phrase Detectives4 (Poesio et al., 2013) or ZombiLingo5 (Guillaume et al., 2016)). Amazon Mechanical Turk has been used by many researchers, directly or through CrowdFlower, including to have POS tags annotation produced (Hovy et al., 2014). In addition to the ethical issues it raises (Fort et al., 2011), this kind of platform is not adapted to the languages we target, as very few (if not none) microworkers are fluent speakers of these languages. Besides, microworking platforms do not allow to train annotators, only to test them. Games with a purpose have proven efficient at getting good quality linguistic data at lower cost than traditional methods (Chamberlain et al., 2013). Yet, developing a fullfledged game is a long-term endeavor which requires a range of skills (Web development, gaming mechanisms knowledge, user experience design, advertising) and has to be made profitable in the long r"
L18-1071,N13-1014,0,0.0641892,"Missing"
L18-1071,C16-1286,1,0.851152,"contains around 50,000 words once very similar articles have been excluded. 455 ii) the awareness of the participants that they are producing data (as this can be hidden underneath a playful interface, for instance). This typology allows to distinguish between transparent, voluntary crowdsourcing (for instance, Wikipedia), microworking platforms which propose rather transparent tasks and a micro-remuneration (such as Amazon Mechanical Turk) and games with a purpose, that more or less hide the task being performed (like, for example, in Phrase Detectives4 (Poesio et al., 2013) or ZombiLingo5 (Guillaume et al., 2016)). Amazon Mechanical Turk has been used by many researchers, directly or through CrowdFlower, including to have POS tags annotation produced (Hovy et al., 2014). In addition to the ethical issues it raises (Fort et al., 2011), this kind of platform is not adapted to the languages we target, as very few (if not none) microworkers are fluent speakers of these languages. Besides, microworking platforms do not allow to train annotators, only to test them. Games with a purpose have proven efficient at getting good quality linguistic data at lower cost than traditional methods (Chamberlain et al., 2"
L18-1071,P14-2062,0,0.0207021,"idden underneath a playful interface, for instance). This typology allows to distinguish between transparent, voluntary crowdsourcing (for instance, Wikipedia), microworking platforms which propose rather transparent tasks and a micro-remuneration (such as Amazon Mechanical Turk) and games with a purpose, that more or less hide the task being performed (like, for example, in Phrase Detectives4 (Poesio et al., 2013) or ZombiLingo5 (Guillaume et al., 2016)). Amazon Mechanical Turk has been used by many researchers, directly or through CrowdFlower, including to have POS tags annotation produced (Hovy et al., 2014). In addition to the ethical issues it raises (Fort et al., 2011), this kind of platform is not adapted to the languages we target, as very few (if not none) microworkers are fluent speakers of these languages. Besides, microworking platforms do not allow to train annotators, only to test them. Games with a purpose have proven efficient at getting good quality linguistic data at lower cost than traditional methods (Chamberlain et al., 2013). Yet, developing a fullfledged game is a long-term endeavor which requires a range of skills (Web development, gaming mechanisms knowledge, user experience"
L18-1071,D12-1127,0,0.0345616,"Missing"
L18-1071,petrov-etal-2012-universal,0,0.0781175,"However, related tasks have been successfully achieved by volunteers, such as the annotation of suicide notes (Pestian et al., 2012) or text message translation in an humanitarian emergency context (Munro, 2013). Finally, there exist some generic platforms for citizen science, such as Crowd4U8 or Zooniverse9 , but none presents a linguistic application yet.10 3. 3.1. Methodology Tagset For the sake of adaptability, we chose to work with the universal POS tagset (see Appendix I), which synthesizes the tagsets of 22 languages and can easily be adapted to the needs of each languageintroduced by (Petrov et al., 2012).11 In fact, the only modification we initially made was to have the X category (“Others”, a catch-all category hard to in4 See: http://anawiki.essex.ac.uk/ phrasedetectives/. 5 See: http://www.zombilingo.org. 6 For more details, see (Lafourcade et al., 2015). 7 Such a platform might exist though, as some did not get any dedicated scientific publication, like LanguageQuiz: http: //quiz.ucomp.eu/. 8 See: https://crowd4u.org. 9 See: https://www.zooniverse.org. 10 A specific platform is under development as we write this paper: https://lingoboingo.org/. 11 See: http://universaldependencies.org/u/"
L18-1071,N03-1033,0,0.010673,"”: Elleb¨oje (OLCA), Elleboje (ACPA); Ellaboja ¨ (OLCA), Allabooga (ACPA). 3.3. Preprocessing of the Corpora The gathered texts were tokenized using a specific Python script, which we completed when cases of wrong segmentation –due to unknown spelling habits– were brought to our attention by the participants. For instance ’r can either be considered as a separated token when placed after a verb (e.g. h¨at’r, “he has”) or as part of a token containing an elided vowel (e.g. d’r, “the”). Both the training and evaluation corpora have been preannotated with two taggers: i) the Stanford POS Tagger (Toutanova et al., 2003) applied to the texts after 456 a transposition of grammatical words in German, following the methodology defined in (Bernhard and Ligozat, 2013). and ii) MElt (Denis and Sagot, 2012), that we regularly trained on the annotated corpus. These pre-annotations were used to provide suggestions to the participants: when the taggers disagree, the two categories they produce are proposed to the participants, while when the agree, the consensual tag can be directly validated: this fastens the annotation process on frequent and weakly ambiguous words and has led to an increase of annotated sequences du"
L18-1071,W14-5303,0,0.0717135,"Missing"
L18-1071,C16-1044,0,0.0602208,"Missing"
lafourcade-fort-2014-propa,J11-2010,1,\N,Missing
lux-pogodalla-etal-2010-fastkwic,P97-1004,0,\N,Missing
lux-pogodalla-etal-2010-fastkwic,W03-1802,0,\N,Missing
R19-1089,L18-1025,1,0.896139,"Missing"
R19-1089,dakota-kubler-2017-towards,0,0.0480764,"Missing"
R19-1089,Q17-1033,0,0.0541077,"Missing"
R19-1089,P13-1166,0,0.272847,"Missing"
R19-1089,D17-1076,0,0.0438822,"Missing"
R19-1089,E99-1046,0,0.432502,"Missing"
R19-1089,E17-4003,0,0.0452396,"Missing"
R19-1089,W17-1603,1,0.897332,"Missing"
R19-1089,C18-1097,0,0.0384908,"Missing"
R19-1089,W16-6110,1,0.903417,"Missing"
R19-1090,L18-1071,1,0.877877,"Missing"
R19-1090,L18-1619,0,0.0430345,"Missing"
R19-1090,petrov-etal-2012-universal,0,0.271512,"Missing"
R19-1090,W09-0303,0,0.0682371,"Missing"
R19-1090,A97-1016,0,0.252799,"g the adjustments described in section 4.2, i.e. (i) forcing the case of potential variants to match the case of the original OOV word, (ii) limiting the application of rules considering only left or right context to words which size is over four characters. • 7 were caused by erroneous matching we were not yet able to correct e.g. kr¨afti (“strongly”, adverb) / kr¨aftiger (“stronger”, adjective), mine (“mine”, determiner) / meine (“believe”, verb) etc. These results show that the generated variant pairs should be hand-checked, a task that can itself 782 Acknowledgments for work described in (Theron and Cloete, 1997), in which the rules are automatically extracted but with a known (morphological) goal. The closest work to ours is that described in (Barteld, 2017), as it focuses on detecting spelling variants in Middle Low German unrelated to a standard. Yet, the described method requires the training of a classifier to filter the generated pairs. This classifier is based on a resource that contains 1,834 pairs of spelling variants, a resource that is unavailable for most nonstandardized languages. Regarding Alsatian more specifically, Bernhard (2014) aligns spelling variants relying on a multivariant bili"
W07-1603,J93-1002,0,0.15729,"Missing"
W07-1603,2006.jeptalnrecital-long.11,1,0.830389,"scription of some entries with the preposition avec [with] in valence dictionaries • DICOVALENCE, a valence dictionary of French, formerly known as PROTON (van den Eynde and Mertens, 2002), which has been based on the pronominal approach. In version 1.1, this dictionary details the subcategorization frames of more than 3,700 verbs (table 1 gives an example of a DICOVALENCE entry). We extracted the simple and multiword prepositions it contains (i.e. more than 40), as well as their associated semantic classes. • We completed this argument prepositions list with information gathered from SynLex (Gardent et al., 2006), a syntactic lexicon created from the LADL lexicon-grammar tables (Gross, 1975) (see table 1 for a SynLex entry). Using these sources, we conducted a systematic study of each preposition, checking its presence in each source, whether in verb subcategorization frames or not, as well as its associated semantic class(es). We then grouped the prepositions that appear both as lexical entries and in verb subcategorization frames. As multiword prepositions show specific characteristics (in particular, their number) and raise particular issues (segmentation), we processed them sepa19 rately, using th"
W07-1603,saint-dizier-2006-prepnet,0,0.0638249,"d FrenchUNL dictionary: 2.2 We then completed the list of prepositions using manually built resources, including lexicons, dictionaries and grammars: • The Grevisse (Grevisse, 1997) grammar, in its paper version, allowed us to check some intuitions concerning the obsolescence or usage of some prepositions. • The TLFi (Tr´esor de la langue franc¸aise informatis´e), that we consulted through the CNRTL2, and that offers a slightly different list of prepositions. In particular, it contains the forms voici and voil`a, that are seldom quoted in the other available resources. • Finally, the PrepNet (Saint-Dizier, 2006) prepositions database was used to check the completeness of our list as well as the semantic information provided by other sources. 2.3 • Lefff (Lexique des Formes Fl´echies du Franc¸ais/French inflected form lexicon (Sagot et al., 2006)) is a large coverage (more than 110,000 lemmas) French morphological and syntactic lexicon (see table 1 for an example of a Lefff syntactic entry). 18 Using reference sources Using verb valence dictionaries We then looked for a way to enrich the list of prepositions appearing in verb subcategorization frames in Lefff and UNL, using resources that focus more p"
W07-1603,C00-2111,0,0.0399785,"Missing"
W07-1603,W06-2106,0,\N,Missing
W07-1603,sagot-etal-2006-lefff,0,\N,Missing
W09-3025,W04-1213,0,0.0201693,"tion. Manual annotations give a precise description of the expected results of the target system. Focusing on manual annotation issues led us to examine what named entities are and what they are used for. 2 Named Entities Annotation: practice and difficulties Named entity recognition is a well-established task (Nadeau and Sekine, 2007). One can recall its evolution according to three main directions. The first corresponds to work in the “general” field, 1 See the evaluation campaigns MET, IREX, CoNNL, ACE, ESTER and HAREM (Ehrmann, 2008, pp. 19-21). 2 See the evaluation campaigns BioCreAtIvE (Kim et al., 2004) and JNLPBA (Hirschman et al., 2005). 3 www.ncbi.nlm.nih.gov/pubmed, http://medline.cos.com 0 This work was partly realised as part of the Quaero Programme, funded by OSEO, French State agency for innovation. 142 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 142–145, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP were vague about the annotation of NEs 4 , and few studies measured annotation quality. For the GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004) or GENETAG (Tanabe et al., 2005) corpora, no inter- or intra-annotator agreement is rep"
W09-3025,W04-3111,0,0.035588,"E, ESTER and HAREM (Ehrmann, 2008, pp. 19-21). 2 See the evaluation campaigns BioCreAtIvE (Kim et al., 2004) and JNLPBA (Hirschman et al., 2005). 3 www.ncbi.nlm.nih.gov/pubmed, http://medline.cos.com 0 This work was partly realised as part of the Quaero Programme, funded by OSEO, French State agency for innovation. 142 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 142–145, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP were vague about the annotation of NEs 4 , and few studies measured annotation quality. For the GENIA (Kim et al., 2003), PennBioIE (Kulick et al., 2004) or GENETAG (Tanabe et al., 2005) corpora, no inter- or intra-annotator agreement is reported. If NE annotation seems a well-established practice, it involves some difficulties. As regards general language corpora, those difficulties are identified (Ehrmann, 2008). The first one relates to the choice of annotation categories and the determination of what they encompass. Indeed, beyond the “universal” triad defined by the MUC conferences (ENAMEX, NUMEX and TIMEX), the inventory of categories is difficult to stabilize. For ENAMEX, although it may be obvious that the name of an individual such as"
W09-3025,M95-1002,0,0.0446481,"ation must follow precise guidelines, satisfy quality criteria and support evaluation. In the general field, the MUC, CoNLL and ACE evaluation campaigns seem to have paid attention to the process of manual NE annotation, with the definition of annotation guidelines and the calculation of inter-annotator (but not intraannotator) agreement, using a back-and-forth process between annotating the corpus and defining the annotation guidelines. Nevertheless, some aspects of the annotation criteria remained problematic, caused mainly by “different interpretations of vague portions of the guidelines” (Sundheim, 1995). In the fields of biology and medicine, texts from specialized databases (PubMed and MedLine3 ) were annotated. Annotation guidelines Today, the named entity recognition task is considered as fundamental, but it involves some specific difficulties in terms of annotation. Those issues led us to ask the fundamental question of what the annotators should annotate and, even more important, for which purpose. We thus identify the applications using named entity recognition and, according to the real needs of those applications, we propose to semantically define the elements to annotate. Finally, w"
W10-1807,J96-2004,0,0.0312586,"nn Treebank annotations as reference and calculated a simple precision as compared to this reference. Figure 1 gives an overview of the obtained results (note that the scale is not regular). However, this is not sufficient to evaluate the quality of the annotation as, actually, the reference annotation is not perfect (see below). We therefore evaluated the reliability of the annotation, calculating the inter-annotator agreement between Annotator1 and Annotator2 on the 100-sentence series they both annotated. We calculated this agreement on some of the subcorpora using π, aka Carletta’s Kappa (Carletta, 1996)4 . The results of this are shown in table 2. 4.2 Impact of the Pre-annotation Accuracy on Annotation Time Before discussing the results of Experiment 2, annotation time measurements during Experiment 3 confirm that using a good quality pre-annotation (say, MEltALL en ) strongly reduces the annotation time as compared with fully manual annotation. For example, Annotator1 needed an average time of approximately 7.5 minutes to annotate 10 sentences without pre-annotation (Experiment 3), whereas Experiment 2 shows that it goes down to approximately 2.5 minutes when using MEltALL en pre-annotation"
W10-1807,Y09-1013,1,0.574324,"Missing"
W10-1807,2009.jeptalnrecital-long.29,1,0.701093,"s are a bit disappointing as they could not find a direct improvement of annotation time using pre-annotation. The authors reckon this might be at least partly due to “an interaction between time savings from pre-annotation and time savings due to a training effect.” For the same reason, they had to exclude some of the annotation results for quality evaluation in order to show that, in line with (Marcus et al., 1993), quality pre-annotation helps increasing annotation quality. They also found that noisy and low quality pre-annotation does not overall corrupt human judgment. On the other hand, Fort et al. (2009) claim that pre-annotation introduces a bias in named entity annotation, due to the preference given by anno3 Experimental Setup The idea underlying our experiments is the following. We split the Penn Treebank corpus (Marcus et al., 1993) in a usual manner, namely we use Sections 2 to 21 to train various instances of a POS tagger, and Section 23 to perform the actual experiments. In order to measure the impact of the POS tagger’s quality, we trained it on subcorpora of increasing sizes, and pre-annotated Section 23 with these various POS taggers. Then, we manually annotated parts of Section 23"
W10-1807,J93-2004,0,0.0756342,"ion time and inter-annotator agreement. This article is organized as follows. In Section 2, we mention some related work, while Section 3 describes the experimental setup, followed by a discussion on the obtained results (Section 4) and a conclusion. This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed. 1 2 2.1 Related Work Pre-annotation for POS Tagging Very few manual annotation projects give details about the campaign itself. One major exception is the Penn Treebank project (Marcus et al., 1993), th"
W10-1807,W96-0213,0,0.479735,"Missing"
W10-1807,W09-3003,0,0.172305,"n 4) and a conclusion. This article details a series of carefully designed experiments aiming at evaluating the influence of automatic pre-annotation on the manual part-of-speech annotation of a corpus, both from the quality and the time points of view, with a specific attention drawn to biases. For this purpose, we manually annotated parts of the Penn Treebank corpus (Marcus et al., 1993) under various experimental setups, either from scratch or using various pre-annotations. These experiments confirm and detail the gain in quality observed before (Marcus et al., 1993; Dandapat et al., 2009; Rehbein et al., 2009), while showing that biases do appear and should be taken into account. They finally demonstrate that even a not so accurate tagger can help improving annotation speed. 1 2 2.1 Related Work Pre-annotation for POS Tagging Very few manual annotation projects give details about the campaign itself. One major exception is the Penn Treebank project (Marcus et al., 1993), that provided detailed information about the manual annotation methodology, evaluation and cost. Marcus et al. (1993) thus showed that manual tagging took twice as long as correcting pre-tagged text and resulted in twice the inter-"
W10-1807,E09-1087,0,0.0102857,"Missing"
W10-1807,W09-3002,0,\N,Missing
W10-1807,J08-4004,0,\N,Missing
W11-0411,W10-1804,0,0.10543,"ation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Following (Bonneau-Maynard et al., 2005; Alex et al., 2010), the annotation guidelines were first written from December 2009 to May 2010 by three researchers managing the manual annotation campaign. During guidelines production, we evaluated the feasibility of this specific annotation task and the usefulness of the guidelines by annotating a small part of the target corpus. Then, these guidelines were delivered to the annotators. They consist of a description of the objects to annotate, general annotation rules and principles, and more than 250 prototypical and real examples extracted from the corpus (Rosset et al., 2010). Rules are important to set t"
W11-0411,J08-4004,0,0.16077,"rics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give us about the annotation. Markables Annotators Both institutes F"
W11-0411,bick-2004-named,0,0.312622,"undheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as"
W11-0411,J96-2004,0,0.23313,"inally, we merged the results with the anno97 5.2 Metrics Because human annotation is an interpretation process (Leech, 1997), there is no “truth” to rely on. It is therefore impossible to really evaluate the validity of an annotation. All we can and should do is to evaluate its reliability, i.e. the consistency of the annotation across annotators, which is achieved through computation of the inter-annotator agreement (IAA). The best way to compute it is to use one of the Kappa family coefficients, namely Cohen’s Kappa (Cohen, 1960) or Scott’s Pi (Scott, 1955), also known as Carletta’s Kappa (Carletta, 1996),11 as they take chance into account (Artstein and Poesio, 2008). However, these coefficients imply a comparison with a “random baseline” to establish whether the correlation between annotations is statistically significant. This baseline depends on the number of “markables”, i.e. all the units that could be annotated. In the case of named entities, as in many others, this “random baseline” is known to be difficult—if not impossible—to identify (Alex et al., 2010). We wish to analyze this in more detail, to see how we could actually compute these coefficients and what information it would give"
W11-0411,W09-3002,0,0.0410686,"we presented an extension of the traditional named entity categories to new types (functions, civilizations) and new coverage (expressions built over a substantive). We created guidelines that were used by graduate annotators to annotate a broadcast news corpus. The organizers also annotated a small part of the corpus to build a mini reference corpus. We evaluated the human annotations with our mini-reference corpus: the actual computed κ is between 0.71 et 0.85 which, given the complexity of the task, seems to indicate a good annotation quality. Our results are consistent with other studies (Dandapat et al., 2009) in demonstrating that human annotators’ training is a key asset to produce quality annotations. 99 We also saw that guidelines are never fixed, but evolve all along the annotation process due to feedback between annotators and organizers; the relationship between guidelines producers and human annotators evolved from “parent” to “peer” (Akrich and Boullier, 1991). This evolution was observed during the annotation development, beyond our expectations. These data have been used for the 2011 Quaero Named Entity evaluation campaign. Extensions and revisions are planned. Our first goal is to add a"
W11-0411,desmet-hoste-2010-towards,0,0.0216477,"rent classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact database through the extraction of named entities from texts, we defined a richer taxonomy than those used in other information extraction works. 93 Followin"
W11-0411,doddington-etal-2004-automatic,0,0.157033,", some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain better agreement. Desmet and Hoste (2010) described the Named Entity annotation realized within the Sonar project, where Named Entity are clearly simpler. They follow the MUC Named Entity definition with the subtypes as proposed by ACE. The agreement computed over the Sonar Dutch corpus ranges from 0.91 to 0.97 (kappa values) depending of the emphasized elements (span, main type, subtype, etc.). 3 3.1 Taxonomy Guidelines production Having in mind the objective of building a fact databa"
W11-0411,C02-1130,0,0.0613942,"cations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific do"
W11-0411,W09-3025,1,0.778972,"ed. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be annotated or the trigger “Mr” too?) and (3) annotating metonymies (“France” can be a sports team, a country, etc.). In the ACE Named Entity task (Doddington et al., 2004), a complex task, the obtained inter-annotator agreement was 0.86 in 2002 and 0.88 in 2003. Some tasks obtain bette"
W11-0411,C96-1079,0,0.902879,"luation campaign on named entity extraction aiming at building a fact database in the news domain, the first step being to define what kind of entities are needed. This campaign focused on broadcast news corpora in French. While traditional named entities include three major classes (persons, locations and organizations), we decided to extend the coverage of our campaign to new types of entities and to broaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; G"
W11-0411,I05-1058,0,0.0948165,"roaden their main parts-of-speech from proper Related work Named entity definitions Named Entity recognition was first defined as recognizing proper names (Coates-Stephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996; SAIC, 1998), named entities have been proper names falling into three major classes: persons, locations and organizations. Proposals were made to sub-divide these entities into finer-grained classes. The “politicians” subclass was proposed for the “person” class by (Fleischman and Hovy, 2002) while the “cities” subclass was added to the “location” class by (Fleischman, 2001; Lee and Lee, 2005). The CONLL conference added a miscellaneous type that includes proper names falling outside the previous classes. Some classes have thus sometimes been added, e.g. the “product” class by (Bick, 2004; Galliano et al., 2009). 92 Proceedings of the Fifth Law Workshop (LAW V), pages 92–100, c Portland, Oregon, 23-24 June 2011. 2011 Association for Computational Linguistics Specific entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard"
W11-0411,sekine-nobata-2004-definition,0,0.926265,"entities are proposed and handled in some tasks: “language” or “shape” for questionanswering systems in specific domains (Rosset et al., 2007), “email address” or “phone number” to process electronic messages (Maynard et al., 2001). Numeric types are also often described and used. They include “date”, “time”, and “amount” types (“amount” generally covers money and percentage). In specific domains, entities such as gene, protein, are also handled (Ohta, 2002), and campaigns are organized for gene detection (Yeh et al., 2005). At the same time, extensions of named entities have been proposed: (Sekine, 2004) defined a complete hierarchy of named entities containing about 200 types. 2.2 Named Entities and Annotation As for any other kind of annotation, some aspects are known to lead to difficulties in obtaining coherence in the manual annotation process (Ehrmann, 2008; Fort et al., 2009). Three different classes of problems are distinguished: (1) selecting the correct category in cases of ambiguity, where one entity can fall into several classes, depending on the context (“Paris” can be a town or a person name); (2) detecting the boundaries (in a person designation, is only the proper name to be a"
W11-0411,vilnat-etal-2010-passage,0,\N,Missing
W11-1810,W09-1402,0,0.0425656,"Missing"
W11-1810,2010.jeptalnrecital-long.35,1,0.889244,"Missing"
W11-1810,W09-1401,0,0.134476,"’ results. Both issued from PubMed scientific literature abstracts, the Rename task aims at extracting gene name synonyms, and the GI task aims at extracting genic interaction events, mainly about gene transcriptional regulations in bacteria. 1 Introduction The extraction of biological events from scientific literature is the most popular task in Information Extraction (IE) challenges applied to molecular biology, such as in LLL (N´edellec, 2005), BioCreative Protein-Protein Interaction Task (Krallinger et al., 2008), or BioNLP (Demner-Fushman et al., 2008). Since the BioNLP 2009 shared task (Kim et al., 2009), this field has evolved from the extraction of a unique binary interaction relation between proteins and/or genes towards a broader acceptation of biological events including localization and transformation (Kim et al., 2008). In the same way, the tasks Bacteria Gene Interactions and Bacteria Gene Renaming deal with the extraction of various molecular events capturing the mechanisms relevant to gene regulation in prokaryotes. The study of bacteria has numerous applications for health, food and industry, and overall, they are considered as organisms of choice for the recent integrative approac"
W11-1810,2004.jeptalnrecital-recitalposter.17,0,0.128053,"Missing"
W11-1810,J08-4004,0,\N,Missing
W12-3606,bick-2004-named,0,0.0325063,"(CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 20"
W12-3606,doddington-etal-2004-automatic,0,0.0405426,"self (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location"
W12-3606,C02-1130,0,0.0265156,"et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the de"
W12-3606,galibert-etal-2010-named,1,0.859086,"can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub"
W12-3606,I11-1058,1,0.836346,"tities on spoken data and the resulting corpus. The training part of the corpus is only composed of broadcast news data while the test corpus is composed of both broadcast news and broadcast conversations data. In order to build a minireference corpus for this annotation campaign (a “gold” corpus), we randomly extracted a sub-corpus from the training one. This sub-corpus was annotated by 6 different annotators following a 4-step procedure. Table 1 gives statistics about training, test and gold corpora. These corpora (“BN” in the remainder of the paper) has been used in an evaluation campaign (Galibert et al., 2011). PP PP Data Training PP Inf. PP P # shows 188 # lines 43,289 # tokens 1,291,225 # entity types 113,885 # distinct types 41 # components 146,405 # distinct comp. 29 Test Gold 18 5,637 108,010 5,523 32 8,902 22 398 11,532 1,161 29 1,778 22 Table 1: Statistics on the annotated BN corpora Structured Named Entities in Old Newspapers We performed the same annotations on a corpus composed of OCRed press archives, henceforth the Old Press (OP) corpus. Human annotation was subcontracted to the same team of annotators as for the BN corpus, thus facilitating the consistency of annotations across corpora"
W12-3606,galibert-etal-2012-extended,1,0.588515,"née e t a p p a r e i l l e r a e n s u i t e nour &lt; loc.adm.town &gt; Toulon. &lt; / loc.adm.town &gt; Figure 4: Example annotated text block Component noisy-entities. When a character recognition error involves an entity boundary, a segmentation error occurs, either between an entity and other tokens, or between several entities and possibly other tokens. To allow the annotators to annotate the entity in that character span, we defined a new component noisy-entities which indicates that an entity is present in the noisy span of characters. A complete description of these adaptations can be found in (Galibert et al., 2012). 3.3 Global corpus extraction Unannotated sub-corpus Global annotated corpus Scientist 1 Scientist 2 Scientist 4 Scientist 3 Adjudication extraction Inter-Annotator Agreement To evaluate the manual annotations of the annotation team (“Global annotated corpus” in Figure 5), we built a mini reference corpus by selecting 255 blocks from the training corpus. We followed the same procedure as the one used for the BN corpus, as illustrated in Figure 5: Adjudication Institute 1 Institute 2 Adjudication Annotated sub-corpus IAA Institutes Adjudication IAA Mini-reference 1. The corpus is annotated ind"
W12-3606,C96-1079,0,0.645168,"he point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of"
W12-3606,W11-0411,1,0.930226,", as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and"
W12-3606,grover-etal-2008-named,0,0.179562,"corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described as recognizing proper names (CoatesStephens, 1992). Since"
W12-3606,A00-1044,0,0.0706869,"ora: the pre-existing broadcast news corpus and this new corpus of old newspapers. This comparison is performed at two levels: the annotation process itself (Section 4.1) and the annotation results (Section 4.2). 2 1 Introduction Named Entity Recognition (NER), and its evaluation methods, constitute an active field of research. NER can be performed on many kinds of documents. On textual data, a few NER applications focus on newspapers, spoken data, as well as digitized data. On specific kinds of data such as historical data, various investigations have been performed to detect named entities (Miller et al., 2000; Crane and Jones, 2006; Byrne, 2007; Grover et al., 2008). From the point of view of both annotation and evaluation campaigns, ACE (Doddington et al., 2004) included NER on OCRed data. For the French language, an evaluation involving classical named entities was performed a few years ago on old newspapers data (Galibert et al., 2010). More recently, we proposed a definition of structured named entities for broadcast news data (Grouin et al., 2011). We follow this definition in the present work. 40 2.1 Related Work Named Entity Definition Initially, Named Entity recognition (NER) was described"
W12-3606,sekine-nobata-2004-definition,0,0.07436,"992). Since MUC-6 (Grishman and Sundheim, 1996), named entities include three major classes: person, location and organization. Some numerical types are also often described and used in the literature: date, time and amount (money and percentages in most cases). Proposals were made to sub-divide existing categories into finer-grained classes: e.g., politician as part of the person class (Fleischman and Hovy, 2002), or city in the location class (Fleischman, 2001). New classes were added during the CONLL conference. More recently, larger extensions were proposed: product by (Bick, 2004) while (Sekine, 2004) defined an extensive hierarchy of named entities containing about 200 types. Numerous investigations concern named entities in historical data (Miller et al., 2000; Crane and Jones, 2006; Proceedings of the 6th Linguistic Annotation Workshop, pages 40–48, c Jeju, Republic of Korea, 12-13 July 2012. 2012 Association for Computational Linguistics Byrne, 2007; Grover et al., 2008). In most cases, the definition of named entity follows the classical definition. Nevertheless, in some cases, new categories were added. For example, the Virginia Banks project (Crane and Jones, 2006) added categories"
W18-4923,J17-4005,0,0.0607614,"Missing"
W18-4923,C16-1286,1,0.791725,"Savary (2018) involves a gamified interface allowing MWE researchers to guess the meaning of opaque MWEs in other languages. However, we could find no publication concerned with evaluating human ability to identify MWEs in a text, without taking their interpretation into account. On the other hand, voluntary crowdsourcing, especially in the form of Games with a Purpose (GWAPs), has proven effective in terms of both the quantity and quality of the data produced. Successful examples of such platforms include JeuxDeMots (Lafourcade, 2007), Phrase Detectives (Poesio et al., 2013), and ZombiLingo (Guillaume et al., 2016). We created a gamified platform named RigorMortis1 (see Figure 1), the first of its kind, for MWEs annotation in French2 . This platform includes a task enabling evaluation of the participants’ intuition concerning MWEs, the results of which we present here. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 See: rigor-mortis.org 2 We believe it is adaptable to any language. License details: http:// 207 Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018"
W18-4923,L16-1262,0,0.0563718,"Missing"
W18-4923,P16-2026,0,0.0276575,"in recent years on the subject, in particular though the PARSEME international network (Savary et al., 2015). However, although some collective expert-based annotation initiatives have been successfully undertaken (Schneider et al., 2016; Savary et al., 2017), language resources are still limited in coverage and the need remains to identify newly-created MWEs. One potential solution is to exploit the so-called &quot;wisdom of the crowd&quot;. There have been several research papers on the interpretation of MWEs by native speakers, in particular by Gibbs (Gibbs, 1992; Gibbs et al., 1997). More recently, Ramisch et al. (2016) involved microworking crowdsourcing on Amazon Mechanical Turk. Finally, the experiment described in Krstev and Savary (2018) involves a gamified interface allowing MWE researchers to guess the meaning of opaque MWEs in other languages. However, we could find no publication concerned with evaluating human ability to identify MWEs in a text, without taking their interpretation into account. On the other hand, voluntary crowdsourcing, especially in the form of Games with a Purpose (GWAPs), has proven effective in terms of both the quantity and quality of the data produced. Successful examples of"
