2020.acl-main.652,P17-1171,0,0.0545817,"Missing"
2020.acl-main.652,D18-1241,0,0.300611,"s are some of the most active ones and contain knowledge of general interest, making it easily accessible for crowdworkers. DoQA contains two scenarios: in the standard scenario the test data comprises the questions and the target document from which the answers need to be extracted; in the information retrieval (IR) scenario the test data contains the questions, but the target document is unknown, and the system needs to select the documents which contain the answers among all documents in the collection. Previous work on conversational QA datasets include CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018). The main focus of CoQA are reading comprehension questions, which are produced with access to the target paragraph. The topic of the questions are delimited by the paragraph, which leads to specific questions about details in the paragraph. Choi et al. (2018) observed that a large percentage of CoQA answers are named entities or short noun phrases. In QuAC, the topic of the conversation is set by a title and first paragraph of a Wikipedia article about people. The user makes up questions about the person of interest. Note that, contrary to our setting, there is no real information need in an"
2020.acl-main.652,P17-1162,0,0.0543763,"Missing"
2020.acl-main.652,P17-1167,0,0.0895749,"Missing"
2020.acl-main.652,Q18-1023,0,0.0515395,"Missing"
2020.acl-main.652,P18-2124,0,0.151341,"Missing"
2020.acl-main.652,D16-1264,0,0.483415,"and Travel). In all cases scores over 50 F1 are reported. Regarding the IR scenario, an IR module complements the conversational system, with a relatively modest drop in performance. The gap with respect to human performance is over 30 points, showing that there is still ample room for system improvement. 2 Related Work Conversational QA systems stem from the body of work on Reading Comprehension, whose goal is to test the capacity of a system to understand a document by answering any question posed over its content. Recent work on the field has resulted in the creation of multiple datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016; Koˇcisk´y et al., 2018; Dunn et al., 2017). These datasets are typically composed of multiple question/answer pairs, often along with a reference passage from which the answer is curated. Whereas the questions are always in free text form, some datasets represent the answers as a contiguous span in the reference passage, while others contain free form answers. The former are usually referred as extractive, whereas the latter are called abstractive. All in all, in these QA datasets the queries are unrelated to each other, and thus there is no dialo"
2020.acl-main.652,N18-1059,0,0.0493118,"Missing"
2020.acl-main.652,W17-2623,0,0.0366228,"es scores over 50 F1 are reported. Regarding the IR scenario, an IR module complements the conversational system, with a relatively modest drop in performance. The gap with respect to human performance is over 30 points, showing that there is still ample room for system improvement. 2 Related Work Conversational QA systems stem from the body of work on Reading Comprehension, whose goal is to test the capacity of a system to understand a document by answering any question posed over its content. Recent work on the field has resulted in the creation of multiple datasets (Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016; Koˇcisk´y et al., 2018; Dunn et al., 2017). These datasets are typically composed of multiple question/answer pairs, often along with a reference passage from which the answer is curated. Whereas the questions are always in free text form, some datasets represent the answers as a contiguous span in the reference passage, while others contain free form answers. The former are usually referred as extractive, whereas the latter are called abstractive. All in all, in these QA datasets the queries are unrelated to each other, and thus there is no dialogue structure involved."
2020.acl-main.652,S17-2003,0,0.0689775,"Missing"
2020.coling-main.230,2020.acl-main.652,1,0.878955,"r makes a set of interrelated questions to the system, which extracts the answers from reference text (Choi et al., 2018). These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia (Choi et al., 2018), and DoQA includes question-answer conversations on cooking, movies and travel FAQs (Campos et al., 2020). Building such datasets comes at a cost, which limits the widespread use of conversational systems built using supervised learning. The fact that conversational systems interact naturally with users poses an exciting opportunity to improve them after deployment. Given enough training data, a company can deploy a basic conversational system, enough to be accepted and used by users. Once the system is deployed, the interaction with users and their feedback can be used to improve the system. In this work we focus on the case where a CQA system trained off-line is deployed and receives explicit b"
2020.coling-main.230,D18-1241,0,0.0512798,"ack is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments (QuAC), and even matching in out-of-domain experiments (DoQA). Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment. 1 Introduction In Conversational Question Answering (CQA) systems, the user makes a set of interrelated questions to the system, which extracts the answers from reference text (Choi et al., 2018). These systems are trained on datasets of human-human dialogues collected using Wizard-of-Oz techniques, where two crowdsourcers are paired at random to emulate the questioner and the answerer. Several projects have shown that it is possible to train effective systems using such datasets. For instance, QuAC includes question and answers about popular people in Wikipedia (Choi et al., 2018), and DoQA includes question-answer conversations on cooking, movies and travel FAQs (Campos et al., 2020). Building such datasets comes at a cost, which limits the widespread use of conversational systems b"
2020.coling-main.230,N19-1423,0,0.00573438,"t understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting the start token of the answer span and another for spotting the end token of the answer span. In reading comprehension, the questions are individual and isolated, that is, they do not have any dialogue structure. Due to the increasing interest on modelling the conversa"
2020.coling-main.230,P19-1358,0,0.101709,"Within this framework of lifelong learning, we particularly focus on building a system that adapts to changes in the data distribution after deployment (Agirre et al., 2019). There have been efforts for learning actively from dialogue during deployment. The question answering (QA) setting was explored in Weston (2016) and Li et al. (2017), where they analyzed a variety of learning strategies for different dialogue tasks with diverse types of feedback. In these studies they also touch on forward prediction, which uses explicit user correction. This idea was later applied to chit-chat systems (Hancock et al., 2019). These works relied on users explicitly providing the correct answer. This strong assumption was relaxed in Weston (2016), where the user provides binary feedback on correct and incorrect answers in a synthetic question answering task (Weston et al., 2015). Our work also uses binary feedback and tests it in more realistic CQA datasets. In a similar online setup to ours, Liu et al. (2018b) explored contextual multi-armed bandits for dialogue response selection using a customized version of Thompson sampling. In this work they use the Ubuntu Dialogue Corpus (Lowe et al., 2015) for user simulati"
2020.coling-main.230,N18-1187,0,0.182184,"Missing"
2020.coling-main.230,W15-4640,0,0.020904,"-chat systems (Hancock et al., 2019). These works relied on users explicitly providing the correct answer. This strong assumption was relaxed in Weston (2016), where the user provides binary feedback on correct and incorrect answers in a synthetic question answering task (Weston et al., 2015). Our work also uses binary feedback and tests it in more realistic CQA datasets. In a similar online setup to ours, Liu et al. (2018b) explored contextual multi-armed bandits for dialogue response selection using a customized version of Thompson sampling. In this work they use the Ubuntu Dialogue Corpus (Lowe et al., 2015) for user simulation. In the case of task-oriented dialogue systems, Liu et al. (2018a) propose a hybrid learning method with supervised pre-training and further improvement using human teaching and feedback. For the human teaching case they use imitation learning with explicit corrections done by an expert. After that, they resort to reinforcement learning for further improvement thanks to long term rewards defined by task completion. 4 Experiments In this section we present the experiments with feedback-weighted learning (FWL). In the experiments we first build a supervised system (S0 ), and"
2020.coling-main.230,W19-4102,0,0.031364,"QA datasets where questions and answers are interrelated have been created following the Wizard-ofOz technique. Among all the datasets we can highlight QuAC (Choi et al., 2018), CoQA (Reddy et al., 2019) and DoQA (Campos et al., 2020). While the first two datasets cover more formal domains as Wikipedia articles and literature, the latter covers different domains extracted from online forums as StackExchange. Contextual versions of the previously mentioned reading comprehension models have successfully modelled the conversational structure in those datasets (Qu et al., 2019b; Qu et al., 2019a; Ohsugi et al., 2019; Ju et al., 2019). 3 Importance Sampling for Learning After Deployment In our learning after deployment scenario we start by training an initial S0 system in an off-line and supervised way. This first system follows the traditional workflow where we have access to limited supervised training and development data. Then, we take the best performing system on the development data and deploy it to serve user queries. In this deployment phase, every time a user makes a query x, the system generates an answer y and the user gives binary feedback to it. Over time, the system generates different answ"
2020.coling-main.230,D14-1162,0,0.0861155,"Missing"
2020.coling-main.230,D16-1264,0,0.120899,"t interactions with real users and improve conversational systems after deployment. All the code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1"
2020.coling-main.230,P18-2124,0,0.0164386,"users and improve conversational systems after deployment. All the code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjac"
2020.coling-main.230,Q19-1016,0,0.0433041,"of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting the start token of the answer span and another for spotting the end token of the answer span. In reading comprehension, the questions are individual and isolated, that is, they do not have any dialogue structure. Due to the increasing interest on modelling the conversational structure behind user questions, several CQA datasets where questions and answers are interrelated have been created following the Wizard-ofOz technique. Among all the datasets we can highlight QuAC (Choi et al., 2018), CoQA (Reddy et al., 2019) and DoQA (Campos et al., 2020). While the first two datasets cover more formal domains as Wikipedia articles and literature, the latter covers different domains extracted from online forums as StackExchange. Contextual versions of the previously mentioned reading comprehension models have successfully modelled the conversational structure in those datasets (Qu et al., 2019b; Qu et al., 2019a; Ohsugi et al., 2019; Ju et al., 2019). 3 Importance Sampling for Learning After Deployment In our learning after deployment scenario we start by training an initial S0 system in an off-line and supervise"
2020.coling-main.230,W17-2623,0,0.0124032,"code and dataset splits are made publicly available 1 . 2 Related Work on Conversational Question Answering CQA research builds on reading comprehension. In reading comprehension the system has to answer questions about a certain passage of text in order to show that it understands the passage. There are two main methods: the extractive method, in which the answer is selected as a contiguous span in the reference passage, and the abstractive method, in which the answer text is generated. Many datasets (Rajpurkar et al., 2016; Rajpurkar et al., 2018; Dunn et al., 2017; Koˇcisk`y et al., 2018; Trischler et al., 2017; Bajaj et al., 2016) and systems have been proposed to address this task, where the extractive scenario has drawn special attention (Wang and Jiang, 2017; Seo et al., 2017). Lately, with the incursion of large pre-trained language models as BERT (Devlin et al., 2019), XLNet (Yang et al., 2019) and their relatives, the state of the art has been dominated by systems that use the representations obtained with these pre-trained language models. The systems learn answer pointer networks that consist of two 1 https://github.com/jjacampos/FeedbackWeightedLearning 2562 classifiers, one for spotting t"
2020.emnlp-main.326,W18-6539,0,0.0323688,"Missing"
2020.emnlp-main.326,P17-1103,0,0.071079,"Missing"
2020.emnlp-main.326,D15-1166,0,0.0277494,"Missing"
2020.emnlp-main.326,2020.acl-main.64,0,0.13663,"to three domains, evaluating several stateof-the-art chatbots, and drawing comparisons to related work. The framework is released as a ready-to-use tool. 1 Introduction Evaluation is a long-standing issue in developing conversational dialogue systems (i.e., chatbots). The underlying difficulty in evaluation lies in the problem’s open-ended nature, as chatbots do not solve a clearly-defined task whose success can be measured in relation to an a priori defined ground truth. Automatic metrics have so far failed to show high correlation with human evaluations (Liu et al., 2016; Lowe et al., 2017; Mehri and Eskenazi, 2020). Human evaluation approaches are mainly classified according to the following: single-turn vs. multi-turn evaluation, and direct user evaluation vs. expert judge evaluation. Single-turn analysis is usually performed by a human judge that rates a single response of the bot to a given context, whereas multi-turn analysis is often performed by a user that interacts with the bot and rates the interaction. Single-turn ratings disregard the multiturn nature of a dialogue (See et al., 2019). Although more and more multi-turn evaluations are performed, most of them are based on human-bot conversation"
2020.emnlp-main.326,P19-1534,0,0.0596159,"Missing"
2020.emnlp-main.326,N19-1170,0,0.138067,"ve so far failed to show high correlation with human evaluations (Liu et al., 2016; Lowe et al., 2017; Mehri and Eskenazi, 2020). Human evaluation approaches are mainly classified according to the following: single-turn vs. multi-turn evaluation, and direct user evaluation vs. expert judge evaluation. Single-turn analysis is usually performed by a human judge that rates a single response of the bot to a given context, whereas multi-turn analysis is often performed by a user that interacts with the bot and rates the interaction. Single-turn ratings disregard the multiturn nature of a dialogue (See et al., 2019). Although more and more multi-turn evaluations are performed, most of them are based on human-bot conversations, which are costly to obtain and tend to suffer from low quality (Dinan et al., 2020a). The instructions to be followed by annotators are often chosen ad-hoc and there are no unified definitions. Compounded with the use of often criticized Likert scales (Amidei et al., 2019a), these evaluations often yield a low agreement. The required cost and time efforts also inhibit the widespread use of such evaluations, which raises questions on the replicability, robustness, and thus significa"
2020.lrec-1.55,2020.lrec-1.588,1,0.807353,"he experiments were carried out using the extractive information of the train/dev/test splits of ElkarHizketak. The baselines that use the monolingual BERTeus model are trained and evaluated using only the ElkarHizketak dataset (native training). Regarding the cross-lingual models, apart from the just mentioned approach, another two different cross-lingual transfer learning approaches are followed: • zero-shot cross-lingual transfer: we use the train data of QuAC for training the model, and evaluate it on ElkarHizketak. BERTeus: We have used the pre-trained BERT model for the Basque Language (Agerri et al., 2020) due to the low representation this language has in the official multilingual BERT model. This Basque BERT model has been trained on a corpus comprising the Basque Wikipedia and news articles from Basque media. • low resource cross-lingual transfer: once we have the previous model, we fine tune it using the small train split of ElkarHizketak and test it on ElkarHizketak test split. For completeness, both development and test figures are shown. mBERT ours: We have pre-trained a multilingual BERT model with the intention of performing transfer ex440 Model Majority without dialogue history BERTeu"
2020.lrec-1.55,C18-1139,0,0.0153044,"first attempt to create a conversational QA dataset in a language other than English. Contextualized word embeddings are representations that are sensitive to the context where the word appear. These models are first pre-trained on big corpora using a language modeling loss. The pre-trained model is then fine-tuned to the task at hand, using manually annotated datasets and appropriate loss functions. They have been successfully used in a variety of natural language processing tasks, including QA and dialogue systems (Devlin et al., 2019; Qu et al., 2019). ELMO (Peters et al., 2018) and Flair (Akbik et al., 2018) are language models built upon LSTM-based architectures. BERT (Devlin et al., 2019) is a model based on a transformer architecture, and pre-trained using a masked language model objective. BERT has been very successful on many NLP tasks, and several variants exists, such as RoBERTA (Liu et al., 2019b) and ALBERT (Lan et al., 2020). These models are trained for English, but some authors have built pre-trained models for other languages such as French (Martin et al., 2019). Interestingly, the knowledge learned by pre-trained models such as BERT has been shown to be transferable across domains."
2020.lrec-1.55,D18-1241,0,0.034992,"Missing"
2020.lrec-1.55,N19-1423,0,0.509734,"interactions, analogous to QuAC but for Basque. Due to the lack of Basque speakers in crowdsourcing platforms, we used social media to recruit volunteers. The resulting dataset contains close to 400 dialogues and more than 1600 question and answers, and its small size presents a realistic low436 resource scenario for CQA systems. Figure 1 displays an example of a dialogue related to a Wikipedia section about the biography of Edorta Jimenez, a Basque writer, with translations into English. Current CQA systems rely heavily on pre-trained language models such as ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and derived models (Liu et al., 2019b; Lample and Conneau, 2019). These models are first trained on large numbers of text using a language model loss, and then finetuned on the train data of a CQA dataset. The most recent models include multilingual versions (Devlin et al., 2019; Lample and Conneau, 2019), where the text in different languages is represented in a common space. The multilingual versions allow to transfer trained models to languages other than that used to fine tune them. In this paper we test the performance of variants of BERT in low-resource scenarios: native training data o"
2020.lrec-1.55,P17-1162,0,0.0549055,"Missing"
2020.lrec-1.55,P17-1167,0,0.0291856,"comparable to those obtained for the analogous QuAC English dataset. (3) Our experiments show that dialogue history models are not directly transferable from one language to another. The dataset is freely available with an open license1 . To our knowledge, this is the first nonEnglish conversational QA dataset, the first conversational dataset for Basque, and the first cross-lingual transfer results on conversational question answering. 2. Related Work Work in conversational QA systems has led to the creation of a variety of datasets for the task (Nguyen et al., 2016; Rajpurkar et al., 2016; Iyyer et al., 2017; Trischler et al., 2017; Koˇcisk´y et al., 2018; Dunn et al., 2017). MS MARCO (Nguyen et al., 2016), NewsQA (Trischler et al., 2017) or SearchQA (Dunn et al., 2017) are some examples of reading comprehension datasets that require systems to understand a document to properly answer the queries. SequentialQA (Iyyer et al., 2017) comprises more than 6.000 question sequences where each question refers and refines previous ones, and therefore can be seen as different turns in a dialogue. More similar to our work, CoQA (Reddy et al., 2018) and QuAC (Choi et al., 2018) are two datasets that contain"
2020.lrec-1.55,Q18-1023,0,0.0538942,"Missing"
2020.lrec-1.55,P19-1227,0,0.103463,"asque. Due to the lack of Basque speakers in crowdsourcing platforms, we used social media to recruit volunteers. The resulting dataset contains close to 400 dialogues and more than 1600 question and answers, and its small size presents a realistic low436 resource scenario for CQA systems. Figure 1 displays an example of a dialogue related to a Wikipedia section about the biography of Edorta Jimenez, a Basque writer, with translations into English. Current CQA systems rely heavily on pre-trained language models such as ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and derived models (Liu et al., 2019b; Lample and Conneau, 2019). These models are first trained on large numbers of text using a language model loss, and then finetuned on the train data of a CQA dataset. The most recent models include multilingual versions (Devlin et al., 2019; Lample and Conneau, 2019), where the text in different languages is represented in a common space. The multilingual versions allow to transfer trained models to languages other than that used to fine tune them. In this paper we test the performance of variants of BERT in low-resource scenarios: native training data only, zeroshot transfer (English train"
2020.lrec-1.55,N18-1202,0,0.0741895,"tak, a small dataset of CQA interactions, analogous to QuAC but for Basque. Due to the lack of Basque speakers in crowdsourcing platforms, we used social media to recruit volunteers. The resulting dataset contains close to 400 dialogues and more than 1600 question and answers, and its small size presents a realistic low436 resource scenario for CQA systems. Figure 1 displays an example of a dialogue related to a Wikipedia section about the biography of Edorta Jimenez, a Basque writer, with translations into English. Current CQA systems rely heavily on pre-trained language models such as ELMO (Peters et al., 2018), BERT (Devlin et al., 2019) and derived models (Liu et al., 2019b; Lample and Conneau, 2019). These models are first trained on large numbers of text using a language model loss, and then finetuned on the train data of a CQA dataset. The most recent models include multilingual versions (Devlin et al., 2019; Lample and Conneau, 2019), where the text in different languages is represented in a common space. The multilingual versions allow to transfer trained models to languages other than that used to fine tune them. In this paper we test the performance of variants of BERT in low-resource scena"
2020.lrec-1.55,D16-1264,0,0.0836777,"Missing"
2020.lrec-1.55,W17-2623,0,0.124438,"sque 1. Introduction Conversational Question Answering (CQA) systems meet user information needs by having conversations with them. Users pose initial queries in free form text, and the systems usually answer the queries by returning relevant excerpts extracted from a reference passage. The answers returned by the system invite users to pose follow up questions, which are again answered, therefore creating a conversation between the system and the user. The field has received much attention in the last years, and there exist nowadays a variety of datasets for the task (Rajpurkar et al., 2016; Trischler et al., 2017; Nguyen et al., 2016; Koˇcisk´y et al., 2018; Dunn et al., 2017; Choi et al., 2018). Some of the datasets are very large. For instance, QuAC (Choi et al., 2018) contains thousands of dialogues and tens of thousands of question answering turns, which have been collected using wizard-of-oz techniques with paid crowdworkers. The dataset is built on top of Wikipedia sections about popular people and organizations. The high results of current systems are encouraging, and seem to show that the technology is ready for industrial adoption. Unfortunately, all current datasets are in English, and data"
2020.lrec-1.588,C18-1139,0,0.508845,"els used in this work are publicly available. Keywords: Neural language representation models, Information Extraction, Less-resourced languages 1. Introduction Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like FastText (Bojanowski et al., 2017), and, more recently, pre-trained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT (Devlin et al., 2019). In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for"
2020.lrec-1.588,N19-1078,0,0.0527324,"he word. In this case, the extracted hidden state contains information propagated from the end of the sentence to this point. Both hidden states are concatenated to generate the final embedding. Pooled Contextualized Embeddings: Flair embeddings, however, struggle to generate an appropriate word representation for words in underspecified contexts, namely, in sentences in which, for example, local information is not sufficient to know the named entity type of a given word. In order to address this issue a variant of the original Flair embeddings is proposed: “Pooled Contextualized Embeddings” (Akbik et al., 2019). In this approach, every contextualized embedding is kept into a memory which is later used in a pooling operation to obtain a global word representation consisting of the concatenation of all the local contextualized embeddings obtained for a given word. They reported significant improvements for NER by using this new version of Flair embeddings. Note that this does not affect to the way the Flair pre-trained embedding models are calculated. The pooling operation is involved in the process of using such pre-trained models in order to obtain word representations for a given task such as NER o"
2020.lrec-1.588,Q17-1010,0,0.647349,"ults than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available. Keywords: Neural language representation models, Information Extraction, Less-resourced languages 1. Introduction Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like FastText (Bojanowski et al., 2017), and, more recently, pre-trained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT (Devlin et al., 2019). In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be"
2020.lrec-1.588,N19-1423,0,0.273673,"ural language representation models, Information Extraction, Less-resourced languages 1. Introduction Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. The most successful models include word embeddings like FastText (Bojanowski et al., 2017), and, more recently, pre-trained language models which can be used to produce contextual embeddings or directly fine-tuned for each task. Good examples of the later are character-based models like Flair (Akbik et al., 2018) and masked language models like BERT (Devlin et al., 2019). In many cases the teams that developed the algorithms also release their models, which facilitates both reproducibility and their application in downstream tasks. Given that the models are expensive to train, many companies and research groups tend to use those models, rather than building their own. This could be suboptimal as, for many languages, the models have been trained on easily obtained public corpora, which tends to be smaller and/or of lower quality that other existing corpora. In addition, pre-trained language models for non-English languages are not always available. In that cas"
2020.lrec-1.588,L18-1550,0,0.0198096,"(Bojanowski et al., 2017). The approach is implemented in the FastText library5 , and two types of pre-trained word vectors are available (based on 300 dimensions): Wiki word vectors (FastText-official-wikipedia) were trained on Wikipedia using the skip-gram model described in (Bojanowski et al., 2017) with default parameters (a window size of 5, 3-6 length character n-grams and 5 negatives). Common Crawl word vectors (FastText-officialcommon-crawl) were trained on Common Crawl and Wikipedia using CBOW with position-weights, with character n-grams of length 5, a window size 5 and 10 negatives(Grave et al., 2018). In this work, we trained BMC word vectors (FastTextBMC) of 300 dimensions on the BMC corpus described above, using CBOW with position-weights and the default parameters of the original paper. (Bojanowski et al., 2017). 3.2. Contextual String Embeddings: Flair Flair refers to both a deep learning system and to a specific type of character-based contextual word embeddings. Flair (embeddings and system) have been successfully applied to sequence labeling tasks obtaining state-of-the-art results for a number of English Named Entity Recognition (NER) and Part-of-Speech tagging benchmarks (Akbik e"
2020.lrec-1.588,P19-1027,0,0.165638,"ik et al., 2018), which are built upon LSTM-based architectures and trained as language models. More recently, Devlin et al. (2019) introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to fine tune them (Heinzerling and Strube, 2019; Pires et al., 2019). When pre-training mBERT the corpora sizes in different languages are very diverse, with English corpora being order of magnitudes larger than that of the minority languages. The authors alleviate this issue by oversampling examples of lower resource languages. However, as the parameters and vocabulary is shared across the languages, this means that each language has to share the quota of substrings and parameters with the rest of the languages. As shown in the introduction, this causes tokenization problems for low resource languages. CamemBERT (Martin et al., 2019) is a"
2020.lrec-1.588,P18-1007,0,0.0195995,"the original multilingual BERT model. In this section we describe the methods used for creating the vocabulary, the model architecture, the pre-training objective and procedure. The main differences between our model and the original implementation are the corpus used for the pre-training, the algorithm for sub-word vocabulary creation and the usage of a different masking strategy that is not available for the BERTBASE model yet. Sub-word vocabulary We create a cased sub-word vocabulary containing 50,000 tokens using the unigram language model based sub-word segmentation algorithm proposed by Kudo (2018). We do not use the same algorithm as BERT because the WordPiece (Wu et al., 2016) implementation they originally used is not publicly available. We have increased the vocabulary size from 30,000 sub-word 6 5 https://fasttext.cc/ BERT language models A single layer of size 128 was used, with word reprojection and a dropout value of 0.3068. 4783 units up to 50,000 expecting to be beneficial for the Basque language due to its agglutinative nature. Our vocabulary is learned from the whole training corpus but we do not cover all the characters in order to avoid very rare ones. We set the coverage"
2020.lrec-1.588,D14-1162,0,0.0882807,"for the four tasks, our goal was not to train and use the systems in the most optimal manner. Instead, and as mentioned before, we have focused on head-to-head comparisons. In this sense, better results could be expected using variations of pretrained models that have reported better results for English (Liu et al., 2019), or making an effort in developing better adaptations to each of the tasks. 2. Related work Deep learning methods in NLP rely on the ability to represent words as continuous vectors on a low dimensional space, called word embeddings. Word2vec (Mikolov et al., 2013) or GloVe (Pennington et al., 2014) are among the best models that build word embeddings by analyzing co-occurrence patterns extracted from large corpora. FastText (Bojanowski et al., 2017) proposes an improvement over those models, consisting on embedding subword units, thereby attempting to introduce morphological information. Rich morphology languages such as Basque should especially profit from such word representations. FastText distributes embeddings for more than 150 languages trained on Common Crawl and Wikipedia. In this paper we build FastText embeddings using a carefully collected corpus in Basque and show that it pe"
2020.lrec-1.588,N18-1202,0,0.351381,"alculate one vector irrespective of the fact that the same word banku may convey different senses when used in different contexts, namely, “financial institution”,“bench”, “supply or stock”, among others. In order to address this problem, contextual word embeddings are proposed; the idea is to be able to generate different word representations ac1 http://ixa2.si.ehu.es/text-representation-models/ basque 2 Also available from Hugginface in https://huggingface. co/ixa-ehu/berteus-base-cased 3 cording to the context in which the word appears. Examples of such contextual representations are ELMO (Peters et al., 2018) and Flair (Akbik et al., 2018), which are built upon LSTM-based architectures and trained as language models. More recently, Devlin et al. (2019) introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to f"
2020.lrec-1.588,P19-1493,0,0.0201834,"ilt upon LSTM-based architectures and trained as language models. More recently, Devlin et al. (2019) introduced BERT, a model based on the transformer architecture trained as a masked language model, which has obtained very good results on a variety of NLP tasks. The multilingual counterpart of BERT, called mBERT, is a single language model pre-trained from corpora in more than 100 languages. mBERT enables to perform transfer knowledge techniques among languages, so that systems can be trained on datasets in languages different to the one used to fine tune them (Heinzerling and Strube, 2019; Pires et al., 2019). When pre-training mBERT the corpora sizes in different languages are very diverse, with English corpora being order of magnitudes larger than that of the minority languages. The authors alleviate this issue by oversampling examples of lower resource languages. However, as the parameters and vocabulary is shared across the languages, this means that each language has to share the quota of substrings and parameters with the rest of the languages. As shown in the introduction, this causes tokenization problems for low resource languages. CamemBERT (Martin et al., 2019) is a recent effort to bui"
2020.nlpcovid19-2.15,D19-1371,0,0.0195916,"n practice. The classical BM25F search algorithm (Zaragoza et al., 2004) is used to retrieve the most relevant paragraphs given a natural language question. Question Answering Given a question in natural language and a paragraph, the QA module returns the answer to the question in the paragraph or “No answer” otherwise. The implemented system is based on neural network techniques. More specifically, we have used the SciBERT language representation model, which is a pretrained language model based on BERT, but trained on a large corpus of scientific text, including text from biomedical domain (Beltagy et al., 2019). Following the usual reading comprehension method we use SciBERT as a pointer network, which selects an answer start and end index given a question and a paragraph. We used both SQuAD and QuAC to fine-tune SciBERT for QA. Manual inspection revealed that a system fine-tuned on SQuAD2.0 produced answers that were specially good for COVID related questions seeking short answers. However, we also observed that a fine-tuning with SQuAD2.0 and QuAC produced answers of better quality, particularly for questions which require longer answers. We thus decided to use a SciBERT model fine-tuned first on"
2020.nlpcovid19-2.15,2020.acl-main.652,1,0.818695,"its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the system trained on the SQuAD 2.0 dataset, which is not built with prospective users, achieves highest automatic metrics but falls behind when attending to user preference. There have been recent efforts for building QA datasets closer to real-world use cases (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020; Castelli et al., 2020). Among all of them, we show that the system trained also on the QuAC dataset is preferred by the users. The contributions of our paper are two-fold. First, we describe the QA system that is able to answer COVID related questions to aid clinical scientists to find the information they need effectively among thousands of scientific papers. Second, we report some analysis on the shortcomings that automatic evaluation of QA systems suffer from. 2 Related Work The release of the CORD-19 dataset has attracted the interest of a wide variety of researchers, who have focused on"
2020.nlpcovid19-2.15,D18-1241,0,0.0685619,"listed in the tasks of the challenge. The system is not tailored towards specific questions, and can be readily used to answer any other question. One of the challenges during development was the lack of resources for automatically evaluating the system. After submission we have been able to partially evaluate the main components of our system, using related shared-tasks and datasets that have been recently released and were not available at the time. The automatic evaluation revealed that the submitted system was not optimal, contradicting some design choices, such as using the QuAC dataset (Choi et al., 2018) to fine-tune the QA system in addition to the SQuAD 2.0 (Rajpurkar et al., 2018) dataset. We thus performed an additional A/B test to check whether users preferred the system using QuAC or not. The test confirmed our intuition, and raises questions on the suitability of automatic metrics and its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the syst"
2020.nlpcovid19-2.15,2021.ccl-1.108,0,0.0605296,"Missing"
2020.nlpcovid19-2.15,2020.nlpcovid19-acl.18,0,0.0288341,"Missing"
2020.nlpcovid19-2.15,P18-2124,0,0.206702,"cific questions, and can be readily used to answer any other question. One of the challenges during development was the lack of resources for automatically evaluating the system. After submission we have been able to partially evaluate the main components of our system, using related shared-tasks and datasets that have been recently released and were not available at the time. The automatic evaluation revealed that the submitted system was not optimal, contradicting some design choices, such as using the QuAC dataset (Choi et al., 2018) to fine-tune the QA system in addition to the SQuAD 2.0 (Rajpurkar et al., 2018) dataset. We thus performed an additional A/B test to check whether users preferred the system using QuAC or not. The test confirmed our intuition, and raises questions on the suitability of automatic metrics and its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the system trained on the SQuAD 2.0 dataset, which is not built with prospective users, a"
2020.nlpcovid19-2.15,Q19-1016,0,0.0150054,"tomatic metrics and its correlation to human satisfaction. Our findings are in line with de Vries et al. (2020), who claim that due to the tradition of prioritizing quantity of data to naturalness and real-world use cases, researchers could end up with findings that are not relevant for the real world. In this sense the system trained on the SQuAD 2.0 dataset, which is not built with prospective users, achieves highest automatic metrics but falls behind when attending to user preference. There have been recent efforts for building QA datasets closer to real-world use cases (Choi et al., 2018; Reddy et al., 2019; Campos et al., 2020; Castelli et al., 2020). Among all of them, we show that the system trained also on the QuAC dataset is preferred by the users. The contributions of our paper are two-fold. First, we describe the QA system that is able to answer COVID related questions to aid clinical scientists to find the information they need effectively among thousands of scientific papers. Second, we report some analysis on the shortcomings that automatic evaluation of QA systems suffer from. 2 Related Work The release of the CORD-19 dataset has attracted the interest of a wide variety of researchers"
2021.acl-long.506,D18-1214,0,0.0326137,"Missing"
2021.acl-long.506,P17-1042,1,0.796074,"g maps that are only locally linear (Nakashole, 2018), or learning a separate map for each word (Glavaˇs and Vuli´c, 2020). However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings (Ormazabal et al., 2019). Self-learning. While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to selflearning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial dictionary (Artetxe et al., 2017), or in a completely unsupervised manner when combined with adversarial training (Conneau et al., 2018a) or initialization heuristics (Artetxe et al., 2018; Hoshen and Wolf, 2018). Our proposed method also incorporates a self-learning procedure, showing that this technique can also be effective with non-mapping methods. Joint CLWE methods. Before the popularization of offline mapping, most CLWE methods extended monolingual embedding algorithms by either incorporating an explicit cross-lingual term in their learning objective, or directly replacing words with their translation equivalents in th"
2021.acl-long.506,P18-1073,1,0.87472,"present words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained e"
2021.acl-long.506,P19-1494,1,0.850814,"nitial dictionary has a negligible impact in the performance of our proposed method, which supports the idea that our approach converges to a similar solution given any reasonable initialization. We report our XNLI results in Table 5. We observe that our method is competitive with the baseline 5.3 Ablation study 12 In particular, most mapping methods use the official Wikipedia embeddings from fastText. Unfortunately, the preprocessed corpus used to train these embeddings is not public, so works that explore other approaches, like ours, need to use their own pre-processed copy of Wikipedia. 13 Artetxe et al. (2019) report even stronger results based on unsupervised machine translation instead of direct retrieval with CLWEs. Note, however, that their method still relies on cross-lingual embeddings to build the underlying phrase-table, so our improvements should be largely orthogonal to theirs. So as to understand the role of self-learning and the iterative restarts in our approach, we perform an ablation study and report our results in Table 6. We observe that the contribution of these components is greatly dependant on the initial dictionary. For the numeral initialization, the basic method works poorly"
2021.acl-long.506,2020.acl-main.421,1,0.777019,"pervision, ranging from bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016) to parallel or documentaligned corpora (Luong et al., 2015; Gouws et al., 2015; Vuli´c and Moens, 2016). More recently, Lample et al. (2018) reported positive results learning regular word embeddings over concatenated monolingual corpora in different languages, relying on identical words as anchor points. Wang et al. (2019) further improved this approach by applying a conventional mapping method afterwards. As shown later in our experiments, our approach outperforms theirs by a large margin. Freezing. Artetxe et al. (2020) showed that it is possible to transfer an English transformer to a new language by freezing all the inner parameters of the network and learning a new set of embeddings for the new language through masked language modeling. This works because the frozen transformer parameters constrain the resulting representations to be aligned with English. Similarly, our proposed approach uses frozen output vectors in the target language as anchor points to learn aligned embeddings in the source language. 3 Proposed method Let xi and x ˜i be the input and output vectors of the ith word in the source langua"
2021.acl-long.506,Q17-1010,0,0.0500202,"fer learning on XNLI. 6479 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6479–6489 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Word embeddings. Embedding methods learn static word representations based on co-occurrence statistics from a corpus. Most approaches use two different matrices to represent the words and the contexts, which are known as the input and output vectors, respectively (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). The output vectors play an auxiliary role, being discarded after training. Our method takes advantage of this fact, leveraging translated output vectors as anchor points to learn cross-lingual embeddings. To that end, we build on the Skip-Gram with Negative Sampling (SGNS) algorithm (Mikolov et al., 2013), which trains a binary classifier to distinguish whether each output word co-occurs with the given input word in the training corpus or was instead sampled from a noise distribution. Mapping CLWE methods. Offline mapping methods separately train word embeddings for each language, and then l"
2021.acl-long.506,D18-1269,0,0.300463,"ords in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In this paper, we propose"
2021.acl-long.506,D16-1136,0,0.0852078,"initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. 1 Introduction Cross-lingual word embeddings (CLWEs) represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hi"
2021.acl-long.506,P19-1070,0,0.0240521,"Missing"
2021.acl-long.506,2020.acl-main.675,0,0.0486004,"Missing"
2021.acl-long.506,N15-1157,0,0.0889411,"uce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. 1 Introduction Cross-lingual word embeddings (CLWEs) represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not genera"
2021.acl-long.506,D18-1043,0,0.290726,"or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint lear"
2021.acl-long.506,D19-1328,0,0.0119385,"ping systems. We complement these results with additional experiments on a downstream task, where our method obtains competitive results, as well as an ablation study and a systematic error analysis. We identify a striking tendency of our method to translate words identically, even if it has no notion of the words being identically spelled. Thanks to this, our method is particularly strong at translating named entities, but we show that our improvements are not limited to this phenomenon. These insights confirm the value of accompanying quantitative results on BLI with qualitative evaluation (Kementchedjhieva et al., 2019) and/or other tasks (Glavaˇs et al., 2019). 6486 In the future, we would like to further explore CLWE methods that go beyond the currently dominant mapping paradigm. In particular, we would like to remove the requirement of a seed dictionary altogether by using adversarial learning, and explore more elaborated context translation and dictionary re-induction schemes. Acknowledgments Aitor Ormazabal, Aitor Soroa, Gorka Labaka and Eneko Agirre were supported by the Basque Government (excellence research group IT1343-19 and DeepText project KK-2020/00088), project BigKnowledge (Ayudas Fundaci´on B"
2021.acl-long.506,J82-2005,0,0.601739,"Missing"
2021.acl-long.506,2020.coling-main.526,0,0.044508,"Missing"
2021.acl-long.506,W15-1521,0,0.164949,"oints, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task. 1 Introduction Cross-lingual word embeddings (CLWEs) represent words from two or more languages in a shared space, so that semantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assump"
2021.acl-long.506,2020.emnlp-main.215,0,0.0175638,"ds separately train word embeddings for each language, and then learn a mapping to align them into a shared space. Most of these methods align the embeddings through a linear map—often enforcing orthogonality constraints—and, as such, they rely on the assumption that the geometric structure of the separately learned embeddings is similar. This assumption has been shown to fail under unfavorable conditions, severely hindering the performance of these methods (Søgaard et al., 2018; Vuli´c et al., 2020). Existing attempts to mitigate this issue include learning non-linear maps in a latent space (Mohiuddin et al., 2020), employing maps that are only locally linear (Nakashole, 2018), or learning a separate map for each word (Glavaˇs and Vuli´c, 2020). However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings (Ormazabal et al., 2019). Self-learning. While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to selflearning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial di"
2021.acl-long.506,D18-1063,0,0.0500434,"Missing"
2021.acl-long.506,D18-1047,0,0.0161733,"a mapping to align them into a shared space. Most of these methods align the embeddings through a linear map—often enforcing orthogonality constraints—and, as such, they rely on the assumption that the geometric structure of the separately learned embeddings is similar. This assumption has been shown to fail under unfavorable conditions, severely hindering the performance of these methods (Søgaard et al., 2018; Vuli´c et al., 2020). Existing attempts to mitigate this issue include learning non-linear maps in a latent space (Mohiuddin et al., 2020), employing maps that are only locally linear (Nakashole, 2018), or learning a separate map for each word (Glavaˇs and Vuli´c, 2020). However, all these methods are supervised, and have the same fundamental limitation of aligning a set of separately trained embeddings (Ormazabal et al., 2019). Self-learning. While early mapping methods relied on a bilingual dictionary to learn the alignment, this requirement was alleviated thanks to selflearning, which iteratively re-induces the dictionary during training. This enabled learning CLWEs in a semi-supervised fashion starting from a weak initial dictionary (Artetxe et al., 2017), or in a completely unsupervise"
2021.acl-long.506,P18-2036,0,0.0134589,"ping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In this paper, we propose an alternative approach that does not have this limitation, but can still work without any parallel resources. The core idea of our method is to fix the target language embeddings, and learn aligned embeddings for the source language from scratch. This prevents structural mismatches that result from independently training embeddings in different languages, as the learning of"
2021.acl-long.506,P19-1492,1,0.897614,"languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In this paper, we propose an alternative approach that does not have this limitation, but can still work without any parallel resources. The core idea of our method is to fix the target language embeddings, and learn aligned embeddings for the source language from scratch. This prevents structural mismatches that result from independently training embeddings in different languages, as the learning of the source embeddings is tailored to each particular set of"
2021.acl-long.506,D14-1162,0,0.0995886,"o-shot crosslingual transfer learning on XNLI. 6479 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6479–6489 August 1–6, 2021. ©2021 Association for Computational Linguistics 2 Related work Word embeddings. Embedding methods learn static word representations based on co-occurrence statistics from a corpus. Most approaches use two different matrices to represent the words and the contexts, which are known as the input and output vectors, respectively (Mikolov et al., 2013; Pennington et al., 2014; Bojanowski et al., 2017). The output vectors play an auxiliary role, being discarded after training. Our method takes advantage of this fact, leveraging translated output vectors as anchor points to learn cross-lingual embeddings. To that end, we build on the Skip-Gram with Negative Sampling (SGNS) algorithm (Mikolov et al., 2013), which trains a binary classifier to distinguish whether each output word co-occurs with the given input word in the training corpus or was instead sampled from a noise distribution. Mapping CLWE methods. Offline mapping methods separately train word embeddings for"
2021.acl-long.506,P18-1072,0,0.0395946,"Missing"
2021.acl-long.506,2020.emnlp-main.257,0,0.0204108,"Missing"
2021.acl-long.506,N18-1101,0,0.0117864,"ction (BLI) and Cross-lingual Natural Language Inference (XNLI). BLI. Following common practice, we induce a bilingual dictionary through CSLS retrieval (Conneau et al., 2018a) for each set of cross-lingual embeddings, and evaluate the precision at 1 (P@1) with respect to the gold standard test dictionary from the MUSE dataset (Conneau et al., 2018a). For the few out-of-vocabulary source words, we revert to copying as a back-off strategy,9 so our reported numbers are directly comparable to prior work in terms of coverage. XNLI. We train an English natural language inference model on MultiNLI (Williams et al., 2018), and evaluate the zero-shot cross-lingual transfer performance on the XNLI test set (Conneau et al., 2018b) for the subset of our languages covered by it. To that end, we follow Glavaˇs et al. (2019) and train an Enhanced Sequential Inference Model (ESIM) on top of our original English embeddings, which are kept frozen during training. At test time, we transfer into the rest of the languages by plugging in the corresponding aligned embeddings. Note that we use the exact same English model for our proposed method and the baseline MUSE and ICP systems,10 which only differ in the set of aligned"
2021.acl-long.506,D18-1268,0,0.0349071,"Missing"
2021.acl-long.506,P17-1179,0,0.0252406,"mantically similar words in different languages are close to each other. Early work focused on jointly learning CLWEs in two languages, relying on a strong cross-lingual supervision in the form of parallel corpora (Luong et al., 2015; Gouws et al., 2015) or bilingual dictionaries (Gouws and Søgaard, 2015; Duong et al., 2016). However, these approaches were later superseded by offline mapping methods, which separately train word embeddings in different languages and align them in an unsupervised manner through self-learning (Artetxe et al., 2018; Hoshen and Wolf, 2018) or adversarial training (Zhang et al., 2017; Conneau et al., 2018a). Despite the advantage of not requiring any parallel resources, mapping methods critically rely on the underlying embeddings having a similar structure, which is known as the isometry assumption. Several authors have observed that this assumption does not generally hold, severely hindering the performance of these methods (Søgaard et al., 2018; Nakashole and Flauger, 2018; Patra et al., 2019). In later work, Ormazabal et al. (2019) showed that this issue arises from trying to align separately trained embeddings, as joint learning methods are not susceptible to it. In t"
2021.acl-long.506,N19-1161,0,0.0387267,"Missing"
agirre-etal-2010-exploring,agirre-soroa-2008-using,1,\N,Missing
agirre-etal-2010-exploring,agirre-de-lacalle-2004-publicly,1,\N,Missing
agirre-etal-2010-exploring,E09-1005,1,\N,Missing
agirre-etal-2010-exploring,H93-1061,0,\N,Missing
agirre-etal-2010-exploring,R09-1039,1,\N,Missing
agirre-etal-2010-exploring,J06-1003,0,\N,Missing
agirre-etal-2010-exploring,D07-1061,0,\N,Missing
agirre-etal-2010-exploring,N09-1003,1,\N,Missing
agirre-etal-2010-exploring,P06-1127,0,\N,Missing
agirre-etal-2012-matching,D11-1072,0,\N,Missing
agirre-etal-2012-matching,E06-1002,0,\N,Missing
agirre-etal-2012-matching,D11-1074,0,\N,Missing
agirre-etal-2012-matching,P11-1115,0,\N,Missing
agirre-etal-2012-matching,P11-1095,0,\N,Missing
agirre-etal-2012-matching,D07-1074,0,\N,Missing
agirre-soroa-2008-using,W04-0811,0,\N,Missing
agirre-soroa-2008-using,H05-1052,0,\N,Missing
agirre-soroa-2008-using,P00-1064,0,\N,Missing
agirre-soroa-2008-using,H93-1061,0,\N,Missing
agirre-soroa-2008-using,P04-1036,0,\N,Missing
alegria-etal-2008-spelling,E99-1040,0,\N,Missing
artola-etal-2014-stream,P11-1080,0,\N,Missing
artola-etal-2014-stream,W03-0804,0,\N,Missing
artola-etal-2014-stream,agerri-etal-2014-ixa,0,\N,Missing
artola-etal-2014-stream,D09-1098,0,\N,Missing
artola-etal-2014-stream,P08-2067,0,\N,Missing
C12-1054,W04-2214,0,0.0316181,"subject/ 880 a controlled vocabulary of keywords (or subject headings), which are widely used in libraries to catalogue materials and facilitate information access. Similarly, in the medical domain MeSH4 , created and maintained by the National Library of Medicine, provides a controlled vocabulary of medical subject headings. In computational linguistics, WordNet (Fellbaum, 1998) is a commonly used lexical knowledge base that links concepts in various ways. WordNet has been expanded with WordNet domain labels which group together words from different syntactic categories and different senses (Bentivogli et al., 2004). These domain labels are organised into a hierarchical structure. There is a body of previous work on automatically deriving taxonomies and relations from free text. Hearst (1992) was perhaps the earliest significant effort to derive hyponym-hypernym relations from free text using the now eponymous Hearst patterns which code common syntactic forms of the hyponymy pattern (e.g. ‘Vehicles such as Cars’). These patterns were hand-crafted. More recently Snow et al. (2004) developed on this work by using an existing knowledge base to automatically derive lexico-syntactic patterns containing the hy"
C12-1054,C92-2082,0,0.28413,"domain MeSH4 , created and maintained by the National Library of Medicine, provides a controlled vocabulary of medical subject headings. In computational linguistics, WordNet (Fellbaum, 1998) is a commonly used lexical knowledge base that links concepts in various ways. WordNet has been expanded with WordNet domain labels which group together words from different syntactic categories and different senses (Bentivogli et al., 2004). These domain labels are organised into a hierarchical structure. There is a body of previous work on automatically deriving taxonomies and relations from free text. Hearst (1992) was perhaps the earliest significant effort to derive hyponym-hypernym relations from free text using the now eponymous Hearst patterns which code common syntactic forms of the hyponymy pattern (e.g. ‘Vehicles such as Cars’). These patterns were hand-crafted. More recently Snow et al. (2004) developed on this work by using an existing knowledge base to automatically derive lexico-syntactic patterns containing the hyponym-hypernym pairs. An alternative to creating hierarchies of concepts from the pattern-based methods is to use statistical methods. Sanderson and Croft (1999) used an approach t"
C14-1213,E09-1005,1,0.730288,", 2011). Nowadays it is one of the most widely used NED systems and attains performances close to state-of-the-art (Daiber et al., 2013)We used the default values of the parameters for all the experiments in this paper. We also tested an in-house reimplementation of the generative probabilistic model presented in (Han and Sun, 2011). This is a state-of-the-art system which got the same accuracy as the best participant (72.0) when evaluated in the non-NIL subset of TAC2013. UKB is a freely-available system for performing Word Sense Disambiguation and Similarity based on random walks on graphs (Agirre and Soroa, 2009). Instead of using it on WordNet, we represented Wikipedia as a graph, where vertices are the wikipedia articles and edges represents bidirectional hyperlinks among Wikipedia pages, effectively implementing a NED system. We used a Wikipedia dump from 2013 in our experiments. UKB is a competitive, state-of-the-art system which attained a score of 69.0 when evaluated in the non-NIL subset of the TAC2013 dataset. The input of the systems is the context of each mention to be disambiguated, in the form of a 100 token window centered in the target mention. In NED, the identification of the correct m"
C14-1213,W09-2404,0,0.0628236,"Missing"
C14-1213,H92-1045,0,0.750493,"Missing"
C14-1213,P11-1095,0,0.286581,"Missing"
C14-1213,P03-1054,0,0.00849852,"yntactic collocations and around 250 examples of propositions. Note that both AIDA and TAC2009 contain mentions that were not linked to a Wikipedia article because the mention referred to an entity which was not listed in the entity inventory. We ignored all those cases (called NIL cases), as we would need to investigate, for each NIL, which actual entity they refer to. The collocations were extracted from the TAC KBP collection (Ji et al., 2010), comprising 1.7 million documents, 1.3 millions from newswire and 0.5 millions from the web. We have parsed them with the Stanford CoreNLP software (Klein and Manning, 2003), obtaining around 650 million dependencies (De Marneffe and Manning, 2008). We selected subject, object, prepositional complements and adjectival modifiers as the source for syntactic collocations. In order to provide more specific collocations, we implemented the syntactic patterns proposed in (Pe˜nas and Hovy, 2010), which produce so-called propositions. The result is a database with 16 million distinct propositions. Table 1 shows the six patterns used in this work, together with some examples. In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia which l"
C14-1213,J13-4004,0,0.0608289,"Missing"
C14-1213,W00-1326,1,0.571223,"Missing"
C14-1213,C10-2113,1,0.863781,"Missing"
C14-1213,spitkovsky-chang-2012-cross,0,0.0439696,"t, prepositional complements and adjectival modifiers as the source for syntactic collocations. In order to provide more specific collocations, we implemented the syntactic patterns proposed in (Pe˜nas and Hovy, 2010), which produce so-called propositions. The result is a database with 16 million distinct propositions. Table 1 shows the six patterns used in this work, together with some examples. In order to know whether a mention is ambiguous, we built a dictionary based on Wikipedia which lists, for each string mention, which entities it can refer to. We followed the construction method of (Spitkovsky and Chang, 2012), which checked article titles, redirects, disambiguation pages and hyperlinks to find mention strings that can be used to refer to entities. Contrary to them, we could not access hyperlinks in the web, so we could use only those in Wikipedia. According to our dictionary, the ambiguity of the mentions that we are studying is very high, 26.4 entities on average for the mentions in AIDA, and 62.6 entities on average for the mentions in TAC2009. 3 One entity per discourse In order to estimate OSPD we divided the number of times a mention string referred to different entities in the document with"
C14-1213,H93-1052,0,0.509804,"Missing"
C14-1213,D11-1072,0,\N,Missing
C14-1213,W08-1301,0,\N,Missing
E09-1005,agirre-soroa-2008-using,1,0.891153,"awbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities. Graphbased WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context. In this sense, they provide a principled solution to the exponential explosion problem, with excellent performance."
E09-1005,atserias-etal-2004-spanish,0,0.0259038,"uced, and hence the rank of node j increases. Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have. Alternatively, PageRank can also be viewed as the result of a random walk process, where the final rank of node i represents the probability of a random walk over the graph ending on node i, at a sufficiently large time. Let G be a graph with N vertices v1 , . . . , vN and di be the outdegree of node i; let M be a 2 (1) http://ixa2.si.ehu.es/ukb 34 • MCR16 + Xwn: The Multilingual Central Repository (Atserias et al., 2004b) is a lexical knowledge base built within the MEANING project3 . This LKB comprises the original WordNet 1.6 synsets and relations, plus some relations from other WordNet versions automatically mapped4 into version 1.6: WordNet 2.0 relations and eXtended WordNet relations (Mihalcea and Moldovan, 2001) (gold, silver and normal relations). The resulting graph has 99, 632 vertices and 637, 290 relations. damping factors. Some preliminary experiments with higher iteration counts showed that although sometimes the node ranks varied, the relative order among particular word synsets remained stable"
E09-1005,H92-1046,0,0.381008,"asque Country Donostia, Basque Contry {e.agirre,a.soroa}@ehu.es Abstract Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004). One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable pro"
E09-1005,P00-1064,0,0.00675098,"ded WordNet. Here the differences are in many cases significant. These results are surprising, as we would expect that the manually disambiguated gloss relations from WordNet 3.0 would lead to better results, compared to the automatically disambiguated gloss relations from the eXtended WordNet (linked to version 1.7). The lower performance of WNet30+gloss can be due to the fact that the Senseval all words data set is tagged using WordNet 1.7 synsets. When using a different LKB for WSD, a mapping to WordNet 1.7 is required. Although the mapping is cited as having a correctness on the high 90s (Daude et al., 2000), it could have introduced sufficient noise to counteract the benefits of the hand-disambiguated glosses. Table 1 also shows the most frequent sense (MFS), as well as the best supervised systems (Snyder and Palmer, 2004; Palmer et al., 2001) that participated in each competition (SMUaw and GAMBL, respectively). The MFS is a baseline for supervised systems, but it is consid5 Comparison to Related work In this section we will briefly describe some graph-based methods for knowledge-based WSD. The methods here presented cope with the problem of sequence-labeling, i.e., they disambiguate all the wo"
E09-1005,S07-1008,0,0.0363883,"Missing"
E09-1005,P04-1036,0,0.72456,"Missing"
E09-1005,C96-1005,1,0.319104,"rre,a.soroa}@ehu.es Abstract Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk, 1986; McCarthy et al., 2004). One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal sol"
E09-1005,H05-1052,0,0.855302,"of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations can grow exponentially with the number of words. Although alternatives like simulated annealing (Cowie et al., 1992) and conceptual density (Agirre and Rigau, 1996) were tried, most of past knowledge based WSD was done in a suboptimal word-by-word process, i.e., disambiguating words one at a time. Recently, graph-based methods for knowledgebased WSD have gained much attention in the NLP community (Sinha and Mihalcea, 2007; Navigli and Lapata, 2007; Mihalcea, 2005; Agirre and Soroa, 2008). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Because the graph is analyzed as a whole, these techniques have the remarkable property of being able to find globally optimal solutions, given the relations between entities. Graphbased WSD methods are particularly suited for disambiguating word sequences, and they manage to exploit the interrelations among the senses in the given context. In this sense, they provide a principled solution to the exponential explosion problem, wi"
E09-1005,S01-1005,0,0.0476183,"ull graph of the LKB efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context. Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build. Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin. As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence. 1 This baseline consists of tagging all occurrences in the test data with the sense of the wo"
E09-1005,S07-1016,0,0.00893313,"Missing"
E09-1005,W04-0811,0,0.653325,"efficiently, performing better than previous approaches in English all-words datasets. We also show that the algorithm can be easily ported to other languages with good results, with the only requirement of having a wordnet. In addition, we make an analysis of the performance of the algorithm, showing that it is efficient and that it could be tuned to be faster. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology that automatically chooses the intended sense of a word in context. Supervised WSD systems are the best performing in public evaluations (Palmer et al., 2001; Snyder and Palmer, 2004; Pradhan et al., 2007) but they need large amounts of hand-tagged data, which is typically very expensive to build. Given the relatively small amount of training data available, current state-of-the-art systems only beat the simple most frequent sense (MFS) baseline1 by a small margin. As an alternative to supervised systems, knowledge-based WSD systems exploit the information present in a lexical knowledge base (LKB) to perform WSD, without using any further corpus evidence. 1 This baseline consists of tagging all occurrences in the test data with the sense of the word that occurs more often"
E09-1005,C92-1056,0,\N,Missing
J14-1003,N09-1003,1,0.0595825,"Missing"
J14-1003,P08-1037,1,0.0810727,"Missing"
J14-1003,P11-2123,1,0.760424,"wledge on a variety of English data sets and a data set on Spanish. We include a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are publicly available, and the results easily reproducible. 1. Introduction Word Sense Disambiguation (WSD) is a key enabling technology that automatically chooses the intended sense of a word in context. It has been the focus of intensive research since the beginning of Natural Language Processing (NLP), and more recently it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan, ¨ Ng, and Chiang 2007), information retrieval (P´erez-Aguera and Zaragoza 2008; Zhong and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and ∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es. ∗∗ IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country. E-mail: oier.lopezdelacalle@gmail.com. † Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es. Submission received: 30 July 2011; Revised submission received"
J14-1003,C96-1005,1,0.199438,"ditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk 1986; Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word overlaps between definitions of the words (Lesk 1986) to finding distances between concepts following the structure of the LKB (Patwardhan, Banerjee, and Pedersen 2003). Usually the distances are calculated using only hierarchical relations on the LKB (Sussna 1993; Agirre and Rigau 1996). Combining both intuitions, Jiang and Conrath (1997) present a metric that combines statistics from corpus and a lexical taxonomy structure. One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations grows exponentially with the number of words—that is, for a sequence of n words where each has up to k senses they need to consider up to kn sense sequences. Although alternatives like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density 1 http://ixa2.si.ehu.es/ukb. 59 Computational"
J14-1003,agirre-soroa-2008-using,1,0.120686,"to k senses, the algorithms had to consider up to kn sense sequences. Greedy methods were often used to avoid the combinatorial explosion (Patwardhan, Banerjee, and Pedersen 2003). As an alternative, graph-based methods are able to exploit the structural properties of the graph underlying a particular LKB. These methods are able to consider all possible combinations of occurring senses on a particular context, and thus offer a way to analyze efficiently the inter-relations among them, gaining much attention in the NLP community (Mihalcea 2005; Navigli and Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010). The nodes in the graph represent the concepts (word senses) in the LKB, and edges in the graph represent relations between them, such as subclass and part-of. Network analysis techniques based on random walks like PageRank (Brin and Page 1998) can then be used to choose the senses that are most relevant in the graph, and thus output those senses. 58 ´ Agirre, Lopez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD In order to deal with large knowledge bases containing more than 100,000 concepts (Fellbaum 1998), previous algorithms had to extract subsets of"
J14-1003,E09-1005,1,0.450244,"walks over large LKBs. The algorithm outperforms other graph-based algorithms when using a LKB built from WordNet and eXtended WordNet. The algorithm and LKB combination compares favorably to the state-of-the-art in knowledge-based WSD on a wide variety of data sets, including four English and one Spanish data set. (2) A detailed analysis of the factors that affect the algorithm. (3) The algorithm together with the corresponding graphs are publicly available1 and can be applied easily to sense inventories and knowledge bases different from WordNet. The algorithm for WSD was first presented in Agirre and Soroa (2009). In this article, we present further evaluation on two more recent data sets, analyze the parameters and options of the system, compare it to the state of the art, and discuss the relation of our algorithm with PageRank and the MFS heuristic. 2. Related Work Traditional knowledge-based WSD systems assign a sense to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk 1986; Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word ove"
J14-1003,S07-1070,0,0.0660895,"Missing"
J14-1003,atserias-etal-2004-spanish,0,0.247279,"Missing"
J14-1003,W97-0703,0,0.229726,"Missing"
J14-1003,D07-1007,0,0.0106246,"nd a data set on Spanish. We include a detailed analysis of the factors that affect the algorithm. The algorithm and the LKBs used are publicly available, and the results easily reproducible. 1. Introduction Word Sense Disambiguation (WSD) is a key enabling technology that automatically chooses the intended sense of a word in context. It has been the focus of intensive research since the beginning of Natural Language Processing (NLP), and more recently it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan, ¨ Ng, and Chiang 2007), information retrieval (P´erez-Aguera and Zaragoza 2008; Zhong and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and ∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es. ∗∗ IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country. E-mail: oier.lopezdelacalle@gmail.com. † Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es. Submission received: 30 July 2011; Revised submission received: 15 November 2012; Accepted for publicati"
J14-1003,P07-1005,0,0.0890249,"Missing"
J14-1003,S07-1054,0,0.30332,"Missing"
J14-1003,H92-1046,0,0.637309,"Missing"
J14-1003,W06-1663,0,0.0450149,"e used semi-automatic and fully automatic methods to enrich WordNet with additional relations. Mihalcea and Moldovan (2001) disambiguated WordNet glosses in a resource called eXtended WordNet. The disambiguated glosses have been shown to improve results of a graph-based system (Agirre and Soroa 2008), and we have also used them in our experiments. Navigli and Velardi (2005) enriched WordNet with cooccurrence relations semi-automatically and showed that those relations are effective in a number of graph-based WSD systems (Navigli and Velardi 2005; Navigli and Lapata 2007, 2010). More recently, Cuadros and Rigau (2006, 2007, 2008) learned automatically so-called KnowNets, and showed that the new provided relations improved WSD performance when plugged into a simple vector-based WSD system. Finally, Ponzetto and Navigli (2010) have acquired relations automatically from Wikipedia, released as WordNet++, and have shown that they are beneficial in a graph-based WSD algorithm. All of these relations are publicly available with the exception of Navigli and Velardi (2005), but note that the system is available on-line.2 Disambiguation is typically performed by applying a ranking algorithm over the graph, and then"
J14-1003,S07-1015,0,0.148169,"Missing"
J14-1003,P00-1064,0,0.178159,"Missing"
J14-1003,W04-0827,0,0.0722021,"Missing"
J14-1003,W00-1322,0,0.246028,"Missing"
J14-1003,D07-1061,0,0.0475121,"nitialize the random walk with the words in the context of the target word, and thus we obtain a context-dependent PageRank. We will show that this method is indeed effective for WSD. Note that in order to use other centrality algorithms (e.g., HITS [Kleinberg 1998]), previous authors had to build a subgraph first. In principle, those algorithms could be made context-dependent when using the full graph and altering their formulae, but we are not aware of such variations. Random walks over WordNet using Personalized PageRank have been also used to measure semantic similarity between two words (Hughes and Ramage 2007; Agirre 61 Computational Linguistics Volume 40, Number 1 et al. 2009). In those papers, the random walks are initialized with a single word, whereas we use all content words in the context. The results obtained by the authors, especially in the latter paper, are well above other WordNet-based methods. Most previous work on knowledge-based WSD has presented results on one or two general domain corpora for English. We present our results on four general domain data sets for English and a Spanish data set (M`arquez et al. 2007). Alternatively, some researchers have applied knowledge-based WSD to"
J14-1003,O97-1002,0,0.381894,"to an ambiguous word by comparing each of its senses with those of the surrounding context. Typically, some semantic similarity metric is used for calculating the relatedness among senses (Lesk 1986; Patwardhan, Banerjee, and Pedersen 2003). The metric varies between counting word overlaps between definitions of the words (Lesk 1986) to finding distances between concepts following the structure of the LKB (Patwardhan, Banerjee, and Pedersen 2003). Usually the distances are calculated using only hierarchical relations on the LKB (Sussna 1993; Agirre and Rigau 1996). Combining both intuitions, Jiang and Conrath (1997) present a metric that combines statistics from corpus and a lexical taxonomy structure. One of the major drawbacks of these approaches stems from the fact that senses are compared in a pairwise fashion and thus the number of computations grows exponentially with the number of words—that is, for a sequence of n words where each has up to k senses they need to consider up to kn sense sequences. Although alternatives like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density 1 http://ixa2.si.ehu.es/ukb. 59 Computational Linguistics Volume 40, Number 1 (Agirre and Rigau 19"
J14-1003,H05-1053,0,0.230575,"Missing"
J14-1003,S07-1008,0,0.0437951,"Missing"
J14-1003,J07-4005,0,0.0556294,"each has up to k senses they need to consider up to kn sense sequences. Although alternatives like simulated annealing (Cowie, Guthrie, and Guthrie 1992) and conceptual density 1 http://ixa2.si.ehu.es/ukb. 59 Computational Linguistics Volume 40, Number 1 (Agirre and Rigau 1996) were tried, most of the knowledge-based WSD at the time was done in a suboptimal word-by-word greedy process, namely, disambiguating words one at a time (Patwardhan, Banerjee, and Pedersen 2003). Still, some recent work on finding predominant senses in domains has applied such similarity-based techniques with success (McCarthy et al. 2007). Recently, graph-based methods for knowledge-based WSD have gained much attention in the NLP community (Mihalcea 2005; Navigli and Velardi 2005; Navigli and Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010). These methods use well-known graph-based techniques to find and exploit the structural properties of the graph underlying a particular LKB. Graph-based techniques consider all the sense combinations of the words occurring on a particular context at once, and thus offer a way to analyze the relations among them with respect to the whole graph. They are p"
J14-1003,H05-1052,0,0.253866,"pairwise similarity for a sequence of n words where each has up to k senses, the algorithms had to consider up to kn sense sequences. Greedy methods were often used to avoid the combinatorial explosion (Patwardhan, Banerjee, and Pedersen 2003). As an alternative, graph-based methods are able to exploit the structural properties of the graph underlying a particular LKB. These methods are able to consider all possible combinations of occurring senses on a particular context, and thus offer a way to analyze efficiently the inter-relations among them, gaining much attention in the NLP community (Mihalcea 2005; Navigli and Lapata 2007; Sinha and Mihalcea 2007; Agirre and Soroa 2008; Navigli and Lapata 2010). The nodes in the graph represent the concepts (word senses) in the LKB, and edges in the graph represent relations between them, such as subclass and part-of. Network analysis techniques based on random walks like PageRank (Brin and Page 1998) can then be used to choose the senses that are most relevant in the graph, and thus output those senses. 58 ´ Agirre, Lopez de Lacalle, and Soroa Random Walks for Knowledge-Based WSD In order to deal with large knowledge bases containing more than 100,000"
J14-1003,H93-1061,0,0.65186,"Missing"
J14-1003,S07-1043,0,0.0235631,"plementation in the next subsection which shows that, when using the same LKB, our method obtains better results. (2) Although not reported in the table, an unsupervised system using automatically acquired training examples from bilingual data (Chan and Ng 2005) obtained very good results on S2AW nouns (77.2 F1, compared with our 70.3 F1 in Table 2). The automatically acquired training examples are used in addition to hand-annotated data in Zhong10 (Zhong and Ng 2010), also reported in the table (see below). We report the best unsupervised systems in S07AW and S07CG on the same row. JU-SKNSB (Naskar and Bandyopadhyay 2007) is a system based on an extended version Table 3 Comparison with state-of-the-art results (F1). The top rows report knowledge-based and unsupervised systems, followed by our system (PPRw2w ). Below we report systems that use annotated data to some degree: (1) MFS or counts from hand-annotated corpora, (2) fully supervised systems, including the best supervised participants in each exercise. Best result among unsupervised systems in each column is shown in bold. Please see text for references of each system. System S2AW S3AW Mih05 Sinha07 Tsatsa10 Agirre08 Nav10 JU-SKNSB / TKB-UO Ponz10 54.2 5"
J14-1003,P96-1006,0,0.626408,"Missing"
J14-1003,W97-0201,0,0.55767,"Missing"
J14-1003,S01-1005,0,0.0511821,"Missing"
J14-1003,P10-1154,0,0.308589,"stance, Chan and Ng (2005) present an unsupervised method to obtain training examples from bilingual data, which was used together with SemCor and DSO to train one of the best performing supervised systems to date (Zhong and Ng 2010). In view of the problems of supervised systems, knowledge-based WSD is emerging as a powerful alternative. Knowledge-based WSD systems exploit the information in a lexical knowledge base (LKB) to perform WSD. They currently perform below supervised systems on general domain data, but are attaining performance close or above MFS without access to hand-tagged data (Ponzetto and Navigli 2010). In this sense, they provide a complementary strand of research which could be combined with supervised ´ methods, as shown for instance in Navigli (2008). In addition, Agirre, Lopez de Lacalle, and Soroa (2009) show that knowledge-based WSD systems can outperform supervised systems in a domain-specific data set, where MFS from general domains also fails. In this article, we will focus our attention on knowledge-based methods. Early work for knowledge-based WSD was based on measures of similarity between pairs of concepts. In order to maximize pairwise similarity for a sequence of n words whe"
J14-1003,W04-0811,0,0.238937,"Missing"
J14-1003,W04-0856,0,0.170834,"Missing"
J14-1003,P08-1082,0,0.0168788,"Missing"
J14-1003,S07-1057,0,0.0964627,"is derived from hand-annotated corpora. In the case of the English WordNet, the use of the first sense also falls in this category, as the order of senses in WordNet is based on sense counts in hand-annotated corpora. Note that for wordnets in other languages, hand-annotated corpus is scarce, and thus our main results do not use this information. Section 6.4.7 analyzes the results of our system when combined with this information. Among supervised systems, the best supervised systems at competition time are reported in a single row (Mihalcea 2002; Decadt et al. 2004; Chan, Ng, and Zhong 2007; Tratz et al. 2007). We also report Zhong10 (Zhong and Ng 2010), which is a freely available supervised system giving some of the strongest results in WSD. We will now discuss in detail the systems that are most similar to our own. We first review the WordNet versions and relations used by each system. Mih05 (Mihalcea 2005) and Sinha07 (Sinha and Mihalcea 2007) apply several similarity methods, which use WordNet information from versions 1.7.1 and 2.0, respectively, including all relations and the text in the glosses.5 Tsatsa10 (Tsatsaronis, Varlamis, and Nørv˚ag 2010) uses WordNet 2.0. Agirre08 (Agirre and Soro"
J14-1003,P10-4014,0,0.878024,": (1) Nav10 (Navigli and Lapata 2010) obtained better results on S07AW. We will compare both systems in more detail below, and also include a reimplementation in the next subsection which shows that, when using the same LKB, our method obtains better results. (2) Although not reported in the table, an unsupervised system using automatically acquired training examples from bilingual data (Chan and Ng 2005) obtained very good results on S2AW nouns (77.2 F1, compared with our 70.3 F1 in Table 2). The automatically acquired training examples are used in addition to hand-annotated data in Zhong10 (Zhong and Ng 2010), also reported in the table (see below). We report the best unsupervised systems in S07AW and S07CG on the same row. JU-SKNSB (Naskar and Bandyopadhyay 2007) is a system based on an extended version Table 3 Comparison with state-of-the-art results (F1). The top rows report knowledge-based and unsupervised systems, followed by our system (PPRw2w ). Below we report systems that use annotated data to some degree: (1) MFS or counts from hand-annotated corpora, (2) fully supervised systems, including the best supervised participants in each exercise. Best result among unsupervised systems in each"
J14-1003,P12-1029,0,0.0190084,"rithm and the LKBs used are publicly available, and the results easily reproducible. 1. Introduction Word Sense Disambiguation (WSD) is a key enabling technology that automatically chooses the intended sense of a word in context. It has been the focus of intensive research since the beginning of Natural Language Processing (NLP), and more recently it has been shown to be useful in several tasks such as parsing (Agirre, Baldwin, and Martinez 2008; Agirre et al. 2011), machine translation (Carpuat and Wu 2007; Chan, ¨ Ng, and Chiang 2007), information retrieval (P´erez-Aguera and Zaragoza 2008; Zhong and Ng 2012), question answering (Surdeanu, Ciaramita, and Zaragoza 2008), and ∗ Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: e.agirre@ehu.es. ∗∗ IKERBASQUE, Basque Foundation for Science, 48011, Bilbao, Basque Country. E-mail: oier.lopezdelacalle@gmail.com. † Informatika Fakultatea, Manuel Lardizabal 1, 20018 Donostia, Basque Country. E-mail: a.soroa@ehu.es. Submission received: 30 July 2011; Revised submission received: 15 November 2012; Accepted for publication: 17 February 2013. doi:10.1162/COLI a 00164 © 2014 Association for Computational Linguistics Computatio"
J14-1003,S07-1006,0,\N,Missing
J14-1003,C92-1056,0,\N,Missing
J14-1003,S07-1016,0,\N,Missing
J14-1003,J14-4005,0,\N,Missing
K18-1017,D17-1277,0,0.210086,"Missing"
K18-1017,C14-1213,1,0.894811,"Missing"
K18-1017,P16-1059,0,0.403782,"s; (2) a context model which measures to which extent the entities fit well in the context of the mention, using textual features; (3) a coherence model which prefers entities that are related to the other entities in the document. The first and second models are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions occurring more than 10 times in Wikipedia. While this has the advantage of reusing the parameters across mentions, it also makes the probl"
K18-1017,P16-1179,1,0.857083,"Missing"
K18-1017,P11-1095,0,0.0120078,"ed on indomain training data. 1 Introduction Named Entity Disambiguation (NED), also known as Entity Linking or Entity Resolution, is a task where entity mentions in running text need to be linked to its entity entry in a Knowledge Base (KB), such as Wikidata, Wikipedia or other derived resources like DBpedia (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hoffart et al., 2011). This task is challenging, as some entity mentions like “London” can refer to a number of places, people, fictional characters, brands, movies, books or songs. Given a mention in context, NED methods (Cucerzan, 2007; Han and Sun, 2011; Ratinov et al., 2011; Lazic et al., 2015) typically rely on three models: (1) a mention model which collects possible entities which can be referred to by the 171 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 171–180 c Brussels, Belgium, October 31 - November 1, 2018. 2018 Association for Computational Linguistics of mentions having very limited training data, e.g. 10 occurrences linking to an entity in Wikipedia. Our results will show that data-augmentation and transfer learning allow us to overcome the sparseness problem, yielding the bes"
K18-1017,E06-1002,0,0.089015,"ounts. Transferring an LSTM which is learned on all datasets is the most effective context representation option for the word experts in all frequency bands. The experiments show that our system trained on out-ofdomain Wikipedia data surpasses comparable NED systems which have been trained on indomain training data. 1 Introduction Named Entity Disambiguation (NED), also known as Entity Linking or Entity Resolution, is a task where entity mentions in running text need to be linked to its entity entry in a Knowledge Base (KB), such as Wikidata, Wikipedia or other derived resources like DBpedia (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hoffart et al., 2011). This task is challenging, as some entity mentions like “London” can refer to a number of places, people, fictional characters, brands, movies, books or songs. Given a mention in context, NED methods (Cucerzan, 2007; Han and Sun, 2011; Ratinov et al., 2011; Lazic et al., 2015) typically rely on three models: (1) a mention model which collects possible entities which can be referred to by the 171 Proceedings of the 22nd Conference on Computational Natural Language Learning (CoNLL 2018), pages 171–180 c Brussels, Belgium, October 31 - November 1, 2"
K18-1017,D11-1072,0,0.365745,"Missing"
K18-1017,D18-2029,0,0.0677648,"Missing"
K18-1017,L16-1139,1,0.901558,"Missing"
K18-1017,Q15-1036,0,0.0617361,"in the document. The first and second models are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions occurring more than 10 times in Wikipedia. While this has the advantage of reusing the parameters across mentions, it also makes the problem unnecessarily complex. In this paper we propose to break the task of NED into 500K classification tasks, one for each target mention, as opposed to building a single model for all 500K mentions. The advanta"
K18-1017,Q15-1023,0,0.0127726,"Lazic et al., 2015) sup. Sparse BoW Continuous BoW LSTM Transfer LSTM (Lazic et al., 2015)† semi-sup. (Chang et al., 2016)* (Yamada et al., 2016)* Local & Global models (Cucerzan, 2012)* (Chisholm and Hachey, 2015)* (Globerson et al., 2016)* (Yamada et al., 2016)* tac10 tac11 tac12 — 85.82 86.96 86.73 87.32 — 84.5* 84.6* 74.5 80.25 81.55 81.44 84.41 79.3† — — 68.7 63.12 67.49 67.32 72.58 74.2† — — — 80.7* 87.2* 85.2* — — 84.3* 72.0* — 82.4* (2012) present a detailed overview of all possible components, but in this section we will focus on the most relevant high performing systems. Please see (Ling et al., 2015) for a more detailed review of past research. 4.1 Among local systems that are trained on Wikipedia alone, (Lazic et al., 2015) was the best performing one to date. Their system is based on probabilistic estimation, with a rich preprocessing pipeline, including dependency parsing, common noun phrase identificacion and coreference resolution. They present the results for both a supervised version, and a graphbased semi-supervised extension which improves results. We think that the results of our method could be improved using richer pre-processing, specially the use of coreference to find longe"
K18-1017,N15-1026,0,0.0365803,"Missing"
K18-1017,N18-1202,0,0.181126,"mada et al., 2016; Sil et al., 2018), ours is trained on Wikipedia and tested out-of-domain. From another perspective, a set of 500K classification problems provides a great experimental framework for testing text representation and classification algorithms. More specifically, deep learning methods provide end-to-end algorithms to learn both representations and classifiers jointly (LeCun et al., 2015). In fact, learning text representations models has become a center topic in natural language understanding, as it allows to transfer representation models across tasks (Conneau and Kiela, 2018; Peters et al., 2018; Wang et al., 2018). In this paper, we explore several popular text representation options, as well as dataaugmentation (Zhang and LeCun, 2015) and transfer learning (Bengio, 2012). All training examples and models in this paper, as well as the pytorch code to reproduce results is availabe 1 . This paper is structured as follows. We first present our models. Section 3 presents the experiments, followed by related work and conclusions. 2 On the other hand, Wikipedia editors have manually added hyperlinks to articles, where the anchor text corresponds to the mention, and the url corresponds to"
K18-1017,P11-1138,0,0.548433,"barrena,a.soroa,e.agirre@ehu.eus Abstract mention string (aliases or surface forms), possibly weighted according to prior probabilities; (2) a context model which measures to which extent the entities fit well in the context of the mention, using textual features; (3) a coherence model which prefers entities that are related to the other entities in the document. The first and second models are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions oc"
K18-1017,spitkovsky-chang-2012-cross,0,0.0211468,"5) and transfer learning (Bengio, 2012). All training examples and models in this paper, as well as the pytorch code to reproduce results is availabe 1 . This paper is structured as follows. We first present our models. Section 3 presents the experiments, followed by related work and conclusions. 2 On the other hand, Wikipedia editors have manually added hyperlinks to articles, where the anchor text corresponds to the mention, and the url corresponds to the entity. We first built a candidate model as a dictionary that links each text anchor to possible entities, using the method presented in (Spitkovsky and Chang, 2012; Barrena et al., 2016). Let M be the set of all unique mention strings m, E the set of all target entities e, and Em = {e1 , . . . , em } the set of entities that can be referred by mention m. We kept the 30 most frequent candidates for each mention for the sake of efficiency. We report the sizes of E and M below. We then extracted annotated examples by scanning through the page contents for hyperlinks that link anchors (the mentions) to the corresponding Wikipedia pages (the entities). For each such hyperlink, we build a context c by first tokenizing and removing the stop words, and then ext"
K18-1017,W18-5446,0,0.164984,"l et al., 2018), ours is trained on Wikipedia and tested out-of-domain. From another perspective, a set of 500K classification problems provides a great experimental framework for testing text representation and classification algorithms. More specifically, deep learning methods provide end-to-end algorithms to learn both representations and classifiers jointly (LeCun et al., 2015). In fact, learning text representations models has become a center topic in natural language understanding, as it allows to transfer representation models across tasks (Conneau and Kiela, 2018; Peters et al., 2018; Wang et al., 2018). In this paper, we explore several popular text representation options, as well as dataaugmentation (Zhang and LeCun, 2015) and transfer learning (Bengio, 2012). All training examples and models in this paper, as well as the pytorch code to reproduce results is availabe 1 . This paper is structured as follows. We first present our models. Section 3 presents the experiments, followed by related work and conclusions. 2 On the other hand, Wikipedia editors have manually added hyperlinks to articles, where the anchor text corresponds to the mention, and the url corresponds to the entity. We first"
K18-1017,K16-1025,0,0.451098,"s are local in that they only require a short context of occurrence and disambiguate each mention in the document separately. The third model is global, in that all mentions are disambiguated simultaneously (Ratinov et al., 2011). Recent work has shown that local models can be improved adding a global coherence model (Ratinov et al., 2011; Globerson et al., 2016). In this work we focus on a local model, and a global model could improve the results further. All local and global systems mentioned above, as well as the current state-of-the-art systems (Lazic et al., 2015; Globerson et al., 2016; Yamada et al., 2016; Ganea and Hofmann, 2017), rely on single models for each of the above, that is, they have a single mention model, context model and coherence model for all entities, e.g. the 500K ambiguous entity mentions occurring more than 10 times in Wikipedia. While this has the advantage of reusing the parameters across mentions, it also makes the problem unnecessarily complex. In this paper we propose to break the task of NED into 500K classification tasks, one for each target mention, as opposed to building a single model for all 500K mentions. The advantage of this approach is that each of the 500K"
L16-1639,J14-1003,1,0.786045,"Missing"
L16-1639,W12-3407,0,0.0305896,"Missing"
L16-1639,C96-2187,0,0.619254,"Missing"
L16-1639,diaz-de-ilarraza-etal-2004-abar,0,0.114192,"Missing"
L16-1639,P98-1062,0,0.17283,"ess is meant to add information that cannot be described by existing layers, it simply adds a new annotation layer. Any previous layers remain intact and can still be used by other processes. Layers are connected by references from one layer to items in another (lower-level) layer. The design goals and main characteristics of the AWA scheme enable multi-level interoperability among linguistic processors, and nowadays there exist several NLP tools that produce and consume AWA documents, such as Morpheus (Alegria et al., 1996), a wide-coverage morphosyntactic analyzer for Basque, and Eustagger (Ezeiza et al., 1998), a general-purpose tagger/lemmatizer. The representation schemes corresponding to the annotations of the following processors have also been defined: Ixati (Alegria et al., 2006), a shallow parser, which includes the NERC tool Eihera+; Edgk/MaltIxa (Aranzabe et al., 2012), a dependency grammar parser; EusWN (Agirre et al., 2014), a word-sense disambiguator that uses an algorithm based on random walks over the Basque WordNet; and a namedentity disambiguator that links entity names with the corresponding Wikipedia pages (Fernandez et al., 2011). In addition, several tools have been developed to"
L16-1639,P98-1063,0,\N,Missing
L16-1639,C98-1060,0,\N,Missing
L16-1714,artola-etal-2014-stream,1,0.527528,"pport a flexible modular pipeline. The details of this approach will be discussed in Section 4. The live streaming computing approach presented in Section 5. relies on the Apache Storm framework6 for implementing scalable processing of data streams. 3 https://hadoop.apache.org/ http://hadoop.apache.org/ docs/current/hadoop-streaming/ HadoopStreaming.html 5 http://www.cascading.org 6 https://storm.apache.org/ 4 Figure 1: Overview of NLP modules Storm is a framework for streaming computing whose aim is to implement highly-scalable and parallel processing of data streams. The system presented in Artola et al. (2014) uses Storm to integrate and orchestrate an NLP pipeline comprised by many modules, but it does so following a batch processing paradigm. The streaming approach presented here is an extension of Artola et al. (2014) to deal with streaming scenarios. 3. A pipeline for event recognition The frameworks we present here are independent of the exact NLP modules that are used in the pipeline. The only requirement posed to the modules is that of using the NLP Annotation Format (Fokkens et al., 2014, NAF) for representing linguistic annotations when using the Storm architecture. The Hadoop setup can in"
L16-1714,vossen-etal-2014-newsreader,0,0.0278599,"., we describe the background of this work. This is followed by a presentation of the NLP modules we use in Section 3. Sections 4. and 5. introduce the Hadoop architecture for batch processing and the Storm architecture for live streaming, respectively. This is followed by the conclusion in Section 6. 2. Background and related work Everyday, around 2 million news articles from thousands of sources are published and this number is increasing.2 Keeping track of all this information, or even a subset of information about a specific domain, is unfeasible without technological support. NewsReader (Vossen et al., 2014) aims to provide such support by performing detailed linguistic analyses identifying what happened to whom, when and where in large amounts of data in four languages, namely, English, Spanish, Italian and Dutch. The extracted information is stored as structured data in RDF (to be specific, as Event-Centric Knowledge Graphs (Rospocher et al., 2016)) so that information specialists can carry out precise search over the data. Vossen et al. (2014) explain how NewsReader functions as a history recorder: keeping track and storing every1 http://www.newsreader-project.eu These numbers are based on an"
N09-1003,E09-1005,1,0.220187,"raph G (Haveliwala, 2002). Basically, personalized PageRank is computed by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the target word. Regarding PageRank implementation details, we chose a damping value of 0.85 and finish the calculation after 30 iterations. These are default values, and we did not optimize them. Our similarity method is similar, but simpler, to that used by (Hughes and Ramage, 2007), which report very good results on similarity datasets. More details of our algorithm can be found in (Agirre and Soroa, 2009). The algorithm and needed resouces are publicly available1 . 2.1 WordNet relations and versions The WordNet versions that we use in this work are the Multilingual Central Repository or MCR (Atserias et al., 2004) (which includes English WordNet version 1.6 and wordnets for several other languages like Spanish, Italian, Catalan and Basque), and WordNet version 3.02 . We used all the relations in MCR (except cooccurrence relations and selectional preference relations) and in WordNet 3.0. Given the recent availability of the disambiguated gloss relations for WordNet 3.03 , we also used a version"
N09-1003,J06-1003,0,0.324061,"ies of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets. An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information Retrieval or CL sponsored search. A discussion on the differences between learning similarity and relatedness scores is provided. The pap"
N09-1003,P06-1127,0,0.0129682,"tant problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from th"
N09-1003,P00-1064,0,0.0140928,"uage dependent). As our WordNet-based method uses the graph of the concepts and relations, we can easily compute the similarity between words from different languages. For example, consider a English-Spanish pair like car – coche. Given that the Spanish WordNet is included in MCR we can use MCR as the common knowledge-base for the relations. We can then compute the personalized PageRank for each of car and coche on the same underlying graph, and then compare the similarity between both probability distributions. As an alternative, we also tried to use publicly available mappings for wordnets (Daude et al., 2000)4 in order to create a 3.0 version of the Spanish WordNet. The mapping was used to link Spanish variants to 3.0 synsets. We used the English WordNet 3.0, including glosses, to construct the graph. The two Spanish WordNet versions are referred to as MCR16 and WN30g. 3 Context-based methods In this section, we describe the distributional methods used for calculating similarities between words, and profiting from the use of a large Web-based corpus. This work is motivated by previous studies that make use of search engines in order to collect cooccurrence statistics between words. Turney (2001) u"
N09-1003,D07-1061,0,0.483155,"-lingual task with minor losses. 1 Introduction Measuring semantic similarity and relatedness between terms is an important problem in lexical semantics. It has applications in many natural language processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense on"
N09-1003,O97-1002,0,0.0918022,"Missing"
N09-1003,P98-2127,0,0.0331888,"ias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007). In this paper, we explore both families. For the first one we apply graph based algorithms to WordNet, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web corpus. Previous work suggests that distributional similarities suffer from certain limitations, which make 19 them less useful than knowledge resources for semantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other problems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the performance of results previously reported on the same datasets. An application to cross-lingual (CL) similarity identification is also described, with applications such as CL Information R"
N09-1003,J07-2002,0,0.0193389,"e entire corpus using an implementation of an Inductive Dependency parser as described in Nivre (2006). For each word w we collect a template of the syntactic context. We consider sequences of governing words (e.g. the parent, grand-parent, etc.) as well as collections of descendants (e.g., immediate children, grandchildren, etc.). This information is then encoded as a contextual template. For example, the context template cooks &lt;term&gt; delicious could be contexts for nouns such as food, meals, pasta, etc. This captures both syntactic preferences as well as selectional preferences. Contrary to Pado and Lapata (2007), we do not use the labels of the syntactic dependencies. Once the vectors have been obtained, the frequency for each dimension in every vector is weighted using the other vectors as contrast set, with the χ2 test, and finally the cosine similarity between vectors is used to calculate the similarity between each pair of terms. Except for the syntactic dependency approach, where closed-class words are needed by the parser, in the other cases we have removed stopwords (pronouns, prepositions, determiners and modal and auxiliary verbs). 3.1 Corpus used We have used a corpus of four billion docume"
N09-1003,W06-2501,0,0.105102,"Missing"
N09-1003,P94-1019,0,0.55483,"Missing"
N09-1003,C98-2122,0,\N,Missing
N15-1165,N09-1003,1,0.755997,"Missing"
N15-1165,agirre-etal-2010-exploring,1,0.950805,"Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations. 1 Figure 1: Main architecture for generating KB word embeddings. A random walk algorithm over the KB produces a synthetic corpus, which is fed into a NNLM to produce continuous word representations. Introduction Graph-based techniques over Knowledge Bases (KB) like WordNet (Fellbaum, 1998) have been widely used in NLP tasks, including word sense disambiguation (Agirre et al., 2014; Moro et al., 2014), semantic similarity and semantic relatedness between terms (Agirre et al., 2009; Agirre et al., 2010; Pilehvar et al., 2013). For instance, Agirre et al. (2009; 2010) apply a random walk algorithm based on Personalized PageRank to WordNet, presenting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset (Finkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vec"
N15-1165,P14-1023,0,0.213115,"timizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically reduced computational complexity by deleting the hidden layer, enabling to compute accurate word representations from very large corpora. The representations obtained by these methods are compact, taking 1.5G for 3M words on 300-dimensional space, and have been shown to outperform other distributional corpus-based methods on several tasks, including the WS353 word similarity dataset (Baroni et al., 2014). In this work we propose to encode the meaning of words using the structural information in knowledge bases. That is, instead of modeling the meaning based on the co-occurrences of words in corpora, we model the meaning based on random walks over the knowledge base. Each random walk is seen as a context for words in the vocabulary, and fed into the NNLM architecture, which optimizes the likelihood of those contexts (cf. Fig. 1). The resulting word representations are more compact than those produced by regular random walk algorithms (300 vs. tens of thousands), and produce very good results o"
N15-1165,N13-1090,0,0.775244,"ay 31 – June 5, 2015. 2015 Association for Computational Linguistics Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations are obtained by optimizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically reduced computational complexity by deleting the hidden layer, enabling to compute accurate word representations from very large corpora. The representations obtained by these methods are compact, taking 1.5G for 3M words on 300-dimensional space, and have been shown to outperform other distributional corpus-based methods on several tasks, including the WS353 word similarity dataset (Baroni et al., 2014). In this work we propose to encode the meaning of words using the structural information in knowledge bases. That is, instea"
N15-1165,Q14-1019,0,0.0361687,"to corpus-based continuous representations, improving the stateof-the-art in the similarity dataset. Our technique opens up exciting opportunities to combine distributional and knowledge-based word representations. 1 Figure 1: Main architecture for generating KB word embeddings. A random walk algorithm over the KB produces a synthetic corpus, which is fed into a NNLM to produce continuous word representations. Introduction Graph-based techniques over Knowledge Bases (KB) like WordNet (Fellbaum, 1998) have been widely used in NLP tasks, including word sense disambiguation (Agirre et al., 2014; Moro et al., 2014), semantic similarity and semantic relatedness between terms (Agirre et al., 2009; Agirre et al., 2010; Pilehvar et al., 2013). For instance, Agirre et al. (2009; 2010) apply a random walk algorithm based on Personalized PageRank to WordNet, presenting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset (Finkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each"
N15-1165,P13-1132,0,0.0124099,"p exciting opportunities to combine distributional and knowledge-based word representations. 1 Figure 1: Main architecture for generating KB word embeddings. A random walk algorithm over the KB produces a synthetic corpus, which is fed into a NNLM to produce continuous word representations. Introduction Graph-based techniques over Knowledge Bases (KB) like WordNet (Fellbaum, 1998) have been widely used in NLP tasks, including word sense disambiguation (Agirre et al., 2014; Moro et al., 2014), semantic similarity and semantic relatedness between terms (Agirre et al., 2009; Agirre et al., 2010; Pilehvar et al., 2013). For instance, Agirre et al. (2009; 2010) apply a random walk algorithm based on Personalized PageRank to WordNet, presenting the best results to date among WordNetbased methods for the well-known WS353 wordsimilarity dataset (Finkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to pr"
N15-1165,D11-1014,0,0.0681162,"nkelstein et al., 2001). For each target word, the method performs a personalized random walk on the WordNet graph. At convergence, the target word is represented as a vector in a multi-dimensional conceptual space, with one dimension for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to produce, 117K dimensions (one per synset) for WordNet. In recent years a wide variety of Neural Network Language Models (NNLM) have been successfully employed in several tasks, including word similarity (Collobert and Weston, 2008; Socher et al., 2011; 1434 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1434–1439, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations"
N15-1165,P10-1040,0,0.0142789,"ion for each concept in the KB. The good results of the algorithm contrast with the large dimensionality of the vectors that it needs to produce, 117K dimensions (one per synset) for WordNet. In recent years a wide variety of Neural Network Language Models (NNLM) have been successfully employed in several tasks, including word similarity (Collobert and Weston, 2008; Socher et al., 2011; 1434 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1434–1439, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Turian et al., 2010). NNLM extract meaning from unlabeled corpora following the distributional hypothesis (Harris, 1954), where semantic features of a word are related to its co-occurrence patterns. NNLM learn word representations in the form of dense scalar vectors in n-dimensional spaces (e.g. 300 dimensions), in which each dimension is a latent semantic feature. The representations are obtained by optimizing the likelihood of existing unlabeled text. More recently, Mikolov et al. have developed simpler NNLM architectures (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c), which drastically r"
N15-1165,D14-1167,0,0.0196997,"d relatedness and similarity: WS353 (Finkelstein et al., 2001) and SL999 (Hill et al., 2014b), respectively. We also show that the obtained representations are complementary to those of random walks alone and to distributional representations obtained by the same NNLM algorithm, improving the results. Some recent work has explored embedding KBs in low-dimensional continous vector spaces, representing each entity in a k-dimensional vector and characterizing typed relations between entities in the KB (e.g. born-in-city in Freebase or part-of in WordNet) as operations in the k-dimensional space (Wang et al., 2014). The model estimates the parameters which maximize the likelihood of the triples, which 1435 can then be used to infer new typed relations which are missing in the KB. In contrast, we use the relations to explicitly model the context of words, in two complementary approaches to embed information in KBs into continuous spaces. 2 NNLM Neural Network Language Models have become a useful tool in NLP on the last years, specially in semantics. We have used the two models proposed in (Mikolov et al., 2013c) due to their simplicity and effectiveness in word similarity and relatedness tasks (Baroni et"
N15-1165,J15-4004,0,\N,Missing
N15-1165,J14-1003,1,\N,Missing
P13-4026,W12-1014,1,0.797453,"ed. Consequently links to relevant articles in Wikipedia were added to each the meta-data of each artefact using Wikipedia Miner (Milne and Witten, 2008) to provide background information. In addition to the link, Wikipedia Miner returns a confidence value between 0 and 1 for each link based on the context of the item. The accuracy of the links added by Wikipedia Miner were evaluated using the meta-data associated with 21 randomly selected artefacts. Three annotators analysed the links added and found that a confidence value of 0.5 represented a good balance between accuracy and coverage. See Fernando and Stevenson (2012) for further details. Filtered 466,958 1,219,731 14,983 1,701,672 Table 1: Number of artefacts in Europeana collections before and after filtering 4 Data Processing A range of pre-preprocessing steps were carried out on these collections to provide additional information to support navigation in the PATHS system. 4.1 Artefact Similarity 4.3 We begin by computing the similarity between the various artefacts in the Europeana collections. This information is useful for navigation and recommendation but is not available in the Europeana collections since they are drawn from a diverse range of sour"
P13-4026,C12-1054,1,0.879666,"Missing"
P13-4026,W07-0907,0,0.022716,"this information is used within the system. 1 2 Related Work Heitzman et al. (1997) describe the ILEX system which acts as a guide through the jewellery collection of the National Museum of Scotland. The user explores the collection through a set of web pages which provide descriptions of each artefact that are personalised for each user. The system makes use of information about the artefacts the user has viewed to build up a model of their interests and uses this to customise the descriptions of each artefact and provide recommendations for further artefacts in which they may be interested. Grieser et al. (2007) also explore providing recommendations based on the artefacts a user has viewed so far. They make use of a range of techniques including language modelling, geospatial modelling and analysis of previous visitors’ behaviour to provide recommendations to visitors to the Melbourne Museum. Grieser et al. (2011) explore methods for determining the similarity between museum artefacts, commenting that this is useful for navigation through these collections and important for personalisation (Bowen and Filippini-Fantoni, 2004; O’Donnell et al., 2001), recommendation (Bohnert et al., 2009; Trant, 2009)"
P15-2045,D11-1049,0,0.085884,"Missing"
P15-2045,N13-1095,0,0.0510502,"ned using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same"
P15-2045,P05-1022,0,0.0134751,"stant supervision is carried out for a target UMLS relation by identifying instance pairs and using them to create a set of positive instance pairs. Any pairs which also occur as an instance pair of another UMLS relation are removed from this set. A set of negative instance pairs is then created by forming new combinations that do not occur within the positive instance pairs. Sentences containing a positive or negative instance pair are then extracted to generate positive and negative training examples for the relation. These candidate sentences are then stemmed (Porter, 1997) and PoS tagged (Charniak and Johnson, 2005). The sets of positive and negative training examples are then filtered to remove sentences that meet any of the following criteria: contain the same positive pair more than once; contain both a positive and negative pair; more than 5 words between the two elements of the instance pair; contain very common instance pairs. 3.2 weight 10.53 6.17 2.80 -0.06 Table 1: Example PRA-induced paths and weights for the NCI relation biological-process-involvesgene-product. The paths induced by PRA are used to identify potential false negatives in the negative training examples (Section 3.1). Each negative"
P15-2045,P09-1113,0,0.161407,"inimal effort. However, this data may contain errors and consequently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take in"
P15-2045,Q13-1030,0,0.0488415,"supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not nece"
P15-2045,W11-0902,0,0.391962,"Missing"
P15-2045,D12-1042,0,0.190986,"sed as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an in"
P15-2045,P12-1076,0,0.159924,"On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different relations at the same time and may not necessarily express the target relation. Other approaches focus directly on the noise in the data. For instance Takamatsu et al. (2012) use a generative model to predict incorrect data while Intxaurrondo et al. (2013) use a range of heuristics including PMI to remove noise. Augenstein et al. (2014) apply techniques to detect highly ambiguous entity pairs and discard them from their labelled training set. This work proposes a novel approach to the problem by applying an inference learning method to identify potential false negatives in distantly labelled data. Our method makes use of a modified version of PRA to learn relation paths from a knowledge base and uses this information to identify false negatives. Introduction Dista"
P15-2045,P13-2141,0,0.168429,"quently systems trained using distant supervision tend not to perform as well as those based on manually labelled data. This work proposes a novel method for detecting potential false negative training examples using a knowledge inference method. Results show that our approach improves the performance of relation extraction systems trained using distantly supervised data. 1 2 Related Work Distant supervision is widely used to train relation extraction systems with Freebase and Wikipedia commonly being used as knowledge bases, e.g. (Mintz et al., 2009; Riedel et al., 2010; Krause et al., 2012; Zhang et al., 2013; Min et al., 2013; Ritter et al., 2013). The main advantage is its ability to automatically generate large amounts of training data automatically. On the other hand, this automatically labelled data is noisy and usually generates lower performance than approaches trained using manually labelled data. A range of filtering approaches have been applied to address this problem including multi-class SVM (Nguyen and Moschitti, 2011) and Multi-Instance learning methods (Riedel et al., 2010; Surdeanu et al., 2012). These approaches take into account the fact that entities might occur in different rel"
P15-2045,P10-1030,0,\N,Missing
P15-2045,P11-2048,0,\N,Missing
P16-1179,P14-2013,0,0.0123141,"ould also introduce additional disambiguation probabilities (Barrena et al., 2015), or apply more sophisticated parameter estimation methods (Lazic et al., 2015). Table 8 includes other high performing or wellknown systems, which usually use complex methods to combine features coming from different sources, where our results are only second to those of (Chisholm and Hachey, 2015) in the CoNLL dataset and best in TAC 2014 DEL. The goal of this paper is not to provide the best performing system, but yet, the results show that our use of background information allows to obtain very good results. Alhelbawy and Gaizauskas (2014) combines local and coherence features by means of a graph ranking scheme, obtaining very good results on the CONLL 2003 dataset. They evaluate on the full dataset, i.e. they test on train, testa and testb (20K, 4.8K and 4.4K mentions respectively). Our results on the same dataset are 84.25 (Full) and 88.07 (Full weighted), but note that we do tune the parameters on testa, so this might be slighly over-estimated. Our system does not use global coherence, and therefore their method is complementary to our NED system. In principle, our proposal for enriching context should improve the results of"
P16-1179,C14-1213,1,0.545309,"Missing"
P16-1179,S15-1011,1,0.879825,"Missing"
P16-1179,W11-0110,0,0.0142721,"and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance. Both distributional similarity and selectional preferences rely"
P16-1179,E06-1002,0,0.170521,"selectional preferences for syntactic positions. We show, using a generative N¨aive Bayes model for NED, that the additional sources of context are complementary, and improve results in the CoNLL 2003 and TAC KBP DEL 2014 datasets, yielding the third best and the best results, respectively. We provide examples and analysis which show the value of the acquired background information. 1 Introduction The goal of Named Entity Disambiguation (NED) is to link each mention of named entities in a document to a knowledge-base of instances. The task is also known as Entity Linking or Entity Resolution (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hachey et al., 2012). NED is confounded by the ambiguity of named entity mentions. For instance, according to Wikipedia, Liechtenstein can refer to the micro-state, several towns, two castles or a national football team, among other instances. Another ambiguous entity is Derbyshire which can refer to a county in England or a cricket team. Most NED research use knowledge-bases derived or closely related to Wikipedia. For a given mention in context, NED systems (Hachey et al., 2012; Lazic et al., 2015) typically rely on two models: (1) a mention module returns possible"
P16-1179,D13-1184,0,0.0699434,"o the rest of the state-of-theart, as they artificially insert the gold standard entity in the candidate list.12 In (Chisholm and Hachey, 2015) the authors explore the use of links gathered from the web as an additional source of information for NED. They present a complex two-staged supervised system that incorporates global coherence features, with large amount of noisy training. Again, using additional training data seems an interesting future direction complementary to ours. We are not aware of other works which try to use additional sources of context or background information as we do. (Cheng and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 20"
P16-1179,Q15-1011,0,0.104626,"o tune the parameters on testa, so this might be slighly over-estimated. Our system does not use global coherence, and therefore their method is complementary to our NED system. In principle, our proposal for enriching context should improve the results of their system. Pershina et al. (2015) propose a system closely resembling (Alhelbawy and Gaizauskas, 2014). They report the best known results on CONNL 2003 so far, but unfortunately, their results are not directly comparable to the rest of the state-of-theart, as they artificially insert the gold standard entity in the candidate list.12 In (Chisholm and Hachey, 2015) the authors explore the use of links gathered from the web as an additional source of information for NED. They present a complex two-staged supervised system that incorporates global coherence features, with large amount of noisy training. Again, using additional training data seems an interesting future direction complementary to ours. We are not aware of other works which try to use additional sources of context or background information as we do. (Cheng and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our us"
P16-1179,P07-1028,0,0.0380211,"the most notable team to carry the name Glamorgan is Glamorgan County Cricket Club, Trevor Barsby is a cricketer, as are all other people in the distributional context. When using these similar entities as context, our system does return the correct entity for this mention. In the second example, the words in the context lead the model to return the football team for Liechtenstein, instead of the country, without being aware that the nominal event “visit to” prefers locations arguments. This kind of background information, known as selections preferences, can be easily acquired from corpora (Erk, 2007). Figure 1 shows the most frequent entities found as arguments of “visit to” in the Reuters corpus. When using these filler entities as context, the context model does return the correct entity for this mention. In this article we explore the addition of two kinds of background information induced from corpora to the usual context of occurrence: (1) given a mention we use distributionally similar entities as additional context; (2) given a mention and the syntactic dependencies in the context sentence, we use the selectional preferences of those syntactic dependencies as additional context. We"
P16-1179,P11-1095,0,0.527785,"frequent dependency was MOD, followed by SUBJ and OBJ 5 The selectional preferences include 400K different named entities as fillers. Note that selectional preferences are different from dependency path features. Dependency path features refer to features in the immediate context of the entity mention, and are sometimes added as additional features of supervised classifiers. Selectional preferences are learnt collecting fillers in the same dependency path, but the fillers occur elsewhere in the corpus. 3 NED system Our disambiguation system is a N¨aive Bayes model as initially introduced by (Han and Sun, 2011a), but adapted to integrate the background information extracted from the Reuters corpus. The model is trained using Wikipedia,6 which is also used to generate the entity candidates for each mention. Following usual practice, candidate generation is performed off-line by constructing an association between strings and Wikipedia articles, which we call dictionary. The association is performed using article titles, redirections, disambiguation pages, and textual anchors. Each association is scored with the number of times the string was 5 1.5M, 0.8M and 0.7M respectively We used a dump from 25-"
P16-1179,N15-1035,0,0.0188925,"of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance. Both distributional similarity and selectional preferences rely on correct mention detection in the background corpus. We detected 12 https://github.com/masha-p/PPRforNED/ readme.txt 1910 that me"
P16-1179,P10-4014,0,0.038073,"tions to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance. Both distributional similarity and selectional preferences rely on correct mention detection in the background corpus. We detected 12 https://github.com/masha-p/PPRforNED/ readme.txt 1910 that mentions where missed, which caused some coverage issues. In addition, the small size of the background corpus sometimes produces arbitrary contexts. For instance, subject position fillers of “score” include mostly basketball players like Mic"
P16-1179,D11-1072,0,0.25864,"Missing"
P16-1179,Q15-1036,0,0.230844,". The task is also known as Entity Linking or Entity Resolution (Bunescu and Pasca, 2006; McNamee and Dang, 2009; Hachey et al., 2012). NED is confounded by the ambiguity of named entity mentions. For instance, according to Wikipedia, Liechtenstein can refer to the micro-state, several towns, two castles or a national football team, among other instances. Another ambiguous entity is Derbyshire which can refer to a county in England or a cricket team. Most NED research use knowledge-bases derived or closely related to Wikipedia. For a given mention in context, NED systems (Hachey et al., 2012; Lazic et al., 2015) typically rely on two models: (1) a mention module returns possible entities which can be referred to by the mention, ordered by prior probabilities; (2) a conFigure 1: Two examples where NED systems fail, motivating our two background models: similar entities (top) and selectional preferences (bottom). The logos correspond to the gold label. text model orders the entities according to the context of the mention, using features extracted from annotated training data. In addition, some systems check whether the entity is coherent with the rest of entities mentioned in the document, although (L"
P16-1179,J03-4004,0,0.0656082,"additional sources of context or background information as we do. (Cheng and Roth, 2013) use relational information from Wikipedia to add constraints to the coherence model, and is somehow reminiscent of our use dependency templates, although they focus on recognizing a fixed set of relations between entities (as in information extraction) and do not model selectional preferences. (Barrena et al., 2014) explored the use of syntactic collocations to ensure coherence, but did not model any selectional preferences. Previous work on word sense disambiguation using selectional preference includes (McCarthy and Carroll, 2003) among others, but they report low results. (Brown et al., 2011) applied wordNet hypernyms for disambiguating verbs, but they did not test the improvement of this feature. (Taghipour and Ng, 2015) use embeddings as features which are fed into a supervised classifier, but our method is different, as we use embeddings to find similar words to be fed as additional context. None of the state-of-the-art systems, e.g. (Zhong and Ng, 2010), uses any model of selectional preferences. 8 Discussion We performed an analysis of the cases where our background models worsened the disambiguation performance."
P16-1179,N15-1026,0,0.126734,"Missing"
P19-1492,P17-1042,1,0.901941,"rable corpora (Vuli´c and Moens, 2016) or large bilingual dictionaries (Duong et al., 2016) have also been proposed. For a more detailed survey, the reader is referred to Ruder et al. (2017). In contrast, offline mapping approaches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training"
P19-1492,D18-1043,0,0.254107,"aches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training corpora, and the parameters of the word embedding algorithms. Similarly, Patra et al. (2019) show that the isomorphism assumption weakens as the languages involved become increasingly etymologically distant. Finally, Nakashole"
P19-1492,W15-1521,0,0.255871,"clear whether this mismatch is a consequence of separately training both embedding spaces, and thus an inherent limitation of mapping approaches, or an insurmountable obstacle that arises from the linguistic divergences across languages, and hence a more general issue when learning cross-lingual word embeddings. Introduction Cross-lingual word embeddings have attracted a lot of attention in recent times. Existing methods can be broadly classified into two categories: joint methods, which simultaneously learn word representations for multiple languages on parallel corpora (Gouws et al., 2015; Luong et al., 2015), and mapping methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations (Mikolov et al., 2013a; Artetxe et al., 2018a). While early work in cross-lingual word embeddings was dominated by joint approaches, recent research has almost exclusively focused on mapping methods, which have the advantage of requirThe goal of this paper is to shed light on this matter so as to better understand the nature and extension of these limitations. For that purpose, we experiment with parallel corpora, which allows us to compare mappi"
P19-1492,P18-1073,1,0.905812,"the linguistic divergences across languages, and hence a more general issue when learning cross-lingual word embeddings. Introduction Cross-lingual word embeddings have attracted a lot of attention in recent times. Existing methods can be broadly classified into two categories: joint methods, which simultaneously learn word representations for multiple languages on parallel corpora (Gouws et al., 2015; Luong et al., 2015), and mapping methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations (Mikolov et al., 2013a; Artetxe et al., 2018a). While early work in cross-lingual word embeddings was dominated by joint approaches, recent research has almost exclusively focused on mapping methods, which have the advantage of requirThe goal of this paper is to shed light on this matter so as to better understand the nature and extension of these limitations. For that purpose, we experiment with parallel corpora, which allows us to compare mapping methods and joint methods under the exact same conditions, and analyze the properties of the resulting embeddings. Our results show that, under these conditions, joint learning yields to more"
P19-1492,P18-2036,0,0.195788,"lf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training corpora, and the parameters of the word embedding algorithms. Similarly, Patra et al. (2019) show that the isomorphism assumption weakens as the languages involved become increasingly etymologically distant. Finally, Nakashole and Flauger (2018) argue that embedding spaces in different languages are linearly equivalent only at local regions, but their global structure is different. Nevertheless, neither of these works does systematically analyze the extent to which these limitations are inherent to mapping approaches. To the best of our knowledge, ours is the first work comparing joint and mapping methods in the exact same conditions, characterizing the nature and impact of such limitations. Experimental design We next describe the cross-lingual embedding methods, evaluation measures and datasets used in our experiments. 3.1 Cross-li"
P19-1492,D16-1136,0,0.101565,"2019 Association for Computational Linguistics 2 3 Related work Cross-lingual word embeddings represent words from multiple languages in a common vector space. So as to train them, joint methods simultaneously learn the embeddings in the different languages, which requires some form of cross-lingual supervision. This supervision usually comes from parallel corpora, which can be aligned at the word level (Luong et al., 2015), or only at the sentence level (Gouws et al., 2015). In addition to that, methods that rely on comparable corpora (Vuli´c and Moens, 2016) or large bilingual dictionaries (Duong et al., 2016) have also been proposed. For a more detailed survey, the reader is referred to Ruder et al. (2017). In contrast, offline mapping approaches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang e"
P19-1492,N13-1073,0,0.0179014,"ld of 1e-5 and 5 training iterations. Having done that, we map the resulting monolingual embeddings to a cross-lingual space using the unsupervised mode in VecMap1 (Artetxe et al., 2018b), which builds an initial solution based on heuristics and iteratively improves it through self-learning. Joint learning: We use the BiVec2 tool proposed by Luong et al. (2015), an extension of skip-gram that, given a word aligned parallel corpus, learns to predict the context of both the source word and the target word aligned with it. For that purpose, we first word align our training corpus using FastText (Dyer et al., 2013). Given that BiVec is a natural extension of skip-gram, we use the exact same hyperparameters as for the mapping method. In both cases, we restrict the vocabulary to the most frequent 200,000 words. 3.2 Evaluation measures We use the following measures to characterize cross-lingual embeddings: Isomorphism. Intuitively, the notion of isomorphism captures the idea of how well the embeddings in both languages fit together (i.e. the degree of their structural similarity). So as to measure it, we use the eigenvalue similarity metric proposed by Søgaard et al. (2018). For that purpose, we first cent"
P19-1492,W18-6488,0,0.0156138,"etrieval. Note that, in addition to having a practical application, BLI performance is an informative measure of the quality of the embeddings, as a good cross-lingual representation should place equivalent words close to each other. 3.3 Datasets We experiment with 4 language pairs with English as the target language, covering 3 relatively close languages (German, Spanish and Italian) and a non-indoeuropean agglutinative language (Finnish). All embeddings were trained on the BiCleaner v3.0 version of the ParaCrawl corpus,4 a parallel corpus collected through crawling and filtered according to Sánchez-Cartagena et al. (2018). The size of this corpus changes from one language to another: German and Spanish are the largest (503 and 492 million tokens in the English side, respectively), followed by Italian (308 million tokens), and Finnish (55 million tokens). As for the evaluation dictionaries for BLI, we use two datasets that have been widely used in the literature. The first one, which we call Eparl, was first introduced by Dinu et al. (2015) and subsequently extended by Artetxe et al. (2017) and Artetxe et al. (2018a), and consists of 1,500 test entries extracted from Europarl word alignments 4992 4 https://para"
P19-1492,P18-1072,0,0.113707,"Missing"
P19-1492,P17-1179,0,0.143559,", 2016) have also been proposed. For a more detailed survey, the reader is referred to Ruder et al. (2017). In contrast, offline mapping approaches work by aligning separately trained word embeddings in different languages. For that purpose, early methods required a training dictionary, which was used to learn a linear transformation that mapped these embeddings into a common space (Mikolov et al., 2013a; Artetxe et al., 2018a). The amount of required supervision was later reduced through selflearning methods (Artetxe et al., 2017), and then completely eliminated through adversarial training (Zhang et al., 2017; Conneau et al., 2018) or more robust iterative approaches combined with initialization heuristics (Artetxe et al., 2018b; Hoshen and Wolf, 2018). There are several authors that have discussed the potential limitations of these mapping approaches. For instance, Søgaard et al. (2018) observe that the assumption that separately trained embeddings are approximately isomorphic is not true in general, showing that the performance of mapping methods is conditioned by the language pair, the comparability of the training corpora, and the parameters of the word embedding algorithms. Similarly, Patra e"
S07-1002,W06-3814,1,0.81499,"Missing"
S07-1002,N06-2015,0,0.0461989,"aximum score to that instance. Finally, the resulting test corpus is evaluated according to the usual precision and recall measures for supervised word sense disambiguation systems. 3 Results In this section we will introduce the gold standard and corpus used, the description of the systems and the results obtained. Finally we provide some material for discussion. Gold Standard The data used for the actual evaluation was borrowed from the SemEval-2007 “English lexical sample subtask” of task 17. The texts come from the Wall Street Journal corpus, and were hand-annotated with OntoNotes senses (Hovy et al., 2006). Note that OntoNotes senses are coarser than WordNet senses, and thus the number of senses to be induced is smaller in this case. Participants were provided with information about 100 target words (65 verbs and 35 nouns), each target word having a set of contexts where the word appears. After removing the sense tags from the train corpus, the train and test parts were joined into the official corpus and given to the participants. Participants had to tag with the induced senses all the examples in this corpus. Table 1 summarizes the size of the corpus. Participant systems In total there were 6"
S07-1002,H05-1053,0,0.00998491,"(Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart´ınez and Agirre, 2000; Koeling et al., 2005). Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions in a continuum (Cruse, 2000). Unsupervised Word Sense Induction and Discrimination (WSID, also known as corpus-based unsupervised systems) has followed this line of thinking, and tries to induce"
S07-1002,W00-1326,1,0.845383,"Missing"
S07-1002,H93-1061,0,0.182601,"systems. We reused the SemEval2007 English lexical sample subtask of task 17, and set up both clustering-style unsupervised evaluation (using OntoNotes senses as gold-standard) and a supervised evaluation (using the part of the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging"
S07-1002,W05-0605,0,0.0122372,"ses (also known as the gold-standard) with the clustering result. The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Sch¨utze, 1998). This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance. In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind. With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solution on it. We eval"
S07-1002,W04-2406,0,0.426149,"tagged with some reference senses (also known as the gold-standard) with the clustering result. The gold standard tags are taken to be the definition of the classes, and standard measures from the clustering literature can be used to evaluate the clusters against the classes. A second alternative would be to devise a method to map the clusters returned by the systems to the senses in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedersen, 2004; Niu et al., 2005). A third alternative is to evaluate the systems according to some performance in an application, e.g. information retrieval (Sch¨utze, 1998). This is a very attractive idea, but requires expensive system development and it is sometimes difficult to separate the reasons for the good (or bad) performance. In this task we decided to adopt the first two alternatives, since they allow for comparison over publicly available systems of any kind. With this goal on mind we gave all the participants an unlabeled corpus, and asked them to induce the senses and create a clustering solu"
S07-1002,J98-1004,0,0.881221,"Missing"
S07-1002,W04-0811,0,0.036541,"the dataset for mapping). We provide a comparison to the results of the systems participating in the lexical sample subtask of task 17. 1 Introduction Word Sense Disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Another problem of the supervised approach is that the inventory and distribution of senses changes dramatically from one domain to the other, requiring additional hand-tagging of corpora (Mart´ınez and Agirre, 2000; Koeling et al., 2005). Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicograph"
S07-1075,S07-1002,1,0.930206,"07 competition, on word sense induction and Web people search, respectively, with mixed results. 1 Introduction This paper describes a graph-based unsupervised system for induction and classification. Given a set of data to be classified, the system first induces the possible clusters and then clusters the data accordingly. The paper is organized as follows. Section 2 gives an description of the general framework of our system. Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval2007 WEPS task (Artiles et al., 2007) and Semeval2007 sense induction task (Agirre and Soroa, 2007), respectively. Section 5 presents the results obtained in both tasks, and Section 6 draws some conclusions. 2 A graph based system for unsupervised classification The system performs a two stage graph based clustering where a co-occurrence graph is first clustered First step: calculating hub score vectors In a first step, and for each entity to be clustered, a graph consisting on context word co-occurrences is built. Vertices in the co-occurrence graph are words and two vertices share an edge whenever they cooccur in the same context. Besides, each edge receives a weight, which indicates how"
S07-1075,W06-3814,1,0.882703,"Missing"
S07-1075,W06-1669,1,0.887189,"Missing"
S07-1075,S07-1012,0,0.0768092,"system has participated in tasks 2 and 13 of the SemEval-2007 competition, on word sense induction and Web people search, respectively, with mixed results. 1 Introduction This paper describes a graph-based unsupervised system for induction and classification. Given a set of data to be classified, the system first induces the possible clusters and then clusters the data accordingly. The paper is organized as follows. Section 2 gives an description of the general framework of our system. Sections 3 and 4 presents in more detail the implementation of the framework for the Semeval2007 WEPS task (Artiles et al., 2007) and Semeval2007 sense induction task (Agirre and Soroa, 2007), respectively. Section 5 presents the results obtained in both tasks, and Section 6 draws some conclusions. 2 A graph based system for unsupervised classification The system performs a two stage graph based clustering where a co-occurrence graph is first clustered First step: calculating hub score vectors In a first step, and for each entity to be clustered, a graph consisting on context word co-occurrences is built. Vertices in the co-occurrence graph are words and two vertices share an edge whenever they cooccur in the same conte"
S07-1075,atserias-etal-2006-freeling,0,0.0602508,"Missing"
S10-1093,E09-1005,1,0.892281,"sources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future. 1 2 We will present in turn UKB, the Tybots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any t"
S10-1093,S10-1013,1,0.884699,"Missing"
S10-1093,bosma-vossen-2010-bootstrapping,1,0.72995,"et 3.0 with gloss relations (Fellbaum, 1998). Dutch: The Dutch LKB is part of the Cornetto database version 1.3 (Vossen et al., 2008). The Cornetto database can be obtained from the Dutch/Flanders Taalunie3 . Cornetto comprises taxonomic relations and equivalence rela2 #rels. Table 1: Wordnets and their sizes (entries, synsets, relations and links to WN30g). Tybots (Term Yielding Robots) are text mining software that mine domain terms from corpus (e.g. web pages), organizing them in a hierarchical structure, connecting them to wordnets and ontologies to create a semantic model for the domain (Bosma and Vossen, 2010). The software is freely available using Subversion 2 . Tybots try to establish a view on the terminology of the domain which is as complete as possible, discovering relations between terms and ranking terms by domain relevance. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus w"
S10-1093,P98-2127,0,0.0122903,"y et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Finally, we used up to 50 related words for each target word. As in run1, we used the monolingual graphs for the LKBs in each language. Table 2: Overall results of our runs, including precision (P) and recall (R), overall and for each PoS. We include the First Sense (1sense) and random baselines, as well as the best run, as provided by the organizers. 3.2 Run2: UKB using related words Run1: UKB using context The first run is an application of the UKB tool in the standard setting, as described in (Agirre and Soroa, 2009). Given the input text, we split it in sentences, and we disambiguate eac"
S10-1093,J07-4005,0,0.0212144,"ea is to first obtain a list of related words for each of the target words, as collected from a domain corpus. On a second step each target word is disambiguated using the N most related words as context (see below). For instance, in order to disambiguate the word environment, we would not take into account the context of occurrence (as in Section 3.2), but we would use the list of most related words in the thesaurus (e.g. “biodiversity, agriculture, ecosystem, nature, life, climate, . . .”). Using UKB over these contexts we obtain the most predominant sense for each target word in the domain(McCarthy et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Fin"
S10-1093,C98-2122,0,\N,Missing
S15-1011,P11-1095,0,0.51305,"P (e)P (s|e)P (cgrf |e) P (e)P (s|e)P (cbow |e)P (cgrf |e) Best (state-of-the-art) Aida 67.54 75.05 76.83 83.28 84.89 Kore 35.42 60.42 54.86 70.83 71.50 Tac09 67.04 77.19 79.40* 82.21* 79.00 Tac10 76.96 85.20* 83.92* 85.98* 80.60 Tac11 67.83 75.55 79.75 81.85* 80.10 Tac12 46.20 57.06 70.13* 71.65* 68.50 Tac13 66.54 74.56* 70.21 73.99* 71.80 Tac14 62.01 71.21 71.28 76.48 79.60 Table 1: Bold marks the best value among probability combinations, and * those results that overcome the best value reported in the state-of-the-art: (Houlsby and Ciaramita, 2014) for Aida, (Moro et al., 2014) for Kore, (Han and Sun, 2011) for Tac09 and see TAC-KBP proceedings for the rest8 . Test P (e)P (s|e)P (cbow |e)P (cgrf |e) P (e)α P (s|e)β P (cbow |e)γ P (cgrf |e)δ (Moro et al., 2014) (Hoffart et al., 2011) (Houlsby and Ciaramita, 2014) Aida 83.28 84.88 82.10 82.54 84.89 Table 2: Micro accuracy results for Aida introducing the Weighted Full Model in row 2. those reported by (Hoffart et al., 2011; Moro et al., 2014) (respectively, 82.5412 and 82.10). Unfortunately the parameter distribution seems to depend on the test dataset, as the same parameters failed to improve the results on the other datasets. 6 Related Work The"
S15-1011,Q14-1019,0,0.0545964,"(s|e) P (e)P (s|e)P (cbow |e) P (e)P (s|e)P (cgrf |e) P (e)P (s|e)P (cbow |e)P (cgrf |e) Best (state-of-the-art) Aida 67.54 75.05 76.83 83.28 84.89 Kore 35.42 60.42 54.86 70.83 71.50 Tac09 67.04 77.19 79.40* 82.21* 79.00 Tac10 76.96 85.20* 83.92* 85.98* 80.60 Tac11 67.83 75.55 79.75 81.85* 80.10 Tac12 46.20 57.06 70.13* 71.65* 68.50 Tac13 66.54 74.56* 70.21 73.99* 71.80 Tac14 62.01 71.21 71.28 76.48 79.60 Table 1: Bold marks the best value among probability combinations, and * those results that overcome the best value reported in the state-of-the-art: (Houlsby and Ciaramita, 2014) for Aida, (Moro et al., 2014) for Kore, (Han and Sun, 2011) for Tac09 and see TAC-KBP proceedings for the rest8 . Test P (e)P (s|e)P (cbow |e)P (cgrf |e) P (e)α P (s|e)β P (cbow |e)γ P (cgrf |e)δ (Moro et al., 2014) (Hoffart et al., 2011) (Houlsby and Ciaramita, 2014) Aida 83.28 84.88 82.10 82.54 84.89 Table 2: Micro accuracy results for Aida introducing the Weighted Full Model in row 2. those reported by (Hoffart et al., 2011; Moro et al., 2014) (respectively, 82.5412 and 82.10). Unfortunately the parameter distribution seems to depend on the test dataset, as the same parameters failed to improve the results on the other"
S15-1011,P11-1138,0,0.293385,"ext, are complementary. We test our method in eight datasets, improving the state-of-the-art results in five, without any tuning, showing that it is robust to out-ofdomain scenarios. When tuning combination weights, we match the best reported results on the widely-used AIDA-CoNLL test-b. 1 Introduction Linking mentions occurring in documents to a knowledge base is the main goal of Entity Linking or Named Entity Disambiguation (NED). This problem has attracted a great number of papers in the NLP and IR communities, and a large number of techniques, including local context and global inference (Ratinov et al., 2011). We propose to use a probabilistic framework that combines entity popularity, name popularity, local mention context and global hyperlink structure, relying on information in Wikipedia alone. Entity and name popularity are useful disambiguation clues in the absence of any context. The local mention context provides direct clues (in the form of words in context) to disambiguate each mention separately. The hyperlink structure of Wikipedia provides a global coherence measure for all entities mentioned in the same context. Eneko Agirre IXA NLP Group UPV/EHU Donostia, Basque Country e.agirre@ehu."
S15-1011,D11-1072,0,\N,Missing
W06-1669,W06-3814,1,0.224507,"Missing"
W06-1669,W04-0807,0,0.0274208,"Missing"
W06-1669,H05-1052,0,0.051105,"lected as its sense. Most of the unsupervised WSD work has been based on the vector space model, where each example is represented by a vector of features (e.g. the words occurring in the context), and the induced senses are either clusters of examples (Sch¨utze, 1998; Purandare and Pedersen, 2004) or clusters of words (Pantel and Lin, 2002). Recently, V´eronis (V´eronis, 2004) has proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graphs. Graph-based methods have gained attention in several areas of NLP, including knowledge-based WSD (Mihalcea, 2005; Navigli and Velardi, 2005) and summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). The HyperLex algorithm presented in (V´eronis, 2004) is entirely corpus-based. It builds a cooccurrence graph for all pairs of words cooccurring in the context of the target word. V´eronis shows that this kind of graph fulfills the properties of small world graphs, and thus possesses highly connected This paper explores the use of two graph algorithms for unsupervised induction and tagging of nominal word senses based on corpora. Our main contribution is the optimization of the free parameters of"
W06-1669,H93-1061,0,0.398686,"rds tasks. The results show that, in spite of the information loss inherent to mapping the induced senses to the gold-standard, the optimization of parameters based on a small sample of nouns carries over to all nouns, performing close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task. 1 Introduction Word sense disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagged data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 700K words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dic1 Unsupervised WSD approaches prefer the te"
W06-1669,W05-0605,0,0.0149339,"et, and then measured the quality of the mapping. More recently, tagged corpora have been used to map the induced senses, and then compare the systems over publicly available benchmarks (Puran2.3 Using hubs for WSD Once the hubs that represent the senses of the word are selected (following any of the methods presented in the last section), each of them is linked to the target word with edges weighting 0, and the Minimum Spanning Tree (MST) of the whole graph is calculated and stored. 5 As G is undirected, the in-degree of a vertex v is equal to its out-degree. 587 is. dare and Pedersen, 2004; Niu et al., 2005; Agirre et al., 2006), which offers the advantage of comparing to other systems, but converts the whole system into semi-supervised. See Section 5 for more details on these systems. Note that the mapping introduces noise and information loss, which is a disadvantage when comparing to other systems that rely on the gold-standard senses. Yet another possibility is to evaluate the induced senses against a gold standard as a clustering task. Induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used. In this case t"
W06-1669,W04-2406,0,0.432566,"duce word senses directly from the corpus. Typical unsupervised WSD systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which represent word uses or senses1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. Most of the unsupervised WSD work has been based on the vector space model, where each example is represented by a vector of features (e.g. the words occurring in the context), and the induced senses are either clusters of examples (Sch¨utze, 1998; Purandare and Pedersen, 2004) or clusters of words (Pantel and Lin, 2002). Recently, V´eronis (V´eronis, 2004) has proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graphs. Graph-based methods have gained attention in several areas of NLP, including knowledge-based WSD (Mihalcea, 2005; Navigli and Velardi, 2005) and summarization (Erkan and Radev, 2004; Mihalcea and Tarau, 2004). The HyperLex algorithm presented in (V´eronis, 2004) is entirely corpus-based. It builds a cooccurrence graph for all pairs of words cooccurring in the context of the target word. V´eroni"
W06-1669,J98-1004,0,0.750175,"Missing"
W06-1669,W04-0811,0,0.038389,"rming close to supervised systems in the lexical sample task and yielding the second-best WSD systems for the Senseval-3 all-words task. 1 Introduction Word sense disambiguation (WSD) is a key enabling-technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagged data. Existing hand-annotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 700K words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dic1 Unsupervised WSD approaches prefer the term ’word uses’ to ’word senses’. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 585 Proceedings of the 2006 Conference on Empirical Methods in"
W06-1669,W04-3252,0,\N,Missing
W06-3814,W04-0807,0,0.0444102,"es” envisioned in (Cruse, 2000). We now think that the idea of having many micro-senses is very attractive for further exploration, especially if we are able to organize them into coarser hubs. 6.3 Comparison to related work Table 4 shows the performance of different systems on the nouns of the S3LS benchmark. When not reported separately, we obtained the results for nouns running the official scorer program on the filtered results, as available in the S3LS web page. The second column shows the type of system (supervised, unsupervised). We include three supervised systems, the winner of S3LS (Mihalcea et al., 2004), an in-house system (kNN-all, CITATION OMITTED) which uses optimized kNN, and the same in-house system restricted to bag-of-words features only (kNN-bow), i.e. discarding other local features like bigrams or trigrams (which is what most unsupervised systems do). The table shows that we are one point from the bag-ofwords classifier kNN-bow, which is an impressive result if we take into account the information loss of the mapping step and that we tuned our parameters on a different set of words. The full kNN system is state-of-the-art, only 4 points below the S3LS win95 System S3LS-best kNN-all"
W06-3814,H93-1061,0,0.0169005,"e). Our results for nouns show that thanks to the optimization of parameters and the mapping method, HyperLex obtains results close to supervised systems using the same kind of bag-ofwords features. Given the information loss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results. 1 Introduction Word sense disambiguation (WSD) is a key enabling technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing handannotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the allwords track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and seman"
W06-3814,W05-0605,0,0.200491,"in a lexicon. Pantel and Lin (2002) automatically map the senses to WordNet, and then measure the quality of the mapping. More recently, the mapping has been used to test the system on publicly available benchmarks (Purandare and Pedp1 p2 p3 p4 p5 p6 p7 Default value 5 10 0.9 4 6 0.8 0.001 p180 Range Best 2-3 2 3-4 3 0.7-0.9 0.7 4 4 6-7 6 0.5-0.8 0.6 0.0005-0.001 0.0009 p1800 Range Best 1-3 2 2-4 3 0.5-0.7 0.5 4 4 3-7 3 0.4-0.8 0.7 0.0005-0.001 0.0009 p6700 Range Best 1-3 1 2-4 3 0.3-0.7 0.4 4 4 1-7 1 0.6-0.95 0.95 0.0009-0.003 0.001 Table 1: Parameters of the HyperLex algorithm ersen, 2004; Niu et al., 2005). See Section 6 for more details on these systems. Yet another possibility is to evaluate the induced senses against a gold standard as a clustering task. Induced senses are clusters, gold standard senses are classes, and measures from the clustering literature like entropy or purity can be used. As we wanted to focus on the comparison against a standard data-set, we decided to leave aside this otherwise interesting option. In this section we present a framework for automatically evaluating unsupervised WSD systems against publicly available hand-tagged corpora. The framework uses three data s"
W06-3814,W04-2406,0,0.126924,"r instance, senses are dense regions in a continuum (Cruse, 2000). Unsupervised WSD has followed this line of thinking, and tries to induce word senses directly from the corpus. Typical unsupervised WSD systems involve clustering techniques, which group together similar examples. Given a set of induced clusters (which represent word uses or senses1 ), each new occurrence of the target word will be compared to the clusters and the most similar cluster will be selected as its sense. Most of the unsupervised WSD work has been based on the vector space model (Sch¨utze, 1998; Pantel and Lin, 2002; Purandare and Pedersen, 2004), where each example is represented by a vector of features (e.g. the words occurring in the context). Recently, V´eronis (V´eronis, 2004) has 1 Unsupervised WSD approaches prefer the term ’word uses’ to ’word senses’. In this paper we use them interchangeably to refer to both the induced clusters, and to the word senses from some reference lexicon. 89 Workshop on TextGraphs, at HLT-NAACL 2006, pages 89–96, c New York City, June 2006. 2006 Association for Computational Linguistics proposed HyperLex, an application of graph models to WSD based on the small-world properties of cooccurrence graph"
W06-3814,J98-1004,0,0.775251,"Missing"
W06-3814,W04-0811,0,0.0472142,"ss inherent in any mapping step and the fact that the parameters were tuned for another set of words, these are very interesting results. 1 Introduction Word sense disambiguation (WSD) is a key enabling technology. Supervised WSD techniques are the best performing in public evaluations, but need large amounts of hand-tagging data. Existing handannotated corpora like SemCor (Miller et al., 1993), which is annotated with WordNet senses (Fellbaum, 1998) allow for a small improvement over the simple most frequent sense heuristic, as attested in the allwords track of the last Senseval competition (Snyder and Palmer, 2004). In theory, larger amounts of training data (SemCor has approx. 500M words) would improve the performance of supervised WSD, but no current project exists to provide such an expensive resource. Supervised WSD is based on the “fixed-list of senses” paradigm, where the senses for a target word are a closed list coming from a dictionary or lexicon. Lexicographers and semanticists have long warned about the problems of such an approach, where senses are listed separately as discrete entities, and have argued in favor of more complex representations, where, for instance, senses are dense regions i"
W09-3206,N09-1003,1,0.631483,"Missing"
W09-3206,J06-1003,0,0.41968,"ssful measure, Explicit Semantic Analysis (ESA) (Gabrilovich and Markovitch, 2007), treats each article as its own dimension in a vector space. Texts are compared by first projecting them into the space of Wikipedia articles and then comparing the resulting vectors. In addition to article text, Wikipedia stores a great deal of information about the relationships between the articles in the form of hyperlinks, info boxes, and category pages. Despite a long history of research demonstrating the effectiveness of incorporating link information into relatedness measures based on the WordNet graph (Budanitsky and Hirst, 2006), previous work on Wikipedia has made limited use of this relationship information, using only category links (Bunescu and Pasca, 2006) or just the actual links in a page (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). In this work, we combine previous approaches by converting Wikipedia into a graph, mapping input texts into the graph, and performing random walks based on Personalized PageRank (Haveliwala, 2002) to obtain stationary distributions that characterize each text. Semantic relatedness between two texts is computed by comparing their distributions. In contrast to previous"
W09-3206,E06-1002,0,0.0140215,"pace. Texts are compared by first projecting them into the space of Wikipedia articles and then comparing the resulting vectors. In addition to article text, Wikipedia stores a great deal of information about the relationships between the articles in the form of hyperlinks, info boxes, and category pages. Despite a long history of research demonstrating the effectiveness of incorporating link information into relatedness measures based on the WordNet graph (Budanitsky and Hirst, 2006), previous work on Wikipedia has made limited use of this relationship information, using only category links (Bunescu and Pasca, 2006) or just the actual links in a page (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008). In this work, we combine previous approaches by converting Wikipedia into a graph, mapping input texts into the graph, and performing random walks based on Personalized PageRank (Haveliwala, 2002) to obtain stationary distributions that characterize each text. Semantic relatedness between two texts is computed by comparing their distributions. In contrast to previous work, we explore the use of all these link types when conComputing semantic relatedness of natural language texts is a key component o"
W09-3206,D07-1061,1,0.852577,"information not present in the page text. 2 PageRank it is chosen from a nonuniform distribution of nodes, specified by a teleport vector. The final weight of node i represents the proportion of time the random particle spends visiting it after a sufficiently long time, and corresponds to that node’s structural importance in the graph. Because the resulting vector is the stationary distribution of a Markov chain, it is unique for a particular walk formulation. As the teleport vector is nonuniform, the stationary distribution will be biased towards specific parts of the graph. In the case of (Hughes and Ramage, 2007) and (Agirre and Soroa, 2009), the teleport vector is used to reflect the input texts to be compared, by biasing the stationary distribution towards the neighborhood of each word’s mapping. The computation of relatedness for a word pair can be summarized in three steps: First, each input word is mapped with to its respective synsets in the graph, creating its teleport vector. In the case words with multiple synsets (senses), the synsets are weighted uniformly. Personalized PageRank is then executed to compute the stationary distribution for each word, using their respective teleport vectors. F"
W09-3206,E09-1005,1,\N,Missing
W10-3301,alvez-etal-2008-complete,1,\N,Missing
W10-3301,E09-1005,1,\N,Missing
W14-4704,E09-1005,1,0.770643,"ques consider a given Knowledge Base (KB) as a graph, where vertices represent KB concepts and relations among concepts are represented by edges. For this particular task we represented WikiPedia as a graph, where articles are the vertices and links between articles are the edges. Contrary to other work using Wikipedia links (Gabrilovich and Markovitch, 2007; Milne and Witten, 2008), the use of the whole graph allows to apply algorithms that take into account the whole structure of Wikipedia. We applied PageRank and Personalized PageRank on the Wikipedia graph using freely available software (Agirre and Soroa, 2009; Agirre et al., 2014)3 . The PageRank algorithm (Brin and Page, 1998) ranks the vertices in a graph according to their relative structural importance. The main idea of PageRank is that whenever a link from vi to vj exists in a graph, a vote from node i to node j is produced, and hence the rank of node j increases. Besides, the strength of the vote from i to j also depends on the rank of node i: the more important node i is, the more strength its votes will have. Alternatively, PageRank can also be viewed as the result of a random walk process, where the final rank of node i represents the pro"
W14-4704,N09-1003,1,0.888771,"Disambiguation or Information Extraction, and other related areas like Information Retrieval. Most of the proposed techniques are evaluated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot"
W14-4704,agirre-etal-2010-exploring,1,0.703945,"formation Extraction, and other related areas like Information Retrieval. Most of the proposed techniques are evaluated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot of models have been d"
W14-4704,P14-1023,0,0.0135501,"in NLP on the last years, specially in semantics. A lot of models have been developed, but all of them share two characteristics: they learn meaning from non-labeled corpora and represent meaning in a distributional way. These models learn the meaning of words from corpora, and they represent it distributionally by the so-called embeddings. This embeddings are low-dimensional and dense vectors composed by integers, where the dimensions are latent semantic features of words. We have used the Mikolov model (Mikolov et al., 2013) for this task, due to its effectiveness in similarity experiments (Baroni et al., 2014). This neural network reduces the computational complexity of previous architectures by deleting the hidden layer, and also, it’s able to train with larger corpora (more than 109 words) and extract embeddings with larger dimensionality. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 31 Zock/Rapp/Huang (eds.): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 31–34, Dublin, Ireland, August 23, 2014. The Miko"
W14-4704,J06-1003,0,0.194346,"Missing"
W14-4704,P06-1127,0,0.040705,"ated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot of models have been developed, but all of them share two characteristics: they learn meaning from non-labeled corpora and represent m"
W14-4704,D07-1061,0,0.0286782,"al Entailment, Word Sense Disambiguation or Information Extraction, and other related areas like Information Retrieval. Most of the proposed techniques are evaluated over manually curated word similarity datasets like WordSim353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, speciall"
W14-4704,N13-1090,0,0.0206542,"353 (Finkelstein et al., 2002), in which the weights returned by the systems for word pairs are compared with human ratings. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or encyclopedias) (Alvarez and Lim, 2007; Yang and Powers, 2005; Hughes and Ramage, 2007; Agirre et al., 2009; Agirre et al., 2010) and those inducing distributional properties of words from corpora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007; Agirre et al., 2009; Mikolov et al., 2013). Our main objective when participating in the CogAlex shared task was to check how a sample of each kind of technique would cope with the task. We thus selected one of the best corpus-based models to date and another approach based on random walks over WordNet and Wikipedia. 2 Word Embeddings Neural Networks have become quite a useful tool in NLP on the last years, specially in semantics. A lot of models have been developed, but all of them share two characteristics: they learn meaning from non-labeled corpora and represent meaning in a distributional way. These models learn the meaning of wo"
W14-4704,J14-1003,1,\N,Missing
W18-2505,P16-1085,0,0.36085,"Missing"
W18-2505,J14-1003,1,0.870351,"Missing"
W18-2505,E09-1005,1,0.922462,"r, graph-based approaches represent the knowledge base as a graph, and apply several well-known graph analysis algorithms to perform WSD. 1 http://compling.hss.ntu.edu.sg/omw/ 29 Proceedings of Workshop for NLP Open Source Software, pages 29–33 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a pre-existing knowledge base such as WordNet, and attained state-ofthe-art results among knowledge-based systems when evaluated on standard benchmarks (Agirre and Soroa, 2009; Agirre et al., 2014). In addition, UKB has been extended to perform disambiguation of medical entities (Agirre et al., 2010), named-entities (Erbs et al., 2012; Agirre et al., 2015), word similarity (Agirre et al., 2009) and to create knowledge-based word embeddings (Goikoetxea et al., 2015). All programs are open source2,3 and are accompanied by the resources and instructions necessary to reproduce the results. The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011. The software is coded in C++ an"
W18-2505,K16-1006,0,0.155847,"Missing"
W18-2505,H93-1061,0,0.635989,"r as well. The difference is of nearly 10 absolute F1 points overall.5 This decrease could be caused by the fact that Raganato et al. (2017a) did not use sense frequencies. In addition to UKB, the table also reports the best performing knowledge-based systems on this dataset. Raganato et al. (2017a) run several wellknown algorithms when presenting their datasets. We also report (Chaplot and Sakajhutdinov, 2018), tion that describe the frequencies of the associations between a word and its possible senses. The frequencies are often derived from manually sense annotated corpora, such as Semcor (Miller et al., 1993). We use the sense frequency accompanying Wordnet, which, according to the documentation, ”represents the decimal number of times the sense is tagged in various semantic concordance texts”. The frequencies are smoothed adding one to all counts (dict weight smooth). The sense frequency is used when initializing context words, and is also used to produce the final sense weights as a linear combination of sense frequencies and graph-based sense probabilities. The use of sense frequencies with UKB was introduced in (Agirre et al., 2014). 4 Comparison to the state-of-the-art We evaluate UKB on the"
W18-2505,N09-1003,1,0.527001,"oftware, pages 29–33 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a pre-existing knowledge base such as WordNet, and attained state-ofthe-art results among knowledge-based systems when evaluated on standard benchmarks (Agirre and Soroa, 2009; Agirre et al., 2014). In addition, UKB has been extended to perform disambiguation of medical entities (Agirre et al., 2010), named-entities (Erbs et al., 2012; Agirre et al., 2015), word similarity (Agirre et al., 2009) and to create knowledge-based word embeddings (Goikoetxea et al., 2015). All programs are open source2,3 and are accompanied by the resources and instructions necessary to reproduce the results. The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011. The software is coded in C++ and released under the GPL v3.0 license. When UKB was released, the papers specified the optimal parameters for WSD (Agirre and Soroa, 2009; Agirre et al., 2014), as well as other key issues like the underlying knowledge-bas"
W18-2505,Q14-1019,0,0.563686,"Missing"
W18-2505,E17-1010,0,0.461973,"e experiments of (Agirre and Soroa, 2009) somewhat arbitrarily, and never changed afterwards. 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb 30 UKB (this work) UKB (elsewhere)†‡ Chaplot and Sakajhutdinov (2018) ‡ Babelfy (Moro et al., 2014)† MFS Basile et al. (2014)† Banerjee and Pedersen (2003)† All 67.3 57.5 66.9 65.5 65.2 63.7 48.7 S2 68.8 60.6 69.0 67.0 66.8 63.0 50.6 S3 66.1 54.1 66.9 63.5 66.2 63.7 44.5 S07 53.0 42.0 55.6 51.6 55.2 56.7 32.0 S13 68.8 59.0 65.3 66.4 63.0 66.2 53.6 S15 70.3 61.2 69.6 70.3 67.8 64.6 51.0 Table 1: F1 results for knowledge-based systems on the (Raganato et al., 2017a) dataset. Top rows show conflicting results for UKB. † for results reported in (Raganato et al., 2017a), ‡ for results reported in (Chaplot and Sakajhutdinov, 2018). Best results in bold. S2 stands for Senseval-2, S3 for Senseval-3, S07 for Semeval-2007, S13 for Semeval-2013 and S15 for Semeval-2015. Yuan et al. (2016) Raganato et al. (2017b) Iacobacci et al. (2016)† Melamud et al. (2016)† IMS (Zhong and Ng, 2010)† All 71.5 69.9 69.7 69.4 68.8 S2 73.8 72.0 73.3 72.3 72.8 S3 71.8 69.1 69.6 68.2 69.2 S07 63.5 64.8 61.1 61.5 60.0 S13 69.5 66.9 66.7 67.2 65.0 S15 72.6 71.5 70.4 71.7 69.3 Table 2"
W18-2505,C14-1151,0,0.502743,"Missing"
W18-2505,D17-1120,0,0.381741,"e experiments of (Agirre and Soroa, 2009) somewhat arbitrarily, and never changed afterwards. 2 http://ixa2.si.ehu.eus/ukb 3 https://github.com/asoroa/ukb 30 UKB (this work) UKB (elsewhere)†‡ Chaplot and Sakajhutdinov (2018) ‡ Babelfy (Moro et al., 2014)† MFS Basile et al. (2014)† Banerjee and Pedersen (2003)† All 67.3 57.5 66.9 65.5 65.2 63.7 48.7 S2 68.8 60.6 69.0 67.0 66.8 63.0 50.6 S3 66.1 54.1 66.9 63.5 66.2 63.7 44.5 S07 53.0 42.0 55.6 51.6 55.2 56.7 32.0 S13 68.8 59.0 65.3 66.4 63.0 66.2 53.6 S15 70.3 61.2 69.6 70.3 67.8 64.6 51.0 Table 1: F1 results for knowledge-based systems on the (Raganato et al., 2017a) dataset. Top rows show conflicting results for UKB. † for results reported in (Raganato et al., 2017a), ‡ for results reported in (Chaplot and Sakajhutdinov, 2018). Best results in bold. S2 stands for Senseval-2, S3 for Senseval-3, S07 for Semeval-2007, S13 for Semeval-2013 and S15 for Semeval-2015. Yuan et al. (2016) Raganato et al. (2017b) Iacobacci et al. (2016)† Melamud et al. (2016)† IMS (Zhong and Ng, 2010)† All 71.5 69.9 69.7 69.4 68.8 S2 73.8 72.0 73.3 72.3 72.8 S3 71.8 69.1 69.6 68.2 69.2 S07 63.5 64.8 61.1 61.5 60.0 S13 69.5 66.9 66.7 67.2 65.0 S15 72.6 71.5 70.4 71.7 69.3 Table 2"
W18-2505,P10-4014,0,0.698767,"Missing"
W18-2505,N15-1165,1,0.864323,"ociation for Computational Linguistics UKB is a collection of programs which was first released for performing graph-based Word Sense Disambiguation using a pre-existing knowledge base such as WordNet, and attained state-ofthe-art results among knowledge-based systems when evaluated on standard benchmarks (Agirre and Soroa, 2009; Agirre et al., 2014). In addition, UKB has been extended to perform disambiguation of medical entities (Agirre et al., 2010), named-entities (Erbs et al., 2012; Agirre et al., 2015), word similarity (Agirre et al., 2009) and to create knowledge-based word embeddings (Goikoetxea et al., 2015). All programs are open source2,3 and are accompanied by the resources and instructions necessary to reproduce the results. The software is quite popular, with 60 stars and 26 forks in github, as well as more than eight thousand direct downloads from the website since 2011. The software is coded in C++ and released under the GPL v3.0 license. When UKB was released, the papers specified the optimal parameters for WSD (Agirre and Soroa, 2009; Agirre et al., 2014), as well as other key issues like the underlying knowledge-base version, specific set of relations to be used, and method to pre-proce"
