2020.sigdial-1.41,afantenos-etal-2012-empirical,0,0.0280683,"Missing"
2020.sigdial-1.41,L16-1432,0,0.0639097,"Missing"
2020.sigdial-1.41,W17-5526,0,0.0318805,"Missing"
2020.sigdial-1.41,W18-5048,0,0.0892071,"al., 339 Proceedings of the SIGdial 2020 Conference, pages 339–351 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics 2018) is related to learning from demonstration (LfD) (Mulling et al., 2013; Rana et al., 2017), where the goal is for agents to learn through immediate and direct experience rather than through offline processing of large datasets. Previous work on learning through communication has focused on joint grounding of perception and language in task learning (Liu et al., 2016), complex concept grounding (Matuszek, 2018), or collaborative action (Galescu et al., 2018; Perera et al., 2018b), rather than dialogue management. Our work investigates reinforcement learning of dialogue policies, which makes it easy to produce and compare many policies. We exploit the ability to control the behavior of simulated dialogue partners to investigate policy training when dialogue partners vary in informativeness. We develop a policy with hierarchical structure based on a global policy for context-switching, and a local policy for formulating specific questions given a context. We present two kinds of experiments. First, we compare the MDP policies for different games a"
2020.sigdial-1.41,W09-0613,0,0.0112817,"al., 2016), an agent learns cloth folding through rich verbal communication, based on AND-OR graphs. It can understand utterances with context dependencies common to human language but challenging for machines (e.g., descriptions of objects that evolve over several utterances). Language interaction via semantic parsing combined with deep reasoning is used in agents that explain their actions (Kasenberg et al., 2019b,a), using existing NLP tools for parsing into a logical form (Steedman and Baldridge, 2011), and a rule-based, broadcoverage toolkit for generating English from structured input (Gatt and Reiter, 2009). Other work that relies on rich, situated reasoning through multimodal communication is based on an architecture for collaborative problem-solving (Galescu et al., 2018), with plan-based dialogue management (Perera et al., 2018a). These works either do not have distinct dialogue management modules, or rely on manually-engineered dialogue management rather than machine-learning. Our work presents machine-learned MDP policies using a method that generalizes across different games, and across differences in dialogue partners’ informativeness. 3 Game-learning Dialogues: Overview Three games our a"
2020.sigdial-1.41,W19-8660,0,0.0166048,"can also be grounded more directly in perception, by machine learning the relevant perceptual categories from data, rather than pre-specifying them in a formal semantics (Pillai et al., 2019). In (Liu et al., 2016), an agent learns cloth folding through rich verbal communication, based on AND-OR graphs. It can understand utterances with context dependencies common to human language but challenging for machines (e.g., descriptions of objects that evolve over several utterances). Language interaction via semantic parsing combined with deep reasoning is used in agents that explain their actions (Kasenberg et al., 2019b,a), using existing NLP tools for parsing into a logical form (Steedman and Baldridge, 2011), and a rule-based, broadcoverage toolkit for generating English from structured input (Gatt and Reiter, 2009). Other work that relies on rich, situated reasoning through multimodal communication is based on an architecture for collaborative problem-solving (Galescu et al., 2018), with plan-based dialogue management (Perera et al., 2018a). These works either do not have distinct dialogue management modules, or rely on manually-engineered dialogue management rather than machine-learning. Our work presen"
2020.sigdial-1.41,W19-8406,0,0.015927,"can also be grounded more directly in perception, by machine learning the relevant perceptual categories from data, rather than pre-specifying them in a formal semantics (Pillai et al., 2019). In (Liu et al., 2016), an agent learns cloth folding through rich verbal communication, based on AND-OR graphs. It can understand utterances with context dependencies common to human language but challenging for machines (e.g., descriptions of objects that evolve over several utterances). Language interaction via semantic parsing combined with deep reasoning is used in agents that explain their actions (Kasenberg et al., 2019b,a), using existing NLP tools for parsing into a logical form (Steedman and Baldridge, 2011), and a rule-based, broadcoverage toolkit for generating English from structured input (Gatt and Reiter, 2009). Other work that relies on rich, situated reasoning through multimodal communication is based on an architecture for collaborative problem-solving (Galescu et al., 2018), with plan-based dialogue management (Perera et al., 2018a). These works either do not have distinct dialogue management modules, or rely on manually-engineered dialogue management rather than machine-learning. Our work presen"
2020.sigdial-1.41,D16-1155,0,0.143646,"dialogue partners vary in their informativeness. Learning through communication (Chai et al., 339 Proceedings of the SIGdial 2020 Conference, pages 339–351 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics 2018) is related to learning from demonstration (LfD) (Mulling et al., 2013; Rana et al., 2017), where the goal is for agents to learn through immediate and direct experience rather than through offline processing of large datasets. Previous work on learning through communication has focused on joint grounding of perception and language in task learning (Liu et al., 2016), complex concept grounding (Matuszek, 2018), or collaborative action (Galescu et al., 2018; Perera et al., 2018b), rather than dialogue management. Our work investigates reinforcement learning of dialogue policies, which makes it easy to produce and compare many policies. We exploit the ability to control the behavior of simulated dialogue partners to investigate policy training when dialogue partners vary in informativeness. We develop a policy with hierarchical structure based on a global policy for context-switching, and a local policy for formulating specific questions given a context. We"
2020.sigdial-1.41,W18-1402,0,0.154698,"of the SIGdial 2020 Conference, pages 339–351 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics 2018) is related to learning from demonstration (LfD) (Mulling et al., 2013; Rana et al., 2017), where the goal is for agents to learn through immediate and direct experience rather than through offline processing of large datasets. Previous work on learning through communication has focused on joint grounding of perception and language in task learning (Liu et al., 2016), complex concept grounding (Matuszek, 2018), or collaborative action (Galescu et al., 2018; Perera et al., 2018b), rather than dialogue management. Our work investigates reinforcement learning of dialogue policies, which makes it easy to produce and compare many policies. We exploit the ability to control the behavior of simulated dialogue partners to investigate policy training when dialogue partners vary in informativeness. We develop a policy with hierarchical structure based on a global policy for context-switching, and a local policy for formulating specific questions given a context. We present two kinds of experiments. First, we compare the MDP policies for different games and different levels o"
2020.sigdial-1.41,W18-5010,0,0.0988035,"of the SIGdial 2020 Conference, pages 339–351 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics 2018) is related to learning from demonstration (LfD) (Mulling et al., 2013; Rana et al., 2017), where the goal is for agents to learn through immediate and direct experience rather than through offline processing of large datasets. Previous work on learning through communication has focused on joint grounding of perception and language in task learning (Liu et al., 2016), complex concept grounding (Matuszek, 2018), or collaborative action (Galescu et al., 2018; Perera et al., 2018b), rather than dialogue management. Our work investigates reinforcement learning of dialogue policies, which makes it easy to produce and compare many policies. We exploit the ability to control the behavior of simulated dialogue partners to investigate policy training when dialogue partners vary in informativeness. We develop a policy with hierarchical structure based on a global policy for context-switching, and a local policy for formulating specific questions given a context. We present two kinds of experiments. First, we compare the MDP policies for different games and different levels o"
2020.sigdial-1.41,rosset-petel-2006-ritel,0,0.13325,"Missing"
2020.sigdial-1.41,W13-4067,0,0.0478641,"Missing"
2021.acl-long.303,P18-2114,0,0.0192738,"ces as part of a summarization evaluation tool. Niklaus et al. (2019a) present a system (DisSim) based on parsing to extract simple sentences from complex ones. Jo et al. (2020) propose seven rules to extract complete propositions from parses of complex questions and imperatives for argumentation mining. Though performance of these methods depends on parser quality, they often achieve very good performance. We include two whose code is available (DCP, DisSim) among our baselines. SPRP models are based on encoder-decoder architectures, and the output is highly depending on the training corpus. Aharoni and Goldberg (2018) present a Copy-augmented network (Copy512 ) based on (Gu et al., 2016) that encour1 ABCD is available at https://github.com/ serenayj/ABCD-ACL2021. ages the model to copy most words from the original sentence to the output. As it achieves improvement over an earlier encoder-decoder SPRP model (Narayan et al., 2017), we include Copy512 among our baselines. Finally, recent neural EDU segmenters (Wang et al., 2018; Li et al., 2018) achieve state-of-the-art performance on a discourse relation corpus, RSTDT (Carlson et al., 2003). As they do not output complete sentences, we do not include any amo"
2021.acl-long.303,K19-1038,1,0.851162,"Drop elements of a special-purpose sentence graph that represents word adjacency and grammatical dependencies, so the model can learn based on both kinds of graph proximity. We also introduce DeSSE (Decomposed Sentences from Students Essays), a new annotated dataset to support our task. The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. 1 2 Related Work Related work falls largely into parsing-based methods, neural models that rewrite, and neural segmenters. Gao et al. (2019) propose a decomposition parser (DCP) that extracts VP constituents and clauses from complex sentences as part of a summarization evaluation tool. Niklaus et al. (2019a) present a system (DisSim) based on parsing to extract simple sentences from complex ones. Jo et al. (2020) propose seven rules to extract complete propositions from parses of complex questions and imperatives for argumentation mining. Though performance of these methods depends on parser quality, they often achieve very good performance. We include two whose code is available (DCP, DisSim) among our baselines. SPRP models are"
2021.acl-long.303,W98-0303,0,0.0402043,"ves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis. 1 Introduction Atomic clauses are fundamental text units for understanding complex sentences. The ability to decompose complex sentences facilitates research that aims to identify, rank or relate distinct predications, such as content selection in summarization (Fang et al., 2016; Peyrard and Eckle-Kohler, 2017), labeling argumentative discourse units in argument mining (Jo et al., 2019) or elementary discourse units in discourse analysis (Mann and Thompson, 1986; Burstein et al., 1998; Demir et al., 2010), or extracting atomic propositions for question answering (Pyatkin et al., 2020). In this work, we propose a new task to decompose complex sentences into a covering set of simple sentences, with one simple output sentence per tensed clause in the source sentence. We focus on tensed clauses rather than other constituents because they are syntactically and semantically more prominent, thus more essential in downstream tasks like argument mining, summarization, and question answering. The complex sentence decomposition task we address has some overlap with related NLP algori"
2021.acl-long.303,P16-1154,0,0.0385784,"ystem (DisSim) based on parsing to extract simple sentences from complex ones. Jo et al. (2020) propose seven rules to extract complete propositions from parses of complex questions and imperatives for argumentation mining. Though performance of these methods depends on parser quality, they often achieve very good performance. We include two whose code is available (DCP, DisSim) among our baselines. SPRP models are based on encoder-decoder architectures, and the output is highly depending on the training corpus. Aharoni and Goldberg (2018) present a Copy-augmented network (Copy512 ) based on (Gu et al., 2016) that encour1 ABCD is available at https://github.com/ serenayj/ABCD-ACL2021. ages the model to copy most words from the original sentence to the output. As it achieves improvement over an earlier encoder-decoder SPRP model (Narayan et al., 2017), we include Copy512 among our baselines. Finally, recent neural EDU segmenters (Wang et al., 2018; Li et al., 2018) achieve state-of-the-art performance on a discourse relation corpus, RSTDT (Carlson et al., 2003). As they do not output complete sentences, we do not include any among our baselines. Our ABCD model leverages the detailed information cap"
2021.acl-long.303,W10-4202,0,0.0478592,"the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis. 1 Introduction Atomic clauses are fundamental text units for understanding complex sentences. The ability to decompose complex sentences facilitates research that aims to identify, rank or relate distinct predications, such as content selection in summarization (Fang et al., 2016; Peyrard and Eckle-Kohler, 2017), labeling argumentative discourse units in argument mining (Jo et al., 2019) or elementary discourse units in discourse analysis (Mann and Thompson, 1986; Burstein et al., 1998; Demir et al., 2010), or extracting atomic propositions for question answering (Pyatkin et al., 2020). In this work, we propose a new task to decompose complex sentences into a covering set of simple sentences, with one simple output sentence per tensed clause in the source sentence. We focus on tensed clauses rather than other constituents because they are syntactically and semantically more prominent, thus more essential in downstream tasks like argument mining, summarization, and question answering. The complex sentence decomposition task we address has some overlap with related NLP algorithms, but each falls"
2021.acl-long.303,N19-1423,0,0.0134454,"ngth l, and the hidden state dimension M , the output from this module is l × M . For a word with index i in the input sentence, we generate its hidden representation hi such that it combines the hidden states from forward and backward LSTMs, with 3923 hi ∈ RM . A positional encoding function is added to the word embeddings. We found this particularly helpful in our task, presumably because the same word type at different positions might have different relations with other words, captured by distinct learned representations. Our experiments compare biLSTM training from scratch to use of BERT (Devlin et al., 2019), to see if pre-trained representations are helpful. To utilize the learned word representations in the context of the relational information captured in the WRG graph, we send the sentence representation to the Edge Triple DB and extract representations hi and hj for the source and target words, based on indices i and j. A one-hot vector with dimensionality N encodes relations between pairs of source and target words; each edge triple is thus converted into three vectors: hsrc , htgt , drel . We take positionwise summation over all one hot vectors if there is more than one label on an edge. 6"
2021.acl-long.303,C16-1055,0,0.068024,"Missing"
2021.acl-long.303,P13-1167,0,0.0582802,"Missing"
2021.acl-long.303,2021.textgraphs-1.6,1,0.787852,"ses in the source sentences matches the number of rephrased propositions. The resulting MinWiki corpus has an 18K/1,075 train/test split. Table 1: Prevalence of five linguistic phenomena in 50 randomly selected examples per dataset. Categories are not mutually exclusive. clauses are explicitly rewritten as simple sentences. It differs from split-and-rephrase corpora such as MinWikiSplit, because of the focus in DeSSE on rephrased simple sentences that have a one-to-one correspondence to tensed clauses in the original complex sentence. DeSSE is also used for connective prediction tasks, as in (Gao et al., 2021).2 We perform our task on Amazon Mechanical Turk (AMT). In a series of pilot tasks on AMT, we iteratively designed annotation instructions and an annotation interface, while monitoring quality. Figure 2 illustrates two steps in the annotation: identification of n split points between tensed clauses, and rephrasing the source into n + 1 simple clauses, where any connectives are dropped. The instructions ask annotators to focus on tensed clauses occurring in conjoined or subordinate structures, relative clauses, parentheticals, and conjoined verb phrases, and to exclude gerundive phrases, infint"
2021.acl-long.303,2020.lrec-1.127,0,0.0245729,"our task. The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. 1 2 Related Work Related work falls largely into parsing-based methods, neural models that rewrite, and neural segmenters. Gao et al. (2019) propose a decomposition parser (DCP) that extracts VP constituents and clauses from complex sentences as part of a summarization evaluation tool. Niklaus et al. (2019a) present a system (DisSim) based on parsing to extract simple sentences from complex ones. Jo et al. (2020) propose seven rules to extract complete propositions from parses of complex questions and imperatives for argumentation mining. Though performance of these methods depends on parser quality, they often achieve very good performance. We include two whose code is available (DCP, DisSim) among our baselines. SPRP models are based on encoder-decoder architectures, and the output is highly depending on the training corpus. Aharoni and Goldberg (2018) present a Copy-augmented network (Copy512 ) based on (Gu et al., 2016) that encour1 ABCD is available at https://github.com/ serenayj/ABCD-ACL2021. a"
2021.acl-long.303,W19-4502,0,0.0210306,"on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis. 1 Introduction Atomic clauses are fundamental text units for understanding complex sentences. The ability to decompose complex sentences facilitates research that aims to identify, rank or relate distinct predications, such as content selection in summarization (Fang et al., 2016; Peyrard and Eckle-Kohler, 2017), labeling argumentative discourse units in argument mining (Jo et al., 2019) or elementary discourse units in discourse analysis (Mann and Thompson, 1986; Burstein et al., 1998; Demir et al., 2010), or extracting atomic propositions for question answering (Pyatkin et al., 2020). In this work, we propose a new task to decompose complex sentences into a covering set of simple sentences, with one simple output sentence per tensed clause in the source sentence. We focus on tensed clauses rather than other constituents because they are syntactically and semantically more prominent, thus more essential in downstream tasks like argument mining, summarization, and question an"
2021.acl-long.303,D17-1064,0,0.142831,"ects. Elementary discourse unit (EDU) segmentation segments source sentences into a sequence of non-overlapping spans (Carlson et al., 2003; Wang et al., 2018). The output EDUs, however, are not always complete clauses. Text simplification rewrites complex sentences using simpler vocabulary and syntax (Zhang and Lapata, 2017). The output, however, does not preserve every tensed clause in the original sentence. The split-and-rephrase (SPRP) task aims to rewrite complex sentences into sets of shorter sentences, where an output sentence can be derived from non-clausal constituents in the source (Narayan et al., 2017). In contrast to the preceding methods, we convert each tensed clause in a source sentence, including each conjunct in a conjoined VP, into an independent simple sentence. Unlike EDU segmentation, a belief verb and its that-complement do not lead to two output units. Unlike text simplification, no propositions in the source are omitted from the output. Unlike SPRP, a phrase that lacks a tensed verb in the source cannot 3919 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 39"
2021.acl-long.303,W19-8662,0,0.291163,"roximity. We also introduce DeSSE (Decomposed Sentences from Students Essays), a new annotated dataset to support our task. The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. 1 2 Related Work Related work falls largely into parsing-based methods, neural models that rewrite, and neural segmenters. Gao et al. (2019) propose a decomposition parser (DCP) that extracts VP constituents and clauses from complex sentences as part of a summarization evaluation tool. Niklaus et al. (2019a) present a system (DisSim) based on parsing to extract simple sentences from complex ones. Jo et al. (2020) propose seven rules to extract complete propositions from parses of complex questions and imperatives for argumentation mining. Though performance of these methods depends on parser quality, they often achieve very good performance. We include two whose code is available (DCP, DisSim) among our baselines. SPRP models are based on encoder-decoder architectures, and the output is highly depending on the training corpus. Aharoni and Goldberg (2018) present a Copy-augmented network (Copy51"
2021.acl-long.303,W19-8615,0,0.195234,"roximity. We also introduce DeSSE (Decomposed Sentences from Students Essays), a new annotated dataset to support our task. The rest of the paper presents two evaluation datasets, our full pipeline, and our ABCD model. Experimental results show that ABCD achieves comparable or better performance than baselines. 1 2 Related Work Related work falls largely into parsing-based methods, neural models that rewrite, and neural segmenters. Gao et al. (2019) propose a decomposition parser (DCP) that extracts VP constituents and clauses from complex sentences as part of a summarization evaluation tool. Niklaus et al. (2019a) present a system (DisSim) based on parsing to extract simple sentences from complex ones. Jo et al. (2020) propose seven rules to extract complete propositions from parses of complex questions and imperatives for argumentation mining. Though performance of these methods depends on parser quality, they often achieve very good performance. We include two whose code is available (DCP, DisSim) among our baselines. SPRP models are based on encoder-decoder architectures, and the output is highly depending on the training corpus. Aharoni and Goldberg (2018) present a Copy-augmented network (Copy51"
2021.acl-long.303,P19-1459,0,0.0225089,"del leverages the detailed information captured by parsing methods, and the powerful representation learning of neural models. As part of a larger pipeline that converts input sentences to graphs, ABCD learns to predict graph edits for a post processor to execute. 3 Datasets Here we present DeSSE, a corpus we collected for our task, and MinWiki, a modification of an existing SPRP corpus (MinWikiSplit (Niklaus et al., 2019b)) to support our aims. We also give a brief description of differences in their distributions. Neural models are heavily biased by the distributions in their training data (Niven and Kao, 2019), and we show that DeSSE has a more even balance of linguistic phenomena. 3.1 DeSSE DeSSE is collected in an undergraduate social science class, where students watched video clips about race relations, and wrote essays in a blog environment to share their opinions with the class. It was created to support analysis of student writing, so that different kinds of feedback mechanisms can be developed regarding sentence organization. Students have difficulty with revision to address lack of clarity in their writing (Kuhn et al., 2016), such as non-specific uses of connectives, run on sentences, rep"
2021.acl-long.303,D17-1062,0,0.0199985,"e they are syntactically and semantically more prominent, thus more essential in downstream tasks like argument mining, summarization, and question answering. The complex sentence decomposition task we address has some overlap with related NLP algorithms, but each falls short in one or more respects. Elementary discourse unit (EDU) segmentation segments source sentences into a sequence of non-overlapping spans (Carlson et al., 2003; Wang et al., 2018). The output EDUs, however, are not always complete clauses. Text simplification rewrites complex sentences using simpler vocabulary and syntax (Zhang and Lapata, 2017). The output, however, does not preserve every tensed clause in the original sentence. The split-and-rephrase (SPRP) task aims to rewrite complex sentences into sets of shorter sentences, where an output sentence can be derived from non-clausal constituents in the source (Narayan et al., 2017). In contrast to the preceding methods, we convert each tensed clause in a source sentence, including each conjunct in a conjoined VP, into an independent simple sentence. Unlike EDU segmentation, a belief verb and its that-complement do not lead to two output units. Unlike text simplification, no proposi"
2021.acl-long.303,P02-1040,0,0.11278,"encoding which leads to F1 scores of 0.90 for A, 0.51 for C, and 0 for both B and D, indicating the importance of positional encoding. 7.2 Intrinsic Evaluation of Output Sentences For baselines, we use Copy512 and DisSim, which both report performance on Wikisplit in previous work. We also include DCP, which relies on three rules applied to token-aligned dependency and constituency parses: DCPvp extracts clauses with tensed verb phrases; DCPsbar extracts SBAR subtrees from constituency trees; DCPrecur recursively applies the preceding rules. For evaluation, we use BLEU with four-grams (BL4) (Papineni et al., 2002) and BERTScore (BS) (Zhang et al., 2019). We also include descriptive measures specific to our task. To indicate whether a model retains roughly the same number of words as the source sentence in the target output, we report average number of tokens per simple sentence (#T/SS). To capture the correspondence between the number of target simple sentences in the ground truth and model predictions, we use percentage of samples where the model predicts the correct number of simple sentences (Match #SS). BL4 captures the 4-gram alignments between candidate and reference word strings, but fails to as"
2021.acl-long.303,D14-1162,0,0.0864023,"ple sentence. Figure 5: Architecture for ABCD model. 6 Neural Model The ABCD model consists of three neural modules depicted in Figure 5: a sentence encoder to learn a hidden representation for the input sentence, a self-attention layer to generate attention scores on every edge label, and a classifier that generates a predicted distribution over the four edit types, based on the word’s hidden representation, the edge label representation, and the attention scores. 6.1 Sentence Representation The sentence representation module has two components: a word embedding look up layer based on GloVe (Pennington et al., 2014), and a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) (see Figure 5). Given an input sentence length l, and the hidden state dimension M , the output from this module is l × M . For a word with index i in the input sentence, we generate its hidden representation hi such that it combines the hidden states from forward and backward LSTMs, with 3923 hi ∈ RM . A positional encoding function is added to the word embeddings. We found this particularly helpful in our task, presumably because the same word type at different positions might have different relations with other words, captured by"
2021.acl-long.303,P17-1100,0,0.021553,"iki, a subset of MinWikiSplit. ABCD achieves comparable performance as two parsing baselines on MinWiki. On DeSSE, which has a more even balance of complex sentence types, our model achieves higher accuracy on the number of atomic sentences than an encoder-decoder baseline. Results include a detailed error analysis. 1 Introduction Atomic clauses are fundamental text units for understanding complex sentences. The ability to decompose complex sentences facilitates research that aims to identify, rank or relate distinct predications, such as content selection in summarization (Fang et al., 2016; Peyrard and Eckle-Kohler, 2017), labeling argumentative discourse units in argument mining (Jo et al., 2019) or elementary discourse units in discourse analysis (Mann and Thompson, 1986; Burstein et al., 1998; Demir et al., 2010), or extracting atomic propositions for question answering (Pyatkin et al., 2020). In this work, we propose a new task to decompose complex sentences into a covering set of simple sentences, with one simple output sentence per tensed clause in the source sentence. We focus on tensed clauses rather than other constituents because they are syntactically and semantically more prominent, thus more essen"
2021.acl-long.303,prasad-etal-2008-penn,0,0.031357,"heir writing (Kuhn et al., 2016), such as non-specific uses of connectives, run on sentences, repetitive statements and the like. These make DeSSE different from corpus with expert written text, such as Wikipedia and newspaper. The annotation process is unique in that it involves identifying where to split a source complex sentence into distinct clauses, and how to rephrase each resulting segment as a semantically complete simple sentence, omitting any discourse connectives. It differs from corpora that identify discourse units within sentences, such as RST-DT (Carlson et al., 2003) and PTDB (Prasad et al., 2008), because 3920 • Orig: (I believe that talking about race more in a civil way can only improve our society), ||but I can see why other people may have a different opinion. • Rephrase 1: I believe that talking about race more in a civil way can only improve our society. • Rephrase 2: I can see why other people may have a different opinion. Figure 2: An original sentences from DeSSE with an intrasentential connective (but), a verb that takes a clausal argument. The annotation first splits the sentence (at ||), then rephrases each segment into a simple sentence, dropping the connective. Dataset M"
2021.acl-long.303,D18-1116,0,0.0520077,"Missing"
2021.emnlp-main.487,P19-1007,0,0.0250391,"ng data and to add irrelevant noise to examples to improve the 3.1 Creating the QRA Relation Vectors robustness of learned models. Recently, data augThe encoded vectors for a given triple are mentation has been found to significantly improve (q, rj , a), j ∈ {0, 1, . . . , n}, where n is the numperformance on NLP tasks as various as paraphrasber of reference answers for a given question q and ing (Wieting et al., 2017), natural language gena student answer a of this question. Corresponding eration (Kedzie and McKeown, 2019), semantic to equation (1) above, the relation vectors lj are parsing (Cao et al., 2019), or various sentiment inferred by gθ , using the concatenation of QRA and opinion classification tasks (Kobayashi, 2018). vectors from the encoding step as the input (left One method is to substitute a random word with a hashed box in Figure 2): synonym drawn from a lexical database like WordNet (Mueller and Thyagarajan, 2016; Zhang et al., lj = gθ ([q, rj , a]) (2) 6032 Figure 2: The structure of SFRN. The gθ -MLP function computes the relation vector for each [Q,R,A] triple. A set of relation vectors is combined (+) using SF T . The fφ -MLP function is the assessment classifier. where g is"
2021.emnlp-main.487,N19-1423,0,0.0220637,"e input to SFT is thus the set of triples C, the set of learned relations L, and for clarity the size n of these sets: SFT(C, L, n) = n X j=1 (α(cj ) lj + β(cj )) (3) SF RN ([Q,R,A]) = fφ (SFT(gθ )) 3.3 (4) SFRN Encoder We experiment with different SFRN encoders. Our baseline SFRN model uses LSTM (Hochreiter and Schmidhuber, 1997), which is relatively easy to train, but prone to overfitting and information loss. We compare LSTM with a BERT-based encoder. BERT is a deep, pre-trained, transformer-based model that has proven to be extremely powerful when fine-tuned for a wide range of NLP tasks (Devlin et al., 2019). We use the last output layer of the BERT base model, and fine-tune it on the ASAG data. We also use the pre-trained BERT base as an encoder in a logistic regression baseline. We use the bert-base-uncased model pre-trained on the BooksCorpus and Wikipedia, with a 30,000 token vocabulary, and 110 million parameters. For fine-tuning, we pre-process the sentences in our QRA triples by prefixing [CLS] and postfixing [SEP] to the word token lists for each question text, or reference or student answer. Then we take the last layer output (of 12 layers in total) as the vector encodings of the element"
2021.emnlp-main.487,2020.coling-main.77,0,0.0788955,"Missing"
2021.emnlp-main.487,S13-2046,0,0.0339787,"es for each pair, comparing rows and columns. dataset are the best results of all runs as the benchTable 1 shows that all the data augmentation con- mark model presented in their paper. ditions are significantly better than org (p-values 6.1 SemEval-2013 Experiments ≤ 0.05). The condition org+ch+fr has the best On SemEval-2013, we compare SFRN and SFRN+ absolute improvement over org, with a difference with eight baselines: 1) LO (Dzikovska et al., in average performance of 7.92. We concluded that 2013), a model based on lexical overlap; 2) ETS back-translation is useful for data augmentation (Heilman and Madnani, 2013) and 3) CoMeT (Ott and re-balancing. et al., 2013), both of which use handcrafted feaUsing the best performing augmentation (org+ch+fr), we created new datasets, Beetle+ tures; 4) TF+SF (Saha et al., 2018), which com2 and SciEntsBank+. Beetle+ (N=12,438) is thrice Our ASAG code: https://github.com/psunlpgroup/ASAG 6035 Task 2 Way 3 Way 5 Way Method LO ETS CoMeT LR RN SFRN LR+ RN+ SFRN+ LO ETS CoMeT LR RN SFRN LR+ RN+ SFRN+ LO ETS CoMeT LR RN SFRN LR+ RN+ SFRN+ Acc 79 81 83 73 75 81 82 84 89 60 63 73 67 69 70 72 73 78 51 71 68 60 55 65 67 69 75 UA M-F1 78 80 83 71 74 80 82 84 89 55 59 71 60 61"
2021.emnlp-main.487,2020.findings-emnlp.372,0,0.0167087,"t al. (2018) developed FiLM, an approach to merge information from language and visual input. A language vector serves as conditioning input, to control the scaling and shifting of the visual feature map in a featurewise fashion. Similarly, we train a function with learnable parameters to combine QRA triples for a given student answer. We have not seen feature wise transformation used for vector combination in ASAG research, which typically relies instead on fixed arithmetic operations or concatenation. 2015; Wei and Zou, 2019), or to use word embeddings to find synonyms (Wang and Yang, 2015; Jiao et al., 2020). Back-translation leverages machine translation to paraphrase a text while retaining the meaning (Wieting et al., 2017; Edunov et al., 2018; Xie et al., 2020). We adopt back-translation for its ease of use, given that machine translation methods have achieved very high performance. 3 SFRN A Relation Network (RN) (Santoro et al., 2017) is particularly suitable for ASAG because it is designed to infer higher order generalizations, meaning generalizations that hold across tuples of examples, in a data efficient manner. RNs have been used in vision to efficiently learn generalizations across pair"
2021.emnlp-main.487,W19-8672,0,0.015061,"essential meaning. In general, data augmentation is used both to increase the size of the training data and to add irrelevant noise to examples to improve the 3.1 Creating the QRA Relation Vectors robustness of learned models. Recently, data augThe encoded vectors for a given triple are mentation has been found to significantly improve (q, rj , a), j ∈ {0, 1, . . . , n}, where n is the numperformance on NLP tasks as various as paraphrasber of reference answers for a given question q and ing (Wieting et al., 2017), natural language gena student answer a of this question. Corresponding eration (Kedzie and McKeown, 2019), semantic to equation (1) above, the relation vectors lj are parsing (Cao et al., 2019), or various sentiment inferred by gθ , using the concatenation of QRA and opinion classification tasks (Kobayashi, 2018). vectors from the encoding step as the input (left One method is to substitute a random word with a hashed box in Figure 2): synonym drawn from a lexical database like WordNet (Mueller and Thyagarajan, 2016; Zhang et al., lj = gθ ([q, rj , a]) (2) 6032 Figure 2: The structure of SFRN. The gθ -MLP function computes the relation vector for each [Q,R,A] triple. A set of relation vectors is"
2021.emnlp-main.487,N18-2072,0,0.0172529,"models. Recently, data augThe encoded vectors for a given triple are mentation has been found to significantly improve (q, rj , a), j ∈ {0, 1, . . . , n}, where n is the numperformance on NLP tasks as various as paraphrasber of reference answers for a given question q and ing (Wieting et al., 2017), natural language gena student answer a of this question. Corresponding eration (Kedzie and McKeown, 2019), semantic to equation (1) above, the relation vectors lj are parsing (Cao et al., 2019), or various sentiment inferred by gθ , using the concatenation of QRA and opinion classification tasks (Kobayashi, 2018). vectors from the encoding step as the input (left One method is to substitute a random word with a hashed box in Figure 2): synonym drawn from a lexical database like WordNet (Mueller and Thyagarajan, 2016; Zhang et al., lj = gθ ([q, rj , a]) (2) 6032 Figure 2: The structure of SFRN. The gθ -MLP function computes the relation vector for each [Q,R,A] triple. A set of relation vectors is combined (+) using SF T . The fφ -MLP function is the assessment classifier. where g is a multilayer perceptron (MLP) with learnable parameters θ, and g produces the relation vectors lj ∈ L for j ∈ {0, 1, . ."
2021.emnlp-main.487,P11-1076,0,0.0879752,"Missing"
2021.emnlp-main.487,E09-1065,0,0.126559,"Missing"
2021.emnlp-main.487,S13-2102,0,0.0626931,"Missing"
2021.emnlp-main.487,D14-1162,0,0.086071,"Missing"
2021.emnlp-main.487,W15-0612,0,0.0615958,"Missing"
2021.emnlp-main.487,W17-5017,0,0.0255182,"Missing"
2021.emnlp-main.487,N16-1123,0,0.0342076,"Missing"
2021.emnlp-main.487,D15-1306,0,0.0214912,"ges and text. Perez et al. (2018) developed FiLM, an approach to merge information from language and visual input. A language vector serves as conditioning input, to control the scaling and shifting of the visual feature map in a featurewise fashion. Similarly, we train a function with learnable parameters to combine QRA triples for a given student answer. We have not seen feature wise transformation used for vector combination in ASAG research, which typically relies instead on fixed arithmetic operations or concatenation. 2015; Wei and Zou, 2019), or to use word embeddings to find synonyms (Wang and Yang, 2015; Jiao et al., 2020). Back-translation leverages machine translation to paraphrase a text while retaining the meaning (Wieting et al., 2017; Edunov et al., 2018; Xie et al., 2020). We adopt back-translation for its ease of use, given that machine translation methods have achieved very high performance. 3 SFRN A Relation Network (RN) (Santoro et al., 2017) is particularly suitable for ASAG because it is designed to infer higher order generalizations, meaning generalizations that hold across tuples of examples, in a data efficient manner. RNs have been used in vision to efficiently learn general"
2021.emnlp-main.487,D19-1670,0,0.0158348,"formation, based on its success with multi-modal data, e.g., images and text. Perez et al. (2018) developed FiLM, an approach to merge information from language and visual input. A language vector serves as conditioning input, to control the scaling and shifting of the visual feature map in a featurewise fashion. Similarly, we train a function with learnable parameters to combine QRA triples for a given student answer. We have not seen feature wise transformation used for vector combination in ASAG research, which typically relies instead on fixed arithmetic operations or concatenation. 2015; Wei and Zou, 2019), or to use word embeddings to find synonyms (Wang and Yang, 2015; Jiao et al., 2020). Back-translation leverages machine translation to paraphrase a text while retaining the meaning (Wieting et al., 2017; Edunov et al., 2018; Xie et al., 2020). We adopt back-translation for its ease of use, given that machine translation methods have achieved very high performance. 3 SFRN A Relation Network (RN) (Santoro et al., 2017) is particularly suitable for ASAG because it is designed to infer higher order generalizations, meaning generalizations that hold across tuples of examples, in a data efficient"
2021.emnlp-main.487,D17-1026,0,0.07249,"leveraging the attentions calculated by a works have been used on large proprietary datasets QRA triple. Finally, a classifier determines which (Wang et al., 2019; Liu et al., 2019; Ha et al., 2020). class the student answer belongs to. Süzen et al. (2020) used text mining to improve To address data insufficiency and class imbal- similarity results. Contextualized semantic repreance, we adopt a simple data augmentation method, sentations like BERT have also been used (Hasback-translation, which was studied for augmen- san et al., 2018; Sung et al., 2019; Camus and Fitation of paraphrase data (Wieting et al., 2017), lighera, 2020). Saha et al. (2018) leveraged both and has been used in other NLP problems. Most hand-crafted features and sentence embeddings to ASAG datasets are relatively small, especially com- achieve high performances on many tasks. 6031 Relation Networks (RN) originated as an alternative to other kinds of graph-based neural models to develop relation-based representations, and were designed to overcome the limitations of CNNs and MLPs for reasoning problems in vision, NLP and symbolic domains such as physics (Santoro et al., 2017). RN performance on a visual question answering dataset"
2021.emnlp-main.487,D19-6119,0,0.0598455,"vironments and educational technology, the potential impact of ASAG has grown. The most common ASAG approach classifies students’ answers into two or more categories. The key challenge is that the classification problem is inherently relational, involving the relation of the student’s answer to the question, as well as to one or more reference answers. Another challenge is that existing datasets are relatively small, especially for the kinds of neural network models that perform best on other NLP tasks. Much of the ASAG work is conducted in industry labs with proprietary methods and datasets (Wang et al., 2019; Liu et al., 2019). Creation of benchmark datasets, however, has fostered broader interest in the problem from the NLP community. The main benchmark dataset, SemEval-2013 (Dzikovska et al., 2013), covers multiple STEM domains, and has multiple classification tasks regarding both the number of classes, and the level of generalization required. For example, one task addresses unseen answers to known questions (UA), another addresses unseen questions (UQ) within the same subject domain, and a third is unseen domains (UD) for student answers to topics and questions not seen in the training data."
2021.textgraphs-1.6,D18-1368,0,0.024537,"oal is to generate clause embeddings specifically for connective prediction, rather than universal sentence representation. 4 DAnCE Architecture The input to DAnCE is a graph for each simple sentence that includes syntactic information from a phrase structure parse to identify the VP, and from a dependency parse to identify the grammatical subject, and dependencies within the VP. Related Work Much research has addressed ways to learn high quality clause representations. Xu et al. (2015) propose a shortest dependency path LSTM for sentence representation in the task of relation classification. Dai and Huang (2018) propose a BiLSTM based model that combines paragraph vectors and word vectors into clause embeddings for a situation entity classification task. Connective prediction has often been addressed: Ji and Eisenstein (2015) and Rutherford et al. (2017) use recursive neural networks with parse trees as input to predict connectives and discourse relations, with solid improvements on PDTB. Malmi et al. (2018) use a decomposable attention model to predict connectives on sentences pairs extracted from Wikipedia. Our work draws on the idea of incorporating syntax into representation for connective predic"
2021.textgraphs-1.6,P13-1167,0,0.486084,"Missing"
2021.textgraphs-1.6,D15-1278,0,0.0219577,"dding), a novel neural architecture that exploits bi-LSTMs at the lower layers for learning inter-word influences, and graph learning of relational structure encoded in the dependency-anchor graph; 3) two datasets for carefully edited versus student text. Our approach outperforms the state-of-the-art on connective prediction. 2 Motivation The question of whether latent representations of sentence meaning can benefit from syntax has been addressed in work that compares recurrence and recursion, and finds the main benefit of recursive models to be better treatment of long-distance dependencies (Li et al., 2015). Two recent works compare tree-based models derived from dependency parses with constituency parses on semantic relatedness tasks, with no clear advantage of one grammar formalism over the other (Tai et al., 2015; Ahmed et al., 2019). As discussed in (Tai et al., 2015), dependency trees provide a more compact structure than constituency trees, through shorter paths from the root to leaf words. Further, all of a verb’s arguments are its direct dependents. The recursive structure of constituency trees, on the other hand, facilitates identification of subtrees that span more of the leaf words as"
2021.textgraphs-1.6,L18-1260,0,0.10976,"e clauses Pm and Qn , and whether Introduction The clause is a fundamental unit in coherent text. Much work in NLP investigates how clauses combine to form larger units, ultimately spanning a whole discourse (Wang et al., 2017; Ji and Eisenstein, 2014); how to decompose complex sentences into distinct propositions (Wang et al., 2018; Li et al., 2018; Narayan et al., 2017); how to identify explicit or implicit semantic relations between clauses (Lee and Goldwasser, 2019; Rutherford and Xue, 2015), or how to select a connective to link multiple clauses into a complex sentence (Nie et al., 2019; Malmi et al., 2018). In this paper, we 54 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 54–66 June 11, 2021. ©2021 Association for Computational Linguistics a connective is appropriate at all, depends on the main verbs and their arguments. Use of although requires only some dimension of contrast between the joined clauses, while because requires that Qn be a precondition for Pm . The table lists two variants of Pm with cook as the main verb, one with three distinct entities (Bob, Tia, a burger) and one with two (Bob, burger). These are conside"
2021.textgraphs-1.6,P14-1002,0,0.0329931,"ccur in both clauses, particularly the grammatical subject. There are a large number of connectives and connective phrases in English; e.g., the Penn Discourse Tree Bank (Prasad et al., 2008) has 141. Here we illustrate the nature of the problem with respect to the two connectives, although and because. Figure 1 illustrates how the choice of connective to join two simple clauses Pm and Qn , and whether Introduction The clause is a fundamental unit in coherent text. Much work in NLP investigates how clauses combine to form larger units, ultimately spanning a whole discourse (Wang et al., 2017; Ji and Eisenstein, 2014); how to decompose complex sentences into distinct propositions (Wang et al., 2018; Li et al., 2018; Narayan et al., 2017); how to identify explicit or implicit semantic relations between clauses (Lee and Goldwasser, 2019; Rutherford and Xue, 2015), or how to select a connective to link multiple clauses into a complex sentence (Nie et al., 2019; Malmi et al., 2018). In this paper, we 54 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 54–66 June 11, 2021. ©2021 Association for Computational Linguistics a connective is appropria"
2021.textgraphs-1.6,D17-1159,0,0.0283954,"n difference between the two settings is that FA treats the anchor as a sequence of words with their BiLSTM hidden states hi , and ignores the dependency relations within the anchor. GA turns the dependencies G into an adjacency matrix and then generates hA i as the anchor node representation by encoding the BiLSTM hidden states within the matrix through graph attention (GAT) (Veli˘ckovi´c et al., 2018). GAT will attend to whatever nodes are within the anchor, thus it fits well for learning the anchor representation for any length anchor. We first explain G the derivation of hA i . Following (Marcheggiani and Titov, 2017), we treat the dependency arcs within the anchor as directed. Given the latent representations of a pair of nodes within the anchor hi , hj , and a one-hot j∈NA (i) Again, there are two alternative settings to generate the anchor embedding. We use maxpool over all the nodes in anchor NA : ( M axpool(||i∈NA hi ) if FA A h = (5) AG M axpool(||i∈NA hi ) if GA The third layer applies graph convolution (GCN) to the subject hidden states from BiLSTM and subject and anchor nodes, where the subject node is the hidden state from the BiLSTM and the anchor node is the anchor embedding hA . Given a node i"
2021.textgraphs-1.6,Q15-1024,0,0.0200766,"syntactic information from a phrase structure parse to identify the VP, and from a dependency parse to identify the grammatical subject, and dependencies within the VP. Related Work Much research has addressed ways to learn high quality clause representations. Xu et al. (2015) propose a shortest dependency path LSTM for sentence representation in the task of relation classification. Dai and Huang (2018) propose a BiLSTM based model that combines paragraph vectors and word vectors into clause embeddings for a situation entity classification task. Connective prediction has often been addressed: Ji and Eisenstein (2015) and Rutherford et al. (2017) use recursive neural networks with parse trees as input to predict connectives and discourse relations, with solid improvements on PDTB. Malmi et al. (2018) use a decomposable attention model to predict connectives on sentences pairs extracted from Wikipedia. Our work draws on the idea of incorporating syntax into representation for connective prediction, 4.1 Dependency-Anchor graph The anchor VP and its subject serve as nodes in a graph, as illustrated in Figure 2. The Stanford CoreNLP dependency grammar has 58 dependency relations, eight of which are a type of s"
2021.textgraphs-1.6,2020.lrec-1.127,0,0.167533,"to allow for contrast or causation. Four cases allow although. The two cases that allow because have a strong semantic relation between the predicates (make someone a burger, be hungry) and, there is no conflict in the to-object of the first clause and the subject of the second. The remaining two cases have no commonality, and neither connective can occur. focus on clause representation to support accurate connective prediction, a task which is important for coherence modeling (Pishdad et al., 2020), finegrained opinion mining (Wiegand et al., 2015), argument mining (Kuribayashi et al., 2019; Jo et al., 2020) and argumentation (Park and Cardie, 2014). We present a case for a model that learns from a novel graph we refer to as a dependency-anchor graph, which retains information from dependency parses and constituency parses of input sentences that is critical for identification of the core proposition of a clause, while omitting structural information that is less relevant. We assume that determining whether two clauses can be joined by a connective, and what connective to choose, depends primarily on the main verb in each clause, and on the arguments that occur in both clauses, particularly the g"
2021.textgraphs-1.6,D17-1064,0,0.15584,"English; e.g., the Penn Discourse Tree Bank (Prasad et al., 2008) has 141. Here we illustrate the nature of the problem with respect to the two connectives, although and because. Figure 1 illustrates how the choice of connective to join two simple clauses Pm and Qn , and whether Introduction The clause is a fundamental unit in coherent text. Much work in NLP investigates how clauses combine to form larger units, ultimately spanning a whole discourse (Wang et al., 2017; Ji and Eisenstein, 2014); how to decompose complex sentences into distinct propositions (Wang et al., 2018; Li et al., 2018; Narayan et al., 2017); how to identify explicit or implicit semantic relations between clauses (Lee and Goldwasser, 2019; Rutherford and Xue, 2015), or how to select a connective to link multiple clauses into a complex sentence (Nie et al., 2019; Malmi et al., 2018). In this paper, we 54 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 54–66 June 11, 2021. ©2021 Association for Computational Linguistics a connective is appropriate at all, depends on the main verbs and their arguments. Use of although requires only some dimension of contrast between"
2021.textgraphs-1.6,D18-1208,0,0.0431052,"Missing"
2021.textgraphs-1.6,N16-3002,0,0.0187541,"Missing"
2021.textgraphs-1.6,P19-1442,0,0.30701,"to join two simple clauses Pm and Qn , and whether Introduction The clause is a fundamental unit in coherent text. Much work in NLP investigates how clauses combine to form larger units, ultimately spanning a whole discourse (Wang et al., 2017; Ji and Eisenstein, 2014); how to decompose complex sentences into distinct propositions (Wang et al., 2018; Li et al., 2018; Narayan et al., 2017); how to identify explicit or implicit semantic relations between clauses (Lee and Goldwasser, 2019; Rutherford and Xue, 2015), or how to select a connective to link multiple clauses into a complex sentence (Nie et al., 2019; Malmi et al., 2018). In this paper, we 54 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 54–66 June 11, 2021. ©2021 Association for Computational Linguistics a connective is appropriate at all, depends on the main verbs and their arguments. Use of although requires only some dimension of contrast between the joined clauses, while because requires that Qn be a precondition for Pm . The table lists two variants of Pm with cook as the main verb, one with three distinct entities (Bob, Tia, a burger) and one with two (Bob, burge"
2021.textgraphs-1.6,P15-1150,0,0.395994,"for carefully edited versus student text. Our approach outperforms the state-of-the-art on connective prediction. 2 Motivation The question of whether latent representations of sentence meaning can benefit from syntax has been addressed in work that compares recurrence and recursion, and finds the main benefit of recursive models to be better treatment of long-distance dependencies (Li et al., 2015). Two recent works compare tree-based models derived from dependency parses with constituency parses on semantic relatedness tasks, with no clear advantage of one grammar formalism over the other (Tai et al., 2015; Ahmed et al., 2019). As discussed in (Tai et al., 2015), dependency trees provide a more compact structure than constituency trees, through shorter paths from the root to leaf words. Further, all of a verb’s arguments are its direct dependents. The recursive structure of constituency trees, on the other hand, facilitates identification of subtrees that span more of the leaf words as one moves up the tree, and that have a compositional contribution to the meaning of the sentence. Tree-based models take input from syntactic parses and compose the latent vectors through a uni-directional traver"
2021.textgraphs-1.6,P19-1459,0,0.15512,"Missing"
2021.textgraphs-1.6,W14-2105,0,0.0146208,"Four cases allow although. The two cases that allow because have a strong semantic relation between the predicates (make someone a burger, be hungry) and, there is no conflict in the to-object of the first clause and the subject of the second. The remaining two cases have no commonality, and neither connective can occur. focus on clause representation to support accurate connective prediction, a task which is important for coherence modeling (Pishdad et al., 2020), finegrained opinion mining (Wiegand et al., 2015), argument mining (Kuribayashi et al., 2019; Jo et al., 2020) and argumentation (Park and Cardie, 2014). We present a case for a model that learns from a novel graph we refer to as a dependency-anchor graph, which retains information from dependency parses and constituency parses of input sentences that is critical for identification of the core proposition of a clause, while omitting structural information that is less relevant. We assume that determining whether two clauses can be joined by a connective, and what connective to choose, depends primarily on the main verb in each clause, and on the arguments that occur in both clauses, particularly the grammatical subject. There are a large numb"
2021.textgraphs-1.6,D14-1162,0,0.0885041,"on from a dependency-anchor graph, DAnCE has the three layers illustrated in Figure 3. An initial embedding lookup layer retrieves word embeddings. A BiLSTM layer captures the hidden states over the input words at each time step. Finally, a graph convolution layer takes the subject word representation from the BiLSTM (the S node in Figure 3), and an anchor embedding that is generated from an separate module (the A node in Figure 3), to produce the final learned semantic representation. The input sequence of words xi ∈ X is first fed into a pre-trained word embedding lookup layer, using GloVe (Pennington et al., 2014), with a bidirectional LSTM of dimension 2D, where D is the dimension of hidden states in the BiLSTM: hi = f (xi , hi−1 ), hi ∈ R2D ei,j = a(W h hi , W d [hj ||arci,j ]) (2) where ||is the concatenation operation, and a, W h , W d are learned parameters for the head and the dependent. Then we apply softmax and a Leaky ReLU activation to normalize the attention weights: αi.j = LeakyReLU ( P exp(ei,j ) ) (3) m∈NA (i) exp(ei,m ) where NA (i) represents all nodes in the anchor that are linked to i, including itself. Leaky Relu activation on ei,j enables the network to learn the importance of node"
2021.textgraphs-1.6,P17-2029,0,0.0181372,"he arguments that occur in both clauses, particularly the grammatical subject. There are a large number of connectives and connective phrases in English; e.g., the Penn Discourse Tree Bank (Prasad et al., 2008) has 141. Here we illustrate the nature of the problem with respect to the two connectives, although and because. Figure 1 illustrates how the choice of connective to join two simple clauses Pm and Qn , and whether Introduction The clause is a fundamental unit in coherent text. Much work in NLP investigates how clauses combine to form larger units, ultimately spanning a whole discourse (Wang et al., 2017; Ji and Eisenstein, 2014); how to decompose complex sentences into distinct propositions (Wang et al., 2018; Li et al., 2018; Narayan et al., 2017); how to identify explicit or implicit semantic relations between clauses (Lee and Goldwasser, 2019; Rutherford and Xue, 2015), or how to select a connective to link multiple clauses into a complex sentence (Nie et al., 2019; Malmi et al., 2018). In this paper, we 54 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 54–66 June 11, 2021. ©2021 Association for Computational Linguistics"
2021.textgraphs-1.6,2020.coling-main.539,0,0.130756,"1: For the propositions Pm , Qn to be joined by although or because, Pm and Qn . must have some semantic commonality to allow for contrast or causation. Four cases allow although. The two cases that allow because have a strong semantic relation between the predicates (make someone a burger, be hungry) and, there is no conflict in the to-object of the first clause and the subject of the second. The remaining two cases have no commonality, and neither connective can occur. focus on clause representation to support accurate connective prediction, a task which is important for coherence modeling (Pishdad et al., 2020), finegrained opinion mining (Wiegand et al., 2015), argument mining (Kuribayashi et al., 2019; Jo et al., 2020) and argumentation (Park and Cardie, 2014). We present a case for a model that learns from a novel graph we refer to as a dependency-anchor graph, which retains information from dependency parses and constituency parses of input sentences that is critical for identification of the core proposition of a clause, while omitting structural information that is less relevant. We assume that determining whether two clauses can be joined by a connective, and what connective to choose, depend"
2021.textgraphs-1.6,D18-1116,0,0.241585,"Missing"
2021.textgraphs-1.6,prasad-etal-2008-penn,0,0.365444,"y-anchor graph, which retains information from dependency parses and constituency parses of input sentences that is critical for identification of the core proposition of a clause, while omitting structural information that is less relevant. We assume that determining whether two clauses can be joined by a connective, and what connective to choose, depends primarily on the main verb in each clause, and on the arguments that occur in both clauses, particularly the grammatical subject. There are a large number of connectives and connective phrases in English; e.g., the Penn Discourse Tree Bank (Prasad et al., 2008) has 141. Here we illustrate the nature of the problem with respect to the two connectives, although and because. Figure 1 illustrates how the choice of connective to join two simple clauses Pm and Qn , and whether Introduction The clause is a fundamental unit in coherent text. Much work in NLP investigates how clauses combine to form larger units, ultimately spanning a whole discourse (Wang et al., 2017; Ji and Eisenstein, 2014); how to decompose complex sentences into distinct propositions (Wang et al., 2018; Li et al., 2018; Narayan et al., 2017); how to identify explicit or implicit semant"
2021.textgraphs-1.6,W98-0315,0,0.634018,"oftmax over the classification categories. Experiments on Book-Simpl predict the correct connective, given positive examples of clause pairs. Experiments on DeSSE predict the correct connective, given positive and negative examples. We compare DAnCE with four baselines on both datasets, reporting accuracy and F1. Student writing is much less coherent than much of the text that applies NLP to tasks related to discourse structure, 3 The interface checked for connectives remaining in step two to warn annotators. Details about the interface and quality control are included in appendix B. 2 As in (Webber and Joshi, 1998), we take connectives to be predicates whose arguments are the clauses they join. 59 Group Model Book-Simpl 5 Book-Simpl 8 DeSSE 5 DeSSE 8 Acc. (σ) F1 Acc. (σ) F1 Acc. (σ) F1 Acc. (σ) F1 BoW CNN 61.89 (1.64) 49.70 46.31 (1.36) 30.62 53.57 (0.27) 17.70 42.95 (0.22) 9.18 SeqLSTM DisSent 68.58 (1.55) 58.78 62.92 (1.39) 48.11 48.93 (0.31) 25.27 39.86 (0.30) 15.91 Tree Tree 67.95 (1.10) 59.67 59.69 (1.58) 45.71 20.35 (0.74) 9.84 16.63 (0.77) 9.29 LSTM Tr-Attn 69.08 (0.82) 62.30 63.48 (1.40) 49.06 18.51 (0.10) 8.68 17.95 (0.72) 12.01 DAnCE FA 71.83 (0.45) 63.59 65.60 (0.55) 51.26 52.64 (0.38) 22.29"
2021.textgraphs-1.6,E17-1027,0,0.0123061,"phrase structure parse to identify the VP, and from a dependency parse to identify the grammatical subject, and dependencies within the VP. Related Work Much research has addressed ways to learn high quality clause representations. Xu et al. (2015) propose a shortest dependency path LSTM for sentence representation in the task of relation classification. Dai and Huang (2018) propose a BiLSTM based model that combines paragraph vectors and word vectors into clause embeddings for a situation entity classification task. Connective prediction has often been addressed: Ji and Eisenstein (2015) and Rutherford et al. (2017) use recursive neural networks with parse trees as input to predict connectives and discourse relations, with solid improvements on PDTB. Malmi et al. (2018) use a decomposable attention model to predict connectives on sentences pairs extracted from Wikipedia. Our work draws on the idea of incorporating syntax into representation for connective prediction, 4.1 Dependency-Anchor graph The anchor VP and its subject serve as nodes in a graph, as illustrated in Figure 2. The Stanford CoreNLP dependency grammar has 58 dependency relations, eight of which are a type of subject (Van Valin, 2001; Schu"
2021.textgraphs-1.6,J16-4006,0,0.0200993,"her syntactic relations (e.g., involving words in the subject phrase or adverbial phrases), are ignored. To encode the graph, we propose DAnCE, which applies graph convolution (GCN) (Kipf and Welling, 2017) to encode the arc between the subject and verb phrase. The input to the graph convolution comes from a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) that encodes all the input tokens, including the words outside the subject and verb phrase. The interaction that DAnCE captures between subject and verb phrase has been essential in word representation but missing in tree based models (Weir et al., 2016; White et al., 2018). We demonstrate the effectiveness of the dependency-anchor graph and DAnCE architecture through its superior performance over baselines, including tree-based models. The rest of the paper is organized as follows: we first present related work and give a detailed discussion of the DependencyAnchor Graph and DAnCE. Then we present the datasets, experiments and discussion. 3 Figure 2: A dependency-anchor graph for a clause (top right) is constructed from its phrase-structure parse (top left) and dependency parse (bottom). Words spanning the VP subtree of the constituency par"
2021.textgraphs-1.6,D18-1501,0,0.054798,"Missing"
2021.textgraphs-1.6,N15-1081,0,0.0182072,"ith respect to the two connectives, although and because. Figure 1 illustrates how the choice of connective to join two simple clauses Pm and Qn , and whether Introduction The clause is a fundamental unit in coherent text. Much work in NLP investigates how clauses combine to form larger units, ultimately spanning a whole discourse (Wang et al., 2017; Ji and Eisenstein, 2014); how to decompose complex sentences into distinct propositions (Wang et al., 2018; Li et al., 2018; Narayan et al., 2017); how to identify explicit or implicit semantic relations between clauses (Lee and Goldwasser, 2019; Rutherford and Xue, 2015), or how to select a connective to link multiple clauses into a complex sentence (Nie et al., 2019; Malmi et al., 2018). In this paper, we 54 Proceedings of the Fifteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-15), pages 54–66 June 11, 2021. ©2021 Association for Computational Linguistics a connective is appropriate at all, depends on the main verbs and their arguments. Use of although requires only some dimension of contrast between the joined clauses, while because requires that Qn be a precondition for Pm . The table lists two variants of Pm with cook as"
2021.textgraphs-1.6,W15-2921,0,0.0143475,"hough or because, Pm and Qn . must have some semantic commonality to allow for contrast or causation. Four cases allow although. The two cases that allow because have a strong semantic relation between the predicates (make someone a burger, be hungry) and, there is no conflict in the to-object of the first clause and the subject of the second. The remaining two cases have no commonality, and neither connective can occur. focus on clause representation to support accurate connective prediction, a task which is important for coherence modeling (Pishdad et al., 2020), finegrained opinion mining (Wiegand et al., 2015), argument mining (Kuribayashi et al., 2019; Jo et al., 2020) and argumentation (Park and Cardie, 2014). We present a case for a model that learns from a novel graph we refer to as a dependency-anchor graph, which retains information from dependency parses and constituency parses of input sentences that is critical for identification of the core proposition of a clause, while omitting structural information that is less relevant. We assume that determining whether two clauses can be joined by a connective, and what connective to choose, depends primarily on the main verb in each clause, and on"
2021.textgraphs-1.6,L16-1376,0,0.0446275,"Missing"
C14-1054,N03-2003,0,0.0539806,"of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 565 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers, pages 565–576, Dublin, Ireland, August 23-29 2014. While the utility of document characterization by genre for empirical language analysis is widely acknowledged, there is relatively little agreement on methodology. In part, this stems from the dif"
C14-1054,A00-2018,0,0.162856,"makes it possible to locate new documents in the defined space, it would be possible to identify which MASC documents a new set of documents is most similar to. PCA scores could be computed on the four dimensions for corresponding features in the new documents. This approach could be used in any application where it is desirable to find similar documents, such as retrieval, language modeling, or domain adaptation. For example, in recent work on domain adaptation of parsers, McClosky et al. (2010) present a confusion matrix with six corpora to demonstrate how performance of a Charniak parser (Charniak, 2000) varies depending on which corpus it is trained on. They assume that a new target domain will be a mixture of their six source domains and build a simple regression (three features) to predict which of the six parsers will perform best on a new corpus. They subsequently state that an alternative approach could use a high-dimensional vector space to compare corpora. Inspired by this suggestion, we are currently developing a web service that will allow researchers to locate their corpora in the 4-dimensional space identified in this study, and to compute the values of their PCA scores. This woul"
C14-1054,N09-2044,0,0.0168024,"er range of traditional genres as well as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage of MASC’s rich set of validated annotations to include features that would not have been (easily) available at the time of Biber’s study, and reconsider the use of some features used in his work. Some work on genre classification contrasts with Biber’s approach, which assumes that documents fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task (Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini, 2004), or combined with topic classification (Rauber and M¨uller-K¨ogler, 2001; Lee and Myaeng, 2002). All of these studies assume that documents fall discretely into distinct classes or clusters. These studies vary in their approach to determining the genre of text, either by using corpora with pre-defined classes (Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets in genre studies have remaine"
C14-1054,P10-2013,1,0.832494,"aim that the dimensions of variation he identified arise from underlying constraints on usage. We find three components similar to his, and a new one he did not find, based on our use of Named Entity features. We find that genres that are separable on one component are often co-extensive on another. To quantify the distinctiveness of each of the genres relative to the others, we use a metric that has previously been used to measure separability of classes. 2 Related work and motivation Our work builds on Biber’s 1988 study, but differs in the corpus and features used. Biber’s corpus and MASC (Ide et al., 2010), the corpus used in our study, differ in source language (British English versus American English), time coverage (skewed towards a single year versus three decades), and the situations of use. Biber’s corpus was drawn from the Lancaster-Oslo-Bergen (LOB) Corpus of British English, consisting of works published in 1961, the London-Lund corpus of spoken English, consisting of 87 texts of British English from private conversation, public interviews and panel discussions, telephone conversations, radio broadcasts, spontaneous speeches and prepared speeches produced in the 1970s. To these Biber a"
C14-1054,C94-2174,0,0.814604,"resents a set of shared regularities among written or spoken documents that enables readers, writers, listeners and speakers to signal discourse function, and that conditions their expectations of linguistic form. Genre distinctions are therefore an important aspect of language use and understanding. They clearly have a role to play in statistical language processing, which relies on regularities of form as well as content. Indeed, with the advent of the Web, statistical methods for genre differentiation have been applied to information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match do"
C14-1054,P97-1005,0,0.632684,"Missing"
C14-1054,W00-1326,0,0.0773224,"thods for genre differentiation have been applied to information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/"
C14-1054,P02-1040,0,0.0950055,"Missing"
C14-1054,D08-1093,0,0.201795,"istical language processing, which relies on regularities of form as well as content. Indeed, with the advent of the Web, statistical methods for genre differentiation have been applied to information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 Int"
C14-1054,sharoff-etal-2010-web,0,0.155756,"Missing"
C14-1054,C00-2117,0,0.134367,"l genres as well as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage of MASC’s rich set of validated annotations to include features that would not have been (easily) available at the time of Biber’s study, and reconsider the use of some features used in his work. Some work on genre classification contrasts with Biber’s approach, which assumes that documents fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task (Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini, 2004), or combined with topic classification (Rauber and M¨uller-K¨ogler, 2001; Lee and Myaeng, 2002). All of these studies assume that documents fall discretely into distinct classes or clusters. These studies vary in their approach to determining the genre of text, either by using corpora with pre-defined classes (Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets in genre studies have remained rather stable over the"
C14-1054,J00-4001,0,0.0976731,"l genres as well as new social media (email, blogs, twitter) and collectively generated fiction (ficlets). We take advantage of MASC’s rich set of validated annotations to include features that would not have been (easily) available at the time of Biber’s study, and reconsider the use of some features used in his work. Some work on genre classification contrasts with Biber’s approach, which assumes that documents fall discretely into distinct classes or clusters. Genre classification has been treated as a standalone task (Karlgren and Cutting, 1994; Kessler et al., 1997; Feldman et al., 2009; Stamatatos et al., 2000a; Santini, 2004), or combined with topic classification (Rauber and M¨uller-K¨ogler, 2001; Lee and Myaeng, 2002). All of these studies assume that documents fall discretely into distinct classes or clusters. These studies vary in their approach to determining the genre of text, either by using corpora with pre-defined classes (Karlgren and Cutting, 1994), manually refining pre-existing classes (Kessler et al., 1997), creating genre classes using annotators, or locating a priori classifications (e.g., web product reviews). The feature sets in genre studies have remained rather stable over the"
C14-1054,2012.amta-papers.18,0,0.0198145,"information retrieval to limit search criteria and organize results (Karlgren and Cutting, 1994; Kessler et al., 1997; Mehler et al., 2010; Ward and Werner, 2013), and the study of genres on the web has become a sub-field in its own right (see for example (Mehler et al., 2010)). More recently, the development of genre-dependent models for a variety of natural language processing (NLP) tasks such as parsing (Ravi et al., 2008; McClosky et al., 2010; Roux et al., 2012), speech recognition (Iyer and Ostendorf, 1999), word sense disambiguation (Martinez and Agirre, 2000), and machine translation (Wang et al., 2012) has been found to significantly improve performance. The ability to match documents by genre has also become important for collecting data to train language models for spoken language understanding, given the difficulty of creating large repositories of transcribed spoken language corpora (Bulyko and Ostendorf, 2003; Sarikaya et al., 2005). This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 565 Proceedings of COLING 2014, the 25th Int"
C14-1054,N10-1004,0,\N,Missing
D15-1261,W10-1808,0,0.0269288,"w et al. (2008) showed that application of the same model to noisy crowd annotations produced data of equal quality to five distinct published gold standards. Hovy et al. (2013) apply a simple and effective model to identify untrustworthy annotators and test it on the same datasets used in (Snow et al., 2008). As they point out, when ties occur among an even number of annotators, it’s necessary to resort to a tie-breaking procedure, e.g., for utterance 155 in Figure 1 where four annotators assign a positive label and four do not. In experiments on an existing dataset of word sense annotation, Dligach et al. (2010) compare singly annotated data with doubly annotated adjudicated data, using trained annotators. They find that with the same amount of data, machine learning performance improves with the doubly annotated adjudicated data by 2191 Figure 2: The annotation interface presented the audio control button on the upper left and the transcript below, with a scroll bar (not shown). Utterances from the two speakers are on the right and left sides, respectively. Each utterance had a checkbox; when selected, a textbox appeared to allow annotators to enter their segment descriptions. a small amount, but th"
D15-1261,W08-0125,0,0.0431797,"Missing"
D15-1261,W10-0713,0,0.47051,"Missing"
D15-1261,P03-1071,0,0.438722,"ics that test whether an annotator’s accuracy relative to a given model remains consistent across different conversations. 1 Introduction A single, spontaneous, spoken interaction can consist of multiple activities, such as to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008"
D15-1261,N03-2012,0,0.0640339,"such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation thro"
D15-1261,N13-1132,0,0.146621,"inaccurate annotator might be biased towards label m whenever the true label is z. Dawid and Skene (1979) present a joint model of true labels, observed labels, and annotator performance. Perhaps its first application to NLP data was the Bruce and Wiebe (1999) investigation of word sense. It has also been applied to more fine-grained word sense with a direct comparison to trained annotator labels in (Passonneau and Carpenter, 2014). Snow et al. (2008) showed that application of the same model to noisy crowd annotations produced data of equal quality to five distinct published gold standards. Hovy et al. (2013) apply a simple and effective model to identify untrustworthy annotators and test it on the same datasets used in (Snow et al., 2008). As they point out, when ties occur among an even number of annotators, it’s necessary to resort to a tie-breaking procedure, e.g., for utterance 155 in Figure 1 where four annotators assign a positive label and four do not. In experiments on an existing dataset of word sense annotation, Dligach et al. (2010) compare singly annotated data with doubly annotated adjudicated data, using trained annotators. They find that with the same amount of data, machine learni"
D15-1261,W09-3908,0,0.0230447,"that can be estimated from the observed labels. How well the estimated ground truth fits the data thus depends on how well the model assumptions accord with the phenomenon of interest. The models do not account for annotator differences in the level of granularity they apply; cf. the contrast between lumpers and splitters in taxonomic classification of the natural world (Branch, 2014). Further, neither model takes linguistic features into account that annotators consider in deciding on segments, such as speaker attitude towards utterance content or speaker role in the conversational activity (Niekrasz and Moore, 2009). We find, however, much agreement between the two models on the proposed segment boundaries, and leave for future work the question of whether more complex models could accoount for differences in granularity or utterance features. As discussed in section 2, we assume that annotators are not equally accurate, and that a probabilistic model based on the distribution of observed labels can do better than majority vote. Inspired by the type of probabilistic model proposed in (Dawid and Skene, 1979) and extended in (Bruce and Wiebe, 1999; Passonneau and Carpenter, 2014), annotator accuracy is a p"
D15-1261,Q14-1025,1,0.903916,"t al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation through application of a probabilistic model to crowdsourced labels, rather than reliance on interannotator agreement computed for a small number of trained annotators, are higher quality, lower cost, and a posterior probability for each ground truth label (Sheng et al., 2008; Snow et al., 2008; Passonneau and Carpenter, 2014). The latter serves as a confidence measure, which contrasts with interannotator agreement measures and with majority"
D15-1261,J97-1005,1,0.579144,"an annotator’s accuracy relative to a given model remains consistent across different conversations. 1 Introduction A single, spontaneous, spoken interaction can consist of multiple activities, such as to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Su"
D15-1261,W11-2038,1,0.819018,"97), seven annotators annotated narrative monologues for segments based on speaker intention. Agreement levels for ground truth boundaries were based on statistical significance using Cochran’s Q. In (Galley et al., 2003), three annotators segmented the ICSI meeting corpus into topical units, and majority agreement was taken as ground truth. A functional segmentation of meetings from the AMI multiparty meeting corpus based on involved participants was segmented by one annotator and finalized by a second annotator (Bokaei et al., 2015). Task-based segmentation of patron-librarian interactions (Passonneau et al., 2011) measured agreement among two annotators using Krippendorff’s Alpha at an average of 0.77 (Krippendorff, 1980). The annotation task here mostly closely resembles (Passonneau and Litman, 1997), and uses a similar number of annotators. No prior work, however, applies a probabilistic model to crowd labels for discourse segmentation. Estimation of ground truth from crowd labels has been applied to many tasks, but is especially useful where judgments are subjective, making ground truth difficult to arrive at. Application areas include disease prevalence estimation (Albert and Dodd, 2008), identific"
D15-1261,W14-4330,1,0.851477,"n a model fit to all but the held out data. To do so, we would extend the models with an additional parameter for the conversation, to account for the observation that while all conversations seem to fit the same empirical distribution, there are differences across conversations. 10 Conclusion Annotation and machine learning of discourse segmentation covers several types of units, including topical segments (Galley et al., 2003), meeting units in which action items are identified or decisions made (Purver et al., 2007; Fern´andez et al., 2008), transaction subtasks for ordering library books (Passonneau et al., 2014), or speaker involvement (Bokaei et al., 2015). This work relies on manual transcription, and draws on many sources of knowledge for machine learned models, including turn-taking, prosody, and linguistic features. The segmentation annotation can be linear (Galley et al., 2003; Bokaei et al., 2015; Passonneau and Litman, 1997; Passonneau et al., 2014) or hierarchical (Purver et al., 2007; Fern´andez et al., 2008; Passonneau et al., 2011). The differences in methods and results across this body of work, points to a need for more datasets for research on the organization of discourse into activit"
D15-1261,2007.sigdial-1.4,0,0.188671,"en resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation through application of a probabilistic model to crowdsourced labels, rather than reliance on interannotator agreem"
D15-1261,W12-1642,0,0.0206911,"le activities, such as to plan a future event, to complain about a past situation, or to carry out a transaction that might consist of subtasks. Speakers shift from one activity to the next with more or less awareness and explicit demarcation. To treat such conversational activities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples i"
D15-1261,D08-1027,0,0.794959,"Missing"
D15-1261,2007.sigdial-1.5,0,0.0202208,"ctivities as a sequence of discrete units is a convenient oversimplification that is often resorted to (Bokaei et al., 2015; Galley et al., 2003; Passonneau and Litman, 1997). Systems that provide automated access to spoken language data often rely on segmentation of spoken discourse into sequential units for summarization (Wang and Cardie, 2012; Dielmann and Renals, 2005) or information retrieval (Ward et al., 2015). Research on the organization of spoken discourse also relies directly or indirectly on identification of such units to detect agreement among participants (Hillard et al., 2003; Somasundaran et al., 2007; Germesin and Wilson, 2009), multiparty meeting action items (Purver et al., 2007), decisions (Fern´andez et al., 2008), or answers to questions (Sun and Chai, Crowdsourced annotation, where each item is labeled by a crowd of many independent annotators, is becoming more common in natural language processing. Examples include word sense (Bruce and Wiebe, 1999; Snow et al., 2008; Passonneau and Carpenter, 2014), named entities (Finin et al., 2010), and several other tasks in (Snow et al., 2008), including textual entailment. Three advantages to corpus annotation through application of a probab"
de-melo-etal-2012-empirical,passonneau-etal-2010-word,1,\N,Missing
de-melo-etal-2012-empirical,W09-1127,0,\N,Missing
de-melo-etal-2012-empirical,W09-3021,1,\N,Missing
gordon-passonneau-2010-evaluation,2005.sigdial-1.11,0,\N,Missing
gordon-passonneau-2010-evaluation,H94-1039,0,\N,Missing
gordon-passonneau-2010-evaluation,C08-1123,0,\N,Missing
gordon-passonneau-2010-evaluation,W09-0506,0,\N,Missing
gordon-passonneau-2010-evaluation,N09-2047,0,\N,Missing
gordon-passonneau-2010-evaluation,W07-0305,0,\N,Missing
gordon-passonneau-2010-evaluation,P03-1004,0,\N,Missing
gordon-passonneau-2010-evaluation,N10-1126,1,\N,Missing
gordon-passonneau-2010-evaluation,hahn-etal-2008-comparison,0,\N,Missing
gordon-passonneau-2010-evaluation,P02-1048,0,\N,Missing
H01-1015,J97-1002,0,0.0277592,"ndicating the system’s confusion about the destination. Note that the question within this pattern could easily be reformulated as a more typical instruction statement, such as Please specify which Springfield you mean, or Please say Missouri, Illinois or Ohio.. 3. THE SPEECH-ACT DIMENSION The SPEECH - ACT dimension characterizes the utterance’s communicative goal, and is motivated by the need to distinguish the communicative goal of an utterance from its form. As an example, consider the functional category of a REQUEST for information, found in many tagging schemes that annotate speech-acts [24, 18, 6]. Keeping the functional category of a REQUEST separate from the sentence modality distinction between question and statement makes it possible to capture the functional similarity between question and statement forms of requests, e.g., Can you tell me what time you would like to arrive? versus Please tell me what time you would like to arrive. In DATE, the speech-act dimension has ten categories. We use familiar speech-act labels, such as OFFER, REQUEST- INFO, PRESENTINFO , ACKNOWLEDGMENT , and introduce new ones designed to help us capture generalizations about communicative behavior in this"
H01-1015,P98-1052,0,0.0783931,"Missing"
H01-1015,J80-3003,0,0.346788,"cts requesting, or implicitly or explicitly confirming the date. A similar example is provided by the subtasks of CAR (rental) and HOTEL, which include dialogue acts requesting, confirming or acknowledging arrangements to rent a car or book a hotel room on the same trip. 1 This dimension is used as an elaboration of each speech-act type in other tagging schemes [24]. 2 It is tempting to also consider this dimension as a means of inferring discourse structure on the basis of utterance level labels, since it is widely believed that models of task structure drive the behavior of dialogue systems [23, 3, 22], and the relationship between discourse structure and task structure has been a core topic of research since Grosz’s thesis [15]. However, we leave the inference of discourse structure as a topic for future work because the multifunctionality of many utterances suggests that the correspondence between task structure and dialogue structure may not be as straightforward as has been proposed in Grosz’s work [30]. Task TOP - LEVEL TRIP ORIGIN DESTINATION DATE TIME AIRLINE TRIP - TYPE RETRIEVAL ITINERARY GROUND HOTEL CAR Example What are your travel plans? And, what city are you leaving from? And,"
H01-1015,C92-1054,1,0.826032,"are APOLO GIES that the system makes for misunderstandings (see Section 3 below), i.e. utterances such as I’m sorry. I’m having trouble understanding you., or My mistake again. I didn’t catch that. or I can see you are having some problems. The last category of ABOUT- COMMUNICATION utterances are the OPENINGS / CLOSINGS by which the system greets or says goodbye to the caller. (Again, see Section 3 below.) 2.2 About-Communication 2.3 About Situation-Frame The ABOUT- COMMUNICATION domain reflects the system goal of managing the verbal channel and providing evidence of what has been understood [29, 8, 25]. Although utterances of this type occur in human-human dialogue, they are more frequent in humancomputer dialogue, where they are motivated by the need to avoid potentially costly errors arising from imperfect speech recognition. In the COMMUNICATOR corpus, many systems use a conservative strategy of providing feedback indicating the system’s understanding of the information provided by the user after each user turn. A typical example is the repetition of the origin and destination cities in Figures 1 and 6. This type of repetition is the IMPLICIT- CONFIRMATION speech-act (see Section 3 below"
H01-1015,J96-2005,1,0.782684,"s a means of inferring discourse structure on the basis of utterance level labels, since it is widely believed that models of task structure drive the behavior of dialogue systems [23, 3, 22], and the relationship between discourse structure and task structure has been a core topic of research since Grosz’s thesis [15]. However, we leave the inference of discourse structure as a topic for future work because the multifunctionality of many utterances suggests that the correspondence between task structure and dialogue structure may not be as straightforward as has been proposed in Grosz’s work [30]. Task TOP - LEVEL TRIP ORIGIN DESTINATION DATE TIME AIRLINE TRIP - TYPE RETRIEVAL ITINERARY GROUND HOTEL CAR Example What are your travel plans? And, what city are you leaving from? And, where are you flying to? What day would you like to leave? Departing at what time?. Did you have an airline preference? Will you return to Boston from San Jose? Accessing the database; this might take a few seconds. The airfare for this trip is 390 dollars. Did you need to make any ground arrangements?. Would you like a hotel near downtown or near the airport?. Do you need a car in San Jose? Figure 5: Example"
H01-1015,P95-1016,0,\N,Missing
ide-etal-2008-masc,W04-0803,0,\N,Missing
ide-etal-2008-masc,W04-0811,0,\N,Missing
ide-etal-2008-masc,passonneau-etal-2006-inter,1,\N,Missing
ide-etal-2008-masc,S07-1018,1,\N,Missing
ide-etal-2008-masc,W07-1501,1,\N,Missing
ide-etal-2008-masc,S07-1048,0,\N,Missing
ide-etal-2008-masc,fillmore-etal-2004-framenet,1,\N,Missing
J88-2005,H86-1012,0,0.108007,"Missing"
J88-2005,P88-1002,1,0.862853,"Missing"
J88-2005,P86-1003,0,0.116316,"n iteration of successive failure events. Of these two possibilities for the second clause, PUNDIT currently generates only the former. Computational Linguistics, Volume 14, Number 2, June 1988 Rebecca J. Passonneau 4.3.2 PERFECT TENSES The perfect tenses have a more complex semantics and pragmatics than the simple tenses. The semantic interpretation given here accounts for the temporal interpretations assigned to the perfect tenses in which the event time and reference time are distinct from one another. There are uses of the perfect that do not have these temporal effects, as pointed out in McCawley 1971, i.e., cases where the event time and reference time would not be distinct. Here we consider only the temporally relevant uses of the perfect, where each perfect tense specifies two temporal relations: in both cases, the event time precedes the reference time; and tense indicates whether the reference time coincides with or precedes the speech time. The following examples illustrate the present and past perfect with a variety of temporal structures. The only difference between these examples and the simple present tenses examined in the preceding section is the relation between the reference"
J88-2005,P87-1001,0,0.0236057,"Missing"
J88-2005,P86-1004,0,0.0671276,"Missing"
J88-2005,J88-2005,1,0.106461,"Missing"
J88-2005,P87-1021,0,0.262421,"Missing"
J88-2005,H86-1011,0,\N,Missing
J88-2005,P87-1019,1,\N,Missing
J96-2008,J88-2003,0,0.0719171,"event with no internal structure, and thus updates a DAT with a new sister node (representing temporal precedence, a strict partial order) to the current node. Accomplishments are filters, which in terms of information flow are plugs or holes, depending on further unspecified inference. Filters are ter Meulen's means of representing the lack of homogeneity in the internal structure of accomplishments; an accomplishment is a nonhomogeneous event consisting of some (perhaps) homogeneous region after which the flow of information might shift (or make a transition [Passonneau 1988], or culminate [Moens and Steedman 1988]). Chapter 4 is an important complement to Chapter 3, dealing with various types of states (e.g., transient states, such as generally indicated by the progressive, as opposed to descriptions of more permanent properties, such as generic statements) and how different kinds of stative information percolate differently through a DAT. Chapter 5 completes the overview of DATs by defining the author's notion of perspective, but the issues raised here are not as fully developed. Briefly, holes preserve perspective, plugs trigger perspective shifts. Flashbacks, also treated in this chapter, involve t"
J96-2008,J88-2005,1,0.736288,"plug, meaning an instantaneous event with no internal structure, and thus updates a DAT with a new sister node (representing temporal precedence, a strict partial order) to the current node. Accomplishments are filters, which in terms of information flow are plugs or holes, depending on further unspecified inference. Filters are ter Meulen's means of representing the lack of homogeneity in the internal structure of accomplishments; an accomplishment is a nonhomogeneous event consisting of some (perhaps) homogeneous region after which the flow of information might shift (or make a transition [Passonneau 1988], or culminate [Moens and Steedman 1988]). Chapter 4 is an important complement to Chapter 3, dealing with various types of states (e.g., transient states, such as generally indicated by the progressive, as opposed to descriptions of more permanent properties, such as generic statements) and how different kinds of stative information percolate differently through a DAT. Chapter 5 completes the overview of DATs by defining the author's notion of perspective, but the issues raised here are not as fully developed. Briefly, holes preserve perspective, plugs trigger perspective shifts. Flashbacks,"
J97-1005,P95-1017,0,0.0534294,"Missing"
J97-1005,J96-2004,0,0.106505,"tract significant data for use in defining segmentations for each narrative. The results indicate that the observed distributions are highly significant, i.e., unlikely to have arisen by chance. In Section 3.2.2, we briefly review Cochran's Q (1950), the statistic that we use, and the test of the null hypothesis. We then partition Cochran's Q to determine the lowest value on the x-axis in Figure 3 at which agreements on boundaries become statistically significant. The results indicate significance arises when at least three subjects agree on a boundary. Reliability metrics (Krippendorff 1980; Carletta 1996) are designed to give a robust measure of how well distinct sets of data agree with, or replicate, one another. They are sensitive to the relative proportion of the different data types (e.g., boundaries versus nonboundaries), but insensitive to the statistical likelihood that agreements will occur. We have already discussed how variable the subjects' responses are, both in number and placement of segment boundaries, so we know that our subjects are not replicating the same behavior. However, all 20 narratives show the same pattern of responses as illustrated in Figure 3: certain boundaries ar"
J97-1005,P84-1055,0,0.105596,"and Pollack 1992; Hearst 1994; Walker 1995), and the magnitude of our segmentation task precludes asking subjects to specify hierarchical relations. Finally, we quantify our results using a significance 106 Passonneau and Litman Discourse Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed i"
J97-1005,P92-1032,0,0.0304954,"Missing"
J97-1005,J95-2003,0,0.115912,"Missing"
J97-1005,J86-3001,0,0.972787,"assign among segments. The nature of any hypothesized interaction between discourse structure and linguistic devices depends both on the model of discourse that is adopted, and on the types of linguistic devices that are investigated. Here we briefly review previous work on characterizing discourse segments, and on correlating discourse segments with utterance features. We conclude each review by summarizing the differences between our study and previous work. 2.1 Characterizing the Notion of a Segment A number of alternative proposals have been presented, which relate segments to intentions (Grosz and Sidner 1986), Rhetorical Structure Theory (RST) relations (Mann and Thompson 1988) or other semantic relations (Polanyi 1988; Hobbs 1979). The linguistic structure of Grosz and Sidner's (1986) discourse model consists of multiutterance segments and structural relations among them, yielding a discourse tree structure. The hierarchical relations of their linguistic structure are isomorphic with the two other levels of their model, intentional structure and attentional state. Rhetorical relations do not play a role in their model. In Hobbs (1979) and Polanyi (1988), segmental structure is an artifact of cohe"
J97-1005,P94-1002,0,0.888745,"d on which speaker had control. Neither study presented any quantitative analysis of the ability to reliably perform the initial utterance classification. However, in Whittaker and Stenton (1988), a higher level of discourse structure based on topic shifts was agreed upon by at least 4 of 5 judges for 46 of the 56 control shifts. In sum, relatively few quantitative empirical studies have been made of how to annotate discourse corpora with features of discourse structure, and those recent ones that exist use various models such as the Grosz and Sidner model (1986), an informal notion of topic (Hearst 1994; Flammia and Zue 1995), transactions (Isard and Carletta 1995), Relational Discourse Analysis (Moser and Moore 1995), or control (Whittaker and Stenton 1988; Walker and Whittaker 1990). The modalities of the corpora investigated include dialogic or monologic, written, spontaneous or read, and the genres also vary. Quantitative evaluations of subjects' annotations using notions of agreement, interrater reliability, a n d / o r significance show that good results can be difficult to achieve. As discussed in Section 3, our initial aim was to explore basic issues about segmentation, thus we used"
J97-1005,P83-1019,0,0.0772369,"Missing"
J97-1005,J93-3003,1,0.800342,"a tree structure has frequently been questioned (Dale 1992; Moore and Pollack 1992; Hearst 1994; Walker 1995), and the magnitude of our segmentation task precludes asking subjects to specify hierarchical relations. Finally, we quantify our results using a significance 106 Passonneau and Litman Discourse Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justi"
J97-1005,P96-1038,0,0.236437,"91; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Hirschberg and Nakatani 1996; Swerts 1995; Swerts and Ostendorf 1995), term repetition (Hearst 1994), cue words (Moser and Moore 1995; Whittaker and Stenton 1988), and discourse anaphora (Walker and Whittaker 1990). Grosz and Hirschberg (1992) investigate the prosodic structuring of discourse. The correlation of various prosodic features with their independently obtained consensus codings of segmental structure (codings on which all labelers agreed) is analyzed using t-tests; the results support the hypothesis that discourse structure is marked intonationally in read speech. For example, pauses tended to precede phrases"
J97-1005,P86-1021,0,0.00787648,"ubjects to specify hierarchical relations. Finally, we quantify our results using a significance 106 Passonneau and Litman Discourse Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirsch"
J97-1005,P92-1030,0,0.0127117,"test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Hirschberg and Nakatani 1996; Swerts 1995; Swerts and Ostendorf 1995), term repetition"
J97-1005,P93-1041,0,0.44939,"e. Hearst's (1994) TextTiling algorithm structures expository text into sequential segments based on term repetition. Hearst (1994) uses information retrieval metrics (see Section 4.1) to evaluate two versions of TextTiling against independently derived segmentations produced by at least three of seven human judges. Precision was .66 for the best version, compared with .81 for humans; recall was .61 compared with .71 for humans. The use of term repetition (and a related notion of lexical cohesion) is not unique to Hearst's work; related studies include Morris and Hirst (1991), Youmans (1991), Kozima (1993), and Reynar (1994). Unlike Hearst's work, these studies either use segmentations that are not empirically justified, or present only qualitative analyses of the correlation with linguistic devices. After identifying segments, and core and contributor relations within segments, Moser and Moore (1995) investigate whether cue words occur, where they occur, and 107 Computational Linguistics Volume 23, Number 1 what word occurs. In their talk, they presented results showing that the occurrence and placement of a discourse usage of a cue word correlates with relative order of core versus contributo"
J97-1005,P95-1015,1,0.687283,"Missing"
J97-1005,J93-4004,0,0.0169987,"Missing"
J97-1005,J92-4007,0,0.075146,"valuation, cause, and so on. Their coherence relations are similar to those posited in RST (Mann and Thompson 1988), which informs much work in generation. Polanyi (1988) distinguishes among four types of Discourse Constituent Units (DCUs) based on different types of structural relations (e.g., sequence). As in Grosz and Sidner's (1986) model, Polanyi (1988) proposes that DCUs (analogous to segments) are structured as a tree, and in both models, the tree structure of discourse constrains how the discourse evolves, and how referring expressions are processed. Recent work (Moore and Paris 1993; Moore and Pollack 1992) has argued that to account for explanation dialogues, it is necessary to independently model both RST relations and intentions. Researchers have begun to investigate the ability of humans to agree with one another on segmentation, and to propose methodologies for quantifying their findings. The types of discourse units being coded and the relations among them vary. Several studies have used trained coders to locally and globally structure spontaneous or read speech using the model of Grosz and Sidner (1986), including Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Stifleman"
J97-1005,J91-1002,0,0.184351,"nteractions, with statistical significance. Hearst's (1994) TextTiling algorithm structures expository text into sequential segments based on term repetition. Hearst (1994) uses information retrieval metrics (see Section 4.1) to evaluate two versions of TextTiling against independently derived segmentations produced by at least three of seven human judges. Precision was .66 for the best version, compared with .81 for humans; recall was .61 compared with .71 for humans. The use of term repetition (and a related notion of lexical cohesion) is not unique to Hearst's work; related studies include Morris and Hirst (1991), Youmans (1991), Kozima (1993), and Reynar (1994). Unlike Hearst's work, these studies either use segmentations that are not empirically justified, or present only qualitative analyses of the correlation with linguistic devices. After identifying segments, and core and contributor relations within segments, Moser and Moore (1995) investigate whether cue words occur, where they occur, and 107 Computational Linguistics Volume 23, Number 1 what word occurs. In their talk, they presented results showing that the occurrence and placement of a discourse usage of a cue word correlates with relative"
J97-1005,P95-1018,0,0.125909,"urse structure, using a spoken corpus of database query interactions. Although the labelers had high levels of agreement, the segmentations were fairly trivial. Isard and Carletta (1995) presented 4 naive subjects and 1 expert coder with transcripts of task-oriented dialogues from the HCRC Map Task Corpus (Anderson et al. 1991). Utterance-like units referred to as moves were identified in the transcripts, and subjects were asked to identify transaction boundaries. Since reliability was lower than the .80 threshold, they concluded that their coding scheme and instructions required improvement. Moser and Moore (1995) investigated the reliability of various features defined in Relational Discourse Analysis (Moser, Moore, and Glendening 1995), based in part on RST. Their corpus consisted of written interactions between tutors and students, using 3 different tutors. Two coders were asked to identify segments, the core utterance of each segment, and certain intentional and informational relations between the core and the other contributor utterances. As reported in their talk (not in the paper), reliability on segment structure and core identification was well over the .80 threshold. Reliability on intentiona"
J97-1005,P94-1050,0,0.164918,"TextTiling algorithm structures expository text into sequential segments based on term repetition. Hearst (1994) uses information retrieval metrics (see Section 4.1) to evaluate two versions of TextTiling against independently derived segmentations produced by at least three of seven human judges. Precision was .66 for the best version, compared with .81 for humans; recall was .61 compared with .71 for humans. The use of term repetition (and a related notion of lexical cohesion) is not unique to Hearst's work; related studies include Morris and Hirst (1991), Youmans (1991), Kozima (1993), and Reynar (1994). Unlike Hearst's work, these studies either use segmentations that are not empirically justified, or present only qualitative analyses of the correlation with linguistic devices. After identifying segments, and core and contributor relations within segments, Moser and Moore (1995) investigate whether cue words occur, where they occur, and 107 Computational Linguistics Volume 23, Number 1 what word occurs. In their talk, they presented results showing that the occurrence and placement of a discourse usage of a cue word correlates with relative order of core versus contributor utterances. For e"
J97-1005,P90-1010,0,0.0125758,"Missing"
J97-1005,J88-2006,0,0.0180438,"Segmentation test, a reliability measure, and, for purposes of comparison with other work, percent agreement. 2.2 Correlation of Segmentation with Utterance Features The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena, e.g., cue phrases (Hirschberg and Litman 1993; Grosz and Sidner 1986; Reichman 1985; Cohen 1984), plans and intentions (Carberry 1990; Litman and Allen 1990; Grosz and Sidner 1986), prosody (Hirschberg and Pierrehumbert 1986; Butterworth 1980), nominal reference (Webber 1991; Grosz and Sidner 1986; Linde 1979), and tense (Webber 1988; Hwang and Schubert 1992; Song and Cohen 1991). However, just as with the early proposals regarding segmentation, many of these proposals are based on fairly informal studies. It is only recently that attempts have been made to quantitatively evaluate how utterance features correlate with independently justified segmentations. Many of the studies discussed in the preceding subsection take this approach. The types of linguistic features investigated ind u d e prosody (Grosz and Hirschberg 1992; Nakatani, Hirschberg, and Grosz 1995; Hirschberg and Nakatani 1996; Swerts 1995; Swerts and Ostendor"
J97-1005,P88-1015,0,0.0343641,"written interactions between tutors and students, using 3 different tutors. Two coders were asked to identify segments, the core utterance of each segment, and certain intentional and informational relations between the core and the other contributor utterances. As reported in their talk (not in the paper), reliability on segment structure and core identification was well over the .80 threshold. Reliability on intentional and informational relations was around .75, high enough to support tentative conclusions. Finally, a method for segmenting dialogues based on a notion of control was used in Whittaker and Stenton (1988) and Walker and Whittaker (1990). Utterances were classified into four types, each of which was associated with a rule that assigned a controller; the discourse was then divided into segments, based on which speaker had control. Neither study presented any quantitative analysis of the ability to reliably perform the initial utterance classification. However, in Whittaker and Stenton (1988), a higher level of discourse structure based on topic shifts was agreed upon by at least 4 of 5 judges for 46 of the 56 control shifts. In sum, relatively few quantitative empirical studies have been made of"
J97-1005,P93-1020,1,\N,Missing
J97-1005,J96-2005,0,\N,Missing
K19-1038,W04-1013,0,0.162007,"rts versus extractive summarizers on forty-six topics from the TAC 2010 summarization challenge used automatic caseframe analysis, and found essentially these same properties in the human summaries, and not in the extractive ones (Cheung and Penn, 2013). Abstractive summarizers, however, are beginning to replicate the first two of these behaviors, as illustrated in many published examples based on encoder-decoder and pointergenerator neural architectures (Nallapati et al., 2016; See et al., 2017; Hsu et al., 2018; Guo et al., 2018). Summarization evaluation relies almost exclusively on ROUGE (Lin, 2004), an automated tool that cannot directly assess importance of summary content, or novel wording for the same inforMatch to a student summary that used synomyms: a craftsmanship exhibition alongside a Scottish inn have plans for their clients to pay in digital currencies Figure 1: Alignment of a single PyrEval SCU of weight 5 to a manual SCU of weight 4 from a dataset of student summaries. The manual and automated SCUs express the same content, and their weights differ only by one. For each of five reference summaries (RSUM1-RSUM5), exact matches of words between the PyrEval and manual contribu"
K19-1038,N18-1049,0,0.0203213,"ESUM) summaries into clause-like units, then convert them to latent vectors. EDUA constructs a pyramid from RSUM vectors (lower left): the horizontal bands of the pyramid represent SCUs of decreasing weight (shaded squares). WMIN matches SCUs to ESUM segments to produce a raw score, and three normalized scores. Data (size) WIM (20) TAC 09 (54) STS-14 (3750) ELMo 0.3873 0.0515 0.1636 USE -0.0290 0.2672 0.5757 GloVe 0.7149 0.1713 0.6129 1.6.3.1 Figure 3: Segmentation output for a sentence from a reference summary for the “CryptoCurrencies” topic of our student summaries. neural training (e.g., (Pagliardini et al., 2018; Peters et al., 2018; Devlin et al., 2018; Vaswani et al., 2017)). The most practical way to rely on completely pre-trained representations is to use word embeddings along with a method to combine them into phrase embeddings. Here we report on a comparison of ELMo (Peters et al., 2018) and the Universal Sentence Encoder for English (USE) (Cer et al., 2018) with two conventional word embedding methods, GloVe (Pennington et al., 2014) and WTMF (Guo and Diab, 2012).4 ELMo is character-based rather than wordbased, relies on a many-layered bidirectional LSTM, and incorporates word sequence (langua"
K19-1038,D14-1162,0,0.0852223,"9 1.6.3.1 Figure 3: Segmentation output for a sentence from a reference summary for the “CryptoCurrencies” topic of our student summaries. neural training (e.g., (Pagliardini et al., 2018; Peters et al., 2018; Devlin et al., 2018; Vaswani et al., 2017)). The most practical way to rely on completely pre-trained representations is to use word embeddings along with a method to combine them into phrase embeddings. Here we report on a comparison of ELMo (Peters et al., 2018) and the Universal Sentence Encoder for English (USE) (Cer et al., 2018) with two conventional word embedding methods, GloVe (Pennington et al., 2014) and WTMF (Guo and Diab, 2012).4 ELMo is character-based rather than wordbased, relies on a many-layered bidirectional LSTM, and incorporates word sequence (language model) information. It was trained on billions of tokens of Wikipedia and news text. To create meaning vectors for strings of words, we use pretrained ELMo vectors, taking the weighted sum of 3 output layers as the word embeddings, then applying mean pooling.5 USE is intended for transfer learning tasks, based on Transformer (Vaswani et al., 2017) or the (Iyyer et al., 2015) deep averaging network (DAN). We create meaning vectors"
K19-1038,N18-1202,0,0.0100812,"e-like units, then convert them to latent vectors. EDUA constructs a pyramid from RSUM vectors (lower left): the horizontal bands of the pyramid represent SCUs of decreasing weight (shaded squares). WMIN matches SCUs to ESUM segments to produce a raw score, and three normalized scores. Data (size) WIM (20) TAC 09 (54) STS-14 (3750) ELMo 0.3873 0.0515 0.1636 USE -0.0290 0.2672 0.5757 GloVe 0.7149 0.1713 0.6129 1.6.3.1 Figure 3: Segmentation output for a sentence from a reference summary for the “CryptoCurrencies” topic of our student summaries. neural training (e.g., (Pagliardini et al., 2018; Peters et al., 2018; Devlin et al., 2018; Vaswani et al., 2017)). The most practical way to rely on completely pre-trained representations is to use word embeddings along with a method to combine them into phrase embeddings. Here we report on a comparison of ELMo (Peters et al., 2018) and the Universal Sentence Encoder for English (USE) (Cer et al., 2018) with two conventional word embedding methods, GloVe (Pennington et al., 2014) and WTMF (Guo and Diab, 2012).4 ELMo is character-based rather than wordbased, relies on a many-layered bidirectional LSTM, and incorporates word sequence (language model) information"
K19-1038,P08-1094,0,0.177769,"against which to compare a given metric. ROUGE 2 has the highest average accuracy on both Task A and B. ROUGE 1 performs nearly as well on Task A. PyrEval performs less well on average accuracy for all tasks, but similarly to ROUGE 1 in Task B. ROUGE-2 has greater sensitivity to topics, as shown by the higher deltas between the bootstrapped accuracy on 41 topics versus the accuracy on all 46. The differences in Table 4 between the bootstrapped averages across 41 topics, and the accuracy scores on all 46 topics, confirms the sensitivity of evaluation results to topics noted in (Nenkova, 2005; Nenkova and Louis, 2008). 7 Conclusion PyrEval outperforms previous automated pyramid methods in accuracy, efficiency, score normalization, and interpretability. It correlates with manual pyramid better than ROUGE on a new dataset of student summaries, and produces output that helps justify the scores (similar to the examples for Figures 1 and 7). While it does not perform as well as ROUGE on extractive summarization, we speculate it would outperform ROUGE on abstractive summarizers. It relies on EDUA, a novel restricted set partition algorithm, that expects semantic vectors of sentence segments as input. The current"
K19-1038,N04-1019,1,0.822065,"Missing"
K19-1038,P17-1100,0,0.550463,"Missing"
K19-1038,P13-1132,0,0.0849421,"Missing"
K19-1038,W12-2601,0,0.046849,"Missing"
K19-1038,D11-1043,0,0.290948,"Missing"
K19-1038,P17-1099,0,0.0570194,"ground knowledge (van Dijk and Kintsch, 1977; Brown and Day, 1983). A recent comparison of summaries from human experts versus extractive summarizers on forty-six topics from the TAC 2010 summarization challenge used automatic caseframe analysis, and found essentially these same properties in the human summaries, and not in the extractive ones (Cheung and Penn, 2013). Abstractive summarizers, however, are beginning to replicate the first two of these behaviors, as illustrated in many published examples based on encoder-decoder and pointergenerator neural architectures (Nallapati et al., 2016; See et al., 2017; Hsu et al., 2018; Guo et al., 2018). Summarization evaluation relies almost exclusively on ROUGE (Lin, 2004), an automated tool that cannot directly assess importance of summary content, or novel wording for the same inforMatch to a student summary that used synomyms: a craftsmanship exhibition alongside a Scottish inn have plans for their clients to pay in digital currencies Figure 1: Alignment of a single PyrEval SCU of weight 5 to a manual SCU of weight 4 from a dataset of student summaries. The manual and automated SCUs express the same content, and their weights differ only by one. For"
L18-1511,P15-1153,1,0.920565,"Moreover, PyrEval has been tested on many datasets derived from humans and machine generated summaries, and shown good performance on both. Keywords: Content Evaluation, Summarization, Summarization Evaluation Tool 1. Introduction Automatic summarization methods that produce a paragraph to express the main ideas of one or more texts have shifted in recent years. The long-standing prevalence of extractive summarizers, which select complete sentences from source documents, has begun to give way to abstractive summarizers, which rewrite or generate sentences to eliminate less important content (Bing et al., 2015; Rush et al., 2015; Liu et al., 2015; Durrett et al., 2016; Nema et al., 2017). Automatic content evaluation of summarization systems has not seen the same progress. ROUGE has long been the dominant method for evaluating summary content, and is used in most of the papers cited above. Here we present PyrEval, which applies pyramid content analysis (Nenkova and Passonneau, 2004), a method intended for evaluation of abstractive summarization. The main innovation in PyrEval is a novel formulation of the pyramid construction problem, and a greedy algorithm whose approximate solution is both effici"
L18-1511,C08-1019,0,0.0221495,".29 57.36 53.33 Min -2.31 -3.27 -13.93 15.64 10.16 Max 76.34 89.18 79.09 84.54 83.88 StdDev 24.42 27.05 18.92 16.67 16.27 Table 2: Pearson correlations (Pearson’s r × 100) with manual pyramid scores from DUC and TAC. opment set. Because the semantic vectors are constructed from pretrained data, there is no training set. As shown, the trend over time is for PyrEval’s average correlation to be higher, and for the standard deviation to be lower. This could result simply from the much larger datasets as of 2008, or that PyrEval performs better on data from summarizers whose performance is better. Conroy & Dang (2008) report that the top ROUGE scores increase from about 0.27 in 2005 to about 0.34 in 2007. The maximum correlations for an individual topic in one year can be as high as 0.89. The minimum values tend to be many standard deviations below the mean, suggesting that these are outliers, and that the median is a better summary statistic. Apart from DUC07, the medians are higher than the means. 6. Discussion and Conclusion The results in Table 2 show that PyrEval can achieve good average correlations of up to 0.55 (TAC10), and much higher (e.g., 0.89) on individual summarization tasks. As discussed in"
L18-1511,P16-1188,0,0.0319321,"Missing"
L18-1511,E17-2055,0,0.0153337,"ooster seat”, ”back seat”, ”crash test”, ”sport utility model”; ”MercedesBenz”, ”BodySmart”). The vector representations treat the compounds as sequences of individual words, which does not capture the correct semantics. Identification of multi-word expressions prior to generating the phrase vectors might correct this. We have also observed that conjunctions, which typically introduce a great deal of syntactic ambiguity, are often parsed incorrectly. This can lead to incoherent output from the decomposition parser. Possible methods to address this include neural net dependency parsing, as in (Ficler and Goldberg, 2017), or clause-chart parsing (Kr´ızˇ and Hladk´a, 2016). Figure 8 also shows a CU produced by PyrEval that captures the same content as the manual content unit shown in Figure 1. The PyrEval CU has no label, but is otherwise similar to the manual CU: it has the same weight, and the contributing clauses come from the same sentences of the same summaries. The phrases selected by the human annotator for this CU are subphrases of the clauses that PyrEval groups together. Future work could rely on improved parsing to identify all distinct propositions, not just tensed clauses. Figure 8: Examples of an"
L18-1511,P12-1091,0,0.0204763,"stinct hyperedges is measured using Align, Disambiguate and Walk (ADW) (Pilehvar et al., 2013), which relies on WordNet (Miller, 1995). Triples from distinct summaries with high similarity are combined into content units. To assess a new summary, its triples are extracted, and the Munkres-Kuhn algorithm (Kuhn, 1955) is applied to a bipartite graph from content unit (CU) nodes u to nodes v that represent target summary triples. PyrScore (Passonneau et al., 2013) uses manual pyramids to score target summaries, with Weighted Textual Matrix Factorization (WTMF) for latent semantic representation (Guo and Diab, 2012), and ngram segmentation to find all possible combinations of covering segmentations of input sentences. The scoring procedure can be formulated as a graph covering problem, where each sentence is a subgraph, and each segmentation assigned to a sentence is a vertex of the graph. The goal is to align a candidate sentence segmentation with content units from a manual pyramid via semantic similarity between each segment and every contributor in a content unit, under the constraint that a content unit can be allocated to no more than one segment. PyrScore applies WMIN (Sakai et al., 2003), a greed"
L18-1511,P16-3013,0,0.0498675,"Missing"
L18-1511,W04-1013,0,0.0198143,"ion of a pyramid content model, and PEAK depends on external resources that are computationally inefficient for largescale use. This paper introduces the PyrEval software and gives an overview of how PyrEval builds on the earlier PyrScore. 1 Details of each component will be presented in other publications. Tests of PyrEval performance on five years of manual pyramid evaluations conducted by NIST show that average performance on all topics for a given year correlates well with manual pyramid. 2. Related Work The most widely used content evaluation tool for machinegenerated summaries is ROUGE (Lin, 2004), which takes model summaries as references and scores target summaries by matching substrings. It correlates well with human scores in ranking performance of systems on multiple summarization tasks. But it is not reliable for evaluating a 1 Availablefordownloadfromhttps://github. com/serenayj/PyrEval 3234 single summary (Louis and Nenkova, 2009). In addition, it does not provide informative feedback on the ideas contained in a summary, or the important ideas that have not been mentioned. In contrast, pyramid content scores are reliable for individual summaries, the scores can be interpreted a"
L18-1511,N15-1114,0,0.0204831,"many datasets derived from humans and machine generated summaries, and shown good performance on both. Keywords: Content Evaluation, Summarization, Summarization Evaluation Tool 1. Introduction Automatic summarization methods that produce a paragraph to express the main ideas of one or more texts have shifted in recent years. The long-standing prevalence of extractive summarizers, which select complete sentences from source documents, has begun to give way to abstractive summarizers, which rewrite or generate sentences to eliminate less important content (Bing et al., 2015; Rush et al., 2015; Liu et al., 2015; Durrett et al., 2016; Nema et al., 2017). Automatic content evaluation of summarization systems has not seen the same progress. ROUGE has long been the dominant method for evaluating summary content, and is used in most of the papers cited above. Here we present PyrEval, which applies pyramid content analysis (Nenkova and Passonneau, 2004), a method intended for evaluation of abstractive summarization. The main innovation in PyrEval is a novel formulation of the pyramid construction problem, and a greedy algorithm whose approximate solution is both efficient and effective. The pyramid method"
L18-1511,D09-1032,0,0.0214248,"nce on five years of manual pyramid evaluations conducted by NIST show that average performance on all topics for a given year correlates well with manual pyramid. 2. Related Work The most widely used content evaluation tool for machinegenerated summaries is ROUGE (Lin, 2004), which takes model summaries as references and scores target summaries by matching substrings. It correlates well with human scores in ranking performance of systems on multiple summarization tasks. But it is not reliable for evaluating a 1 Availablefordownloadfromhttps://github. com/serenayj/PyrEval 3234 single summary (Louis and Nenkova, 2009). In addition, it does not provide informative feedback on the ideas contained in a summary, or the important ideas that have not been mentioned. In contrast, pyramid content scores are reliable for individual summaries, the scores can be interpreted as indicating whether a summary contains mainly important ideas, and whether it contains enough of the important ideas. Finally, the scores can be justified with respect to specific ideas that are included or missing. Interest in the pyramid method has been revived by development of PEAK (Yang et al., 2016), a fully automated version that performs"
L18-1511,P14-5010,0,0.00336207,"n ngrams. Table 1 shows an example of running times for these four steps on one of the development sets from DUC06. PyrEval was developed and tested on English. To apply it with other languages would require replacing the preprocessing steps that segment sentences into distinct clauses, and that produce low-dimensional vectors for their semantic representation. In principle, the current code package could easily be extended to substitute a sentence segmenter designed for a distinct language, and a pretrained matrix of words by sentences. The installation for PyrEval requires Stanford CoreNLP (Manning et al., 2014), perl, python2.7 (or anaconda) with packages NLTK, statistics, beautifulsoup, networkx, sklearn and numpy. All the experiments reported here were conducted on an ubuntu machine, with 4 Intel i5-6600 CPUs. 4.1.1. Sentence Decomposition PyrEval relies on our sentence decomposition parser to extract clause-like units (segments) from sentences. Each set of semantic units yielded by a sentence is a segmentation, and there will be at least one segmentation per sentence. The sentence decomposition parser takes results from the Stanford CoreNLP constituency parser and dependency parser as input. For"
L18-1511,P17-1098,0,0.014929,"chine generated summaries, and shown good performance on both. Keywords: Content Evaluation, Summarization, Summarization Evaluation Tool 1. Introduction Automatic summarization methods that produce a paragraph to express the main ideas of one or more texts have shifted in recent years. The long-standing prevalence of extractive summarizers, which select complete sentences from source documents, has begun to give way to abstractive summarizers, which rewrite or generate sentences to eliminate less important content (Bing et al., 2015; Rush et al., 2015; Liu et al., 2015; Durrett et al., 2016; Nema et al., 2017). Automatic content evaluation of summarization systems has not seen the same progress. ROUGE has long been the dominant method for evaluating summary content, and is used in most of the papers cited above. Here we present PyrEval, which applies pyramid content analysis (Nenkova and Passonneau, 2004), a method intended for evaluation of abstractive summarization. The main innovation in PyrEval is a novel formulation of the pyramid construction problem, and a greedy algorithm whose approximate solution is both efficient and effective. The pyramid method, introduced in (Nenkova and Passonneau, 2"
L18-1511,N04-1019,1,0.631785,"e long-standing prevalence of extractive summarizers, which select complete sentences from source documents, has begun to give way to abstractive summarizers, which rewrite or generate sentences to eliminate less important content (Bing et al., 2015; Rush et al., 2015; Liu et al., 2015; Durrett et al., 2016; Nema et al., 2017). Automatic content evaluation of summarization systems has not seen the same progress. ROUGE has long been the dominant method for evaluating summary content, and is used in most of the papers cited above. Here we present PyrEval, which applies pyramid content analysis (Nenkova and Passonneau, 2004), a method intended for evaluation of abstractive summarization. The main innovation in PyrEval is a novel formulation of the pyramid construction problem, and a greedy algorithm whose approximate solution is both efficient and effective. The pyramid method, introduced in (Nenkova and Passonneau, 2004), groups the distinct ideas (content units) mentioned in reference summaries to create a pyramid model of content, and differentiates the content units by importance as well as meaning. It then evaluates target summaries against the pyramid content model. Early work applied manual annotation of t"
L18-1511,P13-2026,1,0.813448,"embles a hypergraph where each triple is a hyperedge containing three nodes: subject, predicate and object. The semantic similarity of all pairs of nodes in distinct hyperedges is measured using Align, Disambiguate and Walk (ADW) (Pilehvar et al., 2013), which relies on WordNet (Miller, 1995). Triples from distinct summaries with high similarity are combined into content units. To assess a new summary, its triples are extracted, and the Munkres-Kuhn algorithm (Kuhn, 1955) is applied to a bipartite graph from content unit (CU) nodes u to nodes v that represent target summary triples. PyrScore (Passonneau et al., 2013) uses manual pyramids to score target summaries, with Weighted Textual Matrix Factorization (WTMF) for latent semantic representation (Guo and Diab, 2012), and ngram segmentation to find all possible combinations of covering segmentations of input sentences. The scoring procedure can be formulated as a graph covering problem, where each sentence is a subgraph, and each segmentation assigned to a sentence is a vertex of the graph. The goal is to align a candidate sentence segmentation with content units from a manual pyramid via semantic similarity between each segment and every contributor in"
L18-1511,P17-1100,0,0.0491899,"st seconds before a crash record data on vehicle performance (”black box”) black boxes . . . record data pertinent to an auto crash to lead to safety improvements Matched phrase from a peer summary a device similar to an airliner’s black box, . . . that automatically records the vehicle’s Figure 1: A content unit (CU) of weight 3 from a NIST manual pyramid for topic D0608, and a phrase from an extractive summary (peer) that was matched to the CU. neau et al., 2013) or PEAK (Yang et al., 2016) have been used in recent work on abstractive summarization, as in (Bing et al., 2015) (PyrScore) and (Peyrard and Eckle-Kohler, 2017) (PEAK), but have disadvantages that PyrEval addresses. PyrScore requires manual construction of a pyramid content model, and PEAK depends on external resources that are computationally inefficient for largescale use. This paper introduces the PyrEval software and gives an overview of how PyrEval builds on the earlier PyrScore. 1 Details of each component will be presented in other publications. Tests of PyrEval performance on five years of manual pyramid evaluations conducted by NIST show that average performance on all topics for a given year correlates well with manual pyramid. 2. Related W"
L18-1511,P13-1132,0,0.0121555,"ve for generating summaries. PEAK is one of two recent automated systems that implement pyramid evaluation (Yang et al., 2016). It constructs a pyramid from reference summaries, and scores target summaries against its automated pyramid. It extracts subject-predicate-object triples from reference summary sentences using ClauseIE (Del Corro and Gemulla, 2013), then assembles a hypergraph where each triple is a hyperedge containing three nodes: subject, predicate and object. The semantic similarity of all pairs of nodes in distinct hyperedges is measured using Align, Disambiguate and Walk (ADW) (Pilehvar et al., 2013), which relies on WordNet (Miller, 1995). Triples from distinct summaries with high similarity are combined into content units. To assess a new summary, its triples are extracted, and the Munkres-Kuhn algorithm (Kuhn, 1955) is applied to a bipartite graph from content unit (CU) nodes u to nodes v that represent target summary triples. PyrScore (Passonneau et al., 2013) uses manual pyramids to score target summaries, with Weighted Textual Matrix Factorization (WTMF) for latent semantic representation (Guo and Diab, 2012), and ngram segmentation to find all possible combinations of covering segm"
L18-1511,D15-1044,0,0.0165925,"has been tested on many datasets derived from humans and machine generated summaries, and shown good performance on both. Keywords: Content Evaluation, Summarization, Summarization Evaluation Tool 1. Introduction Automatic summarization methods that produce a paragraph to express the main ideas of one or more texts have shifted in recent years. The long-standing prevalence of extractive summarizers, which select complete sentences from source documents, has begun to give way to abstractive summarizers, which rewrite or generate sentences to eliminate less important content (Bing et al., 2015; Rush et al., 2015; Liu et al., 2015; Durrett et al., 2016; Nema et al., 2017). Automatic content evaluation of summarization systems has not seen the same progress. ROUGE has long been the dominant method for evaluating summary content, and is used in most of the papers cited above. Here we present PyrEval, which applies pyramid content analysis (Nenkova and Passonneau, 2004), a method intended for evaluation of abstractive summarization. The main innovation in PyrEval is a novel formulation of the pyramid construction problem, and a greedy algorithm whose approximate solution is both efficient and effective."
moro-etal-2014-annotating,D11-1072,0,\N,Missing
moro-etal-2014-annotating,passonneau-etal-2010-word,1,\N,Missing
moro-etal-2014-annotating,D12-1128,1,\N,Missing
moro-etal-2014-annotating,ide-etal-2008-masc,1,\N,Missing
moro-etal-2014-annotating,H93-1061,0,\N,Missing
moro-etal-2014-annotating,J14-4005,1,\N,Missing
moro-etal-2014-annotating,Q14-1019,1,\N,Missing
moro-etal-2014-annotating,P11-3004,0,\N,Missing
moro-etal-2014-annotating,P10-1154,1,\N,Missing
moro-etal-2014-annotating,J14-1003,0,\N,Missing
moro-etal-2014-annotating,passonneau-etal-2012-masc,1,\N,Missing
moro-etal-2014-annotating,P13-1133,0,\N,Missing
moro-etal-2014-annotating,basile-etal-2012-developing,0,\N,Missing
moro-etal-2014-annotating,Q14-1025,1,\N,Missing
moro-etal-2014-annotating,ehrmann-etal-2014-representing,1,\N,Missing
N04-1019,W03-0508,0,0.666528,"f the Lockerbie bombing A1 [two Libyans]1 [indicted]1 B1 [Two Libyans were indicted]1 C1 [Two Libyans,]1 [accused]1 D2 [Two Libyan suspects were indicted]1 W=4 W=4 SCU2 (w=3): the indictment of the two Lockerbie suspects was in 1991 A1 [in 1991]2 B1 [in 1991]2 D2 [in 1991.]2 Figure 2: Two of six optimal summaries with 4 SCUs The remaining parts of the four sentences above end up as contributors to nine different SCUs of different weight and granularity. Though we look at multidocument summaries rather than single document ones, SCU annotation otherwise resembles the annotation of factoids in (Halteren and Teufel, 2003); as they do with factoids, we find increasing numbers of SCUs as the pool of summaries grows. For our 100 word summaries, we find about 3440 distinct SCUs across four summaries; with ten summaries this number grows to about 60. A more complete comparison of the two approaches follows in section 4. An SCU consists of a set of contributors that, in their sentential contexts, express the same semantic content. An SCU has a unique index, a weight, and a natural language label. The label, which is subject to revision throughout the annotation process, has three functions. First, it frees the annot"
N04-1019,W02-0406,0,0.0701845,"than an obstacle. In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited. However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002). Our approach tailors the evaluation to observed distributions of content over a pool of human summaries, rather than to human judgments of summaries. Our method involves semantic matching of content units to which differential weights are assigned based on their frequency in a corpus of summaries. This can lead to more stable, more informative scores, and hence to a meaningful content evaluation. We create a weighted inventory of Summary Content Units–a pyramid–that is reliable, predictive and diagnostic, and which constitutes a resource for investigating alternate realizations of the same m"
N04-1019,N03-1020,0,0.94887,"erties of pyramid scores, can be found in (Passonneau and Nenkova, 2003). 2 Current Approach: the Document Understanding Conference 1 Introduction 2.1 DUC Evaluating content selection in summarization has proven to be a difficult problem. Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle. In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited. However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002). Our approach tailors the evaluation to observed distributions of content over a pool of human summaries, rather than to human judgments of summaries. Our method involves semantic matching of content units to which differential weights are assig"
N04-1019,P02-1040,0,0.127909,"our next step, the feasibility of automating our method. A more detailed account of the work described here, but not including the study of distributional properties of pyramid scores, can be found in (Passonneau and Nenkova, 2003). 2 Current Approach: the Document Understanding Conference 1 Introduction 2.1 DUC Evaluating content selection in summarization has proven to be a difficult problem. Our approach acknowledges the fact that no single best model summary exists, and takes this as a foundation rather than an obstacle. In machine translation, the rankings from the automatic BLEU method (Papineni et al., 2002) have been shown to correlate well with human evaluation, and it has been widely used since and has even been adapted for summarization (Lin and Hovy, 2003). To show that an automatic method is a reasonable approximation of human judgments, one needs to demonstrate that these can be reliably elicited. However, in contrast to translation, where the evaluation criterion can be defined fairly precisely it is difficult to elicit stable human judgments for summarization (Rath et al., 1961) (Lin and Hovy, 2002). Our approach tailors the evaluation to observed distributions of content over a pool of"
N04-1019,passonneau-2004-computing,1,0.329784,"separate syntactic from semantic agreement, as in (Klavans et al., 2003). Because constituent structure is not relevant here, we normalize all contributors before computing reliability. We treat every word in a summary as a coding unit, and the SCU it was assigned to as the coding value. We require every surface word to be in exactly one contributor, and every contributor to be in exactly one SCU, thus an SCU annotation constitutes a set of equivalence classes. Computing reliability then becomes identical to comparing the equivalence classes constituting a set of coreference annotations. In (Passonneau, 2004), we report our method for computing reliability for coreference annotations, and the use of a distance metric that allows us to weight disagreements. Applying the same data representation and reliability formula (Krippendorff’s Alpha) as in (Passonneau, 2004), and a distance metric that takes into account relative SCU size, to the two codings C1 and C2 yields α = 81. Values above .67 indicate good reliability (Krippendorff, 1980). 0.9 0.8 Summary score Max = C1 C2 Consensus 0.7 0.6 0.5 arv d30042.b min d30042.b max d30042.b arv d30042.q min d30042.q max d30042.q 0.4 0.3 1 (9) 2 (36) 3 (84) 4"
N04-1019,P03-1048,0,0.00835103,"Missing"
N10-1126,gordon-passonneau-2010-evaluation,1,0.82166,"Missing"
N10-1126,P02-1048,0,0.364394,"Missing"
N10-1126,W05-1624,0,0.396798,"Missing"
N10-1126,J06-3004,0,0.201466,"Missing"
N10-1126,P99-1024,0,0.393314,"Missing"
N10-1126,2005.sigdial-1.11,0,0.311282,"Missing"
N10-1126,W09-0506,0,0.330406,"Missing"
N10-1126,H94-1039,0,\N,Missing
N10-1126,N09-2047,0,\N,Missing
N13-1128,D08-1100,0,0.0400105,"Missing"
N13-1128,E06-1009,0,0.0659274,"Missing"
N13-1128,J86-3001,0,0.743047,"y restricting our attention to the tuple selection task, a commonly studied type of information-seeking dialogue in which the user’s goal is to select a tuple from a table. A relational database typically consists of multiple tables, and each table can satisfy different user goals. Given a database composed of multiple tables, an open dialogue system calculates which tables are larger, have more natural language content, and greater connectivity to other tables. We refer to these tables as candidate dialogue foci. This notion of candidate focus for a dialogue is similar to focus of attention (Grosz & Sidner, 1986) in that information-seeking dialogues can be segmented to reflect the table both participants focus their attention on at a given time. We denote the task of identifying candidate foci in a relational database as focus discovery. ODDMER’s focus discovery module returns an ordered list of candidate foci, the focal summary. The highest ranked focus is the most relevant to basic user goals, those goals that pertain to the most information-rich and intelligible table. For our tuple-selection task, the highest-ranked focus is the table from which the system predicts a user will most likely want to"
N13-1128,W09-3922,0,0.0607204,"Missing"
N13-1128,W12-1635,1,0.900652,"Missing"
N13-1128,P10-1008,0,0.0726346,"Missing"
N13-1128,polifroni-walker-2006-learning,0,0.023515,"Missing"
N13-1128,P08-1055,0,0.0177094,"ormation transfer. BOOK clearly dominates. Before information transfer there is more verbal information V in BOOK than in the next four tables combined. After information transfer reaches a steady state, its connectivity with other tables increases their verbality, but BOOK remains the leading candidate by a large margin. HEADING’s verbality decreases sharply after information transfer because of its low connectivity. The focus discovery module returns a focal summary, a list of the top k tables with the highest verbalities. The focal summary is similar to the intensional summary described by Polifroni and Walker (2008), which communicates the contents of a single table to a user. A key difference is that the focal summary pertains to the entire database. ODDMER is currently limited to the table and attribute labels assigned by the database designer. For example, the Heiskell ‘BOOK’ table was labeled ‘BIBREC’ by the database designers, for Bibliographic Record. We renamed this table prior to its use as a backend for the dialogue system. But ODDMER has no way to determine if labels are meaningful. In many cases there is a disincentive towards meaningful table names to avoid conflicts with SQL keywords. Future"
N13-1128,2005.sigdial-1.6,0,0.039932,"ers with all the information needed to carry out a dialogue. Such a completely knowledgeable user can unrealistically describe objects in the domain with the same vocabulary that the system uses. This means it does not experience the vocabulary problem. To test vocabulary selection, we simulate users with incomplete domain knowledge. In contrast with Selfridge and Heeman (2010), our limited-knowledge users are more likely to know some attributes than others. User simulation is often used to stand in for real user dialogues, but it is a concern whether the dialogues are sufficiently realistic (Schatzmann, Georgila, & Young, 2005). Here, we use simulation to exercise each dialogue system with a large number of cases in a highly controlled fashion. For simulated users, we can specify exactly what each user knows about the domain, thus simulation makes it possible to hold everything else the same while contrasting users with complete versus incomplete domain knowledge. We view this as a preliminary step towards evaluation with real users, which we hope to do in future work. Heiskell Grocery Eve C/N/R 15.9 ± 0.4 11.1 ± 0.2 16.3 ± 0.4 C/N/S 9.0 ± 0.0 9.0 ± 0.0 9.0 ± 0.0 C/V/R 11.5 ± 0.2 11.1 ± 0.3 10.6 ± 0.1 C/V/S 9.5 ± 0"
N13-1128,P10-1019,0,0.0289135,"d less effective on Eve than on Heiskell. In Eve the verbality scores of the top tables were close together without one outstanding focus candidate. 4.1 Simulating the vocabulary problem A typical evaluation of a spoken dialogue system provides users with all the information needed to carry out a dialogue. Such a completely knowledgeable user can unrealistically describe objects in the domain with the same vocabulary that the system uses. This means it does not experience the vocabulary problem. To test vocabulary selection, we simulate users with incomplete domain knowledge. In contrast with Selfridge and Heeman (2010), our limited-knowledge users are more likely to know some attributes than others. User simulation is often used to stand in for real user dialogues, but it is a concern whether the dialogues are sufficiently realistic (Schatzmann, Georgila, & Young, 2005). Here, we use simulation to exercise each dialogue system with a large number of cases in a highly controlled fashion. For simulated users, we can specify exactly what each user knows about the domain, thus simulation makes it possible to hold everything else the same while contrasting users with complete versus incomplete domain knowledge."
N13-1128,W06-1304,0,0.193522,"abase contents for users. Neither of these works considers how to choose attributes or find which domain entities are likely objects of dialogue goals. Chotimongkol and Rudnicky (2008) use unsupervised learning to automatically acquire task-specific information from a corpus of in-domain human-human dialogue transcripts. They require a large corpus whereas we need only the underlying database. The vocabulary problem has received relatively little attention in dialogue research, and no method to automatically identify intelligible constraints has been previously demonstrated. Demberg and Moore (2006) choose constraints with a user model that records user importance, such as ‘price’ for a student in a restaurant domain. They require a manually crafted user model and must match model to user. Polifroni and Walker (2006) use attribute entropy to order system initiative prompts, but assume that both the table and the relevant attributes are known a priori. Varges, Weng, and PonBarry (2006) develop a WOZ system in which a wizard recommends to real users what constraint to provide that will best narrow down results. Each of these works assumes all constraints are intelligible. Two recent works"
P01-1066,H92-1006,0,0.0575335,"Missing"
P01-1066,H01-1015,1,0.596513,"ask an utterance contributes to separately from what speech act function it serves. Thus, a central aspect of DATE is that it makes distinctions within three orthogonal dimensions of utterance classification: (1) a SPEECH - ACT dimension; (2) a TASK - SUBTASK dimension; and (3) a CONVERSATIONAL - DOMAIN dimension. We believe that these distinctions are important for using such a scheme for evaluation. Figure 1 shows a COMMUNICATOR dialogue with each system utterance classified on these three dimensions. The tagset for each dimension are briefly described in the remainder of this section. See (Walker and Passonneau, 2001) for more detail. 3.1 Speech Acts In DATE, the SPEECH - ACT dimension has ten categories. We use familiar speech-act labels, such as OFFER, REQUEST- INFO, PRESENT- INFO, AC KNOWLEDGE, and introduce new ones designed to help us capture generalizations about communicative behavior in this domain, on this task, given the range of system and human behavior we see in the data. One new one, for example, is STATUS - REPORT. Examples of each speech-act type are in Figure 2. Speech-Act REQUEST- INFO PRESENT- INFO OFFER ACKNOWLEDGE STATUS - REPORT EXPLICITCONFIRM IMPLICITCONFIRM INSTRUCTION APOLOGY OPEN"
P01-1066,C92-1054,1,0.577698,"n utterance is about. Each speech act can occur in any of three domains of discourse described below. The ABOUT- TASK domain is necessary for evaluating a dialogue system’s ability to collaborate with a speaker on achieving the task goal of making reservations for a specific trip. It supports metrics such as the amount of time/effort the system takes to complete a particular phase of making an airline reservation, and any ancillary hotel/car reservations. The ABOUT- COMMUNICATION domain reflects the system goal of managing the verbal channel and providing evidence of what has been understood (Walker, 1992; Clark and Schaefer, 1989). Utterances of this type are frequent in human-computer dialogue, where they are motivated by the need to avoid potentially costly errors arising from imperfect speech recognition. All implicit and explicit confirmations are about communication; See Figure 1 for examples. The SITUATION - FRAME domain pertains to the goal of managing the culturally relevant framing expectations (Goffman, 1974). The utterances in this domain are particularly relevant in humancomputer dialogues because the users’ expectations need to be defined during the course of the conversation. Ab"
P01-1066,P95-1016,0,\N,Missing
P01-1066,P98-1052,0,\N,Missing
P01-1066,C98-1051,0,\N,Missing
P10-2013,W09-3021,1,0.0814968,"is an XML serialization of the LAF abstract model of annotations, which consists of a directed graph decorated with feature structures providing the annotation content. GrAF’s primary role is to serve as a “pivot” format for transducing among annotations represented in different formats. However, because the underlying data structure is a graph, the GrAF representation itself can serve as the basis for analysis via application of 3.1 WordNet Sense Annotations A focus of the MASC project is to provide corpus evidence to support an effort to harmonize sense distinctions in WordNet and FrameNet (Baker and Fellbaum, 2009), (Fellbaum and Baker, to appear). The WordNet and FrameNet teams have selected for this purpose 100 common polysemous words whose senses they will study in detail, and the MASC team is annotating occurrences of these words in the MASC. As a first step, fifty occurrences of each word are annotated using the WordNet 3.0 inventory and analyzed for problems in sense assignment, after which the WordNet team may make modifications to the inventory if needed. The revised inventory (which will be released as part of WordNet 3.1) is then used to annotate 1000 occurrences. Because of its small size, MA"
P10-2013,passonneau-etal-2010-word,1,0.87312,"Missing"
P10-2013,W07-1501,1,0.76262,"maximal benefit from the semantic information provided by these resources, the entire corpus is also annotated and manually validated for shallow parses (noun and verb chunks) and named entities (person, location, organization, date and time). Several additional types of annotation have either been contracted by the MASC project or contributed from other sources. The 220K words of MASC I and II include seventeen different types of linguistic annotation4 , shown in Table 2. All MASC annotations, whether contributed or produced in-house, are transduced to the Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) defined by ISO TC37 SC4’s Linguistic Annotation Framework (LAF) (Ide and Romary, 2004). GrAF is an XML serialization of the LAF abstract model of annotations, which consists of a directed graph decorated with feature structures providing the annotation content. GrAF’s primary role is to serve as a “pivot” format for transducing among annotations represented in different formats. However, because the underlying data structure is a graph, the GrAF representation itself can serve as the basis for analysis via application of 3.1 WordNet Sense Annotations A focus of the MASC project is to provide"
P10-2013,ide-etal-2008-masc,1,0.866054,"Missing"
P10-2013,ide-etal-2010-anc2go,1,0.809614,"Missing"
P10-2013,J93-2004,0,0.0393414,"t of Computer Science Vassar College Poughkeepsie, NY, USA ide@cs.vassar.edu Collin Baker International Computer Science Institute Berkeley, California USA collinb@icsi.berkeley.edu Christiane Fellbaum Princeton University Princeton, New Jersey USA fellbaum@princeton.edu Rebecca Passonneau Columbia University New York, New York USA becky@cs.columbia.edu Abstract teen million word Open American National Corpus annotations are largely unvalidated. The most well-known multiply-annotated and validated corpus of English is the one million word Wall Street Journal corpus known as the Penn Treebank (Marcus et al., 1993), which over the years has been fully or partially annotated for several phenomena over and above the original part-of-speech tagging and phrase structure annotation. The usability of these annotations is limited, however, by the fact that many of them were produced by independent projects using their own tools and formats, making it difficult to combine them in order to study their inter-relations. More recently, the OntoNotes project (Pradhan et al., 2007) released a one million word English corpus of newswire, broadcast news, and broadcast conversation that is annotated for Penn Treebank sy"
P10-2013,W09-2402,1,0.836095,"Missing"
P10-2013,W03-0804,1,\N,Missing
P13-1086,W11-0705,1,\N,Missing
P13-1086,N12-1086,0,\N,Missing
P13-1086,C04-1051,0,\N,Missing
P13-1086,W06-0301,0,\N,Missing
P13-1086,E09-1004,0,\N,Missing
P13-1086,P11-1144,0,\N,Missing
P13-1086,P07-1124,0,\N,Missing
P13-1086,P02-1034,0,\N,Missing
P13-1086,E06-1015,0,\N,Missing
P13-1086,W12-3716,0,\N,Missing
P13-1086,D11-1121,0,\N,Missing
P13-1086,D09-1012,0,\N,Missing
P13-1086,N09-1031,0,\N,Missing
P13-2026,N03-1020,0,0.0350977,"l., 1998; Burstein, 2003). The pyramid method (Nenkova and Passonneau, 2004), was inspired in part by work in reading comprehension that scores content using human annotation (Beck et al., 1991). An alternate line of research attempts to replicate human reading comprehension. An automated tool to read and answer questions relies on abductive reasoning over logical forms extracted from text (Wellner et al., 2006). One of the performance issues is resolving meanings of words: removal of WordNet features degraded performance. The most widely used automated content evaluation is ROUGE (Lin, 2004; Lin and Hovy, 2003). It relies on model summaries, and depends on ngram overlap measures of different types. Because of its dependence on strings, it performs better with larger sets of model summaries. In contrast to ROUGE, pyramid scoring is robust with as few as four or five model summaries (Nenkova and Passonneau, 2004). A fully automated approach to evaluation for ranking systems that requires no model summaries incorporates latent semantic distributional similarities across words (Louis and Nenkova, 2009). The authors note, however, it does not perform well on individual summaries. 3 4 Approach: Dynamic Pr"
P13-2026,W04-1013,0,0.16589,"rstein et al., 1998; Burstein, 2003). The pyramid method (Nenkova and Passonneau, 2004), was inspired in part by work in reading comprehension that scores content using human annotation (Beck et al., 1991). An alternate line of research attempts to replicate human reading comprehension. An automated tool to read and answer questions relies on abductive reasoning over logical forms extracted from text (Wellner et al., 2006). One of the performance issues is resolving meanings of words: removal of WordNet features degraded performance. The most widely used automated content evaluation is ROUGE (Lin, 2004; Lin and Hovy, 2003). It relies on model summaries, and depends on ngram overlap measures of different types. Because of its dependence on strings, it performs better with larger sets of model summaries. In contrast to ROUGE, pyramid scoring is robust with as few as four or five model summaries (Nenkova and Passonneau, 2004). A fully automated approach to evaluation for ranking systems that requires no model summaries incorporates latent semantic distributional similarities across words (Louis and Nenkova, 2009). The authors note, however, it does not perform well on individual summaries. 3 4"
P13-2026,P98-1032,0,0.0336665,"two criteria, and has superior performance on the third to other methods. has been incorporated with a suite of NLP metrics to assess students’ strategies for reading comprehension using think-aloud protocols (BoonthumDenecke et al., 2011). The resulting tool, and similar assesment tools such as Coh-Metrix, assess aspects of readability of texts, such as coherence, but do not assess students’ comprehension through their writing (Graesser et al., 2004; Graesser et al., 2011). E-rater is an automated essay scorer for standardized tests such as GMAT that also relies on a suite of NLP techniques (Burstein et al., 1998; Burstein, 2003). The pyramid method (Nenkova and Passonneau, 2004), was inspired in part by work in reading comprehension that scores content using human annotation (Beck et al., 1991). An alternate line of research attempts to replicate human reading comprehension. An automated tool to read and answer questions relies on abductive reasoning over logical forms extracted from text (Wellner et al., 2006). One of the performance issues is resolving meanings of words: removal of WordNet features degraded performance. The most widely used automated content evaluation is ROUGE (Lin, 2004; Lin and"
P13-2026,D09-1032,0,0.0530231,"rdNet features degraded performance. The most widely used automated content evaluation is ROUGE (Lin, 2004; Lin and Hovy, 2003). It relies on model summaries, and depends on ngram overlap measures of different types. Because of its dependence on strings, it performs better with larger sets of model summaries. In contrast to ROUGE, pyramid scoring is robust with as few as four or five model summaries (Nenkova and Passonneau, 2004). A fully automated approach to evaluation for ranking systems that requires no model summaries incorporates latent semantic distributional similarities across words (Louis and Nenkova, 2009). The authors note, however, it does not perform well on individual summaries. 3 4 Approach: Dynamic Programming Previous work observed that assignment of SCUs to a target summary can be cast as a dynamic programming problem (Harnly et al., 2005). The method presented there relied on unigram overlap to score the closeness of the match of each eligible substring in a summary against each SCU in the pyramid. It returned the set of matches that yielded the highest score for the summary. It produced good rankings across summarization tasks, but assigned scores much lower than those assigned by hum"
P13-2026,N04-1019,1,0.93148,"l objects and substances in the universe are made of Matter is identified as being present everywhere and in all substances Matter is all the objects and substances around us 4 Figure 1: A Summary Content Unit (SCU) Introduction The pyramid method is an annotation and scoring procedure to assess semantic content of summaries in which the content units emerge from the annotation. Each content unit is weighted by its frequency in human reference summaries. It has been shown to produce reliable rankings of automated summarization systems, based on performance across multiple summarization tasks (Nenkova and Passonneau, 2004; Passonneau, 2010). It has also been applied to assessment of oral narrative skills of children (Passonneau et al., 2007). Here we show its potential for assessment of the reading comprehension of community college students. We then present a method to automate pyramid scores based on latent semantics. The pyramid method depends on two phases of manual annotation, one to identify weighted content units in model summaries written by proficient humans, and one to score target summaries against the models. The first annotation phase yields Summary Content Units (SCUs), sets of text fragments tha"
P13-2026,P12-1091,1,0.926712,"e, we use a Gaussian kernel density estimator to provide a non-parametric estimation of the probability densities of scores assigned by each of the similarity methods to the manually identified SCUs. We then select five threshold values corresponding to those for which the inverse cumulative density function (icdf) is equal to 0.05, 0.10, 0.15, 0.20 and 0.25. Each threshold represents the probability that a manually identified SCU will be missed. Latent Vector Representations To represent the semantics of SCUs and candidate substrings of target summaries, we applied the latent vector model of Guo and Diab (2012).1 Guo and Diab find that it is very hard to learn a 100dimension latent vector based only on the limited observed words in a short text. Hence they include unobserved words that provide thousands more features for a short text. This produces more accurate results for short texts, which makes the method suitable for our problem. Weighted matrix factorization (WMF) assigns a small weight for missing words so that latent semantics depends largely on observed words. A 100-dimension latent vector representation was learned for every span of contiguous words within sentence bounds in a target summa"
P13-2026,C98-1032,0,\N,Missing
P15-1153,P13-1020,0,0.0682145,"s. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) o"
P15-1153,J05-3002,0,0.423913,"compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of"
P15-1153,P14-1086,0,0.0209135,"mmary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely,"
P15-1153,P11-1049,0,0.225126,"nces may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest a"
P15-1153,P13-1121,0,0.0381358,"n in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based a"
P15-1153,P14-1085,0,0.00660705,"011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more compl"
P15-1153,C04-1057,0,0.0172214,"011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the senten"
P15-1153,D08-1019,0,0.237574,"els cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Lingu"
P15-1153,C10-1037,0,0.00615543,"different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather information across sentence boundary, and hence have the potential to cover more content in a more concise manner. In this paper, we propose an abstractive MDS framework that can construct new sentences by 1587 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th"
P15-1153,C10-1039,0,0.0485824,"on to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to su"
P15-1153,W11-1608,0,0.254617,"; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also expl"
P15-1153,P12-2069,0,0.0121625,"l., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, suc"
P15-1153,W09-1802,0,0.203135,"eously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based appro"
P15-1153,W00-0405,0,0.425057,"ch as “sent the boys outside”, “authorities said”, etc. In addition, the VP “killing five girls” of the original sentence with ID 64 is also excluded since it has significant redundancy with the summary sentence with ID 1. 5 Related Work Existing multi-document summarization (MDS) works can be classified into three categories: 1594 extraction-based approaches, compression-based approaches, and abstraction-based approaches. Extraction-based approaches are the most studied of the three. Early studies mainly followed a greedy strategy in sentence selection (C ¸ elikyilmaz and Hakkani-T¨ur, 2011; Goldstein et al., 2000; Wan et al., 2007). Each sentence in the documents is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassilogl"
P15-1153,P12-1091,1,0.836203,"rkshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different from those of TAC, we employ the automated version of pyramid proposed in (Passonneau et al., 2013). The automated pyramid scoring procedure relies on distributional semantics to assign SCUs to a target summary. Specifically, all n-grams within sentence bounds are extracted, and converted into 100 dimension latent topical vectors via a weighted matrix factorization model (Guo and Diab, 2012). Similarly, the contributors and the label of an SCU are transformed into 100 dimensional vector representations. An SCU is assigned to a summary if there exists an n-gram such that the similarity score between the SCU low dimensional vector and the n-gram low dimensional vector exceeds a threshold. Passonneau et al. (2013) showed that the distributional similarity based method produces automated scores that correlate well with manual pyramid scores, yielding more accurate pyramid scores than string matching based automated methods (Harnly et al., 2005). In this paper, we adopt the same setti"
P15-1153,A00-2024,0,0.188299,"is end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whose fragments come from different source sentences. One important work developed by Barzilay and McKeown (2005) employed sentence fusion, followed by (Filippova and Strube, 2008; Filippova, 2010). These works first conduct clustering on sentences to compute the salience of topical themes. Then, sentence fusion is applied within each cluster of related sentences to generate a new sentence containing common information units of the sentences. The abstractive-based approaches gather informat"
P15-1153,W14-1504,0,0.00804706,"Missing"
P15-1153,P03-1054,0,0.0033611,"en we formulate the sentence generation task as an optimization problem, and design constraints. In the end, we perform several post-processing steps to improve the order and the readability of the generated sentences. 2.1 Phrase Salience Calculation The first component decomposes the sentences in documents into a set of noun phrases (NPs) derived from the subject parts of a constituency tree and a set of verb-object phrases (VPs), representing potential key concepts and key facts, respectively. These phrases will serve as the basic elements for sentence generation. We employ Stanford parser (Klein and Manning, 2003) to obtain a constituency tree for each input sentence. After that, we extract NPs and VPs from the tree as follows: (1) The NPs and VPs that are the direct children of the sentence node (repre1588 sented by the S node) are extracted. (2) VPs (NPs) in a path on which all the nodes are VPs (NPs) are also recursively extracted and regarded as having the same parent node S. Recursive operation in the second step will only be carried out in two levels since the phrases in the lower levels may not be able to convey a complete fact. Take the tree in Figure 1 as an example, the corresponding sentence"
P15-1153,J13-4004,0,0.00850382,"ts are jointly considered. 2.2.1 Compatibility Relation Compatibility relation is designed to indicate whether an NP and a VP can be used to form a new sentence. For example, the NP “Police” from another sentence should not be the subject of the VP “sent the boys outside” extracted from Figure 1. We use some heuristics to find compatibility, and then expand the compatibility relation to more phrases by extracting coreference. To find coreference NPs (different mentions for the same entity), we first conduct coreference resolution for each document with Stanford coreference resolution package (Lee et al., 2013). We adopt those resolution rules that are able to achieve high quality and address our need for summarization. In particular, Sieve 1, 2, 3, 4, 5, 9, and 10 in the package are used. A set of clusters are obtained and each cluster contains the mentions that refer to the same entity in a document. The clusters from different documents in the same topic are merged by matching the named entities. After merging, the mentions that are not NPs extracted in the phrase extraction step are removed in each cluster. Two NPs in the same cluster are determined as alternative of each other. To find alternat"
P15-1153,N10-1134,0,0.119631,"s is firstly assigned a salience score. Then, sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant"
P15-1153,N03-1020,0,0.168226,"as to recognize alternate realizations of the same meaning. Different weights are assigned to SCUs based on their frequency in model summaries. A weighted inventory of SCUs named a pyramid is created, which constitutes a resource for investigating alternate realizations of the same meaning. Such property makes pyramid method more suitable to evaluSystem Our 22 43 17 Auto-pyr (Th: .6) 0.905 0.878 0.875 0.860 Auto-pyr (Th: .65) 0.793 0.775 0.756 0.741 Rank in TAC 2011 NA 1 2 3 Table 2: Comparison with the top 3 systems in TAC 2011. ate summaries. Another widely used evaluation metric is ROUGE (Lin and Hovy, 2003) and it evaluates summaries from word overlapping perspective. Because of the strict string matching, it ignores the semantic content units and performs better when larger sets of model summaries are available. In contrast to ROUGE, pyramid scoring is robust with as few as four model summaries (Nenkova and Passonneau, 2004). Therefore, in recent summarization evaluation workshops such as TAC, the pyramid is used as the major metric. Since manual pyramid evaluation is timeconsuming, and the exact evaluation scores are not reproducible especially when the assessors for our results are different"
P15-1153,W03-1101,0,0.416943,"the Hong Kong Special Administrative Region, China (Project Codes: 413510 and 14203414). The work was done when Weiwei Guo was in Columbia University summarization systems adopt the extractionbased approach which selects some original sentences from the source documents to create a short summary (Erkan and Radev, 2004; Wan et al., 2007). However, the restriction that the whole sentence should be selected potentially yields some overlapping information in the summary. To this end, some researchers apply compression on the selected sentences by deleting words or phrases (Knight and Marcu, 2000; Lin, 2003; Zajic et al., 2006; Harabagiu and Lacatusu, 2010; Li et al., 2015), which is the compression-based method. Yet, these compressive summarization models cannot merge facts from different source sentences, because all the words in a summary sentence are solely from one source sentence. In fact, previous investigations show that human-written summaries are more abstractive, which can be regarded as a result of sentence aggregation and fusion (Cheung and Penn, 2013; Jing and McKeown, 2000). Some works, albeit less popular, have studied abstraction-based approach that can construct a sentence whos"
P15-1153,W09-1801,0,0.0671521,"he redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that e"
P15-1153,P14-1115,0,0.0357106,"Missing"
P15-1153,N04-1019,1,0.536063,"f concepts and facts represented by NPs and VPs from the input documents. A salience score is computed for each phrase by exploiting redundancy of the document content in a global manner. The second component constructs new sentences by selecting and merging phrases based on their salience scores, and ensures the validity of new sentences using a integer linear optimization model. The contribution of this paper is two folds. (1) We extract NPs/VPs from constituency trees to represent key concepts/facts, and merge them to construct new sentences, which allows more summary content units (SCUs) (Nenkova and Passonneau, 2004) to be included in a sentence by breaking the original sentence boundaries. (2) The designed optimization framework for addressing the problem is unique and effective. Our optimization algorithm simultaneously selects and merges a set of phrases that maximize the number of covered SCUs in a summary. Meanwhile, since the basic unit is phrases, we design compatibility relations among NPs and VPs, as well as other optimization constraints, to ensure that the generated sentences contain correct facts. Compared with the sentence fusion approaches that compute salience scores of sentence clusters, o"
P15-1153,P14-1084,0,0.113882,"ntly, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al., 2014; Cao et al., 2015) utilized deep learning techniques to tackle some summarization tasks. 6 Conclusions and Future Work We propose an abstractive MDS framework that constructs new sentences by exploring more finegrained syntactic units, namely, noun phrases and verb phrases. The designed optimization framework operates on the summary level so that more complementary semantic content units can be incorporated. The phrase selection and merging is done simultaneously to achieve global optimal. Meanwhile, the cons"
P15-1153,J11-4007,0,0.0159621,"the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Chri"
P15-1153,E12-1023,0,0.123518,", sentence selection is performed by greedily selecting the sentence with the largest salience score among the remaining ones. The redundancy is controlled during the selection by penalizing the remaining ones according to their similarity with the selected sentences. An obvious drawback of such greedy strategy is that it is easily trapped in local optima. Later, unified models are proposed to conduct sentence selection and redundancy control simultaneously (McDonald, 2007; Filatova and Hatzivassiloglou, 2004; Yih et al., 2007; Gillick et al., 2007; Lin and Bilmes, 2010; Lin and Bilmes, 2012; Sipos et al., 2012). However, extraction-based approaches are unable to evaluate the salience and control the redundancy on the granularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integra"
P15-1153,P10-1058,0,0.0225702,"ularity finer than sentences. Thus, the selected sentences may still contain unimportant or redundant phrases. Compression-based approaches have been investigated to alleviate the above limitation. As a natural extension of the extractive method, the early works adopted a two-step approach (Lin, 2003; Zajic et al., 2006; Gillick and Favre, 2009). The first step selects the sentences, and the second step removes the unimportant or redundant units from the sentences. Recently, integrated models have been proposed that jointly conduct sentence extraction and compression (Martins and Smith, 2009; Woodsend and Lapata, 2010; Almeida and Martins, 2013; Berg-Kirkpatrick et al., 2011; Li et al., 2015). Note that our model also jointly conducts phrase selection and phrase merging (new sentence generation). Nonetheless, compressive methods are unable to merge the related facts from different sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items ("
P15-1153,D12-1022,0,0.286119,"n”, “walked into an Amish school, sent the boys outside and tied up and shot the girls, killing three of them”, “walked into an Amish school”, “sent the boys outside”, and “tied up and shot the girls, killing three of them”. 1 Because of the recursive operation, the extracted phrases may have overlaps. Later, we will show how to avoid such overlapping in phrase selection. A salience score is calculated for each phrase to indicate its importance. Different types of salience can be incorporated in our framework, such as position-based method (Yih et al., 2007), statistical feature based method (Woodsend and Lapata, 2012), concept-based method (Li et al., 2011), etc. One key characteristic of our approach is that the considered basic units are phrases instead of sentences. Such finer granularity leaves more room for better global salience score by potentially covering more distinct facts. In our implementation, we adopt a concept-based weight incorporating the position information. The concept set is designated to be the union set of unigrams, bigrams, and named entities in the documents. We remove stopwords and perform lemmatization before extracting unigrams and bigrams. The position-based term frequency is"
P15-1153,I08-1016,0,0.0590528,"sentences. On the other hand, abstraction-based approaches can generate new sentences based on the facts from different source sentences. In addition to the previously mentioned sentence fusion work, new directions have been explored. Researchers developed an information extraction based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a l"
P15-1153,P14-1087,0,0.0220839,"tion based approach that extracts information items (Genest and Lapalme, 2011) or abstraction schemes (Genest and Lapalme, 2012) as components for generating sentences. Summary revision was also investigated to improve the quality of automatic summary by rewriting the noun phrases or people references in the summaries (Nenkova, 2008; Siddharthan et al., 2011). Sentence generation with word graph was applied for summarizing customer opinions and chat conversations (Ganesan et al., 2010; Mehdad et al., 2014). Recently, the factors of information certainty and timeline in MDS task were explored (Ng et al., 2014; Wan and Zhang, 2014; Yan et al., 2011). Researchers also explored some variants of the typical MDS setting, such as query-chain focused summarization that combines aspects of update summarization and query-focused summarization (Baumel et al., 2014), and hierarchical summarization that scales up MDS to summarize a large set of documents (Christensen et al., 2014). A data-driven method for mining sentence structures on large news archive was proposed and utilized to summarize unseen news events (Pighin et al., 2014). Moreover, some works (Liu et al., 2012; K˚ageb¨ack et al., 2014; Denil et al"
P15-1153,P13-2026,1,\N,Missing
P87-1003,H86-1012,0,0.044198,"form itself. When an unbounded interval is located with respect to a particular point in time, it is assumed to extend indefinitely in both directions around t h a t time. In 1), at least part of the interval for which the predication l o w ( p a t |e n t ( [ p r e s s u r e l ] ) ) is asserted to hold is located in the past. However, this interval may or may not end prior to the present. The u n b o u n d e d property of the interval can be illustrated more precisely by examining the relationship between the predication and the temporal adverbial ed by PUNDIT's referenceresolution component (Dahl, 1986). SThough a situation is somethingquite different for Barwise and Perry (1983), they take n similar view of the role of a particular space-time location in tokenising a situation type (of. esp. pp. 51ff). Xlln general, temporal adverbials can modify an existing component of temporal structure or add components of temporal structure. 18 modifying it in example 2): 2) The pressure was low at 08:00. This sentence asserts t h a t the s t a t e of low(patient([pressvwel])) holds at 08:00 and possibly prior and subsequent to 08:00. T h a t is, the sentence would be true if the pressure were low for"
P87-1003,P86-1004,0,\N,Missing
P87-1003,H86-1011,0,\N,Missing
P87-1003,J88-2005,1,\N,Missing
P87-1019,P81-1029,1,0.80468,"Missing"
P87-1019,T87-1032,1,\N,Missing
P87-1019,P86-1004,1,\N,Missing
P87-1019,H86-1011,1,\N,Missing
P88-1002,A88-1007,1,0.884391,"Missing"
P88-1002,J81-2002,0,\N,Missing
P88-1002,J83-3003,0,\N,Missing
P88-1002,J81-4004,0,\N,Missing
P88-1002,J83-3002,0,\N,Missing
P88-1002,P86-1004,1,\N,Missing
P88-1002,H86-1011,1,\N,Missing
P88-1002,P87-1019,1,\N,Missing
P88-1002,P87-1003,1,\N,Missing
P88-1002,C80-1027,0,\N,Missing
P88-1002,J88-2005,1,\N,Missing
P88-1002,A83-1016,0,\N,Missing
P89-1007,P86-1031,0,0.381559,"the antecedent and the target pronoun are syntactic subjects; =~ IT • all other contexts. =~ T H A T Parallelism has sometimes been suggested as an organizing factor across clauses. It is certainly a strong stylistic device, but did not make a strong enough independent contribution to the statistical model to be included as a distinct variable. To repeat, the crucial factor was found to be that both expressions were subjects, not that both had the same grammatical function in their respective clauses. In §4.1 I will review the relationship of these results to the centering literature [I] [5] [6]. Full 3 - w a y m o d e l . Table 2 displays the data in a finer-grained two-dimensionai x-square table in order to show separately all 4 of the possible outcomes, i.e., it or that as a subject or non-subject. In this table, the row headings represent the antecedent's form and grammatical role; the column headings represent the lexical choice and grammatical role of the subsequent pronominal expression. Each cell of the table indicates the absolute frequency, the expected frequency given a nonchance distribution, and the cell x-square, with the latter in boldface type to indicate the signific"
P89-1007,P88-1014,0,0.406096,"nd if the latter, whether the phrase was syntactically more clause-like or more nounlike. Both variables point up the significance of the temporal dimension of discourse in two ways. The first has to do with the evanescence of surface syntactic f o r m - - t h e two features pertaining to the grammatical means used to refer to entities are relevant only for a short time, namely across two co-references [17]. The second has to do with the dual nature of referring expressions--as noted by Isard they are constrained by the prior context but immediately alter the contezt and become part of it [3] [18]. In §4 I discuss how the contrast between the definite and demonstrative pronouns is constrained by the local discourse context, and 51 • it,but not that, can be used non-referentially how the constraining features of the local context in combination with the lexical contrast provides evidence about modelling the attentional state of discourse. 2 ( it/*that is raining; it/*that is hard to find an honest politician) These differences, though they m a y ultimately pertain to the phenomena presented here, won't be discussed further. In general, that can occur with the same syntactic types of ant"
P89-1007,P83-1007,0,0.399461,"which is suppressed (-t). T h e initial states in boldface indicate for each antecedent type which of the two g r a m m a t i c a l role states was more likely, subject or non-subject. Absence of final states for the n o n N P - S u b j initial state indicates t h a t this set of contexts is extremely rare. In the following section, I discuss the relation of these events to an abstract model of attentional state. 4 Definite/Demonstrative Pronouns and Centering T h e centering model predicts that an utterance will contain a referent t h a t is distinguished as the backward looking center (Cb) [1], and that if the Cb of an utterance is coreferentlai with the Cb of the prior utterance, it will be pronominallzed [1]. K a m e y a m a [6] proposes t h a t there are two means for retaining a discourse entity as the Cb, canonical center-retention--both references in subject r o l e - - a n d non-canonical center retention--neither reference in subject role. As shown in Fig. 1, the most enhanced context for lexical choice of it (context 1) was where both the pronoun and its pronominal antecedent were subjects, i.e., canonical center-retention. T h e next most enhanced context for it (context"
P89-1007,J86-3001,0,0.0281115,"two features of the local context. In §3.2, I summarize the statistical results. They were strikingly clearcut, and provide confirmation that grammatical choices made by participants in a dialogue prior to a particular point in time correlate with lexical choice of either participant at that time. 1 Introduction Languages vary in the number and kinds of grammatical distinctions encoded in their nominal and pronominal systems. Language specific means for explicitly mentioning and re-mentioning discourse entities constrain what Grosz and Sidner refer to as the linguistic structure of discourse [2]. This in turn constrains the ways in which discourse participants can exploit linguistic structure for indicating or inferring attentional state. Attentional state, Grosz and Sidner's term for the dynamic representation of the participants' focus of attention [2], represents--among other thingswhich discourse entities are currently most salient. One function of attentional state is to help resolve pronominal references. English has a relatively impoverished set of definite pronouns in which gender is relevant only in the 3rd person singular, and where number---a fairly universal nominal categ"
P91-1009,P89-1007,1,0.696423,"Missing"
P91-1009,P83-1007,0,0.279352,"Missing"
P91-1009,P86-1031,0,\N,Missing
P91-1009,J86-3001,0,\N,Missing
P93-1010,P86-1031,1,0.957403,"he choice of the discourse antecedent of a temporal operator is subject to centering effects. We assume that each temporal operator in a sentence introduces a discourse reference time into the discourse context. We claim that this set of times constitutes a list of potential discourse reference times for the next sentence, which we'll later refer to as the temporal forwardlooking center (TCf), and that the position of a temporal operator in the logical form of the sentence affects the choice of the antecedent through structural parallelism (as a case of the propertysharing effect in centering [16]). We formalize the effect of surface structure on the choice of temporal antecedent by means of defeasible axioms. These axioms must be less specific than axioms encoding causal reasoning. We argue that the choice of discourse reference time is an instance of a general principle in defeasible reasoning, namely, the Penguin Principle [19] that chooses the most specific axiom applicable. We support our claims with data from the Brown corpus. In the next section, we review the three existing proposals most related to ours - - Webber [27], Lascarides and Oberlander [19], and Hwang and Schubert [1"
P93-1010,J88-2005,1,0.852327,"o Poesio D e p t . of C o m p u t e r S c i e n c e U n i v e r s i t y of R o c h e s t e r R o c h e s t e r , N Y 14627-0226 poesio©cs.rochester.edu (2)a. John went over (el) to Mary's house. b. On the way, he had (t2) stopped (t3) by the flower shop for some roses. (t3 -~ t2 ( = t l ) ) c. Unfortunately, they failed (t4) to cheer her up. (t3 -~ t l -~ t4) c'. He picked out (t4') 5 red ones, 3 white ones and 1 pale pink. (t3 -&lt; t4' -&lt; tl) (2c) and (2c') are alternative third sentences. Although both are in the simple past, and both evoke events of the same aspectual type (transition event [23]), they are interpreted differently. We refer to the contextually established time that a past tense is resolved against as the &quot;discourse reference time.&quot; A discourse reference time (tl) is introduced in (2a) with the event of John going to Mary's house at t l ) The past perfect in (2b) introduces two times: John's stopping at the flower shop (t3) precedes the time t2 (t3 -~ t2), and t2 is typically inferred to be equal to the time of going over to Mary's house (tl); hence t3 ~ tl. In (2c), the time of failing to cheer Mary (t4) is inferred to occur just after tl, whereas in the parallel vers"
P93-1010,J90-3001,0,0.0234575,"have been distinguished in centering. In this paper, we will use the following four types: Cb-retention, Cb-establishment, Cb-resumption, and NULL-transition. z We assume the following general picture of discourse processing. A discourse consists of a sequence of utterances uttl,..., uttn. The sentence grammar translates the content of each utterance utti into a (set of) surface logical form(s) containing unresolved anaphoric expressions and operators. We call it here a &quot;surface&quot; formula ¢i. This logical form is similar, in spirit, to Hwang and Schubert's [14] indexical formula and Alshawi's [2] quasi logicalform, whose main motivations are to represent that part of the sentence meaning independent from the particular discourse context. This &quot;baseline&quot; meaning representation acts as a clean interface to the pragmatic processing needed to resolve context-dependent expressions. Utterance interpretation takes place in a context, and outputs an updated context. Part of this dynamic context is the attentional state that represents the currently salient entities partially ordered by relative salience. In Cb-retention, the same entity is retained as the Cb: Cbi-1 = Cbi y£ NULL. In Cbestabli"
P93-1010,J88-2006,0,0.454969,"P a r k , C A 94025 megumi©ai.sri.com CENTERING Rebecca Passonneau D e p t . of C o m p u t e r S c i e n c e Columbia University N e w York, N Y 10027 becky¢cs.columbia.edu ically related, and if they are, what the relative order of the associated events is. The determinant factors have been argued to be discourse structure ([27] [14]), aspectual type ([61 [12] [17]), surface structure ([7] [14]), and commonsense knowledge ([19] [271 [13]). However, no account has adequately addressed all four factors. The problem in tense interpretation that we address is illustrated with Example (2) (from [27]). Abstract We present a semantic and pragmatic account of the anaphoric properties of past and perfect that improves on previous work by integrating discourse structure, aspectual type, surface structure and commonsense knowledge. A novel aspect of our account is that we distinguish between two kinds of temporal intervals in the interpretation of temporal operators - - discourse reference intervals and event intervals. This distinction makes it possible to develop an analogy between centering and temporal centering, which operates on discourse reference intervals. Our temporal propertysharing"
P93-1010,P87-1022,0,0.336244,"ered set of forward-looking centers Cfi comprising the entities realized in ¢i. A member of Cfi might (but need not) be the backward-looking center Cbi, the currently most salient entity. Centering has mainly been used to constrain how discourse anaphoric pronouns are processed; e.g., the centering rule [9] predicts that Cbl will be realized with a pronoun if Cbi=Cbi_l. 2 Also, when Cbi=Cbi-1 and both are realized by definite pronouns, it is predicted that both will be real3Cb-retention and Cb-establishment are due to Kameyama [15] [16]. These two roughly correspond to the three [10] and four [5] transition types proposed elsewhere. Cb-resumption captures Sidner's [26] use of a discourse focus stack in the potential focus list, and can be analogously formalized as a Cb stack within the Cf. NULL-transition has been implicit in Kameyama's work but has not been made an explicit transition type. 2Here we avoid the complication acknowledged in [11] that the two relevant utterances need not literally be adjacent. 72 course sequences from the Brown corpus [8], a heterogeneous corpus that should yield unbiased data. Each multi-sentence sequence contained one of two types of trigger sentences"
P93-1010,J88-2003,0,\N,Missing
P93-1010,P88-1012,0,\N,Missing
P93-1010,P83-1007,0,\N,Missing
P93-1010,J86-3001,0,\N,Missing
P93-1010,C69-7001,0,\N,Missing
P93-1010,C69-6902,0,\N,Missing
P93-1010,P92-1030,0,\N,Missing
P93-1020,P86-1021,0,0.0946445,"ningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even though this is a precondition for avoiding circularity in relating segments to linguistic phenomena. We present the results of a two part study on the reliability of human segmentation, and"
P93-1020,P92-1030,0,0.0216083,"utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even though this is a precondition for avoiding circularity in relating segments to linguistic phenomena. We present the results of a two part study on the reliability of human segmentation, and correlation with linguistic cues. We show that human subjects can reliably perform discourse segmentation using s"
P93-1020,W93-0216,1,0.780477,"consists of 20 narrative monologues about the same movie, taken from Chafe (1980) (N~14,000 words). The subjects were introductory psychology students at the University of Connecticut and volunteers solicited from electronic bulletin boards. Each narrative was segmented by 7 subjects. Subjects were instructed to identify each point in a narrative where the speaker had completed one communicative task, and began a new one. They were also instructed to briefly identify the speaker's intention associated with each segment. Intention was explained in common sense terms and by example (details in (Litman and Passonneau, 1993)). To simplify data collection, we did not ask subjects to identify the type of hierarchical relations among segments illustrated in Figure 1. In a pilot study we conducted, subjects found it difficult and time-consuming to identify non-sequential relations. Given that the average length of our narratives is 700 words, this is consistent with previous findings (Rotondo, 1984) that non-linear segmentation is impractical for naive subjects in discourses longer than 200 words. "" Since prosodic phrases were already marked in the transcripts, we restricted subjects to placing boundaries between pro"
P93-1020,J92-4007,0,0.0592915,"the majority of her subjects. Hearst developed a lexical algorithm based on information retrieval measurements to segment text, then qualitatively compared the results with the structures derived from her subjects, as well as with those produced by Morris and Hirst. Iwanska (1993) compares her segmentations of factual reports with segmentations produced using syntactic, semantic, and pragmatic information. We derive segmentations from our empirical data based on the statistiRELIABILITY The correspondence between discourse segments and more abstract units of meaning is poorly understood (see (Moore and Pollack, 1992)). A number of alternative proposals have been presented which directly or indirectly relate segments to intentions (Grosz and Sidner, 1986), RST relations (Mann et al., 1992) or other semantic relations (Polanyi, 1988). We present initial results of an investigation of whether naive subjects can reliably segment discourse using speaker intention as a criterion. Our corpus consists of 20 narrative monologues about the same movie, taken from Chafe (1980) (N~14,000 words). The subjects were introductory psychology students at the University of Connecticut and volunteers solicited from electronic"
P93-1020,J91-1002,0,0.240467,"he ladder, a n d picks s o m e m o r e pears. Figure 1: Discourse Segment Structure INTRODUCTION A discourse consists not simply of a linear sequence of utterances, 1 hut of meaningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even"
P93-1020,P84-1055,0,0.102942,"g . A - n d u m [ - ~ goes up the ladder, a n d picks s o m e m o r e pears. Figure 1: Discourse Segment Structure INTRODUCTION A discourse consists not simply of a linear sequence of utterances, 1 hut of meaningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'"
P93-1020,P92-1032,0,0.259873,"Missing"
P93-1020,J86-3001,0,0.931042,"Missing"
P93-1020,H92-1089,0,0.137812,"sistent with previous findings (Rotondo, 1984) that non-linear segmentation is impractical for naive subjects in discourses longer than 200 words. "" Since prosodic phrases were already marked in the transcripts, we restricted subjects to placing boundaries between prosodic phrases. In principle, this makes it more likely that subjects will agree on a given boundary than if subjects were completely unrestricted. However, previous studies have shown that the smallest unit subjects use in similar tasks corresponds roughly to a breath group, prosodic phrase, or clause (Chafe, 1980; Rotondo, 1984; Hirschberg and Grosz, 1992). Using smaller units would have artificially lowered the probability for agreement on boundaries. Figure 2 shows the responses of subjects at each potential boundary site for a portion of the excerpt from Figure 1. Prosodic phrases are numbered sequentially, with the first field indicating prosodic phrases with sentence-final contours, and the second 149 3.3 [.35+ [.35] a-nd] he- u-h [.3] p u t s his pears into the basket. l 6 SUBJECTS I NP, PAUSE 4.1 [I.0 [.5] U-hi a number of people are going by, CUE, PAUSE 4.2 [.35+ and [.35]] o n e is [1.15 urn/ / y o u k n o w / I don't know, 4.3 I c a n"
P93-1020,J93-3003,1,0.807764,"the country, m a y b e . i n u-m u-h t h e valley or s o m e t h i n g . A - n d u m [ - ~ goes up the ladder, a n d picks s o m e m o r e pears. Figure 1: Discourse Segment Structure INTRODUCTION A discourse consists not simply of a linear sequence of utterances, 1 hut of meaningful relations among the utterances. As in much of the literature on discourse processing, we assume that certain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little"
P93-1020,J88-2006,0,0.0212679,"rtain spans of utterances, referred to here as discourse segments, form coherent units. The segmental structure of discourse has been claimed to constrain and be constrained by disparate phenomena: cue phrases (Hirschberg and Litman, 1993; Gross and Sidner, 1986; Reichman, 1985; Cohen, 1984); lexical cohesion (Morris and Hirst, 1991); plans and intentions (Carberry, 1990; Litman and Allen, 1990; Gross and Sidner, 1986); prosody (Grosz and Hirschberg, 1992; Hirschberg and Gross, 1992; Hirschberg and Pierrehumbert, 1986); reference (Webber, 1991; Gross and Sidner, 1986; Linde, 1979); and tense (Webber, 1988; Hwang and Schubert, 1992; Song and Cohen, 1991). However, there is weak consensus on the nature of segments and the criteria for recognizing or generating them in a natural language processing system. Until recently, little empirical work has been directed at establishing obje'~ively verifiable segment boundaries, even though this is a precondition for avoiding circularity in relating segments to linguistic phenomena. We present the results of a two part study on the reliability of human segmentation, and correlation with linguistic cues. We show that human subjects can reliably perform disc"
P95-1015,P92-1032,0,0.149119,"Missing"
P95-1015,J86-3001,0,0.976697,"o methods for developing segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and"
P95-1015,P83-1007,0,0.0123328,"was correlated with discourse signaling uses of cue words in (Hirschberg and Litman, 1993); a potential correlation between discourse signaling uses of cue words and adjacency patterns between cue words was also suggested. Finally, (Litman, 1994) found that treating cue phrases individually rather than as a class enhanced the results of (iiirschberg and Litman, 1993). Passonneau (to appear) examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus. Resuits included an absence of correlation of segmental structure with centering (Grosz et al., 1983; Kameyama, 1986), and poor correlation with the contrast between full noun phrases and pronouns. As noted in (Passonneau and Litman, 1993), the NP features largely reflect Passonneau&apos;s hypotheses that adjacent utterances are more likely to contain expressions that corefer, or that are inferentially linked, if they occur within the same segment; and that a definite pronoun is more likely than a full NP to refer to an entity that was mentioned in the current segment, if not in the previous utterance. .63 .64 .72 .68 .06 .07 .12 .11 Table 1: Average human performance. range in length from 51 to"
P95-1015,P94-1002,0,0.413171,"Missing"
P95-1015,H92-1089,0,0.0894321,"segmentation algorithms from training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algori"
P95-1015,J93-3003,1,0.91293,"m training data: hand tuning and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algorithms to identify these units."
P95-1015,P86-1021,0,0.087275,"and machine learning. When multiple types of features are used, results approach human performance on an independent test set (both methods), and using cross-validation (machine learning). 1 that relies on enriched input features and multiple Introduction Many have argued that discourse has a global structure above the level of individual utterances, and that linguistic phenomena like prosody, cue phrases, and nominal reference are partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algorithms to identify these units. We found significant agreement amon"
P95-1015,P86-1031,0,0.0142102,"discourse signaling uses of cue words in (Hirschberg and Litman, 1993); a potential correlation between discourse signaling uses of cue words and adjacency patterns between cue words was also suggested. Finally, (Litman, 1994) found that treating cue phrases individually rather than as a class enhanced the results of (iiirschberg and Litman, 1993). Passonneau (to appear) examined some of the few claims relating discourse anaphoric noun phrases to global discourse structure in the Pear corpus. Resuits included an absence of correlation of segmental structure with centering (Grosz et al., 1983; Kameyama, 1986), and poor correlation with the contrast between full noun phrases and pronouns. As noted in (Passonneau and Litman, 1993), the NP features largely reflect Passonneau&apos;s hypotheses that adjacent utterances are more likely to contain expressions that corefer, or that are inferentially linked, if they occur within the same segment; and that a definite pronoun is more likely than a full NP to refer to an entity that was mentioned in the current segment, if not in the previous utterance. .63 .64 .72 .68 .06 .07 .12 .11 Table 1: Average human performance. range in length from 51 to 162 phrases (Avg."
P95-1015,P93-1041,0,0.328127,"Missing"
P95-1015,J93-4004,0,0.0377588,"Missing"
P95-1015,J92-4007,0,0.0307282,"Missing"
P95-1015,J91-1002,0,0.0792523,"Missing"
P95-1015,P93-1020,1,0.81315,"re partly conditioned by and reflect this structure (cf. (Grosz and Hirschberg, 1992; Grosz and Sidner, 1986; Hirschberg and Grosz, 1992; Hirschberg and Litman, 1993; Hirschberg and Pierrehumbert, 1986; Hobbs, 1979; Lascarides and Oberlander, 1992; Linde, 1979; Mann and Thompson, 1988; Polanyi, 1988; Reichman, 1985; Webber, 1991)). However, an obstacle to exploiting the relation between global structure and linguistic devices in natural language systems is that there is too little data about how they constrain one another. We have been engaged in a study addressing this gap. In previous work (Passonneau and Litman, 1993), we reported on a method for empirically validating global discourse units, and on our evaluation of algorithms to identify these units. We found significant agreement among naive subjects on a discourse segmentation task, which suggests that global discourse units have some objective reality. However, we also found poor correlation of three untuned algorithms (based on features of referential noun phrases, cue words, and pauses, respectively) with the subjects&apos; segmentations. In this paper, we discuss two methods for developing segmentation algorithms using multiple know*Bellcore did not sup"
P95-1015,P94-1050,0,0.212033,"Missing"
passonneau-2004-computing,M98-1022,0,\N,Missing
passonneau-2004-computing,J97-1005,1,\N,Missing
passonneau-2004-computing,M95-1005,0,\N,Missing
passonneau-2004-computing,N04-1019,1,\N,Missing
passonneau-2004-computing,J96-2004,0,\N,Missing
passonneau-2004-computing,kibble-van-deemter-2000-coreference,0,\N,Missing
passonneau-2006-measuring,W04-2709,0,\N,Missing
passonneau-2006-measuring,W04-3254,0,\N,Missing
passonneau-2006-measuring,passonneau-etal-2006-inter,1,\N,Missing
passonneau-2006-measuring,J97-1005,1,\N,Missing
passonneau-2006-measuring,W03-0508,0,\N,Missing
passonneau-2006-measuring,passonneau-2004-computing,1,\N,Missing
passonneau-2006-measuring,N04-1019,1,\N,Missing
passonneau-etal-2006-climb,A97-1054,0,\N,Missing
passonneau-etal-2006-climb,A97-1051,0,\N,Missing
passonneau-etal-2006-inter,W98-1507,0,\N,Missing
passonneau-etal-2006-inter,W04-2709,1,\N,Missing
passonneau-etal-2006-inter,W04-3254,0,\N,Missing
passonneau-etal-2006-inter,passonneau-2006-measuring,1,\N,Missing
passonneau-etal-2006-inter,passonneau-2004-computing,1,\N,Missing
passonneau-etal-2006-inter,di-eugenio-2000-usage,0,\N,Missing
passonneau-etal-2008-relation,passonneau-etal-2006-inter,1,\N,Missing
passonneau-etal-2008-relation,passonneau-2006-measuring,1,\N,Missing
passonneau-etal-2008-relation,J02-4002,0,\N,Missing
passonneau-etal-2008-relation,L06-1000,0,\N,Missing
passonneau-etal-2010-word,passonneau-etal-2006-inter,1,\N,Missing
passonneau-etal-2010-word,W99-0502,0,\N,Missing
passonneau-etal-2010-word,passonneau-2004-computing,1,\N,Missing
passonneau-etal-2010-word,W02-0805,0,\N,Missing
passonneau-etal-2010-word,W09-2402,1,\N,Missing
passonneau-etal-2010-word,W02-0808,1,\N,Missing
passonneau-etal-2010-word,P04-1039,0,\N,Missing
passonneau-etal-2010-word,J08-4004,0,\N,Missing
passonneau-etal-2010-word,D07-1107,0,\N,Missing
passonneau-etal-2012-masc,passonneau-etal-2010-word,1,\N,Missing
passonneau-etal-2012-masc,W10-1806,1,\N,Missing
passonneau-etal-2012-masc,passonneau-etal-2006-inter,1,\N,Missing
passonneau-etal-2012-masc,passonneau-2006-measuring,1,\N,Missing
passonneau-etal-2012-masc,W09-2402,1,\N,Missing
passonneau-etal-2012-masc,N06-2015,0,\N,Missing
passonneau-etal-2012-masc,J08-4004,0,\N,Missing
passonneau-etal-2012-masc,P03-2030,1,\N,Missing
Q14-1025,J08-4004,0,0.181223,"t best practice for creating annotation standards involves iteration over four steps: 1) design or redesign the annotation task, 2) write or revise guidelines to instruct annotators how to carry out the task, possibly with some training, 3) have two or more annotators work independently to annotate a sample of data, 4) measure the interannotator agreement on the data sample. Once the desired agreement has been obtained, the final step is to create a gold standard dataset where each item is annotated by a single annotator. How much chance-adjusted agreement is sufficient has been much debated (Artstein and Poesio, 2008; di Eugenio and Glass, 2004; di Eugenio, 2000; Bruce and Wiebe, 1998). Surprisingly, little attention has been devoted to the question of whether the agreement subset is a representative sample of the corpus. Without such an assurance, there is little justification to take interannotator agreement as a quality measure of the corpus as a whole. Given the influence that a gold standard corpus can have on progress in our field, it is not clear that agreement measures on a corpus subset provide a sufficient guarantee of corpus quality. While it is taken for granted that some annotators perform be"
Q14-1025,W98-1507,0,0.1367,"four steps: 1) design or redesign the annotation task, 2) write or revise guidelines to instruct annotators how to carry out the task, possibly with some training, 3) have two or more annotators work independently to annotate a sample of data, 4) measure the interannotator agreement on the data sample. Once the desired agreement has been obtained, the final step is to create a gold standard dataset where each item is annotated by a single annotator. How much chance-adjusted agreement is sufficient has been much debated (Artstein and Poesio, 2008; di Eugenio and Glass, 2004; di Eugenio, 2000; Bruce and Wiebe, 1998). Surprisingly, little attention has been devoted to the question of whether the agreement subset is a representative sample of the corpus. Without such an assurance, there is little justification to take interannotator agreement as a quality measure of the corpus as a whole. Given the influence that a gold standard corpus can have on progress in our field, it is not clear that agreement measures on a corpus subset provide a sufficient guarantee of corpus quality. While it is taken for granted that some annotators perform better than others,2 agreement metrics do not differentiate annotators."
Q14-1025,W10-0701,0,0.0210392,"Missing"
Q14-1025,di-eugenio-2000-usage,0,0.178035,"Missing"
Q14-1025,N13-1132,0,0.0630021,"ard supervision, but neither models annotator biases, which are critical for estimating true labels. Perhaps the first application of Dawid and Skene’s model to NLP data was the Bruce and Wiebe (1999) investigation of word sense. Much later, Snow et al. (2008) used the same model to show that combining noisy crowdsourced annotations produced data of equal quality to five distinct published gold standards, including an example of word sense. Both works estimate the Dawid and Skene model using supervised gold-standard category data, which allows direct estimation of annotator accuracy and bias. Hovy et al. (2013) recently presented a much add.v ask.v date.n help.v 15 masc 10 count 5 0 15 turk 10 5 0 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 mutual_info Figure 8: Histograms of mutual information estimates for the four example words; trained annotators are in the top row and Turkers in the bottom. 323 simpler model to filter out spam annotators. Crowdsourcing is now so widespread that NAACL 2010 sponsored a workshop on “Creating Speech and Language Data with Amazon’s Mechanical Turk” and in 2011, TREC added a crowdsourcing track. Active learning is a"
Q14-1025,J09-4005,0,0.0092439,"ew. The Gindex (Holley and Guildford, 1964; Vegelius, 1981), for example, is argued to improve over the Matthews Correlation Coefficient (Matthews, 1975). Feinstein and Cicchetti (1990) outline the undesirable behavior that κ-like metrics will have lower values when there is high agreement on highly skewed data. κ assumes that chance agreement on the more prevalent class becomes high. Gwet (2008) presents a metric that estimates the likelihood of chance agreement based on the assumption that chance agreement occurs only when annotators assign labels randomly, which is estimated from the data. Klebanov and Beigman (2009) make a related assumption that annotators agree on easy cases and behave randomly on hard cases, and propose a model to estimate the proportion of hard cases. Model-based gold-standard estimation such as (Dawid and Skene, 1979) has long been the standard in epidemiology, and has been applied to disease prevalence estimation (Albert and Dodd, 2008) and also to many other problems such as human annotation of craters in images of Venus (Smyth et al., 1995). Smyth et al. (1995), Rogers et al. (2010), and Raykar et al. (2010) all discuss the advantages of learning and evaluation with probabilistic"
Q14-1025,passonneau-etal-2012-masc,1,0.929814,"annotation model does not require this many labels per item, and crowdsourced annotation data does not require a probabilistic model. The case study, however, shows how the two benefit each other. MASC (Manually Annotated Sub-Corpus of the Open American National Corpus) contains a subsidiary word sense sentence corpus that consists of approximately one thousand sentences per word for 116 words. Word senses were annotated in their sentence contexts using WordNet sense labels. Chanceadjusted agreement levels ranged from very high to chance levels, with similar variation for pairwise agreement (Passonneau et al., 2012a). As a result, the annotations for certain words appear to be low 311 Transactions of the Association for Computational Linguistics, 2 (2014) 311–326. Action Editor: Chris Callison-Burch. c Submitted 2/2014; Revised 6/2014; Published 10/2014. 2014 Association for Computational Linguistics. quality.1 Our case study shows how we created a more reliable word sense corpus for a randomly selected subset of 45 of the same words, through crowdsourcing and application of the Dawid and Skene model. The model yields a certainty measure for each labeled instance. For most instances, the certainty of th"
Q14-1025,D08-1027,0,0.685591,"Missing"
W07-2312,P06-1049,0,0.613813,". O’Learya,b Judith D. Schlesingerd a Department of Computer Science b Institute for Advanced Computer Studies University of Maryland, College Park {nmadnani,nfa,bonnie,oleary}@cs.umd.edu c Center for Computational Learning Systems, Columbia University becky@cs.columbia.edu d IDA/Center for Computing Sciences {conroy,judith}@super.org e College of Information Studies, University of Maryland, College Park jklavans@umd.edu While a good ordering is essential for summary comprehension (Barzilay et al., 2002), and recent The issue of sentence ordering is an important one work on sentence ordering (Bollegala et al., 2006) for natural language tasks such as multi-document does show promise, it is important to note that desummarization, yet there has not been a quantitatermining an optimal sentence ordering for a given tive exploration of the range of acceptable sentence summary may not be feasible. The question for orderings for short texts. We present results of a evaluation of ordering is whether there is a single sentence reordering experiment with three experibest ordering that humans will converge on, or that mental conditions. Our findings indicate a very high would lead to maximum reading comprehension,"
W07-2312,W98-1507,0,0.0368907,"timates of the expected values within each cell. Given a confusion matrix where the cells on the matrix diagonal are denoted as nii , the row marginals as ni+ , the column marginals as n+i and the matrix total as n++ , the formula for κ is: Variability across Experimental Conditions To measure the variability across the experimental conditions, we developed two methods that assign a global score to each set of reorderings by comparing them to a particular reference point. 4.1 Method 1: Confusion Matrices and κ In NLP evaluation, confusion matrices have typically been used in annotation tasks (Bruce and Wiebe (1998), Tomuro (2001)) where the matrix represents the comparison of two judges, and the κ inter-annotator agreement metric value (Cohen, 1960) gives a measure of the amount of agreement between the two judges, after factoring out chance. However, κ has been used to quantify the observed distribution in confusion matrices of other types in a range of other fields and applications (e.g., assessing map accuracy (Hardin, 1999), or optical recognition (Ross et al., 2002)). Here we use it to quantify variability within sets of reorderings for a summary. Given a representation of each summary as a set of"
W07-2312,W05-1621,0,0.241703,"ber is a significant factor in predicting mean τ scores, we can conclude that the 9 summaries differ from each other in terms of the variability among individuals. As in the earlier ANOVA presented in Table 1, we use Tukey’s HSD to determine the magnitude of the difference in means that is necessary for statistical significance, and use this to identify which summaries have significant differences in the amount of similarity among subjects’ reorderings. Applying Tukey’s method to summary number as a factor yields the differences shown in Table 2. 6 Related Work on Evaluating Sentence Ordering Karamanis and Mellish (2005) also measure the amount of variability between human subjects. However, there are several dimensions of contrast between our experiment and theirs: Their experiment operates in a very distinct domain (archaeology) and genre (descriptions of museum artifacts) whereas we use domain-independent multidocument summaries derived from news articles. We use ordinary, English-speaking volunteers as compared to the domain and genre experts that they employ (archaeologists trained in museum labeling). In terms of the experimental design, we use a Latin square design with three experimental condi86 tions"
W07-2312,P03-1069,0,0.678408,"; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that the sum of the distances between adjacent sentences is minimized. The distance (cjk ) between any pair of sentences j and k is computed by first obtaining a similarity score (bjk ) for the pair, and then normalizing this score: bjk , (cjj = 0) cjk = 1 − p"
W07-2312,J06-4002,0,0.267326,"sum to 6, κ ranges from 1 to 0, with 1 indicating that the set of reorderings all reproduce the initial ordering, and 0 indicating that the set of reorderings conforms to chance. 1 2 6 6 2 2 6 4 2 3 1 1 3 3 1 2 3 4 2 2 4 4 2 3 4 5 3 3 5 5 3 4 5 6 4 4 6 6 4 5 6 1 5 5 1 1 5 3 Figure 3: A hypothetical example illustrating Means Vectors compute means vectors for each condition for each summary, giving 27 such vectors. We compare each means vector representing a set of reorderings to each initial ordering O, R and T using three correlation coefficients: Pearson’s r, Spearman’s ρ, and Kendall’s τ (Lapata, 2006). The three correlation coefficients test the closeness of two series of numbers, or two variables x and y, in different ways. Pearson’s r is a parametric test of whether there is a perfect linear relation between the two variables. Spearman’s ρ and Kendall’s τ are non-parametric tests. Spearman’s ρ is computed by replacing the variable values by their rank and computing the correlation. Kendall’s τ is based on counting the number of pairs xi , xi+1 and yi , yi+1 where the deltas of both pairs have the same sign. In sum, the three metrics test whether x and y are in a linear relation, a rank-p"
W07-2312,C04-1108,0,0.578699,"in Section 8. 2 Sentence Ordering Algorithms A number of approaches have been applied to sentence ordering for multi-document summarization (Radev and McKeown, 1999). The first techniques exploited chronological information in the documents (McKeown et al., 1999; Lin and Hovy, 2002). Barzilay et al. (2002) were the first to discuss the impact of sentence ordering in the context of multi-document summarization in the news genre. They used an augmented chronological ordering algorithm that first identified and clustered related sentences, then imposed an ordering as directed by the chronology. Okazaki et al. (2004) further improved the chronological ordering algorithm by first arranging sentences in simple chronological order, then performing local reorderings. More recent work includes probabilistic approaches that try to model the structure of text (Lapata, 2003) and algorithms that use large corpora to learn an ordering and then apply it to the summary under consideration (Bollegala et al., 2005). Conroy et al. (2006) treat sentence ordering as a Traveling Salesperson Problem (TSP), similar to Althaus et al. (2004). Starting from a designated first sentence, they reorder the other sentences so that t"
W07-2312,P04-1051,0,0.118959,"Missing"
W07-2312,J98-3005,0,\N,Missing
W07-2312,I05-1055,0,\N,Missing
W09-2402,J08-4004,0,0.258989,"eport Cohen’s κ; note the similarity in values3 . As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff (Krippendorff, 1980) considers values above 0.67 moderately good, and values above 0.80 as quite good. (cf. (Artstein and Poesio, 2008) for discussion of agreement measurement for computational linguistic tasks.) Table 2 shows IA for a pair of adjectives, nouns and verbs from our sample for which the IA scores are at the extremes (high and low) in each pair: the average delta is 0.24. Note that the agreement decreases as part-of-speech varies from adjectives to nouns to verbs, but for all three parts-of-speech, there is a wide spread of values. It is striking, given that the same annotators did all words, that one in each pair has relatively better agreement. 3 α handles multiple annotators; Arstein and Poesio (Artstein and P"
W09-2402,W02-0805,0,0.237598,"Missing"
W09-2402,P04-1039,0,0.110351,"Missing"
W09-2402,W02-0808,1,0.90498,"Missing"
W09-2402,W99-0502,0,0.672072,"he variations in IA. We conclude with a summary of our findings goals. 2 Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 2–9, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics Word fair long quiet land time work know say show tell POS Adj Adj Adj Noun Noun Noun Verb Verb Verb Verb No. senses 10 9 6 11 10 7 11 11 12 8 No. occurrences 463 2706 244 1288 21790 5780 10334 20372 11877 4799 Table 1: Ten Words 2005). Pedersen (Pedersen, 2002a) examines variation across individual words in evaluating WSD systems, but does not attempt to explain it. Factors that have"
W09-2402,passonneau-etal-2006-inter,1,0.905734,"Missing"
W09-2402,passonneau-2004-computing,1,0.862694,"ed 8 5 7 8 8 11 Table 2: Varying interannotator agreement across words We expected to find varying levels of interannotator agreement (IA) among all six annotators, depending on obvious grouping factors such as the part of speech, or the number of senses per word. We do find widely varying levels of agreement, but as described here, most of the variation does not depend on these a priori factors. Inherent usage properties of the words themselves, and systematic patterns of variation across annotators, seem to be the primary factors, with a secondary effect of part of speech. In previous work (Passonneau, 2004), we have discussed why we use Krippendorff’s α (Krippendorff, 1980), and for purposes of comparison we also report Cohen’s κ; note the similarity in values3 . As with the various agreement coefficients that factor out the agreement that would occur by chance, values range from 1 for perfect agreement and -1 for perfect opposition, to 0 for chance agreement. While there are no hard and fast criteria for what constitutes good IA, Landis and Koch (Landis and Koch, 1977) consider values between 0.40 and 0.60 to represent moderately good agreement, and values above 0.60 as quite good; Krippendorff"
W09-2402,J91-4003,0,0.397183,"es in word usage. It has been widely observed that usage features such as vocabulary and syntax vary across corpora of different genres and registers (Biber, 1995), and that serve different functions (Kittredge et al., 1991). Still, we are far from able to predict specific morphosyntactic and lexical variations across corpora (Kilgarriff, 2001), much less quantify them in a way that makes it possible to apply the same analysis tools (taggers, parsers) without retraining. In comparison to morphosyntactic properties of language, word and phrasal meaning is fluid, and to some degree, generative (Pustejovsky, 1991; 2 Nancy Ide Department of Computer Science Vassar College Poughkeepsie, NY, USA ide@cs.vassar.edu Nunberg, 1979). Based on our initial observations from a word sense annotation task for relatively polysemous words, carried out by multiple annotators on a heterogeneous corpus, we hypothesize that different words lead to greater or lesser interannotator agreement (IA) for reasons that in the long run should be explicitly modelled in order for Natural Language Processing (NLP) applications to handle usage differences more robustly. This pilot study is a step in that direction. We present relate"
W09-2402,D07-1107,0,0.0851426,"Missing"
W09-2402,W02-0812,0,\N,Missing
W09-2402,W02-0806,0,\N,Missing
W09-3953,J97-1005,1,\N,Missing
W09-3953,C04-1128,0,\N,Missing
W09-3953,W01-1607,0,\N,Missing
W09-3953,H01-1015,1,\N,Missing
W09-3953,P04-1085,0,\N,Missing
W09-3953,J00-3003,0,\N,Missing
W09-3953,H93-1005,0,\N,Missing
W09-3953,P08-1081,0,\N,Missing
W09-3953,kruijff-korbayova-etal-2006-sammie,0,\N,Missing
W09-3953,devillers-etal-2002-annotations,0,\N,Missing
W09-3953,W04-1008,0,\N,Missing
W09-3953,D08-1081,0,\N,Missing
W09-3953,W06-3406,0,\N,Missing
W10-1803,day-etal-2004-callisto,0,0.0222587,"example highlights the difference between our definition of Interaction events and ACE’s definition of Contact events. For this reason, in Figure 5, 51 of our INR relations do not overlap with ACE event categories. (6) In central Baghdad, [a Reuters cameraman] and [a cameraman for Spain’s Telecinco] died when an American tank fired on the Palestine Hotel 4 ACE has annotated the above example as an event of type C ONFLICT-ATTACK in which there are two entities that are of type person: the Reuters cameraman and the cameraman for Annotation Procedure We used Callisto (a configurable workbench) (Day et al., 2004) to annotate the ACE-2005 corpus for 6 The ACE event annotated in the sentence is of type “Personell-Elect” (span election) which is not recorded as an event between two or more entities and is not relevant here. 5 Recall that our event annotations are between exactly two entities of type PER.Individual or PER.Group. 24 62 Documents Verbal INR NonVerbal Conflict (5) Contact (32) Attack Meet Phone-Write Justice-* (13) Life (7) Transaction (2) Die Divorce Injure Transfer-Money Not Found Near (66) 0 26 0 9 0 0 0 0 31 Far (17) 0 0 3 3 0 1 0 0 10 Near (14) 3 0 0 0 2 0 0 1 8 Far (3) 0 0 0 0 0 0 0 1"
W10-1803,doddington-etal-2004-automatic,0,0.189027,"mail exchanges Figure 3: Network formed by augmenting the email exas links. Identical color or shape implies structural equivachange network above with links that occur in the content of lence. Only Sam and Mary are structurally equivalent the emails. Now, Kate and Mary are structurally equivalent, as are Sam and Jacob. for such a technique to be created. This is because our annotations capture interactions described in the content of the email such as face-to-face meetings, physical co-presence and cognizance. notating entities, relations and events in free text, most notably the ACE effort (Doddington et al., 2004). We intend to leverage this work as much as possible. The task of social network extraction can be broadly divided into 3 tasks: 1) entity extraction; 2) social relation extraction; 3) social event extraction. We are only interested in the third task, social event extraction. For the first two tasks, we can simply use the annotation guidelines developed by the ACE effort. Our social events, however, do not clearly map to the ACE events: we introduce a comprehensive set of social events which are very different from the event annotation that already exists for ACE. This paper is about the anno"
W10-1806,W04-2323,0,0.0356907,"Missing"
W10-1806,W02-0805,0,0.0241892,"5a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find that interannotator agreement (IA) among half a dozen annotators varies depending on the word. For ten words nearly balanced with 2 3 49 http://w"
W10-1806,P04-1039,0,0.0209073,"o investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find that interannotator agreement (IA) among half a dozen annotators varies depending on the word. For ten words nearly balanced with 2 3 49 http://www.anc.org http://www"
W10-1806,W09-3953,1,0.886481,"Missing"
W10-1806,W02-0808,1,0.797594,"(WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annota"
W10-1806,P10-2013,1,0.866373,"Missing"
W10-1806,H05-1073,0,0.0809682,"Missing"
W10-1806,J08-4004,0,0.123441,"artificial, annotators might disagree on word senses because they disagree on the boundaries between one sense and another, just as professional lexicographers do. Beyond Interannotator Agreement (IA) Assessing the reliability of an annotation typically addresses the question of whether different annotators (effectively) assign the same annotation labels. Various measures can be used to compare different annotators, including agreement coefficients such as Krippendorff’s alpha (Krippendorff, 1980). Extensive reviews of the properties of such coefficients have been presented elsewhere, e.g., (Artstein and Poesio, 2008). Briefly, an agreement produce values in the interval [-1,1] indicating how much of the observed agreement is above (or below) agreement that would be predicted by chance (value of 0). To measure reliability in this way is to assume that for most of the instances in the data, there is a single correct response. Here we present the use of reliability metrics and other measures for word sense annotation, and we assume that in some cases there may not be a single correct response. When annotators have less than excellent agreement, we aim to examine possible causes. Apart from the artificiality"
W10-1806,D07-1107,0,0.055535,"Missing"
W10-1806,D08-1027,0,0.583911,"Missing"
W10-1806,W99-0502,0,0.0209562,"age cohort, account for the disagreement. 4 Word Sense Annotation Data Related Work There has been a decade-long community-wide effort to evaluate word sense disambiguation (WSD) systems across languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part"
W10-1806,J05-1004,0,0.0104773,"hen annotators disagree, having multiple annotators is necessary in order to determine whether the disagreement is due to noise based on insufficiently clear sense definitions versus a systematic difference between individuals, e.g., those who see a glass as half empty where others see it as half full. To insure the opportunity to observe how varied the labeling of a single word can be, we collect word sense annotations from multiple annotators. One potential benefit of such investigation might be a better understanding of how to model word meaning. In sum, we hypothesize the following cases: Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether t"
W10-1806,passonneau-etal-2006-inter,1,0.845777,"ss languages in the four Senseval efforts (1998, 2001, 2004, and 2007, cf. (Kilgarriff, 1998; Pedersen, 2002a; Pedersen, 2002b; Palmer et al., 2005a)), with a corollary effort to investigate the issues pertaining to preparation of manually annotated gold standard corpora tagged for word senses (Palmer et al., 2005a). Differences in IA and system performance across part-of-speech have been examined, as in (Ng et al., 1999; Palmer et al., 2005a). Factors that have been proposed as affecting agreement include whether annotators are allowed to assign multilabels (V´eronis, 1998; Ide et al., 2002; Passonneau et al., 2006), the number or granularity of senses (Ng et al., 1999), merging of related senses (Snow et al., 2007), sense similarity (Chugur et al., 2002), entropy (Diab, 2004; We chose ten fairly frequent, moderately polysemous words for sense tagging. One hundred occurrences of each word were sense annotated by five or six trained annotators. The ten words are shown in Table 1, the words are grouped by part of speech, with the number of WordNet senses, the number of senses used by the trained annotators (TAs), the number of annotators, and Alpha. We call this the Trained annotator (TA) data. We find tha"
W10-1806,passonneau-etal-2008-relation,1,0.852796,"s in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. • Outliers: A small proportion of annotators may assign senses in a manner that differs markedly from the remaining annotators. • Confusability of senses: If multiple annotators assign multiple senses in an apparently random fashion, it may be that the senses are not sufficiently distinct. 5 5.1 Trained Annotator data The Manually Annotated Sub-Corpus (MASC) project (Ide et al., 2010) is creating a small, representative corpus of American English written and"
W10-1806,J08-3001,0,0.0240304,"w varied the labeling of a single word can be, we collect word sense annotations from multiple annotators. One potential benefit of such investigation might be a better understanding of how to model word meaning. In sum, we hypothesize the following cases: Palmer et al., 2005a), and reactions times required to distinguish senses (Klein and Murphy, 2002; Ide and Wilks, 2006). We anticipate that one of the ways in which the data will be used will be to train machine learning approaches to WSD. Noise in labeling and the impact on machine learning has been discussed from various perspectives. In (Reidsma and Carletta, 2008), it is argued that machine learning performance does not vary consistently with interannotator agreement. Through a simulation study, the authors find that machine learning performance can degrade or not with lower agreement, depending on whether the disagreement is due to noise or systematic behavior. Noise has relatively little impact compared with systematic disagreements. In (Passonneau et al., 2008), a similar lack of correlation between interannotator agreement and machine learning performance is found in an empirical investigation. • Outliers: A small proportion of annotators may assig"
W10-1806,W02-0812,0,\N,Missing
W10-1806,W02-0806,0,\N,Missing
W11-0705,E09-1004,1,0.167584,"ection 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and they report SVM outper"
W11-0705,C10-2005,0,0.777352,"baseline. In addition we explore a different method of data representation and report significant improvement over the unigram models. Another contribution of this paper is that we report results on manually annotated data that does not suffer from any known biases. Our data is a random sample of streaming tweets unlike data collected by using specific queries. The size of our hand-labeled data allows us to perform crossvalidation experiments and check for the variance in performance of the classifier across folds. Another significant effort for sentiment classification on Twitter data is by Barbosa and Feng (2010). They use polarity predictions from three websites as noisy labels to train a model and use 1000 manually labeled tweets for tuning and another 1000 manually labeled tweets for testing. They however do not mention how they collect their test data. They propose the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words. We extend their approach by using real valued prior polarity, and by combining prior polarity with POS. Our results show that the features that enhance the perfo"
W11-0705,C04-1121,0,0.0262243,"d tweets for testing. They however do not mention how they collect their test data. They propose the use of syntax features of tweets like retweet, hashtags, link, punctuation and exclamation marks in conjunction with features like prior polarity of words and POS of words. We extend their approach by using real valued prior polarity, and by combining prior polarity with POS. Our results show that the features that enhance the performance of our classifiers the most are features that combine prior polarity of words with their parts of speech. The tweet syntax features help but only marginally. Gamon (2004) perform sentiment analysis on feeadback data from Global Support Services survey. One aim of their paper is to analyze the role of linguistic features like POS tags. They perform extensive feature analysis and feature selection and demonstrate that abstract linguistic analysis features contributes to the classifier accuracy. In this paper we perform extensive feature analysis and show that the use of only 100 abstract linguistic features performs as well as a hard unigram baseline. 3 Data Description Twitter is a social networking and microblogging service that allows users to post real time"
W11-0705,C04-1200,0,0.0301245,"ional resources. In section 5 we present our prior polarity scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using"
W11-0705,P03-1054,0,0.0424695,"negations (e.g. not, no, never, n’t, cannot) by tag “NOT”, and e) replace a sequence of repeated characters by three characters, for example, convert coooooooool to coool. We do not replace the sequence by only two characters since we want to differentiate between the regular usage and emphasized usage of the word. Acronym gr8, gr8t lol rotf bff English expansion great laughing out loud rolling on the floor best friend forever Table 1: Example acrynom and their expansion in the acronym dictionary. We present some preliminary statistics about the data in Table 3. We use the Stanford tokenizer (Klein and Manning, 2003) to tokenize the tweets. We use a stop word dictionary3 to identify stop words. All the other words which are found in WordNet (Fellbaum, 1998) are counted as English words. We use 1 http://en.wikipedia.org/wiki/List of emoticons http://www.noslang.com/ 3 http://www.webconfs.com/stop-words.php 2 Emoticon :-) :) :o) :] :3 :c) :D C: :-( :( :c :[ D8 D; D= DX v.v :| Polarity Positive Extremely-Positive Negative Extremely-Negative Neutral Table 2: Part of the dictionary of emoticons the standard tagset defined by the Penn Treebank for identifying punctuation. We record the occurrence of three stand"
W11-0705,pak-paroubek-2010-twitter,0,0.566631,"Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and they report SVM outperforms other classifiers. In terms of feature space, they try a Unigram, Bigram model in conjunction 31 with parts-of-speech (POS) features. They note that the unigram model outperforms all other models. Specifically, bigrams and POS features do not help. Pak and Paroubek (2010) collect data following a"
W11-0705,P04-1035,0,0.114343,"ils about the data. In section 4 we discuss our pre-processing technique and additional resources. In section 5 we present our prior polarity scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as po"
W11-0705,P02-1053,0,0.0445841,", we give details about the data. In section 4 we discuss our pre-processing technique and additional resources. In section 5 we present our prior polarity scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons"
W11-0705,H05-1044,0,0.355008,"scoring scheme. In section 6 we present the design of our tree kernel. In section 7 we give details of our feature based approach. In section 8 we present our experiments and discuss the results. We conclude and give future directions of research in section 9. 2 Literature Survey Sentiment analysis has been handled as a Natural Language Processing task at many levels of granularity. Starting from being a document level classification task (Turney, 2002; Pang and Lee, 2004), it has been handled at the sentence level (Hu and Liu, 2004; Kim and Hovy, 2004) and more recently at the phrase level (Wilson et al., 2005; Agarwal et al., 2009). Microblog data like Twitter, on which users post real time reactions to and opinions about “everything”, poses newer and different challenges. Some of the early and recent results on sentiment analysis of Twitter data are by Go et al. (2009), (Bermingham and Smeaton, 2010) and Pak and Paroubek (2010). Go et al. (2009) use distant learning to acquire sentiment data. They use tweets ending in positive emoticons like “:)” “:-)” as positive and negative emoticons like “:(” “:-(” as negative. They build models using Naive Bayes, MaxEnt and Support Vector Machines (SVM), and"
W11-2027,N09-2047,0,0.0169473,"n of ASR consisted primarily of voice search; the highly constrained CFG rules (exact words in exact order) had little impact on performance. For baseline CheckItOut dialogues, and for Full WOz, we required more constrained grammar rules that would preserve Phoenix’s robustness to noise. To avoid the brittleness of exact string CFG rules, and the massive over-generation of BOW CFG rules, we wrote a transducer that mapped dependency parses of book titles to CFG rules. When ASR words are skipped, book title parses can consist of multiple slots. We used MICA, a broad-coverage dependency grammar (Bangalore et al., 2009) to parse the entire book title database. When a set of titles is selected for an experiment, the corresponding MICA parses are transduced to the relevant CFG productions, and inserted into a Phoenix grammar. Productions for the author subgrammar 2 BOW Phoenix rules for book titles are used in a more recent Olympus/RavenClaw system inspired in part by CheckItOut (Lee et al., 2010), with a database of 15,088 eBooks. Exp. Title Pilot Turn Exchange Full WOz AM WSJ1 16kHz WSJ1 16kHz WSJ1 8kHz Adapted NA NA 10 hr. # Titles for LM 500 7,500 3,000 LM unigram trigram Logios + book data Grammar rules N"
W11-2027,W05-1624,0,0.0766744,"Missing"
W11-2027,N10-1126,1,0.781999,"Missing"
W11-2027,W01-1616,0,0.0756659,"Missing"
W11-2027,P06-2085,0,0.0785614,"rates for misunderstandings of form versus intent in human conversation, because the two types cannot always be distinguished (Schlangen and Fern’andez, This paper presents three experiments that progressively address SLU methods to compensate for poor automated speech recognition (ASR), and complementary DM strategies. In two of the experiments, human wizards are embedded in the spoken dialogue system while run-time SLU features are collected. Many wizard-of-Oz investigations have addressed the noisy channel issue for SDS (Zollo, 1999; Skantze, 2003; Williams and Young, 2004; Skantze, 2005; Rieser and Lemon, 2006; Schlangen and Fern’andez, 2005; Rieser and Lemon, 2011). Like them, we study how human wizards solve the joint problem of interpreting users’ words and inferring users’ intents. Our work differs in its exploration of the role context can play in the literal interpretation of noisy language. We rely on knowledge in the backend database to propose candidate linguistic forms for noisy ASR. Our principal results are that both wizards and our 248 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 248–258, c Portland, Oregon, Jun"
W11-2027,J11-1006,0,0.217083,"man conversation, because the two types cannot always be distinguished (Schlangen and Fern’andez, This paper presents three experiments that progressively address SLU methods to compensate for poor automated speech recognition (ASR), and complementary DM strategies. In two of the experiments, human wizards are embedded in the spoken dialogue system while run-time SLU features are collected. Many wizard-of-Oz investigations have addressed the noisy channel issue for SDS (Zollo, 1999; Skantze, 2003; Williams and Young, 2004; Skantze, 2005; Rieser and Lemon, 2006; Schlangen and Fern’andez, 2005; Rieser and Lemon, 2011). Like them, we study how human wizards solve the joint problem of interpreting users’ words and inferring users’ intents. Our work differs in its exploration of the role context can play in the literal interpretation of noisy language. We rely on knowledge in the backend database to propose candidate linguistic forms for noisy ASR. Our principal results are that both wizards and our 248 Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 248–258, c Portland, Oregon, June 17-18, 2011. 2011 Association for Computational Linguis"
W11-2027,W09-0506,0,0.0280628,"ulty ASR, an SDS can rely on strategies to detect and respond to incorrect recognition output (Bohus, 2004). 249 The SDS can repeatedly request user confirmation to avoid misunderstanding, or ask for confirmation using language that elicits responses from the user that the system can handle (Raux and Eskenazi, 2004). When the user adds unanticipated information in response to a system prompt, two-pass recognition can rely on a concept-specific language model to improve the recognition of the domain concepts within the utterance containing unknown words, and thereby achieve better recognition (Stoyanchev and Stent, 2009). An SDS could take this approach one step further and use context-specific language for incremental understanding of noisy input throughout the dialogue (Aist et al., 2007). Current work on error recovery and grounding for SDS assumes that the primary responsibility of a dialogue management strategy is to understand the user’s intent. Errors of understanding are addressed by ignoring the utterances where understanding failures occur, asking users to repeat, or pursuing clarifications about intent. These strategies typically rely on knowledge sources that follow the SLU stage. The RavenClaw di"
W11-2027,H94-1039,0,0.104565,"fter natural language understanding. The next section describes this architecture. 3.2 Architecture CheckItOut, our baseline SDS, employs the Olympus/RavenClaw architecture developed at Carnegie Mellon University (CMU) (Raux et al., 2005; Bohus and Rudnicky, 2009). SDS modules communicate via message passing, controlled by a central hub. However, the information flow is largely a pipeline, as depicted in Figure 1(a). The PocketSphinx recognizer (Huggins-Daines et al., 2006) receives acoustic data segmented by the audio manager, and passes a single recognition hypothesis to the Phoenix parser (Ward and Issar, 1994). Phoenix sends one or more equivalently ranked semantic parses to the Helios confidence annotator (Bohus and Rudnicky, 2002), which selects a parse and assigns a confidence score. The Apollo interaction manager (Raux and Eskenazi, 2007) monitors the three SLU modules–the recognizer, the semantic parser, and the confidence annotator–to determine whether the user or SDS has the current turn. To a limited degree, Apollo can override the early segmentation decisions based solely on pause length. Confidence-annotated concepts from the semantic parse are passed to the RavenClaw DM, which decides wh"
W11-2027,gordon-passonneau-2010-evaluation,1,\N,Missing
W11-2027,N04-1028,0,\N,Missing
W11-2029,H05-1029,0,0.0760519,"Missing"
W11-2029,E09-1022,0,0.0676506,"Missing"
W11-2029,W03-2114,0,0.0774462,"Missing"
W11-2029,J06-3004,0,0.0765099,"Missing"
W11-2029,P99-1024,0,0.134893,"Missing"
W11-2029,N10-1126,1,0.668711,"Missing"
W11-2038,J11-1004,0,\N,Missing
W11-2038,W09-3953,1,\N,Missing
W11-2038,W09-3936,0,\N,Missing
W11-2038,N10-1126,1,\N,Missing
W11-2038,W11-2029,1,\N,Missing
W12-1635,P08-1055,0,0.462618,") distinguish between semantic and linguistic complexity of calls to a spoken DS. Semantic complexity is measured by inheritance relations between call types, the number of type labels per call, and how often calls are routed to human agents. Linguistic complexity is measured by utterance length, vocabulary size and perplexity. Popescu et al. (2003) identify a class of “semantically tractable” natural language questions that can be mapped to an SQL query to return the question’s unique correct answer. Ambiguous questions with multiple correct answers are not considered semantically tractable. Polifroni and Walker (2008) address how to present informative options to users who are exploring a database, for example, to choose a restaurant. When a query returns many options, their system summarizes the return using attribute value pairs shared by many of the members. 3 Semantic Specificity The database queried by a DS can be regarded as the system’s knowledge. Consequently, the semantic structure of the database and the way it is populated constrain the requests the system can address and how much information the user must provide. Intuitively, Table 1 shows that TITLE has a higher semantic specificity than AUTH"
W12-1635,W00-0107,0,0.0539664,"Korea, 5-6 July 2012. 2012 Association for Computational Linguistics the number of query return sizes for one or more attributes. We show through simulation that dialogue length varies with semantic specificity for a DS with a simple system-initiative dialogue strategy. 2 Related Work Little work has been reported on measures of the relationship between dialogue complexity and the semantic structure of a DS application’s database. Zadrozny (1995) proposes Q-Complexity, which roughly corresponds to vocabulary size, and is essentially the number of questions that can be asked about a database. Pollard and Bierman (2000) describe a similar measure that considers the number of bits required to distinguish every object, attribute, and relationship in the semantic space. Gorin et al. (2000) distinguish between semantic and linguistic complexity of calls to a spoken DS. Semantic complexity is measured by inheritance relations between call types, the number of type labels per call, and how often calls are routed to human agents. Linguistic complexity is measured by utterance length, vocabulary size and perplexity. Popescu et al. (2003) identify a class of “semantically tractable” natural language questions that ca"
W13-2323,J08-4004,0,0.355716,"nal agreement measures, application of an annotation model to instances with crowdsourced labels yields higher quality labels at lower cost. 1 Introduction The quality of annotated data for computational linguistics is generally assumed to be good enough if a few annotators can be shown to be consistent with one another. Metrics such as pairwise agreement and agreement coefficients measure consistency among annotators. These descriptive statistics do not support inferences about corpus quality or annotator accuracy, and the absolute values one should aim for are debatable, as in the review by Artstein and Poesio (2008). We argue that high chance-adjusted inter-annotator agreement is neither necessary nor sufficient to ensure high quality gold-standard labels. Agreement measures reveal little about differences among annotators, and nothing about the certainty of the true label, given the observed labels from annotators. In contrast, a probabilistic model of annotation supports statistical inferences about the quality of the observed and inferred labels. This paper presents a case study of a particularly thorny annotation task that is of widespread 2 Chance-Adjusted Agreement Current best practice for collect"
W13-2323,W98-1507,0,0.0830902,"–195, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics structing annotators how to carry out the task, possibly with some training, 3) have two or more annotators work independently to annotate a sample of data, and 4) measure the interannotator agreement on the data sample. Once the desired agreement has been obtained, a gold standard dataset is created where each item is annotated by one annotator. As noted in the introduction, how much agreement is sufficient has been much discussed (Artstein and Poesio, 2008; di Eugenio and Glass, 2004; di Eugenio, 2000; Bruce and Wiebe, 1998). The quality of the gold standard is not explicitly measured. Nor is the accuracy of the annotators. Since there are many ways to be inaccurate, and only one way to be accurate, it is assumed that if annotators agree, then the annotation must be accurate. This is often but not always correct. If two annotators do not agree well, this method does not identify whether one annotator is more accurate than the other. For the individual items they disagree on, no information is gained about the true label. To get a high level sense of the limitations of agreement metrics, we briefly discuss how the"
W13-2323,W10-0701,0,0.0383591,"Missing"
W13-2323,di-eugenio-2000-usage,0,0.107975,"Missing"
W13-2323,D09-1046,0,0.0481296,"et of labels. Note that application of an annotation model does not require this many labels for each item, and crowdsourced annotation data does not require a probabilistic model. This case study, however, does demonstrate a mutual benefit. A highly certain ground truth label for each annotated instance is the ultimate goal of data annotation. Many issues, however, make this complicated for word sense annotation. The number of different senses defined for a word varies across lexical resources, and pairs of senses within a single sense inventory are not equally distinct (Ide and Wilks, 2006; Erk and McCarthy, 2009). A previous annotation effort using WordNet sense labels demonstrates a great deal of variation across words (Passonneau et al., 2012b). On over 116 words, chance-adjusted agreement ranged from very high to chance levels. As a result, the ground truth labels for many words are questionable. On a random subset of 45 of the same words, the crowdsourced data presented here (available as noted below) yields a certainty measure for each ground truth label indicating high certainty for most instances. This paper presents a case study of a difficult and important categorical annotation task (word se"
W13-2323,D08-1027,0,0.696002,"Missing"
W13-2323,J09-4005,0,0.080825,"Missing"
W13-2323,passonneau-etal-2012-masc,1,0.896619,"Missing"
W14-4330,D08-1035,0,0.0432623,"Missing"
W14-4330,P03-1071,0,0.567885,"Missing"
W14-4330,J97-1005,1,0.871251,"Missing"
W14-4330,W11-2038,1,0.188671,"Missing"
W14-4330,J86-3001,0,0.822129,"Missing"
W14-4330,J97-1003,0,0.548354,"Missing"
W14-4330,P06-1003,0,0.0447306,"Missing"
W14-4330,J93-3003,0,0.472088,"Missing"
W14-4330,W09-3953,1,0.90593,"Missing"
W14-4330,ries-etal-2000-shallow,0,0.135986,"Missing"
W14-4330,P06-1004,0,0.0449612,"Missing"
W14-4330,W13-4015,0,0.0222222,"Missing"
W14-4330,W12-1628,0,0.0474619,"Missing"
W18-0531,L18-1511,1,0.91622,"ormalized in different ways, as described further below. A pyramid content model thus consists of all the distinct ideas, or SCUs, in the reference summaries, along with their weights. To construct the pyramid content model automatically, sentences are first decomposed into distinct clausal or phrasal segments, then each segment is converted to a dense vector representation using Weighted Text Matrix Factorization (WTMF) (Guo et al., 2014). These semantic vectors are then grouped into semantically similar sets to form the SCUs, using a restricted set partition algorithm, EDUA, as noted below (Gao et al., 2018b). A new summary is scored against this content model by first segmenting the sentences and vectorizing them, then matching them to the The rubric One hundred and thirty-nine summaries were submitted. These were then scored by hand using a rubric developed from the presentation given during the workshop. The 10 main points identified in the presentation were used as checkpoints in the rubric, which is shown in Figure 1. One point was assigned to each of the ideas listed in the rubric; however, the interpretation of what constituted an idea was open to the discretion of the instructor. Each st"
W18-0531,C14-1047,0,0.0131194,"r of contributors to an SCU is an importance weight that is assigned to ideas in a new summary being scored. The weights of SCUs in a new summary are summed, and the sum is normalized in different ways, as described further below. A pyramid content model thus consists of all the distinct ideas, or SCUs, in the reference summaries, along with their weights. To construct the pyramid content model automatically, sentences are first decomposed into distinct clausal or phrasal segments, then each segment is converted to a dense vector representation using Weighted Text Matrix Factorization (WTMF) (Guo et al., 2014). These semantic vectors are then grouped into semantically similar sets to form the SCUs, using a restricted set partition algorithm, EDUA, as noted below (Gao et al., 2018b). A new summary is scored against this content model by first segmenting the sentences and vectorizing them, then matching them to the The rubric One hundred and thirty-nine summaries were submitted. These were then scored by hand using a rubric developed from the presentation given during the workshop. The 10 main points identified in the presentation were used as checkpoints in the rubric, which is shown in Figure 1. On"
W18-0531,I17-2031,0,0.229999,"Missing"
W18-0531,P02-1040,0,0.104809,"structional methods at the college level (Yang, 2014), and for English language learners (Babinski et al., 2017). Instruction in summarization strategies includes occasional forays into computer-based training (Sung et al., 2008), including intelligent tutoring systems that provide writing practice (Proske et al., 2012)(Roscoe et al., 2015). Recent work built regression models to predict scores based on several rubrics for summaries from L2 business school students (Sladoljev Ageˇ jev and Snajder, 2017). Features were automatically derived from Coh-Metrix (McNamara et al., 2014), BLEU scores (Papineni et al., 2002) and ROUGE scores (Lin, 2004). In (Srihari et al., 2008), OCR was used to digitize handwritten essays, which were then scored using various automated essay scoring methods, including latent semantic analysis and a feature-based approach. Essays are automatically scored in (Zupanc and Bosni, 2017) after constructing an ontology from model essays using information extraction and logic reasoning. PyrEval constructs a content model from a small set of reference summaries, using latent semantic vectors to represent meanings of phrases. There has been recent interest in developing automated revision"
W18-0531,P17-1144,0,0.0284373,"d in (Zupanc and Bosni, 2017) after constructing an ontology from model essays using information extraction and logic reasoning. PyrEval constructs a content model from a small set of reference summaries, using latent semantic vectors to represent meanings of phrases. There has been recent interest in developing automated revision tools for students’ written work but none have, hitherto, been reported in the literature. There is existing work on automated revision of short answers for middle school science writing (Tansomboon et al., 2017), and a corpus on automated revision of argumentation (Zhang et al., 2017). What is distinctive about our work is the feasibility of providing automated feedback on summary content, either for teachers or students, which could ultimately lead to the development of an automated revision tool. Within HE, formative feedback is perceived as information communicated to the students about learning-oriented assignments (Race, 2001) such as essays. This feedback can be oral or written, and is often generated by the instructor. Providing feedback remains the responsibility of the instructor, and with much emphasis being placed on evaluating student learning at the end of an"
W19-4452,W15-0608,0,0.043466,"s (4 vs. 5) we noticed an interesting aspect: for the essays scored with 5 the ratio of argumentative sentences w.r.t total number of sentence was higher than for the essays with a 4 score, while the essays with a 4 scores tend to be longer. In general our correlations scores were much lower than the ones reported by Ghosh et al. (2016). There are several tive features in predicting the quality of argument scores (0-5) we use Logistic Regression (LR) learners and evaluate the learners using quadraticweighted kappa (QWK) against the human scores, a methodology generally used for essay scoring (Farra et al., 2015; Ghosh et al., 2016). QWK corrects for chance agreement between the system prediction and the human prediction, and it takes into account the extent of the disagreement between labels. Since the number of essays is very small we did a five-fold cross validation. Table 7 reports the performance for the three feature groups as well as their combination. The baseline feature (bl) is the number of sentences in the essay, since essay length has been shown to be generally highly correlated with essay scores (Chodorow and Burstein, 2004). As seen in Table 7 out of the individual features groups the"
W19-4452,P16-2089,1,0.774315,"answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies: are they the currencies of the future? For"
W19-4452,W16-2808,0,0.0764335,"s grade the short text answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies: are they the currencies"
W19-4452,D16-1035,0,0.0242251,"to DUCView. The annotator creates SCUs and exports the pyramid XML file (*.pyr). A pyramid file and a student summary are then the input for the annotator to match phrases in the student summary to pyramid SCUs, which is also exported as XML (*.pan). sive reliability measures in past work on pyramid annotation, we did not re-assess its reliability here. To annotate the content of the argument portion of the essay, we identify all distinct Elementary Discourse Units (EDU). Identifying (segmenting) EDUs from text and representing their meanings play a key role in discourse parsing (Marcu, 2000; Li et al., 2016; Braud et al., 2017). Definitions of EDUs vary, thus Prasad et al. (2008) consider the full range of clause types, including verb arguments and non-finite clauses. To simplify the annotation, we restrict EDUs to propositions derived from tensed clauses that are not verb arguments (such as that complements of verbs of belief). In (Gao et al., 2019), we report the iterative development of reliable annotation guidelines for untrained annotators.2 Annotators first identify the start and end of tensed clauses, omitting discourse connectives from the EDU spans, which can be discontinuous. Annotator"
W19-4452,W14-2104,0,0.053287,"Missing"
W19-4452,W17-5102,1,0.86098,"es has 2.07. Cryptocurrencies has shorter length of EDUs compared to Autonomous Vehicle, as 13.62 and 14.00. For the normalized SCU by total number of EDUs and number of matched EDUs, Autonomous Vehicle shows more with 0.08 and 0.84, while Cryptocurrencies has Topic CrypCurr AutoV Total SCUs 34 41 w=5 0 0 w=4 3 6 w=3 5 2 8 Annotation of Argument Structure To annotate the argumentative part of the essays, we used the coarse-grained argumentative structure proposed by Stab and Gurevych (2014): argument components (major claim, claim, premises) and argument relations (support/attack). Similar to Hidey et al. (2017), we took as annotation unit the proposition instead of the clause, given that premises are frequently propositions that conflate multiple clauses. For this pilot annotation task we labeled the 37 Cryptocurrency essays and used two expert annotators with background in linguistics and/or argumentation. We used Brat as annotation tool.4 The set contains 36 main claims, 559 claims, 277 premises, 560 support relations and 101 attack relations. Ghosh et al. (2016) proposed a set of argumentative features and showed that they correlate well with the holistic essay scores (low, medium and high) when"
W19-4452,P15-1053,0,0.019633,"d questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies: are they the currencies of the future? For the second assignment ("
W19-4452,prasad-etal-2008-penn,0,0.0960809,"le (*.pyr). A pyramid file and a student summary are then the input for the annotator to match phrases in the student summary to pyramid SCUs, which is also exported as XML (*.pan). sive reliability measures in past work on pyramid annotation, we did not re-assess its reliability here. To annotate the content of the argument portion of the essay, we identify all distinct Elementary Discourse Units (EDU). Identifying (segmenting) EDUs from text and representing their meanings play a key role in discourse parsing (Marcu, 2000; Li et al., 2016; Braud et al., 2017). Definitions of EDUs vary, thus Prasad et al. (2008) consider the full range of clause types, including verb arguments and non-finite clauses. To simplify the annotation, we restrict EDUs to propositions derived from tensed clauses that are not verb arguments (such as that complements of verbs of belief). In (Gao et al., 2019), we report the iterative development of reliable annotation guidelines for untrained annotators.2 Annotators first identify the start and end of tensed clauses, omitting discourse connectives from the EDU spans, which can be discontinuous. Annotators then provide a paraphrase of the EDU span as an independent simple sente"
W19-4452,P14-2041,0,0.0311314,"ngful classroom interacˇ tions. Agejev and Snajder (2017) uses ROUGE and BLEU in assessing summary writing from college L2 students. Santamar´ıa Lancho et al. (2018) show that using G-Rubric, an LSA-based tool applying rubric assessment, helps the instructors grade the short text answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlat"
W19-4452,W14-2110,0,0.0271382,"elps the instructors grade the short text answers to open-ended questions, and proves to be reliable, with a Pearson correlation between human graders and GRubric of 0.72. Recent work investigates fine-grained writing assessment, especially on content quality evaluation by diving into linguistic phenomena and structures, combined with various NLP techniques. Klebanov et al. (2014) investigated the correlations between essays scores with a content importance model. Another line of research has studied the role of argumentative features in predicting the overall essay quality (Ong et al., 2014; Song et al., 2014; Klebanov et al., 2016; Ghosh et al., 2016; Persing and Ng, 2015). For example, Klebanov et al. (2016) and Ghosh et al. (2016) examine the relations between argumentation structure features and the holistic essay quality (low, medium and high) applied to TOEFL essays. In this paper, we use the argumentative features introduced by Ghosh et al. (2016), but correlate them with the rubric related to the quality of the argument on a scale of 0-5. 3 1. Autonomous Vehicles: will these change how we travel today? 2. Cybercrime: will education and investment provide the solution? 3. Cryptocurrencies:"
W19-4452,C14-1142,0,0.0172333,"0 for Autonomous Vehicle and 36.76 for Cryptocurrencies. Autonomous Vehicle has 2.75 as average weight of SCUs and Cryptocurrencies has 2.07. Cryptocurrencies has shorter length of EDUs compared to Autonomous Vehicle, as 13.62 and 14.00. For the normalized SCU by total number of EDUs and number of matched EDUs, Autonomous Vehicle shows more with 0.08 and 0.84, while Cryptocurrencies has Topic CrypCurr AutoV Total SCUs 34 41 w=5 0 0 w=4 3 6 w=3 5 2 8 Annotation of Argument Structure To annotate the argumentative part of the essays, we used the coarse-grained argumentative structure proposed by Stab and Gurevych (2014): argument components (major claim, claim, premises) and argument relations (support/attack). Similar to Hidey et al. (2017), we took as annotation unit the proposition instead of the clause, given that premises are frequently propositions that conflate multiple clauses. For this pilot annotation task we labeled the 37 Cryptocurrency essays and used two expert annotators with background in linguistics and/or argumentation. We used Brat as annotation tool.4 The set contains 36 main claims, 559 claims, 277 premises, 560 support relations and 101 attack relations. Ghosh et al. (2016) proposed a s"
W93-0216,P92-1032,0,0.156326,"Missing"
W93-0216,J86-3001,0,0.172822,"Missing"
W93-0216,H92-1089,0,0.507239,"Missing"
W93-0216,J93-3003,1,0.892217,"Missing"
W93-0216,P93-1020,1,0.836662,"Missing"
W97-0210,J93-2001,0,0.0299717,"Missing"
W97-0210,J92-4003,0,0.0087093,"Missing"
W97-0210,C94-1042,0,0.2653,"ourse structure (Morris and Hirst, 2 [] m Exploiting domain-independent syntactic clues A given word may have n distinct senses and appear within m different syntactic contexts, but typically, not all n x m combinations are valid. The syntactic context can partly disambiguate the semantic content. For example, when the verb question has a that-clause complement, it cannot have the sense of &quot;ask&quot;, but rather must have the sense of &quot;challenge&quot;. To identify such interacting syntactic and semantic constraints at the lexical level, we utilize three knowledge bases for verbs: * The COMLEX database (Grishman et al., 1994; Macleod and Grishman, 1995), which includes detailed subcategorization information for each verb, and some adjectives and nouns. 58 m m m m m • Levin's classification of verbs in terms of their allowed alternations (Levin, 1993). Alternations include syntactic transformations such as thereinsertion (e.g., A ship appeared on the horizon ---, There appeared a ship on the horizon) and locative-inversion (e.g., --* On the horizon there appeared a ship). Much in the same way as subcategorization frames, alternations are constrained by the sense of the word; for example, the verb appear allows the"
W97-0210,P93-1023,1,0.822265,"irect route, it allows the automatic identification of the predominant sense of a word in a given text or subject topic. It is indirect because the actual result is groups of word forms, but we presume each group to represent a relatively homogeneous semantic class. Thus we presume that the relevant sense of a given word form in a group is in the same lexical field as the senses of the other word forms in the same group. The process is highly domain-dependent, i.e., the same set of words will be partitioned in different ways when the domain changes. For example, when our word grouping system (Hatzivassiloglou and McKeown, 1993) classified about 280 frequent adjectives in stock market reports, it formed, among others, the cluster {common, preferred}. This cluster would look odd were not the domain considered. ~ This information on predominant senses for each word form in a given corpus can be computed automatically, but remains implicit. To map the results onto word sense associations, and thus explicitly identify the predominant senses, we utilize the links between senses provided by WordNet. We note that while words like question and ask are ultimately connected in WordNet, the actual connections are only between s"
W97-0210,P94-1002,0,0.0217563,"Missing"
W97-0210,P96-1038,0,0.0135642,"ld exclude &quot;much of the English language&quot; by narrowing the lexicon, verb tense and aspect, and syntactic complexity. Such observations inform the increasing trend towards analysis of homogeneous corpora to identify linguistic constraints for use in systems intended to understand or generate coherent discourse. Recent work in this vein includes identification of lexical constraints from textual tutorial dialogue (Moser and Moore, 1995), constraints on illocutionary act type from spoken task-oriented dialogue (Allen et al., 1995), prosodic constraints from spoken information-seeking monologues (Hirschberg and Nakatani, 1996), and constraints on referring expressions from spoken narrative monologue (Passonneau, 1996). Related work suggests that constraints of different types are interdependent (Biber, 1993; Passonneau and Litman, forthcoming), hence should be investigated together. Our ultimate goal is to develop methods to tag lexical semantic features in discourse corpora in order to enhance extraction of constraints of the sort just listed. Two types of investigations that would undoubtedly be enhanced are explorations of the interrelation of lexical cohesion and global discourse structure (Morris and Hirst, 2"
W97-0210,C96-1080,0,0.0148988,"ed specifications of the syntactic frames for each verb (92 distinct subcategorization types). The allowed alternations (which we encoded in machine-readable form from the detailed rules supplied in (Levin, 1993)) provide additional constraints. Mapping the more precise syntactic information in COMLEX to the verb frames of WordNet allows the construction of a more detailed syntactic entry for each word sense, and enables the association of alternation constraints with the senses in WordNet. In the future, it will also allow us to use corpora tagged with COMLEX subcategorization frames, e.g., (Macleod et al., 1996). We have manually constructed a table that maps WordNet syntactic constraints to the ones used in COMLEX (and vice versa) and another that maps allowed alternations from (Levin, 1993) to COMLEX or WordNet syntactic frames. A program consuits the three databases and the mapping tables and, for each word occurrence constructs a list of the senses that are compatible with the syntactic constraints. During this process, a detailed entry for the word is formed, containing both syntactic and semantic information. The resulting entries comprise a rich lexical resource that we plan to use for text ge"
W97-0210,J96-2003,0,0.0182327,"we find the following links between question and the verbs ask, inquire, chal. tion, which lenge, and dispute: (question1, asks), (questiou~, asks), (questions, asks), (questions, inquire~), and (question1, challenge~). Thus, if question is placed in the same semantic group with ask and inquire, the three senses {1, 2, 3} survive out of the five senses of question, with a preference for sense 3. If, on the other hand, question is classified with challenge and dispute, only sense 1 survives. We performed an experiment analyzing a specific verb group produced by one semantic clustering program (McMahon and Smith, 1996). This group contains 19 verbs, all but one of them ambiguous, including ask, call, charge, regard, say, and wish. We measured for each sense of the 19 words how many of the other words have at least one sense linked with that sense in WordNet (in the same toplevel verb sense tree). The results, part of which is shown in Table 2, indicate that some senses are much more strongly connected with the other words in the group, and so probably predominate in the corpus that was used to induce the group. For example, one of the senses of ask, &quot;require&quot; (as in This job asks (for) long hours) is not li"
W97-0210,H93-1061,0,0.0774281,"es with pruned word sense tags using the Brown corpus. The first step of the method is to identify the subcategorization pattern for a specific verb token. Here we rely on heuristics to identify the major constituents to the left and right of a verb token, as described in (Jing et al., 1997). After hypothesizing the subcategorization pattern for a specific verb token, we use our sense restriction matrices (as in Table 1) to tag the verb token with a pruned set of senses. W e evaluate the resulting sense tag against the version of the Brown corpus that has been hand-tagged with WordNet senses (Miller et al., 1993). For appear, which we use as an example throughout this paper, we find 100 tokens in the Brown corpus. Of these, 46 are intransitive or have a locative prepositional phrase complement. Our method tags each of these tokens with two or three possible senses, and in all but one case, the sense tag includes the valid sense. Another 31 tokens are followed by to and a subject-controlled infinitive. In all these cases, our method makes a single, correct prediction out of the eight possible senses. For all 100 uses of appear in the corpus, the average number of possible senses predicted by our method"
W97-0210,J91-1002,0,0.0133805,"Missing"
W97-0210,P93-1024,0,0.0309831,"Missing"
W97-0210,J93-2005,0,0.0286451,"f the large number of alternatives and the likely closeness in meaning among them. Selecting a subset of almost synonymous verb senses is significantly harder than, for example, disambiguating bank between the &quot;edge of river&quot; and '~financial institution&quot; senses. z~ool = l[] ,ooo. . u ,~ m 3 SO0. lhlm----. 1'0 ~) 30 Using domain-dependent semantic classifications to identify predomi n a n t senses The process outlined above has two significant advantages: first, it can be automatically applied, assuming a robust method for parsing the relevant verb phrase context (the experiments presented in (Pustejovsky et al., 1993) depend on the same type of information). Second, it reduces the ambiguity of a given word without sacrificing accuracy, insofar as the three input knowledge sources are accurate. To further restrict the size of the set of valid senses produced, we are currently exploring domaindependent, automatically constructed semantic classifications. Semantic classification programs (Brown et al., 1992; Hatzivassiloglou and McKeown, 1993; Pereira et al., 1993) use statistical information based on cooccurrence with appropriate marker words to partition a set of words into semantic groups or classes. 41 Nu"
W97-0210,H93-1052,0,0.0676495,"Missing"
W97-0210,P95-1018,0,\N,Missing
W97-0210,H92-1045,0,\N,Missing
W97-0903,P88-1000,0,0.0679493,"th domain-specific knowledge that is not explicitly present in the input. [n FLOWDoC and ZEDDoc, semantic enrichment is done at various stages by consulting external ontologies. • D i s c o u r s e Organizer: The discourse organizer performs all the remaining functions prior to lexicalization and surface generation2. Three sub-modules apply general discourse coherence constraints at the levels of discourse, sentence, and sentence constituent. The first module performs aggregation and text linearization operations using an ontology of rhetorical predicates derived from Hobbs (1985) and Polanyi (1988). Linear order and prominence of the subconstituents are then determined, followed by constraints on subconstituents that affect lexical choice (e.g., centering and informational constraints, as in (Passonneau, 1996)). 2|n previous work we referred to this module as the Sentence Planner (Passonneau et al., 1996). A Common Representation All three systems employ a consistent, standardized attribute-value data format that persists from each module to the next. Examples of this internal data format were shown in Figures 1-3. This fbrmat is used for representing and processing conceptualsemantic,"
W97-0903,W94-0319,0,0.0301262,"of P L A N D o c &apos; s architecture as possible, often adapting and generalizing modules that were originally written with only the P L A N D o c system in mind. All three systems employ a modular pipeline architecture. A pipeline architecture is one that separates the functions involved in text generation, such as content planning, discourse organization, lexicalization, and syntactic realization, into distinct modules that operate in sequence. Modular pipeline architectures have a long history of use in text gen16 eration systems (Kukich, 1983a; McKeown, 1985; McDonald and Pustejovsky, 1986; Reiter, 1994), although recent work argues for the need for interaction between modules (Danlos, 1987; Rubinoff, 1992; McKeown et al., 1993). The most powerful argument for using pipeline architectures is the potential benefit of re-using individual modules for subsequent applications. However, with the exception of surface realization modules such as F U F / S U R G E (Elhadad, 1992; Robin, 1994), actual code re-use has been minimal due to the lack of agreement about the order and grouping of subprocesses into modules. In P L A N D o c , FLowDoc, and ZEDDoc, we utilize the following main modules, in the o"
W97-0903,P93-1031,1,0.461759,"th only the P L A N D o c system in mind. All three systems employ a modular pipeline architecture. A pipeline architecture is one that separates the functions involved in text generation, such as content planning, discourse organization, lexicalization, and syntactic realization, into distinct modules that operate in sequence. Modular pipeline architectures have a long history of use in text gen16 eration systems (Kukich, 1983a; McKeown, 1985; McDonald and Pustejovsky, 1986; Reiter, 1994), although recent work argues for the need for interaction between modules (Danlos, 1987; Rubinoff, 1992; McKeown et al., 1993). The most powerful argument for using pipeline architectures is the potential benefit of re-using individual modules for subsequent applications. However, with the exception of surface realization modules such as F U F / S U R G E (Elhadad, 1992; Robin, 1994), actual code re-use has been minimal due to the lack of agreement about the order and grouping of subprocesses into modules. In P L A N D o c , FLowDoc, and ZEDDoc, we utilize the following main modules, in the order listed below: • M e s s a g e G e n e r a t o r : The message generator transcribes the raw data from LEIS-PLAN execution"
W97-0903,A94-1002,1,0.829776,"t generation applications. At the same time, to take full advantage of these opportunities, text generation systems must be easily adaptable to new domains, changing data formats, and distinct underlying ontologies. One crucial factor contributing to the generalization and subsequent practical and commercial viability of text generation systems is the adaptation and re-use of text generation modules and the development of re-usable tools and techniques. In this paper, we focus on the lessons learned during the successive development of three text generation systems at Bellcore: P L A N D o c (McKeown et al., 1994) summarizes execution traces of an expert system for telephone network capacity expansion analysis; FLOwDoc (Passonneau et al., 1996) provides summaries of the most important events in flow diagrams constructed during business reengineering; and Z E D D o c (Passonnean et al., 1997) produces summaries of activity for a user-specified set of advertisements within a user-specified time period from logs of WWW page hits. We built FLowDoc and Z E D D o c by adapting components of the P L A N D o c system. The transfer of the original P L A N D o c modules to new domains led to the replacement of s"
W97-0903,P95-1053,0,0.465208,"icantly adapted our P L A N DOC architecture for use in FLOwDOC, but we were able to re-use the F L o w D o c architecture and much of its code in Z E D D o c . Figure 4 contrasts the architecture of P L A N D o c with those of F L O w D o c and Z E D D o c . In fact, the functions of the Lexicalization and Surface Generation modules remained constant across all three systems. But the functions of the first three modules shifted significantly from P L A N D o c to FLOwDOC. In particular, the function of message aggregation lay exclusively in the Discourse Organization module in P L A N D o c (Shaw, 1995), whereas aggregation functions are executed in both the Message Generation and Discourse Organization modules in FLOWDOC. Because the development of domain-independent, plug-and-play ontology modules is one of the major features that affected these shifts in function, and because such modules greatly increase the portability of the system, we devote the next section to a more detailed description of the function of ontological generalization. 6 (a) Overall architecture for P L A N D o c . Ontological Generalization Ontological generalization refers to the problem of composing, with the help o"
