1999.mtsummit-1.57,P96-1006,0,0.153351,"nese MT system. There is a large body of previous work on word sense disambiguation (Kelly 1975; Church 1990; McRoy 1992; Liu 1993; Tanaka 1994; Yarowsky 1995; Hwee 1996). Early work made use of manually coded knowledge, but it required time-consuming and laborious work for knowledge acquisition (Kelly and Stone 1975). The recent emphases on corpus-based WSD usually fall into two categories: supervised and unsupervised. Yarowsky (1995) - 390- used an unsupervised learning procedure. However, his experiment was only restricted to “binary” WSD, a kind of coarse sense distinction (Yarowsky 1995; Hwee and Lee 1996). LEXAS (Hwee and Lee 1996) adopted a supervised learning method using multiple knowledge sources such as a human sense-tagged corpus, the POS of neighboring words, morphological forms, local collocations, unordered sets of surrounding words, and verbobject syntactic relations. Since the extracted knowledge in these methods is stored in words themselves, they require a large amount of space to store the knowledge, and they have lower applicability. In this paper, a corpus-based WSD method is proposed. Unlike previous work (Yarowsky 1995; Hwee 1996) that restricted the use of linguistic knowled"
1999.mtsummit-1.57,P90-1032,0,0.0180866,"pes that can co-occur with the homograph nwun (eye) in the form of LSP type2, and their frequencies. For a homograph W, conceptual frequency patterns (CFPs), i.e., ({&lt;C1,f1>, &lt;C2,f2>, ...,&lt;Ck,fk>}, typei, W(Si)), are constructed for each type of LSP, where fi is the fre- 392- Figure 3. Histogram for Concept Type Frequency. quency (number of appearances) of concept Ci appearing in the corpus, typei is an LSP type, and W(Si) is a homograph W with the sense Si. CFPs for UCWs can have the form of ({&lt;C1,f1>, &lt;C2f2>, ...,&lt;Ck,fk>), UCW, W(Si)). To perform type abstraction, we refer to Smadja's work (Smadja 1990, 1993), and define the standard deviation σ l of the code frequency at level l (denoted as L l) and kf l (the strength of code frequency f at L l and represents the amount of standard deviation above the average frequency fave.l). In the formulas, fk l denotes the frequency of concept code Ck of the Kadokawa thesaurus at L l , and n l the number of concept codes at L l . MT Summit VII _______________________________________________________________ Sept. 1999 The standard deviation σl at Ll characterizes the shape of the distribution of code frequencies. If σl is small, then the shape of the h"
1999.mtsummit-1.57,J93-1007,0,0.0247617,"Missing"
1999.mtsummit-1.57,C94-2116,0,0.0288215,"on which sense is used in a given context. In general, it is much more difficult for Koreanto-Japanese MT to resolve such word sense ambiguities than its opposite, Japanese-to-Korean MT. This is because unlike Japanese words whose stem is usually written in Kanji (the Chinese ideographic script), Korean words may appear written in Hangul (the Korean phonetic alphabet) only. Thus, word sense disambiguation (WSD) is essential to lexical transfer in an Korean-to-Japanese MT system. There is a large body of previous work on word sense disambiguation (Kelly 1975; Church 1990; McRoy 1992; Liu 1993; Tanaka 1994; Yarowsky 1995; Hwee 1996). Early work made use of manually coded knowledge, but it required time-consuming and laborious work for knowledge acquisition (Kelly and Stone 1975). The recent emphases on corpus-based WSD usually fall into two categories: supervised and unsupervised. Yarowsky (1995) - 390- used an unsupervised learning procedure. However, his experiment was only restricted to “binary” WSD, a kind of coarse sense distinction (Yarowsky 1995; Hwee and Lee 1996). LEXAS (Hwee and Lee 1996) adopted a supervised learning method using multiple knowledge sources such as a human sense-tagge"
1999.mtsummit-1.57,P95-1026,0,0.021153,"e is used in a given context. In general, it is much more difficult for Koreanto-Japanese MT to resolve such word sense ambiguities than its opposite, Japanese-to-Korean MT. This is because unlike Japanese words whose stem is usually written in Kanji (the Chinese ideographic script), Korean words may appear written in Hangul (the Korean phonetic alphabet) only. Thus, word sense disambiguation (WSD) is essential to lexical transfer in an Korean-to-Japanese MT system. There is a large body of previous work on word sense disambiguation (Kelly 1975; Church 1990; McRoy 1992; Liu 1993; Tanaka 1994; Yarowsky 1995; Hwee 1996). Early work made use of manually coded knowledge, but it required time-consuming and laborious work for knowledge acquisition (Kelly and Stone 1975). The recent emphases on corpus-based WSD usually fall into two categories: supervised and unsupervised. Yarowsky (1995) - 390- used an unsupervised learning procedure. However, his experiment was only restricted to “binary” WSD, a kind of coarse sense distinction (Yarowsky 1995; Hwee and Lee 1996). LEXAS (Hwee and Lee 1996) adopted a supervised learning method using multiple knowledge sources such as a human sense-tagged corpus, the P"
2001.mtsummit-papers.33,P89-1010,0,0.02324,"er semantic relations Total Number 1,100 112,746 2,093 115,939 Importing semantic relation instances Table 3: Final ontological relation instances in the LIP ontology Applied Valency Patterns <SC, synRel, DC, frequency> Semantic Relation Instances <SC, SR, DC, frequency> Apply secured dictionary information with high precision Semi-Automatic Relation Mapping Verb’s valency information Figure 3: Construction flow of ontology training data Success? YES NO based on the information theoretic concept of mutual information (MI), which is a natural measure of the dependence between random variables (Church & Hanks, 1989). Resnik (1995) suggested a measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. However, his method differs from ours in that we consider all semantic relations in the ontology, not taxonomy relations only. To implement this idea, source concepts (SC) and semantic relations (SR) are bound into one entity, since SR is mainly influenced by SC, not the destination concepts (DC). Therefore, if two entities, < SC, SR>, and DC have probabilities P(<SC, SR>) and P(DC), then their mutual information I(<SC, SR>, DC) is defined as:   P (< SC , SR >, DC ) I ("
2001.mtsummit-papers.33,C00-1079,1,0.831366,"thesaurus (Ohno & Hamanishi, 1981) and its taxonomic hierarchy into the ontology. The second strategy is to extend the hierarchy of the Kadokawa thesaurus by inserting additional semantic relations into its hierarchy. The additional semantic relations can be classified as case relations and other semantic relations. The former can be obtained by converting the established valency information in bilingual dictionaries of COBALT-J/K (Collocation-Based Language Translator from Japanese to Korean) (Park et al., 1997) and COBALT-K/J (Collocation-Based Language Translator from Korean to Japanese) (Moon & Lee, 2000) MT systems, as well as from the case frames in SELK (Sejong Electronic Lexicon of Korean) (Hong & Pak, 2001). The latter can be acquired from concept co-occurrence information, which is extracted automatically from a corpus (Li et al., 2000). The remainder of this paper is organized as follows. In the next section, we describe the principles of ontology design and the semi-automatic ontology construction methodology. The ontology learning phase is explained in Section 3. An ontology-based WSD algorithm is given in Section 4. Experimental results are presented and analyzed in Section 5. Finall"
2009.mtsummit-posters.13,E06-1032,0,0.100778,"Missing"
2009.mtsummit-posters.13,P07-1002,0,0.230533,"ormation embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approaches integrated global reordering within an overall model. This process, however, increases the complexity of decoding and adds to the difficulty of error analysis. Postprocessing approaches to the translation result have received little attention compared to the other approaches. Chang and Toutanova (2007) generated an n-best list using an n-gram language model with projective constraints for target languages in English to Japanese translation. The n-best list was reranked using a log-linear model with various syntactic features. They also modeled the global reordering model for target dependency trees with the local tree order model (LTOM). The LTOM assumes that orders of the local tree in the target dependency tree are independent, and that a dependent node has a relative offset to its head. Chang and Toutanova (2007) obtained a dependency tree of target languages by projecting the tree of so"
2009.mtsummit-posters.13,P05-1066,0,0.267582,"Missing"
2009.mtsummit-posters.13,J08-3003,0,0.0734281,"Missing"
2009.mtsummit-posters.13,N04-1035,0,0.0470636,"ce parse tree (Collins et al., 2005; Li et al., 2009), and others have obtained the reordering statistics from training corpora using word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approaches integrated global reordering within an overall model. This process, however, increases the complexity of decoding and adds to the difficulty of error analysis. Postprocessing approaches to the translation result have received little attention compared to the other approaches. Chang and Toutanova (2007) generated an n-best list using an n-gram language model with projective constraints for target languages in English to Japanese translation."
2009.mtsummit-posters.13,W06-3601,0,0.0207859,"ve used manually built sets of rules to apply to the source parse tree (Collins et al., 2005; Li et al., 2009), and others have obtained the reordering statistics from training corpora using word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approaches integrated global reordering within an overall model. This process, however, increases the complexity of decoding and adds to the difficulty of error analysis. Postprocessing approaches to the translation result have received little attention compared to the other approaches. Chang and Toutanova (2007) generated an n-best list using an n-gram language model with projective constraints"
2009.mtsummit-posters.13,W09-0433,1,0.842029,"ge structure as per the source language. We then recover the original word order of the target language in the postprocessing stage. 2 Previous Work Previous approaches to preprocessing have been focused on reordering the source sentence to follow the word order of the target sentence. Many researchers using PBSMT systems (with and without the distortion model after preprocessing) have tried to solve the global word order during preprocessing and have let the PBSMT adjust the local word order. Some have used manually built sets of rules to apply to the source parse tree (Collins et al., 2005; Li et al., 2009), and others have obtained the reordering statistics from training corpora using word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gild"
2009.mtsummit-posters.13,W05-0904,0,0.0342826,"005), we gain 3.94 and 4.54 points (Table 4). A headword chain is a sequence of nodes from a dependent to its ancestors in a dependency tree. HWCM = Automatic Evaluation Human evaluators score the translation result with respect to adequacy and fluency, and regard the average of the two scores as translation quality. CallisonBurch et al. (2006) criticized n-gram based automatic evaluation metrics for weak correlation with human evaluation, especially with regards to fluency. Dependency-based automatic evaluation metrics have been developed to overcome the limitations of the n-gram based ones (Liu and Gildea, 2005; Owczarzak et al., 2007). They suggested metrics to evaluate machine translation results by parsing both translation results and reference sentences. Consequently, we use both n-gram based and dependencybased metrics in this paper. Using n-gram based metrics, BLEU and NIST8 , we gain 0.49 BLEU but lose 0.23 NIST points (Table 3). On the development corpus, we gain 3.27 BLEU points. Using a dependency-based headword chain based metric (HWCM) proposed by Liu and Gildea 8 We use mt-eval11b.pl. http://www.itl.nist.gov/iad/mig/tools/ P d=1 of c in reference |c|=d # P |c|=d # of c , where D is the"
2009.mtsummit-posters.13,W08-0308,0,0.0187314,"t al., 2009), and others have obtained the reordering statistics from training corpora using word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approaches integrated global reordering within an overall model. This process, however, increases the complexity of decoding and adds to the difficulty of error analysis. Postprocessing approaches to the translation result have received little attention compared to the other approaches. Chang and Toutanova (2007) generated an n-best list using an n-gram language model with projective constraints for target languages in English to Japanese translation. The n-best list was reranked using a log-li"
2009.mtsummit-posters.13,P06-1077,0,0.0280374,"lt sets of rules to apply to the source parse tree (Collins et al., 2005; Li et al., 2009), and others have obtained the reordering statistics from training corpora using word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approaches integrated global reordering within an overall model. This process, however, increases the complexity of decoding and adds to the difficulty of error analysis. Postprocessing approaches to the translation result have received little attention compared to the other approaches. Chang and Toutanova (2007) generated an n-best list using an n-gram language model with projective constraints for target languag"
2009.mtsummit-posters.13,W06-1606,0,0.0238643,"s et al., 2005; Li et al., 2009), and others have obtained the reordering statistics from training corpora using word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approaches integrated global reordering within an overall model. This process, however, increases the complexity of decoding and adds to the difficulty of error analysis. Postprocessing approaches to the translation result have received little attention compared to the other approaches. Chang and Toutanova (2007) generated an n-best list using an n-gram language model with projective constraints for target languages in English to Japanese translation. The n-best list was"
2009.mtsummit-posters.13,H05-1066,0,0.112628,"Missing"
2009.mtsummit-posters.13,J04-4002,0,0.0354654,"rove the fluency of the output of the phrase-based SMT (PBSMT) system. We parse the translation results that follow the source language order into non-projective dependency trees, then reorder dependency trees to obtain fluent target sentences. Our method ensures that the translation results are grammatically correct and achieves major improvements over PBSMT using dependency-based metrics. 1 Introduction Word order divergence is a major issue in phrasebased statistical machine translation (PBSMT). PBSMT assumes symmetry of structure in alignment heuristics. That is, the alignment heuristics (Och and Ney, 2004) used in PBSMT tend to grow diagonally from the intersection of bidirectional word alignments. If the word alignments are not symmetrical, the heuristics fail to find the correct word alignment. Also, the reordering model in PBSMT limits the movement of target phrases to a predefined window size. The heterogeneous structure between two languages requires long-distance movements which are impossible in PBSMT. A pair of sentences in English and Japanese embeds mass structural divergences. For example, English follows an SVO (subject, verb, object) structure and uses prepositions for functional w"
2009.mtsummit-posters.13,P05-1034,0,0.204405,"sing word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approaches integrated global reordering within an overall model. This process, however, increases the complexity of decoding and adds to the difficulty of error analysis. Postprocessing approaches to the translation result have received little attention compared to the other approaches. Chang and Toutanova (2007) generated an n-best list using an n-gram language model with projective constraints for target languages in English to Japanese translation. The n-best list was reranked using a log-linear model with various syntactic features. They also modeled the global reordering mo"
2009.mtsummit-posters.13,C04-1073,0,0.100328,"stprocessing stage. 2 Previous Work Previous approaches to preprocessing have been focused on reordering the source sentence to follow the word order of the target sentence. Many researchers using PBSMT systems (with and without the distortion model after preprocessing) have tried to solve the global word order during preprocessing and have let the PBSMT adjust the local word order. Some have used manually built sets of rules to apply to the source parse tree (Collins et al., 2005; Li et al., 2009), and others have obtained the reordering statistics from training corpora using word alignment (Xia and McCord, 2004; Zhang et al., 2007). These methods are simple but effective especially for global reordering. However, they require an accurate and robust parser for the source language to minimize errors and to avoid parsing failure. Syntax-based approaches are reliant on global reordering information embedded in the translation model using either the parse tree of source languages (Huang et al., 2006; Liu et al., 2006), target languages (Galley et al., 2004; Marcu et al., 2006; Liu and Gildea, 2008), or by using the parse trees of both source and target languages (Quirk et al., 2005). Syntax-based approac"
2009.mtsummit-posters.13,W07-0401,0,0.0544181,"Missing"
2009.mtsummit-posters.13,J08-4010,0,\N,Missing
2010.amta-papers.26,P08-1087,0,0.0129094,"se) is a fully inflected lexical form separated by a space in a sentence. Each eojeol consists of one or more base forms (content morphemes) and inflections (functional morphemes, postpositions or verbal endings). Eojeol easily causes data sparseness problems and we have to consider a morpheme as a translation unit for Korean. In our corpus, each eojeol contains 2.2 morphemes on average. 3 Related work Recently, a number of researchers have studied complex morphology generation in SMT systems where the translation direction is from a morphologicallypoor language to a morphologically-rich one. Avramidis and Koehn (2008) proposed a method that extracts information from the syntax of source sentences to enrich the morphologically poor language using the framework of factored SMT. Also, Ramanathan et al. (2009) adopted factored models to factorize syntactic/semantic relations and suffixes to help generate inflections and case markers. Factored models can tightly combine linguistic features into the decoding phase, while expanding the search space at the same time. Some researchers have tried to develop independent components to handle complex morphology generation. This kind of research has the advantage that i"
2010.amta-papers.26,W09-2307,0,0.0228017,"ition generation leads to ungrammatical output sentences because most postpositions indicate grammatical relations in Korean. This paper describes a method for transferring the syntactic relations of subject-verb-object (SVO) patterns, and enriching the Chinese sentences by inserting the corresponding transferred relations as pseudo words. The SVO pattern refers to a predicate with immediate children that have a subject or an object relation in a dependency tree. Specifically, we adopt grammatical relations that are produced by Stanford Chinese typed dependency parser (Levy and Manning, 2003; Chang et al., 2009). The previous work provides the following 7 grammatical relations that are related to subject and object relation: nsubj, xsubj, nsubjpass, top, dobj, range, and attr.1 In this paper, the SVO pattern is a general term which represents constructions that consist of any number of above 7 grammatical relations with a corresponding head predicate. SVO patterns frequently occur in Chinese dependency trees and cause incorrect postposition generations when they are translated into Korean. Our proposed method has the following characteristics. First, since Korean postpositions indicate grammatical fu"
2010.amta-papers.26,P09-2059,0,0.300383,"the decoding phase, while expanding the search space at the same time. Some researchers have tried to develop independent components to handle complex morphology generation. This kind of research has the advantage that it does not introduce any other complexity to the SMT decoder. Toutanova and Suzuki (2007), Toutanova et al. (2008) and Minkov et al. (2007) suggested postprocessing models that predict inflected word forms utilizing morpho-syntactic information from both source and target sentences. The inflection prediction model chooses the correct inflections of given target language stems. Hong et al. (2009) proposed bridging morphosyntactic gaps as a preprocessing to an English-toKorean SMT system. They utilized a set of syntactic relations from source sentences and directly inserted them as pseudo words to generate intermediate sentences. The main aim of their work was to decrease the null alignments of Korean functional morphemes, and as a result to generate appropriate functional words. However, this method only considers the syntax of source sentences, and therefore it cannot sufficiently reflect the structural differences between the source and target sentences. Subj Parsed Chinese input Ob"
2010.amta-papers.26,P07-2045,0,0.0103987,"Missing"
2010.amta-papers.26,W04-3250,0,0.0901921,"Missing"
2010.amta-papers.26,2005.mtsummit-papers.11,0,0.00695442,"erates more adequate Korean postpositions. We transfer syntactic relations of subject-verb-object patterns in Chinese sentences and enrich them with transferred syntactic relations in order to reduce the morpho-syntactic differences. The effectiveness of our proposed method is measured with lexical units of various granularities. Human evaluation also suggest improvements over previous methods, which are consistent with the result of the automatic evaluation. 1 Introduction Translating from a morphologically poor language to a morphologically rich one is more difficult than the opposite case (Koehn, 2005). If the source language is a morphologically poor language, surface words only cannot provide sufficient linguistic clues to generate the complex morphology needed for the morphologically rich target language in a Statistical Machine Translation (SMT) system. Chinese and Korean are a morpho-syntactically divergent language pair and to generate adequate Korean postpositions is a challenging task in Chinese-to-Korean SMT. Wrong postposition generation leads to ungrammatical output sentences because most postpositions indicate grammatical relations in Korean. This paper describes a method for tr"
2010.amta-papers.26,P03-1056,0,0.0307678,"orean SMT. Wrong postposition generation leads to ungrammatical output sentences because most postpositions indicate grammatical relations in Korean. This paper describes a method for transferring the syntactic relations of subject-verb-object (SVO) patterns, and enriching the Chinese sentences by inserting the corresponding transferred relations as pseudo words. The SVO pattern refers to a predicate with immediate children that have a subject or an object relation in a dependency tree. Specifically, we adopt grammatical relations that are produced by Stanford Chinese typed dependency parser (Levy and Manning, 2003; Chang et al., 2009). The previous work provides the following 7 grammatical relations that are related to subject and object relation: nsubj, xsubj, nsubjpass, top, dobj, range, and attr.1 In this paper, the SVO pattern is a general term which represents constructions that consist of any number of above 7 grammatical relations with a corresponding head predicate. SVO patterns frequently occur in Chinese dependency trees and cause incorrect postposition generations when they are translated into Korean. Our proposed method has the following characteristics. First, since Korean postpositions in"
2010.amta-papers.26,P07-1017,0,0.0247201,"guage using the framework of factored SMT. Also, Ramanathan et al. (2009) adopted factored models to factorize syntactic/semantic relations and suffixes to help generate inflections and case markers. Factored models can tightly combine linguistic features into the decoding phase, while expanding the search space at the same time. Some researchers have tried to develop independent components to handle complex morphology generation. This kind of research has the advantage that it does not introduce any other complexity to the SMT decoder. Toutanova and Suzuki (2007), Toutanova et al. (2008) and Minkov et al. (2007) suggested postprocessing models that predict inflected word forms utilizing morpho-syntactic information from both source and target sentences. The inflection prediction model chooses the correct inflections of given target language stems. Hong et al. (2009) proposed bridging morphosyntactic gaps as a preprocessing to an English-toKorean SMT system. They utilized a set of syntactic relations from source sentences and directly inserted them as pseudo words to generate intermediate sentences. The main aim of their work was to decrease the null alignments of Korean functional morphemes, and as a"
2010.amta-papers.26,P03-1021,0,0.0277518,"Missing"
2010.amta-papers.26,2001.mtsummit-papers.68,0,0.0881925,"Missing"
2010.amta-papers.26,P09-1090,0,0.0551354,"or verbal endings). Eojeol easily causes data sparseness problems and we have to consider a morpheme as a translation unit for Korean. In our corpus, each eojeol contains 2.2 morphemes on average. 3 Related work Recently, a number of researchers have studied complex morphology generation in SMT systems where the translation direction is from a morphologicallypoor language to a morphologically-rich one. Avramidis and Koehn (2008) proposed a method that extracts information from the syntax of source sentences to enrich the morphologically poor language using the framework of factored SMT. Also, Ramanathan et al. (2009) adopted factored models to factorize syntactic/semantic relations and suffixes to help generate inflections and case markers. Factored models can tightly combine linguistic features into the decoding phase, while expanding the search space at the same time. Some researchers have tried to develop independent components to handle complex morphology generation. This kind of research has the advantage that it does not introduce any other complexity to the SMT decoder. Toutanova and Suzuki (2007), Toutanova et al. (2008) and Minkov et al. (2007) suggested postprocessing models that predict inflect"
2010.amta-papers.26,N07-1007,0,0.100329,"word order only, and adverb mostly by prepositions. On the other hand, Korean is a highly agglutinative language with rich functional morphemes such as postpositions and verbal endings. Korean postpositions include case markers, auxiliary particles, and conjunctive particles. Most of the case markers are utilized to signal the grammatical relations of the complement Noun Phrase(NP) and its corresponding predicate. In our training corpus, there are 290 unique postpositions. Among them, 79 are case markers. Japanese, which belongs to the same language family as Korean, has only 18 case markers(Toutanova and Suzuki, 2007). As Korean postpositions are quite diverse and indicate the syntactic relations in a sentence, correct postposition generation directly leads to producing grammatical sentences in SMT systems. The basic translation units in Chinese-to-Korean SMT are usually morphemes. In Chinese, the sentences are segmented into words, and each segmented word is a morpheme. In Korean, eojeol (similar to bunsetsu in Japanese) is a fully inflected lexical form separated by a space in a sentence. Each eojeol consists of one or more base forms (content morphemes) and inflections (functional morphemes, postpositio"
2010.amta-papers.26,P08-1059,0,0.0593895,"the morphologically poor language using the framework of factored SMT. Also, Ramanathan et al. (2009) adopted factored models to factorize syntactic/semantic relations and suffixes to help generate inflections and case markers. Factored models can tightly combine linguistic features into the decoding phase, while expanding the search space at the same time. Some researchers have tried to develop independent components to handle complex morphology generation. This kind of research has the advantage that it does not introduce any other complexity to the SMT decoder. Toutanova and Suzuki (2007), Toutanova et al. (2008) and Minkov et al. (2007) suggested postprocessing models that predict inflected word forms utilizing morpho-syntactic information from both source and target sentences. The inflection prediction model chooses the correct inflections of given target language stems. Hong et al. (2009) proposed bridging morphosyntactic gaps as a preprocessing to an English-toKorean SMT system. They utilized a set of syntactic relations from source sentences and directly inserted them as pseudo words to generate intermediate sentences. The main aim of their work was to decrease the null alignments of Korean funct"
2010.amta-papers.26,P02-1040,0,\N,Missing
2010.amta-papers.30,W09-2307,0,0.0900777,"elocation of one predicate across another in predicate-predicate patterns in the D-tree. Without any linguistic clues from the surface forms, it is difficult to compile reordering rules for the predicatepredicate patterns. In this paper, we explore various linguistic knowledge for the purpose of effective long-distance reordering of Chinese D-trees. As a preprocessing to a phrase-based SMT, a number of researchers have proposed syntactic reordering approaches to phrase structure parse trees (PS-trees) (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2009) and Dtrees (Chang et al., 2009; Xu et al., 2009; Hong et al., 2009). Previous work on deterministic syntactic reordering in a phrase-based SMT has been effective for language pairs that belong to different wordorder typologies such as Chinese and Korean. This kind of reordering approach is very flexible to combine with various decoding models without adding computational complexity to the decoding phase. Syntactic reordering methods for PS-trees and Dtrees have their advantages and disadvantages due to the differences in their constituent and dependency structures. PS-trees contain hierarchy and precedence information of s"
2010.amta-papers.30,P05-1033,0,0.0807493,"Missing"
2010.amta-papers.30,P05-1066,0,0.0375412,"hologically poor language. We define long-distance reordering as the relocation of one predicate across another in predicate-predicate patterns in the D-tree. Without any linguistic clues from the surface forms, it is difficult to compile reordering rules for the predicatepredicate patterns. In this paper, we explore various linguistic knowledge for the purpose of effective long-distance reordering of Chinese D-trees. As a preprocessing to a phrase-based SMT, a number of researchers have proposed syntactic reordering approaches to phrase structure parse trees (PS-trees) (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2009) and Dtrees (Chang et al., 2009; Xu et al., 2009; Hong et al., 2009). Previous work on deterministic syntactic reordering in a phrase-based SMT has been effective for language pairs that belong to different wordorder typologies such as Chinese and Korean. This kind of reordering approach is very flexible to combine with various decoding models without adding computational complexity to the decoding phase. Syntactic reordering methods for PS-trees and Dtrees have their advantages and disadvantages due to the differences in their constituent and dependency st"
2010.amta-papers.30,P09-2059,0,0.0145346,"ther in predicate-predicate patterns in the D-tree. Without any linguistic clues from the surface forms, it is difficult to compile reordering rules for the predicatepredicate patterns. In this paper, we explore various linguistic knowledge for the purpose of effective long-distance reordering of Chinese D-trees. As a preprocessing to a phrase-based SMT, a number of researchers have proposed syntactic reordering approaches to phrase structure parse trees (PS-trees) (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2009) and Dtrees (Chang et al., 2009; Xu et al., 2009; Hong et al., 2009). Previous work on deterministic syntactic reordering in a phrase-based SMT has been effective for language pairs that belong to different wordorder typologies such as Chinese and Korean. This kind of reordering approach is very flexible to combine with various decoding models without adding computational complexity to the decoding phase. Syntactic reordering methods for PS-trees and Dtrees have their advantages and disadvantages due to the differences in their constituent and dependency structures. PS-trees contain hierarchy and precedence information of syntactic units (words or Ch1. En1. Kr"
2010.amta-papers.30,P07-2045,0,0.0074375,"Missing"
2010.amta-papers.30,W04-3250,0,0.0295132,"an IP (simple clause headed by INFL.) that acts as a sentential object in the sentences. 4 5 Table 5: Corpus profile of Dong-A newspaper. Experiment 5.1 Experimental setting Our baseline system is the state-of-the-art phraseand hierarchical phrase-based SMT system built in Moses (Chiang, 2005; Koehn et al., 2007) with 5gram SRI language modeling (Stolcke, 2002) tuned with Minimum Error Rate Training (MERT) (Och, 2003). We adopt NIST (Doddington, 2002) and BLEU (Papineni et al., 2001) as our evaluation metrics. A significance test is also conducted using a paired bootstrap resampling method5 (Koehn, 2004). We use the Stanford Chinese typed dependency parser (Levy and Manning, 2003; Chang et al., 2009) to parse Chinese sentences. Chinese sentences in training and test corpora are first parsed into dependency trees and are applied to a series of syntactic reordering rules recursively from the root to the bottom. Korean sentences are segmented into morphemes using an in-house morphological analyzer6 . We designed two experiments with different types of knowledge: the first is to assess the effectiveness of the heuristic classifier with verb lists from the PKU dictionary, and the second with the S"
2010.amta-papers.30,P03-1056,0,0.0334722,"ject in the sentences. 4 5 Table 5: Corpus profile of Dong-A newspaper. Experiment 5.1 Experimental setting Our baseline system is the state-of-the-art phraseand hierarchical phrase-based SMT system built in Moses (Chiang, 2005; Koehn et al., 2007) with 5gram SRI language modeling (Stolcke, 2002) tuned with Minimum Error Rate Training (MERT) (Och, 2003). We adopt NIST (Doddington, 2002) and BLEU (Papineni et al., 2001) as our evaluation metrics. A significance test is also conducted using a paired bootstrap resampling method5 (Koehn, 2004). We use the Stanford Chinese typed dependency parser (Levy and Manning, 2003; Chang et al., 2009) to parse Chinese sentences. Chinese sentences in training and test corpora are first parsed into dependency trees and are applied to a series of syntactic reordering rules recursively from the root to the bottom. Korean sentences are segmented into morphemes using an in-house morphological analyzer6 . We designed two experiments with different types of knowledge: the first is to assess the effectiveness of the heuristic classifier with verb lists from the PKU dictionary, and the second with the SVM classifier that shows the highest performances in the classification. 5.2"
2010.amta-papers.30,W09-0433,1,0.894281,"-distance reordering as the relocation of one predicate across another in predicate-predicate patterns in the D-tree. Without any linguistic clues from the surface forms, it is difficult to compile reordering rules for the predicatepredicate patterns. In this paper, we explore various linguistic knowledge for the purpose of effective long-distance reordering of Chinese D-trees. As a preprocessing to a phrase-based SMT, a number of researchers have proposed syntactic reordering approaches to phrase structure parse trees (PS-trees) (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2009) and Dtrees (Chang et al., 2009; Xu et al., 2009; Hong et al., 2009). Previous work on deterministic syntactic reordering in a phrase-based SMT has been effective for language pairs that belong to different wordorder typologies such as Chinese and Korean. This kind of reordering approach is very flexible to combine with various decoding models without adding computational complexity to the decoding phase. Syntactic reordering methods for PS-trees and Dtrees have their advantages and disadvantages due to the differences in their constituent and dependency structures. PS-trees contain hierarchy"
2010.amta-papers.30,P03-1021,0,0.0430321,"Missing"
2010.amta-papers.30,2001.mtsummit-papers.68,0,0.0142403,"light.joachims.org, version 6.02. The Penn Chinese Treebank is annotated with the functional tags of phrase such as IP-OBJ. IP-OBJ is an IP (simple clause headed by INFL.) that acts as a sentential object in the sentences. 4 5 Table 5: Corpus profile of Dong-A newspaper. Experiment 5.1 Experimental setting Our baseline system is the state-of-the-art phraseand hierarchical phrase-based SMT system built in Moses (Chiang, 2005; Koehn et al., 2007) with 5gram SRI language modeling (Stolcke, 2002) tuned with Minimum Error Rate Training (MERT) (Och, 2003). We adopt NIST (Doddington, 2002) and BLEU (Papineni et al., 2001) as our evaluation metrics. A significance test is also conducted using a paired bootstrap resampling method5 (Koehn, 2004). We use the Stanford Chinese typed dependency parser (Levy and Manning, 2003; Chang et al., 2009) to parse Chinese sentences. Chinese sentences in training and test corpora are first parsed into dependency trees and are applied to a series of syntactic reordering rules recursively from the root to the bottom. Korean sentences are segmented into morphemes using an in-house morphological analyzer6 . We designed two experiments with different types of knowledge: the first is"
2010.amta-papers.30,D07-1077,0,0.0476522,"Missing"
2010.amta-papers.30,C04-1073,0,0.0348902,"ince Chinese is a morphologically poor language. We define long-distance reordering as the relocation of one predicate across another in predicate-predicate patterns in the D-tree. Without any linguistic clues from the surface forms, it is difficult to compile reordering rules for the predicatepredicate patterns. In this paper, we explore various linguistic knowledge for the purpose of effective long-distance reordering of Chinese D-trees. As a preprocessing to a phrase-based SMT, a number of researchers have proposed syntactic reordering approaches to phrase structure parse trees (PS-trees) (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2009) and Dtrees (Chang et al., 2009; Xu et al., 2009; Hong et al., 2009). Previous work on deterministic syntactic reordering in a phrase-based SMT has been effective for language pairs that belong to different wordorder typologies such as Chinese and Korean. This kind of reordering approach is very flexible to combine with various decoding models without adding computational complexity to the decoding phase. Syntactic reordering methods for PS-trees and Dtrees have their advantages and disadvantages due to the differences in their constit"
2010.amta-papers.30,N09-1028,0,0.124122,"dicate across another in predicate-predicate patterns in the D-tree. Without any linguistic clues from the surface forms, it is difficult to compile reordering rules for the predicatepredicate patterns. In this paper, we explore various linguistic knowledge for the purpose of effective long-distance reordering of Chinese D-trees. As a preprocessing to a phrase-based SMT, a number of researchers have proposed syntactic reordering approaches to phrase structure parse trees (PS-trees) (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007; Li et al., 2009) and Dtrees (Chang et al., 2009; Xu et al., 2009; Hong et al., 2009). Previous work on deterministic syntactic reordering in a phrase-based SMT has been effective for language pairs that belong to different wordorder typologies such as Chinese and Korean. This kind of reordering approach is very flexible to combine with various decoding models without adding computational complexity to the decoding phase. Syntactic reordering methods for PS-trees and Dtrees have their advantages and disadvantages due to the differences in their constituent and dependency structures. PS-trees contain hierarchy and precedence information of syntactic units (w"
2010.amta-papers.30,P02-1040,0,\N,Missing
2010.amta-srw.2,D09-1021,0,0.013473,"example of a pair of the source sentence with the dependency tree H = 30222 and the target sentence. The postorder is P = 15234 and the breadth first order is B = 51234. Therefore, P S15 = (1, 3, 4, 5, 2) and BS15 = (2, 3, 4, 5, 1) by definition. translated as patterns. Shen et al. (2008) introduced tree sequences, i.e. sequences of treelets, as well as treelets in a dependency tree. They reduce the search space for decoding by restricting the extracted translation units of the dependency structure on the target language. Nevertheless, they were not able to handle non-projective dependencies. Carreras and Collins (2009) attempted to allow arbitrary reordering of the source language using tree adjoining grammar. None of the previous work using DG, however, incorporated the relatively free order in the source language as our proposed method. 3 Dependency Sequence Our key observation is that a graph traversal of a dependency tree leads to the discovery of useful patterns. The patterns in the form of treelets and tree sequences would be discontinuous and partial constituent phrases. For example, by the postorder traversal we visit English words in Figure 1 in the following order: “do, you, who, she, invited, ?,"
2010.amta-srw.2,P05-1067,0,0.0680879,"MT systems use DG based on dependency treelets, which are connected subgraphes. A dependency treelet would be discontinuous and therefore useful to extract discontinuous phrases. For example, “who . . . she invited” in Figure 1 is a dependency treelet and discontinuous in the source sentence. Some approaches using dependency treelets assumed the isomorphism of the dependency structure of the source and the target sentence (Lin, 2004; Quirk et al., 2005), which is unrealistic in the real situation. Although other approaches using dependency treelets addressed the non-isomorphism (Eisner, 2003; Ding and Palmer, 2005; Xiong et al., 2007), dependency treelets cannot capture partial constituent phrases such as sequences of dependents. It would cause the low coverage of translation rules since modifiers under a common head are often Table 1: The P Ss of the example when we restrict the maximum length of P S to 2. An underline means that the P S conflicts. Figure 2: An example of a pair of the source sentence with the dependency tree H = 30222 and the target sentence. The postorder is P = 15234 and the breadth first order is B = 51234. Therefore, P S15 = (1, 3, 4, 5, 2) and BS15 = (2, 3, 4, 5, 1) by definitio"
2010.amta-srw.2,P03-2041,0,0.102032,"syntaxbased SMT systems use DG based on dependency treelets, which are connected subgraphes. A dependency treelet would be discontinuous and therefore useful to extract discontinuous phrases. For example, “who . . . she invited” in Figure 1 is a dependency treelet and discontinuous in the source sentence. Some approaches using dependency treelets assumed the isomorphism of the dependency structure of the source and the target sentence (Lin, 2004; Quirk et al., 2005), which is unrealistic in the real situation. Although other approaches using dependency treelets addressed the non-isomorphism (Eisner, 2003; Ding and Palmer, 2005; Xiong et al., 2007), dependency treelets cannot capture partial constituent phrases such as sequences of dependents. It would cause the low coverage of translation rules since modifiers under a common head are often Table 1: The P Ss of the example when we restrict the maximum length of P S to 2. An underline means that the P S conflicts. Figure 2: An example of a pair of the source sentence with the dependency tree H = 30222 and the target sentence. The postorder is P = 15234 and the breadth first order is B = 51234. Therefore, P S15 = (1, 3, 4, 5, 2) and BS15 = (2, 3"
2010.amta-srw.2,W02-1039,0,0.0260258,"tree structure would require a nonprojective relation, supporting non-projective dependency helps broaden the coverage of translation rules. For example, a dependency relation from “invited” from “who” is non-projective in Figure 1. Formally, a non-projective dependency is a relation from a head wi to a dependent wj such that ∃head(wk ) 6∈ [min(i, j), max(i, j)] where k ∈ [min(i, j), max(i, j)] (i, j, and k are indices). DG handles non-projective relations much more easily than CFG, and is also known to be more suitable at handling divergences between two languages than the other formalisms (Fox, 2002). Last but not least, a source sentence would have a relatively free order. Languages with relatively free order such as Korean and Japanese allow various types of ordering of dependents for a given head. Especially for the main predicate of a source sentence, the modifiers such as the subjects and the objects can be located any position before the main predicate. For example, the following six Korean sentences have different word orders but identical meaning “when does the train leave to Seoul?” in English. Modifiers in flexible word order 기차+가 서울+로 언제 서울+로 기차+가 언제 언제 서울+로 기차+가 언제 기차+가 서울+로 서"
2010.amta-srw.2,N10-1140,0,0.0268464,"placeholders (variables) for other translation units, translation rules in syntax-based SMT frameworks embed hierarchical properties. In other words, they are capable of translating discontinuous phrases, while conventional phrase-based SMT systems are unable 1 . A discontinuous source phrase could be translated into a (continuous) target phrase, and vice versa. For example, “who . . . she” is translated into a continuous target phrase in Figure 1. A major challenge of syntax-based SMT frameworks is to broaden the coverage of translation rules. Purely syntactic translation rules allow only 1 Galley and Manning (2010) proposed a phrase-based SMT system which supports discontinuous phrases constituent phrases as translation units (Galley et al., 2004). This restriction is too severe to capture frequent patterns that are smaller than constituent (partial constituent) phrases. For example, an English phrase like “something CC” can be translated into a Japanese phrase “(something) (CC)” where the parentheses mean their translated counterparts. Modifiers in a noun phrase are also a partial constituent phrase as a translation unit. In Figure 1, “who . . . she” is a partial constituent phrase. Many researchers ha"
2010.amta-srw.2,N04-1035,0,0.486717,"her words, they are capable of translating discontinuous phrases, while conventional phrase-based SMT systems are unable 1 . A discontinuous source phrase could be translated into a (continuous) target phrase, and vice versa. For example, “who . . . she” is translated into a continuous target phrase in Figure 1. A major challenge of syntax-based SMT frameworks is to broaden the coverage of translation rules. Purely syntactic translation rules allow only 1 Galley and Manning (2010) proposed a phrase-based SMT system which supports discontinuous phrases constituent phrases as translation units (Galley et al., 2004). This restriction is too severe to capture frequent patterns that are smaller than constituent (partial constituent) phrases. For example, an English phrase like “something CC” can be translated into a Japanese phrase “(something) (CC)” where the parentheses mean their translated counterparts. Modifiers in a noun phrase are also a partial constituent phrase as a translation unit. In Figure 1, “who . . . she” is a partial constituent phrase. Many researchers have integrated partial constituent phrases into translation units. Since a tree structure would require a nonprojective relation, suppor"
2010.amta-srw.2,C04-1090,0,0.171842,"overage and the computational efficiency of the extraction. Since words in head-modifier relations are more colsely related than the others in a sentence, many syntaxbased SMT systems use DG based on dependency treelets, which are connected subgraphes. A dependency treelet would be discontinuous and therefore useful to extract discontinuous phrases. For example, “who . . . she invited” in Figure 1 is a dependency treelet and discontinuous in the source sentence. Some approaches using dependency treelets assumed the isomorphism of the dependency structure of the source and the target sentence (Lin, 2004; Quirk et al., 2005), which is unrealistic in the real situation. Although other approaches using dependency treelets addressed the non-isomorphism (Eisner, 2003; Ding and Palmer, 2005; Xiong et al., 2007), dependency treelets cannot capture partial constituent phrases such as sequences of dependents. It would cause the low coverage of translation rules since modifiers under a common head are often Table 1: The P Ss of the example when we restrict the maximum length of P S to 2. An underline means that the P S conflicts. Figure 2: An example of a pair of the source sentence with the dependenc"
2010.amta-srw.2,P08-1023,0,0.0395234,"ainly a syntax-based SMT. In order to develop the decoder, and also improve the proposed method, we will address the remaining issues as follows: • We regarded the sequences as non-terminals in CFG conceptually. If we use the lexical information directly, however, a data sparseness problem arises as sequences get longer. Therefore we need to generalize the sequence in order for it to be suitable for learning sufficient statistics. • We utilized only the single best dependency tree, which would not be able to resolve the structural ambiguity. As forest-based rule extraction has been suggested (Mi et al., 2008) in the phrase structure, we will incorporate multiple structures as a compressed one such as a packed-forest. • There is an another derivation using γ6 , which produces the same target sentence with different rules γ3 and γ4 : hX by γ6 : by γ4 : by γ3 : , X ⇒ ⇒ ⇒ i h h P S55 , e1 P S12 e4 P S34 i h h P S35 , e1 P S12 e4 [ e5 ] i h h P S15 , e1 [ e2 e3 ] e4 [ e5 ] i In this case, we reduce one step of the derivation using the production of the non-projective treelet P S12 . This indicates that the combination of the minimal rule before decoding, which is commoly used, leads to faster decoding."
2010.amta-srw.2,E06-1010,0,0.0240037,"the target sentence are obtained by projecting the source dependencies, the target sentence in Figure 5 has a non-projective dependency he1 = 3 where hek = i denotes that ei is the head of ek . BS is an alternative for this reason because it defines a different order from P S. The minimal sequence BS12 in Figure 5 captures the nonprojective target phrase using the production rule γ : h BS12 , e1 BS55 e3 BS34 i. Therefore, we expect P S and BS to be complementary. 7.3 Non-projective dependency Trees with non-projective dependencies appear quite often in some languages such as Czech and Danish (Nivre, 2006). Recent work on dependency parsing has suggested various methods for non-projective dependencies. The proposed method easily deals with non-projective source phrases because the sequence P S is always defined in the dependency tree. Table 2 shows an extracted rule γ3 where the P S22 = (3) has the dependents P S11 = (1) and the relation between them is non-projective. 8 Figure 6: Ellapsed time for the extraction algorithm for each sentence Emperical Result and Discussion 8.1 Experiment and environment To investigate the coverage of the extracted translation rules, we extracted the rules from t"
2010.amta-srw.2,P05-1034,0,0.0300027,"the computational efficiency of the extraction. Since words in head-modifier relations are more colsely related than the others in a sentence, many syntaxbased SMT systems use DG based on dependency treelets, which are connected subgraphes. A dependency treelet would be discontinuous and therefore useful to extract discontinuous phrases. For example, “who . . . she invited” in Figure 1 is a dependency treelet and discontinuous in the source sentence. Some approaches using dependency treelets assumed the isomorphism of the dependency structure of the source and the target sentence (Lin, 2004; Quirk et al., 2005), which is unrealistic in the real situation. Although other approaches using dependency treelets addressed the non-isomorphism (Eisner, 2003; Ding and Palmer, 2005; Xiong et al., 2007), dependency treelets cannot capture partial constituent phrases such as sequences of dependents. It would cause the low coverage of translation rules since modifiers under a common head are often Table 1: The P Ss of the example when we restrict the maximum length of P S to 2. An underline means that the P S conflicts. Figure 2: An example of a pair of the source sentence with the dependency tree H = 30222 and"
2010.amta-srw.2,P08-1066,0,0.0437134,"ndency treelets cannot capture partial constituent phrases such as sequences of dependents. It would cause the low coverage of translation rules since modifiers under a common head are often Table 1: The P Ss of the example when we restrict the maximum length of P S to 2. An underline means that the P S conflicts. Figure 2: An example of a pair of the source sentence with the dependency tree H = 30222 and the target sentence. The postorder is P = 15234 and the breadth first order is B = 51234. Therefore, P S15 = (1, 3, 4, 5, 2) and BS15 = (2, 3, 4, 5, 1) by definition. translated as patterns. Shen et al. (2008) introduced tree sequences, i.e. sequences of treelets, as well as treelets in a dependency tree. They reduce the search space for decoding by restricting the extracted translation units of the dependency structure on the target language. Nevertheless, they were not able to handle non-projective dependencies. Carreras and Collins (2009) attempted to allow arbitrary reordering of the source language using tree adjoining grammar. None of the previous work using DG, however, incorporated the relatively free order in the source language as our proposed method. 3 Dependency Sequence Our key observa"
2010.amta-srw.2,W07-0706,0,0.0741696,"on dependency treelets, which are connected subgraphes. A dependency treelet would be discontinuous and therefore useful to extract discontinuous phrases. For example, “who . . . she invited” in Figure 1 is a dependency treelet and discontinuous in the source sentence. Some approaches using dependency treelets assumed the isomorphism of the dependency structure of the source and the target sentence (Lin, 2004; Quirk et al., 2005), which is unrealistic in the real situation. Although other approaches using dependency treelets addressed the non-isomorphism (Eisner, 2003; Ding and Palmer, 2005; Xiong et al., 2007), dependency treelets cannot capture partial constituent phrases such as sequences of dependents. It would cause the low coverage of translation rules since modifiers under a common head are often Table 1: The P Ss of the example when we restrict the maximum length of P S to 2. An underline means that the P S conflicts. Figure 2: An example of a pair of the source sentence with the dependency tree H = 30222 and the target sentence. The postorder is P = 15234 and the breadth first order is B = 51234. Therefore, P S15 = (1, 3, 4, 5, 2) and BS15 = (2, 3, 4, 5, 1) by definition. translated as patt"
2012.iwslt-evaluation.18,P03-2041,0,0.0273286,"ic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is not included in the o"
2012.iwslt-evaluation.18,W02-1039,0,0.0472394,"binarization in tree-to-string models, a binarized forest of phrase structure trees has been proposed [25]. Since the number of all possible binarized trees are exponentially many, the author encode the binarized trees in a packed forest, which was originally proposed to encode the multiple parse trees [14]. In contrast to previous studies, we propose to use a novel binarized forest of dependency trees for syntax-based SMT. A dependency tree represents the grammatical relations between words as shown in Figure 1. Dependency grammar has that holds the best phrasal cohesion across the languages [6]. We utilize dependency labels for the annotation of the virtual nodes in a binarized dependency tree. To the best of our knowledge, this is the ﬁrst attempt to binarize the depen2. Binarized Dependency Forest Forest-to-string translation approaches construct a packed forest for a source sentence, and ﬁnd the mapping between the source forest and the target sentence. A packed forest is a compact representation of exponentially many trees. Most studies focused on the forest of multiple parse trees in order to reduce the side effect of the parsing error [13, 14, 15, 19, 27, 28]. On the other han"
2012.iwslt-evaluation.18,N09-1026,0,0.0162717,"Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is n"
2012.iwslt-evaluation.18,P05-1067,0,0.0232321,"ic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is not included in the o"
2012.iwslt-evaluation.18,P06-1121,0,0.0128788,"Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is n"
2012.iwslt-evaluation.18,N04-1035,0,0.12307,"Missing"
2012.iwslt-evaluation.18,W11-2123,0,0.0870993,"Missing"
2012.iwslt-evaluation.18,2006.amta-papers.8,0,0.0252746,"ring Pohang University of Science and Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method"
2012.iwslt-evaluation.18,W05-1506,0,0.0549943,"Missing"
2012.iwslt-evaluation.18,J09-4009,0,0.0134068,"Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is n"
2012.iwslt-evaluation.18,D08-1022,0,0.0170162,"phrasal cohesion across the languages [6]. We utilize dependency labels for the annotation of the virtual nodes in a binarized dependency tree. To the best of our knowledge, this is the ﬁrst attempt to binarize the depen2. Binarized Dependency Forest Forest-to-string translation approaches construct a packed forest for a source sentence, and ﬁnd the mapping between the source forest and the target sentence. A packed forest is a compact representation of exponentially many trees. Most studies focused on the forest of multiple parse trees in order to reduce the side effect of the parsing error [13, 14, 15, 19, 27, 28]. On the other hand, Zhang et. al. [25] attempted to binarize the best phrase structure tree. A binarization method comprises the conversion of the possibly non-binary tree into a binarized tree. The authors suggested a binarized forest, which is a packed forest that compactly encodes multiple binarized trees. It improves generalization by breaking downs the rules into the smallest possible parts. Thus, a binarized forest that the authors suggested covers non-constituent phrases by introducing a virtual node, for example, “beavers build” or “dams with” in Figure 1. In this paper, we propose a"
2012.iwslt-evaluation.18,P08-1023,0,0.0981676,"ring Pohang University of Science and Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method"
2012.iwslt-evaluation.18,P10-1145,0,0.0200959,"phrasal cohesion across the languages [6]. We utilize dependency labels for the annotation of the virtual nodes in a binarized dependency tree. To the best of our knowledge, this is the ﬁrst attempt to binarize the depen2. Binarized Dependency Forest Forest-to-string translation approaches construct a packed forest for a source sentence, and ﬁnd the mapping between the source forest and the target sentence. A packed forest is a compact representation of exponentially many trees. Most studies focused on the forest of multiple parse trees in order to reduce the side effect of the parsing error [13, 14, 15, 19, 27, 28]. On the other hand, Zhang et. al. [25] attempted to binarize the best phrase structure tree. A binarization method comprises the conversion of the possibly non-binary tree into a binarized tree. The authors suggested a binarized forest, which is a packed forest that compactly encodes multiple binarized trees. It improves generalization by breaking downs the rules into the smallest possible parts. Thus, a binarized forest that the authors suggested covers non-constituent phrases by introducing a virtual node, for example, “beavers build” or “dams with” in Figure 1. In this paper, we propose a"
2012.iwslt-evaluation.18,P03-1021,0,0.00998539,"Missing"
2012.iwslt-evaluation.18,P00-1056,0,0.284757,"Missing"
2012.iwslt-evaluation.18,P08-1066,0,0.136479,"Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is n"
2012.iwslt-evaluation.18,C10-1123,0,0.0218426,"phrasal cohesion across the languages [6]. We utilize dependency labels for the annotation of the virtual nodes in a binarized dependency tree. To the best of our knowledge, this is the ﬁrst attempt to binarize the depen2. Binarized Dependency Forest Forest-to-string translation approaches construct a packed forest for a source sentence, and ﬁnd the mapping between the source forest and the target sentence. A packed forest is a compact representation of exponentially many trees. Most studies focused on the forest of multiple parse trees in order to reduce the side effect of the parsing error [13, 14, 15, 19, 27, 28]. On the other hand, Zhang et. al. [25] attempted to binarize the best phrase structure tree. A binarization method comprises the conversion of the possibly non-binary tree into a binarized tree. The authors suggested a binarized forest, which is a packed forest that compactly encodes multiple binarized trees. It improves generalization by breaking downs the rules into the smallest possible parts. Thus, a binarized forest that the authors suggested covers non-constituent phrases by introducing a virtual node, for example, “beavers build” or “dams with” in Figure 1. In this paper, we propose a"
2012.iwslt-evaluation.18,D11-1020,0,0.012321,"ring Pohang University of Science and Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method"
2012.iwslt-evaluation.18,W07-0706,0,0.0194688,"ring Pohang University of Science and Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method"
2012.iwslt-evaluation.18,P11-1084,0,0.0595139,"hat transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is not included in the original tree. The virtual nodes in a binarized phrase structure grammar are annotated using the phrasal categories in the original tree. Unfortunately, these approaches are available only for string-to-tree models, because we are not aware of the correct binarization of the source tree at the decoding stage. To take the advantage of binarization in tree-to-string models, a binarized forest of phrase structure trees has been proposed [25]. Since the number of all possible binarized trees are exponentially many, the author encode the binarized trees in a packed forest, which was originally proposed to encode the multiple parse trees [14]. In contrast to previous studies, we propose to use a novel binarized forest of dependency trees for syntax-based SMT. A dependency tree represents the grammatical relations between words as shown in Figure 1. Dependency grammar has that holds the best phrasal cohesion across the languages [6]. We utilize dependency labels for the annotation of the virtual nodes in a binarized dependency tree."
2012.iwslt-evaluation.18,N06-1033,0,0.0262934,"Technology (POSTECH), Republic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is n"
2012.iwslt-evaluation.18,P09-1020,0,0.0131059,"phrasal cohesion across the languages [6]. We utilize dependency labels for the annotation of the virtual nodes in a binarized dependency tree. To the best of our knowledge, this is the ﬁrst attempt to binarize the depen2. Binarized Dependency Forest Forest-to-string translation approaches construct a packed forest for a source sentence, and ﬁnd the mapping between the source forest and the target sentence. A packed forest is a compact representation of exponentially many trees. Most studies focused on the forest of multiple parse trees in order to reduce the side effect of the parsing error [13, 14, 15, 19, 27, 28]. On the other hand, Zhang et. al. [25] attempted to binarize the best phrase structure tree. A binarization method comprises the conversion of the possibly non-binary tree into a binarized tree. The authors suggested a binarized forest, which is a packed forest that compactly encodes multiple binarized trees. It improves generalization by breaking downs the rules into the smallest possible parts. Thus, a binarized forest that the authors suggested covers non-constituent phrases by introducing a virtual node, for example, “beavers build” or “dams with” in Figure 1. In this paper, we propose a"
2012.iwslt-evaluation.18,D10-1043,0,0.0608122,"ic of Korea { leona, jhlee } @postech.ac.kr Abstract We participated in the OLYMPICS task in IWSLT 2012 and submitted two formal runs using a forest-to-string translation system. Our primary run achieved better translation quality than our contrastive run, but worse than a phrase-based and a hierarchical system using Moses. Figure 1: An example dependency tree with dependency labels dency grammar. 1. Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is not included in the o"
2012.iwslt-evaluation.18,J10-2004,0,0.0126391,"Introduction Syntax-based SMT approaches incorporate tree structures of sentences to the translation rules in the source language [10, 14, 23, 22], the target language [1, 7, 12, 18, 26], or both [2, 3, 28]. Due to the structural constraint, the transducer grammar extracted from parallel corpora tends to be quite large and ﬂat. Hence, the extracted grammar consists of translation rules that appear few times, and it is difﬁcult to apply most translation rules in the decoding stage. For generalization of transducer grammar, binarization methods of a phrase structure grammar have been suggested [1, 12, 20, 26]. Binarization is a process that transforms an n-ary grammar into a binary grammar. During the transformation, a binarization method introduces the virtual nodes which is not included in the original tree. The virtual nodes in a binarized phrase structure grammar are annotated using the phrasal categories in the original tree. Unfortunately, these approaches are available only for string-to-tree models, because we are not aware of the correct binarization of the source tree at the decoding stage. To take the advantage of binarization in tree-to-string models, a binarized forest of phrase struc"
2012.iwslt-evaluation.18,P10-1034,0,0.0303204,"Missing"
2013.iwslt-evaluation.9,W07-0401,0,0.0227052,"pace for efficiency. For example, the simple distortion model in phrase-based SMT (PBSMT) prohibit a long distance jump beyond a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus"
2013.iwslt-evaluation.9,D09-1105,0,0.0928852,"pace for efficiency. For example, the simple distortion model in phrase-based SMT (PBSMT) prohibit a long distance jump beyond a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus"
2013.iwslt-evaluation.9,D11-1045,0,0.0629565,"pace for efficiency. For example, the simple distortion model in phrase-based SMT (PBSMT) prohibit a long distance jump beyond a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus"
2013.iwslt-evaluation.9,N13-1032,0,0.0116887,"pace for efficiency. For example, the simple distortion model in phrase-based SMT (PBSMT) prohibit a long distance jump beyond a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus"
2013.iwslt-evaluation.9,C04-1073,0,0.129679,"a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hi"
2013.iwslt-evaluation.9,P05-1066,0,0.117129,"a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hi"
2013.iwslt-evaluation.9,N09-1028,0,0.0511101,"a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hi"
2013.iwslt-evaluation.9,C10-1043,0,0.0676678,"a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hi"
2013.iwslt-evaluation.9,W10-1736,0,0.01293,"a window size during translation. Therefore, PBSMT suffers from the lack of ability for word reordering at a long distance. Pre-ordering is one of the most prevailing approaches to overcome this limitation of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hi"
2013.iwslt-evaluation.9,D11-1018,0,0.0583368,"of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hierarchical structures of the third approach restrict word reordering within a conFigure 1: The overall architecture of our system, consisting of a reordering module (reordering parser) and a SMT modu"
2013.iwslt-evaluation.9,D12-1077,0,0.651149,"of PBSMT. It is a pre-processing method that reorders the source sentence in advance to the later translation using PBSMT. We categorize previous works into three categories. First, pre-ordering using local information reorders either a (flat) word or chunk sequence [1, 2, 3, 4]. Second, pre-ordering using syntactic information manipulates a syntactic tree so that yield a reordered sentence [5, 6, 7, 8, 9]. Third, pre-ordering using an ad-hoc structure for word reordering induces a discriminative parser trained from a parallel corpus, and apply the parser to obtain a reordered source sentence [10, 11]. Both the second and third approaches work with hierarchical structures of the source sentence. While the second approach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hierarchical structures of the third approach restrict word reordering within a conFigure 1: The overall architecture of our system, consisting of a reordering module (reordering parser) and a SMT modu"
2013.iwslt-evaluation.9,J97-3002,0,0.0822866,"ach requires a syntactic parser which might not be available for resource-poor languages, the third one requires only a small manual word aligned corpus in addition to a large parallel corpus. Hereinafter, therefore, we focus on the third approach. Because of the efficiency, the hierarchical structures of the third approach restrict word reordering within a conFigure 1: The overall architecture of our system, consisting of a reordering module (reordering parser) and a SMT module tinuous sequence under a sub-structure, i.e., the hierarchical structures obey Inversion Transduction Grammar (ITG) [12] constraints. We participated in the IWSLT 2013 Evaluation Campaign for the MT track, and submitted runs for two official directions: German↔English. As German and English have different word orders, we applied a pre-ordering method to resolve the difference requires word reordering. 2. System description Our system consists of two modules: a reordering module and a SMT module. The reordering module rearranges the words in the source sentence, and the SMT module translates the reordered sentence into the target sentence. The overall architecture of our system is shown in Figure 1. Because we u"
2013.iwslt-evaluation.9,P07-2045,0,0.00475398,"IWSLT 2013 Evaluation Campaign for the MT track, and submitted runs for two official directions: German↔English. As German and English have different word orders, we applied a pre-ordering method to resolve the difference requires word reordering. 2. System description Our system consists of two modules: a reordering module and a SMT module. The reordering module rearranges the words in the source sentence, and the SMT module translates the reordered sentence into the target sentence. The overall architecture of our system is shown in Figure 1. Because we utilized an off-the-shelf SMT system [13] as the SMT module, we focus on the reordering module here. 2.1. Discriminative reordering parser We briefly summarize the discriminative reordering parser in this section. The most relevant work of this paper was proposed to induce a tree for word reordering produced by a discriminative parser [11]. The goal of their method is to find the best permutation π ˆ for a given source sentence F , according to the following discriminative model. Algorithm 1: Online learning for a training instance 1 2 3 4 5 6 7 Algorithm 2: Cube Growing in parallel CYK parsing procedure UpdateWeight(F, A, w) D ← Par"
2013.iwslt-evaluation.9,W05-1506,0,0.0465371,"ances. First, we adopted a faster search algorithm known as Cube Growing, and integrated it into a parallel CYK parsing method. Second, the feature generation process run in parallel because it is a major bottleneck of parsing efficiency. Third, the features generated at the first iteration are stored on disk and used in the remaining epochs. In a consequence, our proposed methods enable us to utilize tens of thousands training instances in our experiments. 2.2.1. Cube Growing in parallel CYK parsing Cube Growing is a dynamic programming algorithm for searching over a hyper graph, proposed by [15]. It produces the kth-best parse on-the-fly, and thus does not enumerate unnecessary hypotheses during the search process. More specifically, two data structures manage the hypotheprocedure ModifiedCubeGrowing input : A cell covering [l, r] output: A priority queue q with candidates for m ∈ (l, r] do left ← cell [l,m] right ← cell [m,r] L ← peek(left.q) R ← peek(right.q) push( q, Hyp(L, R)) // straight push( q, Hyp(R, L)) // inverted end 24 25 26 27 28 29 2.2. Scalable training method procedure Parse input : A sentence w1 . . . wN output: A hyper graph with K-best parses for L ∈ [1, N ] do for"
2013.iwslt-evaluation.9,W11-2920,0,0.0271712,"n push(q, Hyp(best.L, R)) // straight push(q, Hyp(R, best.L)) // inverted end ses: a list of best hypotheses and a priority queue of candidates for the next best hypothesis. If the kth-best parse is already produced, it is the kth hypothesis in the best list. Otherwise, Cube Growing enumerates hypotheses by taking the best candidate from the priority queue until the kth hypothesis can be found. Whenever the best candidate is taken from the priority queue, successors of the candidate are pushed on the priority queue, if possible. To obtain the successors, Cube Growing is recursively performed. [16] proposed that the original CYK parsing algorithm can be parallelized in three levels: sentence-level, cell-level, and grammar-level. Although they reported the grammarlevel parallelization achieved the fastest result using thousands of GPUs, we adopted the cell-level parallelization. It is possible to parallelize the original CYK parsing at cell-level because the hypotheses in different chart cells covering same number of words in the sentence do not affect each other. Unfortunately, this property does not hold anymore in Cube Growing because k-best hypotheses are enumerated on demand. Theref"
2013.iwslt-evaluation.9,2012.eamt-1.60,0,0.0496106,"Missing"
2013.iwslt-evaluation.9,W08-0509,0,0.0312615,"sentence. Then the parser begins to search the best permutation π using the features according to the discriminative model. For each iteration, in other words, we skip the feature generation process and reuse the generated features at the first time. 3. Experimental result In our experiments, we developed a reordering parser based on [11], LADER1 , and utilized a phrase-based SMT system Moses [13] for a reordering module and SMT module, respectively. The tokenize.perl2 segmented German and English sentences into words. Word alignment of the segmented sentence pairs was performed using MGIZA++ [18] for both German↔English directions, and refined using the 1 https://github.com/hwidongna/lader 2 http://statmt.org/wmt08/scripts.tgz Table 2: The official evaluation results. XYZ in the first column refers the source X, the target Y and the priority of our run, where 1 is the primary and 2 is the contrastive. tst2013* denotes the results are measured on the reference with disfluency. Case-sensitive Case-insensitive Run Data BLEU TER BLEU TER DE1 tst2013* 0.2126 0.6760 0.2174 0.6671 DE1 tst2013 0.2117 0.6890 0.2165 0.6804 ED1 tst2011 0.2348 0.5370 0.2406 0.5289 ED1 tst2012 0.2043 0.5913 0.2102"
2013.iwslt-evaluation.9,2013.iwslt-evaluation.1,0,0.0588789,"Missing"
2020.ngt-1.16,D18-2012,0,0.0271508,"Missing"
2020.ngt-1.16,2020.acl-main.703,0,0.114801,"Missing"
2020.ngt-1.16,2020.ngt-1.28,0,0.027379,"transfer learning based simultaneous translation model by extending BART. We pretrained BART with Korean Wikipedia and a Korean news dataset, and fine-tuned it with an additional web-crawled parallel corpus and the 2020 Duolingo official training dataset. In our experiments on the 2020 Duolingo test dataset, our submission achieves 0.312 in weighted macro F1 score, and ranks second among the submitted En-Ko systems. 1 Introduction Simultaneous Translation And Paraphrase for Language Education (STAPLE) is the task of automatically producing multiple translations from a single source sentence (Mayhew et al., 2020). Because STAPLE can be regarded as a mixture of the machine translation (MT) and paraphrasing problem, MT and paraphrasing techniques play an important role in this task. Unlike in a typical MT task, systems are demanded to generate high-coverage sets on a sentence-level, as opposed to word-level. Subsequently, systems require a deeper linguistic understanding of the target language to generate accurate target sentences. Recent NLP studies have alleviated this problem by transfer learning (Ventura and Warnick, 2007) from pre-trained language models. Radford et al. (2018) proposed a generative"
2020.ngt-1.16,N19-4009,0,0.029321,"dditional training and development data for the source-side encoder. As with the pretraining corpus, we filtered out any training or development samples longer than 100 tokens. Source-side Encoder Pre-trained BART is a monolingual model, so the proposed model needs an additional encoder to function as translation model. After pre-training BART, we removed the embedding layer of the pretrained encoder and added a randomly-initialized encoder instead (Lewis et al., 2019). In order to prevent corruption from the high loss in the randomly140 3.2 Training Details Settings. We modified the Fairseq (Ott et al., 2019) implementation of BART to build our model. Most hyperparameters of BART pre-training such as dropout ratio, hidden size, and etc. were copied from the base model described in Lewis et al. Decoding Option Weighted Macro F1 ↑ Weighted Recall ↑ Precision ↑ 50 75 100 140 500 0.3192 0.3280 0.3234 0.3108 0.2218 0.3092 0.3651 0.4008 0.4394 0.5817 0.5202 0.4628 0.4214 0.3680 0.1865 5 10 100 100 0.1673 0.1164 0.2069 0.1474 0.2212 0.1601 – – – 50 65 70 0.2695 0.3064 0.3163 0.2546 0.3197 0.3410 0.4630 0.4615 0.4596 Beam Size Diverse Nbest (weight) Beam search 50 75 100 140 500 – – – – – Diverse beam sea"
2020.ngt-1.16,P16-1162,0,0.0623568,"Missing"
2020.wmt-1.82,P19-1292,0,0.447097,"2016). Given that neural-network systems require a large quantity of training data, creating APE triplets, which each consist of a source sentence (src), a machine-translated sentence (mt), and a manually post-edited sentence (pe), requires a lot of human labor. Furthermore, because neural APE is a recently minted field of study, only a few smallsized training data sets are available at present. To mitigate such data shortage, several methods are proposed such that 1) create artificial APE triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018); and 2) apply ‘transfer learning’ (Correia and Martins, 2019; Lopes et al., 2019). We believe that pre-trained models such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) helped APE models learn rich language representations that compensated for the performance loss caused by using an insufficient quantity of training data. APE is a task that handles both src and mt simultaneously, and learning a joint representation of these two inputs requires an understanding of both languages. Although previous works that used BERT have shown that transfer learning is effective in APE (Correia and Martins, 2019; Lope"
2020.wmt-1.82,N19-1423,0,0.197057,"translated sentence (mt), and a manually post-edited sentence (pe), requires a lot of human labor. Furthermore, because neural APE is a recently minted field of study, only a few smallsized training data sets are available at present. To mitigate such data shortage, several methods are proposed such that 1) create artificial APE triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018); and 2) apply ‘transfer learning’ (Correia and Martins, 2019; Lopes et al., 2019). We believe that pre-trained models such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) helped APE models learn rich language representations that compensated for the performance loss caused by using an insufficient quantity of training data. APE is a task that handles both src and mt simultaneously, and learning a joint representation of these two inputs requires an understanding of both languages. Although previous works that used BERT have shown that transfer learning is effective in APE (Correia and Martins, 2019; Lopes et al., 2019), adopting BERT as a pre-trained language model may restrict to properly model the relation between two different languages because BERT is trai"
2020.wmt-1.82,W16-2378,0,0.0899796,"effort compared to correcting machine-translated sentences manually from scratch (Pal et al., 2016). Given that neural-network systems require a large quantity of training data, creating APE triplets, which each consist of a source sentence (src), a machine-translated sentence (mt), and a manually post-edited sentence (pe), requires a lot of human labor. Furthermore, because neural APE is a recently minted field of study, only a few smallsized training data sets are available at present. To mitigate such data shortage, several methods are proposed such that 1) create artificial APE triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018); and 2) apply ‘transfer learning’ (Correia and Martins, 2019; Lopes et al., 2019). We believe that pre-trained models such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) helped APE models learn rich language representations that compensated for the performance loss caused by using an insufficient quantity of training data. APE is a task that handles both src and mt simultaneously, and learning a joint representation of these two inputs requires an understanding of both languages. Although previous works that used BERT have"
2020.wmt-1.82,W18-6467,0,0.0506476,"Translation News Commentary v15 Task WikiMatrix UN Parallel Corpus Back-translated news Wikipedia OPUS MultiUN QED WMT2019 QE Task Parallel Corpus En–De X X X X X X – – X X X X En–Zh – – – – X X X X – X X – Table 1: The list of data sets we used to train TLM in the pre-training stage for the En–De & En–Zh language pairs. All data sets were filtered to contain only such sentences with a length between 3 and 70 tokens. 4 Experiments 4.1 Figure 2: The architecture of our proposed APE model Input representation. Unlike APE models that use a multi-source encoder that encodes src and mt separately (Junczys-Dowmunt and Grundkiewicz, 2018; Lee et al., 2019), we followed Lopes et al. (2019) so that the concatenation of src and mt was fed in to a single encoder. To distinguish one language from the other, we assigned different segment-embeddings to src and mt, respectively, and we also assigned individual positional-embeddings to src and mt. 3.2 Decoder Because our pre-trained language model does not have a decoder, between two options, either randomly initializing the decoder or using another set of pre-trained weights, we chose the former; in contrast to Correia and Martins (2019), who made the encoder’s self-attention weights"
2020.wmt-1.82,P07-2045,0,0.0094455,"the encoder and decoder share their parameters. To compensate for resulting variations in performance, we made an ensemble model of three to four individual models that have identical structures. Dataset We applied Byte-Pair Encoding (Sennrich et al., 2016) to all the corpora in both the source and target language. We used the En–De shared sub-word vocabulary that is released on XLM GitHub, but we compiled an En–Zh shared vocabulary by using Wikipedia’s dump files in English and Chinese. As in the WMT2020 official data, all English and German data sets were truncated and tokenized with Moses (Koehn et al., 2007) scripts, and the Chinese data set was tokenized with the Jieba tokenizer.2 4.1.1 Pre-training stage We collected parallel corpora from the WMT2020 News Translation Task website,3 OPUS,4 and the WMT2019 Quality Estimation website.5 Table 1 shows the list of parallel corpora that we used to pre-train our models for the two language pairs. To build a pre-trained language model for En–Zh, we built a MLM+TLM model from scratch because we did not have available MLM models that are trained only on the English and Chinese data. Whereas we trained TLM on the whole parallel corpora, we trained MLM only"
2020.wmt-1.82,W19-5412,1,0.929377,"iMatrix UN Parallel Corpus Back-translated news Wikipedia OPUS MultiUN QED WMT2019 QE Task Parallel Corpus En–De X X X X X X – – X X X X En–Zh – – – – X X X X – X X – Table 1: The list of data sets we used to train TLM in the pre-training stage for the En–De & En–Zh language pairs. All data sets were filtered to contain only such sentences with a length between 3 and 70 tokens. 4 Experiments 4.1 Figure 2: The architecture of our proposed APE model Input representation. Unlike APE models that use a multi-source encoder that encodes src and mt separately (Junczys-Dowmunt and Grundkiewicz, 2018; Lee et al., 2019), we followed Lopes et al. (2019) so that the concatenation of src and mt was fed in to a single encoder. To distinguish one language from the other, we assigned different segment-embeddings to src and mt, respectively, and we also assigned individual positional-embeddings to src and mt. 3.2 Decoder Because our pre-trained language model does not have a decoder, between two options, either randomly initializing the decoder or using another set of pre-trained weights, we chose the former; in contrast to Correia and Martins (2019), who made the encoder’s self-attention weights be shared with the"
2020.wmt-1.82,W19-5413,0,0.104748,"Missing"
2020.wmt-1.82,L18-1004,0,0.239583,"ranslated sentences manually from scratch (Pal et al., 2016). Given that neural-network systems require a large quantity of training data, creating APE triplets, which each consist of a source sentence (src), a machine-translated sentence (mt), and a manually post-edited sentence (pe), requires a lot of human labor. Furthermore, because neural APE is a recently minted field of study, only a few smallsized training data sets are available at present. To mitigate such data shortage, several methods are proposed such that 1) create artificial APE triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018); and 2) apply ‘transfer learning’ (Correia and Martins, 2019; Lopes et al., 2019). We believe that pre-trained models such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) helped APE models learn rich language representations that compensated for the performance loss caused by using an insufficient quantity of training data. APE is a task that handles both src and mt simultaneously, and learning a joint representation of these two inputs requires an understanding of both languages. Although previous works that used BERT have shown that transfer"
2020.wmt-1.82,C16-1241,0,0.0581438,"Missing"
2020.wmt-1.82,N18-1202,0,0.0297715,"triplets, which each consist of a source sentence (src), a machine-translated sentence (mt), and a manually post-edited sentence (pe), requires a lot of human labor. Furthermore, because neural APE is a recently minted field of study, only a few smallsized training data sets are available at present. To mitigate such data shortage, several methods are proposed such that 1) create artificial APE triplets (Junczys-Dowmunt and Grundkiewicz, 2016; Negri et al., 2018); and 2) apply ‘transfer learning’ (Correia and Martins, 2019; Lopes et al., 2019). We believe that pre-trained models such as ELMo (Peters et al., 2018), OpenAI GPT (Radford et al., 2018), and BERT (Devlin et al., 2019) helped APE models learn rich language representations that compensated for the performance loss caused by using an insufficient quantity of training data. APE is a task that handles both src and mt simultaneously, and learning a joint representation of these two inputs requires an understanding of both languages. Although previous works that used BERT have shown that transfer learning is effective in APE (Correia and Martins, 2019; Lopes et al., 2019), adopting BERT as a pre-trained language model may restrict to properly mode"
2020.wmt-1.82,P16-1162,0,0.0131374,"e-trained language model does not have a decoder, between two options, either randomly initializing the decoder or using another set of pre-trained weights, we chose the former; in contrast to Correia and Martins (2019), who made the encoder’s self-attention weights be shared with the decoder, we randomly initialized the context attention layers and did not make the encoder and decoder share their parameters. To compensate for resulting variations in performance, we made an ensemble model of three to four individual models that have identical structures. Dataset We applied Byte-Pair Encoding (Sennrich et al., 2016) to all the corpora in both the source and target language. We used the En–De shared sub-word vocabulary that is released on XLM GitHub, but we compiled an En–Zh shared vocabulary by using Wikipedia’s dump files in English and Chinese. As in the WMT2020 official data, all English and German data sets were truncated and tokenized with Moses (Koehn et al., 2007) scripts, and the Chinese data set was tokenized with the Jieba tokenizer.2 4.1.1 Pre-training stage We collected parallel corpora from the WMT2020 News Translation Task website,3 OPUS,4 and the WMT2019 Quality Estimation website.5 Table"
2020.wmt-1.83,N19-1423,0,0.196736,"editing operations: insertion, deletion, substitution, and shifting. In other words, mt texts contain the following types of errors (The examples on the left side and right side are mt and pe, respectively.): • Insertion operation implies that mt includes deletion errors: We the world → We are the world • Deletion operation implies that mt includes insertion errors: We are in the world → we are the world Related Work Noise injection to input sentences has become a popular method to let auto-encoders (Hill et al., 2016; Vincent et al., 2008) or pre-trained language models (Lewis et al., 2019; Devlin et al., 2019) learn how to reconstruct the original input. Because post-editing is a process of reconstructing corrupted translations, simulating corrupted MT outputs by injecting noise to the target sentence is a way to get synthetic APE training samples. In the APE task, Xu et al. (2019) employed a data noising technique that incorporates a noise vector generated from a Gaussian or uniform distribution into the word embedding vector. However, their noising process has an effect on all tokens in a Method • Substitution operation implies that mt includes substitution errors: We is the world → we are the wo"
2020.wmt-1.83,N16-1162,0,0.0501095,"Missing"
2020.wmt-1.83,W16-2378,0,0.132685,"proach to constructing APE models (Chatterjee et al., 2019, 2018), which requires a large quantity of training samples. However, APE data1 — comprising triplets of three texts: source (src), a machine-translation (mt) of src, and a human-crafted post-edited sentence (pe) of mt — is too small and costly to acquire. Consequently, the lack of APE data becomes a great 1 http://www.statmt.org/wmt20/ape-task.html obstacle to a satisfactory performance of sequenceto-sequence models. To reduce such data scarcity, there have been several attempts at constructing synthetic APE data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). Most notably, Negri et al. (2018) proposed a simple but effective way to construct a large-scale synthetic APE data set eSCAPE (src, mt, ref ), of which src and ref is the source and target text of freely available parallel corpora, respectively, and mt is a translation of src produced by the MT system that had been trained on those parallel corpora. As eSCAPE has shown to be beneficial in training APE models (Chatterjee et al., 2019), it has become feasible to train deep APE models and also what most recent works have been relying on so far. Nevertheless, the availability of a limited quant"
2020.wmt-1.83,W19-5412,1,0.787017,"Missing"
2020.wmt-1.83,2020.acl-main.703,0,0.0618304,"Missing"
2020.wmt-1.83,L18-1004,0,0.0346946,"become a dominant approach to constructing APE models (Chatterjee et al., 2019, 2018), which requires a large quantity of training samples. However, APE data1 — comprising triplets of three texts: source (src), a machine-translation (mt) of src, and a human-crafted post-edited sentence (pe) of mt — is too small and costly to acquire. Consequently, the lack of APE data becomes a great 1 http://www.statmt.org/wmt20/ape-task.html obstacle to a satisfactory performance of sequenceto-sequence models. To reduce such data scarcity, there have been several attempts at constructing synthetic APE data (Negri et al., 2018; Junczys-Dowmunt and Grundkiewicz, 2016). Most notably, Negri et al. (2018) proposed a simple but effective way to construct a large-scale synthetic APE data set eSCAPE (src, mt, ref ), of which src and ref is the source and target text of freely available parallel corpora, respectively, and mt is a translation of src produced by the MT system that had been trained on those parallel corpora. As eSCAPE has shown to be beneficial in training APE models (Chatterjee et al., 2019), it has become feasible to train deep APE models and also what most recent works have been relying on so far. Neverthe"
2020.wmt-1.83,2006.amta-papers.25,0,0.0971791,"rallel corpora, resulting in additional APE triplets (src, refnoise , ref ), where src and ref is the source and target text of parallel corpora, respectively. During post-editing, certain editing operations including the word insertion, deletion, substitution, and shifting are applied to translated texts (noisy texts) for error correction. Thus, we applied such operations to target texts (clean texts) of parallel corpora to inject errors in reverse. Moreover, to simulate the quantity of errors that the target MT system produces, we refer to the distribution of “Translation Error Rate” (TER) (Snover et al., 2006) occurring in the actual APE data to determine the quantity of errors to be injected. 783 Proceedings of the 5th Conference on Machine Translation (WMT), pages 783–788 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics Figure 1: An illustration of the noising procedure While we trained our models using the noising module, we supplied to the models synthetic APE data in addition to the WMT’20 APE data set as training data. The synthetic data was produced by using the eSCAPE method, which uses parallel corpora and a trained NMT model. We observed that models with nois"
2020.wmt-1.83,W19-5417,0,0.0290693,"Missing"
2021.eacl-main.322,P15-2026,0,0.0231587,"E training dataset1 . Experimental results on the WMT English-German APE benchmarks demonstrate that our enlarged datasets are effective in improving APE performance. 1 Table 1: An example of APE triplets from the WMT dataset (Bojar et al., 2017). Boldface words are either incorrect words in mt or post-edited words in pe. Introduction Automatic Post-Editing (APE) seeks to automatically correct errors included in the output of a blackbox machine translation (MT) system to improve the final translation quality, thereby reducing the effort required for manual post-editing (Allen and Hogan, 2000; Chatterjee et al., 2015; Bojar et al., 2016; Chatterjee et al., 2018). In general, APE can be considered as a task of sequence-to-sequence supervised learning, which requires a considerable amount of human-annotated data. However, constructing an APE corpus—a set of triplets (Table 1), each of which includes a source text (src), a machine-translated text (mt), and a manually postedited text (pe)—is labor-intensive work because ∗ Equal contribution to this work. Our synthetic APE data is available at https:// github.com/wonkeelee/APE-backtranslation. git 1 Manipulates the shape of an item . Bearbeitet die Form eines"
2021.eacl-main.322,P19-1292,0,0.0731229,"BLEU(↑) TER(↓) BLEU(↑) TER(↓) BLEU(↑) TER(↓) BLEU(↑) 24.76 16.84 62.11 24.48 62.49 24.24 62.99 24.49 62.53 74.73 eSCAPE 16.97 73.94 17.35 72.93 17.74 72.34 17.35 73.07 16.39 75.70 eSCAPEFG 17.06 73.96 17.40 72.81 18.00 72.19 17.48 72.98 16.30 75.77 eSCAPEBG 17.25 73.58 17.85 72.30 17.93 72.12 17.66 72.66 16.50 75.40 73.30∗ 17.32∗ 72.95∗ 17.05∗ 73.50∗ 16.09∗ 76.11∗ eSCAPE + eSCAPEFG 16.79 74.25 17.05∗ eSCAPE + eSCAPEBG 16.73 74.32∗ 16.96∗ 73.41∗ 17.26∗ 73.14∗ 16.98∗ 73.62∗ 15.95∗ 76.14∗ eSCAPE + eSCAPEFG + eSCAPEBG 16.57∗ 74.52∗ 16.99∗ 73.50∗ 17.29∗ 73.11∗ 16.95∗ 73.71∗ 16.15∗ 76.00∗ BERT-APE (Correia and Martins, 2019) 16.91 74.29 17.26 73.42 17.71 72.74 17.29 73.48 – BERT-APE (Correia and Martins, 2019) (Ensemble) 16.49 74.98 16.83 73.94 17.15 73.60 16.82 74.17 – – – – – – – – – 16.06 75.96 BERT-APE (Lopes et al., 2019) – – Table 3: Evaluation results of our APE models using different configurations on training datasets. ‘*’ represents that our model’s improvement is significant enough compared to the eSCAPE baseline in the second row with p < 0.05. The best result among our models in each column is in bold type. The three models at the bottom are current state-of-the-art models. on Di and use 2,000 random"
2021.eacl-main.322,N19-1423,0,0.0201695,"ine. One possible reason that we expect is the gap between those synthetic mt and mt in the WMT dataset; in other words, synthetic mt is not produced by an existing MT system. Nevertheless, we found that when we augment eSCAPE with eSCAPEFG and/or eSCAPEBG , the trained APE model shows consistent improvements in its APE performance and most of the improvements upon the eSCAPE baseline are statistically significant. Moreover, the results also surpass current state-of-the-art (except the ensemble models) APE models (Correia and Martins, 2019; Lopes et al., 2019), which are built on top of BERT (Devlin et al., 2019), thus contain more model parameters, and exploit a huge amount of monolingual data. We expect that these results are because, in addition to an increase in the total quantity of training samples, the integration of multiple synthetic datasets, each of which focuses on different aspects of APE from the other—eSCAPE contains actual MT outputs; on the other hand, synthetic triplets better satisfy the minimal-edit criterion—appears to have an effect on the models’ APE performance. We found that our proposed methods derive a large number of new mt (Table 4) from eSCAPE and also yield a more simila"
2021.eacl-main.322,W16-2378,0,0.0375294,"Missing"
2021.eacl-main.322,P07-2045,0,0.00781914,"the output mtBG may not have a big difference from the original mt; on the other hand, if the model has been barely trained, mtBG would be almost the same as ref. In both instances, APE models trained on eSCAPEBG may not learn error-correction patterns sufficiently. We use the same arrangements (§4) as in FG to find an optimal value for the BG model’s stop point. 4 Experiments Metric. Following the evaluation setting used in the WMT APE shared task, we adopt TER (Snover et al., 2006) as the primary metric to measure the distance between the model’s prediction and the reference text; and BLEU (Koehn et al., 2007) as the secondary metric to measure the degree of ngram match. In addition, all evaluations in our experiments are case-sensitive. Dataset. We use two kinds of APE datasets: human-made APE datasets, which are provided by WMT, and eSCAPE. Both are English–German (EN–DE) APE corpora; they are further categorized according to their subtask depending on whether the target MT system is a phrase-based statistical MT (PBSMT) system or a neural MT (NMT) system. The WMT datasets are in the IT domain, whereas eSCAPE was made out of domaingeneral parallel corpora. Detailed data statistics are presented i"
2021.eacl-main.322,D18-2012,0,0.0250697,"orreia and Martins, 2019) (Ensemble) 16.49 74.98 16.83 73.94 17.15 73.60 16.82 74.17 – – – – – – – – – 16.06 75.96 BERT-APE (Lopes et al., 2019) – – Table 3: Evaluation results of our APE models using different configurations on training datasets. ‘*’ represents that our model’s improvement is significant enough compared to the eSCAPE baseline in the second row with p < 0.05. The best result among our models in each column is in bold type. The three models at the bottom are current state-of-the-art models. on Di and use 2,000 randomly extracted heldout samples from fi for validation. cePiece (Kudo and Richardson, 2018). Model Configuration. We implemented a Transformer-based APE model, the “sequential” model proposed by Lee et al. (2019), which is one of the best performing models. We use this model both as generation models that create synthetic mt with our two proposed methods and also as the final APE models to examine the effectiveness of those synthesized data as additional training data. We follow the hyperparameter setting described in Lee et al. (2019), which again follows almost the same setting of the “base” Transformer described in the original paper Vaswani et al. (2017). However, we adjust the"
2021.eacl-main.322,2020.wmt-1.83,1,0.826501,"ulates the shape of an item . Bearbeitet die Form eines Elements an . Ver¨andert die Form eines Elements . post-editors should create pe in principle by minimally editing mt while preserving the meaning of src. In fact, the sizes of currently available ‘genuine’ APE corpora provided by WMT (Bojar et al., 2016, 2017; Chatterjee et al., 2018, 2019, 2020) are too small to train deep APE models effectively. To overcome the lack of genuine APE corpora, several previous studies have proposed methods to construct synthetic training datasets (JunczysDowmunt and Grundkiewicz, 2016; Negri et al., 2018; Lee et al., 2020), and they appear to be partially helpful in mitigating the data scarcity problem. One such study is eSCAPE (Negri et al., 2018), which has been shown to be effective in training deep models and adopted in a number of APE works (do Carmo et al., 2020). Utilizing parallel corpora, which comprise pairs of a source sentence (src) and a reference sentence (ref ), eSCAPE was constructed as a set of synthetic APE triplets in the form of (src, mt, ref ) where mt is a machine translation of src, and ref serves as an alternative to pe of a genuine APE triplet. Despite the effectiveness of eSCAPE, we ar"
2021.eacl-main.322,W19-5412,1,0.848265,"et al., 2019) – – Table 3: Evaluation results of our APE models using different configurations on training datasets. ‘*’ represents that our model’s improvement is significant enough compared to the eSCAPE baseline in the second row with p < 0.05. The best result among our models in each column is in bold type. The three models at the bottom are current state-of-the-art models. on Di and use 2,000 randomly extracted heldout samples from fi for validation. cePiece (Kudo and Richardson, 2018). Model Configuration. We implemented a Transformer-based APE model, the “sequential” model proposed by Lee et al. (2019), which is one of the best performing models. We use this model both as generation models that create synthetic mt with our two proposed methods and also as the final APE models to examine the effectiveness of those synthesized data as additional training data. We follow the hyperparameter setting described in Lee et al. (2019), which again follows almost the same setting of the “base” Transformer described in the original paper Vaswani et al. (2017). However, we adjust the warm-up rate to 15,000 and the batch size to 25,000. We used OpenNMT-py2 to implement and execute all models. Synthetic D"
2021.eacl-main.322,W19-5413,0,0.0590319,"BLEU(↑) TER(↓) BLEU(↑) TER(↓) BLEU(↑) TER(↓) BLEU(↑) 24.76 16.84 62.11 24.48 62.49 24.24 62.99 24.49 62.53 74.73 eSCAPE 16.97 73.94 17.35 72.93 17.74 72.34 17.35 73.07 16.39 75.70 eSCAPEFG 17.06 73.96 17.40 72.81 18.00 72.19 17.48 72.98 16.30 75.77 eSCAPEBG 17.25 73.58 17.85 72.30 17.93 72.12 17.66 72.66 16.50 75.40 73.30∗ 17.32∗ 72.95∗ 17.05∗ 73.50∗ 16.09∗ 76.11∗ eSCAPE + eSCAPEFG 16.79 74.25 17.05∗ eSCAPE + eSCAPEBG 16.73 74.32∗ 16.96∗ 73.41∗ 17.26∗ 73.14∗ 16.98∗ 73.62∗ 15.95∗ 76.14∗ eSCAPE + eSCAPEFG + eSCAPEBG 16.57∗ 74.52∗ 16.99∗ 73.50∗ 17.29∗ 73.11∗ 16.95∗ 73.71∗ 16.15∗ 76.00∗ BERT-APE (Correia and Martins, 2019) 16.91 74.29 17.26 73.42 17.71 72.74 17.29 73.48 – BERT-APE (Correia and Martins, 2019) (Ensemble) 16.49 74.98 16.83 73.94 17.15 73.60 16.82 74.17 – – – – – – – – – 16.06 75.96 BERT-APE (Lopes et al., 2019) – – Table 3: Evaluation results of our APE models using different configurations on training datasets. ‘*’ represents that our model’s improvement is significant enough compared to the eSCAPE baseline in the second row with p < 0.05. The best result among our models in each column is in bold type. The three models at the bottom are current state-of-the-art models. on Di and use 2,000 random"
2021.eacl-main.322,L18-1004,0,0.016561,"slation. git 1 Manipulates the shape of an item . Bearbeitet die Form eines Elements an . Ver¨andert die Form eines Elements . post-editors should create pe in principle by minimally editing mt while preserving the meaning of src. In fact, the sizes of currently available ‘genuine’ APE corpora provided by WMT (Bojar et al., 2016, 2017; Chatterjee et al., 2018, 2019, 2020) are too small to train deep APE models effectively. To overcome the lack of genuine APE corpora, several previous studies have proposed methods to construct synthetic training datasets (JunczysDowmunt and Grundkiewicz, 2016; Negri et al., 2018; Lee et al., 2020), and they appear to be partially helpful in mitigating the data scarcity problem. One such study is eSCAPE (Negri et al., 2018), which has been shown to be effective in training deep models and adopted in a number of APE works (do Carmo et al., 2020). Utilizing parallel corpora, which comprise pairs of a source sentence (src) and a reference sentence (ref ), eSCAPE was constructed as a set of synthetic APE triplets in the form of (src, mt, ref ) where mt is a machine translation of src, and ref serves as an alternative to pe of a genuine APE triplet. Despite the effectivene"
2021.eacl-main.322,D19-1616,0,0.0185211,"to better simulate the characteristics of real APE data by making our synthetic mt better approximate the error distribution of the WMT APE benchmark dataset. 2 Background and Related Work Back-translation. Back-translation is a method to create synthetic source texts from clean target texts by using an MT system that is trained in the target–to–source direction. Back-translation has allowed many MT studies to use monolingual data to generate additional parallel data so that they alleviate data scarcity; moreover, it has also been successfully adopted by other NLP tasks such as summarization (Parida and Motlicek, 2019; Jernite, 2019) and grammatical error correction (Xie et al., 2018). Learning Objective of APE. Given that APE aims to revise mt to pe while preserving the meaning of src, each one of the two sources (src, mt) plays a distinct and critical role: src is treated as an auxiliary source, not only offering intact semantic and contextual information but also being helpful in identifying mistranslation; mt, meanwhile, serves as the primary source, which needs to be Figure 2: An illustration of our two synthetic data generation methods. x, y, and z are examples of src, mt, and ref, respectively. ˜y a"
2021.eacl-main.322,P16-1009,0,0.0991787,"Missing"
2021.eacl-main.322,2006.amta-papers.25,0,0.117427,"etween the BG output and ref becomes similar to the edit distance of real APE data. However, if we let the BG model fully converge, the output mtBG may not have a big difference from the original mt; on the other hand, if the model has been barely trained, mtBG would be almost the same as ref. In both instances, APE models trained on eSCAPEBG may not learn error-correction patterns sufficiently. We use the same arrangements (§4) as in FG to find an optimal value for the BG model’s stop point. 4 Experiments Metric. Following the evaluation setting used in the WMT APE shared task, we adopt TER (Snover et al., 2006) as the primary metric to measure the distance between the model’s prediction and the reference text; and BLEU (Koehn et al., 2007) as the secondary metric to measure the degree of ngram match. In addition, all evaluations in our experiments are case-sensitive. Dataset. We use two kinds of APE datasets: human-made APE datasets, which are provided by WMT, and eSCAPE. Both are English–German (EN–DE) APE corpora; they are further categorized according to their subtask depending on whether the target MT system is a phrase-based statistical MT (PBSMT) system or a neural MT (NMT) system. The WMT dat"
2021.eacl-main.322,N18-1057,0,0.022927,"tic mt better approximate the error distribution of the WMT APE benchmark dataset. 2 Background and Related Work Back-translation. Back-translation is a method to create synthetic source texts from clean target texts by using an MT system that is trained in the target–to–source direction. Back-translation has allowed many MT studies to use monolingual data to generate additional parallel data so that they alleviate data scarcity; moreover, it has also been successfully adopted by other NLP tasks such as summarization (Parida and Motlicek, 2019; Jernite, 2019) and grammatical error correction (Xie et al., 2018). Learning Objective of APE. Given that APE aims to revise mt to pe while preserving the meaning of src, each one of the two sources (src, mt) plays a distinct and critical role: src is treated as an auxiliary source, not only offering intact semantic and contextual information but also being helpful in identifying mistranslation; mt, meanwhile, serves as the primary source, which needs to be Figure 2: An illustration of our two synthetic data generation methods. x, y, and z are examples of src, mt, and ref, respectively. ˜y and ˜z are expected outputs that the proposed methods will produce. B"
2021.iwslt-1.30,C18-1139,0,0.0335143,"Missing"
2021.iwslt-1.30,2020.lrec-1.467,0,0.0201704,"Missing"
2021.iwslt-1.30,W03-2201,0,0.192958,"the score by 0.38 BLEU points. Subword tokenization reduced these effects to +0.22 points and –0.22 points respectively. Nonetheless, this demonstrates the feasibility of using certain pre-trained tagging outputs to improve translation quality. 2 next tag Softmax Softmax Linear Linear Encoder Decoder + + Embed. Embed. tokens tags src  next token Embed. Embed. previous previous tokens tags tgt Figure 1: Tagged seq2seq Related Work Very early work addressed named entity translation by treating automatically identified named entities with a special translation system, usually a transliterator (Babych and Hartley, 2003). This work did not attempt to integrate the translation models for one to benefit from information learned by the other. Later, especially with neural machine translation (NMT) systems, source-side feature augmentation research studied the inclusion of linguistic feature information into the source-side token embeddings, usually by adding in or concatenating additional learned feature vectors to the token embedding vectors, as we do in this work (Sennrich and Haddow, 2016; Hoang et al., 2016b; Ugawa et al., 2018; Modrzejewski et al., 2020; Modrzejewski, 2020; Armengol-Estap´e et al., 2020). T"
2021.iwslt-1.30,2014.iwslt-evaluation.1,0,0.0513107,"Missing"
2021.iwslt-1.30,N16-1149,0,0.0288801,"atically identified named entities with a special translation system, usually a transliterator (Babych and Hartley, 2003). This work did not attempt to integrate the translation models for one to benefit from information learned by the other. Later, especially with neural machine translation (NMT) systems, source-side feature augmentation research studied the inclusion of linguistic feature information into the source-side token embeddings, usually by adding in or concatenating additional learned feature vectors to the token embedding vectors, as we do in this work (Sennrich and Haddow, 2016; Hoang et al., 2016b; Ugawa et al., 2018; Modrzejewski et al., 2020; Modrzejewski, 2020; Armengol-Estap´e et al., 2020). This approach can also be adopted on the target-side, as presented here or in (Hoang et al., 2016a, 2018; Nguyen et al., 2018). However, these methods only add linguistic feature information to the input, without encouraging the system to model that information in any particular way. Factored translation systems, under both statistical and neural machine translation, instead explore the addition of externally supplied linguistic features to the raw text at both input and output. These features"
2021.iwslt-1.30,U16-1001,0,0.017636,"atically identified named entities with a special translation system, usually a transliterator (Babych and Hartley, 2003). This work did not attempt to integrate the translation models for one to benefit from information learned by the other. Later, especially with neural machine translation (NMT) systems, source-side feature augmentation research studied the inclusion of linguistic feature information into the source-side token embeddings, usually by adding in or concatenating additional learned feature vectors to the token embedding vectors, as we do in this work (Sennrich and Haddow, 2016; Hoang et al., 2016b; Ugawa et al., 2018; Modrzejewski et al., 2020; Modrzejewski, 2020; Armengol-Estap´e et al., 2020). This approach can also be adopted on the target-side, as presented here or in (Hoang et al., 2016a, 2018; Nguyen et al., 2018). However, these methods only add linguistic feature information to the input, without encouraging the system to model that information in any particular way. Factored translation systems, under both statistical and neural machine translation, instead explore the addition of externally supplied linguistic features to the raw text at both input and output. These features"
2021.iwslt-1.30,U18-1001,0,0.0397082,"Missing"
2021.iwslt-1.30,D07-1091,0,0.143777,"is approach can also be adopted on the target-side, as presented here or in (Hoang et al., 2016a, 2018; Nguyen et al., 2018). However, these methods only add linguistic feature information to the input, without encouraging the system to model that information in any particular way. Factored translation systems, under both statistical and neural machine translation, instead explore the addition of externally supplied linguistic features to the raw text at both input and output. These features include part-of-speech (POS) tags, word lemmatizations, morphological analysis, and semantic analysis (Koehn and Hoang, 2007; GarciaMartinez et al., 2016, 2017; Tan et al., 2020). Factored translation models map feature-augmented input into feature-augmented output, however outputs include only an underlying lemma together with the predicted features. These systems also use a rule-based morphology toolkit in post-processing to generate the output surface forms from predicted output features, requiring knowledge of appropriate rule systems for the output language. An additional tagged architecture (N˘adejde et al., 2017) predicted syntax-tagged surface forms, but did so by appending the tags to the surface form toke"
2021.iwslt-1.30,P18-1007,0,0.0205705,"this amount of structural information might be difficult to model with the same decoder architecture used for token prediction. Second, POS tags tend to carry the same amount of information for each tag at each position, compared to NER tags only conveying most of their information at the named entity spans which are few and far between. This also lends itself to the idea that POS tags have a higher information content that is less easily modeled by the decoder, leading to worse results than NER tagging. 6 6.1 tokenization, so additional experiments were conducted with a shared SentencePiece (Kudo, 2018) vocabulary of 32,000 subwords, built from the training split and used to tokenize both languages. After subword tokenization, the BIOES structure of named entity spans was propagated across subword tokens in the natural way to maintain spans. For POS tags, subwords received the same tag as their parent word. 5 Experiments Results BLEU scores from untagged and tagged translation experiments show an improvement from the use of NER tags (Table 1). Adding NER tags, the 3 baseline enhanced baseline / ablation study 5 ablation study 4 258 Enhanced baselines and ablation study For both NER and POS t"
2021.iwslt-1.30,2020.eamt-1.6,0,0.0722484,"Missing"
2021.iwslt-1.30,W17-4707,0,0.0336268,"Missing"
2021.iwslt-1.30,2020.lrec-1.497,0,0.0218828,"Missing"
2021.iwslt-1.30,W16-2209,0,0.0227282,"anslation by treating automatically identified named entities with a special translation system, usually a transliterator (Babych and Hartley, 2003). This work did not attempt to integrate the translation models for one to benefit from information learned by the other. Later, especially with neural machine translation (NMT) systems, source-side feature augmentation research studied the inclusion of linguistic feature information into the source-side token embeddings, usually by adding in or concatenating additional learned feature vectors to the token embedding vectors, as we do in this work (Sennrich and Haddow, 2016; Hoang et al., 2016b; Ugawa et al., 2018; Modrzejewski et al., 2020; Modrzejewski, 2020; Armengol-Estap´e et al., 2020). This approach can also be adopted on the target-side, as presented here or in (Hoang et al., 2016a, 2018; Nguyen et al., 2018). However, these methods only add linguistic feature information to the input, without encouraging the system to model that information in any particular way. Factored translation systems, under both statistical and neural machine translation, instead explore the addition of externally supplied linguistic features to the raw text at both input and ou"
2021.iwslt-1.30,2020.emnlp-main.455,0,0.0123408,"sented here or in (Hoang et al., 2016a, 2018; Nguyen et al., 2018). However, these methods only add linguistic feature information to the input, without encouraging the system to model that information in any particular way. Factored translation systems, under both statistical and neural machine translation, instead explore the addition of externally supplied linguistic features to the raw text at both input and output. These features include part-of-speech (POS) tags, word lemmatizations, morphological analysis, and semantic analysis (Koehn and Hoang, 2007; GarciaMartinez et al., 2016, 2017; Tan et al., 2020). Factored translation models map feature-augmented input into feature-augmented output, however outputs include only an underlying lemma together with the predicted features. These systems also use a rule-based morphology toolkit in post-processing to generate the output surface forms from predicted output features, requiring knowledge of appropriate rule systems for the output language. An additional tagged architecture (N˘adejde et al., 2017) predicted syntax-tagged surface forms, but did so by appending the tags to the surface form tokens directly, rather than predicting separate factors."
2021.iwslt-1.30,tiedemann-2012-parallel,0,0.0922889,"Missing"
2021.iwslt-1.30,W03-0419,0,0.383884,"Missing"
2021.iwslt-1.30,C18-1274,0,0.0212339,"amed entities with a special translation system, usually a transliterator (Babych and Hartley, 2003). This work did not attempt to integrate the translation models for one to benefit from information learned by the other. Later, especially with neural machine translation (NMT) systems, source-side feature augmentation research studied the inclusion of linguistic feature information into the source-side token embeddings, usually by adding in or concatenating additional learned feature vectors to the token embedding vectors, as we do in this work (Sennrich and Haddow, 2016; Hoang et al., 2016b; Ugawa et al., 2018; Modrzejewski et al., 2020; Modrzejewski, 2020; Armengol-Estap´e et al., 2020). This approach can also be adopted on the target-side, as presented here or in (Hoang et al., 2016a, 2018; Nguyen et al., 2018). However, these methods only add linguistic feature information to the input, without encouraging the system to model that information in any particular way. Factored translation systems, under both statistical and neural machine translation, instead explore the addition of externally supplied linguistic features to the raw text at both input and output. These features include part-of-spee"
2021.iwslt-1.30,D19-1519,0,0.0278925,"Missing"
2021.nuse-1.6,N15-1113,0,0.173902,"s to produce concise texts containing only the essential information in the original texts. Although most researches have been focusing on summarizing news articles (Narayan et al., 2018; See et al., 2017), as various contents with different structures increase these days, there has been growing interests in applying text summarization to various domains, including social media (Sharifi et al., 2010; Kim and Monroy-Hernandez, 2016), dialogue (Goo and Chen, 2018), scientific articles (Cohan and Goharian, 2017; Yasunaga et al., 2019), books (Mihalcea and Ceylan, 2007), screenplays (or scripts) (Gorinski and Lapata, 2015; Papalampidi et al., 2020a). Among them, this paper focuses on screenplay summarization. A screenplay is a type of literary text, which typically contains around 120 pages and has a strictly structured format (Figure 1). It usually contains various storytelling elements, such as a story, dialogues, characters’ actions, and what the camera sees, thereby elaborating a complex story. In a real-life situation, filmmakers and directors hire script readers to select a script that seems to be a popular movie among numerous candidate scripts. They create a coverage per script, a report of about four"
2021.nuse-1.6,D19-1387,0,0.0188622,"173.0 (235.0) 22.2 (31.5) 7.8 (6.0) 150.5 (198.3) 19.9 (26.9) 7.6 (6.4) Model PA ↑ D↓ TAM GraphTP Transformer TAM GraphTP Transformer 8.15 7.41 10.37 7.41 13.33 11.11 9.33 10.67 10.67 9.33 14.67 12.00 10.59 9.24 9.12 9.97 11.61 9.82 # of parameters Training time (ratio) 40.1k 41.6k 46.3k 1.12 1.45 1.00 TAM GraphTP Transformer Table 4: The number of parameters and training time of models. Numbers in ‘Training time’ are ratios to the training time of our proposed model set at 1. Experimental Setting For our experiments, we adapted source codes in two repositories1 2 Papalampidi et al. (2020b); Liu and Lapata (2019) to implement our model. We set the training hyperparameters as follows: L = 1, H = 128, A = 4, and Pdrop = 0.0, where L is the number of layers, H is the hidden size, A is the number of heads, and Pdrop is the dropout rate. We consider two previous methods that receive raw sentence representations as inputs as the baseline systems: TAM (Papalampidi et al., 2019) and GraphTP (Papalampidi et al., 2020b). During training, because TRIPOD does not contain a validation set, we conducted n-fold cross-validation with n = 5 to extract the validation set from the existing test set. Finally, we averaged"
2021.nuse-1.6,D07-1040,0,0.0239155,"Text summarization is one major task in NLP that seeks to produce concise texts containing only the essential information in the original texts. Although most researches have been focusing on summarizing news articles (Narayan et al., 2018; See et al., 2017), as various contents with different structures increase these days, there has been growing interests in applying text summarization to various domains, including social media (Sharifi et al., 2010; Kim and Monroy-Hernandez, 2016), dialogue (Goo and Chen, 2018), scientific articles (Cohan and Goharian, 2017; Yasunaga et al., 2019), books (Mihalcea and Ceylan, 2007), screenplays (or scripts) (Gorinski and Lapata, 2015; Papalampidi et al., 2020a). Among them, this paper focuses on screenplay summarization. A screenplay is a type of literary text, which typically contains around 120 pages and has a strictly structured format (Figure 1). It usually contains various storytelling elements, such as a story, dialogues, characters’ actions, and what the camera sees, thereby elaborating a complex story. In a real-life situation, filmmakers and directors hire script readers to select a script that seems to be a popular movie among numerous candidate scripts. They"
2021.nuse-1.6,D18-1206,0,0.0228702,"cene is an event that takes place at the same time or place. Every scene starts with a scene heading (starts with &quot;INT.&quot; or &quot;EXT.&quot;) and is followed by action descriptions and dialogues. ‘Scene heading’ denotes when and where actions take place. ‘Action description’ explains who and what are in the scene. ‘Character’ is the speaker. ‘Dialogue’ is a spoken utterance. Introduction Text summarization is one major task in NLP that seeks to produce concise texts containing only the essential information in the original texts. Although most researches have been focusing on summarizing news articles (Narayan et al., 2018; See et al., 2017), as various contents with different structures increase these days, there has been growing interests in applying text summarization to various domains, including social media (Sharifi et al., 2010; Kim and Monroy-Hernandez, 2016), dialogue (Goo and Chen, 2018), scientific articles (Cohan and Goharian, 2017; Yasunaga et al., 2019), books (Mihalcea and Ceylan, 2007), screenplays (or scripts) (Gorinski and Lapata, 2015; Papalampidi et al., 2020a). Among them, this paper focuses on screenplay summarization. A screenplay is a type of literary text, which typically contains aroun"
2021.nuse-1.6,2020.acl-main.174,0,0.168866,"containing only the essential information in the original texts. Although most researches have been focusing on summarizing news articles (Narayan et al., 2018; See et al., 2017), as various contents with different structures increase these days, there has been growing interests in applying text summarization to various domains, including social media (Sharifi et al., 2010; Kim and Monroy-Hernandez, 2016), dialogue (Goo and Chen, 2018), scientific articles (Cohan and Goharian, 2017; Yasunaga et al., 2019), books (Mihalcea and Ceylan, 2007), screenplays (or scripts) (Gorinski and Lapata, 2015; Papalampidi et al., 2020a). Among them, this paper focuses on screenplay summarization. A screenplay is a type of literary text, which typically contains around 120 pages and has a strictly structured format (Figure 1). It usually contains various storytelling elements, such as a story, dialogues, characters’ actions, and what the camera sees, thereby elaborating a complex story. In a real-life situation, filmmakers and directors hire script readers to select a script that seems to be a popular movie among numerous candidate scripts. They create a coverage per script, a report of about four pages containing a logline"
2021.nuse-1.6,D19-1180,0,0.23451,", a synopsis (the informative summary), recommendations, ratings, and comments. The goal of screenplay summarization is to help speeding up script browsing; to provide an overview of the script’s contents and storyline; and to reduce the reading time (Gorinski and Lapata, 2015). As shown in Figure 2, to make this long narrative-text summarization feasible, early work in screenplay summarization (Gorinski and Lapata, 2015; Papalampidi et al., 2020a) defined the task as extracting a sequence of scenes that represents informative summary (i.e., scene-level extractive summarization). To this end, Papalampidi et al. (2019, 2020b) 56 Proceedings of the 3rd Workshop on Narrative Understanding, pages 56–61 June 11, 2021. ©2021 Association for Computational Linguistics Screenplay Scene #1 〓 Summary Scene-level Extractive Summarization Scene #2 Scene #3  TP1: Opportunity Introductory event that occurs after presentation of setting and background of main characters Scene #7 Scene #11 Scene #12 … …  TP2: Change of Plans Main goal of story is defined; action begins to increase Scene #Ν Scene #Μ Input Output  TP3: Point of No Return Event that pushes the main characters to fully commit to their goal Figure 2: Screen"
2021.nuse-1.6,P17-1099,0,0.0229941,"takes place at the same time or place. Every scene starts with a scene heading (starts with &quot;INT.&quot; or &quot;EXT.&quot;) and is followed by action descriptions and dialogues. ‘Scene heading’ denotes when and where actions take place. ‘Action description’ explains who and what are in the scene. ‘Character’ is the speaker. ‘Dialogue’ is a spoken utterance. Introduction Text summarization is one major task in NLP that seeks to produce concise texts containing only the essential information in the original texts. Although most researches have been focusing on summarizing news articles (Narayan et al., 2018; See et al., 2017), as various contents with different structures increase these days, there has been growing interests in applying text summarization to various domains, including social media (Sharifi et al., 2010; Kim and Monroy-Hernandez, 2016), dialogue (Goo and Chen, 2018), scientific articles (Cohan and Goharian, 2017; Yasunaga et al., 2019), books (Mihalcea and Ceylan, 2007), screenplays (or scripts) (Gorinski and Lapata, 2015; Papalampidi et al., 2020a). Among them, this paper focuses on screenplay summarization. A screenplay is a type of literary text, which typically contains around 120 pages and has"
C00-1079,C90-2057,0,0.0914202,"Missing"
C00-1079,C90-2043,0,0.0936953,"Missing"
C94-2147,C86-1045,0,0.0131103,"eme lattice] A CYK-based morphological analyzer is used to extract a morpheme lattice from the phoneme lattice. In the morphological analysis, special procedural attachments resolve the phonological changes. The use of phoneme lattice gives the problem of exponential number of Eonjeol candidates. For this problem, trie data structure is used for the p h o n e t i c t r a n s c r i p t i o n - t o orthographic morpheme dictionary (morphemelevel phonetic dictionary). 3.1. Extending the Categorial Grammar To model the syntax of Korean, we extended the Categorial Grammar in two ways (Zeevat 1988; Uszkoreit 1986). A (directional) Categorial Grammar is an ordered quintuple G = <V, C, S, R, f>, where 912 (Howells 1988) developed an interactive relaxation parsing method which used a dynamic network building scheme, and decay over time with competition instead of explicit inhibitory links, which is similar to the (Reggia 1987)'s approach. The interactive relaxation algorithm consists of the following steps (Howells 1988): 1) add nodes, 2) spread actiwltion and 3) decay. Bottom-up information gathering and top-down expectations occur during the parsing. 1) to add a node: A grammar node is added tot&quot; each s"
C94-2147,C86-1138,0,\N,Missing
C98-1107,J97-4001,0,\N,Missing
C98-2120,J93-1007,0,\N,Missing
I05-1014,W95-0107,0,0.0254416,"n this paper, we present how CRFs can be applied to the task of chunking in Korean texts. Experiments using the STEP 2000 dataset show that the proposed method significantly improves the performance as well as outperforms previous systems. 1 Introduction Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3]. Abney first proposed it as an intermediate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3]. Most previous works with relatively high performance in English used machine learning methods for chunking [4,13]. Machine learning methods are mainly divided into the generative approach and conditional approach. The generative approach relies on generative probabilistic models that assign a joint probability p(X,Y) of paired input sequence and label sequence, X and Y respectively. It provides straightforward understanding"
I05-1014,W00-0726,0,0.0661412,"Korean and Japanese, a rule-based chunking method is predominantly used for its simplicity and efficiency. A hybrid of a rule-based and machine learning method was also proposed to handle exceptional cases of the rules. In this paper, we present how CRFs can be applied to the task of chunking in Korean texts. Experiments using the STEP 2000 dataset show that the proposed method significantly improves the performance as well as outperforms previous systems. 1 Introduction Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3]. Abney first proposed it as an intermediate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3]. Most previous works with relatively high performance in English used machine learning methods for chunking [4,13]. Machine learning methods are mainly divided into the generative approach"
I05-1014,N01-1025,0,0.0394209,"n this paper, we present how CRFs can be applied to the task of chunking in Korean texts. Experiments using the STEP 2000 dataset show that the proposed method significantly improves the performance as well as outperforms previous systems. 1 Introduction Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3]. Abney first proposed it as an intermediate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3]. Most previous works with relatively high performance in English used machine learning methods for chunking [4,13]. Machine learning methods are mainly divided into the generative approach and conditional approach. The generative approach relies on generative probabilistic models that assign a joint probability p(X,Y) of paired input sequence and label sequence, X and Y respectively. It provides straightforward understanding"
I05-1014,P03-1063,0,0.210453,"n this paper, we present how CRFs can be applied to the task of chunking in Korean texts. Experiments using the STEP 2000 dataset show that the proposed method significantly improves the performance as well as outperforms previous systems. 1 Introduction Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3]. Abney first proposed it as an intermediate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3]. Most previous works with relatively high performance in English used machine learning methods for chunking [4,13]. Machine learning methods are mainly divided into the generative approach and conditional approach. The generative approach relies on generative probabilistic models that assign a joint probability p(X,Y) of paired input sequence and label sequence, X and Y respectively. It provides straightforward understanding"
I05-1014,J96-1002,0,0.0674284,"n this paper, we present how CRFs can be applied to the task of chunking in Korean texts. Experiments using the STEP 2000 dataset show that the proposed method significantly improves the performance as well as outperforms previous systems. 1 Introduction Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3]. Abney first proposed it as an intermediate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3]. Most previous works with relatively high performance in English used machine learning methods for chunking [4,13]. Machine learning methods are mainly divided into the generative approach and conditional approach. The generative approach relies on generative probabilistic models that assign a joint probability p(X,Y) of paired input sequence and label sequence, X and Y respectively. It provides straightforward understanding"
I05-1014,N03-1028,0,0.620828,"n this paper, we present how CRFs can be applied to the task of chunking in Korean texts. Experiments using the STEP 2000 dataset show that the proposed method significantly improves the performance as well as outperforms previous systems. 1 Introduction Text chunking is a process to identify non-recursive cores of various phrase types without conducting deep parsing of text [3]. Abney first proposed it as an intermediate step toward full parsing [1]. Since Ramshaw and Marcus approached NP chunking using a machine learning method, many researchers have used various machine learning techniques [2,4,5,6,10,11,13,14]. The chunking task was extended to the CoNLL2000 shared task with standard datasets and evaluation metrics, which is now a standard evaluation task for text chunking [3]. Most previous works with relatively high performance in English used machine learning methods for chunking [4,13]. Machine learning methods are mainly divided into the generative approach and conditional approach. The generative approach relies on generative probabilistic models that assign a joint probability p(X,Y) of paired input sequence and label sequence, X and Y respectively. It provides straightforward understanding"
I05-2044,W03-3017,0,0.0794111,"Missing"
I05-2044,W03-3023,0,0.53447,"p word of the stack and the input word, according to the direction of the dependency arc, it can be either Left-Arc or Right-Arc. Otherwise, the transition can be either shift or reduce. If the head of the top word of the stack is already determined, then the transition is reduce, otherwise shift. The action of each transition is shown in Fig.1. For details, please refer to Nivre[3,10]. Fig.2 gives an example 1 of parsing a Chinese sentence using Nivre’s algorithm. Nivre’s[3,10] approach has several advantages. First, the dependency structure produced by the algorithm is projective and acyclic[3]. Second, the algorithm performs very well for deciding short-distance dependences. Third, at each parsing step, all of the dependency relations on the left side of the input word are determined. Also as the author emphasizes, the time complexity is linear. However, wrong decision of reduce transition, like early reduce, cause the word at the top of the stack loses the chance to be the head of others. Some words lose the chance to be the head of other following words. As a result, the dependents of this word will have a wrong head or may have no head. The parsing steps of a Chinese sentence us"
I05-2044,C96-1058,0,0.747936,"Fig. 3. The correct parse tree of Example-1 Fig.4. gives the parsing step of another example. As the final dependency tree in Fig.4 shows, there is no head for word 消息。After Step-5, the top of the stack is word 给 and input word is 一 . There is no dependency relation between these two words. Since the head of the word 给 is already determined in step-2，the next transition is R(educe). As a result, word 给 loses the chance to be the head of word 消息. So, there is no head assigned to word 消息 in Fig.4. Therefore, Nivre’s algorithm causes some errors for determining the right-side dependents. Yamada’s[4] approach is similar to Nivre’s[3]. Yamada’s algorithm define three actions: left, right and shift, which were similar to those of Nivre’s. Yamada parsed a sentence by scanning the sentence word by word from left to right, during the meantime, left or right or shift actions were decided. For short dependents, Yamada’s algorithm can cope with it easily. For long dependents, Yamada tried to solve by increasing the iteration of scanning the sentences. As Yamada pointed out, ‘shift’ transition was executed for two kinds of structure. This may cause wrong decision while deciding the action of trans"
I05-2044,J05-1003,0,0.0206133,"Missing"
I05-2044,P03-1056,0,0.0349958,"Missing"
I05-2044,C04-1010,0,0.127897,"ck, I is a list of (remaining) input tokens, and A is the set of determined dependency relations. Nivre defined four transitions: Left-Arc, Right-Arc, Reduce, and Shift. If there is a dependency relation between the top word of the stack and the input word, according to the direction of the dependency arc, it can be either Left-Arc or Right-Arc. Otherwise, the transition can be either shift or reduce. If the head of the top word of the stack is already determined, then the transition is reduce, otherwise shift. The action of each transition is shown in Fig.1. For details, please refer to Nivre[3,10]. Fig.2 gives an example 1 of parsing a Chinese sentence using Nivre’s algorithm. Nivre’s[3,10] approach has several advantages. First, the dependency structure produced by the algorithm is projective and acyclic[3]. Second, the algorithm performs very well for deciding short-distance dependences. Third, at each parsing step, all of the dependency relations on the left side of the input word are determined. Also as the author emphasizes, the time complexity is linear. However, wrong decision of reduce transition, like early reduce, cause the word at the top of the stack loses the chance to be"
I05-2044,P97-1003,0,0.418995,"c set the dependency relation for two words; pop the top of the stack else if the action is Right-arc set the dependency relation for two words Fig. 6. Types of transitions in the phase I x.pos : POS tag of word x x.left.child : the left-side nearest dependent of word x punc : the surface form of punctuation between top word of the stack and input word, if there is any if_adjoin : a binary indicator to show if the top word of the stack and input word are adjoined The type of transition is determined by the top word of the stack, input word and their context. Most of the previous parsing models[4,12,13] use lexical words as features. Compared to Penn English Treebank, the size of Penn Chinese Treebank (version 4.0, abbreviated as CTB) is rather small. Considering the data sparseness problem, we use POS tags instead of lexical words itself. As Fig.7. shows, the window for feature extraction is the top word of the stack, input word, previous word of the top of the stack, next word of the input. The left-side nearest dependent of these is also taken into consideration. Besides, we use two more features, if_adjoin, and Punc. The feature vector for Phase I is shown in Fig.7. The feature vector fo"
I05-2044,A00-2018,0,0.115011,"c set the dependency relation for two words; pop the top of the stack else if the action is Right-arc set the dependency relation for two words Fig. 6. Types of transitions in the phase I x.pos : POS tag of word x x.left.child : the left-side nearest dependent of word x punc : the surface form of punctuation between top word of the stack and input word, if there is any if_adjoin : a binary indicator to show if the top word of the stack and input word are adjoined The type of transition is determined by the top word of the stack, input word and their context. Most of the previous parsing models[4,12,13] use lexical words as features. Compared to Penn English Treebank, the size of Penn Chinese Treebank (version 4.0, abbreviated as CTB) is rather small. Considering the data sparseness problem, we use POS tags instead of lexical words itself. As Fig.7. shows, the window for feature extraction is the top word of the stack, input word, previous word of the top of the stack, next word of the input. The left-side nearest dependent of these is also taken into consideration. Besides, we use two more features, if_adjoin, and Punc. The feature vector for Phase I is shown in Fig.7. The feature vector fo"
I05-2044,P96-1025,0,0.0675869,"ced by Vapnik[16]. SVM has been used for various NLP tasks, and gives reasonable outputs. For the experiments reported in this paper, we used the software package SVMlight [17]. For evaluation matrix, we use Dependency Accuracy and Root Accuracy defined by Yamada[4]. An additional evaluation measure, None Head is defined as following. 4.1 Conversion of Penn Chinese Treebank to Dependency Trees Annotating a Treebank is a tedious task. To take the advantage of CTB, we made some heuristic rules to convert CTB into dependency Treebank. This kind of conversion task has been done on English Treebank[14,10,4]. We use the dependency formalism as Zhou[15] defined. CTB contains 15,162 newswire sentences (including titles, fragments and headlines). The contents of CTB are from Xinhua of mainland, information services department of HKSAR and Sinorama magazine of Taiwan. For experiments, 12,142 sentences are extracted, excluding all the titles, headlines and fragments. For the conversion task, we made some heuristic rules. CTB defines total 23 syntactic phrases and verb compounds[11]. A phrase is composed of several words accompanied to a head word. The head word of each phrase is used as an important r"
I05-2044,W00-1201,0,\N,Missing
I08-4002,P06-1010,0,0.323829,"out 40 years. However, some new foreign names still cannot be found in the dictionary. Constructing an unknown word dictionary is a difficult and time consuming job, so in this paper 9 we propose a novel approach to automatically construct the resource by efficiently extracting transliteration pairs from bilingual texts. Recently, much research has been conducted on machine transliteration. Machine transliteration is classified into two types. One is automatic generation of transliterated word from the source language [6]; the other one is extracting transliteration pairs from bilingual texts [2]. Generally, the generation process performs worse than the extraction process. Especially in Chinese, people do not always transliterate foreign words only by sound but also consider the meanings. For example, the word ‘blog’ is not transliterated into ‘ 布 劳 哥 ’ (BuLaoGe) which is phonetically equivalent to the source word, but transliterated into ‘博客’(BoKe) which means ‘a lot of guests’. In this case, it is too difficult to automatically generate correct transliteration words. Therefore, our approach is based on the method of extracting transliteration pairs from bilingual texts. The type of"
I08-4002,J98-4003,0,0.209629,"Missing"
I08-4002,W02-2017,0,0.0586658,"Missing"
I08-4002,C02-1099,0,0.0317267,"f the world&apos;s peoples”[12] containing 630,000 entries in 1993, which took about 40 years. However, some new foreign names still cannot be found in the dictionary. Constructing an unknown word dictionary is a difficult and time consuming job, so in this paper 9 we propose a novel approach to automatically construct the resource by efficiently extracting transliteration pairs from bilingual texts. Recently, much research has been conducted on machine transliteration. Machine transliteration is classified into two types. One is automatic generation of transliterated word from the source language [6]; the other one is extracting transliteration pairs from bilingual texts [2]. Generally, the generation process performs worse than the extraction process. Especially in Chinese, people do not always transliterate foreign words only by sound but also consider the meanings. For example, the word ‘blog’ is not transliterated into ‘ 布 劳 哥 ’ (BuLaoGe) which is phonetically equivalent to the source word, but transliterated into ‘博客’(BoKe) which means ‘a lot of guests’. In this case, it is too difficult to automatically generate correct transliteration words. Therefore, our approach is based on the"
I08-4002,Y03-1035,0,0.0239884,"ect the Chinese character sequence with highest score and back-track the alignment result to extract the correct transliteration word. [Fig 5] shows the entire process of using the dynamic window approach to extract the correct transliteration word. English Word 齐格 Chinese Sentence 与 Ziegler 学家居 奥共 获 1963 诺贝 学奖。 Ziegler and Italian Chemist Julio received the Nobel prize of 1963 together. English Sentence Extracted transliteration without using dynamic window Correct transliteration 家居 奥 (JiaJuLiAo) 齐格 (QiGeLe) Steps 1. Set Chinese transliteration’s range according to English word “Ziegler” to [2, 7] (After analyzing the distribution between an English word and a Chinese transliteration word, we found that if the English word length is Ｌ, then the Chinese transliteration word is between Ｌ/3 andＬ.) 2. Slide each window to find sequence with highest score. 3 Select the Chinese character sequence with highest score and back-track the alignment result to extract a correct transliteration word. Score WinChinese character sequence with high- (normaldow est score of each window (underline ize with size the back-tracking result) window size) 2 -9.327 格 (QiGe) 齐格 3 齐格 4 齐格 5 6 家居 奥共 7 齐格 与 与 (QiGe"
I08-4002,W03-0317,0,0.635058,"Missing"
J02-1004,J95-4004,0,0.767037,"ion purposes at http://nlp.postech.ac.kr/. Follow the link OpenResources→DownLoad. c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 Previous techniques for guessing unknown words mostly utilize the guessing rules to analyze the word features by looking at leading and trailing characters. Most of them employ the analysis of trailing characters and other features such as capitalization and hyphenation (Kupiec 1992; Weischedel et al. 1993). Some of them use more morphologically oriented word features such as suffixes, prefixes, and character lengths (Brill 1995; Voutilainen 1995). The guessing rules are usually handcrafted using knowledge of morphology but sometimes are acquired automatically using lexicons and corpora (Brill 1995; Mikheev 1996; Oflazer and Tur ¨ 1996). Previously developed methods for guessing unknown morphemes in Korean are not much different from the methods used for English. Basically, they rely on the rules that reflect knowledge of Korean morphology and word formation. The usual way of handling unknown morphemes is to guess all the possible POS tags for an unknown morpheme by checking connectable functional morphemes in the sa"
J02-1004,A92-1018,0,0.291606,"pattern-based generalized unknown-morpheme estimation method using a morpheme pattern dictionary that enables us to treat unknown morphemes in the same way as registered known morphemes, and thereby to guess them regardless of their numbers or positions in an eojeol. The method for estimating unknown morphemes using the morpheme pattern dictionary in Korean needs to be tightly integrated into morphological analysis and POS disambiguation systems. POS disambiguation has usually been performed by statistical approaches, mainly using the hidden Markov model (HMM) in English research communities (Cutting et al. 1992; Kupiec 1992; Weischedel et al. 1993). These approaches are also dominant for Korean, with slight improvements to accommodate the agglutinative nature of Korean. For Korean, early HMM tagging was based on eojeols. The eojeol-based tagging model calculates lexical and transition probabilities with eojeols as a unit; it suffers from severe data sparseness problems since a single eojeol consists of many different morphemes (Lee, Choi, and Kim 1993). Later, morpheme-based HMM tagging was tried; such models assign a single tag to a morpheme regardless of the space in a sentence. Morpheme-based tag"
J02-1004,J94-2001,0,0.043092,"Missing"
J02-1004,P96-1043,0,0.0203309,"s techniques for guessing unknown words mostly utilize the guessing rules to analyze the word features by looking at leading and trailing characters. Most of them employ the analysis of trailing characters and other features such as capitalization and hyphenation (Kupiec 1992; Weischedel et al. 1993). Some of them use more morphologically oriented word features such as suffixes, prefixes, and character lengths (Brill 1995; Voutilainen 1995). The guessing rules are usually handcrafted using knowledge of morphology but sometimes are acquired automatically using lexicons and corpora (Brill 1995; Mikheev 1996; Oflazer and Tur ¨ 1996). Previously developed methods for guessing unknown morphemes in Korean are not much different from the methods used for English. Basically, they rely on the rules that reflect knowledge of Korean morphology and word formation. The usual way of handling unknown morphemes is to guess all the possible POS tags for an unknown morpheme by checking connectable functional morphemes in the same eojeol (Kang 1993).2 However, in this way, it is only possible to guess probable POS tags for a single unknown morpheme when it occurs at the beginning of an eojeol. Unlike in English,"
J02-1004,C94-1032,0,0.0269678,"Lexical Probability Estimation for Unknown-Morpheme Guessing The lexical probabilities for unknown morphemes cannot be precalculated using Equation (8) since we assume the unknown morphemes do not appear in the training corpus, so a special on-the-fly estimation method must be applied. We suggest using syllable trigrams since Korean syllables can play an important role in restricting units for guessing the POS of a morpheme. The lexical probability PrPr(t(ti |mi )i ) for unknown morphemes can be estimated using the frequency of syllable trigram products according to the formula in (11)–(13) (Nagata 1994), m Pr (t |m) Pr (t) = e1 e2 . . . en ≈ Pr t (e1 |#, # )Pr t (e2 |#, e1 ) × n Y i=3 62 Pr t (ei |ei−2 , ei−1 ) (11) Lee, Cha, and Lee Syllable-Pattern-Based Unknown-Morpheme Estimation × Pr (# |en−1 , en ) (12) Pr t (ei |ei−2 , ei−1 ) ≈ ft (ei |ei−2 , ei−1 ) + ft (ei |ei−1 ) (13) + ft (ei ) where m is a morpheme, e is a syllable, t is a POS tag, “#” is a morpheme boundary symbol, and ft (ei |ei−2 , ei−1 ) is a frequency datum for tag t with co-occurrence syllables ei−2 , ei−1 , and ei . Trigram probabilities are smoothed by Equation (13) to cope with the data sparseness problem. For example, P"
J02-1004,W96-0207,0,0.0193826,"single unknown morpheme when it occurs at the beginning of an eojeol. Unlike in English, in Korean, more than one unknown morpheme can appear in a single eojeol because an eojeol can include complex components such as Chinese characters, Japanese words, and other foreign words. If an eojeol contains more than one unknown morpheme or if the unknown morphemes appear in other than first position in the eojeol, all previous methods fail to efficiently estimate them. This is the reason why we try to avoid conventional guessing rules using word morphology features such as those proposed in Mikheev (1996) and Oflazer and Tur ¨ (1996).3 In this paper, we propose a syllable-pattern-based generalized unknown-morpheme estimation method using a morpheme pattern dictionary that enables us to treat unknown morphemes in the same way as registered known morphemes, and thereby to guess them regardless of their numbers or positions in an eojeol. The method for estimating unknown morphemes using the morpheme pattern dictionary in Korean needs to be tightly integrated into morphological analysis and POS disambiguation systems. POS disambiguation has usually been performed by statistical approaches, mainly"
J02-1004,J93-2006,0,0.14406,"POSTECH), Pohang, 790-784, Korea. E-mail: jhlee@postech.ac.kr. 1 The binary code of POSTAG is open to the public for research and evaluation purposes at http://nlp.postech.ac.kr/. Follow the link OpenResources→DownLoad. c 2002 Association for Computational Linguistics Computational Linguistics Volume 28, Number 1 Previous techniques for guessing unknown words mostly utilize the guessing rules to analyze the word features by looking at leading and trailing characters. Most of them employ the analysis of trailing characters and other features such as capitalization and hyphenation (Kupiec 1992; Weischedel et al. 1993). Some of them use more morphologically oriented word features such as suffixes, prefixes, and character lengths (Brill 1995; Voutilainen 1995). The guessing rules are usually handcrafted using knowledge of morphology but sometimes are acquired automatically using lexicons and corpora (Brill 1995; Mikheev 1996; Oflazer and Tur ¨ 1996). Previously developed methods for guessing unknown morphemes in Korean are not much different from the methods used for English. Basically, they rely on the rules that reflect knowledge of Korean morphology and word formation. The usual way of handling unknown mo"
J02-1004,E95-1022,0,\N,Missing
J02-1004,A94-1008,0,\N,Missing
li-etal-2008-annotation,J93-2003,0,\N,Missing
li-etal-2008-annotation,J96-2004,0,\N,Missing
li-etal-2008-annotation,J03-1002,0,\N,Missing
li-etal-2008-annotation,kruijff-korbayova-etal-2006-annotation,0,\N,Missing
N16-1059,C04-1046,0,0.111689,"s the input of the final recurrent neural network. The prediction method uses bi-directional recurrent neural network architecture both on source and target sentence to fully utilize the bi-directional quality information from source and target sentence. Our experiments show that the proposed recurrent neural networks approach achieves a performance comparable to the existing stateof-the-art models for estimating the sentencelevel quality of English-to-Spanish translation. 1 Introduction Estimating the quality of machine translation output, called quality estimation (QE) (Specia et al., 2009; Blatz et al., 2004), is to predict quality scores/categories for unseen machinetranslated sentences without reference translations at various granularity levels (sentence-level/wordlevel/document-level). Quality estimation is of growing importance in the field of machine translation (MT) since MT systems are widely used and the quality of each machine-translated sentence is able to vary considerably. Previous research on QE, addressed as a regression/classification problem to compute quality scores/categories, has mainly focused on feature extraction and feature selection. Feature extraction is to find the relev"
N16-1059,D14-1179,0,0.0605412,"Missing"
N16-1059,2005.mtsummit-papers.11,0,0.0322323,"l winning Figure 3: The ways of computing the quality vector qyj ( ◦ is  MAE ↓ 0.1325 0.1334 0.1335 0.1342 0.1359 0.1371 0.1482 sults for the scoring variant of WMT15 Quality Estimation [Wo2 tj ]> [ System ID • RTM-DCU/RTM-FS+PLS-SVR • LORIA/17+LSI+MT+FILTRE • RTM-DCU/RTM-FS-SVR • LORIA/17+LSI+MT Bi-RNN • UGENT-LT3/SCATE-SVM Baseline SVM Experiments The proposed RNNs approach was evaluated on the WMT15 Quality Estimation Shared Task9 at sentence level of English-Spanish. We trained10 the proposed model through a twostep process. First, by using English-Spanish parallel corpus of Europarl v7 (Koehn, 2005), we trained bi-directional RNNs having 1000 hidden units on source and target sentence to make quality vectors. Next, by using the training set of WMT15 QE task, to predicte QE scores we trained the final RNN that 9 http://www.statmt.org/wmt15/qualityestimation-task.html 10 Stochastic gradient descent (SGD) algorithm with adaptive learning rate (Adadelta) (Zeiler, 2012) is used to train the proposed model. 497 sults for the ranking variant of WMT15 Quality Estimation Shared Task at sentence level. A total of 4 tied official winning systems are indicated by a •. DeltaAvg metric is used as a pr"
N16-1059,W15-3038,0,0.0174432,"t various granularity levels (sentence-level/wordlevel/document-level). Quality estimation is of growing importance in the field of machine translation (MT) since MT systems are widely used and the quality of each machine-translated sentence is able to vary considerably. Previous research on QE, addressed as a regression/classification problem to compute quality scores/categories, has mainly focused on feature extraction and feature selection. Feature extraction is to find the relevant features, such as baseline features (Specia et al., 2013) and latent semantic indexing (LSI) based features (Langlois, 2015), capturing various aspects of quality from source and target sentences1 and external resources. Feature selection is to select the best features by using selection algorithms, such as Gaussian processes (Shah et al., 2015) and heuristic (Gonz´alez-Rubio et al., 2013), among already extracted features. Finding desirable features has played a key role in the QE research. In this paper we present a recurrent neural networks approach for estimating the quality of machine translation output at sentence level, which does not require manual effort for finding the best relevant features. The remainde"
N16-1059,2006.amta-papers.25,0,0.143491,"???????] Figure 1: An illustration of the proposed recurrent neural networks model for quality estimation get words in a target sentence are properly translated from a source sentence. We will refer to this sequence of vectors as quality vectors (qy1 , ... , qyT ). y Each quality vector qyj 2 has the quality information about how well a target word yj in a target sentence y = (y1 , ... , yT y ) is translated from a source sentence3 x = (x1 , ... , xT x ). Quality vectors are generated from the prediction method (of Section 3). To predict a quality estimation score (QE score) as an HTER score (Snover et al., 2006) in [0,1] for each target sentence, a logistic sigmoid function is used such that QE score(y, x) = QE score0 (qy1 , ... , qyT y ) 3 Prediction method using Bi-directional RNN Architecture to Make Quality Vectors In this section, we detail the ways to get the quality vectors (qy1 , ... , qyT ) for computing QE score. y (1) > = σ(WQE s) where s is a summary unit of the whole quality vectors and WQE ∈ Rr . r is the dimensionality of summary unit. To get the summary unit s, the hidden state vj employing p gated hidden units for the target word yj is computed by vj = f (qyj , vj−1 ) . dencies of tr"
N16-1059,2009.eamt-1.5,0,0.0671711,"tion method is used as the input of the final recurrent neural network. The prediction method uses bi-directional recurrent neural network architecture both on source and target sentence to fully utilize the bi-directional quality information from source and target sentence. Our experiments show that the proposed recurrent neural networks approach achieves a performance comparable to the existing stateof-the-art models for estimating the sentencelevel quality of English-to-Spanish translation. 1 Introduction Estimating the quality of machine translation output, called quality estimation (QE) (Specia et al., 2009; Blatz et al., 2004), is to predict quality scores/categories for unseen machinetranslated sentences without reference translations at various granularity levels (sentence-level/wordlevel/document-level). Quality estimation is of growing importance in the field of machine translation (MT) since MT systems are widely used and the quality of each machine-translated sentence is able to vary considerably. Previous research on QE, addressed as a regression/classification problem to compute quality scores/categories, has mainly focused on feature extraction and feature selection. Feature extraction"
N16-1059,P13-4014,0,0.215744,"Missing"
N16-1059,W15-3001,0,\N,Missing
N19-1372,I17-2028,0,0.0658504,"work that contains historic utterance vectors encoded by RNN has been used to select the most relevant history vector by multiplicative softattention (Chen et al., 2016a); the selected vector is fed to an RNN-based slot tagger as context information. A memory network can be regarded as use of content-based similarity between the current utterance and previous utterances. A memory network can be separated to capture historic utterances for each speaker independently (Kim et al., 2017), and a contextual model can use different LSTM layers to separately encode a history summary for each speaker (Chi et al., 2017). For another task, addressee and response selection in multi-party conversations, a distinct RNN-based encoder for each speaker-role (sender, addressee, or observer) has been used to generate distinct history summaries (Zhang et al., 2018). Recent work on contextual SLU has introduced time information of contexts into models because content-based attention may cause a wrong choice that introduce noises. The reciprocal of temporal distance between a current utterance and contexts can be used as a time-decay function, and the function can be decomposed into trainable parameters (Chen et al., 20"
N19-1372,D14-1162,0,0.0916047,"ted attribute(s). The speaker information (guide and tourist) is also provided. Humanhuman dialogues contain rich and complex human behaviors and bring much difficulty to all tasks that are involved in SLU. We used the same training dataset, the same test dataset and the same validation set as in the DSTC 4 competition: 14 dialogues as the training dataset, 6 dialogues as the validation dataset, and 9 dialogues as the test dataset. We used Adam (Kingma and Ba, 2015) as the optimizer in training the model. We set the batch size to 256, and used pretrained 200-dimensional word embeddings GloVe (Pennington et al., 2014). We applied 30 training epochs with early stopping. We set the size dim of every hidden layer to 128, and the context length to 7. We used the ground truth intents (semantic labels) to form an intentdense vector like previous work. To evaluate SLU accuracy, we used the F1 score, which is the harmonic mean of precision and recall. To validate the significance of improvements, we used a onetailed t-test. We ran each model ten times, and report their average scores. As baseline models, we used the state-of-the-art contextual models, and most accurate participant of DSTC 4 (DSTC 4 - Best) (Kim et"
N19-1372,N18-1194,0,0.836626,"much work has used contextual information of dialogues to alleviate the ambiguity of recognition of the given utterance. In SLU, selecting important history information is crucial, and it directly influences the improvement of SLU accuracy. To summarize this history, content-aware models (Chen et al., 2016a; Kim et al., 2017) similar to attention models in machine translation (Bahdanau et al., 2014) have been proposed. However, content-aware models are likely to select the wrong history when the histories are similar in content. To alleviate this problem, time-aware models (Chen et al., 2017; Su et al., 2018a,b) which pay attention to recent previous utterances by using the temporal distance between a previous utterance and a current utterance are being considered; the models are based on mathematical formulas, time-decay functions, which are formulated by human, and decomposed into trainable parameters. However, the previous time-aware models may not be sufficiently accurate. In the models, either a single time-decay function is used or a limited number of time-decay functions are linearly combined; these manual functions may not be sufficiently flexible to learn an optimal time-decay function."
P09-1029,esuli-sebastiani-2006-sentiwordnet,0,0.439034,"t our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier. While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). 3.1 Characteristics of Good Sentiment Terms This section examines the qualities of useful terms for sentiment analysis tasks and corresponding features. For the sake of organization, we categorize the sources of features into either global or local knowledge, and either topic-independent or topic-dependent knowledge. Topic-independently speaking, a good sentiment term is discriminative and prominent, such that the appearance of the term imposes greater influence on the judgment of the analysis system. The rare occurrence of terms in document collections has been regarded as a very important"
P09-1029,C04-1121,0,0.0630001,"NLP also some interesting works on modeling the topic and sentiment of documents in a unified way (Mei et al., 2007; Zhang and Ye, 2008). text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). 3 Term Weighting and Sentiment Analysis In this section, we describe the characteristics of terms that are useful in sentiment analysis, and present our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier. While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on rec"
P09-1029,P97-1023,0,0.0318834,"this section, we describe the characteristics of terms that are useful in sentiment analysis, and present our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier. While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). 3.1 Characteristics of Good Sentiment Terms This section examines the qualities of useful terms for sentiment analysis tasks and corresponding features. For the sake of organization, we categorize the sources of features into either global or local knowledge, and either topic-independent or topic-dependent knowledge. Topic-independently speaking, a good sentiment term is discriminative and prominent, such that the appearance of the term imposes greater influence on the judgment of the analysis s"
P09-1029,kamps-etal-2004-using,0,0.0496124,"Missing"
P09-1029,W04-3239,0,0.0518384,"Missing"
P09-1029,J04-3002,0,0.0226392,"ximated as such measure. Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may exploit different types of characteristics from the topical tasks, that, unlike fact-based text analysis tasks, repetition of terms does not imply a significance on the overall sentiment. On the other hand, Wiebe et al. (2004) have noted that hapax legomena (terms that only appear once in a collection of texts) are good signs for detecting subjectivity. Other works have also exploited rarely occurring terms for sentiment analysis tasks (Dave et al., 2003; Yang et al., 2006). The opinion retrieval task is a relatively recent issue that draws both the attention of IR and NLP communities. Its task is to find relevant documents that also contain sentiments about a given topic. Generally, the opinion retrieval task has been approached as a two–stage task: first, retrieving topically relevant documents, then reranking th"
P09-1029,W04-3253,0,0.295115,"that are relevant and descriptive of the subject should be regarded to be more useful than topically-irrelevant and extraneous terms. One way of measuring this is using associations between the query and terms. Statistical measures of associations between terms include estimations by the co-occurrence in the whole collection, such as Point-wise Mutual Information (PMI) and Latent Semantic Analysis (LSA). Another method is to use proximal information of the query and the word, using syntactic structure such as dependency relations of words that provide the graphical representation of the text (Mullen and Collier, 2004). The minimum spans of words in such graph may represent their associations in the text. Also, the distance between words in the local context or in the thesauruslike dictionaries such as WordNet may be approximated as such measure. Interestingly, there are conflicting conclusions about the usefulness of the statistical features in sentiment analysis tasks (Pang and Lee, 2008). Pang et al. (2002) presents empirical results indicating that using term presence over term frequency is more effective in a data-driven sentiment classification task. Such a finding suggests that sentiment analysis may"
P09-1029,H05-1044,0,0.121977,"uments as below. 3.2.1 Word Sentiment Model Modeling the sentiment of a word has been a popular approach in sentiment analysis. There are many publicly available lexicon resources. The size, format, specificity, and reliability differ in all these lexicons. For example, lexicon sizes range from a few hundred to several hundred thousand. Some lexicons assign real number scores to indicate sentiment orientations and strengths (i.e. probabilities of having positive and negative sentiments) (Esuli and Sebastiani, 2006) while other lexicons assign discrete classes (weak/strong, positive/negative) (Wilson et al., 2005). There are manually compiled lexicons (Stone et al., 1966) while some are created semi-automatically by expanding a set of seed terms (Esuli and Sebastiani, 2006). The goal of this paper is not to create or choose an appropriate sentiment lexicon, but rather it is to discover useful term features other than the sentiment properties. For this reason, one sentiment lexicon, namely SentiWordNet, is utilized throughout the whole experiment. SentiWordNet is an automatically generated sentiment lexicon using a semi-supervised method (Esuli and Sebastiani, 2006). It consists of WordNet synsets, wher"
P09-1029,P06-2079,0,0.108334,"deling the topic and sentiment of documents in a unified way (Mei et al., 2007; Zhang and Ye, 2008). text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statistical features (Pang and Lee, 2008). Pang et al. (2002) employed n-gram and POS features for ML methods to classify movie-review data. Also, syntactic features such as the dependency relationship of words and subtrees have been shown to effectively improve the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). 3 Term Weighting and Sentiment Analysis In this section, we describe the characteristics of terms that are useful in sentiment analysis, and present our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier. While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and"
P09-1029,W03-1017,0,0.0217445,"the performances of sentiment analysis (Kudo and Matsumoto, 2004; Gamon, 2004; Matsumoto et al., 2005; Ng et al., 2006). 3 Term Weighting and Sentiment Analysis In this section, we describe the characteristics of terms that are useful in sentiment analysis, and present our sentiment analysis model as part of an opinion retrieval system and an ML sentiment classifier. While these features are usually employed by data-driven approaches, there are unsupervised approaches for sentiment analysis that make use of a set of terms that are semantically oriented toward expressing subjective statements (Yu and Hatzivassiloglou, 2003). Accordingly, much research has focused on recognizing terms’ semantic orientations and strength, and compiling sentiment lexicons (Hatzivassiloglou and Mckeown, 1997; Turney and Littman, 2003; Kamps et al., 2004; Whitelaw et al., 2005; Esuli and Sebastiani, 2006). 3.1 Characteristics of Good Sentiment Terms This section examines the qualities of useful terms for sentiment analysis tasks and corresponding features. For the sake of organization, we categorize the sources of features into either global or local knowledge, and either topic-independent or topic-dependent knowledge. Topic-independ"
P09-1029,W02-1011,0,0.0412434,"n mining and sentiment analysis involves extracting opinionated pieces of text, determining the polarities and strengths, and extracting holders and targets of the opinions. Much research has focused on creating testbeds for sentiment analysis tasks. Most notable and widely used are Multi-Perspective Question Answering (MPQA) and Movie-review datasets. MPQA is a collection of newspaper articles annotated with opinions and private states at the subsentence level (Wiebe et al., 2003). Movie-review dataset consists of positive and negative reviews from the Internet Movie Database (IMDb) archive (Pang et al., 2002). 2 Related Work Representing text with salient features is an important part of a text processing task, and there exists many works that explore various features for 253 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 253–261, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP also some interesting works on modeling the topic and sentiment of documents in a unified way (Mei et al., 2007; Zhang and Ye, 2008). text analysis systems (Sebastiani, 2002; Forman, 2003). Sentiment analysis task have also been using various lexical, syntactic, and statist"
P10-1061,P07-1123,0,0.692518,"cell phones have excellent battery life. * 삼성(Samsung) 휴대전화(cell phone) 배터리는 (battery) 그럭저럭(somehow or other) 오래간다(last long). Related Work Much research have been put into developing methods for multilingual subjectivity analysis recently. With the high availability of subjectivity resources and tools in English, an easy and straightforward approach would be to employ a machine translation (MT) system to translate input texts in target languages into English then carry out the analyses using an existing subjectivity analysis tool (Kim and Hovy, 2006; Bautin et al., 2008; Banea et al., 2008). Mihalcea et al. (2007) and Banea et al. (2008) proposed a number of approaches exploiting a bilingual dictionary, a parallel corpus, and an MT system to port the resources and systems available in English to languages with limited resources. For subjectivity lexicons translation, Mihalcea et al. (2007) and Wan (2008) used the first sense in a bilingual dictionary, Kim and Hovy (2006) used a parallel corpus and a word alignment tool to extract translation pairs, and Kim et al. (2009) used a dictionary to translate and a link analysis algorithm to refine the matching intensity. To overcome the shortcomings of availab"
P10-1061,W02-1011,0,0.0113247,"Missing"
P10-1061,W03-1014,0,0.170407,"Missing"
P10-1061,D08-1014,0,0.456638,"entiments: * Samsung cell phones have excellent battery life. * 삼성(Samsung) 휴대전화(cell phone) 배터리는 (battery) 그럭저럭(somehow or other) 오래간다(last long). Related Work Much research have been put into developing methods for multilingual subjectivity analysis recently. With the high availability of subjectivity resources and tools in English, an easy and straightforward approach would be to employ a machine translation (MT) system to translate input texts in target languages into English then carry out the analyses using an existing subjectivity analysis tool (Kim and Hovy, 2006; Bautin et al., 2008; Banea et al., 2008). Mihalcea et al. (2007) and Banea et al. (2008) proposed a number of approaches exploiting a bilingual dictionary, a parallel corpus, and an MT system to port the resources and systems available in English to languages with limited resources. For subjectivity lexicons translation, Mihalcea et al. (2007) and Wan (2008) used the first sense in a bilingual dictionary, Kim and Hovy (2006) used a parallel corpus and a word alignment tool to extract translation pairs, and Kim et al. (2009) used a dictionary to translate and a link analysis algorithm to refine the matching intensity. To overcome the"
P10-1061,D08-1058,0,0.0141385,"nglish, an easy and straightforward approach would be to employ a machine translation (MT) system to translate input texts in target languages into English then carry out the analyses using an existing subjectivity analysis tool (Kim and Hovy, 2006; Bautin et al., 2008; Banea et al., 2008). Mihalcea et al. (2007) and Banea et al. (2008) proposed a number of approaches exploiting a bilingual dictionary, a parallel corpus, and an MT system to port the resources and systems available in English to languages with limited resources. For subjectivity lexicons translation, Mihalcea et al. (2007) and Wan (2008) used the first sense in a bilingual dictionary, Kim and Hovy (2006) used a parallel corpus and a word alignment tool to extract translation pairs, and Kim et al. (2009) used a dictionary to translate and a link analysis algorithm to refine the matching intensity. To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. Abbasi et al. (2008) and Boiy and Moens (2009) have created manually annotated gold standards in target lan"
P10-1061,P09-1027,0,0.0278534,"ilingual dictionary, a parallel corpus, and an MT system to port the resources and systems available in English to languages with limited resources. For subjectivity lexicons translation, Mihalcea et al. (2007) and Wan (2008) used the first sense in a bilingual dictionary, Kim and Hovy (2006) used a parallel corpus and a word alignment tool to extract translation pairs, and Kim et al. (2009) used a dictionary to translate and a link analysis algorithm to refine the matching intensity. To overcome the shortcomings of available resources and to take advantage of ensemble systems, Wan (2008) and Wan (2009) explored methods for developing a hybrid system for Chinese using English and Chinese sentiment analyzers. Abbasi et al. (2008) and Boiy and Moens (2009) have created manually annotated gold standards in target languages and studied various feature selection and learning techniques in machine learning approaches to analyze sentiments in multilingual web documents. For learning multilingual subjectivity, the literature tentatively concludes that translating lexicon is less dependable in terms of preserving subjectivity than corpus translation (Mihalcea et al., 2007; Wan, 2008), and though corp"
P10-1061,R09-1010,0,0.0571059,"ns (2009) have created manually annotated gold standards in target languages and studied various feature selection and learning techniques in machine learning approaches to analyze sentiments in multilingual web documents. For learning multilingual subjectivity, the literature tentatively concludes that translating lexicon is less dependable in terms of preserving subjectivity than corpus translation (Mihalcea et al., 2007; Wan, 2008), and though corpus translation results in modest performance degradation, it provides a viable approach because no manual labor is required (Banea et al., 2008; Brooke et al., 2009). Based on the observation that the performances of subjectivity analysis systems in comparable experimental settings for two languages differ, Figure 1: Examples of sentiments in multilingual text Banea et al. (2008) have attributed the variations in the difficulty level of subjectivity learning to the differences in language construction. Bautin et al. (2008)’s system analyzes the sentiment scores of entities in multilingual news and blogs and adjusted the sentiment scores using entity sentiment probabilities of languages. 3 3.1 Multilanguage-Comparability Motivation The quality of a subject"
P10-1061,esuli-sebastiani-2006-sentiwordnet,0,0.0318962,"ects. Also, we developed a multilingual subjectivity evaluation corpus from a parallel text, and studied inter-annotator, inter-language agreements on subjectivity, and observed persistent subjectivity projections from one language to another from a parallel text. For future work, we aim extend this work to constructing a multilingual sentiment analysis system and evaluate it with multilingual datasets such as product reviews collected from different countries. We also plan to resolve the lexiconbased classifiers’ classification bias towards subjective meanings with a list of objective words (Esuli and Sebastiani, 2006) and their multilingual expansion (Kim et al., 2009), and evaluate the multilanguage-comparability of systems constructed with resources from different sources. Discussion Which approach is most suitable for multilingual subjectivity analysis? In our experiments, the corpus-based systems trained on corpora translated from English to the target languages (T-CB) perform well for subjectivity classification and multilanguagecomparability measures on the whole. However, the methods we employed to expand the languages were naively carried out without much considerations for optimization. Further ad"
P10-1061,H05-1044,0,0.114224,"with its relative order of intensity. 4.1 Multilingual Subjectivity System Source Language System We adopt the three systems described below as our source language systems: a state-of-the-art subjectivity classifier, a corpus-based, and a lexiconbased systems. The resources needed for developing the systems or the system itself are readily available for research purposes. In addition, these systems cover the general spectrum of current approaches to subjectivity analysis. State-of-the-art (S-SA): OpinionFinder is a publicly-available NLP tool for subjectivity analysis (Wiebe and Riloff, 2005; Wilson et al., 2005).3 The software and its resources have been widely used in the field of subjectivity analysis, and it has been the de facto standard system against which new systems are validated. We use a highcoverage classifier from the OpinionFinder’s two sentence-level subjectivity classifiers. This Naive Bayes classifier builds upon a corpus annotated by a high-precision classifier with the bootstrapping of the corpus and extraction patterns. The classifier assesses a sentence’s subjectivity with a label and a score for confidence in its judgment. Corpus-based (S-CB): The MPQA opinion corpus is a collect"
P10-1061,N06-1026,0,0.117815,"ts with different strengths of positive sentiments: * Samsung cell phones have excellent battery life. * 삼성(Samsung) 휴대전화(cell phone) 배터리는 (battery) 그럭저럭(somehow or other) 오래간다(last long). Related Work Much research have been put into developing methods for multilingual subjectivity analysis recently. With the high availability of subjectivity resources and tools in English, an easy and straightforward approach would be to employ a machine translation (MT) system to translate input texts in target languages into English then carry out the analyses using an existing subjectivity analysis tool (Kim and Hovy, 2006; Bautin et al., 2008; Banea et al., 2008). Mihalcea et al. (2007) and Banea et al. (2008) proposed a number of approaches exploiting a bilingual dictionary, a parallel corpus, and an MT system to port the resources and systems available in English to languages with limited resources. For subjectivity lexicons translation, Mihalcea et al. (2007) and Wan (2008) used the first sense in a bilingual dictionary, Kim and Hovy (2006) used a parallel corpus and a word alignment tool to extract translation pairs, and Kim et al. (2009) used a dictionary to translate and a link analysis algorithm to refi"
P98-1111,J97-4001,0,\N,Missing
P98-2125,J93-1007,0,\N,Missing
U03-1004,W00-1411,0,0.0610277,"Missing"
U03-1005,P98-1083,0,0.0604914,"Missing"
U03-1005,C00-2156,0,0.0516071,"Missing"
U03-1005,C90-3088,0,0.0597652,"rb. The major problem with clause identification concerns the sharing of the same subject by different clauses (Vilson 1998). When a subject is omitted in a clause, Vilson(1998) attached the features of the previous subject to the conjunctions. However, the subject of a clause is not always the nearest subject. Therefore, a new method is required to detect the correct subject of a clause. In addition, many studies have focused on segmentation in long sentences. Some try to segment a long sentence using patterns and rules and to analyze each segment independently (Doi 1993; Kim 1995; Kim 2002 ;Li 1990). Similarly, an intrasentence segmentation method using machine learning is proposed (Kim 2001). Although this method reduces the complexity of syntactic analysis by segmenting a long sentence, the ambiguity problem with the dependency of subjects remains unsolved. Further, Lyon and Dickerson take advantage of the fact that declarative sentences can almost always be segmented into three concatenated sections (pre-subject, subject, predicate) which can reduce the complexity of parsing English sentences (Lyon and Dickerson 1995; Lyon and Dickerson 1997). This approach is useful for a simple sent"
U03-1005,E95-1030,0,0.0329043,"and rules and to analyze each segment independently (Doi 1993; Kim 1995; Kim 2002 ;Li 1990). Similarly, an intrasentence segmentation method using machine learning is proposed (Kim 2001). Although this method reduces the complexity of syntactic analysis by segmenting a long sentence, the ambiguity problem with the dependency of subjects remains unsolved. Further, Lyon and Dickerson take advantage of the fact that declarative sentences can almost always be segmented into three concatenated sections (pre-subject, subject, predicate) which can reduce the complexity of parsing English sentences (Lyon and Dickerson 1995; Lyon and Dickerson 1997). This approach is useful for a simple sentence that contains a subject and a predicate. A long sentence generally contains more than a subject and a predicate. Therefore, the segmentation methods proposed by Lyon and Dickerson are inefficient for parsing long sentences. In studies on segmenting long sentences, little attention has been paid to detecting the boundaries of predicates which share a common subject. To determine the correct subject of some subjectless VPs, we define the ‘S-clause’ and propose an S-clause segmentation method. In previous work, a clause is"
U03-1005,P95-1037,0,0.0426904,"Missing"
U03-1005,C00-2116,0,0.0273122,"Missing"
U03-1005,J97-2002,0,\N,Missing
U03-1005,1997.iwpt-1.29,0,\N,Missing
U03-1005,W01-0708,0,\N,Missing
U03-1005,J94-4001,0,\N,Missing
U03-1005,A88-1030,0,\N,Missing
U03-1005,C98-1080,0,\N,Missing
U03-1005,P92-1003,0,\N,Missing
U03-1005,W98-1125,0,\N,Missing
U03-1013,J93-1007,0,\N,Missing
U03-1013,C92-2070,0,\N,Missing
U03-1013,J92-1001,0,\N,Missing
U03-1016,W98-0606,0,0.0477195,"(Allen 1995). In order to improve domain portability, many researchers have concentrated on isolating translation knowledge from linguistic knowledge through loosely-coupled approaches. These approaches can be further classified according to the extent that question analysis is performed. Syntax-oriented systems (Ballard et al., 1984; Damerau 1985; Lee and Park 2002) analyze questions up to a syntactic level, after which translation knowledge is applied to generate a database query. Logical form systems (Warren and Pereira 1982; Grosz et al., 1987; Alshawi et al., 1992; Androutsopoulos 1993; Klein et al., 1998) interpret a user question into a domainindependent literal meaning level. Thus, in loosely-coupled approaches, transporting to a new database domain does not need to change linguistic knowledge at all, only tailoring translation knowledge to new domains. Even in this case, however, translation knowledge is difficult to describe. For example, syntax-oriented systems have to devise conversion rules that transform parse trees into database query expressions (Androutsopoulos et al. 1995), and logical form systems should define database relations for logical predicates. In addition, creating these"
U03-1016,J82-3002,0,0.106315,"ting to other domains, new semantic grammars should be created with a considerable effort (Allen 1995). In order to improve domain portability, many researchers have concentrated on isolating translation knowledge from linguistic knowledge through loosely-coupled approaches. These approaches can be further classified according to the extent that question analysis is performed. Syntax-oriented systems (Ballard et al., 1984; Damerau 1985; Lee and Park 2002) analyze questions up to a syntactic level, after which translation knowledge is applied to generate a database query. Logical form systems (Warren and Pereira 1982; Grosz et al., 1987; Alshawi et al., 1992; Androutsopoulos 1993; Klein et al., 1998) interpret a user question into a domainindependent literal meaning level. Thus, in loosely-coupled approaches, transporting to a new database domain does not need to change linguistic knowledge at all, only tailoring translation knowledge to new domains. Even in this case, however, translation knowledge is difficult to describe. For example, syntax-oriented systems have to devise conversion rules that transform parse trees into database query expressions (Androutsopoulos et al. 1995), and logical form systems"
U03-1016,A83-1002,0,\N,Missing
W01-1006,P89-1010,0,0.0382273,"ion Instances &lt;SC, SR, DC, frequency> which has eight hundred and ten thousand sentences. During this process, more specific semantic relation instances are obtained when compared with previous instances obtained in Section 3. Since such specific instances reflect the context of a practical situation, they are also imported into the LIP ontology. Table 3 shows the final number of semantic relations inserted into the LIP ontology. Table 3. Final relation instances in the LIP ontology Semi-Automatic Relation Mapping Figure 7. Construction flow of ontology training data between random variables (Church & Hanks, 1989). Resnik (1995) suggested a measure of semantic similarity in an IS-A taxonomy, based on the notion of information content. However, his method differs from ours in that we consider all semantic relations in the ontology, not taxonomy relations only. To implement this idea, we bind source concepts (SC) and semantic relations (SR) into one entity, since SR is mainly influenced by SC, not the destination concepts (DC). Therefore, if two entities, &lt; SC, SR>, and DC have probabilities P(&lt;SC, SR>) and P(DC), then their mutual information I(&lt;SC, SR>, DC) is defined as:   P (&lt; SC , SR >, DC ) I (&lt;"
W01-1006,C00-1079,1,0.83385,"Missing"
W01-1006,W97-0803,0,0.0470387,"Missing"
W01-1006,C96-2161,0,0.0260184,"struction method, which takes full advantage of already existing knowledge resources and practical usages in large corpora. First, we define our ontology representation language (ORL) by modifying the most suitable among previously developed ORLs, and then design a language-independent and practical (LIP) ontology structure based on the defined ORL. Afterwards, we construct a practical ontology by the semi-automatic construction method given below. We extend the existing Kadokawa thesaurus (Ohno & Hamanishi, 1981) by inserting additional semantic relations into the hierarchy of the thesaurus. Uramoto (1996) and Tokunaga (1997) propose thesaurus extension methods for positioning unknown words in an existing thesaurus. Our approach differs in that the objects inserted are not words but semantic relations. Additional semantic relations can be classified as case relations and other semantic relations. The former can be obtained by converting the established valency information in bilingual dictionaries of COBALT-J/K (Collocation-Based Language Translator from Japanese to Korean) and COBALT-K/J (Collocation-Based Language Translator from Korean to Japanese) (Moon & Lee, 2000) MT systems, as well as f"
W02-1606,H93-1051,0,0.0298131,"biguation has revealed that several different types of information can contribute to the resolution of lexical ambiguity. These include surrounding words (an unordered set of words surrounding a target word), local collocations (a short sequence of words near a target word, taking word order into account), syntactic relations (selectional restrictions), parts of speech, morphological forms, etc (McRoy, 1992, Ng and Zelle, 1997). Some researchers use neural networks in their word sense disambiguation systems Because of its strong capability in classification (Waltz et al., 1985, Gallant, 1991, Leacock et al., 1993, and Mooney, 1996). Since, however, most such methods require a few thousands of features or large amounts of hand-written data for training, it is not clear that the same neural network models will be applicable to real world applications. We propose a word sense disambiguation method that combines both the neural net-based approach and the work of Li et al (2000), especially focusing on the practicality of the method for application to real world MT systems. To reduce the number of input features of neural networks to a practical size, we use concept codes of a thesaurus as features. In thi"
W02-1606,W96-0208,0,\N,Missing
W02-1606,J93-1007,0,\N,Missing
W02-1606,J92-1001,0,\N,Missing
W04-1101,J97-2002,0,0.0349563,"ch achievement to industry is the characteristic of this development district. (10) 学生们来到了操场，高高兴兴地。 The students happily come to the playground. 2 2.1 Related Work Related Work for Clause Segmentation Syntactic ambiguity problems increase drastically as the input sentence becomes longer. Long sentence segmentation is a way to avoid the problem. Many studies have been made on clause segmentation (Carreras and Marquez, 2002, Leffa, 1998, Sang and Dejean,2001). In addition, many studies also have been done on long sentences segmentation by certain patterns (Kim and Zhang, 2001, Li and Pei, 1990, Palmer and Hearst, 1997). However, some researchers merely ignore punctuation, including the comma, and some researchers use a comma as one feature to detect the segmentation point, not fully using the information from the comma. 2.2 Related Work for Punctuation Processing Several researchers have provided descriptive treatment of the role of punctuations: Jones (1996b) determined the syntactic function of the punctuation mark. Bayraktar and Akman (1998) classified commas by means of the syntax-patterns in which they occur. However, theoretical forays into the syntactic roles of punctuation were limited. Many researc"
W04-1101,W01-0708,0,\N,Missing
W04-1101,C94-1069,0,\N,Missing
W04-1101,C90-3088,0,\N,Missing
W04-1101,P96-1052,0,\N,Missing
W04-1101,P03-1056,0,\N,Missing
W04-1809,J96-1001,0,0.457745,"Missing"
W09-0433,P07-1017,0,0.0591343,"Missing"
W09-0433,C04-1073,0,0.140308,"us forms and in different stages of translation processes. During preprocessing source language sentences undergo reordering and morpho-syntactic reconstruction phases to generate more target language-like sentences. Also, fixing erroneous words, generating complex morphology, and reranking translation results in post-processing phases may utilize syntactic information of both source and target languages. A syntax-based SMT system encodes the syntactic information in its translation model of the decoding step. A number of researchers have proposed syntactic reordering as a preprocessing step (Xia and McCord, 2004; Collins et al., 2005; Wang et al., 2007). In these syntactic reordering approaches, source sentences are first parsed and a series of reordering rules are applied to the parsed trees to reorder the source sentences into target languagelike word orders. Such an approach is an effective method for a phrase-based SMT system that employs a relatively simple distortion model in the decoding phase. This paper concentrates upon reordering source sentences in the preprocessing step of a Chinese-to-Korean phrase-based SMT system using syntactic information. Chinese-to-Korean SMT has more difficulties"
W09-0433,li-etal-2008-annotation,1,0.802682,"VV0 PU1 IP2 IP2 PU1 VV0 Reordered parse tree: VP NP (NN 需要) PP (P 按) PP (P 对) NP (PN 它们) VP (VV 进行) NP (NN 配置) Original parse tree: VP PP (P 按) NP (NN 需要) PP (P 对) 3 VV: common verb; AS: aspect marker; P: preposition; PU: punctuation; PN: pronoun; 192 3.3 Modality-bearing words Reordered parse tree: VP Verb affixes in Korean verbal phrases indicate modality information such as tense, aspect, mood, negation, and voice. The corresponding modality information is implicitly or explicitly expressed in Chinese. It is important to figure out what features are used to represent modality information. Li et al. (2008) describes in detail the features in Chinese that express modality information. However, since only lexical features can be reordered, we consider explicit modality features only. Modality-bearing words are scattered over an entire sentence. We move them near their verbal heads because their correspondences in Korean sentences are always placed right after their verbs. When constructing reordering rules, we consider temporal adverbs, auxiliary verbs, negation particles, and aspect particles only. The following example sentences show the results of a few of our reordering rules for modality-bea"
W09-0433,P08-1059,0,0.0408355,"Missing"
W09-0433,P05-1066,0,0.468444,"Missing"
W09-0433,P03-1021,0,0.00757524,"P (VV 运行) Reordered parse tree: VP PP (P 以) NP (NN 管理员) (NN 身份) ADVP (AD 不) VP (VV 应该) VP (VV 运行) Generally speaking, Chinese does not have grammatical forms for voice. Although, voice is also a grammatical category expressing modality information, we have left it out of the current phase of our experiment since voice detection is another research issue and reordering rules for voice are unavoidably complicated. 4 Experiment Our baseline system is a popular phrase-based SMT system, Moses (Koehn et al., 2007), with 5-gram SRILM language model (Stolcke, 2002), tuned with Minimum Error Training (Och, 2003). We adopt NIST (NIST, 2002) and BLEU (Papineni et al., 2001) as our evaluation metrics. Chinese sentences in training and test corpora are first parsed and are applied a series of syntactic reordering rules. To evaluate the contribution of the three categories of syntactic reordering rules, we perform the experiments applying each category independently. Experiments of various combinations are also carried out. 4.1 Corpus profile We automatically collected and constructed a sentence-aligned parallel corpus from the online 193 Dong-A newspaper 4 . Strictly speaking, it is a non-literally trans"
W09-0433,P07-2045,0,0.00588355,"rse tree: VP ADVP (AD 不)  Negation particle VP (VV 应该)  Auxiliary verb VP PP (P 以) NP (NN 管理员) (NN 身份) VP (VV 运行) Reordered parse tree: VP PP (P 以) NP (NN 管理员) (NN 身份) ADVP (AD 不) VP (VV 应该) VP (VV 运行) Generally speaking, Chinese does not have grammatical forms for voice. Although, voice is also a grammatical category expressing modality information, we have left it out of the current phase of our experiment since voice detection is another research issue and reordering rules for voice are unavoidably complicated. 4 Experiment Our baseline system is a popular phrase-based SMT system, Moses (Koehn et al., 2007), with 5-gram SRILM language model (Stolcke, 2002), tuned with Minimum Error Training (Och, 2003). We adopt NIST (NIST, 2002) and BLEU (Papineni et al., 2001) as our evaluation metrics. Chinese sentences in training and test corpora are first parsed and are applied a series of syntactic reordering rules. To evaluate the contribution of the three categories of syntactic reordering rules, we perform the experiments applying each category independently. Experiments of various combinations are also carried out. 4.1 Corpus profile We automatically collected and constructed a sentence-aligned parall"
W09-0433,P03-1056,0,\N,Missing
W09-0433,vilar-etal-2006-error,0,\N,Missing
W09-0433,I05-3027,0,\N,Missing
W11-1005,P10-1146,0,0.0143289,"ax-based SMT overcomes the limitations of PBSMT because it finds discontinuous patterns along with the hier42 Figure 2: The maximum branching factor (BF) and depth factor (DF) in a dependency tree in our corpus archical structure. For example, the two discontinuous source words have a head-dependent relation (Figure 3). Especially with the dependency tree, we can easily identify patterns that have non-projectivity (Na et al., 2010). However, syntax-based patterns such as constituents or treelets do not sufficiently cover various useful patterns, even if we have the correct syntactic analysis (Chiang, 2010). For this reason, many researchers have proposed supplementary patterns such as an intra/inter constituent or sequence of treelets (Galley et al., 2006; Shen et al., 2008). Unlike PSG, DG does not include nonterminal symbols, which represent constituent information. This makes DG simpler than PSG. For instance, it directly associates syntatic role with the structure, but introduces a difficulty in syntax-based SMT. The branching factor of a dependency tree becomes larger when a head word dominates many dependents. We observe that the maximum branching factor of an automatically parsed depende"
W11-1005,P05-1067,0,0.0188874,"consistent. L • A horizontal vertex is a sequence of modifiers for a common head word, and • A vertical vertex is a path from a word to one of the ancestors, and head(e) {3} {8} {3, 8} {3, 8} {4, 5} {5} {6, 7} {4} {5} {4, 5} {5, 6} {3, 8} tails(e) {8} : x1 {4} : x1 , {5} : x2 {4, 5} : x1 {4} : x1 , {5} : x2 {6, 7} : x1 {6, 7} : x1 N/A rhs(γ) x1 when x1 x2 when x1 when x1 x2 I’m in x1 in x1 the States I’m in I’m in in the State When • A combination of the horizontal vertices and the vertical vertices, and the total time complexity also becomes linear to the length of the sentence n similar to Ding and Palmer (2005), i.e. O(|V |k 2 |τ (v)|3 ), where |V |= O(na2(a−1) ) and a = min(m, b, d). • A combination of the vertical vertices and the vertical vertices. 4 The computational complexity of the initializaion directly affects the complexity of the entire procedure. For each word, generating the horizontal vertices takes O(b2 ), and the vertical vertices take O(bd−1 ), where b is the maximum branching factor and d is the maximum depth of a dependency tree. The two combinations take O(bd+1 ) and O(b2(d−1) ) time to initialize the vertices. However, it takes O(mm+1 ) and O(m2(m−1) ) if we restrict the maximum"
W11-1005,J94-4004,0,0.147959,"Missing"
W11-1005,P08-1115,0,0.0161748,"of the extracted patterns for syntax-based SMT in section 4. We also report empirical results related to the usefulness of the extracted pattern in section 5. The experimental results show that the MWU-forest representation gives more applicable translation patterns than the original word-based tree. 2 Related Work Previous studies have proposed merging alternative analyses to deal with analysis errors for two reasons: 1) the strongest alternative is not necessarily the correct analysis, and 2) most alternatives contain similar elements such as common sub-trees. For segmentation alternatives, Dyer et al. (2008) proposed a word lattice that represents exponentially large numbers of segmentations of a source sentence, and integrates reordering information into the lattice as 43 well. For parsing alternatives, Mi et al. (2008) suggested a packed forest that encodes alternative PSG derivations. Futher, Mi et al. (2010) combined the two approaches in order to benefit from both. The translation literature also shows that translation requires non-isomorphic transformation from the source to the target. This yields translation divergences such as head-switching (Dorr, 1994). Ding and Palmer (2005) reported"
W11-1005,P03-2041,0,0.115707,"Missing"
W11-1005,N04-1035,0,0.049429,"e words in a vertex to a constant m. Ding and Palmer (2005) insisted that the Viterbi decoding of an MWU-forest takes linear time. In our case, we enumerate the k-best incoming hyperedeges instead of the best one. Because each enumeration takes O(k 2 |τ (v)|3 ), 46 MWU-Forest-to-String Translation Rule Extraction As an application of our proposed MWU-forest, we extract translation rules for syntax-based SMT. Forest-based translation rule extraction has been suggested by Mi and Huang (2008) although their forest compacts the k-best PSG trees. The extraction procedure is essentially the same as Galley et al. (2004), which identifies the cutting points (frontiers) and extracts the sub-structures from a root to frontiers. The situation changes in DG because DG does not have intermediate representation. At the dependency structure, a node corresponds to two kinds of target spans. We borrow the definitions of the aligned span (aspan), and the covered span (cspan) from Na et al. (2010), i.e. • aspan(v) = [min(av ), max(av )], and • cspan(v) S = aspan(v) d∈tails(e) cspan(d) e∈IN (v) , where av = {i|j ∈ v ∧ (i, j) ∈ A}. Figure 5 shows aspans and cspans of a sub-forest of of the MWU-forest in the previous examp"
W11-1005,P06-1121,0,0.0131712,"or (BF) and depth factor (DF) in a dependency tree in our corpus archical structure. For example, the two discontinuous source words have a head-dependent relation (Figure 3). Especially with the dependency tree, we can easily identify patterns that have non-projectivity (Na et al., 2010). However, syntax-based patterns such as constituents or treelets do not sufficiently cover various useful patterns, even if we have the correct syntactic analysis (Chiang, 2010). For this reason, many researchers have proposed supplementary patterns such as an intra/inter constituent or sequence of treelets (Galley et al., 2006; Shen et al., 2008). Unlike PSG, DG does not include nonterminal symbols, which represent constituent information. This makes DG simpler than PSG. For instance, it directly associates syntatic role with the structure, but introduces a difficulty in syntax-based SMT. The branching factor of a dependency tree becomes larger when a head word dominates many dependents. We observe that the maximum branching factor of an automatically parsed dependency tree ranges widely, while most trees have depth under a certain degree (Figure 2). This indicates that we have a horizontally flat dependency tree s"
W11-1005,D08-1022,0,0.0926723,"b2(d−1) ) time to initialize the vertices. However, it takes O(mm+1 ) and O(m2(m−1) ) if we restrict the maximum number of the words in a vertex to a constant m. Ding and Palmer (2005) insisted that the Viterbi decoding of an MWU-forest takes linear time. In our case, we enumerate the k-best incoming hyperedeges instead of the best one. Because each enumeration takes O(k 2 |τ (v)|3 ), 46 MWU-Forest-to-String Translation Rule Extraction As an application of our proposed MWU-forest, we extract translation rules for syntax-based SMT. Forest-based translation rule extraction has been suggested by Mi and Huang (2008) although their forest compacts the k-best PSG trees. The extraction procedure is essentially the same as Galley et al. (2004), which identifies the cutting points (frontiers) and extracts the sub-structures from a root to frontiers. The situation changes in DG because DG does not have intermediate representation. At the dependency structure, a node corresponds to two kinds of target spans. We borrow the definitions of the aligned span (aspan), and the covered span (cspan) from Na et al. (2010), i.e. • aspan(v) = [min(av ), max(av )], and • cspan(v) S = aspan(v) d∈tails(e) cspan(d) e∈IN (v) ,"
W11-1005,P08-1023,0,0.022543,"r. At the second step, we compute the outside probability 45 Algorithm 1 Build Forest 1: Initialize V 2: for v ∈ V in bottom-up order do 3: Create a chart C = |τ (v)|2 4: for chart span [p, q] do 5: Initialize C[p, q] if ∃v s.t. [p, q] = v or σ(v) 6: Combine C[p, i] and C[i + 1, q] 7: end for 8: Set IN (v) to the k-best in C[T OP ] 9: Set β(v) as in Eq. 1 10: end for 11: for v ∈ V in top-down order do 12: Set α(v) as in Eq. 2 13: end for 14: Prune out e if p(e) ≤ δ 15: return v0 (line 12) for each vertex in a top-down manner. Finally we prune out less probable hyperedges (line 14) similar to (Mi et al., 2008). The inside and outside probabilities are defined as follows: β(v) = X Y p(e) e∈IN (v) β(d) (1) d∈tails(e) where β(v) = 1.0 if IN (v) = ∅, and α(v) = X h∈OU T (v) e∈IN (head(h)) · α(head(e))p(e) |OU T (v)| Y β(d) (2) d∈tails(e){v} where α(v) = 1.0 if OU T (v) = ∅. In practice, we restrict the number of words in a vertex in the initialization (line 1). We approximate all possible alternative MWUs that include each word as follows: Table 2: The extracted rules in Figure 5. N denotes the non-lexicalized rules with variables xi for each v ∈ tails(e), and L denotes the lexicalized rule. N Figure"
W11-1005,C10-2096,0,0.0312399,"Missing"
W11-1005,2010.amta-srw.2,1,0.897297,"o a set of discontinuous words in the target language, or vice versa (Table 1). Discontinuous translation patterns frequently appear in many languages (Søgaard and Kuhn, 2009). Syntax-based SMT overcomes the limitations of PBSMT because it finds discontinuous patterns along with the hier42 Figure 2: The maximum branching factor (BF) and depth factor (DF) in a dependency tree in our corpus archical structure. For example, the two discontinuous source words have a head-dependent relation (Figure 3). Especially with the dependency tree, we can easily identify patterns that have non-projectivity (Na et al., 2010). However, syntax-based patterns such as constituents or treelets do not sufficiently cover various useful patterns, even if we have the correct syntactic analysis (Chiang, 2010). For this reason, many researchers have proposed supplementary patterns such as an intra/inter constituent or sequence of treelets (Galley et al., 2006; Shen et al., 2008). Unlike PSG, DG does not include nonterminal symbols, which represent constituent information. This makes DG simpler than PSG. For instance, it directly associates syntatic role with the structure, but introduces a difficulty in syntax-based SMT. Th"
W11-1005,2009.iwslt-evaluation.1,0,0.0137416,"stent word alignments, i.e. span 6= ∅ ∧ ∀i ∈ span, {j|(i, j) ∈ A ∩ (i′ , j) ∈ A s.t. i′ 6∈ span} = ∅. Algorithm 2 shows the pseudo code of the extraction. Because a vertex has aspan and csapn, we extract a lexicalized rule (line 3-5) and/or non-lexicalized rules (line 6-12) for each vertex. 5 Experiment We used the training corpus provided for the DIALOG Task in IWSLT10 between Chinese and English . The corpus is a collection of 30,033 sentence pairs and consists of dialogs in travel situations (10,061) and parts of the BTEC corpus (19,972). Details about the provided corpus are described in (Paul, 2009). We used the Stanford Parser 1 to obtain word-level dependency structures of Chinese sentences, and GIZA++ 2 to obtain word alignments of the biligual corpus. We extracted the SCFG-MWU from the biligual corpus with word alignment. In order to investigate the coverage of the extracted rule, we counted the number of the recovered sentences, i.e. counted if the extracted rule for each sentence pair generates the target sentence by combining the extracted rules. As we collected many alternatives in an MWU-forest, we wanted to determine the importance of each source fragment. Mi and Huang (2008) p"
W11-1005,P08-1066,0,0.160991,"tor (DF) in a dependency tree in our corpus archical structure. For example, the two discontinuous source words have a head-dependent relation (Figure 3). Especially with the dependency tree, we can easily identify patterns that have non-projectivity (Na et al., 2010). However, syntax-based patterns such as constituents or treelets do not sufficiently cover various useful patterns, even if we have the correct syntactic analysis (Chiang, 2010). For this reason, many researchers have proposed supplementary patterns such as an intra/inter constituent or sequence of treelets (Galley et al., 2006; Shen et al., 2008). Unlike PSG, DG does not include nonterminal symbols, which represent constituent information. This makes DG simpler than PSG. For instance, it directly associates syntatic role with the structure, but introduces a difficulty in syntax-based SMT. The branching factor of a dependency tree becomes larger when a head word dominates many dependents. We observe that the maximum branching factor of an automatically parsed dependency tree ranges widely, while most trees have depth under a certain degree (Figure 2). This indicates that we have a horizontally flat dependency tree structure. The transl"
W11-1005,W09-2303,0,0.0144835,"sentence. The second advantage of using syntax in TM is that syntax guides us to discontinuous translation patterns. Because PBSMT regards only a continuous sequence of words as a translation pattern, it often fails to utilize many useful discontinuous translation patterns. For example, two discontinuous source words correspond to a target word in Figure 1. In our inspection of the training corpus, a continuous word sequence often corresponds to a set of discontinuous words in the target language, or vice versa (Table 1). Discontinuous translation patterns frequently appear in many languages (Søgaard and Kuhn, 2009). Syntax-based SMT overcomes the limitations of PBSMT because it finds discontinuous patterns along with the hier42 Figure 2: The maximum branching factor (BF) and depth factor (DF) in a dependency tree in our corpus archical structure. For example, the two discontinuous source words have a head-dependent relation (Figure 3). Especially with the dependency tree, we can easily identify patterns that have non-projectivity (Na et al., 2010). However, syntax-based patterns such as constituents or treelets do not sufficiently cover various useful patterns, even if we have the correct syntactic anal"
W11-1005,C10-1123,0,0.0797255,"ither a treelet (a connected sub-graph), or a sequence of treelets under a common head. In other words, each vertex in an MWU-forest is either “fixed on head” or “floating with children”. The formal definitions can be found in (Shen et al., 2008). We propose encoding multiple dependency structures based on MWUs into a hypergraph. A hypergraph is a compact representation of exponetially many variations in a polynominal space. Unlike PSG, DG does not have non-terminals that represent the linguistically motivated, intermediate structure such as noun phrases and verb phrases. For this simplicity, Tu et al. (2010) proposed a dependency forest as a hypergraph, regarding a word as a vertex with a span that ranges for all its descendants. The dependency forest offers tolerence of parsing errors. Our representation is different from the dependency forest of Tu et al. (2010) since a vertex corresponds to multiple words as well as words. Note that our representation is also capable of incorporating multiple parse trees. Therefore, MWU-forests will also be tolerant of the parsing error if we provide multiple parse trees. In this work, we concentrate on the effectiveness of MWUs, and hence utilize the best dep"
W11-1005,D09-1159,0,0.0387561,"nder a certain degree (Figure 2). This indicates that we have a horizontally flat dependency tree structure. The translation patterns extracted from the flat dependency tree are also likely to be flat. Unfortunately, the flat patterns are less applicable at the decoding stage. When one of the modifiers does not match, for instance, we fail to apply the translation pattern. Therefore, we need a more generally applicable representation for syntax-based SMT using DG. We propose a novel representation of DG that regards a set of words as a unit of the dependency relations, similar to (Ding, 2006; Wu et al., 2009; Na et al., 2010). Unlike their work, we consider many alternatives without predefined units, and construct a packed forest of the multi-word units (MWUs) from a dependency tree. For brevity, we denote the forest based on MWUs as an MWU-forest. Because all possible alternatives are exponentially many, we give an efficient algorithm that enumerates the k-best alternatives in section 3. As an application, we extract translation patterns in form of a source MWU-forest to the target string in order to broaden the coverage of the extracted patterns for syntax-based SMT in section 4. We also report"
W11-2926,H05-1066,0,0.249873,"Missing"
W11-2926,W06-2920,0,0.0816322,"nnotation need corresponding changes for parsing algorithms. Since the same linguistic structures are presented differently with different annotation strategies. In section 3.1, we discuss for such changes required for the case of coordinate structures. Nivre, 2007). Considering the frequent use of coordinate structures among sentences, for hight performance parsing, it is crucial to select a parsing algorithm that can parse such structures well. 4 Experiments We conduct our experiments on two data sets, the Chinese corpus of CoNLL’07 (Nivre et al., 2007a), and the Slovene corpus of CoNLL’06 (Buchholz and Marsi, 2006) shared task for multilingual dependency parsing track. Both corpora are of the CCH-type. Table 2 lists the performance of two systems, i.e., the 2nd -order system of (McDonald et al., 2006) and the proposed system with Alg. 6 (Table 1) as the parse tree decoder. As the discussions given in the previous section, the 2nd -order gives low performance for coordinate structures compared to the overall parsing results. The proposed system gives better coordinate disambiguation by modeling the relation between dependents located on different-sides of the head. 3.1 Diversity for Denpendency Annotatio"
W11-2926,D07-1101,0,0.291225,"f Computer Science and Engineering Pohang University of Science and Technology (POSTECH) San 31 Hyoja Dong, Pohang, 790-784, Republic of Korea meixunj,leona,jhlee@postech.ac.kr Abstract in (McDonald et al., 2006) in contrast to the algorithm used in (McDonald et al., 2005). Parsing histories are partial results generated from previous parsing steps. In a chart parser, these histories can be used in subsequent parsing steps. Previous works have shown that the use of parsing histories helps to resolve syntactic ambiguities (Yamada and Mastumoto, 2003; Nivre et al., 2007b; McDonald et al., 2006; Carreras, 2007; Chen et al., 2010). Obviously, using more histories provides better parsing disambiguation. However, there is a trade-off between using more histories and parsing efficiently. One option is to incorporate only important histories. The selection of different histories requires changes to the parsing algorithm. Another reason for the careful selection of parsing algorithms is from the diverse dependency annotation strategies. The dependency annotations for the same linguistic structures, i.e., coordinate structures can vary (Section 3.1). Additionally, in our opinion, some linguistic or corpus"
W11-2926,W06-2932,0,0.39469,"-Hyeok Lee Department of Computer Science and Engineering Pohang University of Science and Technology (POSTECH) San 31 Hyoja Dong, Pohang, 790-784, Republic of Korea meixunj,leona,jhlee@postech.ac.kr Abstract in (McDonald et al., 2006) in contrast to the algorithm used in (McDonald et al., 2005). Parsing histories are partial results generated from previous parsing steps. In a chart parser, these histories can be used in subsequent parsing steps. Previous works have shown that the use of parsing histories helps to resolve syntactic ambiguities (Yamada and Mastumoto, 2003; Nivre et al., 2007b; McDonald et al., 2006; Carreras, 2007; Chen et al., 2010). Obviously, using more histories provides better parsing disambiguation. However, there is a trade-off between using more histories and parsing efficiently. One option is to incorporate only important histories. The selection of different histories requires changes to the parsing algorithm. Another reason for the careful selection of parsing algorithms is from the diverse dependency annotation strategies. The dependency annotations for the same linguistic structures, i.e., coordinate structures can vary (Section 3.1). Additionally, in our opinion, some ling"
W11-2926,C10-2015,0,0.65359,"ce and Engineering Pohang University of Science and Technology (POSTECH) San 31 Hyoja Dong, Pohang, 790-784, Republic of Korea meixunj,leona,jhlee@postech.ac.kr Abstract in (McDonald et al., 2006) in contrast to the algorithm used in (McDonald et al., 2005). Parsing histories are partial results generated from previous parsing steps. In a chart parser, these histories can be used in subsequent parsing steps. Previous works have shown that the use of parsing histories helps to resolve syntactic ambiguities (Yamada and Mastumoto, 2003; Nivre et al., 2007b; McDonald et al., 2006; Carreras, 2007; Chen et al., 2010). Obviously, using more histories provides better parsing disambiguation. However, there is a trade-off between using more histories and parsing efficiently. One option is to incorporate only important histories. The selection of different histories requires changes to the parsing algorithm. Another reason for the careful selection of parsing algorithms is from the diverse dependency annotation strategies. The dependency annotations for the same linguistic structures, i.e., coordinate structures can vary (Section 3.1). Additionally, in our opinion, some linguistic or corpus-oriented characters"
W11-2926,P99-1059,0,0.777731,"rmation related to the head word is stored in corresponding cell of the chart. In a modified CYK algorithm(Younger, 1967), the cell reserves the possibility for each word of the substring to be the head. Thus, n kinds of sub-dependency-trees are reserved for processing a substring of length n. The space complexity for a cell is O(n), and the overall space complexity is O(n3 ) and time complexity is O(n5 ) for parsing of a sentence of length n. For a detailed description, refer to (Nivre, 2006). A span is a half-constituent that is formed by splitting a constituent at the position of its head (Eisner and Satta, 1999). The span is characterized by the head being located either on the left or right edge. In the span-based dependency chart parser proposed by (Eisner and Satta, 1999), there are two kinds of subtrees reserved in each cell of the chart, i.e., the head is the left-most word or the right-most word. Given this condition, Eisner’s algorithm can parse with a time complexity of O(n3 ) and a space complexity of O(n2 ). In bottom-up parsing, either a constituent-based or a span-based algorithm derives parse tree for a sequence by combining two (or more) subsequences with a new dependency arc. These sub"
W11-2926,C96-1058,0,0.349567,"of such a two-step operation is that it reduces the time complexity O(n5 ) of Alg. 1 to O(n4 ). In Algs. 3 and 4, one of the constituents is divided into two parts (spans). The first step combines the constituent with the closer span and makes a decision about the new dependency arc. The other span is attached to the result of the first step. The available parsing histories is ( , ) for Alg.3, and ( , ) for Alg.4. The two-step processing requires the reservation of the partial results generated at the first step: 2.1 Constituent-Based Algorithms Alg. 5 is the span-based algorithm proposed by (Eisner, 1996). The algorithm has been widely used in data-driven graph-based dependency parsers, because of its efficiency by parsing with a complexity of O(n3 ). When combined with a learning method, the training for a data-driven parser involves repeatedly decoding of parse trees. Parsing efficiency is an important factor in such an approach. Some extensions to Alg. 5 have been proposed (Carreras, 2007; McDonald et al., 2006) with the aim of enriching the information available for new dependency arc detection. The work for Alg. 2, for Alg. 3, and for Alg. 4 (Table 1). Reserving these partial results in t"
W11-2926,W03-3023,0,0.122422,"arsing Algorithms Meixun Jin, Hwidong Na and Jong-Hyeok Lee Department of Computer Science and Engineering Pohang University of Science and Technology (POSTECH) San 31 Hyoja Dong, Pohang, 790-784, Republic of Korea meixunj,leona,jhlee@postech.ac.kr Abstract in (McDonald et al., 2006) in contrast to the algorithm used in (McDonald et al., 2005). Parsing histories are partial results generated from previous parsing steps. In a chart parser, these histories can be used in subsequent parsing steps. Previous works have shown that the use of parsing histories helps to resolve syntactic ambiguities (Yamada and Mastumoto, 2003; Nivre et al., 2007b; McDonald et al., 2006; Carreras, 2007; Chen et al., 2010). Obviously, using more histories provides better parsing disambiguation. However, there is a trade-off between using more histories and parsing efficiently. One option is to incorporate only important histories. The selection of different histories requires changes to the parsing algorithm. Another reason for the careful selection of parsing algorithms is from the diverse dependency annotation strategies. The dependency annotations for the same linguistic structures, i.e., coordinate structures can vary (Section 3"
W11-2926,W11-2926,1,0.0513221,"a-driven parser involves repeatedly decoding of parse trees. Parsing efficiency is an important factor in such an approach. Some extensions to Alg. 5 have been proposed (Carreras, 2007; McDonald et al., 2006) with the aim of enriching the information available for new dependency arc detection. The work for Alg. 2, for Alg. 3, and for Alg. 4 (Table 1). Reserving these partial results in the chart in addition to the constituent , only increases the constant factor, the overall space complexity remains as O(n3 ). For more information on Algs. 2 and 3 see (Eisner and Satta, 1999). and Alg. 4 see (Jin, 2011). 2.2 Span-Based Algorithms Table 1 lists the step for various dependency chart parsing algorithms to decide a new dependency and arc connecting h(ead) and d(ependent). present the constituents dominated by h and d respectively. The derivation of the new constituent in Alg. 1 (Table 1) is processed in one-step, and the onestep processing can be defined as the function f of Alg. 1, which involves three parameters: two constituents and and the evaluation of the 221 Chart combinations Combination function Time / Space Score function for new dependency arc Complexity f( Alg. 1 score , ( f( Alg. 2"
W11-2926,P10-1001,0,0.104393,"rc, and it is necessary for the parsing algorithm to take them into consideration. Each dependency treebank may have some unique characteristics, and it requires for the parser to model them by certain parsing histories. We show in experiments that proper selection of the parsing algorithm which reflect the dependency annotation of the coordinate structures improves the overall performance. 1 Introduction In data-driven graph-based parsing, a chart parser is frequently combined with a learning method to derive and evaluate the parse forest and output the optimal parse tree (Chen et al., 2010; Koo and Collins, 2010). The proper selection of a parsing algorithm is important for efficiency and correctness. Chart parsing is the realization of dynamic programming for syntactic analysis. It is suitable for ambiguous grammars, for example, the grammars of natural languages. Practically, according to the diverse implementations of dynamic programming for dependency syntactic analysis, there are a number of dependency chart parsing algorithms. In this paper, we list a number of bottom-up dependency chart parsing algorithms in terms of their use of the parsing histories (section 2). Incorporating parsing historie"
W11-2926,D07-1013,0,0.130781,"Missing"
W11-2926,D07-1096,0,\N,Missing
W14-3327,P07-2045,0,0.0156403,"ries and PBSMT system, and then score these candidates using co-occurrence word frequency measure to select the best candidate. We have done experiment on two language pairs • English-German • German-English The rest of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus and experiment result, and section 4 shows a brief conclusion of our work. 2 2.1 Method Phrase-based machine translation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1 , a multi-thread version of GIZA++ (Och & Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit 2 (Federico, Bertoldi, & Cettolo, 2008) with improved Kneser Ney smoothing (Chen & Goodman, 1996; Kneser & Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences"
W14-3327,N03-1017,0,0.0371225,"anslate the input query into relevant terms in target language. One way to translate queries is dictionarybased query translation. However, an input query usually consists of multiple terms, which cause low coverage of bilingual dictionary. Alternative way is translating queries using statistical machine translation (SMT) system. However, translation model could contain some noise that is meaningless translation. The goal of our method is to overcome the shortcomings of these approaches by a heuristic hybrid approach. As a baseline, we use phrase-based statistical machine translation (PBSMT) (Koehn, Och, & Marcu, 2003) techniques to handle queries that consist of multiple terms. To identify multiple terms in a query, we analyze three cases of the formation of queries and generate query translation candidates using term-to-term dictionaries and PBSMT system, and then score these candidates using co-occurrence word frequency measure to select the best candidate. We have done experiment on two language pairs • English-German • German-English The rest of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus"
W14-3327,J03-1002,0,0.00408891,"he best candidate. We have done experiment on two language pairs • English-German • German-English The rest of parts in this paper are organized as following: section 2 describes the techniques and system settings used in our experiment, section 3 presents used corpus and experiment result, and section 4 shows a brief conclusion of our work. 2 2.1 Method Phrase-based machine translation system The phrase-based statistical machine translation system is implemented using MOSE’S toolkits (Koehn et al., 2007). Bidirectional word alignments were built by MGIZA 1 , a multi-thread version of GIZA++ (Och & Ney, 2003), run on a 24 threads machine. The alignment symmetrization method is grow-diag-final-and (Koehn et al., 2003), and lexicalized-reordering method is msdbidirectional-fe (Koehn et al., 2007). For each monolingual corpus, we used a fivegram language model, which was built by IRSTLM toolkit 2 (Federico, Bertoldi, & Cettolo, 2008) with improved Kneser Ney smoothing (Chen & Goodman, 1996; Kneser & Ney, 1995). The language model was integrated as a loglinear feature to decoder. All the sentences in the training, development and test corpus were tokenized by inserting spaces between words and punctua"
W14-3327,P96-1041,0,\N,Missing
W16-2384,2006.amta-papers.25,0,0.221945,"????????? ???????? ?????????? ???????????? ???????????? Figure 2: Recurrent neural network based sentence-level QE model (SENT/RNN) (Kim and Lee, 2016) tion such that QEsentence ( y, x ) = QE0sentence ( qy1 , ... , qyT ) y (1) = σ(Ws s) . Ws is the weight matrix of sigmoid function5 at sentence-level QE. s is a summary unit of the sequential quality vectors and is fixed to the last hidden state6 hT y of RNN. The hidden state hj is computed by (2) RNN based Sentence-level QE Model hj = f (qyj , hj−1 ) In RNN based sentence-level QE model (Figure 2), HTER (human-targeted translation edit rate) (Snover et al., 2006) in [0,1] for target sentence is predicted by using a logistic sigmoid funcwhere f is the activation function of RNN (Kim and Lee, 2016). 2.1 2.2 RNN based Word-level QE Model In RNN based word-level QE model (Figure 3), we apply bidirectional RNN based binary classification (OK/BAD) using quality vectors as inputs. Through the bidirectional RNN, bidirectional hidden states {~hj , hj } for each target word yj are 2 In MT/NMT, only source sentence is used as a input. In QE, however, both source and target sentences can be used as inputs. 3 In all activation functions of RNN, the gated hidden un"
W16-2384,P15-4020,0,0.0869543,"ation (QE) model for predicting the sentence, word and phrase-level translation qualities, without relying on manual efforts to find QE related features. Existing QE researches have been usually focused on finding desirable QE related features to use machine learning algorithms. Recently, however, there have been efforts to apply neural networks to QE and these neural approaches have shown potential for QE. Shah et al. (2015) use continuous space language model features for sentence-level QE and word embedding features for word-level QE, in combination with other features produced by QuEst++ (Specia et al., 2015). Kreutzer et al. (2015) apply neural networks using pre-trained alignments and word lookup-table to word-level QE, which achieve the excellent performance by using the combination of baseline In the first part (Figure 1), modified RNN-based NMT model generates quality vectors, which indicate a sequence of vectors about target words’ translation qualities. Each quality vector for each target word has, as not a number unit but a vector unit, the quality information about whether each target word is properly translated from source sentence. Each quality vector is generated by decomposing the pro"
W16-2384,W12-3102,0,\N,Missing
W16-2384,P13-1045,0,\N,Missing
W16-2384,C04-1046,0,\N,Missing
W16-2384,D14-1179,0,\N,Missing
W16-2384,2009.eamt-1.5,0,\N,Missing
W16-2384,W14-3340,0,\N,Missing
W16-2384,2005.mtsummit-papers.11,0,\N,Missing
W16-2384,W15-3041,0,\N,Missing
W16-2384,W15-3037,0,\N,Missing
W16-2384,D13-1170,0,\N,Missing
W16-2384,W15-3001,0,\N,Missing
W16-2384,W14-3302,0,\N,Missing
W16-2384,W13-2201,0,\N,Missing
W16-2384,W15-3039,0,\N,Missing
W16-2384,W15-3042,0,\N,Missing
W17-4763,D14-1179,0,0.00639027,"Missing"
W17-4763,W16-2387,0,0.282003,"tor was 1000, and the size of the hidden units of the quality estimator was 150. 5) The vocabulary size was 100,000 words, the word embedding dimensionality was 700, the size of the hidden units of the word predictor was 1000, and the size of the hidden units of the quality estimator was 200. 6 In the submissions for WMT17 QE task, Results of Ensembles of Multiple Instances To develop ensemble-based submissions for the WMT17 QE task, we used two types of single models: the simplest (base model) and most sophisticated (Predictor-Estimator using multilevel task learning with stack propagation). Martins et al. (2016) combined 15 instances of neural models to make ensembles; they used three types of neural models and trained five instances for each type by using different data shuffles. In our experiments, we made ensembles of multiple instances trained under different set565 Sentence Level PredictorEstimator + (SingleLevel) Stackprop + MultiLevel Stackprop Pearson’s r ↑ Word Level PredictorEstimator + (SingleLevel) Stackprop + MultiLevel Stackprop Phrase Level PredictorEstimator + (SingleLevel) Stackprop + MultiLevel Stackprop 0.6826 0.6888 0.6985 MAE ↓ RMSE ↓ 0.0987 0.0977 0.0952 0.1428 0.1458 0.1461 F1"
W17-4763,2006.amta-papers.25,0,0.0464218,"????? ???????????????????? ???????? ?????????????? ?????????????????? ???? ???????? ⋯ : ?????????????????? ?????????????? : ??????????????????&?????????????????? ?????????????? Figure 2: Applied stack propagation (Zhang and Weiss, 2016) to Predictor-Estimator architecture by alternating stochastic updates. This approach is based on the idea that QE at all levels has a common origin because quality annotations at each level of QE data1 are obtained by comparing the same post-edited target references with the same target translations to calculate the human-targeted translation edit rate (HTER) (Snover et al., 2006). By using multilevel task learning with stack propagation, mutually beneficial relationships can be learned between each level. We alternate not only between stochastic updates to word prediction or QE objectives but also between stochastic updates to sentence/word/phrase-level QE objectives for jointly learning mutual common parts of the Predictor-Estimator network2 . 3 3.1 allel corpora including the Europarl corpus, common crawl corpus, news commentary, rapid corpus of EU press releases for the WMT17 translation task3 , and src-pe (source sentences-their target post-editions) pairs for the"
W17-4763,P13-4014,0,0.168536,"Missing"
W17-4763,P16-1147,0,0.031279,"Missing"
W17-4763,N16-1059,1,\N,Missing
W18-6470,W17-4772,0,0.0125583,"e assumed to be a different representation of a common abstracted meaning. However, in APE problem, we cannot adopt this assumption because the machine translation output is considered to have systematic errors. These errors make a gap between the machine translation output and the post-edited sentence. Therefore, for APE problem, we should aim to reduce the gap, not to find the common abstracted meaning. In this intuition, the three directions should be considered to model the APE problem, sentence correction (mt→pe), ideal translation (src→pe), and original translation (src→mt). Even though Bérard et al. (2017) used a chained architecture for the context information of original translation, most of previous approaches focused on combining sentence correction and ideal translation. However, in terms of reducing the gap, APE problem is close to modeling the relation between original translation and ideal translation, rather than the relation between the machine translation output and the post-edited sentence. Our multi-encoder transformer network is based on this idea. Figure 1 illustrates the overall architecture of our multi-encoder transformer network 841 for APE problem. We extend transformer netw"
W18-6470,W17-4775,0,0.0441476,"ulti-Encoder Architecture For a multi-source translation problem, the proper modeling of the relation between the multiple sources and the target is important. Combining two separate single-source translation models for 840 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 840–845 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64097 each source-target relation (Junczys-Dowmunt and Grundkiewicz, 2016) or constructing single input by combining the all sources (Hokamp, 2017) may be a solution, but these are not the exactly modeling the multi-source translation problem. Zoph and Knight (2016) proposed the basic model of the multi-source translation problem. Their multi-encoder architecture uses trilingual data and contains separate encoders for each input to model the conditional probability of the target over the two sources. Libovický et al. (2016) showed the application of this multi-encoder architecture to model APE problem. They used the same architecture in both APE task and multi-modal translation task, because the two tasks can be defined as multi-source t"
W18-6470,W16-2378,0,0.240957,"ti-source translation problem between two sources (mt, src) and a target (pe). 2 2.1 Related Work Multi-Encoder Architecture For a multi-source translation problem, the proper modeling of the relation between the multiple sources and the target is important. Combining two separate single-source translation models for 840 Proceedings of the Third Conference on Machine Translation (WMT), Volume 2: Shared Task Papers, pages 840–845 c Belgium, Brussels, October 31 - Novermber 1, 2018. 2018 Association for Computational Linguistics https://doi.org/10.18653/v1/W18-64097 each source-target relation (Junczys-Dowmunt and Grundkiewicz, 2016) or constructing single input by combining the all sources (Hokamp, 2017) may be a solution, but these are not the exactly modeling the multi-source translation problem. Zoph and Knight (2016) proposed the basic model of the multi-source translation problem. Their multi-encoder architecture uses trilingual data and contains separate encoders for each input to model the conditional probability of the target over the two sources. Libovický et al. (2016) showed the application of this multi-encoder architecture to model APE problem. They used the same architecture in both APE task and multi-modal"
W18-6470,W16-2361,0,0.481753,"nsformer Network for Automatic Post-Editing Jaehun Shin and Jong-hyeok Lee Department of Computer Science and Engineering, Pohang University of Science and Technology {jaehun.shin, jhlee}@postech.ac.kr Due to the additional source, APE has two translation directions, the mt→pe direction and the src→pe direction. Previous researches have suggested various methods to combine the two directions with neural network architecture, such as loglinear combination of two translation models (Junczys-Dowmunt and Grundkiewicz, 2016), factored translation model (Hokamp, 2017) and multiencoder architecture (Libovický et al., 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Variš and Bojar, 2017). Among the methods, we focus on the multi-encoder approach because it is more appropriate to model the multi-source translation problem. Also, considering the importance of proper attention mechanism, as shown in the research of JunczysDowmunt and Grundkiewicz (2017), we use the transformer network (Vaswani et al., 2017) composed of a novel attention mechanism. With this consideration, our submission to the WMT 2018 shared task on Automatic Post-Editing is a neural multi-encoder model based on the transfo"
W18-6470,P02-1040,0,0.102958,"g step, we reduced the training data to the sum of the official training data set and artificial-small data set. We trained the base model on the reduced training data during 30k steps more and selected the model with the lowest validation loss (1st-tuned). For the second tuning step, we used the official training data to fine-tune the 1st-tuned model. We used the same tuning method with 1k training step. The model with lowest validation was selected as the final model (2nd-tuned). 4.4 Evaluation We evaluated the models using the WMT data set, computing the TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) scores on the decoded output. The decoding parameter is the same as the default decoding parameter of the Tensor2tensor. We used the scores of original machine translation output as the baseline to compare our results. Table 2 shows the results of the evaluation on PBSMT data set and NMT data set. The result on PBSMT data set is comparable to the last year’s top result without any additional post-processing. In contrast, the result on NMT data set shows almost no improvement. We guess that the different characteristics of PBSMT artificial data set from the NMT training data set causes the res"
W18-6470,W17-4717,0,0.0377431,"Missing"
W18-6470,2006.amta-papers.25,0,0.110043,"n two step. For the first tuning step, we reduced the training data to the sum of the official training data set and artificial-small data set. We trained the base model on the reduced training data during 30k steps more and selected the model with the lowest validation loss (1st-tuned). For the second tuning step, we used the official training data to fine-tune the 1st-tuned model. We used the same tuning method with 1k training step. The model with lowest validation was selected as the final model (2nd-tuned). 4.4 Evaluation We evaluated the models using the WMT data set, computing the TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) scores on the decoded output. The decoding parameter is the same as the default decoding parameter of the Tensor2tensor. We used the scores of original machine translation output as the baseline to compare our results. Table 2 shows the results of the evaluation on PBSMT data set and NMT data set. The result on PBSMT data set is comparable to the last year’s top result without any additional post-processing. In contrast, the result on NMT data set shows almost no improvement. We guess that the different characteristics of PBSMT artificial data set from the NMT"
W18-6470,W17-4777,0,0.189493,"Computer Science and Engineering, Pohang University of Science and Technology {jaehun.shin, jhlee}@postech.ac.kr Due to the additional source, APE has two translation directions, the mt→pe direction and the src→pe direction. Previous researches have suggested various methods to combine the two directions with neural network architecture, such as loglinear combination of two translation models (Junczys-Dowmunt and Grundkiewicz, 2016), factored translation model (Hokamp, 2017) and multiencoder architecture (Libovický et al., 2016; Chatterjee et al., 2017; Junczys-Dowmunt and Grundkiewicz, 2017; Variš and Bojar, 2017). Among the methods, we focus on the multi-encoder approach because it is more appropriate to model the multi-source translation problem. Also, considering the importance of proper attention mechanism, as shown in the research of JunczysDowmunt and Grundkiewicz (2017), we use the transformer network (Vaswani et al., 2017) composed of a novel attention mechanism. With this consideration, our submission to the WMT 2018 shared task on Automatic Post-Editing is a neural multi-encoder model based on the transformer network. We extend the transformer network implementation in Tensor2Tensor (Vaswani"
W18-6470,W18-1819,0,0.0332914,"r, 2017). Among the methods, we focus on the multi-encoder approach because it is more appropriate to model the multi-source translation problem. Also, considering the importance of proper attention mechanism, as shown in the research of JunczysDowmunt and Grundkiewicz (2017), we use the transformer network (Vaswani et al., 2017) composed of a novel attention mechanism. With this consideration, our submission to the WMT 2018 shared task on Automatic Post-Editing is a neural multi-encoder model based on the transformer network. We extend the transformer network implementation in Tensor2Tensor (Vaswani et al., 2018) library to implement our model. We participated in both PBSMT task and NMT task with this multi-encoder model. In this paper, we introduce the multi-encoder transformer network for APE. The remainder of the paper is organized as follows: Section 2 contains the related work. Section 3 describes our method. Section 4 gives the experimental results, and Section 5 is the conclusion. Abstract This paper describes the POSTECH’s submission to the WMT 2018 shared task on Automatic Post-Editing (APE). We propose a new neural end-to-end post-editing model based on the transformer network. We modified t"
W19-5412,W16-2378,0,0.101641,"BLEU compared to the baseline, and ranks second among all submitted systems. 1 Introduction Automatic Post-Editing (APE) is the task of automatically correcting errors in a given the machine translation (MT) output to generate a better translation (Chatterjee et al., 2018). Because APE can be regarded as a sequence-to-sequence problem, MT techniques have been previously applied to this task. Subsequently, it is only natural that neural APE has been proposed following the appearance of neural machine translation (NMT). Among the initial approaches to neural APE, a log-linear combination model (Junczys-Dowmunt and Grundkiewicz, 2016) that combines bilingual * Both authors equally contributed to this work 112 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 3: Shared Task Papers (Day 2) pages 112–117 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics Output Probabilities Encoder Softmax Add & Norm Feed Forward Add & Norm Multi-Head Attention Add & Norm Linear Feed Forward Add & Norm Add & Norm Feed Forward Multi-Head Attention ×N Encoder Multi-source Attention Add & Norm Add & Norm (Masked) Multi-Head Attention Masked Multi-Head Attention Positional Encoding Positio"
W19-5412,W18-6467,0,0.39185,"eSCAPE-NMT-filtered 4,303,876 39.65 Table 1: Dataset statistics – number of sentence triplets (src, mt, pe) and TER score. Multi-Head Attention nection layers, then the same operation is performed between the result and ???? . Add & Norm ???? = LayerNorm(? ′ + ???? ) Add & Norm Multi-Head Attention where ?′ = LayerNorm(??? + ?????? ) ???? = MultiHead(?′ , ???? , ???? ) ?????? = MultiHead(??? , ?????? , ?????? ). (b) Figure 3: Illustrations of the multi-source attention layer. (a) and (b) refer to the linear and sequential combinations, respectively. This approach is structurally equivalent to Junczys-Dowmunt and Grundkiewicz (2018), except that the encoder states being passed on are different. encoder states is replaced with a multi-source attention layer. Figure 2 shows our decoder architecture including the multi-source attention layer that attends to both outputs of the encoder. Furthermore, we construct two types of the multisource attention layer by utilizing different strategies in combining attention over two encoder output states. 3 3.1 Experiments Dataset We used the WMT19 official English-German APE dataset (Chatterjee et al., 2018) which consists of a training and development set. In addition, we adopted the"
W19-5412,P17-4012,0,0.0513913,"monolingual pair (mt, pe) less than 1.4. Table 1 summarizes the statistic of the datasets. Multi-source parallel attention. Figure 3a illustrates the structure of parallel attention. The decoder's hidden state simultaneously attends to each output of the multi-source encoder, followed by residual connection, and the results are linearly combined by summing them at the end: ????????? = ?1 + ?2 where ?1 = LayerNorm(??? + ?????? ) ?2 = LayerNorm(??? + ???? ) ?????? = MultiHead(??? , ?????? , ?????? ) ???? = MultiHead(??? , ???? , ???? ). 3.2 Training Details Settings. We modified the OpenNMT-py (Klein et al., 2017) implementation of Transformer to build our models. Most hyperparameters such as the dimensionality of hidden states, optimizer settings, dropout ratio, etc. were copied from the “base model” described in Vaswani et al. (2017). We adjusted the warm-up learning steps and batch size per triplets to 18k and ~25k, respectively. For data preprocessing, we employed subword encoding (Kudo, 2018) with 32k shared vocabulary. Note that ??? ∈ ℝ??×? denotes the hidden states for decoder input pe z = (?1 , … , ??? ). Multi-source sequential attention. As shown in Figure 3b, two outputs of the encoder are s"
W19-5412,N16-1004,0,0.0564367,"Missing"
W19-5412,P18-1007,0,0.0209367,"re ?1 = LayerNorm(??? + ?????? ) ?2 = LayerNorm(??? + ???? ) ?????? = MultiHead(??? , ?????? , ?????? ) ???? = MultiHead(??? , ???? , ???? ). 3.2 Training Details Settings. We modified the OpenNMT-py (Klein et al., 2017) implementation of Transformer to build our models. Most hyperparameters such as the dimensionality of hidden states, optimizer settings, dropout ratio, etc. were copied from the “base model” described in Vaswani et al. (2017). We adjusted the warm-up learning steps and batch size per triplets to 18k and ~25k, respectively. For data preprocessing, we employed subword encoding (Kudo, 2018) with 32k shared vocabulary. Note that ??? ∈ ℝ??×? denotes the hidden states for decoder input pe z = (?1 , … , ??? ). Multi-source sequential attention. As shown in Figure 3b, two outputs of the encoder are sequentially combined with the decoder’s hidden state: ?????? and the decoder’s hidden state are first assigned to multi-head attention and residual con114 Teacherforcing Ratios Parallel w/ masking TER BLEU w/o tuning 1.00 0.95 0.90 0.85 15.06 15.02 15.07 77.18 77.25 77.24 14.75 14.86 0.80 14.98 Architecture Parallel Sequential w/o masking w/ masking TER BLEU TER BLEU 77.29 77.54 77.37 15."
W19-5412,W16-2361,0,0.0941734,"Missing"
W19-5412,L18-1004,0,0.200868,"coder states being passed on are different. encoder states is replaced with a multi-source attention layer. Figure 2 shows our decoder architecture including the multi-source attention layer that attends to both outputs of the encoder. Furthermore, we construct two types of the multisource attention layer by utilizing different strategies in combining attention over two encoder output states. 3 3.1 Experiments Dataset We used the WMT19 official English-German APE dataset (Chatterjee et al., 2018) which consists of a training and development set. In addition, we adopted the eSCAPE NMT dataset (Negri et al., 2018) as additional training data. We extracted sentence triplets from the eSCAPE-NMT dataset according to the following criteria, to which the official training dataset mostly adheres. Selected triplets have no more than 70 words in each sentence, a TER less than or equal to 75, and a reciprocal length ratio within the monolingual pair (mt, pe) less than 1.4. Table 1 summarizes the statistic of the datasets. Multi-source parallel attention. Figure 3a illustrates the structure of parallel attention. The decoder's hidden state simultaneously attends to each output of the multi-source encoder, follow"
W19-5412,W18-6468,0,0.0644779,"Missing"
W19-5412,P02-1040,0,0.104051,"ere the model involves the multi-source sequential attention layer with the encoder mask.  Sequential w/o masking in which the encoder mask is excluded from Seq. w/ masking. 3.3  Ens_set_2: top-N candidates for variants in each architecture, in terms of TER.  Ens_set_3: two candidates for variants in each architecture, achieving the best TER and BLEU scores, respectively. Results We trained a generic model for each of the four model variations mentioned in §3.2. Then, we fine-tuned those models using various teacherforcing ratios. For evaluation, we used TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) scores on the WMT official development dataset. Table 2 shows the scores of the generic and fine-tuned models according to their architectures and teacher-forcing ratios. The result shows that adjusting teacher-forcing ratio helps improve the post-editing performance of the models. Table 3 gives the results of the ensemble models. The ensemble models had slightly worse TER scores (+0.02 ~ +0.13) than the best TER score in the fine-tuned variants, but better BLEU scores (+0.09 ~ +0.27) than the best BLEU score. We Teacher-forcing ratio. During training, because the decoder takes as input the t"
W19-5412,W18-6470,1,0.847603,"ibovický et al., 2016) based on multi-source translation (Zoph and Knight, 2016) has become the prevalent approach (Bojar et al., 2017). Recently, with the advent of Transformer (Vaswani et al., 2017), most of the participants in the WMT18 APE shared task proposed Transformerbased multi-encoder APE models (Chatterjee et al., 2018). Previous multi-encoder APE models employ separate encoders for each input (src, mt), and combine their outputs in various ways: by 1) sequentially applying attention between the hidden state of the decoder and the two outputs (JunczysDowmunt and Grundkiewicz, 2018; Shin and Lee, 2018) or 2) simply concatenating them (Pal et al., 2018; Tebbifakhr et al., 2018). However, these approaches seem to overlook one of the key differences between general multi-source translation and APE. Because the errors mt may contain are dependent on the MT system, the encoding process for mt should reflect its relationship with the source sentence. Furthermore, we believe that it would be helpful to incorporate information from the source sentence, which should ideally be error-free, in addition to the jointly encoded mt in generating post-edited sentence. From these points of view, we propose"
W19-5412,2006.amta-papers.25,0,0.358891,"ing.  Sequential w/ masking where the model involves the multi-source sequential attention layer with the encoder mask.  Sequential w/o masking in which the encoder mask is excluded from Seq. w/ masking. 3.3  Ens_set_2: top-N candidates for variants in each architecture, in terms of TER.  Ens_set_3: two candidates for variants in each architecture, achieving the best TER and BLEU scores, respectively. Results We trained a generic model for each of the four model variations mentioned in §3.2. Then, we fine-tuned those models using various teacherforcing ratios. For evaluation, we used TER (Snover et al., 2006) and BLEU (Papineni et al., 2002) scores on the WMT official development dataset. Table 2 shows the scores of the generic and fine-tuned models according to their architectures and teacher-forcing ratios. The result shows that adjusting teacher-forcing ratio helps improve the post-editing performance of the models. Table 3 gives the results of the ensemble models. The ensemble models had slightly worse TER scores (+0.02 ~ +0.13) than the best TER score in the fine-tuned variants, but better BLEU scores (+0.09 ~ +0.27) than the best BLEU score. We Teacher-forcing ratio. During training, because"
W98-1110,H92-1022,0,0.436219,"usually complex morphological structures. In agglutinative languages, a word (called eojeol in Korean) usually consists of separable single stem-morpheme plus one or more functional morphemes, and the POS tag should be assigned to each morpheme to cope with the complex morphological phenomena. Recently, rule-based approaches are tAn eojeol is a Korean spacing unit(similar to English word) which usually consists of one or more stem morphemes and functional morphemes. re-studied to overcome the limitations of staffstical approaches by learning symbolic tagging rules automatically from a corpus (Brill, 1992; Bril!. 1994). Some systems even perform the POS tagging as part of a syntactic analysis process (Voutilainen, 1995). However, rule-based approaches alone, in general, are not very robust, and not portable enough to be adjusted to new tag sets and new languages. Also the. performance is usually no better than the statistical counterparts (Brill, 1992). To gain the portability and robustness and also to overcome the limited coverage of statistical approaches, we adopt a hybrid method that can combine both statistical and rule-based approaches for POS disambiguation. 2 Linguistic characteristic"
W98-1110,A92-1018,0,0.260735,"re than one unknown-morphemes or if unknown-morphemes appear other than the first position, all the previous methods cannot efficiently estimate them. sO, we propose a morpheme-pattern dictionary which enables us to treat unknownmorphemes in the same way as registered known morphemes, and thereby to guess them regardless of their numbers and positions in an eojeol. The unknown-morpheme handling using the morpheme-pattern dictionary is integrated into a hybrid POS disambiguation. The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model (HMM) (Cutting et al., 1992; Kupiec. 1992; Weischedel et al., 1993). However. since statistical approaches take into account neighboring tags only within a limited window (usually two or three), sometimes the decision cannot cover all linguistic contexts necessary for POS disambiguation. Also the approaches are inappropriate for idiomatic expressions for which lexical terms need to be directly referenced. The statistical approaches are not enough especially for agglutinative languages (such as Korean) which have usually complex morphological structures. In agglutinative languages, a word (called eojeol in Korean) usuall"
W98-1110,J94-2001,0,0.0248662,"Missing"
W98-1110,J93-2006,0,0.033213,"unknown-morphemes appear other than the first position, all the previous methods cannot efficiently estimate them. sO, we propose a morpheme-pattern dictionary which enables us to treat unknownmorphemes in the same way as registered known morphemes, and thereby to guess them regardless of their numbers and positions in an eojeol. The unknown-morpheme handling using the morpheme-pattern dictionary is integrated into a hybrid POS disambiguation. The POS disambiguation has usually been performed by statistical approaches mainly using hidden markov model (HMM) (Cutting et al., 1992; Kupiec. 1992; Weischedel et al., 1993). However. since statistical approaches take into account neighboring tags only within a limited window (usually two or three), sometimes the decision cannot cover all linguistic contexts necessary for POS disambiguation. Also the approaches are inappropriate for idiomatic expressions for which lexical terms need to be directly referenced. The statistical approaches are not enough especially for agglutinative languages (such as Korean) which have usually complex morphological structures. In agglutinative languages, a word (called eojeol in Korean) usually consists of separable single stem-morp"
Y09-1028,P99-1008,0,0.0507217,"(Maedche and Staab, 2002) and those that semi-automatically construct the ontology using a mass corpus (Kavalec and Svatek, 2005). The first method focuses on expanding the ontology using existing resources, and the second method focuses on constructing new ontologies. Particularly, constructing the ontology using a mass corpus begins with automatic extraction of semantic relations. Extraction of semantic relations is a process that extracts term pairs that satisfy a specified target relation. The most common approach in this process is a pattern-based approach for is-a and part-of relations (Berland and Charniak, 1999; Girju et al., 2003; Hearst, 1992; Pantel and Pennacchiotti, 2006; Ravichandran and Hovy, 2002). This approach regards words appearing between terms as patterns, and extracts term pairs using the patterns. Hearst (1992) extracts is-a relations using manual patterns, and proposes a bootstrapping algorithm using seed term pairs. To extract part-of relations, Berland and Charniak (1999) measure the reliabilities of terms using the frequencies of terms that have the concept of “whole” or “part,” and Girju et al. (2003) uses a lexical database (WordNet) and a decision tree. Ravichandran and Hovy ("
Y09-1028,N03-1011,0,0.0246465,"d those that semi-automatically construct the ontology using a mass corpus (Kavalec and Svatek, 2005). The first method focuses on expanding the ontology using existing resources, and the second method focuses on constructing new ontologies. Particularly, constructing the ontology using a mass corpus begins with automatic extraction of semantic relations. Extraction of semantic relations is a process that extracts term pairs that satisfy a specified target relation. The most common approach in this process is a pattern-based approach for is-a and part-of relations (Berland and Charniak, 1999; Girju et al., 2003; Hearst, 1992; Pantel and Pennacchiotti, 2006; Ravichandran and Hovy, 2002). This approach regards words appearing between terms as patterns, and extracts term pairs using the patterns. Hearst (1992) extracts is-a relations using manual patterns, and proposes a bootstrapping algorithm using seed term pairs. To extract part-of relations, Berland and Charniak (1999) measure the reliabilities of terms using the frequencies of terms that have the concept of “whole” or “part,” and Girju et al. (2003) uses a lexical database (WordNet) and a decision tree. Ravichandran and Hovy (2002) extract semant"
Y09-1028,C92-2082,0,0.290263,"tomatically construct the ontology using a mass corpus (Kavalec and Svatek, 2005). The first method focuses on expanding the ontology using existing resources, and the second method focuses on constructing new ontologies. Particularly, constructing the ontology using a mass corpus begins with automatic extraction of semantic relations. Extraction of semantic relations is a process that extracts term pairs that satisfy a specified target relation. The most common approach in this process is a pattern-based approach for is-a and part-of relations (Berland and Charniak, 1999; Girju et al., 2003; Hearst, 1992; Pantel and Pennacchiotti, 2006; Ravichandran and Hovy, 2002). This approach regards words appearing between terms as patterns, and extracts term pairs using the patterns. Hearst (1992) extracts is-a relations using manual patterns, and proposes a bootstrapping algorithm using seed term pairs. To extract part-of relations, Berland and Charniak (1999) measure the reliabilities of terms using the frequencies of terms that have the concept of “whole” or “part,” and Girju et al. (2003) uses a lexical database (WordNet) and a decision tree. Ravichandran and Hovy (2002) extract semantic relations f"
Y09-1028,N04-1041,0,0.0179011,"terms and the words between the terms. The positions use a variable X and Y. If a sentence is “A human is an animal,” terms are “human” and “animal,” and a pattern is “X is an Y.” Espresso algorithm (Pantel and Pennacchiotti, 2006) uses a bootstrapping algorithm (Hearst, 1992) and has good performance. This algorithm extracts patterns appearing between some prepared term pairs (seed term pairs) that satisfy a specified target relation. The reliabilities of these patterns are measured by the pattern reliability equation (rP) using pointwise mutual information (pmi) and discounting factor (df) (Pantel and Ravichandran, 2004) as: rP  p     pd x, y , p  r x, y  XY  x , y XY XY pd  x, y , p   where , pmi  x, y , p  df x, y , p  max pmidf (1) . In (1), p is a pattern; (x, y) is a term pair; XY is a set of term pairs used to extract patterns and rXY is the term pair reliability equation, rXY x, y   where   pd x, y , p  r  p  P pP P (2) , where P is a set of cumulative patterns used to extract term pairs. The patterns are sorted by rP, and the most reliable pattern is used to extract new term pairs. The reliabilities of new term pairs are also measured by rXY. The reliable t"
Y09-1028,P06-1015,0,0.133203,"nstruct the ontology using a mass corpus (Kavalec and Svatek, 2005). The first method focuses on expanding the ontology using existing resources, and the second method focuses on constructing new ontologies. Particularly, constructing the ontology using a mass corpus begins with automatic extraction of semantic relations. Extraction of semantic relations is a process that extracts term pairs that satisfy a specified target relation. The most common approach in this process is a pattern-based approach for is-a and part-of relations (Berland and Charniak, 1999; Girju et al., 2003; Hearst, 1992; Pantel and Pennacchiotti, 2006; Ravichandran and Hovy, 2002). This approach regards words appearing between terms as patterns, and extracts term pairs using the patterns. Hearst (1992) extracts is-a relations using manual patterns, and proposes a bootstrapping algorithm using seed term pairs. To extract part-of relations, Berland and Charniak (1999) measure the reliabilities of terms using the frequencies of terms that have the concept of “whole” or “part,” and Girju et al. (2003) uses a lexical database (WordNet) and a decision tree. Ravichandran and Hovy (2002) extract semantic relations for various terms in a question a"
Y09-1028,P02-1006,0,0.0370639,"s corpus (Kavalec and Svatek, 2005). The first method focuses on expanding the ontology using existing resources, and the second method focuses on constructing new ontologies. Particularly, constructing the ontology using a mass corpus begins with automatic extraction of semantic relations. Extraction of semantic relations is a process that extracts term pairs that satisfy a specified target relation. The most common approach in this process is a pattern-based approach for is-a and part-of relations (Berland and Charniak, 1999; Girju et al., 2003; Hearst, 1992; Pantel and Pennacchiotti, 2006; Ravichandran and Hovy, 2002). This approach regards words appearing between terms as patterns, and extracts term pairs using the patterns. Hearst (1992) extracts is-a relations using manual patterns, and proposes a bootstrapping algorithm using seed term pairs. To extract part-of relations, Berland and Charniak (1999) measure the reliabilities of terms using the frequencies of terms that have the concept of “whole” or “part,” and Girju et al. (2003) uses a lexical database (WordNet) and a decision tree. Ravichandran and Hovy (2002) extract semantic relations for various terms in a question answering (QA) system. Pantel a"
Y09-1028,P07-1067,0,0.0162337,"a question answering (QA) system. Pantel and  The work reported in this paper was supported in part by the Korea Science and Engineering Foundation (KOSEF) grant funded by the Korean government (MEST No. 2009-0075211), and in part by the BK 21 Project in 2009. Copyright 2009 by Se-Jong Kim, Yong-Hun Lee, and Jong-Hyeok Lee 23rd Pacific Asia Conference on Language, Information and Computation, pages 260–268 260 Pennacchiotti (2006) propose Espresso algorithm that uses the reliabilities of terms and patterns, and that has good performance. This algorithm is applied to a coreference resolution (Yang and Su, 2007). However, these methods consider only single sentences to extract patterns, and can not extract pairs of terms in different sentences. In single sentences, the term pairs satisfying is-a relations are, in fact, few, and the kind of patterns reflecting each target relation are very restricted. If previous methods use an open-domain corpus, these methods may not be useful. We propose a method that extracts term pairs satisfying is-a relations or part-of relations from a mass corpus using pairs of patterns sharing a term, and compare the result with a previous method (Pantel and Pennacchiotti, 2"
