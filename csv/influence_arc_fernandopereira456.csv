C08-1060,P07-1124,0,0.0288534,"designed for this problem. The news system improves market prediction over baseline market systems. 1 Introduction The mass media can affect world events by swaying public opinion, officials and decision makers. Financial investors who evaluate the economic performance of a company can be swayed by positive and negative perceptions about the company in the media, directly impacting its economic position. The same is true of politics, where a candidate’s performance is impacted by media influenced public perception. Computational linguistics can discover such signals in the news. For example, Devitt and Ahmad (2007) gave a computable metric of polarity in financial news text consistent with human judgments. Koppel and Shtrimberg (2004) used a daily news analysis to predict financial market performance, though predictions could not be used for future investment decisions. Recently, c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. a study conducted of the 2007 French presidential election showed a correlation between the frequency of a candidate’s name in the news and electoral s"
C08-1060,W06-2932,1,0.807763,"Missing"
C08-1060,N03-1033,0,0.0395658,"Missing"
C08-1060,D07-1113,0,0.221988,"rom syntactic dependency parses of the news and a user-defined set of market entities. Successive news days are compared to determine the novel component of each day’s news resulting in features for a machine learning system. A combination system uses this information as well as predictions from internal market forces to model prediction markets better than several baselines. Results show that news articles can be mined to predict changes in public opinion. Opinion forecasting differs from that of opinion analysis, such as extracting opinions, evaluating sentiment, and extracting predictions (Kim and Hovy, 2007). Contrary to these tasks, our system receives objective news, not subjective opinions, and learns what events will impact public opinion. For example, “oil prices rose” is a fact but will likely shape opinions. This work analyzes news (cause) to predict future opinions (effect). This affects the structure of our task: we consider a timeseries setting since we must use past data to predict future opinions, rather than analyzing opinions in batch across the whole dataset. 473 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 473–480 Manchester, A"
C98-2142,P96-1031,1,0.685153,"s Results and 3. For each SCC S, construct the automaton K(S). For each X e S, build M(X) from We have a full implementation of the compilation algorithm presented in the previous section, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a general set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&T Labs. The GRM Library also includes an efficient compilation tool for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the GRM library is compatible with the FSM general-purpose finite-state machine li~ brary (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers T(G) and the weighted automata in the compilation output. We did several experiments that show the efficiency of our compilation method. A key feature of our grammar compilation method is the representation of the grammar by a weighted transducer that can then be preoptimized using weighted transducer determinizat"
C98-2142,P83-1014,0,0.0521812,"r is thus used together with other information sources - pronunciation dictionary, phonemic context-dependency model, acoustic model (Bahl et al., 1983; Rabiner and Juang, 1993) - to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for reM-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often represent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of/~ more general CFG (Pereira and Wright, 1997).t Assuming that the g r a m m a r can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (U1lian; 1967). However, we will describe a useful class of grammars that"
C98-2142,P91-1032,1,\N,Missing
D07-1112,W07-2416,0,0.0334886,"Missing"
D07-1112,W04-3111,0,0.0744338,"Missing"
D07-1112,J93-2004,0,0.0316121,"Missing"
D07-1112,W06-2932,1,0.426727,"Missing"
D07-1112,W97-0309,1,0.621417,"Missing"
D07-1112,D07-1096,0,\N,Missing
D08-1061,C92-2082,0,0.419541,"Missing"
D08-1061,A00-1040,0,0.0119261,"Angola, or active volcanoes like Etna and Kilauea. Note that zoonotic diseases, surgical procedures, African countries and active volcanoes serve as useful class labels that capture the semantics of the associated sets of class instances. Such interest in a wide variety of specific domains highlights the utility of constructing large collections of fine-grained classes. Comprehensive and accurate class-instance information is useful not only in search but also in a variety of other text processing tasks including co-reference resolution (McCarthy and Lehnert, 1995), named entity recognition (Stevenson and Gaizauskas, 2000) and seed-based information extraction (Riloff and Jones, 1999). 1.2 Contributions We study the acquisition of open-domain, labeled classes and their instances from both structured and unstructured textual data sources by combining and ranking individual extractions in a principled way with the Adsorption label-propagation algorithm (Baluja et al., 2008), reviewed in Section 3 below. A collection of labeled classes acquired from text (Van Durme and Pas¸ca, 2008) is extended in two ways: 1. Class label coverage is increased by identifying additional class labels (such as public agencies and gov"
D08-1061,C02-1144,0,\N,Missing
D10-1017,N09-1014,0,0.0434584,"which is unlikely to scale up because its dual formulation requires the inversion of a matrix whose size depends on the graph size. Gupta et al. (2009) also constrain similar trigrams to have similar POS tags by forming cliques of similar trigrams and maximizing the agreement score over these cliques. Computing clique agreement potentials however is NP-hard and so they propose approximation algorithms that are still quite complex computationally. We achieve similar effects by using our simple, scalable convex graph regularization framework. Further, unlike other graph-propagation algorithms (Alexandrescu and Kirchhoff, 2009), our approach is inductive. While one might be able to make inductive extensions of transductive approaches (Sindhwani et al., 2005), these usually require extensive computational resources at test time. 6 Experiments and Results We use the Wall Street Journal (WSJ) section of the Penn Treebank as our labeled source domain training set. We follow standard setup procedures for this task and train on sections 00-18, comprising of 38,219 POS-tagged sentences with a total of 912,344 words. To evaluate our domain-adaptation approach, we consider two different target domains: questions and biomedic"
D10-1017,W06-1615,1,0.263887,"here α is a mixing coefficient which reflects the relative confidence between the original posteriors from the CRF and the smoothed posteriors from the graph. We discuss how we set α in Section 6. 172 −η l+u X  2 log p(yi∗ |xi ; Λ(t) )+γkΛk (5) n i=l+1 where η and γ are hyper-parameters whose setting we discuss in Section 6. Given the new CRF parameters Λ we loop back to step 1 (Section 4.1) and iterate until convergence. It is important to note that every step of our algorithm is convex, although their combination clearly is not. 5 Related Work Our work differs from previous studies of SSL (Blitzer et al., 2006; III, 2007; Huang and Yates, 2009) for improving POS tagging in several ways. First, our algorithm can be generalized to other structured semi-supervised learning problems, although POS tagging is our motivating task and test application. Unlike III (2007), we do not require target domain labeled data. While the SCL algorithm (Blitzer et al., 2006) has been evaluated without target domain labeled data, that evaluation was to some extent transductive in that the target test data (unlabeled) was included in the unsupervised stage of SCL training that creates the structural correspondence betwee"
D10-1017,P09-1056,0,0.0267525,"ich reflects the relative confidence between the original posteriors from the CRF and the smoothed posteriors from the graph. We discuss how we set α in Section 6. 172 −η l+u X  2 log p(yi∗ |xi ; Λ(t) )+γkΛk (5) n i=l+1 where η and γ are hyper-parameters whose setting we discuss in Section 6. Given the new CRF parameters Λ we loop back to step 1 (Section 4.1) and iterate until convergence. It is important to note that every step of our algorithm is convex, although their combination clearly is not. 5 Related Work Our work differs from previous studies of SSL (Blitzer et al., 2006; III, 2007; Huang and Yates, 2009) for improving POS tagging in several ways. First, our algorithm can be generalized to other structured semi-supervised learning problems, although POS tagging is our motivating task and test application. Unlike III (2007), we do not require target domain labeled data. While the SCL algorithm (Blitzer et al., 2006) has been evaluated without target domain labeled data, that evaluation was to some extent transductive in that the target test data (unlabeled) was included in the unsupervised stage of SCL training that creates the structural correspondence between the two domains. We mentioned alr"
D10-1017,P07-1033,0,0.0159588,"Missing"
D10-1017,P06-1063,0,0.0202101,"Missing"
D10-1017,N03-1033,0,0.0271084,"Missing"
D10-1017,P95-1026,0,0.149289,"t-of-domain data may lag far behind. Annotating training data for all sub-domains of a varied domain such as all of Web text is impractical, giving impetus to the development of SSL techniques that can learn from unlabeled data to perform well across domains. The earliest SSL algorithm is self-training (Scudder, 1965), where one makes use of a previously trained model to annotate unlabeled data which is then used to re-train the model. While self-training is widely Fernando Pereira Google Research Mountain View, CA 94043 pereira@google.com used and can yield good results in some applications (Yarowsky, 1995), it has no theoretical guarantees except under certain stringent conditions, which rarely hold in practice(Haffari and Sarkar, 2007). Other SSL methods include co-training (Blum and Mitchell, 1998), transductive support vector machines (SVMs) (Joachims, 1999), and graph-based SSL (Zhu et al., 2003). Several surveys cover a broad range of methods (Seeger, 2000; Zhu, 2005; Chapelle et al., 2007; Blitzer and Zhu, 2008). A majority of SSL algorithms are computationally expensive; for example, solving a transductive SVM exactly is intractable. Thus we have a conflict between wanting to use SSL wit"
D10-1017,P07-1096,0,\N,Missing
D12-1093,N06-1038,0,0.0074744,"y define features to be shortest paths on dependency trees which connect pairs of NP candidates. 1019 This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying relation mentions in text (microreading),this work attempts to infer new tuples by gathering path evidence over the whole corpus (macroreading). In addition, their work involves a few thousand examples, while we aim for Web-scale extraction. Do and Roth (2010) use a KB (YAGO) to aid the generation of features from free text. However their method is designed specifically for extracting hierarchical taxonomic structures, while our algorithm can be use"
D12-1093,D10-1107,0,0.0120882,"heir path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying relation mentions in text (microreading),this work attempts to infer new tuples by gathering path evidence over the whole corpus (macroreading). In addition, their work involves a few thousand examples, while we aim for Web-scale extraction. Do and Roth (2010) use a KB (YAGO) to aid the generation of features from free text. However their method is designed specifically for extracting hierarchical taxonomic structures, while our algorithm can be used to discover relations for general general graph-based KBs. In this paper we extend the PRA algorithm along two dimensions: combining syntactic and semantic cues in text with existing knowledge in the KB; and a distributed implementation of the learning and inference algorithms that works at Web scale. 2 Path Ranking Algorithm We briefly review the Path Ranking algorithm (PRA), described in more detail"
D12-1093,D09-1120,0,0.0146574,"also collect a large Web corpus and identify 60 million pages that mention concepts relevant to this study. The free text on those pages are POS-tagged and dependency parsed with an accuracy comparable to that of the current Stanford dependency parser (Klein and Manning, 2003). The parser produces a dependency tree for each sentence with each edge labeled with a standard dependency tag (see Figure 1). In each of the parsed documents, we use POS tags and dependency edges to identify potential referring noun phrases (NPs). We then use a within-document coreference resolver comparable to that of Haghighi and Klein (2009) to group referring NPs into co-referring clusters. For each cluster that contains a proper-name mention, we find the Freebase concept or concepts, if any, with a name or alias that matches 3 www.wikipedia.org, www.allmusic.com, www. imdb.com. 1022 provided that there exists at least one c ∈ C(m) and m ∈ M (u) such that p(c|m) &gt; 0. Note that M (c) only contains the proper-name mentions in cluster c. 5 Results We use three relations profession, nationality and parents for our experiments. For each relation, we select its current set of triples in Freebase, and apply the stratified sampling (Sec"
D12-1093,C92-2082,0,0.285256,"rning (SRL) seeks to combine statistical and relational learning methods to address such tasks. However, most SRL approaches (Friedman et al., 1999; Richardson and Domingos, 2006) suffer the complexity of inference and learning when applied to large scale problems. Recently, Lao and Cohen (2010) introduced Path Ranking algorithm, which is applicable to larger scale problems such as literature recommendation (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be sho"
D12-1093,P03-1054,0,0.00390782,"ase as our knowledge base. Freebase data is harvested from many sources, including Wikipedia, AMG, and IMDB.3 As of this writing, it contains more than 21 million concepts and 70 million labeled edges. For a large majority of concepts that appear both in Freebase and Wikipedia, Freebase maintains a link to the Wikipedia page of that concept. We also collect a large Web corpus and identify 60 million pages that mention concepts relevant to this study. The free text on those pages are POS-tagged and dependency parsed with an accuracy comparable to that of the current Stanford dependency parser (Klein and Manning, 2003). The parser produces a dependency tree for each sentence with each edge labeled with a standard dependency tag (see Figure 1). In each of the parsed documents, we use POS tags and dependency edges to identify potential referring noun phrases (NPs). We then use a within-document coreference resolver comparable to that of Haghighi and Klein (2009) to group referring NPs into co-referring clusters. For each cluster that contains a proper-name mention, we find the Freebase concept or concepts, if any, with a name or alias that matches 3 www.wikipedia.org, www.allmusic.com, www. imdb.com. 1022 pro"
D12-1093,D11-1049,1,0.487434,"William W. Cohen1 1 Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, PA 15213, USA 2 Google Research, 1600 Amphitheatre Parkway, Mountain View, CA 94043, USA nlao@cs.cmu.edu, {asubram, pereira}@google.com, wcohen@cs.cmu.edu Abstract then the person is a national of the country.” Of course, rules like this may be defeasible, in this case for example because of naturalization or political changes. Nevertheless, many such imperfect rules can be learned and combined to yield useful KB completions, as demonstrated in particular with the Path-Ranking Algorithm (PRA) (Lao and Cohen, 2010; Lao et al., 2011), which learns such rules on heterogenous graphs for link prediction tasks. We study how to extend a large knowledge base (Freebase) by reading relational information from a large Web text corpus. Previous studies on extracting relational knowledge from text show the potential of syntactic patterns for extraction, but they do not exploit background knowledge of other relations in the knowledge base. We describe a distributed, Web-scale implementation of a path-constrained random walk model that learns syntactic-semantic inference rules for binary relations from a graph representation of the pa"
D12-1093,P09-1113,0,0.820977,"to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. 1019 This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-values features. We show experimentally that fractional-valued features generated by random walks provide much higher accuracy than binary-valued ones. Culotta et al. (2006)’s work is similar to our approach in the sense of relation extraction by discovering relational patterns. However while they focus on identifying"
D12-1093,P02-1006,0,0.0266667,"tion (Lao and Cohen, 2010) and inference on a large knowledge base (Lao et al., 2011). Much of the previous work on automatic relation extraction was based on certain lexico-syntactic patterns. Hearst (1992) first noticed that patterns such as “NP and other NP” and “NP such as NP” often imply hyponym relations (NP here refers to a noun phrase). However, such approaches to relation extraction are limited by the availability of domain knowledge. Later systems for extracting arbitrary relations from text mostly use shallow surface text patterns (Etzioni et al., 2004; Agichtein and Gravano, 2000; Ravichandran and Hovy, 2002). The idea of using sequences of dependency edges as features for relation extraction was explored by Snow et al. (2005) and Suchanek et al. (2006). They define features to be shortest paths on dependency trees which connect pairs of NP candidates. 1019 This study is most closely related to work of Mintz et al. (2009), who also study the problem of extending Freebase with extraction from parsed text. As in our work, they use a logistic regression model with path features. However, their approach does not exploit existing knowledge in the KB. Furthermore, their path patterns are used as binary-"
E06-1011,P05-1012,1,0.361592,"example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing. Sometimes a non-projective representations are preferred, as in the sentence in Figure 2.1 In particular, for freer-word order languages, non-projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid. The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent. Though trees are 1 Examples are drawn from McDonald et al. (2005c). more common, some formalisms allow for words to modify multiple parents (Hudson, 1984). Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its ed"
E06-1011,H05-1066,1,0.625225,"Missing"
E06-1011,A00-2018,0,0.242316,"projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges. This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions. Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000). In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs. We also present an algorithm for acyclic dependency graphs, that is, dependency graphs in which a word may depend on multiple heads. In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable. We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new models lead to s"
E06-1011,H05-1011,0,0.0128157,"we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance. Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm. This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002). Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daum´e and Marcu, 2005; McDonald et al., 2005a) and phrase-structure parsing (Collins and Roark, 2004). This robustness to approximations comes from the fact that the online framework sets weights with respect to inference. In other words, the learning method sees common errors due to 85 Training data: T = {(xt , yt )}Tt=1 1. w (0) = 0; v = 0; i = 0 1st-order-projective 2nd-order-projective 2. for n : 1..N 3. 4. for t : 1..T ‚ ‚ ‚ ‚ min ‚w(i+1) − w(i) ‚ s.t. s(xt , yt ; w (i+1) ) 0 (i) where y = arg maxy 0 s(xt , y ; w ) 5. 6. Complete 36.7 42.1 Table 1: Dependency parsing"
E06-1011,P05-1013,0,0.490079,"ective MST parsing is NP-hard, as shown in appendix A. To circumvent this, we designed an approximate algorithm based on the exact O(n3 ) second-order projective Eisner algorithm. The approximation works by first finding the highest scoring projective parse. It then rearranges edges in the tree, one at a time, as long as such rearrangements increase the overall score and do not violate the tree constraint. We can easily motivate this approximation by observing that even in non-projective languages like Czech and Danish, most trees are primarily projective with just a few non-projective edges (Nivre and Nilsson, 2005). Thus, by starting with the highest scoring projective tree, we are typically only a small number of transformations away from the highest scoring non-projective tree. The algorithm is shown in Figure 4. The expression y[i → j] denotes the dependency graph identical to y except that xi ’s parent is xi instead 83 FIRST-ORDER h1 h1 h3 h3 ⇒ h1 r r+1 h3 h1 (A) SECOND-ORDER h3 (B) h1 h1 h2 h2 h3 h1 h2 h2 h3 ⇒ h1 (A) h2 h2 r r+1 h3 h3 ⇒ h1 h2 h2 (B) h3 h1 h3 (C) Figure 3: A O(n3 ) extension of the Eisner algorithm to second-order dependency parsing. This figure shows how h1 creates a dependency to"
E06-1011,P04-1015,0,0.184333,"adation in performance. Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm. This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002). Online learning algorithms have been shown to be robust even with approximate rather than exact inference in problems such as word alignment (Moore, 2005), sequence analysis (Daum´e and Marcu, 2005; McDonald et al., 2005a) and phrase-structure parsing (Collins and Roark, 2004). This robustness to approximations comes from the fact that the online framework sets weights with respect to inference. In other words, the learning method sees common errors due to 85 Training data: T = {(xt , yt )}Tt=1 1. w (0) = 0; v = 0; i = 0 1st-order-projective 2nd-order-projective 2. for n : 1..N 3. 4. for t : 1..T ‚ ‚ ‚ ‚ min ‚w(i+1) − w(i) ‚ s.t. s(xt , yt ; w (i+1) ) 0 (i) where y = arg maxy 0 s(xt , y ; w ) 5. 6. Complete 36.7 42.1 Table 1: Dependency parsing results for English. −s(xt , y 0 ; w(i+1) ) ≥ L(yt , y 0 ) 0 English Accuracy 90.7 91.5 1st-order-projective 2nd-order-pro"
E06-1011,P99-1065,0,0.0624493,"Missing"
E06-1011,W02-1001,0,0.771205,"ed learning, we assume a training set T = {(xt , yt )}Tt=1 , consisting of pairs of a sentence xt and its correct dependency representation yt . The algorithm is an extension of the Margin Infused Relaxed Algorithm (MIRA) (Crammer and Singer, 2003) to learning with structured outputs, in the present case dependency structures. Figure 6 gives pseudo-code for the algorithm. An online learning algorithm considers a single training instance for each update to the weight vector w. We use the common method of setting the final weight vector as the average of the weight vectors after each iteration (Collins, 2002), which has been shown to alleviate overfitting. On each iteration, the algorithm considers a single training instance. We parse this instance to obtain a predicted dependency graph, and find the smallest-norm update to the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al., 2005b). Note that we only impose margin constraints between the single highest-scoring graph and the correct"
E06-1011,C96-1058,0,0.974845,"lish and the best accuracy we know of for Czech and Danish. 2 Maximum Spanning Tree Parsing Dependency-tree parsing as the search for the maximum spanning tree (MST) in a graph was 81 root John saw a dog yesterday which was a Yorkshire Terrier Figure 2: An example non-projective dependency structure. root hit John ball with the bat the root0 John1 hit2 the3 ball4 with5 the6 bat7 Figure 1: An example dependency structure. proposed by McDonald et al. (2005c). This formulation leads to efficient parsing algorithms for both projective and non-projective dependency trees with the Eisner algorithm (Eisner, 1996) and the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) respectively. The formulation works by defining the score of a dependency tree to be the sum of edge scores, X s(x, y) = s(i, j) (i,j)∈y where x = x1 · · · xn is an input sentence and y a dependency tree for x. We can view y as a set of tree edges and write (i, j) ∈ y to indicate an edge in y from word xi to word xj . Consider the example from Figure 1, where the subscripts index the nodes of the tree. The score of this tree would then be, s(0, 2) + s(2, 1) + s(2, 4) + s(2, 5) + s(4, 3) + s(5, 7) + s(7, 6) We call this first"
E06-1011,W05-1505,0,0.0916131,"Missing"
E06-1011,H05-1124,1,0.22089,"example of a projective (or nested) tree representation, in which all edges can be drawn in the plane with none crossing. Sometimes a non-projective representations are preferred, as in the sentence in Figure 2.1 In particular, for freer-word order languages, non-projectivity is a common phenomenon since the relative positional constraints on dependents is much less rigid. The dependency structures in Figures 1 and 2 satisfy the tree constraint: they are weakly connected graphs with a unique root node, and each non-root node has a exactly one parent. Though trees are 1 Examples are drawn from McDonald et al. (2005c). more common, some formalisms allow for words to modify multiple parents (Hudson, 1984). Recently, McDonald et al. (2005c) have shown that treating dependency parsing as the search for the highest scoring maximum spanning tree (MST) in a graph yields efficient algorithms for both projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its ed"
E06-1011,W96-0213,0,0.292784,"eature space. We evaluate dependencies on per word accuracy, which is the percentage of words in the sentence with the correct parent in the tree, and on complete dependency analysis. In our evaluation we exclude punctuation for English and include it for Czech and Danish, which is the standard. 5.1 English Results To create data sets for English, we used the Yamada and Matsumoto (2003) head rules to extract dependency trees from the WSJ, setting sections 2-21 as training, section 22 for development and section 23 for evaluation. The models rely on part-of-speech tags as input and we used the Ratnaparkhi (1996) tagger to provide these for the development and evaluation set. These data sets are exclusively projective so we only compare the projective parsers using the exact projective parsing algorithms. The purpose of these experiments is to gauge the overall benefit from including second-order features with exact parsing algorithms, which can be attained in the projective setting. Results are shown in Table 1. We can see that there is clearly an advantage in introducing second-order features. In particular, the complete tree metric is improved considerably. 5.2 Czech Results ith where xi -pos is th"
E06-1011,W04-3201,0,0.0050333,"the weight vector w that ensures that the training graph outscores the predicted graph by a margin proportional to the loss of the predicted graph relative to the training graph, which is the number of words with incorrect parents in the predicted tree (McDonald et al., 2005b). Note that we only impose margin constraints between the single highest-scoring graph and the correct graph relative to the current weight setting. Past work on tree-structured outputs has used constraints for the k-best scoring tree (McDonald et al., 2005b) or even all possible trees by using factored representations (Taskar et al., 2004; McDonald et al., 2005c). However, we have found that a single margin constraint per example leads to much faster training with a negligible degradation in performance. Furthermore, this formulation relates learning directly to inference, which is important, since we want the model to set weights relative to the errors made by an approximate inference algorithm. This algorithm can thus be viewed as a large-margin version of the perceptron algorithm for structured outputs Collins (2002). Online learning algorithms have been shown to be robust even with approximate rather than exact inference i"
E06-1011,W03-3023,0,0.91412,"fficient algorithms for both projective and non-projective trees. When combined with a discriminative online learning algorithm and a rich feature set, these models provide state-of-the-art performance across multiple languages. However, the parsing algorithms require that the score of a dependency tree factors as a sum of the scores of its edges. This first-order factorization is very restrictive since it only allows for features to be defined over single attachment decisions. Previous work has shown that conditioning on neighboring decisions can lead to significant improvements in accuracy (Yamada and Matsumoto, 2003; Charniak, 2000). In this paper we extend the MST parsing framework to incorporate higher-order feature representations of bounded-size connected subgraphs. We also present an algorithm for acyclic dependency graphs, that is, dependency graphs in which a word may depend on multiple heads. In both cases parsing is in general intractable and we provide novel approximate algorithms to make these cases tractable. We evaluate these algorithms within an online learning framework, which has been shown to be robust with respect approximate inference, and describe experiments displaying that these new"
H05-1066,P99-1065,0,0.0348557,"Missing"
H05-1066,J93-2004,0,0.0564691,"ta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. However, there are certain examples in which a nonprojective tree is preferable. Consider the sentence John saw a dog yesterday which was a Yorkshire Terrier. Here the relative clause which was a Yorkshire Terrier and the object it modifies (the dog) are separated by an adverb. There is no way to draw the dependency tree for this sentence in the plane with no crossing edges, as illustrated in Figure 2. In languages with more flexible word order than English, such as German, Dutch and Czech, non-projective dependencies are more frequent. Rich inflec"
H05-1066,W02-1001,0,0.419449,"he highest score, s(x, y). The resulting online update (to be inserted in Figure 4, line 4) would then be: min w(i+1) − w(i) s.t. s(xt , yt ) − s(xt , y 0 ) ≥ L(yt , y 0 ) where y 0 = arg maxy0 s(xt , y 0 ) McDonald et al. (2005) used a similar update with k constraints for the k highest-scoring trees, and showed that small values of k are sufficient to achieve the best accuracy for these methods. However, here we stay with a single best tree because kbest extensions to the Chu-Liu-Edmonds algorithm are too inefficient (Hou, 1996). This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy. 3.2 Factored MIRA It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003; Taskar et al., 2004). For the directed maximum spanning tree problem, we can factor the output by edges to obtain the fol"
H05-1066,P05-1012,1,0.17838,".ms.mff.cuni.cz Abstract root We formalize weighted dependency parsing as searching for maximum spanning trees (MSTs) in directed graphs. Using this representation, the parsing algorithm of Eisner (1996) is sufficient for searching over all projective trees in O(n 3 ) time. More surprisingly, the representation is extended naturally to non-projective parsing using Chu-Liu-Edmonds (Chu and Liu, 1965; Edmonds, 1967) MST algorithm, yielding an O(n2 ) parsing algorithm. We evaluate these methods on the Prague Dependency Treebank using online large-margin learning techniques (Crammer et al., 2003; McDonald et al., 2005) and show that MST parsing increases efficiency and accuracy for languages with non-projective dependencies. John hit the ball with the bat Figure 1: An example dependency tree. Dependency representations, which link words to their arguments, have a long history (Hudson, 1984). Figure 1 shows a dependency tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in"
H05-1066,P05-1013,0,0.802738,"es, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees. The main idea of our method is that dependency parsing can be formalized as the search for a maximum spanning tree in a directed graph. This formalization generalizes standard projective parsing models bas"
H05-1066,P04-1054,0,0.385345,"tree for the sentence John hit the ball with the bat. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence. 1 Introduction Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) a"
H05-1066,P05-1067,0,0.184843,"at. We restrict ourselves to dependency tree analyses, in which each word depends on exactly one parent, either another word or a dummy root symbol as shown in the figure. The tree in Figure 1 is projective, meaning that if we put the words in their linear order, preceded by the root, the edges can be drawn above the words without crossings, or, equivalently, a word and its descendants form a contiguous substring of the sentence. 1 Introduction Dependency parsing has seen a surge of interest lately for applications such as relation extraction (Culotta and Sorensen, 2004), machine translation (Ding and Palmer, 2005), synonym generation (Shinyama et al., 2002), and lexical resource augmentation (Snow et al., 2004). The primary reasons for using dependency structures instead of more informative lexicalized phrase structures is that they are more efficient to learn and parse while still encoding much of the predicate-argument information needed in applications. In English, projective trees are sufficient to analyze most sentence types. In fact, the largest source of English dependency trees is automatically generated from the Penn Treebank (Marcus et al., 1993) and is by convention exclusively projective. H"
H05-1066,C96-1058,0,0.827257,"w vˇetˇsinou a dog nem´a yesterday ani z´ajem which a taky was na a Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a pa"
H05-1066,C04-1010,0,0.0300593,"Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dep"
H05-1066,W04-3201,0,0.00698393,"s algorithm are too inefficient (Hou, 1996). This model is related to the averaged perceptron algorithm of Collins (2002). In that algorithm, the single highest scoring tree (or structure) is used to update the weight vector. However, MIRA aggressively updates w to maximize the margin between the correct tree and the highest scoring tree, which has been shown to lead to increased accuracy. 3.2 Factored MIRA It is also possible to exploit the structure of the output space and factor the exponential number of margin constraints into a polynomial number of local constraints (Taskar et al., 2003; Taskar et al., 2004). For the directed maximum spanning tree problem, we can factor the output by edges to obtain the following constraints: min w(i+1) − w(i) s.t. s(l, j) − s(k, j) ≥ 1 ∀(l, j) ∈ yt , (k, j) ∈ / yt This states that the weight of the correct incoming edge to the word xj and the weight of all other incoming edges must be separated by a margin of 1. It is easy to show that when all these constraints are satisfied, the correct spanning tree and all incorrect spanning trees are separated by a score at least as large as the number of incorrect incoming edges. This is because the scores for all the corr"
H05-1066,W04-0307,0,0.0166533,"non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-projective edges into dependency trees through learned edge transformations within their memory-based parser. They test this system on Czech and show improved accuracy relative to a projective parser. Our approach differs from those earlier efforts in searching optimally and efficiently the full space of non-projective trees. The main idea of ou"
H05-1066,W03-3023,0,0.934387,"z´ajem which a taky was na a Yorkshire to vˇetˇsinou Terrier nem´a pen´ıze He is mostly not even interested in the new things and in most cases, he has no money for it either. Figure 2: Non-projective dependency trees in English and Czech. grammatical relations, allowing non-projective dependencies that we need to represent and parse efficiently. A non-projective example from the Czech Prague Dependency Treebank (Hajiˇc et al., 2001) is also shown in Figure 2. Most previous dependency parsing models have focused on projective trees, including the work of Eisner (1996), Collins et al. (1999), Yamada and Matsumoto (2003), Nivre and Scholz (2004), and McDonald et al. (2005). These systems have shown that accurate projective dependency parsers can be automatically learned from parsed data. However, non-projective analyses have recently attracted some interest, not only for languages with freer word order but also for English. In particular, Wang and Harper (2004) describe a broad coverage non-projective parser for English based on a hand-constructed constraint dependency grammar rich in lexical and syntactic information. Nivre and Nilsson (2005) presented a parsing model that allows for the introduction of non-"
H05-1124,W96-0213,0,0.122148,"stems rely on such segmentations to accurately process the data. Depending on the application, segments may be tokens, phrases, or sentences. However, in this paper we primarily focus on segmenting sentences into tokens. The most common approach to text segmentation is to use finite-state sequence tagging models, in which each atomic text element (character or token) is labeled with a tag representing its role in a segmentation. Models of that form include hidden Markov models (Rabiner, 1989; Bikel et al., 1999) as well as discriminative tagging models based on maximum entropy classification (Ratnaparkhi, 1996; McCallum et al., 2000), conditional random fields (Lafferty et al., 2001; Sha and Pereira, 2003), and large-margin techniques (Kudo and Matsumoto, 2001; Taskar et al., 2003). Tagging models are the best previous methods for text segmentation. However, their purely sequential form limits their ability to naturally handle overlapping or noncontiguous segments. We present here an alternative view of segmentation as structured multilabel classification. In this view, a segmentation of a text is a set of segments, each of which is defined by the set of text positions that belong to the segment. T"
H05-1124,W02-1001,0,0.106215,"pproach can be seen as multilabel variant of the work of McDonald et al. (2004), which creates a set of constraints to separate the score of the single correct output from the k highest scoring outputs with an appropriate large margin. 4 Experiments We now describe a set of experiments on named entity and base NP segmentation. For these experiments, we set k = n, where n is the length of the sentence. This represents a reasonable upper bound on the number of entities or chunks in a sentence and results in a time complexity of O(n3 T ). We compare our methods with both the averaged perceptron (Collins, 2002) and conditional random fields (Lafferty et al., 2001) using identical predicate sets. Though all systems use identical predicates, the actual features of the systems are different due to the fundamental differences between the multilabel classification and sequential tagging models. 4.1 Standard data sets Our first experiments are standard named entity and base NP data sets with no overlapping, embedded or non-contiguous segments. These experiments will show that, for simple segmentations, our model is competitive with sequential tagging models. For the named entity experiments we used the Co"
H05-1124,P05-1040,0,0.0113443,"abeling techniques using the BIO label set will encounter problems. Figure 2 shows two simple examples of segmentations involving overlapping, non-contiguous segments. In both cases, it is difficult to see how a sequential tagger could extract the segments correctly. It would be possible to grow the tag set to represent a bounded number of overlapping, noncontiguous segments by representing all possible combinations of segment membership over k overlapping segments, but this would require an arbitrary upper bound on k and would lead to models that generalize poorly and are expensive to train. Dickinson and Meurers (2005) point out that, as language processing begins to tackle problems in free-word order languages and discourse analysis, annotating and extracting non-contiguous segmentations of text will become increasingly important. Though we focus primarily on entity extraction and NP chunking in this paper, there is no reason why ideas presented here could not be extended to managing other non-contiguous phenomena. 3 Structured Multilabel Classification As outlined in Section 1, we represent segmentation as multilabel classification, assigning to each text the set of segments it contains. Figure 3 shows th"
H05-1124,N03-1028,1,0.921751,", segments may be tokens, phrases, or sentences. However, in this paper we primarily focus on segmenting sentences into tokens. The most common approach to text segmentation is to use finite-state sequence tagging models, in which each atomic text element (character or token) is labeled with a tag representing its role in a segmentation. Models of that form include hidden Markov models (Rabiner, 1989; Bikel et al., 1999) as well as discriminative tagging models based on maximum entropy classification (Ratnaparkhi, 1996; McCallum et al., 2000), conditional random fields (Lafferty et al., 2001; Sha and Pereira, 2003), and large-margin techniques (Kudo and Matsumoto, 2001; Taskar et al., 2003). Tagging models are the best previous methods for text segmentation. However, their purely sequential form limits their ability to naturally handle overlapping or noncontiguous segments. We present here an alternative view of segmentation as structured multilabel classification. In this view, a segmentation of a text is a set of segments, each of which is defined by the set of text positions that belong to the segment. Thus, a particular segment may not be a set of consecutive positions in the text, and segments may"
H05-1124,W03-0419,0,0.0313345,"Missing"
H05-1124,N01-1025,0,\N,Missing
H90-1005,J82-3004,0,\N,Missing
H90-1005,P85-1018,0,\N,Missing
H90-1005,W89-0229,0,\N,Missing
H92-1024,W89-0209,0,0.0250144,"ble with a bracketing B of w if no span of a symbol occurrence in the derivation overlaps a span in B. 3. THE INSIDE-OUTSIDE ALGORITHM The inside-outside algorithm [2] is a reestimation procedure for the rule probabilities of a Chomsky normal-form (CNF) SCFG. It takes as inputs an initial CNF SCFG and a training corpus of sentences and it iteratively reestimates rule probabilities to maximize the probability that the grammar used as a stochastic generator would produce the corpus. A reestimation algorithm can be used both to refine the parameter estimates for a CNF SCFG derived by other means [7] or to infer a grammar from scratch. In the latter case, the initial grammar for the inside-outside algorithm consists of all possible CNF rules over given sets 123 N of nonterminals and E of terminals, with suitable assigned nonzero probabilities. In what follows, we will take N, E as fixed, n = [NI, t = I~1, and assume enumerations N = { A 1 , . . . , A n } and E = { b l , . . . , b t } , with A1 the grammar start symbol. A CNF SCFG over N, E can then be specified by the n s + nt probabilities Bp,q,r of each possible binary rule Ap ~ Aq Ar and Up,m of each possible unary rule A n ~ bin. Sinc"
H92-1024,H90-1021,0,0.029291,"raw text decreases rapidly (1.57 initially, 0.87 after 22 iterations). Similarly, the cross entropy estimate of the bracketed text with respect to the grammar improves rapidly (2.85 initially, 0.87 after 22 iterations). The inferred grammar models correctly the palindrome language. Its high probability rules (p > 0.1, p / p &apos; > 30 125 4.2. Experiments on the ATIS Corpus We also conducted an experiment on inferring grammars for the language consisting of part-of-speech sequences of spoken-language transcriptions in the Texas Instruments subset of the Air Travel Information System (ATIS) corpus [8]. We take advantage of the availability of the hand-parsed version of the ATIS corpus provided by the Penn Treebank project [9] and use the corresponding bracketed corpus over parts of speech as training data. Out of the 770 bracketed sentences (7812 words) in the corpus, we used 700 as training data and 70 (901 words) as test set. The following is an example training string ( ( ( VB ( DT ~NS ( Im ( ( ~ CD) ) ) ) ) ) . ) ) ( n corresponding to the parsed sentence As a first example, GB gives the following bracketings: ( ( ( I (would (like (to (take (((Delta ((([List (the fares (for ((flight) ("
H92-1024,H90-1055,0,0.0335164,"ext with respect to the grammar improves rapidly (2.85 initially, 0.87 after 22 iterations). The inferred grammar models correctly the palindrome language. Its high probability rules (p > 0.1, p / p &apos; > 30 125 4.2. Experiments on the ATIS Corpus We also conducted an experiment on inferring grammars for the language consisting of part-of-speech sequences of spoken-language transcriptions in the Texas Instruments subset of the Air Travel Information System (ATIS) corpus [8]. We take advantage of the availability of the hand-parsed version of the ATIS corpus provided by the Penn Treebank project [9] and use the corresponding bracketed corpus over parts of speech as training data. Out of the 770 bracketed sentences (7812 words) in the corpus, we used 700 as training data and 70 (901 words) as test set. The following is an example training string ( ( ( VB ( DT ~NS ( Im ( ( ~ CD) ) ) ) ) ) . ) ) ( n corresponding to the parsed sentence As a first example, GB gives the following bracketings: ( ( ( I (would (like (to (take (((Delta ((([List (the fares (for ((flight) (number 891)))))) .) (flight number)) 83) (to The initial grammar consists of all possible CNF rules (4095 rules) over 15 nonter"
H92-1024,H91-1060,0,0.0173671,"Missing"
H92-1024,C92-2066,1,0.729367,"Missing"
H94-1050,J94-3001,0,0.0630817,"re detail in the next section. In this paper we will be concerned with the weighted rational case, although some of the theory can be profitably extended beyond the finite-state case [3, 4]. 262 The notion of weighted rational transduction arises from the combination of two ideas in automata theory: rational transductions, used in many aspects of formal language theory [2], and weighted languages and automata, developed in pattern recognition [5, 6] and algebraic automata theory [7, 8, 9]. Ordinary (unweighted) rational transductions have been successfully applied by researchers at Xerox PARC [10] and at the University of Paris 7 [11], among others, to several problems in language processing, includifig morphological analysis, dictionary compression and syntactic analysis. Hidden Markov Models and probabilistic finite-state language models can be shown to be equivalent to WFSAs. In algebraic automata theory, rational series and rational transductions [8] are the algebraic counterparts of WFSAs and WFSTs and give the correct generalizations to the weighted case of the standard algebraic operations on formal languages and transductions, such as union, concatenation, intersection, restric"
H94-1050,C90-3049,0,0.12806,"re derived via productive morphological processes are not generally to be found in the dictionary. One such case in Chinese involves words derived via the nominal plural affix r~l -men. While some words in ~I will be found in the dictionary ( e . g . , / ! ! ~ tal-men 'they'; ~ ren2-men 'people'), many attested instances will not: for example, ~ f ~ jiang4-men '(military) generals', ~ qingl-wal-men 'frogs'. Given that the basic dictionary is represented as a finite-state automaton, it is a simple matter to augment the model just described with standard techniques from finite-state morphology ([15, 16], inter alia). For in3We are currently using the 'Behavior Chinese-English Electronic Dictionary', Copyright Number 112366, from Behavior Design Corporation, R.O.C.; we also wish to thank United Informaties, Inc., R.O.C. for providing us with the Chinese text corpus that we used in estimating lexieal probabilities. Finally we thank Dr. Jyun-Sheng Chang for kindly providing us with Chinese personal name corpora. 266 stance, we can represent the fact that f ] attaches to nouns by allowing e-transitions from the final states of noun entries, to the initial state of a sub-transducer containing f ]"
J82-3002,J81-4003,1,0.198267,"nguistics, gramming power, and so can describe any &quot;transformational&quot; relationship, but they cannot do so by specific, well motivated grammar rules. In particular, this applies to &quot;left extraposition&quot;, the underlying concept in most grammars for such important constructions as WH-questions, relative clauses, and auxiliary fronting. Similar comments apply to ATNs (Woods 1970), even those using the HOLD/VIR facility. To handle &quot;left extraposition&quot;, and some other &quot;transformational&quot; concepts, we have introduced the grammar formalism of extraposition grammars, which are described fully elsewhere (Pereira 1981). An XG, like a DCG, is no more than &quot; s y n t a c t i c sugar&quot; for clauses of logic. As the Chat-80 grammar is intended partly as a demonstration of the power of XGs for treating left extraposition in English, the coverage of questions and relative clauses is fairly extensive. Of course, this wide coverage is essential if complex queries are to be formulated in a single sentence. A major limitation in the present coverage of English syntax is that the only phrases that may be conjoined (with &quot; a n d &quot; , etc.) are noun postmodifiers and predications introduced by the verb &apos;to be&apos;. To cover mor"
J82-3002,J81-3002,0,\N,Missing
J90-1001,J87-1005,0,0.291286,"content of semantic interpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings. 1 AN OBVIOUS CONSTRAINT.9 Treatments of quantifier scope in Montague grammar (Montague 1973; Dowty et al. 1981; Cooper 1983), transformational grammar (Reinhart 1983; May 1985; Heim 1982; Roberts 1987), and computational linguistics (Hobbs and Shieber 1987; Moran 1988; Alshawi et al. 1989) have depended implicitly or explicitly on a constraint on possible logical forms to explain why examples 1 such as 1. *A woman who saw every man disliked him are ungrammatical, and why in examples such as 2. Every man saw a friend of his 3. Every admirer of a picture of himself is vain the e v e r y . . , noun phrase must have wider scope than the a • . . noun phrase if the pronoun in each example is assumed to be bound by its antecedent. What exactly counts as bound anaphora varies between different accounts of the phenomena, but the rough intuition is that"
J90-1001,P88-1005,0,0.108745,"erpretations goes against the spirit of compositionality. I will show that those scoping restrictions follow from simple and fundamental facts about functional application and abstraction, and can be expressed as constraints on the derivation of possible meanings for sentences rather than constraints of the alleged forms of those meanings. 1 AN OBVIOUS CONSTRAINT.9 Treatments of quantifier scope in Montague grammar (Montague 1973; Dowty et al. 1981; Cooper 1983), transformational grammar (Reinhart 1983; May 1985; Heim 1982; Roberts 1987), and computational linguistics (Hobbs and Shieber 1987; Moran 1988; Alshawi et al. 1989) have depended implicitly or explicitly on a constraint on possible logical forms to explain why examples 1 such as 1. *A woman who saw every man disliked him are ungrammatical, and why in examples such as 2. Every man saw a friend of his 3. Every admirer of a picture of himself is vain the e v e r y . . , noun phrase must have wider scope than the a • . . noun phrase if the pronoun in each example is assumed to be bound by its antecedent. What exactly counts as bound anaphora varies between different accounts of the phenomena, but the rough intuition is that semantically"
J90-1001,P89-1019,1,0.117361,"Missing"
J90-1001,P88-1010,1,0.805,"occurs. The main goal of this paper is to argue that the freevariable constraint is actually a consequence of basic semantic properties that hold in a semantic domain allowing functional application and abstraction, and are thus independent of a particular logical-form representation. As a corollary, I will also show that the constraint is better expressed as a restriction on the derivations of meanings of sentences from the meanings of their parts rather than a restriction on logical forms. The resulting system is related to the earlier system of conditional interpretation rules developed by Pollack and Pereira (1988), but avoids that system's use of formal conditions on the order of assumption discharge. 2 CURRY'S CALCULUS OF FUNCTIONALITY Work in combinatory logic and the k-calculus is concerned with the elucidation of the basic notion of functionality: how to construct functions, and how to apply functions to 2 /5 (x:~) u:a [app] . v:a--./5 v(u) :/5 u:/5 [abs] : kx.u : a --""/3 A formula u: a will be called a type assignment, assigning the type a to term u. Thus, the two rules above state that if u has type a and v has type a --~/5 then v(u) has type/5, and if by assuming that x has type a, we can show t"
J90-1004,E89-1032,0,0.0953576,"d&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of Computational Linguistics Volume 16, Number 1, March 1990 Shieber et al. Semantic Head-Driven Grammar PATR-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 1.2 PRELIMINARIES Despite the general applicability of the algorithm, we will, for the sake of concreteness, describe it and other generation algorithms in terms of their implementation for definiteclause grammars (DCG). For ease of exposition, the encoding will be a bit more cumbersome than is typically found in Prolog DCG interpreters. The standard DCG encoding in Prolog uses the notation (cat o) --&gt; (cat I ) . . . . . (cat,). where the (cat i) are terms representing the grammatica"
J90-1004,J87-1005,1,0.800251,"they might be because during the distribution of store elements among the subject and complements of a verb no check is performed as to whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (1987) would not be difficult. 38 4 EXTENSIONS Tile basic semantic-head-driven generation algorithm can be augmented in various ways so as to encompass some important analyses and constraints. In particular, we discuss the incorporation of • completeness and coherence constraints, • the postponing of lexical choice, and • the ability to handle certain problematic empty-headed phrases 4.1 COMPLETENESS AND COHERENCE Wedckind (1988) defines completeness and coherence of a generation algorithm as follows. Suppose a generator derives a string w from a logical form s, and the grammar assigns to w the logi"
J90-1004,P83-1021,1,0.813733,"INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with"
J90-1004,J81-4003,1,0.825833,"Missing"
J90-1004,C88-2128,1,0.954033,"evious bottom-up generator, it allows use of semlantically nonmonotonic grammars, yet unlike top-down methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 INTRODUCTION The problem of generating a well-formed natural language expression from an encoding of its meaning possesses properties that distinguish it from the converse problem of recovering a meaning encoding from a given natural language expression. This much is axiomatic. In previous work (Shieber 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983), but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensi"
J90-1004,C88-2150,0,0.136143,"a small number of parameters. As a method for generating natural language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose well-foundedness relies on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property; further discussion can be found in Section 2.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called---has its 30 own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. Tile algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be s"
J93-3006,P92-1005,0,0.0300206,"ed, but the computational intractability arising from having to consider all the combinations of smaller alternative constraints into larger consistent constraints is bypassed. Finally, the radical route suggests a discipline of trying to replace as much as possible disjunctive or negative constraints by somewhat weaker kinds of underspecification that admit of purely conjunctive formulations. That program was already suggested in the context of deterministic parsing (d-theory) (Marcus, Hindle, and Fleck 1983), and more recently in the context of incremental monotonic semantic interpretation (Alshawi and Crouch 1992), and might also be profitably employed in the more abstract feature-logic setting. 2.4 Prerequisites I am well aware of the difficulties in determining what should be taken as a reasonable common background for readers in an interdisciplinary topic. There is little enough commonality in the theoretical backgrounds of computer scientists trained in different schools, and the common background becomes even more difficult to find when one wants to reach also theoretical linguists and AI researchers. Still, the introductory chapters, including their historical portions, seem to assume more than i"
J93-3006,P88-1035,0,0.0422557,"Missing"
J93-3006,P85-1032,0,0.0550278,"Missing"
J93-3006,P86-1038,0,0.151817,"played no direct role in LFG, GPSG, PATR-II, or logic grammars, and it came into play first as a lexicon organization discipline (Flickinger, Pollard, and Wasow 1985; Sheiber 1985), not as a central part of the formalism. The organization of the first part of the book follows naturally from the emphasis on KR ideas. Types and inheritance are discussed first, followed by feature (attributevalue) structures and the relations and operations that they inherit from the underlying type system: subsumption and join (unification). The last introductory chapter addresses in detail the crucial move of Kasper and Rounds (1986) to clarify the meaning of feature structures by viewing them as models of appropriately chosen modal logics. 2.1 Feature Structures and Feature Logics The fruitful connection between feature structures and feature logics is pursued throughout the book, with soundness and completeness results for the basic system and all the major extensions and variations considered later. If something is missing in that comprehensive development, it might be some effort to relate feature logics to modal logics, and feature structures to modal frames. I believe that the original Kasper-Rounds logic was to som"
J93-3006,P83-1020,0,0.117803,"Missing"
J93-3006,W89-0203,0,0.0219165,"rises from the computational issues just discussed. The intractability of satisfiability for expressive feature logics might seem a serious roadblock in the practical application of those logics in grammatical description systems. Two escape routes, one pragmatic and the other more radical, suggest themselves. The pragmatic route involves trying to identify more tractable subcases that may cover most of the situations of interest in actual grammatical description. Such &quot;optimistic&quot; algorithms were already suggested in Kasper&apos;s dissertation (1987), and have been extensively investigated since (Maxwell and Kaplan 1989; Eisele and D6rre 1988). The more radical route, which as far as I know has not been pursued vigorously, looks more closely at the search control aspects of language processing systems based on feature computations. It takes conjunctive description as the only one that can have global extent in a computation. Nonconjunctive aspects of a description are then ephemeral in that nonconjunctive connectives introduced in a derivation must be eliminated within a bounded number of steps by a committed choice operation based on some preference-based search control mechanism. Such a view can be seen as"
J93-3006,P84-1027,1,0.778092,"ed phrase structure formalisms, the semantics of feature-based definite clause programs, and the specification of recursive type constraints. 5.1 Semantics of Grammar Formalisms Carpenter&apos;s account of the denotational semantics of unification-based phrase structure grammars benefits greatly from the extensive use of feature algebras and featurealgebra morphisms to connect derivation steps. Earlier treatments were much less perspicuous, because they were based on complex encodings of phrase-structure rules as feature structures and of derivation steps as formal manipulations on rule encodings (Pereira and Shieber 1984; Shieber 1984; Rounds and Manaster-Ramer 1987). As a minor terminological point, the qualifier unification-based used here is somewhat unfortunate, because unification is just a particular constraint-solving algorithm applicable for certain kinds of constraint-based grammars. The term constraint-based grammar is both less biased and more appropriate to modern formalisms in which unification is only one of several constraint-solving methods. Historically, neither LFG nor GPSG were originally thought of in terms of unification. GPSG features and feature constraints were seen as abbreviatory con"
J93-3006,P87-1013,0,0.0179478,"ntics of feature-based definite clause programs, and the specification of recursive type constraints. 5.1 Semantics of Grammar Formalisms Carpenter&apos;s account of the denotational semantics of unification-based phrase structure grammars benefits greatly from the extensive use of feature algebras and featurealgebra morphisms to connect derivation steps. Earlier treatments were much less perspicuous, because they were based on complex encodings of phrase-structure rules as feature structures and of derivation steps as formal manipulations on rule encodings (Pereira and Shieber 1984; Shieber 1984; Rounds and Manaster-Ramer 1987). As a minor terminological point, the qualifier unification-based used here is somewhat unfortunate, because unification is just a particular constraint-solving algorithm applicable for certain kinds of constraint-based grammars. The term constraint-based grammar is both less biased and more appropriate to modern formalisms in which unification is only one of several constraint-solving methods. Historically, neither LFG nor GPSG were originally thought of in terms of unification. GPSG features and feature constraints were seen as abbreviatory conventions for large collections of context549 Co"
J93-3006,P84-1075,0,0.132393,"ently arisen in generalized phrase structure grammar (GPSG) (Gazdar et al. 1985), lexical-functional grammar (LFG) (Bresnan and Kaplan 1982), functionalunification grammar (FUG) (Kay 1985), logic programming (Colmerauer 1978, Pereira and Warren 1980), and terminological reasoning systems (Ait-Kaci 1984). It took, however, a lot of experimental and theoretical work to identify precisely what the core notions were, how particular systems related to the core notions, and what were the most illuminating mathematical accounts of that core. The development of the unificationbased formalism PATR-II (Shieber 1984) was an early step toward the definition of the core, but its mathematical analysis, and the clarification of the connections between the various systems, are only now coming to a reasonable closure. The Logic of Typed Feature Structures is the first monograph that brings all the main theoretical ideas into one place where they can be related and compared in a unified setting. Carpenter&apos;s book touches most of the crucial questions of the developments during the decade, provides proofs for central results, and reaches right up to the edge of current research in the field. These contributions al"
J93-3006,J92-2002,0,0.060692,"Missing"
N03-1028,W02-2018,0,0.766415,"on. In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian. However in our case the Hessian has the form Hλ with gradient def = = ∇ 2 Lλ X − {E [F (Y , xk ) × F (Y , xk )] k ∇L0λ = X k −EF (Y , xk ) × EF (Y , xk )} λ F (y k , xk ) − Epλ (Y |xk ) F (Y , xk ) − 2 σ  3 Training Methods Lafferty et al. (2001) used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (Berger et al., 1996; Della Pietra et al., 1997). Those methods are very simple and guaranteed to converge, but as Minka (2001) and Malouf (2002) showed for classification, their convergence is much slower than that of general-purpose convex where the expectations are taken with respect to pλ (Y |xk ). Therefore, every Hessian element, including the diagonal ones, involve the expectation of a product of global feature values. Unfortunately, computing those expectations is quadratic on sequence length, as the forward-backward algorithm can only compute expectations of quantities that are additive along label sequences. We solve both problems by discarding the off-diagonal terms and approximating expectation of the square of a global fea"
N03-1028,W99-0606,0,0.0384522,"ence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is 1 Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is al"
N03-1028,J96-1002,0,0.0886458,"Fs for two reasons. First, the size of the Hessian is dim(λ)2 , leading to unacceptable space and time requirements for the inversion. In such situations, it is common to use instead the (inverse of) the diagonal of the Hessian. However in our case the Hessian has the form Hλ with gradient def = = ∇ 2 Lλ X − {E [F (Y , xk ) × F (Y , xk )] k ∇L0λ = X k −EF (Y , xk ) × EF (Y , xk )} λ F (y k , xk ) − Epλ (Y |xk ) F (Y , xk ) − 2 σ  3 Training Methods Lafferty et al. (2001) used iterative scaling algorithms for CRF training, following earlier work on maximumentropy models for natural language (Berger et al., 1996; Della Pietra et al., 1997). Those methods are very simple and guaranteed to converge, but as Minka (2001) and Malouf (2002) showed for classification, their convergence is much slower than that of general-purpose convex where the expectations are taken with respect to pλ (Y |xk ). Therefore, every Hessian element, including the diagonal ones, involve the expectation of a product of global feature values. Unfortunately, computing those expectations is quadratic on sequence length, as the forward-backward algorithm can only compute expectations of quantities that are additive along label seque"
N03-1028,W02-1001,0,0.496954,"f-speech tagging task. In the present work, we show that CRFs beat all reported single-model NP chunking results on the standard evaluation dataset, and are statistically indistinguishable from the previous best performer, a voting arrangement of 24 forward- and backward-looking support-vector classifiers (Kudo and Matsumoto, 2001). To obtain these results, we had to abandon the original iterative scaling CRF training algorithm for convex optimization algorithms with better convergence properties. We provide detailed comparisons between training methods. The generalized perceptron proposed by Collins (2002) is closely related to CRFs, but the best CRF training methods seem to have a slight edge over the generalized perceptron. 2 Conditional Random Fields We focus here on conditional random fields on sequences, although the notion can be used more generally (Lafferty et al., 2001; Taskar et al., 2002). Such CRFs define conditional probability distributions p(Y |X) of label sequences given input sequences. We assume that the random variable sequences X and Y have the same length, and use x = x1 · · · xn and y = y1 · · · yn for the generic input sequence and label sequence, respectively. A CRF on ("
N03-1028,P02-1034,0,0.553588,"Missing"
N03-1028,P02-1036,0,0.0441498,"Missing"
N03-1028,N01-1025,0,0.802562,"inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is 1 Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based method. not as closely tied to the accuracy metrics of interest if the actual data was not generated by the model, as is always the case in practice. However, since sequential cla"
N03-1028,W95-0107,0,0.0728199,"tasks in language and biology are often described as mappings from input sequences to sequences of labels encoding the analysis. In language processing, examples of such tasks include part-of-speech tagging, named-entity recognition, and the task we shall focus on here, shallow parsing. Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction (Abney, 1991). The paradigmatic shallowparsing problem is NP chunking, which finds the nonrecursive cores of noun phrases called base NPs. The pioneering work of Ramshaw and Marcus (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (Freitag and McCallum, 2000; Kupiec, 1992) or multilevel Markov models (Bikel e"
N03-1028,W96-0213,0,0.426973,"umptions. For instance, it is not practical to make the label at a given position depend on a window on the input sequence as well as the surrounding labels, since the inference problem for the corresponding graphical model would be intractable. Non-independent features of the inputs, such as capitalization, suffixes, and surrounding words, are important in dealing with words unseen in training, but they are difficult to represent in generative models. The sequential classification approach can handle many correlated features, as demonstrated in work on maximum-entropy (McCallum et al., 2000; Ratnaparkhi, 1996) and a variety of other linear classifiers, including winnow (Punyakanok and Roth, 2001), AdaBoost (Abney et al., 1999), and support-vector machines (Kudo and Matsumoto, 2001). Furthermore, they are trained to minimize some function related to labeling error, leading to smaller error in practice if enough training data are available. In contrast, generative models are trained to maximize the joint probability of the training data, which is 1 Ramshaw and Marcus (1995) used transformation-based learning (Brill, 1995), which for the present purposes can be tought of as a classification-based meth"
N03-1028,W97-0301,0,0.0708036,"Missing"
N03-1028,P02-1035,0,0.132123,"Missing"
N03-1028,W00-0726,0,0.845552,"ity recognition, and the task we shall focus on here, shallow parsing. Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction (Abney, 1991). The paradigmatic shallowparsing problem is NP chunking, which finds the nonrecursive cores of noun phrases called base NPs. The pioneering work of Ramshaw and Marcus (1995) introduced NP chunking as a machine-learning problem, with standard datasets and evaluation metrics. The task was extended to additional phrase types for the CoNLL2000 shared task (Tjong Kim Sang and Buchholz, 2000), which is now the standard evaluation task for shallow parsing. Most previous work used two main machine-learning approaches to sequence labeling. The first approach relies on k-order generative probabilistic models of paired input sequences and label sequences, for instance hidden Markov models (HMMs) (Freitag and McCallum, 2000; Kupiec, 1992) or multilevel Markov models (Bikel et al., 1999). The second approach views the sequence labeling problem as a sequence of classification problems, one for each of the labels in the sequence. The classification result at each position may depend on the"
N03-1028,C00-2137,0,0.0591791,"Missing"
N03-1028,J95-4004,0,\N,Missing
P05-1012,P99-1059,0,0.387427,"rithm and requires only two additional binary variables that specify the direction of the item (either gathering left dependents or gathering right dependents) and whether an item is complete (available to gather more dependents). Figure 2 shows the algorithm schematically. As with normal CKY parsing, larger elements are created bottom-up from pairs of smaller elements. Eisner showed that his algorithm is sufficient for both searching the space of dependency parses and, with slight modification, finding the highest scoring tree y for a given sentence x under the edge factorization assumption. Eisner and Satta (1999) give a cubic algorithm for lexicalized phrase structures. However, it only works for a limited class of languages in which tree spines are regular. Furthermore, there is a large grammar constant, which is typically in the thousands for treebank parsers. 2.3 Online Learning Figure 3 gives pseudo-code for the generic online learning setting. A single training instance is considered on each iteration, and parameters updated by applying an algorithm-specific update rule to the instance under consideration. The algorithm in Figure 3 returns an averaged weight vector: an auxiliary weight vector v i"
P05-1012,W05-1506,0,0.0536885,"(xt , yt ) − s(xt , y 0 ) ≥ L(yt , y 0 ) ∀y 0 ∈ bestk (xt ; w(i) ) reducing the number of constraints to the constant k. We tested various values of k on a development data set and found that small values of k are sufficient to achieve close to best performance, justifying our assumption. In fact, as k grew we began to observe a slight degradation of performance, indicating some 94 overfitting to the training data. All the experiments presented here use k = 5. The Eisner (1996) algorithm can be modified to find the k-best trees while only adding an additional O(k log k) factor to the runtime (Huang and Chiang, 2005). A more common approach is to factor the structure of the output space to yield a polynomial set of local constraints (Taskar et al., 2003; Taskar et al., 2004). One such factorization for dependency trees is min w(i+1) − w(i) s.t. s(l, j) − s(k, j) ≥ 1 ∀(l, j) ∈ yt , (k, j) ∈ / yt It is trivial to show that if these O(n2 ) constraints are satisfied, then so are those in (1). We implemented this model, but found that the required training time was much larger than the k-best formulation and typically did not improve performance. Furthermore, the k-best formulation is more flexible with respec"
P05-1012,A00-2018,0,0.037919,"online largemargin multi-class training (Crammer and Singer, 2003; Crammer et al., 2003) on top of efficient parsing techniques for dependency trees (Eisner, 1996). The trained parsers achieve a competitive dependency accuracy for both English and Czech with no language specific enhancements. 1 Introduction Research on training parsers from annotated data has for the most part focused on models and training algorithms for phrase structure parsing. The best phrase-structure parsing models represent generatively the joint probability P (x, y) of sentence x having the structure y (Collins, 1999; Charniak, 2000). Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set. However, generative models make complicated and poorly justified independence assumptions and estimations, so we might expect better performance from discriminatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the train"
P05-1012,J93-2004,0,0.059141,".1 and show a significant improvement in parsing times on the testing data. The major limiting factor of the system is its restriction to features over single dependency attachments. Often, when determining the next depen95 dent for a word, it would be useful to know previous attachment decisions and incorporate these into the features. It is fairly straightforward to modify the parsing algorithm to store previous attachments. However, any modification would result in an asymptotic increase in parsing complexity. 3 Experiments We tested our methods experimentally on the English Penn Treebank (Marcus et al., 1993) and on the Czech Prague Dependency Treebank (Hajiˇc, 1998). All experiments were run on a dual 64-bit AMD Opteron 2.4GHz processor. To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999). We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for evaluation. Currently the system has 6, 998, 447 features. Each instance only uses a tiny fraction of these features making sparse vector calculations possible. Our"
P05-1012,P04-1014,0,0.0387577,"Missing"
P05-1012,C04-1010,0,0.281245,"ohn hit the ball with the 2 System Description bat 2.1 Figure 1: An example dependency tree. case, all algorithms are easily extendible to typed structures. The following work on dependency parsing is most relevant to our research. Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees. Yamada and Matsumoto (2003) trained support vector machines (SVM) to make parsing decisions in a shift-reduce dependency parser. As in Ratnaparkhi’s parser, the classifiers are trained on individual decisions rather than on the overall quality of the parse. Nivre and Scholz (2004) developed a history-based learning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003). Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhi’s parser, our parsers are trained to maximize the accuracy of the overall tree. Our approach is related to those of"
P05-1012,P04-1015,0,0.507943,"developed a history-based learning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003). Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhi’s parser, our parsers are trained to maximize the accuracy of the overall tree. Our approach is related to those of Collins and Roark (2004) and Taskar et al. (2004) for phrase structure parsing. Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm. However, to use parse features with sufficient history, their parsing algorithm must prune heuristically most of the possible parses. Taskar et al. (2004) formulate the parsing problem in the large-margin structured classification setting (Taskar et al., 2003), but are limited to parsing sentences of 15 words or less due to computation time. Though these approaches represent good first steps towards discriminatively-trained parsers, th"
P05-1012,W96-0213,0,0.442753,"98). All experiments were run on a dual 64-bit AMD Opteron 2.4GHz processor. To create dependency structures from the Penn Treebank, we used the extraction rules of Yamada and Matsumoto (2003), which are an approximation to the lexicalization rules of Collins (1999). We split the data into three parts: sections 02-21 for training, section 22 for development and section 23 for evaluation. Currently the system has 6, 998, 447 features. Each instance only uses a tiny fraction of these features making sparse vector calculations possible. Our system assumes POS tags as input and uses the tagger of Ratnaparkhi (1996) to provide tags for the development and evaluation sets. Table 2 shows the performance of the systems that were compared. Y&M2003 is the SVM-shiftreduce parsing model of Yamada and Matsumoto (2003), N&S2004 is the memory-based learner of Nivre and Scholz (2004) and MIRA is the the system we have described. We also implemented an averaged perceptron system (Collins, 2002) (another online learning algorithm) for comparison. This table compares only pure dependency parsers that do Y&M2003 N&S2004 Avg. Perceptron MIRA English Accuracy 90.3 87.3 90.6 90.9 Root 91.6 84.3 94.0 94.2 Complete 38.4 30."
P05-1012,P99-1065,0,0.0331032,"Missing"
P05-1012,W02-1001,0,0.820528,"ic update rule to the instance under consideration. The algorithm in Figure 3 returns an averaged weight vector: an auxiliary weight vector v is maintained that accumulates 93 Training data: T = {(xt , yt )}Tt=1 1. w0 = 0; v = 0; i = 0 2. for n : 1..N 3. for t : 1..T 4. w(i+1) = update w(i) according to instance (xt , yt ) 5. v = v + w(i+1) 6. i= i+1 7. w = v/(N ∗ T ) Figure 3: Generic online learning algorithm. the values of w after each iteration, and the returned weight vector is the average of all the weight vectors throughout training. Averaging has been shown to help reduce overfitting (Collins, 2002). 2.3.1 MIRA Crammer and Singer (2001) developed a natural method for large-margin multi-class classification, which was later extended by Taskar et al. (2003) to structured classification: min kwk s.t. s(x, y) − s(x, y 0 ) ≥ L(y, y 0 ) ∀(x, y) ∈ T , y 0 ∈ dt(x) where L(y, y 0 ) is a real-valued loss for the tree y 0 relative to the correct tree y. We define the loss of a dependency tree as the number of words that have the incorrect parent. Thus, the largest loss a dependency tree can have is the length of the sentence. Informally, this update looks to create a margin between the correct depe"
P05-1012,P02-1035,0,0.00976901,"Missing"
P05-1012,N03-1028,1,0.261004,"t phrase-structure parsing models represent generatively the joint probability P (x, y) of sentence x having the structure y (Collins, 1999; Charniak, 2000). Generative parsing models are very convenient because training consists of computing probability estimates from counts of parsing events in the training set. However, generative models make complicated and poorly justified independence assumptions and estimations, so we might expect better performance from discriminatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the training data, performed nearly as well as generative Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984). Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005). Yet, they can be parsed i"
P05-1012,W04-3201,0,0.0652711,"rning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learning algorithms of Crammer and Singer (2003) and Crammer et al. (2003). Unlike the SVM parser of Yamada and Matsumoto (2003) and Ratnaparkhi’s parser, our parsers are trained to maximize the accuracy of the overall tree. Our approach is related to those of Collins and Roark (2004) and Taskar et al. (2004) for phrase structure parsing. Collins and Roark (2004) presented a linear parsing model trained with an averaged perceptron algorithm. However, to use parse features with sufficient history, their parsing algorithm must prune heuristically most of the possible parses. Taskar et al. (2004) formulate the parsing problem in the large-margin structured classification setting (Taskar et al., 2003), but are limited to parsing sentences of 15 words or less due to computation time. Though these approaches represent good first steps towards discriminatively-trained parsers, they have not yet been able"
P05-1012,P04-1054,0,0.375314,"minatively trained models, as has been shown for other tasks like document classification (Joachims, 2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the training data, performed nearly as well as generative Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984). Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005). Yet, they can be parsed in O(n3 ) time (Eisner, 1996). Therefore, dependency parsing is a potential “sweet spot” that deserves investigation. We focus here on projective dependency trees in which a word is the parent of all of its arguments, and dependencies are non-crossing with respect to word order (see Figure 1). However, there are cases where crossing dependencies may occur, as is the case for Czech (Hajiˇc, 1998). Edges in a dependency tree may be typed (for instance to indicate grammatical"
P05-1012,W03-3023,0,0.944924,"for instance to indicate grammatical function). Though we focus on the simpler non-typed 91 Proceedings of the 43rd Annual Meeting of the ACL, pages 91–98, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics and Czech treebank data. root John hit the ball with the 2 System Description bat 2.1 Figure 1: An example dependency tree. case, all algorithms are easily extendible to typed structures. The following work on dependency parsing is most relevant to our research. Eisner (1996) gave a generative model with a cubic parsing algorithm based on an edge factorization of trees. Yamada and Matsumoto (2003) trained support vector machines (SVM) to make parsing decisions in a shift-reduce dependency parser. As in Ratnaparkhi’s parser, the classifiers are trained on individual decisions rather than on the overall quality of the parse. Nivre and Scholz (2004) developed a history-based learning model. Their parser uses a hybrid bottom-up/topdown linear-time heuristic parser and the ability to label edges with semantic types. The accuracy of their parser is lower than that of Yamada and Matsumoto (2003). We present a new approach to training dependency parsers, based on the online large-margin learni"
P05-1012,P05-1067,0,0.371058,"2002) and shallow parsing (Sha and Pereira, 2003). Ratnaparkhi’s conditional maximum entropy model (Ratnaparkhi, 1999), trained to maximize conditional likelihood P (y|x) of the training data, performed nearly as well as generative Dependency trees are an alternative syntactic representation with a long history (Hudson, 1984). Dependency trees capture important aspects of functional relationships between words and have been shown to be useful in many applications including relation extraction (Culotta and Sorensen, 2004), paraphrase acquisition (Shinyama et al., 2002) and machine translation (Ding and Palmer, 2005). Yet, they can be parsed in O(n3 ) time (Eisner, 1996). Therefore, dependency parsing is a potential “sweet spot” that deserves investigation. We focus here on projective dependency trees in which a word is the parent of all of its arguments, and dependencies are non-crossing with respect to word order (see Figure 1). However, there are cases where crossing dependencies may occur, as is the case for Czech (Hajiˇc, 1998). Edges in a dependency tree may be typed (for instance to indicate grammatical function). Though we focus on the simpler non-typed 91 Proceedings of the 43rd Annual Meeting of"
P05-1012,J04-4004,0,\N,Missing
P05-1012,C96-1058,0,\N,Missing
P05-1012,J03-4003,0,\N,Missing
P05-1061,C00-1030,0,0.0121031,"(IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involvMost relation extraction systems focus on the specific problem of extracting binary relations, such as the employee of relation or protein-protein interaction relation. Very little work has been done in recognizing and extracting more complex relations. We define a complex relation as any n-ary relation among n typed entities. The relation is defined by the schema (t1 , . . . , tn ) where ti ∈ T are entity types. An i"
P05-1061,J02-3001,0,0.0155543,"ere has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is always associated with a verb in the sentence that specifies the relation type. Though this system is very general, it is limited by the fact that the design ignores relations not expressed by a verb, as the employee of relation in“John Smith, CEO of Inc. Corp., announced he will resign”. Most relation extraction systems work primarily on a sentential level and never consider relations that cross sentences or paragraphs. Since current data sets typically only annotate intra-sentence relations, this has not yet proven to be a problem. Complex Relations Recall that a complex n"
P05-1061,N01-1025,0,0.012856,"lated, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 1 Introduction Most research on text information extraction (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involvMost relation extraction systems focus on the specific problem of extracting binary relations, such as the employee of relation or protein-p"
P05-1061,A00-2030,0,0.00792299,"hallow parse representation. This approach — enumerating all possible entity pairs and classifying each as positive or negative — is the standard method in relation extraction. The main differences among systems are the choice 492 of trainable classifier and the representation for instances. For binary relations, this approach is quite tractable: if the relation schema is (t 1 , t2 ), the number of potential instances is O(|t1 ||t2 |), where |t |is the number of entity mentions of type t in the text under consideration. One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. Once this augmentation is made, any standard parser can be trained and then run on new sentences to extract new relations. Miller et al. show such an approach can yield good results. However, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrat"
P05-1061,P04-1055,0,0.0134126,"rsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is always associated with a verb in the sentence"
P05-1061,W04-2401,0,0.0245476,"owever, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrating relation extraction with parsing, the system cannot consider long-range dependencies due to the local parsing constraints of current probabilistic parsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume"
P05-1061,J96-1002,0,0.0349033,"Missing"
P07-1056,W06-1615,1,0.840251,"ain adaptation, see the ICML 2006 Workshop on Structural Knowledge Transfer for Machine Learning (http://gameairesearch.uta. edu/) and the NIPS 2006 Workshop on Learning when test and training inputs have different distribution (http://ida. first.fraunhofer.de/projects/different06/) 2 The dataset will be made available by the authors at publication time. Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 440–447, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics respondence learning (SCL) domain adaptation algorithm (Blitzer et al., 2006) for use in sentiment classification. A key step in SCL is the selection of pivot features that are used to link the source and target domains. We suggest selecting pivots based not only on their common frequency but also according to their mutual information with the source labels. For data as diverse as product reviews, SCL can sometimes misalign features, resulting in degradation when we adapt between domains. In our second extension we show how to correct misalignments using a very small number of labeled instances. Second, we evaluate the A-distance (Ben-David et al., 2006) between domain"
P07-1056,W04-3237,0,0.102684,"e any SCL features. We note that the baseline is very close to just using the source domain classifier, because with only 50 target domain instances we do not have enough data to relearn all of the parameters in w. As we can see, though, relearning the 50 parameters in v is quite helpful. The corrected model always improves over the baseline for every possible transfer, including those not shown in the figure. The idea of using the regularizer of a linear model to encourage the target parameters to be close to the source parameters has been used previously in domain adaptation. In particular, Chelba and Acero (2004) showed how this technique can be effective for capitalization adaptation. The major difference between our approach and theirs is that we only penalize deviation from the source parameters for the weights v of projected features, while they work with the weights of the original features only. For our small amount of labeled target data, attempting to penalize w using ws performed no better than our baseline. Because we only need to learn to ignore projections that misalign features, we can make much better use of our labeled data by adapting only 50 parameters, rather than 200,000. Table 3 su"
P07-1056,N04-1001,0,0.0662213,"L w0 xi + v0 θxi , yi + λ||w||2 + µ||v||2 , min w,v i where y is the label. The weight vector w ∈ Rd weighs the original features, while v ∈ Rk weighs the projected features. Ando and Zhang (2005) and Blitzer et al. (2006) suggest λ = 10−4 , µ = 0, which we have used in our results so far. Suppose now that we have trained source model weight vectors ws and vs . A small amount of target domain data is probably insufficient to significantly change w, but we can correct v, which is much smaller. We augment each labeled target instance xj with the label assigned by the source domain classifier (Florian et al., 2004; Blitzer et al., 2006). Then we solve P minw,v j L (w0 xj + v0 θxj , yj ) + λ||w||2 +µ||v − vs ||2 . Since we don’t want to deviate significantly from the source parameters, we set λ = µ = 10−1 . Figure 2 shows the corrected SCL-MI model using 50 target domain labeled instances. We chose this number since we believe it to be a reasonable amount for a single engineer to label with minimal effort. For reasons of space, for each target domain 444 dom  model base books dvd electron kitchen average 8.9 8.9 8.3 10.2 9.1 base +targ 9.0 8.9 8.5 9.9 9.1 scl scl-mi 7.4 7.8 6.0 7.0 7.1 5.8 6.1 5.5 5.6"
P07-1056,P05-1015,0,0.338398,"on all the domains. Using the proxy A-distance as a criterion, we observe that we would choose one domain from either books or DVDs, but not both, since then we would not be able to adequately cover electronics or kitchen appliances. Similarly we would also choose one domain from either electronics or kitchen appliances, but not both. 7 Related Work Sentiment classification has advanced considerably since the work of Pang et al. (2002), which we use as our baseline. Thomas et al. (2006) use discourse structure present in congressional records to perform more accurate sentiment classification. Pang and Lee (2005) treat sentiment analysis as an ordinal ranking problem. In our work we only show improvement for the basic model, but all of these new techniques also make use of lexical features. Thus we believe that our adaptation methods could be also applied to those more refined models. While work on domain adaptation for sentiment classifiers is sparse, it is worth noting that other researchers have investigated unsupervised and semisupervised methods for domain adaptation. The work most similar in spirit to ours that of Turney (2002). He used the difference in mutual information with two human-selecte"
P07-1056,W02-1011,0,0.0789665,". This is not the case for sentiment classification, however. Therefore, we require that pivot features also be good predictors of the source label. Among those features, we then choose the ones with highest mutual information to the source label. Table 1 shows the set-symmetric SCL, not SCL-MI SCL-MI, not SCL book one &lt;num&gt; so all a must a wonderful loved it very about they like weak don’t waste awful good when highly recommended and easy Table 1: Top pivots selected by SCL, but not SCLMI (left) and vice-versa (right) 2004). On the polarity dataset, this model matches the results reported by Pang et al. (2002). When we report results with SCL and SCL-MI, we require that pivots occur in more than five documents in each domain. We set k, the number of singular vectors of the weight matrix, to 50. 4 Experiments with SCL and SCL-MI differences between the two methods for pivot selection when adapting a classifier from books to kitchen appliances. We refer throughout the rest of this work to our method for selecting pivots as SCL-MI. 3 Dataset and Baseline We constructed a new dataset for sentiment domain adaptation by selecting Amazon product reviews for four different product types: books, DVDs, elect"
P07-1056,W06-1639,0,0.425813,"tes well with the potential for adaptation of a classifier from one domain to another. This measure could for instance be used to select a small set of domains to annotate whose trained classifiers would transfer well to many other domains. 1 Introduction Sentiment detection and classification has received considerable attention recently (Pang et al., 2002; Turney, 2002; Goldberg and Zhu, 2004). While movie reviews have been the most studied domain, sentiment analysis has extended to a number of new domains, ranging from stock message boards to congressional floor debates (Das and Chen, 2001; Thomas et al., 2006). Research results have been 440 deployed industrially in systems that gauge market reaction and summarize opinion from Web pages, discussion boards, and blogs. With such widely-varying domains, researchers and engineers who build sentiment classification systems need to collect and curate data for each new domain they encounter. Even in the case of market analysis, if automatic sentiment classification were to be used across a wide range of domains, the effort to annotate corpora for each domain may become prohibitive, especially since product features change over time. We envision a scenario"
P07-1056,P02-1053,0,0.129593,"the result by 100. We refer to this quantity as the proxy A-distance. When it is 100, the two domains are completely distinct. When it is 0, the two domains are indistinguishable using a linear classifier. Figure 3 is a correlation plot between the proxy A-distance and the adaptation error. Suppose we wanted to label two domains out of the four in such a a completely unsupervised manner. Then he classified documents according to various functions of these mutual information scores. We stress that our method improves a supervised baseline. While we do not have a direct comparison, we note that Turney (2002) performs worse on movie reviews than on his other datasets, the same type of data as the polarity dataset. 14 12 BE, BK DK Adaptation Loss 10 DE 8 BD 6 4 EK 2 0 60 65 70 75 80 85 90 95 100 Proxy A-distance Figure 3: The proxy A-distance between each domain pair plotted against the average adaptation loss of as measured by our baseline system. Each pair of domains is labeled by their first letters: EK indicates the pair electronics and kitchen. way as to minimize our error on all the domains. Using the proxy A-distance as a criterion, we observe that we would choose one domain from either book"
P10-1149,C92-2082,0,0.142165,"Missing"
P10-1149,D09-1098,0,0.0267187,"similar class labels. In other words, the label smoothness assumption (see Section 2.2) holds on such graphs. We applied the same graph construction process on a subset of the Freebase dataset consisting of topics from 18 randomly selected domains: astronomy, automotive, biology, book, business, 1476 chemistry, comic books, computer, film, food, geography, location, people, religion, spaceflight, tennis, travel, and wine. The topics in this subset were further filtered so that only cell-value nodes with frequency 10 or more were retained. We call the resulting graph Freebase-1 (see Table 1). Pantel et al. (2009) have made available a set of gold class-instance pairs derived from Wikipedia, which is downloadable from http://ow.ly/13B57. From this set, we selected all classes which had more than 10 instances overlapping with the Freebase graph constructed above. This resulted in 23 classes, which along with their overlapping instances were used as the gold standard set for the experiments in this section. Experimental results with 2 and 10 seeds (labeled nodes) per class are shown in Figure 3. From the figure, we see that that LP-ZGL and Adsorption performed comparably on this dataset, with MAD signifi"
P10-1149,D09-1025,0,0.4392,"ars (Hearst, 1992; Riloff and Jones, 1999; Etzioni et al., 2005; Talukdar et al., 2006; Van Durme and Pas¸ca, 2008). Starting with a few seed instances for some classes, these methods, through analysis of unstructured text, extract new instances of the same class. This line of work has evolved to incorporate ideas from graph-based semi-supervised learning in extraction from semi-structured text (Wang and Cohen, 2007), and in combining extractions from free text and from structured sources (Talukdar et al., 2008). The benefits of combining multiple sources have also been demonstrated recently (Pennacchiotti and Pantel, 2009). We make the following contributions: Graph-based semi-supervised learning (SSL) algorithms have been successfully used to extract class-instance pairs from large unstructured and structured text collections. However, a careful comparison of different graph-based SSL algorithms on that task has been lacking. We compare three graph-based SSL algorithms for class-instance acquisition on a variety of graphs constructed from different domains. We find that the recently proposed MAD algorithm is the most effective. We also show that class-instance extraction can be significantly improved by adding"
P10-1149,W09-3209,0,0.0146352,"ised edge weights; and R is an n × m matrix of per-node label prior, if any, with Rl representing the lth column of R. As in Adsorption, MAD allows labels on seed nodes to change. In case of MAD, the three random-walk cont , and pabnd , defined by probabilities, pinj v , pv v Adsorption on each node are folded inside the ma0 trices S, L , and R, respectively. The optimization problem in (3) can be solved with an efficient iterative algorithm described in detail by Talukdar and Crammer (2009). These three algorithms are all easily parallelizable in a MapReduce framework (Talukdar et al., 2008; Rao and Yarowsky, 2009), which makes them suitable for SSL on large datasets. Additionally, all three algorithms have similar space and time complexity. Gender ··· Male Male Male ··· LP-ZGL Adsorption MAD 0.725 0.65 0.575 0.5 23 x 2 23 x 10 Amount of Supervision (# classes x seeds per class) Figure 3: Comparison of three graph transduction methods on a graph constructed from the Freebase dataset (see Section 3.1), with 23 classes. All results are averaged over 4 random trials. In each group, MAD is the rightmost bar. Freebase (Metaweb Technologies, 2009)2 is a large collaborative knowledge base. The knowledge base h"
P10-1149,W06-2919,1,0.405186,"Missing"
P10-1149,D08-1061,1,0.724626,"To overcome these difficulties, seed-based information extraction methods have been developed over the years (Hearst, 1992; Riloff and Jones, 1999; Etzioni et al., 2005; Talukdar et al., 2006; Van Durme and Pas¸ca, 2008). Starting with a few seed instances for some classes, these methods, through analysis of unstructured text, extract new instances of the same class. This line of work has evolved to incorporate ideas from graph-based semi-supervised learning in extraction from semi-structured text (Wang and Cohen, 2007), and in combining extractions from free text and from structured sources (Talukdar et al., 2008). The benefits of combining multiple sources have also been demonstrated recently (Pennacchiotti and Pantel, 2009). We make the following contributions: Graph-based semi-supervised learning (SSL) algorithms have been successfully used to extract class-instance pairs from large unstructured and structured text collections. However, a careful comparison of different graph-based SSL algorithms on that task has been lacking. We compare three graph-based SSL algorithms for class-instance acquisition on a variety of graphs constructed from different domains. We find that the recently proposed MAD al"
P10-2036,afonso-etal-2002-floresta,0,0.11378,"Missing"
P10-2036,P04-1061,0,0.703784,"Association for Computational Linguistics probability of a sentence with POS tags x and dependency tree y is given by: lows. Section 2 and 3 review the models and several previous approaches for learning them. Section 4 describes learning with PR. Section 5 describes experiments across 12 languages and Section 6 analyzes the results. For additional details on this work see Gillenwater et al. (2010). 2 pθ (x, y) = proot (r(x))× Y pstop (f alse |yp , yd , yvs )pchild (yc |yp , yd , yvc )× y∈y Y Parsing Model (1) The models we use are based on the generative dependency model with valence (DMV) (Klein and Manning, 2004). For a sentence with tags x, the root POS r(x) is generated first. Then the model decides whether to generate a right dependent conditioned on the POS of the root and whether other right dependents have already been generated for this head. Upon deciding to generate a right dependent, the POS of the dependent is selected by conditioning on the head POS and the directionality. After stopping on the right, the root generates left dependents using the mirror reversal of this process. Once the root has generated all its dependents, the dependents generate their own dependents in the same manner."
P10-2036,D07-1072,0,0.0669081,"Missing"
P10-2036,P08-1099,0,0.0203037,"7 can be viewed as a mixed-norm penalty on the features φcpi or φcpij : the sum corresponds to an `1 norm and the max to an `∞ norm. Thus, the quantity we want to minimize fits precisely into the PR penalty framework. Formally, to optimize the PR objective, we complete the following E-step: Learning with Sparse Posteriors We would like to penalize models that predict a large number of distinct dependency types. To enforce this penalty, we use the posterior regularization (PR) framework (Graça et al., 2007). PR is closely related to generalized expectation constraints (Mann and McCallum, 2007; Mann and McCallum, 2008; Bellare et al., 2009), and is also indirectly related to a Bayesian view of learning with constraints on posteriors (Liang et al., 2009). The PR framework uses constraints on posterior expectations to guide parameter estimation. Here, PR allows a natural and tractable representation of sparsity constraints based on edge type counts that cannot easily be encoded in model parameters. We use a version of PR where the desired bias is a penalty on the log likelihood (see Ganchev et al. (2010) for more details). For a distribution pθ , we define a penalty as the (generic) β-norm of expectations of"
P10-2036,J93-2004,0,0.0352961,"Missing"
P10-2036,dzeroski-etal-2006-towards,0,0.00769833,"Missing"
P10-2036,P07-1035,0,0.0350278,"Missing"
P10-2036,simov-etal-2002-building,0,0.016325,"Missing"
P10-2036,P06-1072,0,0.11495,"m is: min q(Y),ξcp KL(q(Y) k pθ (Y|X)) + σ X ξcp cp s. t. (9) ξcp ≤ Eq [φ(X, Y)] where ξcp corresponds to the maximum expectation of φ over all instances of c and p. Note that the projection problem can be solved efficiently in the dual (Ganchev et al., 2010). min KL(q(Y) k pθ (Y|X)) + σ ||Eq [φ(X, Y)]||β (6) q 5 where σ is the strength of the regularization. PR seeks to maximize L(θ) minus this penalty term. The resulting objective can be optimized by a variant of the EM (Dempster et al., 1977) algorithm used to optimize L(θ). Experiments We evaluate on 12 languages. Following the example of Smith and Eisner (2006), we strip punctuation from the sentences and keep only sentences of length ≤ 10. For simplicity, for all models we use the “harmonic” initializer from Klein 196 Model DMV 2-1 2-2 3-3 4-4 EM 45.8 45.1 54.4 55.3 55.1 PR 62.1 62.7 62.9 64.3 64.4 Type PR-S PR-S PR-S PR-S PR-AS σ 140 100 80 140 140 Learning Method PR-S (σ = 140) LN families SLN TieV & N PR-AS (σ = 140) DD (α = 1, λ learned) Table 1: Attachment accuracy results. Column 1: Vc Vs used for the E-DMV models. Column 3: Best PR result for each model, which is chosen by applying each of the two types of constraints (PR-S and PR-AS) and tr"
P10-2036,N09-1012,0,0.608174,"Missing"
P10-2036,N09-1009,0,\N,Missing
P11-1080,P98-1012,0,0.8945,"reference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical"
P11-1080,D08-1029,0,0.392855,"clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-document coreference. Since many related methods use clustering, there are a number of distributed clustering algorithms that may help scale these approaches. Datta et al. (2006) propose an algorithm for distributed kmeans. Chen et al. (2010) describe a parallel spectral clustering algorithm. We use the Subsqu"
P11-1080,D08-1031,0,0.029693,"Missing"
P11-1080,W02-1001,0,0.0443813,"chosen mention l from its current entity es to a randomly chosen entity et . For such a proposal, the log-model ratio is: X X p(e0 ) log = ψa (l, m) + ψr (l, n) p(e) m∈et n∈es X X − ψa (l, n) − ψr (l, m) (2) n∈es m∈et Note that since only the factors between mention l and mentions in es and et are involved in this computation, the acceptance probability of each proposal is calculated efficiently. In general, the model may contain arbitrarily complex set of features over pairs of mentions, with parameters associated with them. Given labeled data, these parameters can be learned by Perceptron (Collins, 2002), which uses the MAP configuration according to the model (ˆ e). There also exist more efficient training algorithms such as SampleRank (McCallum et al., 2009; Wick et al., 2009b) that update parameters during inference. However, we only focus on inference in this work, and the only parameter that we set manually is the bias b, which indirectly influences the number of entities in ˆ e. Unless specified otherwise, in this work the initial configuration for MCMC is the singleton configuration, i.e. all entities have a size of 1. This MCMC inference technique, which has been used in McCallum and"
P11-1080,N07-1011,1,0.856675,"and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al., 2007; Poon et al., 2008; Wellner et al., 2004; Wick et al., 2009a). These models contain pairwise factors between all pairs of mentions capturing similarity between them. Many of these models also enforce transitivity and enable features over 793 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 793–803, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics ... The Physiological Basis of Politics,” by Kevin B. Smith, Douglas Oxley, Matthew Hibbing... ...during the late 60's and early 70's, Kevin Smith worked with several l"
P11-1080,P07-1094,0,0.00627756,"p is accepted with the following Metropolis-Hastings acceptance probability: !   0 ) 1/t q(e) p(e α(e, e0 ) = min 1, (1) p(e) q(e0 ) 1 Number of possible entities is Bell(n) in the number of mentions, i.e. number of partitions of n items where t is the annealing temperature parameter. MCMC chains efficiently explore the highdensity regions of the probability distribution. By slowly reducing the temperature, we can decrease the entropy of the distribution to encourage convergence to the MAP configuration. MCMC has been used for optimization in a number of related work (McCallum et al., 2009; Goldwater and Griffiths, 2007; Changhe et al., 2004). The proposal function moves a randomly chosen mention l from its current entity es to a randomly chosen entity et . For such a proposal, the log-model ratio is: X X p(e0 ) log = ψa (l, m) + ψr (l, n) p(e) m∈et n∈es X X − ψa (l, n) − ψr (l, m) (2) n∈es m∈et Note that since only the factors between mention l and mentions in es and et are involved in this computation, the acceptance probability of each proposal is calculated efficiently. In general, the model may contain arbitrarily complex set of features over pairs of mentions, with parameters associated with them. Give"
P11-1080,N04-1002,0,0.713737,"ber of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al., 2007; Poon et al., 2"
P11-1080,P07-1107,0,0.135709,"Missing"
P11-1080,D09-1120,0,0.0130614,"arge dataset, demonstrating the scalability of our approach. 1 Introduction Given a collection of mentions of entities extracted from a body of text, coreference or entity resolution consists of clustering the mentions such that two mentions belong to the same cluster if and only if they refer to the same entity. Solutions to this problem are important in semantic analysis and knowledge discovery tasks (Blume, 2005; Mayfield et al., 2009). While significant progress has been made in within-document coreference (Ng, 2005; Culotta et al., 2007; Haghighi and Klein, 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010), the larger problem of cross-document coreference has not received as much attention. Unlike inference in other language processing tasks that scales linearly in the size of the corpus, the hypothesis space for coreference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years."
P11-1080,N10-1061,0,0.223887,"g the scalability of our approach. 1 Introduction Given a collection of mentions of entities extracted from a body of text, coreference or entity resolution consists of clustering the mentions such that two mentions belong to the same cluster if and only if they refer to the same entity. Solutions to this problem are important in semantic analysis and knowledge discovery tasks (Blume, 2005; Mayfield et al., 2009). While significant progress has been made in within-document coreference (Ng, 2005; Culotta et al., 2007; Haghighi and Klein, 2007; Bengston and Roth, 2008; Haghighi and Klein, 2009; Haghighi and Klein, 2010), the larger problem of cross-document coreference has not received as much attention. Unlike inference in other language processing tasks that scales linearly in the size of the corpus, the hypothesis space for coreference grows superexponentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have bi"
P11-1080,P08-1099,1,0.562065,"esent in real-world applications. By utilizing parallelism across machines, our method can run on very large datasets simply by increasing the number of machines used. Second, approaches that use clustering are limited to using pairwise distance functions for which additional supervision and features are difficult to incorporate. In addition to representing features from all of the related work, graphical models can also use more complex entity-wide features (Culotta et al., 2007; Wick et al., 2009a), and parameters can be learned using supervised (Collins, 2002) or semisupervised techniques (Mann and McCallum, 2008). Finally, the inference for most of the related approaches is greedy, and earlier decisions are not revisited. Our technique is based on MCMC inference and simulated annealing, which are able to escape local maxima. 801 7 Conclusions Motivated by the problem of solving the coreference problem on billions of mentions from all of the newswire documents from the past few decades, we make the following contributions. First, we introduce distributed version of MCMC-based inference technique that can utilize parallelism to enable scalability. Second, we augment the model with hierarchical variables"
P11-1080,W03-0405,0,0.174727,"ent coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervis"
P11-1080,D10-1004,0,0.0166425,"Missing"
P11-1080,P05-1020,0,0.0171755,"Missing"
P11-1080,P04-1076,0,0.246817,"ion for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and the clustering algorithm used. Daum´e III and Marcu (2005) propose a generative approach to supervised clustering, and Haghighi and Klein (2010) use entity profiles to assist within-"
P11-1080,W04-2406,0,0.00791484,"between pairs of contexts, which are then used for clustering. One of the first approaches to cross-document coreference (Bagga and Baldwin, 1998) uses an idf-based cosine-distance scoring function for pairs of contexts, similar to the one we use. Ravin and Kazi (1999) extend this work to be somewhat scalable by comparing pairs of contexts only if the mentions are deemed “ambiguous” using a heuristic. Others have explored multiple methods of context similarity, and concluded that agglomerative clustering provides effective means of inference (Gooi and Allan, 2004). Pedersen et al. (2006) and Purandare and Pedersen (2004) integrate second-order co-occurrence of words into the similarity function. Mann and Yarowsky (2003) use biographical facts from the Web as features for clustering. Niu et al. (2004) incorporate information extraction into the context similarity model, and annotate a small dataset to learn the parameters. A number of other approaches include various forms of hand-tuned weights, dictionaries, and heuristics to define similarity for name disambiguation (Blume, 2005; Baron and Freedman, 2008; Popescu et al., 2008). These approaches are greedy and differ in the choice of the distance function and"
P11-1080,C10-2121,0,0.735477,"ng the entities. mawat, 2004) to manage the distributed computation. This approach to distribution is equivalent to inference with all mentions and entities on a single machine with a restricted proposer, but is faster since it exploits independencies to propose multiple jumps simultaneously. By restricting the jumps as described above, the acceptance probability calculation is exact. Partitioning the entities and proposing local jumps are restrictions to the single-machine proposal distribution; redistribution stages ensure the equivalent Markov chains are still irreducible. See Singh et al. (2010) for more details. 4 Hierarchical Coreference Model The proposal function for MCMC-based MAP inference presents changes to the current entities. Since we use MCMC to reach high-scoring regions of the hypothesis space, we are interested in the changes that improve the current configuration. But as the number of mentions and entities increases, these fruitful samples become extremely rare due to the blowup in the possible space of configurations, resulting in rejection of a large number of proposals. By distributing as described in the previous section, we propose samples in parallel, improving"
P11-1080,W99-0202,0,0.773167,"nentially with the number of mentions. Consequently, most of the current approaches are developed on small datasets containing a few thousand mentions. We believe that cross-document coreference resolution is most useful when applied to a very large set of documents, such as all the news articles published during the last 20 years. Such a corpus would have billions of mentions. In this paper we propose a model and inference algorithms that can scale the cross-document coreference problem to corpora of that size. Much of the previous work in cross-document coreference (Bagga and Baldwin, 1998; Ravin and Kazi, 1999; Gooi and Allan, 2004; Pedersen et al., 2006; Rao et al., 2010) groups mentions into entities with some form of greedy clustering using a pairwise mention similarity or distance function based on mention text, context, and document-level statistics. Such methods have not been shown to scale up, and they cannot exploit cluster features that cannot be expressed in terms of mention pairs. We provide a detailed survey of related work in Section 6. Other previous work attempts to address some of the above concerns by mapping coreference to inference on an undirected graphical model (Culotta et al."
P11-1080,D10-1001,0,0.0176713,"Missing"
P11-1080,C98-1012,0,\N,Missing
P16-1059,P14-2013,0,0.0616602,"Missing"
P16-1059,E06-1002,0,0.032353,"Missing"
P16-1059,D14-1082,0,0.0920162,"Missing"
P16-1059,D13-1184,0,0.290758,"Missing"
P16-1059,Q15-1011,0,0.637018,"Missing"
P16-1059,D07-1074,0,0.290695,"Missing"
P16-1059,C10-1032,0,0.131313,"Missing"
P16-1059,N10-1112,0,0.0320538,"Missing"
P16-1059,P11-1095,0,0.0716315,"Missing"
P16-1059,D12-1010,0,0.0551626,"Missing"
P16-1059,P13-2006,0,0.660807,"Missing"
P16-1059,D11-1072,0,0.902982,"Missing"
P16-1059,P14-1036,0,0.0411391,"Missing"
P16-1059,Q15-1036,1,0.768241,"Missing"
P16-1059,Q15-1023,0,0.458092,"Missing"
P16-1059,K16-1025,0,0.390478,"Missing"
P16-1059,N15-1026,0,0.345326,"Missing"
P16-1059,P11-1138,0,0.876108,"Missing"
P16-1059,P15-1032,0,0.0241318,"Missing"
P83-1021,P81-1036,0,0.0355489,"Missing"
P83-1021,J82-3002,1,0.394361,"Missing"
P83-1021,J81-4003,1,\N,Missing
P83-1021,J80-1003,0,\N,Missing
P84-1027,P84-1008,0,0.0587832,"the effect of the CRP and the old HFC without any notion of the NCP. Our final version of the HFC merely requires that the parent's head features be the generalization of the head (i head) In the case of parents with one head child, this final HFC reduces to the old HFC requiring identity; it reduces to the newer one, however, in cases {like coordinate structures} where there are several head constituents. Furthermore, by utilizing an order structure on the domain of constants C, it may be possible to model that troublesome coordination phenomenon, number agreement in coordinated noun phrases [8,15]. 7. Conclusion We have approached the problem of analyzing the meaning of grammar formalisms by applying the techniques of denotational semantics taken from work on the semantics of computer languages. This has enabled us to In addition to formal insights, linguistic insights have also been gleaned from this work. First of all, we note 'that while the systems make crucial use of unification, generalization is also a well-defined notion therein and might indeed be quite useful. In fact, it was this availability of the generalization operation that suggested a simplified account of coordination"
P84-1027,J81-4003,1,0.82584,"Missing"
P84-1027,P83-1021,1,0.835293,"b 1 agr hum) = pl . . . . }) (&quot;many knights storms Cornwall&quot;, T) Applications We have used the techniques discussed here to analyze the feature systems of GPSG [15], LFG [2] and PATR-II [17]. All of them turn out to be specializations of our domain D of descriptions. Figure 1 provides a summary of two of the most critical formal properties of context-free-based grammar formalisms, the domains of their feature systems (full F~ finite elements of F, or elements of F based on nonrecursive domain equations) and whether the contextfree skeletons of grammars are constrained to be off-line paraeable [13] thereby guaranteeing decidability. 0 &lt; i &lt; r. and only then applying the rule constraints--now viewed as constraining parts of a single description. This is done by the indexing and combination steps described below. The 127 DCG-IIa PATR-II LFG GPSGb FEATURE SYSTEM full finite finite nonrec. CF SKELETON full full off-line full aDCGs based on Prolog-lI which allows cyclic terms. bHPSG, the current Hewlett-Packard implementation derived from GPSG, would come more accurately under the PATR-II classification. Figure 1: Summary of Grammar System Properties Though notational differences and some gr"
P84-1027,P84-1075,1,0.949993,"al possibilities--among them that of imposing a type discipline on the use of a formalism, with all the attendant advantages of compile-time error checking, modularity, and optimized compilation techniques for grammar rules, and that of relating grammar formalisms to other knowledge representation languages [l]. As a specific contribution of this study, we elucidate the nature of the feature systems used in augmented phrasestructure grammar formalisms, in particular those of recent versions of generalized phrase structure grammar (GPSG) [5,15], lexical functional grammar (LFG) [2] and PATR-II [18,17]; we find that the mathematical structures developed for this purpose contain an operation of feature generalization, not available in those grammar formalisms, that can be used to give a partial account of the effect of coordination on syntactic features. Just as studies in the semantics of programming languages start by giving semantics for simple languages, so we will start with simple grammar formalisms that capture the essence of the method without an excess of obscuring detail. The present enterprise should be contrasted with studies of the generative capacity of formalisms using the tec"
P84-1027,P83-1020,0,\N,Missing
P85-1017,P84-1027,1,0.877842,"Missing"
P85-1017,P83-1021,1,0.793267,"tion - - virtual-copy array~ ~ which requires O(logn) time to access or update an array with highest used index 139 mo,.~,2~~ skeleton ~ _ m I &quot;~ environment °-° I,ef I,.f Daniel own I ref I ref Spot initial Daniel update Figure 2: Molecule X unification . / ~ ~ &lt; &gt; ~ ° - &quot;_L_ &lt;-/ J °- &apos;:_L_ III xa y~d Figure 3: Unification of Two Molecules 140 &lt;&gt; Spot assume, for ease of exposition, that a derived instance is always a subdag of the unification of its right parent with a subdag of its left parent. This is the case for most common parsing algorithms, although more general schemes are possible [7]. If the original Boyer-Moore scheme were used directly, the environment for a derived instance would consist of pointers to left and right parent instances, as well as a list of the updates needed to build the current instance from its parents. As noted before, this method requires a worst-case O(Idl} search to find the updates that result in the current instance. The present scheme relies on the fact that in the great majority of cases no instance is both the left and the right ancestor of another instance. [ shall assume for the moment that this is always the case. In Section 9 this restric"
P85-1017,P84-1075,0,\N,Missing
P85-1017,P79-1001,0,\N,Missing
P88-1010,P87-1019,0,0.0239514,"imilar comments apply to Webber&apos;s work [29]. In addition, none of the aforementioned accounts has been concerned with as wide a range of phenomena as is currently haw died in Candide. 12 One of the motivations for our work has been to see how Barwise&apos;s directinterpretation approach could be turned into a two-stage one in which phrases are first &quot;compiled&quot; into conditional interpretations, which are then &quot;executed&quot; by applying pragmatic-discharge rules. Finally, several other computational systems developed recently are concerned with interactions between context and meaning, especially Pundit [8,21] and Tacitus [17,16]. Both these systems have emphasized solutions to such difficult pragmatic problems as reference resolution. In particular, the Pundit project has made notable progress on the question of resolving missing arguments, while the Tacitus group has done the same for questions involving the determination of implicit relations. In Candide, solutions to such pragmatic problems should be encoded in the procedures that discharge assumptions; in future versions of the system the discharge procedures might be improved applying some of the techniques developed in this other work. What"
P88-1010,J82-1003,0,0.103189,"onal nature of semantic analysis, without unduly constraining the order in which pragmatic decisions are made. To achieve this goal, we introduce the idea of a conditional interpretation: one that depends upon a set of assumptions about subsequent pragmatic processing. Conditional interpretations are constructed compositionally according to a set of declaratively specified interpretation rules. The mechanism can handle a wide range of pragmatic phenomena and their interactions. 1 Introduction Compositional systems of semantic interpretation, while logically and computationally very attractive [6,20,26], seem unable to cope with the fact that the syntactic form of a a utterance is not the only source of information about its meaning. Contextual information--information about the world and about the history of a discourse-influences not only an utterance&apos;s meaning, but even its preferred syntactic analysis [3,5,7,16]. Of course, context also influences the interpretation (or meaning in context) of the utterance, in which, for example, referring expressions have been resolved. One possible solution is to move to an integrated system of semantic and pragmatic interpretation, defined recursively"
P88-1010,P86-1004,0,\N,Missing
P88-1010,H86-1011,0,\N,Missing
P88-1010,P81-1028,0,\N,Missing
P88-1010,P88-1012,0,\N,Missing
P88-1010,P83-1007,0,\N,Missing
P88-1010,J86-3001,0,\N,Missing
P88-1010,H86-1003,0,\N,Missing
P89-1002,E89-1032,0,0.151303,"ieve the underlying method to be more broadly applicable. A variant of our method is used in Van Noord&apos;s BUG (Bottom-Up Generator) system, part of MiMo2, an experimental machine translation system for translating international news items of Teletext, which uses a Prolog version of PATI~-II similar to that of Hirsh (1987). According to Martin Kay (personal communication), the STREP machine translation project at the Center for the Study of Language and Information uses a version of our algorithm to generate with respect to grammars based on head-driven phrase-structure grammar (HPSG). Finally, Calder et al. (1989) report on a generation algorithm for unification categorial grammar that appears to be a special case of ours. 3 Problems with Generators Existing Existing generation algorithms have efficiency or termination problems with respect to certain classes of grammars. We review the problems of both top-down and bottom-up regimes in this section. 3.1 Problems with Top-Down Generators Consider a naive top-down generation mechanism that takes as input the semantics to generate from and a corresponding syntactic category and builds a complete tree, top-down, left-to-right by applying rules of the gramm"
P89-1002,1988.tmi-1.12,0,0.737425,"ction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for pa"
P89-1002,J87-1005,1,0.63248,"than necessary because the distribution of store elements among the subject and complements of a verb does not check whether the variable bound by a store element actually appears in the semantics of the phrase to which it is being assigned, leading to many dead ends in the generation process. Also, the rules are sound for generation but not for analysis, because they do not enforce the constraint that every occurrence of a variable in logical form be outscoped by the variable&apos;s binder. Adding appropriate side conditions to the rules, following the constraints discussed by Hobbs and Shieber (Hobbs and Shieber, 1987) would not be difficult. 6.3 Postponing Lexical Choice As it stands, the generation algorithm chooses particular lexical forms on-line. This approach can lead to a certain amount of unnecessary nondeterminism. For instance, the choice of verb form might depend on syntactic features of the verb&apos;s subject available only after the subject has been generated. This nondeterminism can be eliminated by deferring lexical choice to a postprocess. The generator will yield a list of lexical items instead of a list of words. To this list a small phonological front end is applied. BUG uses such a mechanism"
P89-1002,P83-1021,1,0.865954,"iven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with r"
P89-1002,P85-1018,1,0.928086,"ortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance of a uniform architecture for parsing and generation--just as the extended Earley parser (Shieber, 1985b) and the bottom-up generator were instances of the generalized Earley deduction architecture= our efforts to date have been aimed foremost toward the development of the algorithm for generation alone. We will have little to say about its relation to parsing, leaving such questions for later research.1 2 Applicability of the Algorithm As does the Earley-based generator, the new algorithm assumes that the grammar is a unificationbased or logic grammar with a phrase-structure backbone and complex nonterminMs. Furthermore, and again consistent with previous work, we assume that the nonterminals"
P89-1002,C88-2128,1,0.294441,"n Earley deduction generator (Shieber, 1988), it allows use of semantically nonmonotonic grammars, yet unlike topdown methods, it also permits left-recursion. The enabling design feature of the algorithm is its implicit traversal of the analysis tree for the string being generated in a semantic-head-driven fashion. 1 Introduction The problem of generating a well-formed naturallanguage expression from an encoding of its meaning possesses certain properties which distinguish it from the converse problem of recovering a meaning encoding from a given natural-language expression. In previous work (Shieber, 1988), however, one of us attempted to characterize these differing properties in such a way that a single uniform architecture, appropriately parameterized, might be used for both natural-language processes. In particular, we developed an architecture inspired by the Earley deduction work of Pereira and Warren (1983) but which generalized that work allowing for its use in both a parsing and generation mode merely by setting the values of a small number of parameters. As a method for generating natural-language expressions, the Earley deduction method is reasonably successful along certain dimensio"
P89-1002,C88-2150,0,0.450177,"ns, the Earley deduction method is reasonably successful along certain dimensions. It is quite simple, general in its applicability to a range of unification-based and logic grammar formalisms, and uniform, in that it places only one restriction (discussed below) on the form of the linguistic analyses allowed by the grammars used in generation. In particular, generation from grammars with recursions whose welbfoundedness relies t D e p a r t m e n t of Linguistics Rijksuniversiteit Utrecht Utrecht, Netherlands on lexical information will terminate; top-down generation regimes such as those of Wedekind (1988) or Dymetman and Isabelle (1988) lack this property, discussed further in Section 3.1. Unfortunately, the bottom-up, left-to-right processing regime of Earley generation--as it might be called--has its own inherent frailties. Efficiency considerations require that only grammars possessing a property of semantic monotonicity can be effectively used, and even for those grammars, processing can become overly nondeterministic. The algorithm described in this paper is an attempt to resolve these problems in a satisfactory manner. Although we believe that this algorithm could be seen as an instance"
P89-1002,J81-4003,1,\N,Missing
P89-1019,J87-1005,0,0.264232,"&quot; by noting that but this is &quot;ill-formed&quot; because variable m occurs as an argument of DISLIKED outside the scope of its binder Vm. 2 As for Examples (2) and (3), the argument is similar: wide scope for the logical form of the a... noun phrase would leave an occurrence of the variable that the logical form of every.., binds outside the scope of this quantifier. For lack of an official name in the literature for this constraint, I will call it here the free-variable constraint. In accounts of scoping possibilities based on quantifier raising or storage (Cooper, 1983; van Eijck, 1985; May, 1985; Hobbs and Shieber, 1987), the free-variable constraint is enforced either by keeping track of the set of free variables FREE(q) in each ralsable (storable) term q and when z E FREE(q) blocking the raising of q from any context Bz.t in which z is bound by some binder B, or by checking after all applications of raising (unstoring) that no variable occurs outside the scope of its binder. The argument above is often taken to be so obvions and uncontroversial that it warrants only a remark in passing, if any (Cooper, 1983; Reinhart, 1983; Partee and Bach, 1984; May, 1985; van Riernsdijk and Williams, 1986; Williams, 1986;"
P89-1019,P88-1005,0,0.129474,"examples t h a t follow, the pronoun a n d its intended antecedent are italicized. As usual, starred exampies are supposed to be u n g r m t i c a L 2In fact, this is & perfectly good ope~t well-formed for~ nmla a n d therefore the precise formulation of the constraint is more delicate t h a n seems to be realized in the literature. 1 An Obvious Constraint? Treatments of quantifier scope in Montague grammar (Montague, 1973; Dowty et al., 1981; Cooper, 1983), transformational grammar (Reinhart, 1983; May, 1985; Helm, 1982; Roberts, 1987) and computational linguistics (Hobbs and Shieber, 1987; Moran, 1988) have depended implicitly or explicitly on a constraint on possible logical forms to explain why examples 1 such as (1) * A woman who saw every man disliked him are ungrammatical, and why in examples such as (2) Every man saw a friend of his (3) Every admirer of a picture of himself is vain 152 Second, and most relevant to Montague grammar and related approaches, the constraint is formulated in terms of restrictions on formal objects (logical forms) which in turn are related to meanings through a denotation relation. However, compositionaiity as it is commonly understood requires meanings of p"
P89-1019,P88-1010,1,0.770127,"s. 153 The main goal of this paper is to argue that the free-variable constraint is actually a consequence of basic semantic properties that hold in a semantic domain allowing functional application and abstraction, and are thus independent of a particular 10gical-form representation. As a corollary, I will also show that the constraint is better expressed as a restriction on the derivations of meanings of sentences from the meanings of their parts rather than a restriction on logical forms. The resulting system is related to the earlier system of conditional interpretation rules developed by Pollack and Pereira (1988), but avoids that system&apos;s use of formal conditions on the order of assumption discharge. 2 C u r r y &apos; s Calculus of Functionality Work in combinatory logic and the A-calculus is concerned with the elucidation of the basic notion of functionality: how to construct functions, and how to apply functions to their arguments. There is a very large body of results in this area, of which I will need only a very small part. • One of the simplest and most elegant accounts of functionality, originally introduced by Curry and Feys (1968) and further elaborated by other authors (Stenlund, 1972; Lambek, 1"
P91-1032,P85-1018,0,0.279402,"o construct finite-state language models for limiteddomain speech recognition tasks. 1 Motivation Grammars for spoken language systems are subject to the conflicting requirements of language modeling for recognition and of language analysis for sentence interpretation. Current recognition algorithms can most directly use finite-state acceptor (FSA) language models. However, these models are inadequate for language interpretation, since they cannot express the relevant syntactic and semantic regularities. Augmented phrase structure grammar (APSG) formalisms, such as unification-based grammars (Shieber, 1985a), can express many of those regularities, but they are computationally less suitable for language modeling, because of the inherent cost of computing state transitions in APSG parsers. The above problems might be circumvented by using separate grammars for language modeling and language interpretation. Ideally, the recognition grammar should not reject sentences acceptable by the interpretation grammar and it should contain as much as reasonable of the constraints built into the interpretation grammar. 1 At first sight, this requirement may be seen as conflicting with the undecidability of d"
P91-1032,W89-0229,0,0.0825271,"realization of suggestions of Church and Patil (1980; 1982) on algebraic simplifications of CFGs of regular languages. Other work on finite state approximations of phrase structure grammars has typically relied on arbitrary depth cutoffs in rule application. While this is reasonable for psycholinguistic modeling of performance restrictions on center embedding (Pulman, 1986), it does not seem appropriate for speech recognition where the approximating FSA is intended to work as a filter and not reject inputs acceptable by the given grammar. For instance, depth cutoffs in the method described by Black (1989) lead to approximating FSAs whose language is neither a subset nor a superset of the language of the given phrase-structure grammar. In contrast, our method will produce an exact FSA for many interesting grammars generating regular languages, such as those arising from systematic attachment ambiguities (Church and Patil, 1982). It important to note, however, that even when the result FSA accepts the same language, the original grammar is still necessary because interpretaS -+ aSb I ~ for the nonregular language a&apos;~b&apos;~,n &gt; O. This grammar is mapped by the algorithm into an FSA accepting ~ I a+b"
P91-1032,J82-3004,0,\N,Missing
P91-1032,J89-4001,0,\N,Missing
P92-1017,W89-0209,0,0.263064,"Missing"
P92-1017,H90-1021,0,0.0354795,"Missing"
P92-1017,H91-1060,0,0.019785,"Missing"
P92-1017,C92-2066,1,0.737415,"Missing"
P92-1017,H90-1055,0,0.0301975,"ext W with respect to the grammar G, not The first experiment involves an artificial example used by Lari and Young (1990) in a previous evaluation of the inside-outside algorithm. In this (8). 131 case, training on a bracketed corpus can lead to a good solution while no reasonable solution is found training on raw text only. The initial grammar consists of all possible CNF rules over five nonterminals and the terminals a and b (135 rules), with random rule probabilities. The second experiment uses a naturally occurring corpus and its partially bracketed version provided by the Penn Treebank (Brill et al., 1990). We compare the bracketings assigned by grammars inferred from raw and from bracketed training material with the Penn Treebank bracketings of a separate test set. As shown in Figure 1, with an unbracketed training set W the cross-entropy estimate H(W, GR) remains almost unchanged after 40 iterations (from 1.57 to 1.44) and no useful solution is found. In contrast, with a fully bracketed version C of the same training set, the cross-entropy estimate /~(W, GB) decreases rapidly (1.57 initially, 0.88 after 21 iterations). Similarly, the cross-entropy estimate H ( C , GB) of the bracketed text wi"
P93-1024,A88-1019,0,0.0266565,"between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire text automatically parsed by 183 Hindle&apos;s parser Fidditch (Hindle, 1993). More recently, we have constructed similar tables with the help of a statistical part-of-speech tagger (Church, 1988) and of tools for regular expression pattern matching on tagged corpora (Yarowsky, 1992). We have not yet compared the accuracy and coverage of the two methods, or what systematic biases they might introduce, although we took care to filter out certain systematic errors, for instance the misparsing of the subject of a complement clause as the direct object of a main verb for report verbs like ""say"". We will consider here only the problem of classifying nouns according to their distribution as direct objects of verbs; the converse problem is formally similar. More generally, the theoretical bas"
P93-1024,P93-1022,0,0.0456394,"e models reconstruct missing d a t a in the 1 awe use unweighted averages because we are interested her on how well the noun distributions are approximated by the cluster model. If we were interested on the total information loss of using the asymmetric model to encode a test corpus, we would instead use the weighted average ~,~e~t fnD(t,~ll~,,) where f,, is the relative frequency of n in the test set. 189 verb distribution for n from the cluster centroids close to n. The d a t a for this test was built from the training data for the previous one in the following way, based on a suggestion by Dagan et al. (1993). 104 noun-verb pairs with a fairly frequent verb (between 500 and 5000 occurrences) were randomly picked, and all occurrences of each pair in the training set were deleted. The resulting training set was used to build a sequence of cluster models as before. Each model was used to decide which of two verbs v and v ~ are more likely to appear with a noun n where the (v, n) d a t a was deleted from the training set, and the decisions were compared with the corresponding ones derived from the original event frequencies in the initial d a t a set. The error rate for each model is simply the propor"
P93-1024,P90-1034,0,0.658019,"s proposed by a grammar. It is well known that a simple tabulation of frequencies of certain words participating in certain configurations, for example of frequencies of pairs of a transitive main verb and the head noun of its direct object, cannot be reliably used for comparing the likelihoods of different alternative configurations. The problemis that for large enough corpora the number of possible joint events is much larger than the number of event occurrences in the corpus, so many events are seen rarely or never, making their frequency counts unreliable estimates of their probabilities. Hindle (1990) proposed dealing with the Problem Setting In what follows, we will consider two major word classes, 12 and Af, for the verbs and nouns in our experiments, and a single relation between them, in our experiments the relation between a transitive main verb and the head noun of its direct object. Our raw knowledge about the relation consists of the frequencies f~n of occurrence of particular pairs (v,n) in the required configuration in a training corpus. Some form of text analysis is required to collect such a collection of pairs. The corpus used in our first experiment was derived from newswire"
P93-1024,C92-2066,0,\N,Missing
P93-1024,J92-4003,0,\N,Missing
P94-1038,J92-4003,0,0.0475503,"Missing"
P94-1038,P93-1005,0,0.00927013,"Finally, the similarity-based model m a y be applied to configurations other than bigrams. For trigrams, it is necessary to measure similarity between different conditioning bigrams. This can be done directly, 276 by measuring the distance between distributions of the form P(w31wl, w2), corresponding to different bigrams (wl, w~). Alternatively, and more practically, it would be possible to define a similarity measure between bigrams as a function of similarities between corresponding words in them. Other types of conditional cooccurrence probabilities have been used in probabilistic parsing (Black et al., 1993). If the configuration in question includes only two words, such as P(objectlverb), then it is possible to use the model we have used for bigrams. If the configuration includes more elements, it is necessary to adjust the method, along the lines discussed above for trigrams. for help with his baseline back-off model, and Andre Ljolje and Michael Riley for providing the word lattices for our experiments. Conclusions Similarity-based models suggest an appealing approach for dealing with data sparseness. Based on corpus statistics, they provide analogies between words that often agree with our li"
P94-1038,P93-1022,1,0.847024,"Missing"
P94-1038,H92-1021,0,0.4253,"not possible to estimate probabilities from observed frequencies, a n d s o m e other estimation scheme has to be used. We focus here on a particular kind of configuration, word cooccurrence. Examples of such cooccurrences include relationships between head words in syntactic constructions (verb-object or adjective-noun, for example) and word sequences (n-grams). In commonly used models, the probability estimate for a previously unseen cooccurrence is a function of the probability esti272 Similarity-based estimation was first used for language modeling in the cooccurrence smoothing method of Essen and Steinbiss (1992), derived from work on acoustic model smoothing by Sugawara et al. (1985). We present a different method that takes as starting point the back-off scheme of Katz (1987). We first allocate an appropriate probability mass for unseen cooccurrences following the back-off method. Then we redistribute that mass to unseen cooccurrences according to an averaged cooccurrence distribution of a set of most similar conditioning words, using relative entropy as our similarity measure. This second step replaces the use of the independence assumption in the original back-off model. We applied our method to e"
P94-1038,H93-1050,0,0.0786007,"Missing"
P94-1038,H91-1055,0,0.0409352,"Missing"
P94-1038,P93-1024,1,0.855009,"Missing"
P94-1038,H92-1027,0,0.0497973,"Missing"
P94-1038,C92-2066,0,\N,Missing
P97-1008,J92-4003,0,0.0417534,"Missing"
P97-1008,A88-1019,0,0.0192643,"Missing"
P97-1008,P94-1038,1,0.513543,"e unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the dissimilarity between two probability mass functions (Cover and Thomas, 1991). We can apply it to the conditional distribution P(.[wl) induced by Wl on words in V2: ~V(Wl, W~) ~ ~s(~1 ) P,.(w2lwl) = 7P(w2) + Measures of Similarity T o t a l d i v e r g e n c e to t h e a v e r a g e A related measure is based on the total KL divergence to the average of the two distributions: A(wx, W11)= D (w, wl + ) + D (w~[ wl + w~) 2 7)PsiM(W2lWl), where 7 is determined experimentally (Dagan, Pereira, and Lee, 1994). This represents, in effect, a linear combination of the similarity es58 where tion (Wl ÷ w~)/2 shorthand for the distribu2.3.4 Essen and Steinbiss (1992) introduced confusion probability 2, which estimates the probabil(P(.IwJ + P(.Iw~)) ity that word w~ can be substituted for word Wl: Since D('II-) > O, A(Wl,W~) >_O. Furthermore, letting p(w2) = P(w2[wJ, p'(w2) = P(w2lw~) and C : {w2 : p(w2) > O,p'(w2) > O}, it is straightforward to show by grouping terms appropriately that Pc(w lWl) = w(wl, = ~ , P(wllw2)P(w~[w2)P(w2) w2 P(Wl) A(wi,wb= Unlike the measures described above, wl may not neces"
P97-1008,H92-1021,0,0.12642,"defined it must be the case that P(w2]w~l) > 0 whenever P(w21wl) > 0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of P(w2]w~) that redistribute some probability mass to zerofrequency events. However, using smoothed estimates for P(w2[wl) as well requires a sum over all w2 6 172, which is expensive ['or the large vocabularies under consideration. Given the smoothed denominator distribution, we set Considerable latitude is allowed in defining the set $(Wx), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set 8(wl) = V1. However, it may be desirable to restrict ,5(wl) in some fashion, especially if 1/1 is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w~ such that the dissimilarity between wl and w~ is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace P,.(w2[wl) in the back-off equation (2) with PSIM(W21Wl). However, other variations a r e possible, such as interpolating with the unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard informa"
P97-1008,W96-0104,0,0.0133278,"at P(w2]w~l) > 0 whenever P(w21wl) > 0. Unfortunately, this will not in general be the case for MLEs based on samples, so we would need smoothed estimates of P(w2]w~) that redistribute some probability mass to zerofrequency events. However, using smoothed estimates for P(w2[wl) as well requires a sum over all w2 6 172, which is expensive ['or the large vocabularies under consideration. Given the smoothed denominator distribution, we set Considerable latitude is allowed in defining the set $(Wx), as is evidenced by previous work that can be put in the above form. Essen and Steinbiss (1992) and Karov and Edelman (1996) (implicitly) set 8(wl) = V1. However, it may be desirable to restrict ,5(wl) in some fashion, especially if 1/1 is large. For instance, Dagan. Pereira, and Lee (1994) use the closest k or fewer words w~ such that the dissimilarity between wl and w~ is less than a threshold value t; k and t are tuned experimentally. Now, we could directly replace P,.(w2[wl) in the back-off equation (2) with PSIM(W21Wl). However, other variations a r e possible, such as interpolating with the unigram probability (1 KL divergence Kullback-Leibler (KL) divergence is a standard information-theoretic measure of the"
P97-1008,P93-1024,1,0.599038,"ge model consists of three parts: a scheme for deciding which word pairs require a similarity-based estimate, a method for combining information from similar words, and, of course, a function measuring the similarity between words. We give the details of each of these three parts in the following three sections. We will only be concerned with similarity between words in V1. 1To the best of our ""knowledge, this is the first use of this particular distribution dissimilarity function in statistical language processing. The function itself is implicit in earlier work on distributional clustering (Pereira, Tishby, and Lee, 1993}, has been used by Tishby (p.e.) in other distributional similarity work, and, as suggested by Yoav Freund (p.c.), it is related to results of Hoeffding (1965) on the probability that a given sample was drawn from a given joint distribution. 57 2.1 Discounting and Redistribution Data sparseness makes the maximum likelihood estimate (MLE) for word pair probabilities unreliable. The MLE for the probability of a word pair (Wl, w2), conditional on the appearance of word wl, is simply PML(W2[wl) -- c(wl, w2) c( i) (1) where c(wl, w2) is the frequency of (wl, w2) in the training corpus and c(wl) is"
P97-1008,P92-1053,0,\N,Missing
P98-2147,P83-1014,0,0.0563808,"r is thus used together with other information sources - pronunciation dictionary, phonemic context-dependency model, acoustic model (Bahl et al., 1983; Rabiner and Juang, 1993) - to generate an overall set of transcription hypotheses with corresponding probabilities. General CFGs are computationally too demanding for real-time speech recognition systems, since the amount of work required to expand a recognition hypothesis in the way just described would in general be unbounded for an unrestricted grammar. Therefore, CFGs used in spoken-dialogue applications often represent regular languages (Church, 1983; Brown and Buntschuh, 1994), either by construction or as a result of a finite-state approximation of £ more general CFG (Pereira and Wright, 1997). 1 Assuming that the grammar can be efficiently converted into a finite automaton, appropriate techniques can then be used to combine it with other finite-state recognition models for use in real-time recognition (Mohri et al., 1998b). There is no general algorithm that would map an arbitrary CFG generating a regular language into a corresponding finite-state automaton (UIlian; 1967). However, we will describe a useful class of grammars that can b"
P98-2147,P96-1031,1,0.673028,"each X E S, build M(X) from 3. O p t i m i z a t i o n s , E x p e r i m e n t s a n d Results We have a full implementation of the compilation algorithm presented in the previous section, including the lazy representations that are crucial in reducing the space requirements of speech recognition applications. Our implementation of the compilation algorithm is part of a genera] set of grammar tools, the GRM Library (Mohri, 1998b), currently used in speech processing projects at AT&T Labs. The GRM Library also includes an efficient compilation too] for weighted context-dependent rewrite rules (Mohri and Sproat, 1996) that is used in textto-speech projects at Lucent Bell Laboratories. Since the G R M library is compatible with the FSM general-purpose finite-state machine library (Mohri et al., 1998a), we were able to use the tools provided in FSM library to optimize the input weighted transducers r(G) and the weighted a u t o m a t a in the compilation output. We did several experiments that show the efficiency of our compilation method. A key feature of our grammar compilation method is the representation of the grammar by a weighted transducer that can then be preoptimized using weighted transducer deter"
P98-2147,J97-2003,1,0.915892,"oth of those dynamic grammar changes without recompiling the grammar. 2.2. P r e p r o c e s s i n g Our compilation algorithm operates on a weighted transducer v(G) encoding a factored representation of the weighted CFG G, which is generated from G by a separate preprocessor. This preprocessor is not strictly needed, since we could use a version of the main algorithm that works directly on G. However, preprocessing can improve dramatically the time and space needed for the main compilation step, since the preprocessor uses determinization and minimization algorithms for weighted transducers (Mohri, 1997) to increase the sharing - - fact o r i n g - among grammar rules that start or end the same way. The preprocessing step builds a weighted transducer in which each path corresponds to a grammar rule. Rule X(~ -+ Y1 --.Y~ has a corresponding path that maps X to the sequence I/1 ...Y~ with weight ~. For example, the small CFG in Figure 1 is preprocessed into the compacted transducer shown in Figure 2. 2.3. C o m p i l a t i o n The compilation of weighted left-linear or rightlinear grammars into weighted automata is straightforward (Aho and Ullman, 1973). In the right-linear case, for instance,"
P98-2147,P91-1032,1,\N,Missing
P99-1070,H92-1026,0,0.0936674,"investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending th"
P99-1070,J93-1002,0,0.130127,"mum likelihood parsing. We investigate the precise relationship between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on string"
P99-1070,P98-1035,0,0.0140654,"formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending the classical result to the stochastic case. We also touch on th"
P99-1070,1997.iwpt-1.10,0,0.0211139,"xt-free grammar in which each production is associated with a weight in the interval [0, 1] and such that the weights of the productions from any given nonterminal sum to 1. For instance, the sentence Mary admired the towering strong old oak can be derived using a lexicalized P C F G based on the productions in Figure 1. Production probabilities in the P C F G would reflect the likelihood that a phrase headed by a certain word can be expanded in a certain way. Since it can be difficult to estimate fully these likelihoods, we might restrict ourselves to models based on bilexical relationships (Eisner, 1997), those between pairs of words. The simplest bilexical relationship is a bigram statistic, the fraction of times that ""oak"" follows ""old"". Bilexical relationships for a P C F G include that between the head-word of a phrase and the head-word of a non-head immediate constituent, for instance. In particular, the generation of the above sentence using a P C F G based on Figure 1 would exploit a bilexical statistic between ""towering"" and ""oak"" contained in the weight of the fifth production. This bilexical relationship between 543 ""towering"" and ""oak"" would not be exploited in either a trigram mod"
P99-1070,W97-0301,0,0.0530743,"between these two formalisms, showing that, while they define the same classes of probabilistic languages, they appear to impose different inductive biases. 1 Introduction Current work in stochastic language models and maximum likelihood parsers falls into two main approaches. The first approach (Collins, 1998; Charniak, 1997) uses directly the definition of stochastic grammar, defining the probability of a parse tree as the probability that a certain top-down stochastic generative process produces that tree. The second approach (Briscoe and Carroll, 1993; Black et al., 1992; Magerman, 1994; Ratnaparkhi, 1997; Chelba and Jelinek, 1998) defines the probability of a parse tree as the probability that a certain shiftreduce stochastic parsing automaton outputs that tree. These two approaches correspond to the classical notions of context-free grammars and nondeterministic pushdown automata respectively. It is well known that these two classical formalisms define the same language class. In this paper, we show that probabilistic contextfree grammars (PCFGs) and probabilistic pushdown automata (PPDAs) define the same class of distributions on strings, thus extending the classical result to the stochasti"
P99-1070,J03-4003,0,\N,Missing
P99-1070,C98-1035,0,\N,Missing
P99-1070,P93-1005,0,\N,Missing
Q15-1036,P14-2013,0,0.376245,"2007) and Han and Sun (2011) used na¨ıve Bayes models similar to our baseline. Dredze et al. (2010) and Ratinov et al. (2011) used a ranking support vector machine (SVM), trained to put the correct entity above the remaining ones. More recently He et al. (2013) used stacked autoencoders to learn a context score. While Bunescu and Pasca (2006) and Mihalcea and Csomai (2007) used local models only, others (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Zhao, 2009; Ratinov et al., 2011; Han et al., 2011; Hoffart et al., 2011; He et al., 2013; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) used a coherency model in conjunction with a local model. One popular approach to coherency has been to use variants of the PageRank algorithm (Page et al., 1999) to re-score candidates (He et al., 2013; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). Pershina et al. (2015) achieve the highest accuracy to date on the CoNLL 2003 dataset using a version of Personalized PageRank. Chisholm and Hachey (2015) demonstrate the benefits of supplementing curated Wikipedia data with a large noisy Web corpus. Their model, relying on simple distance scores and an SVM ranker"
Q15-1036,E06-1002,0,0.0528669,"ning and inference in the proposed model can easily be distributed across many servers, allowing it to scale to over 107 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets. 1 Introduction Given a document collection and a knowledge base (KB) of entities, entity resolution, also known as entity disambiguation or entity linking, is the process of mapping each entity mention in a document to the corresponding entity record in the KB (Bunescu and Pasca, 2006; Cucerzan, 2007; Dredze et al., 2010; Hachey et al., 2013). Entity resolution is challenging because referring expressions are often ambiguous on their own and can only be disambiguated by their surrounding context. Consider the name Newcastle; it can refer to the city of Newcastle upon Tyne in UK, to the football (soccer for US readers) club Newcastle United F.C., to a popular beverage (Newcastle Brown Ale), or to several other entities. The ambiguity can only be resolved with appropriate context. Another complicating factor is that no KB is complete, and so a name in a document may refer to"
Q15-1036,Q15-1011,0,0.739221,"008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Zhao, 2009; Ratinov et al., 2011; Han et al., 2011; Hoffart et al., 2011; He et al., 2013; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) used a coherency model in conjunction with a local model. One popular approach to coherency has been to use variants of the PageRank algorithm (Page et al., 1999) to re-score candidates (He et al., 2013; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015). Pershina et al. (2015) achieve the highest accuracy to date on the CoNLL 2003 dataset using a version of Personalized PageRank. Chisholm and Hachey (2015) demonstrate the benefits of supplementing curated Wikipedia data with a large noisy Web corpus. Their model, relying on simple distance scores and an SVM ranker, achieves highly competitive results when trained on both Wikipedia and the Wikilinks corpus (Orr et al., 2013). While all of the systems described thus far rely on supervised learning, Plato is semisupervised, and our experimental results (Section 9) confirm that incorporating unlabeled Web data can lead to significant performance gains. Several recent entity resolution systems (Kataria et al., 2011; Han and Sun, 2012; Houlsby and Ci"
Q15-1036,D07-1074,0,0.140574,"proposed model can easily be distributed across many servers, allowing it to scale to over 107 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets. 1 Introduction Given a document collection and a knowledge base (KB) of entities, entity resolution, also known as entity disambiguation or entity linking, is the process of mapping each entity mention in a document to the corresponding entity record in the KB (Bunescu and Pasca, 2006; Cucerzan, 2007; Dredze et al., 2010; Hachey et al., 2013). Entity resolution is challenging because referring expressions are often ambiguous on their own and can only be disambiguated by their surrounding context. Consider the name Newcastle; it can refer to the city of Newcastle upon Tyne in UK, to the football (soccer for US readers) club Newcastle United F.C., to a popular beverage (Newcastle Brown Ale), or to several other entities. The ambiguity can only be resolved with appropriate context. Another complicating factor is that no KB is complete, and so a name in a document may refer to an entity that"
Q15-1036,C10-1032,0,0.0458095,"can easily be distributed across many servers, allowing it to scale to over 107 entities. We evaluate Plato on three standard datasets for entity resolution. Our approach achieves the best results to-date on TAC KBP 2011 and is highly competitive on both the CoNLL 2003 and TAC KBP 2012 datasets. 1 Introduction Given a document collection and a knowledge base (KB) of entities, entity resolution, also known as entity disambiguation or entity linking, is the process of mapping each entity mention in a document to the corresponding entity record in the KB (Bunescu and Pasca, 2006; Cucerzan, 2007; Dredze et al., 2010; Hachey et al., 2013). Entity resolution is challenging because referring expressions are often ambiguous on their own and can only be disambiguated by their surrounding context. Consider the name Newcastle; it can refer to the city of Newcastle upon Tyne in UK, to the football (soccer for US readers) club Newcastle United F.C., to a popular beverage (Newcastle Brown Ale), or to several other entities. The ambiguity can only be resolved with appropriate context. Another complicating factor is that no KB is complete, and so a name in a document may refer to an entity that In this paper we pres"
Q15-1036,D09-1120,0,0.0382872,"the uncertainty about the mention Newcastle. Since a coherency model introduces dependencies between the resolutions of all the mentions in a document, it is seen as a global model, while mention and context models are usually referred to as local (Ratinov et al., 2011). Coherency models typically have an increased inference cost, as they require access to the relevant entity relationships in the KB. Plato does not include a full coherency component. Instead, mentions in a given document are sorted into coreference clusters by a simple within-document coreference algorithm similar to that of Haghighi and Klein (2009). Each coreference cluster is then resolved to the KB independently of the resolution of the other clusters in the document. The context features for each mention cluster in our model include the names of other referring phrases in the document. Since many referring phrases are unambiguous, our hypothesis is that such context can capture much of the discourse coherence usually represented by a coherency model. Plato detects and links both nominal and named mentions, but following previous work, we evaluate it only on the resolution of gold named entity mentions to either KB or NIL. We train Pl"
Q15-1036,P11-1095,0,0.0591031,"Missing"
Q15-1036,D12-1010,0,0.111162,"Missing"
Q15-1036,P13-2006,0,0.551016,"7; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Nguyen and Cao, 2008) on the entity resolution problem focused on linking named entities to the relevant Wikipedia pages (also known as Wikification). Most of these systems resolved mentions by defining a similarity score between mention contexts 508 and Wikipedia page contents. Mihalcea and Csomai (2007) and Han and Sun (2011) used na¨ıve Bayes models similar to our baseline. Dredze et al. (2010) and Ratinov et al. (2011) used a ranking support vector machine (SVM), trained to put the correct entity above the remaining ones. More recently He et al. (2013) used stacked autoencoders to learn a context score. While Bunescu and Pasca (2006) and Mihalcea and Csomai (2007) used local models only, others (Cucerzan, 2007; Milne and Witten, 2008; Kulkarni et al., 2009; Ferragina and Scaiella, 2010; Han and Zhao, 2009; Ratinov et al., 2011; Han et al., 2011; Hoffart et al., 2011; He et al., 2013; Alhelbawy and Gaizauskas, 2014; Pershina et al., 2015) used a coherency model in conjunction with a local model. One popular approach to coherency has been to use variants of the PageRank algorithm (Page et al., 1999) to re-score candidates (He et al., 2013; Al"
Q15-1036,D11-1072,0,0.264206,"Missing"
Q15-1036,D11-1140,0,0.0156772,"tive impact on load-balancing, while at the same time ensuring that most names will only be in one or a few clusters. Each cluster is then assigned to a server. Phrases such as Washington that evoke many entities may be distributed across multiple servers. Plato can also be made more responsive and resilient to server failure by replicating entity models across multiple servers. 6 Related Work Entity resolution is a key step in many language-processing tasks such as text classification (Gabrilovich and Markovitch, 2007), information extraction (Lin et al., 2012) and grounded semantic parsing (Kwiatkowski et al., 2011). It can also help upstream tasks such as part-of-speech tagging, parsing, and coreference resolution (Finin et al., 2009; Mayfield et al., 2009) as it provides a link to world knowledge such as entity types, aliases, and gender. Early work (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Nguyen and Cao, 2008) on the entity resolution problem focused on linking named entities to the relevant Wikipedia pages (also known as Wikification). Most of these systems resolved mentions by defining a similarity score between mention contexts 508 and Wikipedia p"
Q15-1036,W12-3016,0,0.104613,"that penalizes large clusters as they have a negative impact on load-balancing, while at the same time ensuring that most names will only be in one or a few clusters. Each cluster is then assigned to a server. Phrases such as Washington that evoke many entities may be distributed across multiple servers. Plato can also be made more responsive and resilient to server failure by replicating entity models across multiple servers. 6 Related Work Entity resolution is a key step in many language-processing tasks such as text classification (Gabrilovich and Markovitch, 2007), information extraction (Lin et al., 2012) and grounded semantic parsing (Kwiatkowski et al., 2011). It can also help upstream tasks such as part-of-speech tagging, parsing, and coreference resolution (Finin et al., 2009; Mayfield et al., 2009) as it provides a link to world knowledge such as entity types, aliases, and gender. Early work (Bunescu and Pasca, 2006; Cucerzan, 2007; Mihalcea and Csomai, 2007; Milne and Witten, 2008; Nguyen and Cao, 2008) on the entity resolution problem focused on linking named entities to the relevant Wikipedia pages (also known as Wikification). Most of these systems resolved mentions by defining a simi"
Q15-1036,J93-2004,0,0.0565288,"In addition to the above sources, we also used anchors of Wikipedia pages as aliases if they occurred more than 500 times. Unlabeled data was used to reestimate the parameters τ e using Equation 4; however, we did not introduce any new aliases. Context Features To extract context features, all documents were processed as follows. The free text in each document was POS-tagged and dependencyparsed using a parser that is a reimplementation of the MaltParser (Nivre et al., 2007) with a linear kernel SVM. When trained on Sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank (Marcus et al., 1993), our parser achieves an unlabeled attachment score (UAS) of 88.24 and a labeled attachment score (LAS) of 84.69 on WSJ Section 22. Named mentions (such as Barack Obama) and common noun phrases (such as the president) were identified using a simple rulebased tagger with rules over POS tag sequences and dependency parses. We then used a withindocument coreference resolver comparable to that of Haghighi and Klein (2009) to cluster all identified mentions. As context features in our model, we used the phrases of all mentions in each coreference cluster in the document. We did not differentiate be"
Q15-1036,P11-1138,0,0.297247,"with a very large unlabeled corpus of Web documents. The use of unlabeled data enables us to obtain a better estimate of feature distributions and discover new features that are not present in the (labeled) training data. Plato scales up easily to very large KBs with millions of entities and includes NIL detection as a natural by-product of inference. We named our system after the Greek philosopher because the system’s inference of real underlying entities from imperfect evidence reminds us of Plato’s Theory of Forms. Previous entity resolution studies (Milne and Witten, 2008; Cucerzan, 2007; Ratinov et al., 2011; Hoffart et al., 2011; Hachey et al., 2013) have typically relied on three main components: a mention model, a context model, and a coherency model. The mention model, perhaps the most important component (Hachey et al., 2013), estimates the prior belief that a particular phrase refers to a particular entity 503 Transactions of the Association for Computational Linguistics, vol. 3, pp. 503–515, 2015. Action Editor: Noah Smith. Submission batch: 4/2015; Revision batch: 6/2015; Published 9/2015. c 2015 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. in the KB."
Q15-1036,N15-1026,0,\N,Missing
W06-1615,I05-1006,0,0.0338683,"model can be easily combined with all other domain adaptation techniques (Section 7.2). We are simply inducing a feature representation that generalizes well across domains. This feature representation can then be used in all the techniques described above. The key difference between the previous four pieces of work and our own is the use of unlabeled data. We do not require labeled training data in the new domain to demonstrate an improvement over our baseline models. We believe this is essential, since many domains of application in natural language processing have no labeled training data. Lease and Charniak (2005) adapt a WSJ parser to biomedical text without any biomedical treebanked data. However, they assume other labeled resources in the target domain. In Section 7.3 we give similar parsing results, but we adapt a source domain tagger to obtain the PoS resources. 9 Conclusion Structural correspondence learning is a marriage of ideas from single domain semi-supervised learning and domain adaptation. It uses unlabeled data and frequently-occurring pivot features from both source and target domains to find correspondences among features from these domains. Finding correspondences involves estimating t"
W06-1615,J93-2004,0,0.0648391,"Missing"
W06-1615,H05-1124,1,0.0923218,"ments we use a version of the discriminative online large-margin learning algorithm MIRA (Crammer et al., 2006). MIRA learns and outputs a linear classification score, s(x, y; w) = w · f (x, y), where the feature representation f can contain arbitrary features of the input, including the correspondence features described earlier. In particular, MIRA aims to learn weights so that the score of correct output, yt , for input xt is separated from the highest scoring incorrect outputs2 , with a margin proportional to their Hamming losses. MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b). As with any structured predictor, we need to factor the output space to make inference tractable. We use a first-order Markov factorization, allowing for an efficient Viterbi inference procedure. 2 7 Empirical Results All the results we present in this section use the MIRA tagger from Section 5.3. The ASO and structural correspondence results also use projection features learned using ASO and SCL. Section 7.1 presents results comparing structural correspondence learning with the supervised baseline and ASO in the case where we have no labeled"
W06-1615,P05-1012,1,0.17897,"ments we use a version of the discriminative online large-margin learning algorithm MIRA (Crammer et al., 2006). MIRA learns and outputs a linear classification score, s(x, y; w) = w · f (x, y), where the feature representation f can contain arbitrary features of the input, including the correspondence features described earlier. In particular, MIRA aims to learn weights so that the score of correct output, yt , for input xt is separated from the highest scoring incorrect outputs2 , with a margin proportional to their Hamming losses. MIRA has been used successfully for both sequence analysis (McDonald et al., 2005a) and dependency parsing (McDonald et al., 2005b). As with any structured predictor, we need to factor the output space to make inference tractable. We use a first-order Markov factorization, allowing for an efficient Viterbi inference procedure. 2 7 Empirical Results All the results we present in this section use the MIRA tagger from Section 5.3. The ASO and structural correspondence results also use projection features learned using ASO and SCL. Section 7.1 presents results comparing structural correspondence learning with the supervised baseline and ASO in the case where we have no labeled"
W06-1615,N04-1043,0,0.193212,"more direct ways of applying structural correspondence To the best of our knowledge, SCL is the first method to use unlabeled data from both domains for domain adaptation. By using just the unlabeled data from the target domain, however, we can view domain adaptation as a standard semisupervised learning problem. There are many possible approaches for semisupservised learning in natural language processing, and it is beyond the scope of this paper to address them all. We chose to compare with ASO because it consistently outperforms cotraining (Blum and Mitchell, 1998) and clustering methods (Miller et al., 2004). We did run experiments with the top-k version of ASO (Ando and Zhang, 2005a), which is inspired by cotraining but consistently outperforms it. This did not outperform the supervised method for domain adaptation. We speculate that this is because biomedical and financial data are quite different. In such a situation, bootstrapping techniques are likely to introduce too much noise from the source domain to be useful. Structural correspondence learning is most similar to that of Ando (2004), who analyzed a 127 learning when we have labeled data from both source and target domains. In particular"
W06-1615,P03-1021,0,0.0658229,"Missing"
W06-1615,P05-1001,0,0.125762,"which behave in the same way for discriminative learning in both domains. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner. Even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in a compact way. There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003). In this work we choose to use the technique of structural learning (Ando and Zhang, 2005a; Ando and Zhang, 2005b). Structural learning models the correlations which are most useful for semi-supervised learning. We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task. Here, Discriminative learning methods are widely used in natural language processing. These methods work best when their training and test data are drawn from the same distribution. For many NLP tasks, however, we are confronted with n"
W06-1615,P04-3013,0,0.0814238,"Missing"
W06-1615,P93-1024,1,0.0933909,"eatures from different domains by modeling their correlations with pivot features. Pivot features are features which behave in the same way for discriminative learning in both domains. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner. Even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in a compact way. There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003). In this work we choose to use the technique of structural learning (Ando and Zhang, 2005a; Ando and Zhang, 2005b). Structural learning models the correlations which are most useful for semi-supervised learning. We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task. Here, Discriminative learning methods are widely used in natural language processing. These methods work best when their trai"
W06-1615,W96-0213,0,0.0962513,"els from a resourcerich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. 1 Introduction Discriminative learning methods are ubiquitous in natural language processing. Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (Ratnaparkhi, 1996; Sha and Pereira, 2003). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use"
W06-1615,N03-1027,0,0.0505862,"). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (Roark and Bacchiani, 2003; Florian et al., 2004; Chelba 1 Structural learning is different from learning with structured outputs, a common paradigm for discriminative natural language processing models. To avoid terminological confusion, we refer throughout the paper to a specific structural learning method, alternating structural optimization (ASO) (Ando and Zhang, 2005a). 120 Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 120–128, c Sydney, July 2006. 2006 Association for Computational Linguistics (a) Wall Street Journal DT JJ VBZ The clash is CC NN IN and"
W06-1615,P04-1007,0,0.0262199,"espondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. 1 Introduction Discriminative learning methods are ubiquitous in natural language processing. Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (Ratnaparkhi, 1996; Sha and Pereira, 2003). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (Roark and Bacchiani, 2003; Florian et al., 2004; Chelba 1 Structural learning i"
W06-1615,J92-4003,0,0.146823,"respondences among features from different domains by modeling their correlations with pivot features. Pivot features are features which behave in the same way for discriminative learning in both domains. Non-pivot features from different domains which are correlated with many of the same pivot features are assumed to correspond, and we treat them similarly in a discriminative learner. Even on the unlabeled data, the co-occurrence statistics of pivot and non-pivot features are likely to be sparse, and we must model them in a compact way. There are many choices for modeling co-occurrence data (Brown et al., 1992; Pereira et al., 1993; Blei et al., 2003). In this work we choose to use the technique of structural learning (Ando and Zhang, 2005a; Ando and Zhang, 2005b). Structural learning models the correlations which are most useful for semi-supervised learning. We demonstrate how to adapt it for transfer learning, and consequently the structural part of structural correspondence learning is borrowed from it.1 SCL is a general technique, which one can apply to feature based classifiers for any task. Here, Discriminative learning methods are widely used in natural language processing. These methods wor"
W06-1615,N03-1028,1,0.0833164,"rich source domain to a resource-poor target domain. We introduce structural correspondence learning to automatically induce correspondences among features from different domains. We test our technique on part of speech tagging and show performance gains for varying amounts of source and target training data, as well as improvements in target domain parsing accuracy using our improved tagger. 1 Introduction Discriminative learning methods are ubiquitous in natural language processing. Discriminative taggers and chunkers have been the state-of-the-art for more than a decade (Ratnaparkhi, 1996; Sha and Pereira, 2003). Furthermore, end-to-end systems like speech recognizers (Roark et al., 2004) and automatic translators (Och, 2003) use increasingly sophisticated discriminative models, which generalize well to new data that is drawn from the same distribution as the training data. However, in many situations we may have a source domain with plentiful labeled training data, but we need to process material from a target domain with a different distribution from the source domain and no labeled data. In such cases, we must take steps to adapt a model trained on the source domain for use in the target domain (R"
W06-1615,W04-3237,0,0.0875466,"incorporating SCL features into a discriminative parser to improve its adaptation properties. 8 Related Work Domain adaptation is an important and wellstudied area in natural language processing. Here we outline a few recent advances. Roark and Bacchiani (2003) use a Dirichlet prior on the multinomial parameters of a generative parsing model to combine a large amount of training data from a source corpus (WSJ), and small amount of training data from a target corpus (Brown). Aside from Florian et al. (2004), several authors have also given techniques for adapting classification to new domains. Chelba and Acero (2004) first train a classifier on the source data. Then they use maximum a posteriori estimation of the weights of a Improving Parser Performance We emphasize the importance of PoS tagging in a pipelined NLP system by incorporating our SCL 126 maximum entropy target domain classifier. The prior is Gaussian with mean equal to the weights of the source domain classifier. Daum´e III and Marcu (2006) use an empirical Bayes model to estimate a latent variable model grouping instances into domain-specific or common across both domains. They also jointly estimate the parameters of the common classificatio"
W06-1615,N03-1033,0,0.15532,"c Sydney, July 2006. 2006 Association for Computational Linguistics (a) Wall Street Journal DT JJ VBZ The clash is CC NN IN and divisiveness in (b) MEDLINE DT JJ The oncogenic RB JJ constitutively active DT a NNP Japan VBN mutated CC and NN sign POS ’s NNS forms VBP interfere IN of JJ once-cozy IN of IN with DT a JJ financial DT the JJ normal JJ new NN circles NN ras NN signal NN toughness . . NNS proteins NN transduction VBP are . . Figure 1: Part of speech-tagged sentences from both corpora (a) An ambiguous instance we investigate its use in part of speech (PoS) tagging (Ratnaparkhi, 1996; Toutanova et al., 2003). While PoS tagging has been heavily studied, many domains lack appropriate training corpora for PoS tagging. Nevertheless, PoS tagging is an important stage in pipelined language processing systems, from information extractors to speech synthesizers. We show how to use SCL to transfer a PoS tagger from the Wall Street Journal (financial news) to MEDLINE (biomedical abstracts), which use very different vocabularies, and we demonstrate not only improved PoS accuracy but also improved end-to-end parsing accuracy while using the improved tagger. An important but rarely-explored setting in domain"
W06-1615,N04-1001,0,\N,Missing
W06-2919,P05-1001,0,0.0144376,"Missing"
W06-2919,W03-0425,0,0.0139299,"Missing"
W06-2919,N04-1043,0,0.142536,"Missing"
W06-2919,W02-1028,0,0.106868,"d with these large corpora, the recent availability of entity lists in those domains has opened up interesting opportunities and challenges. Such lists are never complete and suffer from sampling biases, but we would like to exploit them, in combination with large unlabeled corpora, to speed up the creation of information extraction systems for different domains and languages. In this paper, we concentrate on exploring utility of such resources for named entity extraction. Previous approaches to context pattern induction were described by Riloff and Jones (1999), Agichtein and Gravano (2000), Thelen and Riloff (2002), Lin et al. (2003), and Etzioni et al. (2005), among others. The main advance in the present method is the combination of grammatical induction and statistical techniques to create high-precision patterns. The paper is organized as follows. Section 2 describes our pattern induction algorithm. Section 3 shows how to extend seed sets with entities extracted by the patterns from unlabeled data. Section 4 gives experimental results, and Section 5 compares our method with previous work. 1 For example, based on approximate matching, there is an overlap of only 22 organizations between the 2403 orga"
W06-2919,W03-0434,0,0.0325757,"Missing"
W06-2932,W06-2920,0,0.886664,"ania Philadelphia, PA {ryantm,klerman,pereira}@cis.upenn.edu Abstract We present a two-stage multilingual dependency parser and evaluate it on 13 diverse languages. The first stage is based on the unlabeled dependency parsing models described by McDonald and Pereira (2006) augmented with morphological features for a subset of the languages. The second stage takes the output from the first and labels all the edges in the dependency graph with appropriate syntactic categories using a globally trained sequence classifier over components of the graph. We report results on the CoNLL-X shared task (Buchholz et al., 2006) data sets and present an error analysis. 1 Introduction Often in language processing we require a deep syntactic representation of a sentence in order to assist further processing. With the availability of resources such as the Penn WSJ Treebank, much of the focus in the parsing community had been on producing syntactic representations based on phrase-structure. However, recently their has been a revived interest in parsing models that produce dependency graph representations of sentences, which model words and their arguments through directed edges (Hudson, 1984; Mel0 cˇ uk, 1988). This inte"
W06-2932,P05-1067,0,0.0246308,"Hudson, 1984; Mel0 cˇ uk, 1988). This interest has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages. Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies. Dependency graphs also encode much of the deep syntactic information needed for further processing. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005). In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler. We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2"
W06-2932,H05-1049,0,0.0119017,"putationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages. Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies. Dependency graphs also encode much of the deep syntactic information needed for further processing. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005). In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler. We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ın, 2002; Nilsson et al., 2005; Oflazer et al.,"
W06-2932,E06-1011,1,0.394749,"nd xi is the corresponding head. Each edge can be assigned a label l(i,j) from a finite set L of predefined labels. We 216 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 216–220, New York City, June 2006. 2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use. 2 Stage 1: Unlabeled Parsing The first stage of our system creates an unlabeled parse y for an input sentence x. This system is primarily based on the parsing models described by McDonald and Pereira (2006). That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph. An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge. That system uses MIRA, an online large-margin learning algorithm, to compute model parameters. Its power lies in the ability to define a rich set of features over parsing decisions, as well as surfac"
W06-2932,P05-1012,1,0.431684,"ined labels. We 216 Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X), c pages 216–220, New York City, June 2006. 2006 Association for Computational Linguistics assume that all dependency graphs are trees but may be non-projective, both of which are true in the data sets we use. 2 Stage 1: Unlabeled Parsing The first stage of our system creates an unlabeled parse y for an input sentence x. This system is primarily based on the parsing models described by McDonald and Pereira (2006). That work extends the maximum spanning tree dependency parsing framework (McDonald et al., 2005a; McDonald et al., 2005b) to incorporate features over multiple edges in the dependency graph. An exact projective and an approximate non-projective parsing algorithm are presented, since it is shown that nonprojective dependency parsing becomes NP-hard when features are extended beyond a single edge. That system uses MIRA, an online large-margin learning algorithm, to compute model parameters. Its power lies in the ability to define a rich set of features over parsing decisions, as well as surface level features relative to these decisions. For instance, the system of McDonald et al. (2005a)"
W06-2932,H05-1066,1,0.313302,"Missing"
W06-2932,E06-1038,1,0.676313,"has generally come about due to the computationally efficient and flexible nature of dependency graphs and their ability to easily model non-projectivity in freer-word order languages. Nivre (2005) gives an introduction to dependency representations of sentences and recent developments in dependency parsing strategies. Dependency graphs also encode much of the deep syntactic information needed for further processing. This has been shown through their successful use in many standard natural language processing tasks, including machine translation (Ding and Palmer, 2005), sentence compression (McDonald, 2006), and textual inference (Haghighi et al., 2005). In this paper we describe a two-stage discriminative parsing approach consisting of an unlabeled parser and a subsequent edge labeler. We evaluate this parser on a diverse set of 13 languages using data provided by the CoNLL-X shared-task organizers (Buchholz et al., 2006; Hajiˇc et al., 2004; Simov et al., 2005; Simov and Osenova, 2003; Chen et al., 2003; B¨ohmov´a et al., 2003; Kromann, 2003; van der Beek et al., 2002; Brants et al., 2002; Kawata and Bartels, 2000; Afonso et al., 2002; Dˇzeroski et al., 2006; Civit Torruella and Mart´ı Anton´ı"
W06-2932,W03-2403,0,\N,Missing
W06-2932,dzeroski-etal-2006-towards,0,\N,Missing
W06-2932,W03-2405,0,\N,Missing
W06-2932,afonso-etal-2002-floresta,0,\N,Missing
W07-0206,N04-1042,0,0.0240394,"d using a zeromean Gaussian prior with a variance of 1. The first dataset is the pitch-accent prediction dataset used in semi-supervised learning by Altun et al. (2006). There are 31 real and binary features (all are encoded as real values) and only two labels. Instances correspond to an utterance and each token corresponds to a word. Altun et al. (2006) perform experiments on 4 and 40 training instances using at most 200 unlabeled instances. The second dataset is the reference part of the Cora information extraction dataset.1 This 1 The Cora IE dataset has been used in Seymore et al. (1999), Peng and McCallum (2004), McCallum et al. (2000) and Han et al. (2003), among others. We 42 consists of 500 computer science research paper citations. Each token in a citation is labeled as being part of the name of an author, part of the title, part of the date or one of several other labels that we combined into a single category (“other”). The third dataset is the chunking dataset from the CoNLL 2000 (Sang and Buchholz, 2000) shared task restricted to noun phrases. The task for this dataset is, given the words in a sentence as well as automatically assigned parts of speech for these words, label each word with B-N"
W07-0206,W00-0726,0,0.138497,"Missing"
W07-1509,W95-0107,0,0.0642037,"work. Section 4 describes our experiments and Section 5 concludes the paper. In what follows, x denotes the generic input sentence, Y (x) the set of possible labelings of x, and Y + (x) the set of correct labelings of x. There is also a distinguished “gold” labeling y(x) ∈ Y + (x). For each pair of a sentence x and labeling y ∈ Y (x), we compute a vector-valued feature representation f (x, y). Given a weight vector w, the score w · f (x, y) ranks possible labelings of x, and we denote by Yk,w (x) the set of k top scoring labelings for x. We use the standard B,I,O encoding for named entities (Ramshaw and Marcus, 1995). Thus Y (x) for x of length n is the set of all sequences of length n matching the regular expression (O|(BI∗))∗. In a linear sequence model, for suitable feature functions f , Yk,w (x) can be computed efficiently with Viterbi decoding. 2 2.2 1 Introduction Methods Throughout this work, we use a linear sequence model. This class of models includes popular tagging models for named entities such as conditional k-best MIRA and Loss Functions The learning portion of our method finds a weight vector w that scores the correct labelings of the test data higher than incorrect labelings. We used a k53"
W07-1509,H01-1026,0,0.354425,"n particular we investigate the following method: train a high-recall named entity tagger on the annotated data and use that to tag the remaining corpus. Now ask a human annotator to filter the resulting mentions. The mentions rejected by the annotator are simply dropped from the annotation, leaving the remaining mentions. Previous work on semi-automated annotation The most common approach to semi-automatic annotation is to automatically tag an instance and then ask an annotator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messages. They modify the s"
W07-1509,W06-0602,0,0.106092,"i-automated annotation The most common approach to semi-automatic annotation is to automatically tag an instance and then ask an annotator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messages. They modify the system’s best guess as the user makes corrections, resulting in less annotation actions. 4 Experiments This section relates our approach to previous work on semi-automated approaches. First we discuss how semi-automated annotation is different from active learning and then discuss some previous semiautomated annotation work. We now evaluate to"
W07-1509,P05-1012,1,0.691756,"be computed efficiently with Viterbi decoding. 2 2.2 1 Introduction Methods Throughout this work, we use a linear sequence model. This class of models includes popular tagging models for named entities such as conditional k-best MIRA and Loss Functions The learning portion of our method finds a weight vector w that scores the correct labelings of the test data higher than incorrect labelings. We used a k53 Proceedings of the Linguistic Annotation Workshop, pages 53–56, c Prague, June 2007. 2007 Association for Computational Linguistics best version of the MIRA algorithm (Crammer et al., 2006; McDonald et al., 2005). This is an online learning algorithm that starts with a zero weight vector and for each training sentence makes the smallest possible update that would score the correct label higher than the old top k labels. That is, for each training sentence x we update the weight vector w according to the rule: wnew = arg minw kw − wold k s. t. w · f (x, y(x)) − w · f (x, y) ≥ L(Y + (x), y) ∀y ∈ Yk,wold (x) to alleviate the burden of creating an annotated corpus, they do so in a completely orthogonal manner. Active learning tries to select which instances should be labeled in order to make the most impa"
W07-1509,C02-1145,0,0.0609744,"ate the following method: train a high-recall named entity tagger on the annotated data and use that to tag the remaining corpus. Now ask a human annotator to filter the resulting mentions. The mentions rejected by the annotator are simply dropped from the annotation, leaving the remaining mentions. Previous work on semi-automated annotation The most common approach to semi-automatic annotation is to automatically tag an instance and then ask an annotator to correct the results. We restrict our discussion to this paradigm due to space constraints. Marcus et al. (1994), Chiou et al. (2001) and Xue et al. (2002) apply this approach with some minor modifications to part of speech tagging and phrase structure parsing. The automatic system of Marcus et al. only produces partial parses that are then assembled by the annotators, while Chiou et al. modified their automatic parser specifically for use in annotation. Chou et al. (2006) use this tag and correct approach to create a corpus of predicate argument structures in the biomedical domain. Culota et al. (2006) use a refinement of the tag and correct approach to extract addressbook information from email messages. They modify the system’s best guess as"
W07-1509,J93-2004,0,\N,Missing
W18-1406,Q13-1005,0,0.0222811,"otation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evaluations with implicit feedback rather than prediction and evaluation of labels on text and images. Spatial tasks are natural fits for this strategy, since both evaluation metrics and reward functions (in reinforcement learning) can use spatial proximity to an end location (MacMahon et al., 2006; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Artzi and Zettlemoyer, 2013) or spatial configuration (Bisk et al., 2018; Misra et al., 2017; Tan and Bansal, 2018). There are trade-offs between model-driven and user-driven corpus building. The former defines inventories of spatial relations and generating assignments that will cover them. This may omit phenomena or distinctions not covered in the model and requires considerable expertise and tooling—both of which increase cost and limit scale. User-driven annotation is more exploratory and may be limited by the preferences and tendencies of contributors. We will mitigate such effects by composing diverse crowds from v"
W18-1406,W16-1721,1,0.844969,"d image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real world navigation tasks. Paths Understanding salient features and spatial relations in images and text naturally extends into navigation tasks that connect such points. To avoid"
W18-1406,N18-1177,0,0.108584,"have access to a God’s eye view, like that available to mapping applications (with access to full geographic features via databases). Instead, such tasks must be solvable by interpreting visual and textual stimuli relevant to the locations. This should put a greater emphasis on challenging spatial descriptions and relationships rather than known and named routes. Nonetheless, maps as visual artifacts (e.g., PDFs) may be incorporated in some cases, giving automated agents the ability to use them as a hiker might use a paper map without access to a GPS-based mapping application. Mirowski et al. (2018) is a recent example that takes a first-person perspective in a real world simulation, though one that does not incorporate language. They learn a model for navigating the Google Street View graph via reinforcement learning, where the goal location is specified via its distance to several other landmark locations and no explicit maps are used. Two especially interesting aspects of their approach are their use of curriculum learning (start with nearby goals and then tackle more distant ones) and showing successful adaptation from one city to another. These ideas are complementary to those that"
W18-1406,L18-1254,1,0.897777,"h spatially relevant points and paths—on the scale of at least hundreds of thousands. Multilinguality For both theoretical and practical reasons, we cannot focus on just one language. Different languages have different spatial relations, often involving the three different frames of reference—relative, intrinsic, and absolute (Levinson, 2003)—in different ways. Navigational systems supporting vague reference off the grid are needed even more in locations where English and other majority languages are not spoken. One way we already target multilinguality is via community-driven crowd-sourcing (Funk et al., 2018). In our approach, we intentionally cycle our iterations throughout the world and we involve developers from each locale because they have insights into how the local context affects how language is used and how the task is performed. User-driven annotation We seek to complement previous efforts that have focused on finegrained linguistic annotation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evalua"
W18-1406,W16-3202,0,0.0497127,"Missing"
W18-1406,Q18-1004,0,0.0227929,"Missing"
W18-1406,D14-1086,0,0.0338832,"nguage artifacts provides a natural and mutually reinforcing progression from points to paths to playscapes. Points Scene understanding—building a model for a point in space—is the bedrock of real world spatial language tasks. We must be able to observe and describe visible objects and the spatial relationships between them. Before addressing paths and navigation tasks, we can make considerable progress by improving our data and modeling for spatial relations in tasks like image segmentation and image captioning (Hall et al., 2011; H¨urlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities fo"
W18-1406,D17-1015,0,0.0208315,"to playscapes. Points Scene understanding—building a model for a point in space—is the bedrock of real world spatial language tasks. We must be able to observe and describe visible objects and the spatial relationships between them. Before addressing paths and navigation tasks, we can make considerable progress by improving our data and modeling for spatial relations in tasks like image segmentation and image captioning (Hall et al., 2011; H¨urlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague region"
W18-1406,P17-2033,0,0.0135746,"ounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real world navigation task"
W18-1406,P10-1083,0,0.0402714,"finegrained linguistic annotation, such as Iso-Space (Pustejovsky, 2017). We will obtain scale through both crowd-sourcing and gaming environments— that is, annotations that can be derived from competent language speakers (Chang et al., 2016). This places an emphasis on task evaluations with implicit feedback rather than prediction and evaluation of labels on text and images. Spatial tasks are natural fits for this strategy, since both evaluation metrics and reward functions (in reinforcement learning) can use spatial proximity to an end location (MacMahon et al., 2006; Chen and Mooney, 2011; Vogel and Jurafsky, 2010; Artzi and Zettlemoyer, 2013) or spatial configuration (Bisk et al., 2018; Misra et al., 2017; Tan and Bansal, 2018). There are trade-offs between model-driven and user-driven corpus building. The former defines inventories of spatial relations and generating assignments that will cover them. This may omit phenomena or distinctions not covered in the model and requires considerable expertise and tooling—both of which increase cost and limit scale. User-driven annotation is more exploratory and may be limited by the preferences and tendencies of contributors. We will mitigate such effects by c"
W18-1406,D14-1039,1,0.842799,"rlimann and Bos, 2016), grounding referential expressions (Kazemzadeh et al., 2014; Mao et al., 2016; Hu et al., 2017), relative positioning of objects (Kitaev and Klein, 2017) and image geolocation (Hays and Efros, 2008; Zamir et al., 2016). We will create collaborative image identification and description tasks that emphasize spatial relations and geographically salient landmarks. There has also been much work on annotating and calculating spatial relations in text (Pustejovsky et al., 2015; Pustejovsky, 2017), resolving toponyms (Leidner, 2007; DeLozier et al., 2015), and text geolocation (Wing and Baldridge, 2014; Rahimi et al., 2017). There are further opportunities for building or exploiting annotations on spatially focused texts—e.g., identifying vague regions (DeLozier et al., 2016) or writing a WikiVoyage page for a city given all available information in Wikipedia, akin to Liu et al. (2018). Most importantly, the extensive mappings we have between texts and images and their corresponding locations motivate a focus on simulations of the real world. Learning spatial relations within massive amounts of images and texts can serve as a pretraining step to building components of models that solve real"
W18-1406,D17-1106,0,0.0455013,"Missing"
W95-0108,P90-1034,0,0.0977309,"Missing"
W97-0309,J92-4003,0,0.613643,"sentences were drawn randomly without replacement from the NAB corpus. All perplexity figures given in the paper are computed by combining sentence probabilities; the probability of sentence wow1 . . . w ~ w n + l is given by yIn+lP(wilwo ..wi-1), where w0 and wn+l are i=1 the start- and end-of-sentence markers, respectively. Though not reported below, we also confirmed that the results did not vary significantly for different randomly drawn test sets of the same size. The organization of this paper is as follows. In Section 2, we examine aggregate Markov models, or class-based bigram models (Brown et al., 1992) in which the mapping from words to classes is probabilistic. We describe an iterative algorithm for discovering &quot;soft&quot; word classes, based on the Expectation-Maximization (EM) procedure for maximum likelihood estimation (Dempster, Laird, and Rubin, 1977). Several features make this algorithm attractive for large-vocabulary language modeling: it has no tuning parameters, converges monotonically in the log-likelihood, and handles probabilistic constraints in a natural way. The number of classes, C, can be small or large depending on the constraints of the modeler. Varying the number of classes"
W97-0309,P96-1041,0,0.0117468,"ords that precede it. The problems in estimating robust models of this form are well-documented. The number of parameters--or transition probabilities-scales as V n, where V is the vocabulary size. For typical models (e.g., n = 3, V = 104), this number exceeds by many orders of magnitude the total number of words in any feasible training corpus. 81 The transition probabilities in n-gram models are estimated from the counts of word combinations in the training corpus. Maximum likelihood (ML) estimation leads to zero-valued probabilities for unseen n-grams. In practice, one adjusts or smoothes (Chen and Goodman, 1996) the ML estimates so that the language model can generalize to new phrases. Smoothing can be done in many ways--for example, by introducing artificial counts, backing off to lowerorder models (Katz, 1987), or combining models by interpolation (Jelinek and Mercer, 1980). Often a great deal of information:is lost in the smoothing procedure. This is due to the great discrepancy between n-gram models of different order. The goal of this paper is to investigate models that are intermediate, in both size and accuracy, between different order n-gram models. We show that such models can &quot;intervene&quot; be"
W97-0309,P94-1038,1,0.533339,"Missing"
W97-0309,P93-1024,1,0.672179,"ate Markov models are: P(clwl) ~ P(w2[c) ~- ~ w N(wl, w)P(ClWl, w) ~wc, N(wl , w)P(c &apos; [wl, w) , (3) Ew N(w, w2)P(clw, w~) Eww&apos;g(w,w&apos;)P(clw, w&apos;)&apos; (4) where N(Wl, w2) denotes the number of counts of wlw2 in the training set. These updates are guaranteed to increase the overall log-likelihood, g= Z N(Wl&apos;W2)lnP(w21wl)&apos; (5) WlW2 at each iteration. In general, they converge to a local (though not global) maximum of the log-likelihood. The perplexity V* is related to the log-likelihood by V* : e - ~ / N , where N is the total number of words processed. Though several algorithms (Brown et al., 1992; Pereira, Tishby, and Lee, 1993) have been proposed 100( 1000 9o( goo 80( 80~ 4O( 41111 20( 5 10 15 20 iteration of EM 25 2@ 30 (a) 5 10 15 20 iteration of EM 25 30 (b) Figure 1: Plots of (a) training and (b) test perplexity versus number of iterations of the EM algorithm, for the aggregate Markov model with C = 32 classes. C 1 2 4 8 16 32 V train 964.7 771.2 541.9 399.5 328.8 278.9 123.6 for performing the decomposition in eq. (1), it is worth noting that only the EM algorithm directly optimizes the log-likelihood in eq. (5). This has obvious advantages if the goal of finding word classes is to improve the perplexity of a"
