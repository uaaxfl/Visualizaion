2007.sigdial-1.16,W02-1011,0,0.0251785,"rst break down the review into its various content zones, and then see opinion classification only as one subproblem, pertaining to a subset of the paragraphs. The subtask of opinion identification has received much attention in recent years. Subjectivity in natural language encompasses a range of different phenomena, including the means to express opinions, emotions, or evaluations. Example applications are automatic classification of opinion texts (e.g. editorials) vs. factual texts (e.g. business texts or news) (Wiebe et al., 2004) or positive vs. negative ratings in reviews (Turney, 2002; Pang et al., 2002; Zhuang et al., 2006). The classification is applied to documents (e.g., Wiebe et al. (2004)) or sentences (Yu and Hatzivassiloglou, 2003). In contrast to the above approaches, which are exclusively developed for English, we aim at learning subjectivity clues for German data. Moreover, in our classification task, paragraphs rather than documents or sentences are being classified. &lt;author> &lt;author place> &lt;author rating> &lt;cast> &lt;credits> &lt;country year> &lt;date> &lt;director> &lt;format> &lt;genre> &lt;language> &lt;language-subtitles> &lt;legal-notice> &lt;note> &lt;quote> &lt;rating> &lt;runtime> &lt;show-loc date> &lt;structure>"
2007.sigdial-1.16,P02-1053,0,0.00884897,"er step: We first break down the review into its various content zones, and then see opinion classification only as one subproblem, pertaining to a subset of the paragraphs. The subtask of opinion identification has received much attention in recent years. Subjectivity in natural language encompasses a range of different phenomena, including the means to express opinions, emotions, or evaluations. Example applications are automatic classification of opinion texts (e.g. editorials) vs. factual texts (e.g. business texts or news) (Wiebe et al., 2004) or positive vs. negative ratings in reviews (Turney, 2002; Pang et al., 2002; Zhuang et al., 2006). The classification is applied to documents (e.g., Wiebe et al. (2004)) or sentences (Yu and Hatzivassiloglou, 2003). In contrast to the above approaches, which are exclusively developed for English, we aim at learning subjectivity clues for German data. Moreover, in our classification task, paragraphs rather than documents or sentences are being classified. &lt;author> &lt;author place> &lt;author rating> &lt;cast> &lt;credits> &lt;country year> &lt;date> &lt;director> &lt;format> &lt;genre> &lt;language> &lt;language-subtitles> &lt;legal-notice> &lt;note> &lt;quote> &lt;rating> &lt;runtime> &lt;show-loc"
2007.sigdial-1.16,J04-3002,0,0.0498426,"it and Zhou (2005)). Our work in effect takes a significant further step: We first break down the review into its various content zones, and then see opinion classification only as one subproblem, pertaining to a subset of the paragraphs. The subtask of opinion identification has received much attention in recent years. Subjectivity in natural language encompasses a range of different phenomena, including the means to express opinions, emotions, or evaluations. Example applications are automatic classification of opinion texts (e.g. editorials) vs. factual texts (e.g. business texts or news) (Wiebe et al., 2004) or positive vs. negative ratings in reviews (Turney, 2002; Pang et al., 2002; Zhuang et al., 2006). The classification is applied to documents (e.g., Wiebe et al. (2004)) or sentences (Yu and Hatzivassiloglou, 2003). In contrast to the above approaches, which are exclusively developed for English, we aim at learning subjectivity clues for German data. Moreover, in our classification task, paragraphs rather than documents or sentences are being classified. &lt;author> &lt;author place> &lt;author rating> &lt;cast> &lt;credits> &lt;country year> &lt;date> &lt;director> &lt;format> &lt;genre> &lt;language> &lt;language-subtitles>"
2007.sigdial-1.16,W03-1017,0,0.100803,"ng to a subset of the paragraphs. The subtask of opinion identification has received much attention in recent years. Subjectivity in natural language encompasses a range of different phenomena, including the means to express opinions, emotions, or evaluations. Example applications are automatic classification of opinion texts (e.g. editorials) vs. factual texts (e.g. business texts or news) (Wiebe et al., 2004) or positive vs. negative ratings in reviews (Turney, 2002; Pang et al., 2002; Zhuang et al., 2006). The classification is applied to documents (e.g., Wiebe et al. (2004)) or sentences (Yu and Hatzivassiloglou, 2003). In contrast to the above approaches, which are exclusively developed for English, we aim at learning subjectivity clues for German data. Moreover, in our classification task, paragraphs rather than documents or sentences are being classified. &lt;author> &lt;author place> &lt;author rating> &lt;cast> &lt;credits> &lt;country year> &lt;date> &lt;director> &lt;format> &lt;genre> &lt;language> &lt;language-subtitles> &lt;legal-notice> &lt;note> &lt;quote> &lt;rating> &lt;runtime> &lt;show-loc date> &lt;structure> &lt;tagline> &lt;title> &lt;DATA> &lt;dvd-DATA> Description Age restrictions for viewing (in the U.S.: MPAA rating) Author of review Author of review a"
2007.sigdial-1.16,P97-1005,0,\N,Missing
2007.sigdial-1.16,P99-1032,0,\N,Missing
2007.sigdial-1.16,P05-1015,0,\N,Missing
2020.argmining-1.6,W16-2801,0,0.623199,"documents (Moens et al., 2007; Stab and Gurevych, 2014), less work has been done on user-generated web content (Park and Cardie, 2014; Habernal and Gurevych, 2015). This shortcoming poses a problem as systems trained on formal and edited texts tend to be inapt of extracting patterns from the more informal user-generated content ˇ (Snajder, 2016). In this paper we focus on tweets, which are of great interest for the argument mining community due to the increasing use of the microblogging service Twitter1 in political online discourse. While some first work on argument mining in tweets exists (Addawood and Bashir, 2016; Dusmanu et al., 2017), only a small number of available annotated corpora have been created that can be utilized for training tweet-specific argument mining systems (Bosc et al., 2016). To improve on this point, we present a new corpus of German tweets annotated for claim and evidence2 . To the best of our knowledge, this is the first argument tweet corpus not exclusively annotated with the full tweet as the unit of annotation. Instead, argumentative spans within tweets, henceforth called argumentative discourse units (ADU) (Peldszus and Stede, 2013), have been annotated as well. They render"
2020.argmining-1.6,W14-2109,0,0.0608715,"n argument mining. Section 3 describes the corpus, the annotation scheme and the annotation procedure. In Section 4 we present first classification and sequence labeling results using the annotated data. Section 5 discusses our results and gives a brief outlook. 2 Related Work Related work on tweet-based argument mining has focused on separating argumentative tweets from non-argumentative ones and on defining new Twitter-specific tasks. This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. 1 https://twitter.com/ 2 We follow Aharoni et al. (2014) and others by using the term evidence instead of premise. 53 Proceedings of the 7th Workshop on Argument Mining, pages 53–58 Barcelona, Spain (Online), December 13, 2020. License details: http:// Addawood and Bashir (2016) present a corpus of English tweets annotated for arguments and evidence types like news media accounts or expert opinions. First, arguments are identified on the full tweet level, followed by the subsequent annotation of evidence types. Annotators achieved Cohen’s Kappa scores of 0.67 and 0.79, respectively. An SVM trained on linguistic and Twitter-related features yielded"
2020.argmining-1.6,N19-4010,0,0.0198656,"eddings Target Argument Argument Claim Claim Evidence Evidence Preproc l,p,s p l,p p l,p p,s F1 (w) 0.8 0.82 0.79 0.82 0.67 0.59 Precision (w) 0.75 0.8 0.78 0.8 0.68 0.59 Recall (w) 0.86 0.86 0.82 0.85 0.68 0.62 Table 3: Classification Results (l = lowercase, p = punctuation, s = stopword, w = weighted) Tweet level. Classification models were trained on different combinations of n-grams and on pretrained BERT-based document embeddings (Devlin et al., 2019). The latter were created using FLAIR, an NLP framework that contains a unified interface for employing different types of text embeddings (Akbik et al., 2019). All shown classification results are yielded using eXtreme Gradient Boosting (XGBoost) (Chen and Guestrin, 2016), which is a variant of the Gradient Boosting approach introduced by Friedman (2000). We implemented three different classification tasks based on the respective binary target sets: argumentative vs non-argumentative, claim vs no claim or evidence vs no evidence. All results are 10-fold cross-validated. Table 3 shows macro F1, precision and recall scores, which are weighted for the unbalanced distribution of classes. Pretrained BERT embeddings yield better F1 scores for argument (0"
2020.argmining-1.6,J08-4004,0,0.173257,"Missing"
2020.argmining-1.6,L16-1200,0,0.601739,"oblem as systems trained on formal and edited texts tend to be inapt of extracting patterns from the more informal user-generated content ˇ (Snajder, 2016). In this paper we focus on tweets, which are of great interest for the argument mining community due to the increasing use of the microblogging service Twitter1 in political online discourse. While some first work on argument mining in tweets exists (Addawood and Bashir, 2016; Dusmanu et al., 2017), only a small number of available annotated corpora have been created that can be utilized for training tweet-specific argument mining systems (Bosc et al., 2016). To improve on this point, we present a new corpus of German tweets annotated for claim and evidence2 . To the best of our knowledge, this is the first argument tweet corpus not exclusively annotated with the full tweet as the unit of annotation. Instead, argumentative spans within tweets, henceforth called argumentative discourse units (ADU) (Peldszus and Stede, 2013), have been annotated as well. They render the corpus suitable not only for supervised classification but also for sequence labeling approaches. We also present first promising experimental results using this corpus. This paper"
2020.argmining-1.6,N19-1423,0,0.0169061,"we only present the best results here. Features Bigrams Pretrained BERT Embeddings Uni- & Bigrams Pretrained BERT Embeddings Uni- & Bigrams Pretrained BERT Embeddings Target Argument Argument Claim Claim Evidence Evidence Preproc l,p,s p l,p p l,p p,s F1 (w) 0.8 0.82 0.79 0.82 0.67 0.59 Precision (w) 0.75 0.8 0.78 0.8 0.68 0.59 Recall (w) 0.86 0.86 0.82 0.85 0.68 0.62 Table 3: Classification Results (l = lowercase, p = punctuation, s = stopword, w = weighted) Tweet level. Classification models were trained on different combinations of n-grams and on pretrained BERT-based document embeddings (Devlin et al., 2019). The latter were created using FLAIR, an NLP framework that contains a unified interface for employing different types of text embeddings (Akbik et al., 2019). All shown classification results are yielded using eXtreme Gradient Boosting (XGBoost) (Chen and Guestrin, 2016), which is a variant of the Gradient Boosting approach introduced by Friedman (2000). We implemented three different classification tasks based on the respective binary target sets: argumentative vs non-argumentative, claim vs no claim or evidence vs no evidence. All results are 10-fold cross-validated. Table 3 shows macro F1"
2020.argmining-1.6,D17-1245,0,0.640235,"007; Stab and Gurevych, 2014), less work has been done on user-generated web content (Park and Cardie, 2014; Habernal and Gurevych, 2015). This shortcoming poses a problem as systems trained on formal and edited texts tend to be inapt of extracting patterns from the more informal user-generated content ˇ (Snajder, 2016). In this paper we focus on tweets, which are of great interest for the argument mining community due to the increasing use of the microblogging service Twitter1 in political online discourse. While some first work on argument mining in tweets exists (Addawood and Bashir, 2016; Dusmanu et al., 2017), only a small number of available annotated corpora have been created that can be utilized for training tweet-specific argument mining systems (Bosc et al., 2016). To improve on this point, we present a new corpus of German tweets annotated for claim and evidence2 . To the best of our knowledge, this is the first argument tweet corpus not exclusively annotated with the full tweet as the unit of annotation. Instead, argumentative spans within tweets, henceforth called argumentative discourse units (ADU) (Peldszus and Stede, 2013), have been annotated as well. They render the corpus suitable no"
2020.argmining-1.6,D15-1255,0,0.0214104,"ts but also argumentative spans within tweets. We further report first promising results using supervised classification (F1: 0.82) and sequence labeling (F1: 0.72) approaches. 1 Introduction In recent years the field of argument mining, which focuses on the automatic identification of argument components and their relations in text, has developed substantially (Stede and Schneider, 2018). However, while the majority of research concentrates on well-structured documents (Moens et al., 2007; Stab and Gurevych, 2014), less work has been done on user-generated web content (Park and Cardie, 2014; Habernal and Gurevych, 2015). This shortcoming poses a problem as systems trained on formal and edited texts tend to be inapt of extracting patterns from the more informal user-generated content ˇ (Snajder, 2016). In this paper we focus on tweets, which are of great interest for the argument mining community due to the increasing use of the microblogging service Twitter1 in political online discourse. While some first work on argument mining in tweets exists (Addawood and Bashir, 2016; Dusmanu et al., 2017), only a small number of available annotated corpora have been created that can be utilized for training tweet-speci"
2020.argmining-1.6,P12-3005,0,0.0340188,"roach makes their work comparable to ours. They report F1 scores of 0.77 and 0.42 for the two tasks, respectively. 3 Corpus Annotation Our complete initial corpus consists of 77,100 tweets collected in 2019 via the Twitter API using the Python library Tweepy3 . All tweets contain the keyword klima (“climate”) and mainly concentrate on the topic of climate change, which was intensely discussed by German media and politics during that time. We conducted the following preprocessing steps. First, we removed all retweets and excluded non-German tweets using the language identification tool langid (Lui and Baldwin, 2012). These steps led to a subset of 29,525 tweets. In the following, we grouped the tweets into pairs, consisting of a tweet, henceforth called context tweet, and the tweet to be annotated, which is a reply to the context tweet and, for this reason, is called reply tweet. This approach is motivated by the assumption that tweets in a reply relation are more likely to contain argumentation (Dykes et al., 2020). Moreover, given the short nature of tweets, providing a context is supposed to help interpreting the reply tweet’s content. All tweets that were no replies were removed and missing context t"
2020.argmining-1.6,W14-2105,0,0.0654197,"nly annotated full tweets but also argumentative spans within tweets. We further report first promising results using supervised classification (F1: 0.82) and sequence labeling (F1: 0.72) approaches. 1 Introduction In recent years the field of argument mining, which focuses on the automatic identification of argument components and their relations in text, has developed substantially (Stede and Schneider, 2018). However, while the majority of research concentrates on well-structured documents (Moens et al., 2007; Stab and Gurevych, 2014), less work has been done on user-generated web content (Park and Cardie, 2014; Habernal and Gurevych, 2015). This shortcoming poses a problem as systems trained on formal and edited texts tend to be inapt of extracting patterns from the more informal user-generated content ˇ (Snajder, 2016). In this paper we focus on tweets, which are of great interest for the argument mining community due to the increasing use of the microblogging service Twitter1 in political online discourse. While some first work on argument mining in tweets exists (Addawood and Bashir, 2016; Dusmanu et al., 2017), only a small number of available annotated corpora have been created that can be uti"
2020.argmining-1.6,D14-1006,0,0.0312957,"mponents. To the best of our knowledge, this is the first corpus containing not only annotated full tweets but also argumentative spans within tweets. We further report first promising results using supervised classification (F1: 0.82) and sequence labeling (F1: 0.72) approaches. 1 Introduction In recent years the field of argument mining, which focuses on the automatic identification of argument components and their relations in text, has developed substantially (Stede and Schneider, 2018). However, while the majority of research concentrates on well-structured documents (Moens et al., 2007; Stab and Gurevych, 2014), less work has been done on user-generated web content (Park and Cardie, 2014; Habernal and Gurevych, 2015). This shortcoming poses a problem as systems trained on formal and edited texts tend to be inapt of extracting patterns from the more informal user-generated content ˇ (Snajder, 2016). In this paper we focus on tweets, which are of great interest for the argument mining community due to the increasing use of the microblogging service Twitter1 in political online discourse. While some first work on argument mining in tweets exists (Addawood and Bashir, 2016; Dusmanu et al., 2017), only a"
2020.codi-1.7,I11-1120,0,0.0315958,"ddition, we propose a second model based on the first one, which successfully combines connective disambiguation with sense classification as an auxiliary task. We follow the idea of previous work that sense classification can be performed without extracting the connectives’ arguments (Pitler and Nenkova, 2009; Lin et al., 2014; Qin et al., 2016). Further, it has been previously shown that, for the identification of an explicit relation’s sense, the connective itself as well as its context already provide significant information (Pitler and Nenkova, 2009; Lin et al., 2014; Wang and Lan, 2015; Ghosh et al., 2011). Consequently, we assume the necessary information for sense classification to be already accessible by our neural connective disambiguation model to some degree. Also, this approach elim3.1 Embedding-Based Connective Disambiguation For parsing explicit discourse relations, the first task usually involves the identification of possible connective candidates. For this purpose, we use a list of candidate patterns based on PDTB2. Some candidates might look like discourse connectives, however, they might only be in sentential use. Connective annotation in PDTB2 is quite flexible. Connectives can"
2020.codi-1.7,2020.lrec-1.145,0,0.0293891,"ks and add similar features for more syntactic context information of the connective. Oepen et al. (2016) combine previous feature sets with work on identifying expressions of speculation and negation (Velldal et al., 2012). Recent work of Webber et al. (2019) highlights the complexity of several kinds of ambiguity when working with discourse connectives. sense classification. They compare word pair features with Brown clusters and low-dimensional word embeddings. Bai and Zhao (2018) use different levels of input representations, ranging from character level to contextualized word embeddings. Kishimoto et al. (2020) adapt BERT to perform implicit discourse sense classification. They show promising results by adding tasks, such as connective prediction, for pretraining. Multitask learning is also successfully applied to implicit sense classification (Liu et al., 2016). The authors combine four different tasks related to discourse parsing, but in contrast to our work, they rely on previously extracted argument spans. Qin et al. (2017) propose a model that, in addition to their main task (implicit sense classification), also learns to predict a possible connective that could be inserted. Lan et al. (2017) i"
2020.codi-1.7,C18-1048,0,0.0130634,"er extend those features by category paths from the connective to the root. Wang and Lan (2015) further extend the previous two works and add similar features for more syntactic context information of the connective. Oepen et al. (2016) combine previous feature sets with work on identifying expressions of speculation and negation (Velldal et al., 2012). Recent work of Webber et al. (2019) highlights the complexity of several kinds of ambiguity when working with discourse connectives. sense classification. They compare word pair features with Brown clusters and low-dimensional word embeddings. Bai and Zhao (2018) use different levels of input representations, ranging from character level to contextualized word embeddings. Kishimoto et al. (2020) adapt BERT to perform implicit discourse sense classification. They show promising results by adding tasks, such as connective prediction, for pretraining. Multitask learning is also successfully applied to implicit sense classification (Liu et al., 2016). The authors combine four different tasks related to discourse parsing, but in contrast to our work, they rely on previously extracted argument spans. Qin et al. (2017) propose a model that, in addition to th"
2020.codi-1.7,D17-1134,0,0.0676969,"imoto et al. (2020) adapt BERT to perform implicit discourse sense classification. They show promising results by adding tasks, such as connective prediction, for pretraining. Multitask learning is also successfully applied to implicit sense classification (Liu et al., 2016). The authors combine four different tasks related to discourse parsing, but in contrast to our work, they rely on previously extracted argument spans. Qin et al. (2017) propose a model that, in addition to their main task (implicit sense classification), also learns to predict a possible connective that could be inserted. Lan et al. (2017) introduce various models that perform multitask learning, and their focus also lies on implicit sense classification. The connective and its explicit sense have a strong correlation as shown by Pitler and Nenkova (2009), who report accuracy higher than the interannotator agreement for their connective disambiguation features on coarse-grained level senses. Lin et al. (2014) use only context features and evaluate their work on second-level senses. Wang and Lan (2015) extend previous features and develop a model for the CoNNL Shared Task. Oepen et al. (2016) use an ensemble of three types of cl"
2020.codi-1.7,Q17-1010,0,0.0247336,"main tasks of identifying connectives, demarcating their arguments, assigning senses to them, and finding the senses of so-called implicit relations (which hold between adjacent text spans without a lexical signal being present). In this work, we focus in particular on the binary connective disambiguation of explicit discourse relations and, further, integrate explicit sense prediction into our model, as those two tasks are highly related. Word embeddings provide dense token representations in a low-dimensional vector space pretrained on large unannotated text corpora. First, we use fastText (Bojanowski et al., 2017), which is based on the skip-gram model (Mikolov et al., 2013) and integrates character ?-grams into its representation. Second, we use GloVe (Pennington et al., 2014); as opposed to fastText, those embeddings were calculated through co-occurrence statistics rather than trained by a neural network. Recently, models were introduced that provide contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) on 1. Mr. Perkins believes, however, that the market could be stabilized. 2. “The 1987 crash was a false alarm however you view it,” says university of Chicago economist. 1 Example"
2020.codi-1.7,D15-1262,0,0.0498477,"Missing"
2020.codi-1.7,P19-1441,0,0.0164353,"mist. 1 Examples 1–2 are from PDTB (see Section 2); examples 3–6 are artificially constructed; senses follow PDTB2 style. 65 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 65–75 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 demand and thus tackle the problem of identical representations for homonymous words with different senses, which had been indistinguishable in older models. For our experiments, we use BERT (Devlin et al., 2019), which was successful in many areas of NLP (Liu and Lapata, 2019; Liu et al., 2019). In this work, we present a novel approach to identifying explicit relations in shallow discourse parsing. We introduce a simple yet powerful model that outperforms previous research on the binary disambiguation of connective candidates. Furthermore, we adopt connective sense classification as an auxiliary task to improve performance and generalization capabilities and study the benefits of jointly training the auxiliary task in addition to the main task. This is because, in various cases, training neural models on multiple related tasks has shown beneficial for the learned representation (Ca"
2020.codi-1.7,D19-1387,0,0.0191839,"rsity of Chicago economist. 1 Examples 1–2 are from PDTB (see Section 2); examples 3–6 are artificially constructed; senses follow PDTB2 style. 65 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 65–75 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 demand and thus tackle the problem of identical representations for homonymous words with different senses, which had been indistinguishable in older models. For our experiments, we use BERT (Devlin et al., 2019), which was successful in many areas of NLP (Liu and Lapata, 2019; Liu et al., 2019). In this work, we present a novel approach to identifying explicit relations in shallow discourse parsing. We introduce a simple yet powerful model that outperforms previous research on the binary disambiguation of connective candidates. Furthermore, we adopt connective sense classification as an auxiliary task to improve performance and generalization capabilities and study the benefits of jointly training the auxiliary task in addition to the main task. This is because, in various cases, training neural models on multiple related tasks has shown beneficial for the learned"
2020.codi-1.7,N19-1423,0,0.153521,"are highly related. Word embeddings provide dense token representations in a low-dimensional vector space pretrained on large unannotated text corpora. First, we use fastText (Bojanowski et al., 2017), which is based on the skip-gram model (Mikolov et al., 2013) and integrates character ?-grams into its representation. Second, we use GloVe (Pennington et al., 2014); as opposed to fastText, those embeddings were calculated through co-occurrence statistics rather than trained by a neural network. Recently, models were introduced that provide contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) on 1. Mr. Perkins believes, however, that the market could be stabilized. 2. “The 1987 crash was a false alarm however you view it,” says university of Chicago economist. 1 Examples 1–2 are from PDTB (see Section 2); examples 3–6 are artificially constructed; senses follow PDTB2 style. 65 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 65–75 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 demand and thus tackle the problem of identical representations for homonymous words with different senses, which"
2020.codi-1.7,K16-2021,0,0.012562,"lind Model ? 1conn ? 1sense ? 1conn ? 1sense Ecnuc OPT Stepanov Soochow 93.96 94.43 92.43 94.74 90.13 90.13 — — 91.34 91.79 88.56 91.04 77.41 77.17 — — ctx-embd (bert-ctx-1) ctx-embd-mtl (bert-ctx-0) embd (wiki-ctx-1) embd-mtl (wiki-ctx-1) 97.32 97.18 92.09 92.63 — 86.26 — 80.39 96.98 96.28 88.74 89.74 — 71.69 — 62.74 Table 3: Task-related F1 scores. Results are taken from the CoNLL 2016 Shared Task website (http://www. cs.brandeis.edu/~clp/conll16st/results.html) for the following parsers: OPT (Oepen et al., 2016), Ecnuc (Wang and Lan, 2016), Stepanov (Stepanov and Riccardi, 2016a), Soochow (Fan et al., 2016). The ending -mtl refers to the results for fine-sense classification in Table 2. binary features that check whether categories are contained by the nodes’ traces and pairwise interaction features. In addition to these features, Lin et al. (2014) propose a set of lexicosyntactic features, as they observe that a connective’s immediate context and part of speech is already a strong indicator for disambiguation. The authors further extend those features by category paths from the connective to the root. Wang and Lan (2015) further extend the previous two works and add similar features for more sy"
2020.codi-1.7,K16-2012,0,0.741521,"Missing"
2020.codi-1.7,K16-2002,1,0.939705,"Missing"
2020.codi-1.7,D14-1162,0,0.0852046,"n adjacent text spans without a lexical signal being present). In this work, we focus in particular on the binary connective disambiguation of explicit discourse relations and, further, integrate explicit sense prediction into our model, as those two tasks are highly related. Word embeddings provide dense token representations in a low-dimensional vector space pretrained on large unannotated text corpora. First, we use fastText (Bojanowski et al., 2017), which is based on the skip-gram model (Mikolov et al., 2013) and integrates character ?-grams into its representation. Second, we use GloVe (Pennington et al., 2014); as opposed to fastText, those embeddings were calculated through co-occurrence statistics rather than trained by a neural network. Recently, models were introduced that provide contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) on 1. Mr. Perkins believes, however, that the market could be stabilized. 2. “The 1987 crash was a false alarm however you view it,” says university of Chicago economist. 1 Examples 1–2 are from PDTB (see Section 2); examples 3–6 are artificially constructed; senses follow PDTB2 style. 65 Proceedings of the First Workshop on Computational Approa"
2020.codi-1.7,J12-2005,0,0.0208618,"and pairwise interaction features. In addition to these features, Lin et al. (2014) propose a set of lexicosyntactic features, as they observe that a connective’s immediate context and part of speech is already a strong indicator for disambiguation. The authors further extend those features by category paths from the connective to the root. Wang and Lan (2015) further extend the previous two works and add similar features for more syntactic context information of the connective. Oepen et al. (2016) combine previous feature sets with work on identifying expressions of speculation and negation (Velldal et al., 2012). Recent work of Webber et al. (2019) highlights the complexity of several kinds of ambiguity when working with discourse connectives. sense classification. They compare word pair features with Brown clusters and low-dimensional word embeddings. Bai and Zhao (2018) use different levels of input representations, ranging from character level to contextualized word embeddings. Kishimoto et al. (2020) adapt BERT to perform implicit discourse sense classification. They show promising results by adding tasks, such as connective prediction, for pretraining. Multitask learning is also successfully app"
2020.codi-1.7,N18-1202,0,0.0168785,"l, as those two tasks are highly related. Word embeddings provide dense token representations in a low-dimensional vector space pretrained on large unannotated text corpora. First, we use fastText (Bojanowski et al., 2017), which is based on the skip-gram model (Mikolov et al., 2013) and integrates character ?-grams into its representation. Second, we use GloVe (Pennington et al., 2014); as opposed to fastText, those embeddings were calculated through co-occurrence statistics rather than trained by a neural network. Recently, models were introduced that provide contextualized word embeddings (Peters et al., 2018; Devlin et al., 2019) on 1. Mr. Perkins believes, however, that the market could be stabilized. 2. “The 1987 crash was a false alarm however you view it,” says university of Chicago economist. 1 Examples 1–2 are from PDTB (see Section 2); examples 3–6 are artificially constructed; senses follow PDTB2 style. 65 Proceedings of the First Workshop on Computational Approaches to Discourse, pages 65–75 c Online, November 20, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 demand and thus tackle the problem of identical representations for homonymous words with d"
2020.codi-1.7,K15-2002,0,0.773891,"n more clearly. In addition, we propose a second model based on the first one, which successfully combines connective disambiguation with sense classification as an auxiliary task. We follow the idea of previous work that sense classification can be performed without extracting the connectives’ arguments (Pitler and Nenkova, 2009; Lin et al., 2014; Qin et al., 2016). Further, it has been previously shown that, for the identification of an explicit relation’s sense, the connective itself as well as its context already provide significant information (Pitler and Nenkova, 2009; Lin et al., 2014; Wang and Lan, 2015; Ghosh et al., 2011). Consequently, we assume the necessary information for sense classification to be already accessible by our neural connective disambiguation model to some degree. Also, this approach elim3.1 Embedding-Based Connective Disambiguation For parsing explicit discourse relations, the first task usually involves the identification of possible connective candidates. For this purpose, we use a list of candidate patterns based on PDTB2. Some candidates might look like discourse connectives, however, they might only be in sentential use. Connective annotation in PDTB2 is quite flexi"
2020.codi-1.7,P09-2004,0,0.890321,"y developed contextualized embeddings (represented by BERT). We first hypothesize that contextualized embeddings yield better results than their noncontextualized counterpart. Second, we expect the context span to influence the model’s performance, as the context may indicate a word’s function more clearly. In addition, we propose a second model based on the first one, which successfully combines connective disambiguation with sense classification as an auxiliary task. We follow the idea of previous work that sense classification can be performed without extracting the connectives’ arguments (Pitler and Nenkova, 2009; Lin et al., 2014; Qin et al., 2016). Further, it has been previously shown that, for the identification of an explicit relation’s sense, the connective itself as well as its context already provide significant information (Pitler and Nenkova, 2009; Lin et al., 2014; Wang and Lan, 2015; Ghosh et al., 2011). Consequently, we assume the necessary information for sense classification to be already accessible by our neural connective disambiguation model to some degree. Also, this approach elim3.1 Embedding-Based Connective Disambiguation For parsing explicit discourse relations, the first task u"
2020.codi-1.7,K16-2004,0,0.402459,"the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008). This corpus provides about 43,000 annotated discourse relations, of which roughly 18,000 are signalled by explicit discourse connectives. Those relations are further annotated with a three-level sense hierarchy (one or two senses per relation). All discourse relations consist of two arguments and are associated with one of various types; the focus of our work is on relations of the explicit type. The Shared Tasks at CoNLL 2015 and 2016 (Xue et al., 2015, 2016) used PDTB2 with minor changes. Successful systems were Wang et al. (2015); Wang and Lan (2016); Oepen et al. (2016). They largely follow a pipeline architecture (Lin et al., 2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Recently, PDTB3 (Prasad et al., 2018) was published, which extends the previous work with more available relations and corrects several former annotations. The authors also adjusted the relations’ sense labels for a more balanced class distribution. For the sake of comparison with previous work on SDP, we stick to the PDTB2 corpus and assume to achieve similar"
2020.codi-1.7,prasad-etal-2008-penn,0,0.358571,"onjunction Comparison.Contrast Contingency.Condition Temporal.Sync Comparison.Concession Contingency.Cause.Reason Temporal.Async.Succession Temporal.Async.Precedence Contingency.Cause.Result Expansion.Instantiation Expansion.Alternative Expansion.Restatement Expansion.Alternative.Chosen Expansion.Exception 34174 5007 4382 2752 2578 69.90 10.24 8.96 5.63 5.27 34174 4323 2956 1147 1133 1079 943 842 770 487 236 195 121 95 13 70.44 8.91 6.09 2.36 2.34 2.22 1.94 1.74 1.59 1.00 0.49 0.40 0.25 0.20 0.03 Table 1: Class distribution of the training data. version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008). This corpus provides about 43,000 annotated discourse relations, of which roughly 18,000 are signalled by explicit discourse connectives. Those relations are further annotated with a three-level sense hierarchy (one or two senses per relation). All discourse relations consist of two arguments and are associated with one of various types; the focus of our work is on relations of the explicit type. The Shared Tasks at CoNLL 2015 and 2016 (Xue et al., 2015, 2016) used PDTB2 with minor changes. Successful systems were Wang et al. (2015); Wang and Lan (2016); Oepen et al. (2016). They largely fol"
2020.codi-1.7,K15-2014,0,0.0200188,"ing data. version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008). This corpus provides about 43,000 annotated discourse relations, of which roughly 18,000 are signalled by explicit discourse connectives. Those relations are further annotated with a three-level sense hierarchy (one or two senses per relation). All discourse relations consist of two arguments and are associated with one of various types; the focus of our work is on relations of the explicit type. The Shared Tasks at CoNLL 2015 and 2016 (Xue et al., 2015, 2016) used PDTB2 with minor changes. Successful systems were Wang et al. (2015); Wang and Lan (2016); Oepen et al. (2016). They largely follow a pipeline architecture (Lin et al., 2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Recently, PDTB3 (Prasad et al., 2018) was published, which extends the previous work with more available relations and corrects several former annotations. The authors also adjusted the relations’ sense labels for a more balanced class distribution. For the sake of comparison with previous work on SDP, we stick to the PDTB2 corpus and assu"
2020.codi-1.7,W18-4710,0,0.0215008,"e or two senses per relation). All discourse relations consist of two arguments and are associated with one of various types; the focus of our work is on relations of the explicit type. The Shared Tasks at CoNLL 2015 and 2016 (Xue et al., 2015, 2016) used PDTB2 with minor changes. Successful systems were Wang et al. (2015); Wang and Lan (2016); Oepen et al. (2016). They largely follow a pipeline architecture (Lin et al., 2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Recently, PDTB3 (Prasad et al., 2018) was published, which extends the previous work with more available relations and corrects several former annotations. The authors also adjusted the relations’ sense labels for a more balanced class distribution. For the sake of comparison with previous work on SDP, we stick to the PDTB2 corpus and assume to achieve similar results with PDTB3. Table 1 summarizes the distribution regarding sense classes, where we denote candidate words with sentential reading by NoConn. In both settings, NoConn dominates other classes and, thus, serves 2. We present a novel approach that successfully combines t"
2020.codi-1.7,W19-0411,0,0.0252979,"addition to these features, Lin et al. (2014) propose a set of lexicosyntactic features, as they observe that a connective’s immediate context and part of speech is already a strong indicator for disambiguation. The authors further extend those features by category paths from the connective to the root. Wang and Lan (2015) further extend the previous two works and add similar features for more syntactic context information of the connective. Oepen et al. (2016) combine previous feature sets with work on identifying expressions of speculation and negation (Velldal et al., 2012). Recent work of Webber et al. (2019) highlights the complexity of several kinds of ambiguity when working with discourse connectives. sense classification. They compare word pair features with Brown clusters and low-dimensional word embeddings. Bai and Zhao (2018) use different levels of input representations, ranging from character level to contextualized word embeddings. Kishimoto et al. (2020) adapt BERT to perform implicit discourse sense classification. They show promising results by adding tasks, such as connective prediction, for pretraining. Multitask learning is also successfully applied to implicit sense classification"
2020.codi-1.7,K16-2010,0,0.3883,"sented by BERT). We first hypothesize that contextualized embeddings yield better results than their noncontextualized counterpart. Second, we expect the context span to influence the model’s performance, as the context may indicate a word’s function more clearly. In addition, we propose a second model based on the first one, which successfully combines connective disambiguation with sense classification as an auxiliary task. We follow the idea of previous work that sense classification can be performed without extracting the connectives’ arguments (Pitler and Nenkova, 2009; Lin et al., 2014; Qin et al., 2016). Further, it has been previously shown that, for the identification of an explicit relation’s sense, the connective itself as well as its context already provide significant information (Pitler and Nenkova, 2009; Lin et al., 2014; Wang and Lan, 2015; Ghosh et al., 2011). Consequently, we assume the necessary information for sense classification to be already accessible by our neural connective disambiguation model to some degree. Also, this approach elim3.1 Embedding-Based Connective Disambiguation For parsing explicit discourse relations, the first task usually involves the identification of"
2020.codi-1.7,P17-1093,0,0.0132118,"and low-dimensional word embeddings. Bai and Zhao (2018) use different levels of input representations, ranging from character level to contextualized word embeddings. Kishimoto et al. (2020) adapt BERT to perform implicit discourse sense classification. They show promising results by adding tasks, such as connective prediction, for pretraining. Multitask learning is also successfully applied to implicit sense classification (Liu et al., 2016). The authors combine four different tasks related to discourse parsing, but in contrast to our work, they rely on previously extracted argument spans. Qin et al. (2017) propose a model that, in addition to their main task (implicit sense classification), also learns to predict a possible connective that could be inserted. Lan et al. (2017) introduce various models that perform multitask learning, and their focus also lies on implicit sense classification. The connective and its explicit sense have a strong correlation as shown by Pitler and Nenkova (2009), who report accuracy higher than the interannotator agreement for their connective disambiguation features on coarse-grained level senses. Lin et al. (2014) use only context features and evaluate their work"
2020.codi-1.7,W17-4814,0,0.0183428,"t) 4. You should take the deal or even try to negotiate this price down. (Expansion.Alternative) 5. If things work out, then everybody will be happy. (Contingency.Condition) Introduction 6. While it is raining outside, I clean the dishes. (Temporal.Synchronous) Coherence is crucial for humans to be able to interpret text. The area of discourse parsing models this by identifying certain phrases (arguments) within a text and using discourse relations to unfold their underlying connections. These discourse relations and their understanding are important for tasks such as machine translation (Sim Smith, 2017), abstractive summarization (Wu and Hu, 2018), and text simplification (Zhong et al., 2020). A subset of these relations is signaled by specific words, socalled discourse connectives (or discourse markers or cues), and thus referred to as explicit discourse relations. However, such cues can be ambiguous, as they may signal more than one relation type or may not always function as a relation indicator. Two challenges arise1 —first, distinguishing connectives from words with mere sentential meaning: Shallow discourse parsing (SDP) is the area that builds models to uncover such discourse structur"
2020.codi-1.7,K15-2001,0,0.0608024,".74 1.59 1.00 0.49 0.40 0.25 0.20 0.03 Table 1: Class distribution of the training data. version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008). This corpus provides about 43,000 annotated discourse relations, of which roughly 18,000 are signalled by explicit discourse connectives. Those relations are further annotated with a three-level sense hierarchy (one or two senses per relation). All discourse relations consist of two arguments and are associated with one of various types; the focus of our work is on relations of the explicit type. The Shared Tasks at CoNLL 2015 and 2016 (Xue et al., 2015, 2016) used PDTB2 with minor changes. Successful systems were Wang et al. (2015); Wang and Lan (2016); Oepen et al. (2016). They largely follow a pipeline architecture (Lin et al., 2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Recently, PDTB3 (Prasad et al., 2018) was published, which extends the previous work with more available relations and corrects several former annotations. The authors also adjusted the relations’ sense labels for a more balanced class distribution. For the sa"
2020.codi-1.7,K16-2001,0,0.206648,"k learning with regard to discourse parsing is outlined. For connective disambiguation, Pitler and Nenkova (2009) defined a set of syntactic features extracted from constituency trees. Beside the connective’s surface and category information from related tree nodes (parent, siblings), they also used In our final comparison, Table 3 shows our bestperforming models for each category (standard vs. contextualized embeddings and connective disambiguation vs. joint training for sense classification). For comparison, we included test results from successful submissions to the CoNLL 2016 Shared Task (Xue et al., 2016)—in particular, results that were achieved for connective disambiguation in the first part of the Shared Task and results for explicit 71 Test Blind Model ? 1conn ? 1sense ? 1conn ? 1sense Ecnuc OPT Stepanov Soochow 93.96 94.43 92.43 94.74 90.13 90.13 — — 91.34 91.79 88.56 91.04 77.41 77.17 — — ctx-embd (bert-ctx-1) ctx-embd-mtl (bert-ctx-0) embd (wiki-ctx-1) embd-mtl (wiki-ctx-1) 97.32 97.18 92.09 92.63 — 86.26 — 80.39 96.98 96.28 88.74 89.74 — 71.69 — 62.74 Table 3: Task-related F1 scores. Results are taken from the CoNLL 2016 Shared Task website (http://www. cs.brandeis.edu/~clp/conll16st/r"
2020.coling-main.505,C18-1048,0,0.0212929,"ame sentence. Consider the example sentence ”But traders took profits and focused on crude oil inventories once that factor was eliminated.” (wsj 1932), where and is annotated as having sentential reading and once is annotated as having discourse reading. Including the candidate separately prevents feeding the classifier two identical samples with different labels. Since we use the base version of a German BERT model,10 this returns a 2304-dimensional vector.11 This is then fed as input to a MultiLayer Perceptron classifier, following earlier work on similar problems (Ostendorff et al., 2019; Bai and Zhao, 2018; Pacheco et al., 2016). BERT + DiMLex (surface form only) This system is essentially the same as the baseline system, but instead of using all connectives in the PCC as candidates, we now use all entries (plus their orthographical variants) of DiMLex as candidates. With the connectives in DiMLex being a superset of those in the PCC, in the PCC setup, this effectively only adds negative examples to the data; particular candidates occurring in the PCC, but never with discourse reading are now considered too. This adds 638 items to our data set. The main motivation for using all DiMLex entries a"
2020.coling-main.505,W18-5037,1,0.888469,"nd-to-end shallow discourse parsing (Xue et al., 2015; Xue et al., 2016) have spiked interest in the overarching task, which includes connective identification and sense classification. The two year’s winning systems (Wang and Lan, 2015; Oepen et al., 2016) report F1 scores of 94.16 and 94.4, respectively, for connective identification. For German, our language of interest, early work on connective identification is described in Dipper and Stede (2006), who use a subset of nine connectives and report an F1 -score of 93.95 for the functional disambiguation task on this subset. In earlier work (Bourgonje and Stede, 2018), we include all connectives present in the Potsdam Commentary Corpus and report an F1 -score of 83.89, by extending the syntactically inspired features of Pitler and Nenkova (2009). In addition, we discuss the effect of training data volumes for the connective identification task for English by iteratively down-sampling training data size and reporting the results. The work reported on in this paper improves upon Bourgonje and Stede (2018) by combining the syntactically inspired approach with a contextualised vector-based approach for connective identification (for German), and by introducing"
2020.coling-main.505,2020.lrec-1.133,1,0.893393,"in (Rutherford et al., 2017) for English and Chinese, but in our case, volumes are considerably lower still. The idea of augmenting neural approaches with external knowledge for the purpose of implicit sense classification is explored by Rutherford and Xue (2014), who use Brown cluster pairs and coreference patterns, and Kishimoto et al. (2018), who use ConceptNet in combination with coreference resolution. 3 These numbers are without error propagation. 5739 3 Data 3.1 Potsdam Commentary Corpus To train and evaluate our approach, we use the Potsdam Commentary Corpus (PCC) in its 2.2 version (Bourgonje and Stede, 2020). The PCC is a corpus of 176 news commentary articles, comprising ~33k words and 1,120 explicit discourse relations. It is a multi-layer corpus, annotated for, among others, coreference chains, information structure, RST trees, and sentential syntax. For the syntactic features used in our experiments though, we use automatically produced parse trees instead of gold syntax trees. We argue that this provides a more realistic impression of performance on new, incoming text, and it allows us to directly compare results based on additional data (see subsection 3.2), for which we have no gold syntax"
2020.coling-main.505,W18-5042,1,0.836729,"ge base in the form of a lexicon that (ideally) exhaustively lists the connectives of a particular language, augmented with additional information on ambiguity and potential senses. As pointed out earlier, such lexicons are already available for ten languages in a web-based database. For a language that does not have a connective lexicon, its creation is a relatively labour-intense task, but it can be sped up in various ways. Provided that a corpus with the required annotations is available, a lexicon can be at least semiautomatically extracted from the annotations. This approach was taken by Das et al. (2018), who extracted an English lexicon from the PDTB and the RST Signaling Corpus (Das and Taboada, 2018). If the required annotations are not available, or not on a scale supporting the extraction of a list anywhere near exhaustive, parallel corpora or machine translation, in combination with annotation projection can speed up development. An approach based on a parallel corpus is explored in Bourgonje et al. (2017). The combination of machine translation and annotation projection is described in Sluyter-G¨athje et al. (2020), who foremost aim to create German annotations, but populating (or exte"
2020.coling-main.505,N19-1423,0,0.0178048,"er 8-13, 2020 containing ~53k annotated relations in its 3.0 version (Prasad et al., 2019), any language other than English can be considered a low-resource language. However, in recent years, many connective lexicons, listing all connectives of the respective language and some of their properties, have become available for several languages.2 The main contribution of this paper is to investigate to what extent linguistic knowledge encoded in such lexicons can augment a purely-empirical approach to connective disambiguation in a low-resource scenario. Particularly, at first we fine-tune BERT (Devlin et al., 2019) to the task of both connective identification and sense classification for German, and then attempt to improve over this approach by exploiting DiMLex (Scheffler and Stede, 2016), a German connective lexicon. Additionally, we experiment with syntactically inspired features in the tradition of Pitler and Nenkova (2009). Our results demonstrate that exploiting a connective lexicon can improve performance both within-domain and across-domain in situations where limited training data is available. The rest of this paper is structured as follows: Section 2 provides a more detailed definition of co"
2020.coling-main.505,D15-1267,0,0.0258609,"tment is suspended from trading, the options on those investments also don’t trade. (wsj 1962) Differentiating between sentential and discourse reading is often referred to as connective identification, and classifying the particular sense of a connective, or the relation it is involved in, is often referred to as sense classification. Both are sub-tasks of discourse parsing, which in turn has applications, for example, in text summarisation (Schilder, 2002; Yoshida et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Joty et al., 2014; Sim Smith, 2017) and argumentation mining (Eckle-Kohler et al., 2015). The availability of annotated data for the task of discourse parsing as a whole, and consequently its sub-tasks, is limited. With the PDTB being by far the largest corpus annotated for discourse relations, This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 All examples are taken from the PDTB (Prasad et al., 2008). 5737 Proceedings of the 28th International Conference on Computational Linguistics, pages 5737–5748 Barcelona, Spain (Online), December 8-13, 2020 containing ~53k annotated relations"
2020.coling-main.505,W14-3352,0,0.0414182,"Missing"
2020.coling-main.505,C18-1049,0,0.0169392,"corpus, but do not attempt to classify the annotated instances and to the best of our knowledge, no prior work on sense classification for German exists. The combination of low volumes of training data and neural network architectures for discourse parsing has been discussed in (Rutherford et al., 2017) for English and Chinese, but in our case, volumes are considerably lower still. The idea of augmenting neural approaches with external knowledge for the purpose of implicit sense classification is explored by Rutherford and Xue (2014), who use Brown cluster pairs and coreference patterns, and Kishimoto et al. (2018), who use ConceptNet in combination with coreference resolution. 3 These numbers are without error propagation. 5739 3 Data 3.1 Potsdam Commentary Corpus To train and evaluate our approach, we use the Potsdam Commentary Corpus (PCC) in its 2.2 version (Bourgonje and Stede, 2020). The PCC is a corpus of 176 news commentary articles, comprising ~33k words and 1,120 explicit discourse relations. It is a multi-layer corpus, annotated for, among others, coreference chains, information structure, RST trees, and sentential syntax. For the syntactic features used in our experiments though, we use auto"
2020.coling-main.505,W12-0117,0,0.117156,", a company can do with it what it wishes. (wsj 0989) (4) Normally, once the underlying investment is suspended from trading, the options on those investments also don’t trade. (wsj 1962) Differentiating between sentential and discourse reading is often referred to as connective identification, and classifying the particular sense of a connective, or the relation it is involved in, is often referred to as sense classification. Both are sub-tasks of discourse parsing, which in turn has applications, for example, in text summarisation (Schilder, 2002; Yoshida et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Joty et al., 2014; Sim Smith, 2017) and argumentation mining (Eckle-Kohler et al., 2015). The availability of annotated data for the task of discourse parsing as a whole, and consequently its sub-tasks, is limited. With the PDTB being by far the largest corpus annotated for discourse relations, This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 All examples are taken from the PDTB (Prasad et al., 2008). 5737 Proceedings of the 28th International Conference on Computational Linguistics, pages 57"
2020.coling-main.505,K16-2002,1,0.923598,"having sentential reading. The same holds for senses, where many connectives can signal only one particular relation sense, rendering their sense classification 2 The connective lexicon platform connective-lex.info currently contains freely-available lexicons for Arabic, Bangla, Czech, Dutch, English, French, German, Italian, Portugese and Ukrainian. 5738 redundant. Others may signal several different senses, with their particular sense distribution again being heavily skewed (see Section 3.1 for examples). State of the art results in end-to-end shallow discourse parsing (Wang and Lan, 2015; Oepen et al., 2016) have been achieved using a pipeline architecture, introduced by Lin et al. (2014). In this pipeline, connective identification is the first, and explicit relation sense classification the third component (after argument extraction once a particular connective is located). Errors made here are propagated down the pipeline. Improving performance for these two sub-tasks can thus have major impact on end-to-end performance. The following subsection provides an overview of performance for these two tasks on different corpora and languages. 2.2 Related Work Because of the data situation, most work"
2020.coling-main.505,K16-2019,0,0.0125934,"er the example sentence ”But traders took profits and focused on crude oil inventories once that factor was eliminated.” (wsj 1932), where and is annotated as having sentential reading and once is annotated as having discourse reading. Including the candidate separately prevents feeding the classifier two identical samples with different labels. Since we use the base version of a German BERT model,10 this returns a 2304-dimensional vector.11 This is then fed as input to a MultiLayer Perceptron classifier, following earlier work on similar problems (Ostendorff et al., 2019; Bai and Zhao, 2018; Pacheco et al., 2016). BERT + DiMLex (surface form only) This system is essentially the same as the baseline system, but instead of using all connectives in the PCC as candidates, we now use all entries (plus their orthographical variants) of DiMLex as candidates. With the connectives in DiMLex being a superset of those in the PCC, in the PCC setup, this effectively only adds negative examples to the data; particular candidates occurring in the PCC, but never with discourse reading are now considered too. This adds 638 items to our data set. The main motivation for using all DiMLex entries as the candidate list is"
2020.coling-main.505,P09-2004,0,0.53617,"ilable for several languages.2 The main contribution of this paper is to investigate to what extent linguistic knowledge encoded in such lexicons can augment a purely-empirical approach to connective disambiguation in a low-resource scenario. Particularly, at first we fine-tune BERT (Devlin et al., 2019) to the task of both connective identification and sense classification for German, and then attempt to improve over this approach by exploiting DiMLex (Scheffler and Stede, 2016), a German connective lexicon. Additionally, we experiment with syntactically inspired features in the tradition of Pitler and Nenkova (2009). Our results demonstrate that exploiting a connective lexicon can improve performance both within-domain and across-domain in situations where limited training data is available. The rest of this paper is structured as follows: Section 2 provides a more detailed definition of connectives and discusses related work on connective identification and sense classification. Section 3 explains the resources we use in our experiments. Section 4 outlines our method of combining BERT with information from the connective lexicon, and also syntactically inspired features for connective disambiguation. Se"
2020.coling-main.505,prasad-etal-2008-penn,0,0.0385794,", in text summarisation (Schilder, 2002; Yoshida et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Joty et al., 2014; Sim Smith, 2017) and argumentation mining (Eckle-Kohler et al., 2015). The availability of annotated data for the task of discourse parsing as a whole, and consequently its sub-tasks, is limited. With the PDTB being by far the largest corpus annotated for discourse relations, This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 All examples are taken from the PDTB (Prasad et al., 2008). 5737 Proceedings of the 28th International Conference on Computational Linguistics, pages 5737–5748 Barcelona, Spain (Online), December 8-13, 2020 containing ~53k annotated relations in its 3.0 version (Prasad et al., 2019), any language other than English can be considered a low-resource language. However, in recent years, many connective lexicons, listing all connectives of the respective language and some of their properties, have become available for several languages.2 The main contribution of this paper is to investigate to what extent linguistic knowledge encoded in such lexicons can"
2020.coling-main.505,W08-1006,0,0.131681,"Missing"
2020.coling-main.505,E14-1068,0,0.0129064,"spoken language. The authors present statistics for sense distribution in their GECCo corpus, but do not attempt to classify the annotated instances and to the best of our knowledge, no prior work on sense classification for German exists. The combination of low volumes of training data and neural network architectures for discourse parsing has been discussed in (Rutherford et al., 2017) for English and Chinese, but in our case, volumes are considerably lower still. The idea of augmenting neural approaches with external knowledge for the purpose of implicit sense classification is explored by Rutherford and Xue (2014), who use Brown cluster pairs and coreference patterns, and Kishimoto et al. (2018), who use ConceptNet in combination with coreference resolution. 3 These numbers are without error propagation. 5739 3 Data 3.1 Potsdam Commentary Corpus To train and evaluate our approach, we use the Potsdam Commentary Corpus (PCC) in its 2.2 version (Bourgonje and Stede, 2020). The PCC is a corpus of 176 news commentary articles, comprising ~33k words and 1,120 explicit discourse relations. It is a multi-layer corpus, annotated for, among others, coreference chains, information structure, RST trees, and senten"
2020.coling-main.505,E17-1027,0,0.0118426,"report scores of 90.79 and 90.01, respectively.3 For German, earlier work (Kunz and Lapshinova-Koltunski, 2014) investigates connectives (referred to as cohesive conjunction strategies) in both German and English in both written and spoken language. The authors present statistics for sense distribution in their GECCo corpus, but do not attempt to classify the annotated instances and to the best of our knowledge, no prior work on sense classification for German exists. The combination of low volumes of training data and neural network architectures for discourse parsing has been discussed in (Rutherford et al., 2017) for English and Chinese, but in our case, volumes are considerably lower still. The idea of augmenting neural approaches with external knowledge for the purpose of implicit sense classification is explored by Rutherford and Xue (2014), who use Brown cluster pairs and coreference patterns, and Kishimoto et al. (2018), who use ConceptNet in combination with coreference resolution. 3 These numbers are without error propagation. 5739 3 Data 3.1 Potsdam Commentary Corpus To train and evaluate our approach, we use the Potsdam Commentary Corpus (PCC) in its 2.2 version (Bourgonje and Stede, 2020). T"
2020.coling-main.505,Y14-1052,0,0.0496847,"clusion of AltLex (for alternative lexicalisation) instances, indicating relations signaled by, in principle, open-class words and phrases, such as the reason is that. More recently, in the PDTB3, AltLexC instances are added, where a particular syntactic (as opposed to lexical) construction signals the discourse relation. With regard to connectives, their long tradition of research (Schiffrin, 1987; Redeker, 1991; Knott and Dale, 1994; Degand et al., 2013), has recently been discussed by Danlos et al. (2018), who, in addition, differentiate between primary and secondary connectives, following Rysova and Rysova (2014). In the DiMLex approach, the definition is based on Pasch et al. (2003), and then follows Stede (2002) by including certain prepositions. We thus adopt the characterisation that a lexical item X is a connective when: • • • • X is not inflectable, X expresses some specific, two-place semantic relation, the arguments of the relational meaning of X are propositional structures, the verbalisations of the arguments of the relational meaning of X can be clauses. Note that this definition does not include any syntactic categorisation, and following this definition, connectives are a heterogeneous gr"
2020.coling-main.505,L16-1160,1,0.852114,"r, in recent years, many connective lexicons, listing all connectives of the respective language and some of their properties, have become available for several languages.2 The main contribution of this paper is to investigate to what extent linguistic knowledge encoded in such lexicons can augment a purely-empirical approach to connective disambiguation in a low-resource scenario. Particularly, at first we fine-tune BERT (Devlin et al., 2019) to the task of both connective identification and sense classification for German, and then attempt to improve over this approach by exploiting DiMLex (Scheffler and Stede, 2016), a German connective lexicon. Additionally, we experiment with syntactically inspired features in the tradition of Pitler and Nenkova (2009). Our results demonstrate that exploiting a connective lexicon can improve performance both within-domain and across-domain in situations where limited training data is available. The rest of this paper is structured as follows: Section 2 provides a more detailed definition of connectives and discusses related work on connective identification and sense classification. Section 3 explains the resources we use in our experiments. Section 4 outlines our meth"
2020.coling-main.505,W17-4814,0,0.0529436,"Missing"
2020.coling-main.505,2020.lrec-1.131,1,0.841985,"Missing"
2020.coling-main.505,stede-neumann-2014-potsdam,1,0.892303,"Missing"
2020.coling-main.505,K15-2002,0,0.259508,"he other hand rarely having sentential reading. The same holds for senses, where many connectives can signal only one particular relation sense, rendering their sense classification 2 The connective lexicon platform connective-lex.info currently contains freely-available lexicons for Arabic, Bangla, Czech, Dutch, English, French, German, Italian, Portugese and Ukrainian. 5738 redundant. Others may signal several different senses, with their particular sense distribution again being heavily skewed (see Section 3.1 for examples). State of the art results in end-to-end shallow discourse parsing (Wang and Lan, 2015; Oepen et al., 2016) have been achieved using a pipeline architecture, introduced by Lin et al. (2014). In this pipeline, connective identification is the first, and explicit relation sense classification the third component (after argument extraction once a particular connective is located). Errors made here are propagated down the pipeline. Improving performance for these two sub-tasks can thus have major impact on end-to-end performance. The following subsection provides an overview of performance for these two tasks on different corpora and languages. 2.2 Related Work Because of the data"
2020.coling-main.505,K15-2001,0,0.0158148,") contains ~53k annotated relations, compared to just over 1k explicit relations in the German corpus we use (see Section 3.1 for details). For most other languages that have corpora annotated for discourse relations (see Zeldes et al. (2019) for an overview), the number of available annotations is equally low, yet connective lexicons may exist for these languages (see Stede et al. (2019) for an overview). Thus, our approach, applied to German, is potentially useful for other languages for which the required lexicon exists. Two consecutive shared tasks on end-to-end shallow discourse parsing (Xue et al., 2015; Xue et al., 2016) have spiked interest in the overarching task, which includes connective identification and sense classification. The two year’s winning systems (Wang and Lan, 2015; Oepen et al., 2016) report F1 scores of 94.16 and 94.4, respectively, for connective identification. For German, our language of interest, early work on connective identification is described in Dipper and Stede (2006), who use a subset of nine connectives and report an F1 -score of 93.95 for the functional disambiguation task on this subset. In earlier work (Bourgonje and Stede, 2018), we include all connective"
2020.coling-main.505,K16-2001,0,0.0145613,"notated relations, compared to just over 1k explicit relations in the German corpus we use (see Section 3.1 for details). For most other languages that have corpora annotated for discourse relations (see Zeldes et al. (2019) for an overview), the number of available annotations is equally low, yet connective lexicons may exist for these languages (see Stede et al. (2019) for an overview). Thus, our approach, applied to German, is potentially useful for other languages for which the required lexicon exists. Two consecutive shared tasks on end-to-end shallow discourse parsing (Xue et al., 2015; Xue et al., 2016) have spiked interest in the overarching task, which includes connective identification and sense classification. The two year’s winning systems (Wang and Lan, 2015; Oepen et al., 2016) report F1 scores of 94.16 and 94.4, respectively, for connective identification. For German, our language of interest, early work on connective identification is described in Dipper and Stede (2006), who use a subset of nine connectives and report an F1 -score of 93.95 for the functional disambiguation task on this subset. In earlier work (Bourgonje and Stede, 2018), we include all connectives present in the Po"
2020.coling-main.505,D14-1196,0,0.02784,"than rosy. (wsj 0564) (3) Once it gets there, a company can do with it what it wishes. (wsj 0989) (4) Normally, once the underlying investment is suspended from trading, the options on those investments also don’t trade. (wsj 1962) Differentiating between sentential and discourse reading is often referred to as connective identification, and classifying the particular sense of a connective, or the relation it is involved in, is often referred to as sense classification. Both are sub-tasks of discourse parsing, which in turn has applications, for example, in text summarisation (Schilder, 2002; Yoshida et al., 2014), machine translation (Meyer and Popescu-Belis, 2012; Joty et al., 2014; Sim Smith, 2017) and argumentation mining (Eckle-Kohler et al., 2015). The availability of annotated data for the task of discourse parsing as a whole, and consequently its sub-tasks, is limited. With the PDTB being by far the largest corpus annotated for discourse relations, This work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details: http://creativecommons.org/licenses/by/4.0/. 1 All examples are taken from the PDTB (Prasad et al., 2008). 5737 Proceedings of the 28th Internation"
2020.coling-main.508,W18-0701,1,0.897529,"Missing"
2020.coling-main.508,2020.findings-emnlp.222,1,0.840139,"Missing"
2020.coling-main.508,amoia-etal-2012-coreference,0,0.0170269,"Missing"
2020.coling-main.508,P02-1011,0,0.269992,"Missing"
2020.coling-main.508,W18-2406,0,0.0440645,"Missing"
2020.coling-main.508,W16-0704,0,0.0604175,"Missing"
2020.coling-main.508,W15-2521,0,0.0552344,"Missing"
2020.coling-main.508,P14-5010,0,0.00245327,"ad structure, introduced by the @ sign, are automatically added to the content of the reply message in Twitter. Since these are not inserted to the post intentionally by the user, we consider such usernames (≈ 5K in total) as part of the metadata of the tweet and do not count them as tokens in the text. Linguistic Annotation OntoNotes and Switchboard have gold part-of-speech (PoS) and syntax (constituency parse trees) annotation layers compatible with Penn TreeBank conventions (Taylor et al., 2003). TwiConv does not come with gold annotations for these layers. We thus use the Stanford parser (Manning et al., 2014) to automatically create the PoS and syntax annotations for Twitter texts, which are also compatible with PTB conventions. However, the predicted parses are not reliably accurate for tweet texts, and therefore we manually checked and corrected the structures computed through these parses in our analysis. The applied procedures are described below. The other annotation layer of interest is the coreference annotations. All three corpora contain gold annotations for coreference, but with various differences in the definition of markables. For instance, in OntoNotes and Switchboard, singletons, co"
2020.coling-main.508,W13-3516,0,0.0653623,"Missing"
2020.coling-main.508,W16-2607,0,0.0277034,"Missing"
2020.coling-main.508,P03-1022,0,0.305866,"Missing"
2020.coling-main.508,uryupina-poesio-2012-domain,0,0.0616412,"Missing"
2020.coling-main.508,W18-0704,0,0.0335992,"Missing"
2020.findings-emnlp.222,W18-0701,1,0.664991,"Missing"
2020.findings-emnlp.222,D15-1271,0,0.0198621,"eference resolution, but at the same time highly relevant for many applications that seek to extract information or opinions from users’ messages. In this paper, we use a state-of-the-art resolution system built with the OntoNotes corpus (Pradhan To our knowledge, there is no work specifically on adapting coreference resolution to Twitter, other than the aforementioned study of Aktas¸ et al. (2018), which showed a significant drop in performance when a system with OntoNotes models is applied to Twitter. More generally, one of the few studies on domain adaptation for coreference resolution is (Do et al., 2015), which adapts the Berkeley system (Durrett and Klein, 2013) to narrative stories. Do et al. do not retrain the system but add linguistic features of narratives as soft constraints to the resolver. – At the same time, Twitter-adaptation has been investigated for other NLP tasks, such as NER. As an example, in (Ritter et al., 2011), performance is measured using tools trained with Twitter-related and out-of-domain data. Regarding OntoNotes genre differences, Uryupina and Poesio (2012) and Pradhan et al. (2013) report varying performance in coreference resolution for distinct corpus sections; th"
2020.findings-emnlp.222,D13-1203,0,0.0176043,"evant for many applications that seek to extract information or opinions from users’ messages. In this paper, we use a state-of-the-art resolution system built with the OntoNotes corpus (Pradhan To our knowledge, there is no work specifically on adapting coreference resolution to Twitter, other than the aforementioned study of Aktas¸ et al. (2018), which showed a significant drop in performance when a system with OntoNotes models is applied to Twitter. More generally, one of the few studies on domain adaptation for coreference resolution is (Do et al., 2015), which adapts the Berkeley system (Durrett and Klein, 2013) to narrative stories. Do et al. do not retrain the system but add linguistic features of narratives as soft constraints to the resolver. – At the same time, Twitter-adaptation has been investigated for other NLP tasks, such as NER. As an example, in (Ritter et al., 2011), performance is measured using tools trained with Twitter-related and out-of-domain data. Regarding OntoNotes genre differences, Uryupina and Poesio (2012) and Pradhan et al. (2013) report varying performance in coreference resolution for distinct corpus sections; this work inspired our experiments reported in the following."
2020.findings-emnlp.222,N18-2108,0,0.0495417,"Missing"
2020.findings-emnlp.222,N18-1202,0,0.0132684,"units of independent annotation). ONT tc bc bn mz nw pt wb TW’ docs 2632 111 284 711 410 622 320 174 185 tokens 1289K 81K 144K 172K 164K 387K 210K 131K 48K chains 34K 1931 4236 6138 3534 9404 6611 2993 1534 Figure 1: A thread sample in TW ONT; we thus call the dataset TW’ here. 3 mentions 152K 12K 18K 21K 13K 34K 42K 12K 6K Experiments For our experiments, we chose ’e2e-coref’ (Lee et al., 2018), an update of the end-to-end neural coreference resolver presented at EMNLP 2017. It introduced a refined approach based on differentiable approximation to higher-order inference, and ELMo embeddings (Peters et al., 2018) for span scoring, which significantly improved performance on English ONT. The approach achieved 73.0 F1, representing the 2018 state-of-art. Due to its cost efficiency, speed and flexibility, it was later used as basis for several recent state-of-art models, including SpanBERT (Joshi et al., 2020). Table 1: Corpus size and basic coreference statistics Our second dataset is the Twitter Conversation corpus (TW) presented in (Aktas¸ et al., 2018). They are tree structures where each tweet has a parent (i.e. the tweet it is replied-to) except for the initial tweet starting the conversation. A tr"
2020.findings-emnlp.222,D11-1141,0,0.0834732,"n to Twitter, other than the aforementioned study of Aktas¸ et al. (2018), which showed a significant drop in performance when a system with OntoNotes models is applied to Twitter. More generally, one of the few studies on domain adaptation for coreference resolution is (Do et al., 2015), which adapts the Berkeley system (Durrett and Klein, 2013) to narrative stories. Do et al. do not retrain the system but add linguistic features of narratives as soft constraints to the resolver. – At the same time, Twitter-adaptation has been investigated for other NLP tasks, such as NER. As an example, in (Ritter et al., 2011), performance is measured using tools trained with Twitter-related and out-of-domain data. Regarding OntoNotes genre differences, Uryupina and Poesio (2012) and Pradhan et al. (2013) report varying performance in coreference resolution for distinct corpus sections; this work inspired our experiments reported in the following. Section 2 describes our data sets, and Section 3 the experiments. Section 4 provides various additional analyses that shed light on the domain adaptation problem, and Section 5 concludes. ∗ * indicates equal contribution. 2454 Findings of the Association for Computational"
2020.findings-emnlp.222,uryupina-poesio-2012-domain,0,0.331646,"models is applied to Twitter. More generally, one of the few studies on domain adaptation for coreference resolution is (Do et al., 2015), which adapts the Berkeley system (Durrett and Klein, 2013) to narrative stories. Do et al. do not retrain the system but add linguistic features of narratives as soft constraints to the resolver. – At the same time, Twitter-adaptation has been investigated for other NLP tasks, such as NER. As an example, in (Ritter et al., 2011), performance is measured using tools trained with Twitter-related and out-of-domain data. Regarding OntoNotes genre differences, Uryupina and Poesio (2012) and Pradhan et al. (2013) report varying performance in coreference resolution for distinct corpus sections; this work inspired our experiments reported in the following. Section 2 describes our data sets, and Section 3 the experiments. Section 4 provides various additional analyses that shed light on the domain adaptation problem, and Section 5 concludes. ∗ * indicates equal contribution. 2454 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2454–2460 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Data For our experiments,1 we use the E"
2020.findings-emnlp.222,W13-3516,0,0.128083,"Missing"
2020.findings-emnlp.222,W12-4501,0,0.0318613,"orpus sections; this work inspired our experiments reported in the following. Section 2 describes our data sets, and Section 3 the experiments. Section 4 provides various additional analyses that shed light on the domain adaptation problem, and Section 5 concludes. ∗ * indicates equal contribution. 2454 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2454–2460 c November 16 - 20, 2020. 2020 Association for Computational Linguistics 2 Data For our experiments,1 we use the English portion of the OntoNotes benchmark used as training set in the CoNLL-2012 shared task (Pradhan et al., 2012). It has texts from spoken and written registers, and contains gold annotations at different layers, including coreference chains, i.e., sets of mentions referring to the same entity. Spoken data includes telephone conversations (tc), broadcast conversations (bc), and broadcast news (bn); written data contains magazine (mz), newswire (nw), pivot text (pt) and web blogs (wb). As shown in Table 1, the ONT corpus contains 1289K tokens in 2632 documents (in CoNLL terminology, documents are the units of independent annotation). ONT tc bc bn mz nw pt wb TW’ docs 2632 111 284 711 410 622 320 174 185"
2020.lrec-1.131,2020.lrec-1.133,1,0.579762,"for coherence relations, with over 1m words and over 40k annotated relations in its 2.0 version. Other corpora annotated for coherence relations are considerably smaller, and also distributed over different frameworks, most notably Rhetorical Structure Theory (Mann and Thompson, 1988) and Segmented Discourse Representation Theory (Asher and Lascarides, 2005). We refer the reader to Zeldes et al. (2019) for an overview of corpora for different languages and frameworks. For German, our language of interest, to date the largest annotated corpus is the Potsdam Commentary Corpus (henceforth: PCC, (Bourgonje and Stede, 2020)) and a smaller corpus exists as a discourse annotation layer over parts of ¨ the TUBA-D/Z corpus (Versley and Gastel, 2012). The PCC contains 2,208 relations, annotated according to the guidelines used for the PDTB2 (Prasad et al., 2008). Because of the much larger size of the PDTB, in our experiments we hope to collect considerably more instances of discourse relations in German. Through our method, exploiting machine translation and annotation projection, we will extract silver data in the sense that the resulting annotations cannot be guaranteed to be correct (i.e., they are not all checke"
2020.lrec-1.131,P07-2045,0,0.0290566,"and English-French (Laali, 2017)). The novelty of our work lies in using the procedure for entire coherence relations, as opposed to restricting it to connectives. In contrast to Versley (2010) and Laali and Kosseim (2014) who use existing parallel corpora for which they automatically annotated the English side, we create a parallel corpus by machine-translating the manually annotated PDTB. After machine translation, we rely on word alignments produced with GIZA++ (Och and Ney, 2003) that are post-processed using some heuristics implemented in the Moses statistical machine translation system (Koehn et al., 2007). In addition, we perform an automatic corpus analysis to examine how different types of annotation interdepend, and accordingly we compile rules for the projection process. 3. Annotation Structure In the PDTB framework, coherence annotations are divided into five different relation types. (1) Explicit relations consist of an overtly realised discourse connective (such as because, although, if ) and two arguments; one external argument (Arg1) and one internal argument (Arg2), the latter being syntactically integrated with the discourse connective. Finally, they contain a relation sense, to be"
2020.lrec-1.131,C14-1058,0,0.0161412,"n, though the next sections describe heuristics for quality assurance), but because of the much larger size of the PDTB, we end up with many more instances of relations in German, and from a slightly different domain, with the PCC representing 1044 the news editorial/commentary domain, and PDTB articles representing the financial news domain. The procedure of annotation projection has been used in the context of coherence relations before, but remained restricted to explicit discourse connectives, for example to create or extend discourse lexicons and disambiguate connectives (English-French (Laali and Kosseim, 2014), English-Chinese (Zhou et al., 2012) and German-Italian (Bourgonje et al., 2017)), to compile a metric to score machine translation output (English-Arabic (Hajlaoui and Popescu-Belis, 2013)) or to create a corpus annotated with discourse markers to train a parser (English-German (Versley, 2010) and English-French (Laali, 2017)). The novelty of our work lies in using the procedure for entire coherence relations, as opposed to restricting it to connectives. In contrast to Versley (2010) and Laali and Kosseim (2014) who use existing parallel corpora for which they automatically annotated the Eng"
2020.lrec-1.131,K16-2008,0,0.0239106,"he classifier described in (Bourgonje and Stede, 2018), on the GermanPDTB data, we get a binary f1-score of 94.04. When using the same classifier on the English PDTB, Bourgonje and Stede (2018) report a very similar binary f1-score of 93.64. Comparing this, in turn, to the English competition, we note that the overall winning system of the 2016 CoNLL shared task on discourse parsing (Oepen et al., 2016) reports an f1score of 91.79 for the sub-task of connective disambiguation. The system with the highest score for this sub-task in that same competition, however, achieved an f1-score of 98.38 (Li et al., 2016). We suspect the difference in performance to be due to language-specifics, similar to those reported in Section 5., where German in some cases tends to implicit realisation, whereas English uses an explicit form. Further investigation would be needed to find the root cause of the 0.4 point difference in f1-score, but we consider the fact that scores are relatively close together a confirmation of generally good quality which we observed from manual evaluation in Section 6.1. 2. Minor word errors: One word is not included in the annotation even though it is included in the PDTB relation, or vi"
2020.lrec-1.131,W13-3303,0,0.105909,"licit connective only, while regarding the modifier as an optional element (some of those are focus particles whose combination with connectives is restricted, which is also recorded in the DiMLex entries). To be able to reliably evaluate the explicit connectives we only annotate the ”pure” form in the GermanPDTB, i.e., we iterate over all (German) words that are aligned to the (English) explicit connective and only annotate the ones matching an entry in DiMLex. After this step, some explicit connectives were found to be correctly aligned yet not present in DiMLex. For such cases, inspired by Meyer and Webber (2013), we extracted all explicit discourse markers from the PDTB, translated them with DeepL, checked them against DiMLex and discussed the ones not yet present. This resulted in 17 candidates that can be considered as additions to DiMLex. 4.4. Projection We project the annotations from the English to the German side of the parallel corpus sentence-wise and relation-wise (several discourse relations can be annotated for one sentence). To not rely on the word alignments alone, we conducted an automatic analysis of the PDTB and compiled rules for the projection of the arguments and for the projection"
2020.lrec-1.131,D13-1032,0,0.0296516,"Missing"
2020.lrec-1.131,J03-1002,0,0.0346851,"nd Popescu-Belis, 2013)) or to create a corpus annotated with discourse markers to train a parser (English-German (Versley, 2010) and English-French (Laali, 2017)). The novelty of our work lies in using the procedure for entire coherence relations, as opposed to restricting it to connectives. In contrast to Versley (2010) and Laali and Kosseim (2014) who use existing parallel corpora for which they automatically annotated the English side, we create a parallel corpus by machine-translating the manually annotated PDTB. After machine translation, we rely on word alignments produced with GIZA++ (Och and Ney, 2003) that are post-processed using some heuristics implemented in the Moses statistical machine translation system (Koehn et al., 2007). In addition, we perform an automatic corpus analysis to examine how different types of annotation interdepend, and accordingly we compile rules for the projection process. 3. Annotation Structure In the PDTB framework, coherence annotations are divided into five different relation types. (1) Explicit relations consist of an overtly realised discourse connective (such as because, although, if ) and two arguments; one external argument (Arg1) and one internal argum"
2020.lrec-1.131,K16-2002,1,0.782119,"n between two propositions, and puts them in a contrastive relation. Our task entails binary classification, classifying candidates as having either sentential (as in (1)) or discourse (as in (2)) reading. Using the classifier described in (Bourgonje and Stede, 2018), on the GermanPDTB data, we get a binary f1-score of 94.04. When using the same classifier on the English PDTB, Bourgonje and Stede (2018) report a very similar binary f1-score of 93.64. Comparing this, in turn, to the English competition, we note that the overall winning system of the 2016 CoNLL shared task on discourse parsing (Oepen et al., 2016) reports an f1score of 91.79 for the sub-task of connective disambiguation. The system with the highest score for this sub-task in that same competition, however, achieved an f1-score of 98.38 (Li et al., 2016). We suspect the difference in performance to be due to language-specifics, similar to those reported in Section 5., where German in some cases tends to implicit realisation, whereas English uses an explicit form. Further investigation would be needed to find the root cause of the 0.4 point difference in f1-score, but we consider the fact that scores are relatively close together a confi"
2020.lrec-1.131,P02-1040,0,0.106839,"in a relatively exhaustive and stable lexicon of German discourse connectives. Still, in the process of creating the GermanPDTB, we found several items we consider candidate entries for the lexicon. 4.1. Creation of the Parallel Corpus We use machine translation to produce a parallel corpus. We considered and tested five different systems – Google Translate1 , DeepL2 , Bing3 , Edinburgh’s Neural Machine Translation system (Sennrich et al., 2016) and Moses (Koehn et al., 2007) – by translating the English side of a parallel news corpus (Tiedemann, 2012) and scoring the translation using BLEU (Papineni et al., 2002). Google Translate and DeepL produced the best translations with BLEU scores of 26.6 and 28.07 respectively, so we proceeded with these two systems and translated the English raw text of the PDTB. Though the BLEU scores are not particularly good, we determined by manual inspection that the translations can generally be considered good enough for creating the corpus. Next, we performed a separate manual evaluation on a subset of 50 sentences, following the approach proposed by Popovic et al. (2013). Since the translations for these 50 sentences were of equal quality for both systems, we determi"
2020.lrec-1.131,2013.mtsummit-posters.5,0,0.016792,"English side of a parallel news corpus (Tiedemann, 2012) and scoring the translation using BLEU (Papineni et al., 2002). Google Translate and DeepL produced the best translations with BLEU scores of 26.6 and 28.07 respectively, so we proceeded with these two systems and translated the English raw text of the PDTB. Though the BLEU scores are not particularly good, we determined by manual inspection that the translations can generally be considered good enough for creating the corpus. Next, we performed a separate manual evaluation on a subset of 50 sentences, following the approach proposed by Popovic et al. (2013). Since the translations for these 50 sentences were of equal quality for both systems, we determined for which one a direct alignment (German-English) retrieved more explicit connectives. As this was the case for the DeepL translation, we continued to work with this system. 4.2. Alignment Heuristics Having obtained parallel English-German sentences, we proceeded with extracting word alignment using GIZA++. 1 https://translate.google.com/ https://www.deepl.com/en/translator 3 https://www.bing.com/translator 2 1045 First experiments with direct alignments were not promising and we encountered s"
2020.lrec-1.131,W06-0305,0,0.0541767,"ingly, Arg1 spans are easier to detect in the GermanPDTB, while Arg2 spans are more difficult to detect, compared to the original PDTB. We refer the reader to (Bourgonje and Stede, 2019), Section 5 for more details on how this compares to other competitors. Upon manual investigation, we found that for both argument types (Arg1 and Arg2), attribution was a frequent source of error. The heuristics described in (Bourgonje and Stede, 2019) were devised based on the PCC, which consists of news commentary and contains very few cases of attribution. The PDTB contains such cases much more frequently (Prasad et al., 2006), and the token span expressing the attribution is typically left out of the annotated argument, but is included by the heuristics. This is supported by the lower precision and higher recall for both Arg1 spans and Arg2 spans (59.08 (precision), 66.25 (recall) and 78.79 (precision), 84.04 (recall), respectively). This, however, impacts both the German and English processing, and does not explain the difference in performance between the two. We leave further investigation into the cause for this difference to future work. 7. Conclusion & Future Work We demonstrate how a large discourse-annotat"
2020.lrec-1.131,prasad-etal-2008-penn,0,0.4493,"on the availability of training data annotated for the type of information to be extracted. In the case of coherence relations, such annotations are notoriously difficult and time-consuming to obtain, and inter-annotator agreement rates are lower than for many other tasks. As a result, the amount of available training data is comparatively small, especially for languages other than English (see Section 2.). In this paper, we present a corpus annotated for discourse relations obtained through automatically translating an existing English corpus (the Penn Discourse TreeBank, henceforth: PDTB, (Prasad et al., 2008)), and using word alignment to project the English annotations on the German target text. The result is the GermanPDTB, a German corpus annotated for shallow discourse relations in the (financial) news domain. We provide details on the method used to create this corpus, sum up the key characteristics and use the GermanPDTB to enrich a pre-existing German connective lexicon. In addition, we provide an extrinsic evaluation of the corpus using components of a German discourse parser and compare performance of selected (sub-)tasks on GermanPDTB to the original English PDTB. The rest of this paper"
2020.lrec-1.131,W16-2323,0,0.0162108,"already existing German lexicon of connectives: DiMLex (Stede, 2002). First introduced in 1998, this lexicon has been extended and refined over the last 20 years, resulting in a relatively exhaustive and stable lexicon of German discourse connectives. Still, in the process of creating the GermanPDTB, we found several items we consider candidate entries for the lexicon. 4.1. Creation of the Parallel Corpus We use machine translation to produce a parallel corpus. We considered and tested five different systems – Google Translate1 , DeepL2 , Bing3 , Edinburgh’s Neural Machine Translation system (Sennrich et al., 2016) and Moses (Koehn et al., 2007) – by translating the English side of a parallel news corpus (Tiedemann, 2012) and scoring the translation using BLEU (Papineni et al., 2002). Google Translate and DeepL produced the best translations with BLEU scores of 26.6 and 28.07 respectively, so we proceeded with these two systems and translated the English raw text of the PDTB. Though the BLEU scores are not particularly good, we determined by manual inspection that the translations can generally be considered good enough for creating the corpus. Next, we performed a separate manual evaluation on a subset"
2020.lrec-1.131,tiedemann-2012-parallel,0,0.0299254,"en extended and refined over the last 20 years, resulting in a relatively exhaustive and stable lexicon of German discourse connectives. Still, in the process of creating the GermanPDTB, we found several items we consider candidate entries for the lexicon. 4.1. Creation of the Parallel Corpus We use machine translation to produce a parallel corpus. We considered and tested five different systems – Google Translate1 , DeepL2 , Bing3 , Edinburgh’s Neural Machine Translation system (Sennrich et al., 2016) and Moses (Koehn et al., 2007) – by translating the English side of a parallel news corpus (Tiedemann, 2012) and scoring the translation using BLEU (Papineni et al., 2002). Google Translate and DeepL produced the best translations with BLEU scores of 26.6 and 28.07 respectively, so we proceeded with these two systems and translated the English raw text of the PDTB. Though the BLEU scores are not particularly good, we determined by manual inspection that the translations can generally be considered good enough for creating the corpus. Next, we performed a separate manual evaluation on a subset of 50 sentences, following the approach proposed by Popovic et al. (2013). Since the translations for these"
2020.lrec-1.131,C12-2138,0,0.0624469,"Missing"
2020.lrec-1.133,W18-5037,1,0.877126,"reference, and rhetorical structure. In Stede and Neumann (2014), an updated version (PCC 2.0) was presented. In addition to revisions on the rhetorical structure and coreference layers, it introduced a new layer consisting of discourse connectives and their arguments. The annotation scheme for this layer was based on that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). In contrast to the PDTB however, the connectives and arguments layer of the PCC 2.0 contained instances of explicit relations only, and also it did not contain the senses for its explicit relations. More recently, Bourgonje and Stede (2018b) introduced the PCC 2.1, in which a new layer containing information-structural aboutness topics and automatically produced dependency parses using the Universal Dependencies scheme were added. As part of the 2.1 release, the entire corpus was made available in the interactive corpus browser ANNIS3 (Krause and Zeldes, 2016). In this paper, we present the PCC 2.2. In addition to a revision of the connectives and arguments layer, resulting in correction of some minor inconsistencies, and a conversion from inline XML to standoff XML format (more convenient for automatic processing), the key con"
2020.lrec-1.133,J15-3002,0,0.0262226,"languages and frameworks, are conveniently summarised by Zeldes et al. (2019). Not included in this overview are the Prague Discourse Treebank (Rysov´a et al., 2016), which contains Czech texts, and the corpus described by Zeyrek et al. (2019), which includes six different languages. Both use the PDTB guidelines as a starting point for annotations. Annotations of this kind are typically exploited for the task of discourse parsing. Some discourse parsers that exploit Rhetorical Structure trees (Mann and Thompson, 1988) are 1061 described by Soricut and Marcu (2003), Hernault et al. (2010) and Joty et al. (2015). Especially more recently though, encouraged by the 2015 and 2016 CoNLL shared tasks on shallow1 discourse parsing, but also generally reflected by available training data volumes, using PDTB for this task is relatively popular (Lin et al., 2014; Oepen et al., 2016; Li et al., 2016; Wang and Lan, 2015). With the systems cited above working for English and Chinese, for German, to the best of our knowledge, no end-to-end system for discourse parsing is available. Smaller sub-tasks are discussed by Dipper and Stede (2006) and Bourgonje and Stede (2018a) (connective disambiguation) and Bourgonje"
2020.lrec-1.133,C16-2026,0,0.046033,"Missing"
2020.lrec-1.133,K16-2008,0,0.433946,"Missing"
2020.lrec-1.133,K16-2002,1,0.791883,"ifferent languages. Both use the PDTB guidelines as a starting point for annotations. Annotations of this kind are typically exploited for the task of discourse parsing. Some discourse parsers that exploit Rhetorical Structure trees (Mann and Thompson, 1988) are 1061 described by Soricut and Marcu (2003), Hernault et al. (2010) and Joty et al. (2015). Especially more recently though, encouraged by the 2015 and 2016 CoNLL shared tasks on shallow1 discourse parsing, but also generally reflected by available training data volumes, using PDTB for this task is relatively popular (Lin et al., 2014; Oepen et al., 2016; Li et al., 2016; Wang and Lan, 2015). With the systems cited above working for English and Chinese, for German, to the best of our knowledge, no end-to-end system for discourse parsing is available. Smaller sub-tasks are discussed by Dipper and Stede (2006) and Bourgonje and Stede (2018a) (connective disambiguation) and Bourgonje and Stede (2019) (argument extraction). The main motivation for augmenting the connectives and arguments layer with senses and adding additional relation types to the corpus is to increase usability of the PCC for the task of discourse parsing. 3. Annotation Structu"
2020.lrec-1.133,prasad-etal-2008-penn,0,0.842698,"llowing investigation of different linguistic phenomena related to subjectivity and argumentation and the ways they interact on syntactic, semantic and discourse level. As such, the original version was annotated for sentence syntax, coreference, and rhetorical structure. In Stede and Neumann (2014), an updated version (PCC 2.0) was presented. In addition to revisions on the rhetorical structure and coreference layers, it introduced a new layer consisting of discourse connectives and their arguments. The annotation scheme for this layer was based on that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). In contrast to the PDTB however, the connectives and arguments layer of the PCC 2.0 contained instances of explicit relations only, and also it did not contain the senses for its explicit relations. More recently, Bourgonje and Stede (2018b) introduced the PCC 2.1, in which a new layer containing information-structural aboutness topics and automatically produced dependency parses using the Universal Dependencies scheme were added. As part of the 2.1 release, the entire corpus was made available in the interactive corpus browser ANNIS3 (Krause and Zeldes, 2016). In this paper, we present the"
2020.lrec-1.133,N03-1030,0,0.125489,"d for coherence relations, distributed over different languages and frameworks, are conveniently summarised by Zeldes et al. (2019). Not included in this overview are the Prague Discourse Treebank (Rysov´a et al., 2016), which contains Czech texts, and the corpus described by Zeyrek et al. (2019), which includes six different languages. Both use the PDTB guidelines as a starting point for annotations. Annotations of this kind are typically exploited for the task of discourse parsing. Some discourse parsers that exploit Rhetorical Structure trees (Mann and Thompson, 1988) are 1061 described by Soricut and Marcu (2003), Hernault et al. (2010) and Joty et al. (2015). Especially more recently though, encouraged by the 2015 and 2016 CoNLL shared tasks on shallow1 discourse parsing, but also generally reflected by available training data volumes, using PDTB for this task is relatively popular (Lin et al., 2014; Oepen et al., 2016; Li et al., 2016; Wang and Lan, 2015). With the systems cited above working for English and Chinese, for German, to the best of our knowledge, no end-to-end system for discourse parsing is available. Smaller sub-tasks are discussed by Dipper and Stede (2006) and Bourgonje and Stede (20"
2020.lrec-1.133,C04-1061,1,0.599202,"tive later. For AltLexes, the annotator also had to assign a relation sense. If no clear relation sense could be inferred for a given sentence pair, there was the option to select either an EntRel (for cases where the two sentences talk about the same entities, but do not clearly express a particular kind of relation between the propositions expressed therein) or a NoRel (for sentences that were not related in any of the above described ways). Regarding the actual tools used for the annotation, two particularly relevant ones specifically aimed at discourse relation annotation exist: Connanno (Stede and Heintze, 2004) and the PDTB Annotator (Lee et al., 2016). The explicit relations in the PCC had been annotated using Connanno. However, since Conanno is triggered by (explicit) connectives and skips any text not containing a connective, it is not useful for relations without overtly realised connectives. For explicit connectives, it allows selecting the sense from a dropdown-menu, limiting the annotator to a set of senses pre-defined by the dictionary used by Conanno. Since we wanted to allow for additional (new) senses for given connectives, we did not use Connanno for the sense annotation task either. The"
2020.lrec-1.133,stede-neumann-2014-potsdam,1,0.931071,"k words in over 2,100 sentences, sourced from a German regional newspaper. The main idea behind the PCC was to provide a single-genre corpus, collected in an ”unbalanced” way, to specifically address research questions on subjectivity and argumentation. It was manually annotated on several different levels independently, allowing investigation of different linguistic phenomena related to subjectivity and argumentation and the ways they interact on syntactic, semantic and discourse level. As such, the original version was annotated for sentence syntax, coreference, and rhetorical structure. In Stede and Neumann (2014), an updated version (PCC 2.0) was presented. In addition to revisions on the rhetorical structure and coreference layers, it introduced a new layer consisting of discourse connectives and their arguments. The annotation scheme for this layer was based on that of the Penn Discourse TreeBank (PDTB) (Prasad et al., 2008). In contrast to the PDTB however, the connectives and arguments layer of the PCC 2.0 contained instances of explicit relations only, and also it did not contain the senses for its explicit relations. More recently, Bourgonje and Stede (2018b) introduced the PCC 2.1, in which a n"
2020.lrec-1.133,K15-2002,0,0.0471195,"guidelines as a starting point for annotations. Annotations of this kind are typically exploited for the task of discourse parsing. Some discourse parsers that exploit Rhetorical Structure trees (Mann and Thompson, 1988) are 1061 described by Soricut and Marcu (2003), Hernault et al. (2010) and Joty et al. (2015). Especially more recently though, encouraged by the 2015 and 2016 CoNLL shared tasks on shallow1 discourse parsing, but also generally reflected by available training data volumes, using PDTB for this task is relatively popular (Lin et al., 2014; Oepen et al., 2016; Li et al., 2016; Wang and Lan, 2015). With the systems cited above working for English and Chinese, for German, to the best of our knowledge, no end-to-end system for discourse parsing is available. Smaller sub-tasks are discussed by Dipper and Stede (2006) and Bourgonje and Stede (2018a) (connective disambiguation) and Bourgonje and Stede (2019) (argument extraction). The main motivation for augmenting the connectives and arguments layer with senses and adding additional relation types to the corpus is to increase usability of the PCC for the task of discourse parsing. 3. Annotation Structure & Method An example of the current"
2020.lrec-1.138,L18-1288,1,0.187253,"niversity of Potsdam, Mihup Communications, Jadavpur University dasdebop@bu-berlin.de, stede@uni-potsdam.de, soumya@mihup.com, lahari.chatterjee@gmail.com Abstract We present DiMLex-Bangla, a newly developed lexicon of discourse connectives in Bangla. The lexicon, upon completion of its first version, contains 123 Bangla connective entries, which are primarily compiled from the linguistic literature and translation of English discourse connectives. The lexicon compilation is later augmented by adding more connectives from a currently developed corpus, called the Bangla RST Discourse Treebank (Das and Stede, 2018). DiMLex-Bangla provides information on syntactic categories of Bangla connectives, their discourse semantics and non-connective uses (if any). It uses the format of the German connective lexicon DiMLex (Stede and Umbach, 1998), which provides a cross-linguistically applicable XML schema. The resource is the first of its kind in Bangla, and is freely available for use in studies on discourse structure and computational applications. Keywords: discourse connectives, lexicon, discourse relation, Bangla 1. Introduction Successful interpretation of texts (at the cognitive level) and the establishm"
2020.lrec-1.138,W15-0501,0,0.0402866,"Missing"
2020.lrec-1.138,W11-2022,0,0.0204256,"of discourse relations that hold between text segments. Discourse relations are often indicated by discourse connectives (and, but, however, thus, etc.), which are widely believed to be the most explicit, most prototypical and most reliable relational signals (Danlos et al., 2018). Accordingly, the role of connectives in discourse interpretation has been extensively studied in both (psycholinguistic) discourse processing (see Kleijn et al. (2019) for an overview) and computational discourse applications such as discourse parsing (Hernault et al., 2010; Lin et al., 2014), machine translation (Meyer et al., 2011), text summarization (Alemany, 2005), or argumentation mining (Kirschner et al., 2015). This has led to the production of an increasingly large number of text corpora in different languages, annotated for discourse connectives (most often, in addition to other signals), based on different discourse frameworks (for references, see Das and Taboada (2019)). Parallel to the practice of developing discourse-annotated corpora, initiatives have also been taken to construct ancillary resources such as lexicons of discourse connectives, which can provide the list of connectives in a language, along wit"
2020.lrec-1.138,W15-2132,0,0.052283,"Missing"
2020.lrec-1.138,P98-2202,1,0.137639,"urse connectives in Bangla. The lexicon, upon completion of its first version, contains 123 Bangla connective entries, which are primarily compiled from the linguistic literature and translation of English discourse connectives. The lexicon compilation is later augmented by adding more connectives from a currently developed corpus, called the Bangla RST Discourse Treebank (Das and Stede, 2018). DiMLex-Bangla provides information on syntactic categories of Bangla connectives, their discourse semantics and non-connective uses (if any). It uses the format of the German connective lexicon DiMLex (Stede and Umbach, 1998), which provides a cross-linguistically applicable XML schema. The resource is the first of its kind in Bangla, and is freely available for use in studies on discourse structure and computational applications. Keywords: discourse connectives, lexicon, discourse relation, Bangla 1. Introduction Successful interpretation of texts (at the cognitive level) and the establishment of valid text structures (at the metadiscourse level) largely depend on the identification of the presence and type of discourse relations that hold between text segments. Discourse relations are often indicated by discours"
2020.lrec-1.138,W16-1704,0,0.0203625,"argely substituting the use of Sadhubhasha in a wide range of contexts in the present era. In DiMLex-Bangla, we primarily include Bangla connectives from Chalitbhasha (as they are used in the standard Bangla language). In addition, we also include the earlier forms of those connectives from Sadhubhasha (if available), and record them as their stylistic variants. For example, we include /yAte/ from Chalitbhasha and /yAhAte/ from Sadhubhasha, both meaning ‘so that’, and record the latter as a stylistic variant of the former. • According to the Penn Discourse Treebank (PDTB) 3.0 sense hierarchy (Webber et al., 2016) • According to the RST (Rhetorical Structure Theory) taxonomy used in the Potsdam Commentary Corpus (PCC) (Stede, 2016) • Examples of coherence relations, mostly from the Bangla RST-DT (Das and Stede, 2018) We provide relational information for each connective based on two discourse frameworks: PDTB 3.0 and RST (as in PCC). In the PDTB 3.0, discourse relations (known as senses) are organized in a hierarchical taxonomy, organized into three levels: sense classes, sense types and sense subtypes (as shown in Table 2). On the other hand, the PCC relational taxonomy (also used in annotating the Ba"
2020.lrec-1.138,W18-5042,1,0.57868,"can provide the list of connectives in a language, along with useful information about their syntactic and semantic-pragmatic properties. Such lexicons are developed and are becoming increasingly available in different languages, beginning with DiMLex in German (Stede and Umbach, 1998), later DPDE for Spanish (Briz et al., 2008) and LexConn for French (Roze et al., 2012), and more recently LICO for Italian (Feltracco et al., 2016), CzeDLex for Czech (M´ırovsk´y et al., 2017), LDM-PT for Portuguese (Mendes et al., 2018), DisCoDict for Dutch (Bourgonje et al., 2018) and DiMLex-Eng for English (Das et al., 2018). We extend this line of work by developing a new lexicon of discourse connectives in Bangla, called DiMLex-Bangla. The lexicon, upon completion of its first version, contains 123 Bangla discourse connectives, which are primarily accumulated from the linguistic literature on Bangla and translation of English discourse connectives into Bangla. The lexicon compilation is further augmented by including more connectives from a currently developed corpus, called the Bangla RST Discourse Treebank (Das and Stede, 2018). Each connective entry in DiMLex-Bangla is accompanied by information on the possi"
2020.lrec-1.138,L18-1693,1,0.849308,"so been taken to construct ancillary resources such as lexicons of discourse connectives, which can provide the list of connectives in a language, along with useful information about their syntactic and semantic-pragmatic properties. Such lexicons are developed and are becoming increasingly available in different languages, beginning with DiMLex in German (Stede and Umbach, 1998), later DPDE for Spanish (Briz et al., 2008) and LexConn for French (Roze et al., 2012), and more recently LICO for Italian (Feltracco et al., 2016), CzeDLex for Czech (M´ırovsk´y et al., 2017), LDM-PT for Portuguese (Mendes et al., 2018), DisCoDict for Dutch (Bourgonje et al., 2018) and DiMLex-Eng for English (Das et al., 2018). We extend this line of work by developing a new lexicon of discourse connectives in Bangla, called DiMLex-Bangla. The lexicon, upon completion of its first version, contains 123 Bangla discourse connectives, which are primarily accumulated from the linguistic literature on Bangla and translation of English discourse connectives into Bangla. The lexicon compilation is further augmented by including more connectives from a currently developed corpus, called the Bangla RST Discourse Treebank (Das and Ste"
2020.lrec-1.138,prasad-etal-2008-penn,0,0.146931,"re not subject to inflection. We make a list of such 45 connectives from the available literature on the standard Bangla grammars (David, 2015; Thompson, 2010; Chaki, 1996; Chatterji, 1988)3 . 3.2. Translation of English Connectives In order to collect more connectives, we adopt a translational strategy, generating a list of translational equivalents in Bangla for English connectives. For this purpose, we use as our source a lexicon of English connectives, called DiMLex-Eng (Das et al., 2018), which comprises 149 English connectives compiled from resources such as the Penn Discourse Treebank (Prasad et al., 2008) and RST Signalling Corpus (Das et al., 2015). To translate these connectives into Bangla, we consult the available standard bilingual dictionaries (Biswas, 1957; Ghosh, 1999), mainly for items that comprise only a single word (furthermore, since, etc.). For translating the multiword connectives of English (as a result of, quite the contrary, etc.), we use a wellrecognized online translation tool Google Translate. Using the translational method, we include 58 additional connectives in DiMlex-Bangla. 3.3. The Bangla RST Discourse Treebank We also use a corpus of Bangla texts, called the Bangla"
2020.lrec-1.138,L16-1160,1,0.738375,"onnectives into Bangla. The lexicon compilation is further augmented by including more connectives from a currently developed corpus, called the Bangla RST Discourse Treebank (Das and Stede, 2018). Each connective entry in DiMLex-Bangla is accompanied by information on the possible orthographic/stylistic variants of the connective, its syntactic category, non-connective usage (if any), and the set of discourse relations indicated by the connective (with examples). For organizing the lexicon, we follow the German connective lexicon, DiMLex1 (Stede and Umbach, 1998), whose most updated version (Scheffler and Stede, 2016) includes an exhaustive list of 275 German discourse connectives. For each connective entry, DiMLex provides a number of features, characterizing its syntactic, semantic and pragmatic behaviour. Bangla (also known by its English exonym Bengali) is an Indo-Aryan language which is natively spoken in the Indian subcontinent (India and Bangladesh). Bangla has long been a relatively well-studied language (although more from the viewpoint of traditional grammars than from the modern linguistic perspectives). More recently, Bangla has also become subject to NLP-based research, mainly pertaining to ar"
2020.lrec-1.139,D18-1217,0,0.0115515,"he PDTB2. Malmi et al. (2018) also improve data availability but focus on connective prediction and further limit themselves to special cases with two consecutive sentences where the connective is at the beginning of the second sentence. Our approach instead applies to the full range of spans of explicit discourse relations, but it is limited by the model’s window size used to predict a relation. 2.2. Semi-Supervised Learning Semi-supervised learning is a research area that tries to jointly learn from labeled and unlabeled data. Many different ways have been proposed to tackle this challenge. Clark et al. (2018) combine their main task with additional auxiliary tasks such as predicting masked words from context to gain performance. Also, semi-supervised learning is used on unlabeled data to identify co-occurring features (Hernault et al., 2010a; Hernault et al., 2010b). These authors define auxiliary training tasks to improve model performance, but the unlabeled data is restricted to individual text spans that contain single relations. docs 1,756 79 91 71 2,223 736 length 22.28 21.35 25.67 18.95 18.60 17.29 tokens 933,049 39,712 55,453 34,621 958,212 284,528 relations 14,722 680 923 556 *19,576 *8,18"
2020.lrec-1.139,I11-1120,0,0.0212205,"mpetitions (e.g., (Wang et al., 2015; Wang and Lan, 2016; Oepen et al., 2016)), and they largely follow the pipeline model of Lin et al. (2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Argument labeling with recurrent neural networks was done first by Wang et al. (2015) in their DCU parser. In addition to word embeddings, they also used other features, such as POS tags, syntactic relations, and lexical features. They distinguish between intra-sentential and intersentential relations (Ghosh et al., 2011), and thus train separate models for each individual labeling task. In contrast, the approach used for this paper does not rely on sentence boundaries and uses word embeddings only. Recently, fixed sized windows were introduced with neural networks for argument labeling as a 4-class sequence classification task. First, Hooda and Kosseim (2017) studied explicit argument extraction on predefined windows with a fixed length corresponding to the maximum span length of arguments. Then, in our previous work, we adapted their approach and described a more general procedure that integrates connective"
2020.lrec-1.139,D10-1039,0,0.085664,"Missing"
2020.lrec-1.139,W10-4309,0,0.0339147,"ce. Our approach instead applies to the full range of spans of explicit discourse relations, but it is limited by the model’s window size used to predict a relation. 2.2. Semi-Supervised Learning Semi-supervised learning is a research area that tries to jointly learn from labeled and unlabeled data. Many different ways have been proposed to tackle this challenge. Clark et al. (2018) combine their main task with additional auxiliary tasks such as predicting masked words from context to gain performance. Also, semi-supervised learning is used on unlabeled data to identify co-occurring features (Hernault et al., 2010a; Hernault et al., 2010b). These authors define auxiliary training tasks to improve model performance, but the unlabeled data is restricted to individual text spans that contain single relations. docs 1,756 79 91 71 2,223 736 length 22.28 21.35 25.67 18.95 18.60 17.29 tokens 933,049 39,712 55,453 34,621 958,212 284,528 relations 14,722 680 923 556 *19,576 *8,186 Table 1: General statistics to contrast labeled and unlabeled data. Both sets have comparable sizes. (*) The number of relations for unlabeled data is approximated by a connective classifier. There are various approaches for bootstrap"
2020.lrec-1.139,hooda-kosseim-2017-argument,0,0.0182522,"orks was done first by Wang et al. (2015) in their DCU parser. In addition to word embeddings, they also used other features, such as POS tags, syntactic relations, and lexical features. They distinguish between intra-sentential and intersentential relations (Ghosh et al., 2011), and thus train separate models for each individual labeling task. In contrast, the approach used for this paper does not rely on sentence boundaries and uses word embeddings only. Recently, fixed sized windows were introduced with neural networks for argument labeling as a 4-class sequence classification task. First, Hooda and Kosseim (2017) studied explicit argument extraction on predefined windows with a fixed length corresponding to the maximum span length of arguments. Then, in our previous work, we adapted their approach and described a more general procedure that integrates connective classification into the process of argument extraction by using moving windows over a discourse (Knaebel et al., 2019). This approach was limited by the relatively small amount of data available in the PDTB2. Malmi et al. (2018) also improve data availability but focus on connective prediction and further limit themselves to special cases with"
2020.lrec-1.139,P18-1249,0,0.0189556,"both parts of the BBC data separately. 3.1. Preprocessing We preprocess the raw BBC dataset1 such that the format is comparable to the provided PDTB (CoNLL-format) 1 1104 http://mlg.ucd.ie/datasets/bbc.html dataset. First of all, for each document in a corpus, we extract the main document text. After normalization, we use the Penn Treebank Tokenizer, implemented in NLTK2 , for similarity. We further process each tokenized document using Spacy3 and use their part-of-speech tags and dependency trees. For generating constituency trees, we use an additional module provided by the Benepar project(Kitaev and Klein, 2018). Except for the normalized tokens, predicted information is only used for traditional models and left out for the neural models. We publish all our scripts and findings for future research.4 4. Method The main goal of our work is to consider unlabeled data for training and, additionally, to produce new automatically annotated data that can be used in other settings as well. Our focus is on explicit discourse relations and in particular identifying argument spans of those relations, also called explicit argument labeling. We split this general task into two phases, where the problem of generat"
2020.lrec-1.139,K19-1072,1,0.81666,"d for this paper does not rely on sentence boundaries and uses word embeddings only. Recently, fixed sized windows were introduced with neural networks for argument labeling as a 4-class sequence classification task. First, Hooda and Kosseim (2017) studied explicit argument extraction on predefined windows with a fixed length corresponding to the maximum span length of arguments. Then, in our previous work, we adapted their approach and described a more general procedure that integrates connective classification into the process of argument extraction by using moving windows over a discourse (Knaebel et al., 2019). This approach was limited by the relatively small amount of data available in the PDTB2. Malmi et al. (2018) also improve data availability but focus on connective prediction and further limit themselves to special cases with two consecutive sentences where the connective is at the beginning of the second sentence. Our approach instead applies to the full range of spans of explicit discourse relations, but it is limited by the model’s window size used to predict a relation. 2.2. Semi-Supervised Learning Semi-supervised learning is a research area that tries to jointly learn from labeled and"
2020.lrec-1.139,L18-1260,0,0.0149434,"ws were introduced with neural networks for argument labeling as a 4-class sequence classification task. First, Hooda and Kosseim (2017) studied explicit argument extraction on predefined windows with a fixed length corresponding to the maximum span length of arguments. Then, in our previous work, we adapted their approach and described a more general procedure that integrates connective classification into the process of argument extraction by using moving windows over a discourse (Knaebel et al., 2019). This approach was limited by the relatively small amount of data available in the PDTB2. Malmi et al. (2018) also improve data availability but focus on connective prediction and further limit themselves to special cases with two consecutive sentences where the connective is at the beginning of the second sentence. Our approach instead applies to the full range of spans of explicit discourse relations, but it is limited by the model’s window size used to predict a relation. 2.2. Semi-Supervised Learning Semi-supervised learning is a research area that tries to jointly learn from labeled and unlabeled data. Many different ways have been proposed to tackle this challenge. Clark et al. (2018) combine t"
2020.lrec-1.139,N06-1020,0,0.0114989,"raining tasks to improve model performance, but the unlabeled data is restricted to individual text spans that contain single relations. docs 1,756 79 91 71 2,223 736 length 22.28 21.35 25.67 18.95 18.60 17.29 tokens 933,049 39,712 55,453 34,621 958,212 284,528 relations 14,722 680 923 556 *19,576 *8,186 Table 1: General statistics to contrast labeled and unlabeled data. Both sets have comparable sizes. (*) The number of relations for unlabeled data is approximated by a connective classifier. There are various approaches for bootstrapping and selftraining with neural models. In Self-Training (McClosky et al., 2006; Yarowsky, 1995), predictions of the same model are used with respect to the model’s confidence in a particular prediction. Despite its simple mechanism, this algorithm comes with a high bias, which is unfavorable for learning new directions within the data. In contrast, MultiView-Training (Zhou and Goldman, 2004; Søgaard, 2010) tries to compensate this bias by different views of the data. These different views may be approached by separate feature sets, data splits, and models. In a recent study, Ruder and Plank (2018) show that Tri-Training (Zhi-Hua Zhou and Ming Li, 2005), a form of Multi-"
2020.lrec-1.139,K16-2002,1,0.841824,"in the area of discourse parsing and on the model used in this paper. Next, an overview of semi-supervised learning is provided as well as other work that deals with the problem of data sparsity. 2.1. Shallow Discourse Parsing Shallow discourse parsing is a challenging task, which was promoted by the development of the second version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008) and further adapted by the shared tasks at CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016). Several systems have been proposed at the competitions (e.g., (Wang et al., 2015; Wang and Lan, 2016; Oepen et al., 2016)), and they largely follow the pipeline model of Lin et al. (2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Argument labeling with recurrent neural networks was done first by Wang et al. (2015) in their DCU parser. In addition to word embeddings, they also used other features, such as POS tags, syntactic relations, and lexical features. They distinguish between intra-sentential and intersentential relations (Ghosh et al., 2011), and thus train separate models for each individual label"
2020.lrec-1.139,P09-2004,0,0.0551987,"l Argument Extraction For argument extraction, we adapt our previous work on window-based neural models for discourse relation extraction (Knaebel et al., 2019). There, we describe tasks of different complexity that the neural model is being trained on. For our experiments, we use two of these settings; one model is trained to extract argument spans around previously extracted connectives, and another one is trained to jointly extract connectives and their arguments. The Neural Connective Argument Extractor (NCA) is a twocomponent pipeline that consists of a traditional connective classifier (Pitler and Nenkova, 2009) and a neural model. The prediction of the first component is used to identify 2 www.nltk.org www.spacy.io 4 https://github.com/rknaebel/bbc-discourse 3 the connective. A fixed-sized window is placed over this area such that the connective is centered within the model’s window. Then, each token is classified as being part of one of the argument. The Neural Explicit Argument Extractor (NEA) solves a more complex task where the connective is not given by an external model. Instead, the model is trained to determine the presence or absence of relations. A sliding window approach is used, where ea"
2020.lrec-1.139,prasad-etal-2008-penn,0,0.413683,"ing senses to them, and finding the senses of so-called implicit relations holding between adjacent text spans without a lexical signal being present. In this work, however, we focus on explicit discourse relations only, and further, we leave the sense selection aside, as relation span labeling can be handled independently of a relation’s sense. Because of its complexity, it is hard to get annotated data for training statistical models to perform SDP. Therefore, our goal is to produce high-quality annotations other than the standard corpus used in the field—the Penn Discourse Treebank (PDTB) (Prasad et al., 2008)—in order to improve performance on explicit argument extraction. Learning from unlabeled data as in unsupervised learning still remains a challenging problem. The branch which combines supervised and unsupervised methods is referred to as semi-supervised learning. In particular, in this paper, we focus on learning from unlabeled data by producing proxy labels. This is a hard problem in shallow discourse argument extraction because the proposed models do not work very well. Therefore, we adopt two variations of the same problem. The first is slightly easier and better suited for labeling new d"
2020.lrec-1.139,P18-1096,0,0.0149879,"s for bootstrapping and selftraining with neural models. In Self-Training (McClosky et al., 2006; Yarowsky, 1995), predictions of the same model are used with respect to the model’s confidence in a particular prediction. Despite its simple mechanism, this algorithm comes with a high bias, which is unfavorable for learning new directions within the data. In contrast, MultiView-Training (Zhou and Goldman, 2004; Søgaard, 2010) tries to compensate this bias by different views of the data. These different views may be approached by separate feature sets, data splits, and models. In a recent study, Ruder and Plank (2018) show that Tri-Training (Zhi-Hua Zhou and Ming Li, 2005), a form of Multi-View-Training with three independently trained models, should be considered as a strong baseline for neural semi-supervised learning. Recently, Chen et al. (2018) use a network architecture called Tri-net that works similar to ours. Their experiments deal with the task of image classification, though. 3. Data Collection For additional data, we consider the work of Greene and Cunningham (2006) who present the BBC Datasets. This data collection consists of two parts, bbc-news and bbcsport, both from the years 2004 to 2005."
2020.lrec-1.139,P10-2038,0,0.0304389,"ontrast labeled and unlabeled data. Both sets have comparable sizes. (*) The number of relations for unlabeled data is approximated by a connective classifier. There are various approaches for bootstrapping and selftraining with neural models. In Self-Training (McClosky et al., 2006; Yarowsky, 1995), predictions of the same model are used with respect to the model’s confidence in a particular prediction. Despite its simple mechanism, this algorithm comes with a high bias, which is unfavorable for learning new directions within the data. In contrast, MultiView-Training (Zhou and Goldman, 2004; Søgaard, 2010) tries to compensate this bias by different views of the data. These different views may be approached by separate feature sets, data splits, and models. In a recent study, Ruder and Plank (2018) show that Tri-Training (Zhi-Hua Zhou and Ming Li, 2005), a form of Multi-View-Training with three independently trained models, should be considered as a strong baseline for neural semi-supervised learning. Recently, Chen et al. (2018) use a network architecture called Tri-net that works similar to ours. Their experiments deal with the task of image classification, though. 3. Data Collection For addit"
2020.lrec-1.139,K16-2004,0,0.0166084,"some relevant work in the area of discourse parsing and on the model used in this paper. Next, an overview of semi-supervised learning is provided as well as other work that deals with the problem of data sparsity. 2.1. Shallow Discourse Parsing Shallow discourse parsing is a challenging task, which was promoted by the development of the second version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008) and further adapted by the shared tasks at CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016). Several systems have been proposed at the competitions (e.g., (Wang et al., 2015; Wang and Lan, 2016; Oepen et al., 2016)), and they largely follow the pipeline model of Lin et al. (2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Argument labeling with recurrent neural networks was done first by Wang et al. (2015) in their DCU parser. In addition to word embeddings, they also used other features, such as POS tags, syntactic relations, and lexical features. They distinguish between intra-sentential and intersentential relations (Ghosh et al., 2011), and thus train separate models for"
2020.lrec-1.139,K15-2014,0,0.0190531,"n, we first discuss some relevant work in the area of discourse parsing and on the model used in this paper. Next, an overview of semi-supervised learning is provided as well as other work that deals with the problem of data sparsity. 2.1. Shallow Discourse Parsing Shallow discourse parsing is a challenging task, which was promoted by the development of the second version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008) and further adapted by the shared tasks at CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016). Several systems have been proposed at the competitions (e.g., (Wang et al., 2015; Wang and Lan, 2016; Oepen et al., 2016)), and they largely follow the pipeline model of Lin et al. (2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Argument labeling with recurrent neural networks was done first by Wang et al. (2015) in their DCU parser. In addition to word embeddings, they also used other features, such as POS tags, syntactic relations, and lexical features. They distinguish between intra-sentential and intersentential relations (Ghosh et al., 2011), and thus train"
2020.lrec-1.139,K15-2001,0,0.0257509,"ns in Section 6.. 1103 2. Related Work Corpus train dev test blind bbc-news bbc-sport In this section, we first discuss some relevant work in the area of discourse parsing and on the model used in this paper. Next, an overview of semi-supervised learning is provided as well as other work that deals with the problem of data sparsity. 2.1. Shallow Discourse Parsing Shallow discourse parsing is a challenging task, which was promoted by the development of the second version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008) and further adapted by the shared tasks at CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016). Several systems have been proposed at the competitions (e.g., (Wang et al., 2015; Wang and Lan, 2016; Oepen et al., 2016)), and they largely follow the pipeline model of Lin et al. (2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Argument labeling with recurrent neural networks was done first by Wang et al. (2015) in their DCU parser. In addition to word embeddings, they also used other features, such as POS tags, syntactic relations, and lexical features. They dis"
2020.lrec-1.139,K16-2001,0,0.0194784,"1103 2. Related Work Corpus train dev test blind bbc-news bbc-sport In this section, we first discuss some relevant work in the area of discourse parsing and on the model used in this paper. Next, an overview of semi-supervised learning is provided as well as other work that deals with the problem of data sparsity. 2.1. Shallow Discourse Parsing Shallow discourse parsing is a challenging task, which was promoted by the development of the second version of the Penn Discourse Treebank (PDTB2) (Prasad et al., 2008) and further adapted by the shared tasks at CoNLL 2015 and 2016 (Xue et al., 2015; Xue et al., 2016). Several systems have been proposed at the competitions (e.g., (Wang et al., 2015; Wang and Lan, 2016; Oepen et al., 2016)), and they largely follow the pipeline model of Lin et al. (2014), which consists of successive tasks of connective identification, argument labeling, and sense classification for both explicit and implicit relations. Argument labeling with recurrent neural networks was done first by Wang et al. (2015) in their DCU parser. In addition to word embeddings, they also used other features, such as POS tags, syntactic relations, and lexical features. They distinguish between in"
2020.lrec-1.139,P95-1026,0,0.787977,"e model performance, but the unlabeled data is restricted to individual text spans that contain single relations. docs 1,756 79 91 71 2,223 736 length 22.28 21.35 25.67 18.95 18.60 17.29 tokens 933,049 39,712 55,453 34,621 958,212 284,528 relations 14,722 680 923 556 *19,576 *8,186 Table 1: General statistics to contrast labeled and unlabeled data. Both sets have comparable sizes. (*) The number of relations for unlabeled data is approximated by a connective classifier. There are various approaches for bootstrapping and selftraining with neural models. In Self-Training (McClosky et al., 2006; Yarowsky, 1995), predictions of the same model are used with respect to the model’s confidence in a particular prediction. Despite its simple mechanism, this algorithm comes with a high bias, which is unfavorable for learning new directions within the data. In contrast, MultiView-Training (Zhou and Goldman, 2004; Søgaard, 2010) tries to compensate this bias by different views of the data. These different views may be approached by separate feature sets, data splits, and models. In a recent study, Ruder and Plank (2018) show that Tri-Training (Zhi-Hua Zhou and Ming Li, 2005), a form of Multi-View-Training wit"
2021.germeval-1.2,2020.argmining-1.6,1,0.729945,"eddings overfit to the training data, we decided against fine-tuning in submissions II and III. For submission III we employed the same pretrained BERT embeddings. Instead of training a transformer model, however, we trained the same set of ML models on the encoded comments that we used for baselines 2 and 3. Our experiments revealed that XGBoost models perform best for this feature type, which is why, in the following, we will exclusively focus on this classifier. This approach is comparable to other previous work of ours, which focused on argument detection in tweets (Iskender et al., 2021; Schaefer and Stede, 2020). We hypothesize the following ranking of submitted approaches for both subtasks: Table 1: Definitions of Linguistic Features that they cannot compete against the more sophisticated DL approaches we decided on using them for mere comparison. For baseline 2 (unigram) we derive unigrams from the data. We experimented with different variations of n-grams but simple unigrams perform best. During preprocessing we set all tokens to lower case and removed stopwords. Final vocabulary size is 19085. Baseline 3 (linguistic features) is based on a set of linguistic and textrelated features which was comp"
2021.germeval-1.2,N19-4010,0,0.0642438,"Missing"
2021.germeval-1.2,W17-1101,0,0.0298058,"political issues. While the latter have the potential to contribute to public political discourse in general, social media has been found to contain not only respectful and engaging discussions but also hateful speech, which threatens the respectful exchange and possibly also the mental well-being of its participants. The GermEval 2021 Shared Task (Risch et al., 2021) aims to stimulate research on 2 Related Work Given that we do not participate in Subtask 1 (toxic comments) we will not go further into details here. For surveys on tackling this issue using NLP techniques we refer the reader to Schmidt and Wiegand (2017) and Mishra et al. (2019). Subtask 2 (engaging comments) may be seen as a complement task to toxic comment classification as it focuses more on the identification of respectful 1 https://www.facebook.com/ https://www.statista.com/statistics/ 278414/number-of-worldwide-social-networ k-users/ 2 3 Code Repository: https://github.com/Robin Schaefer/GermEval2021 13 Proceedings of the GermEval 2021 Shared Task on the Identification of Toxic, Engaging, and Fact-Claiming Comments co-located with KONVENS Toxic Comments: Toxic comments include different types of uncivil behavior like insults, sarcastic"
2021.germeval-1.2,D17-1245,0,0.1025,"tsdam, Germany stede@uni-potsdam.de robin.schaefer@uni-potsdam.de Abstract this issue, while also going beyond the single task of toxic comment classification. In this paper we present UPAppliedCL’s contribution3 to the GermEval 2021 Shared Task which consists of three subtasks revolving around the mentioned characteristics of social media discussions: 1. Toxic Comment Classification; 2. Engaging Comment Classification; 3. Fact-Claiming Comment Classification. Here we especially focus on Subtask 3 (fact-claiming comments), which is also relevant for tasks in the field of argument mining (AM) (Dusmanu et al., 2017; Schaefer and Stede, 2021). In addition, we also participate in Subtask 2 (engaging comments), which we consider as a first albeit facultative step in an AM system in order to identify potential argumentative comments. As we consider Subtask 1 (toxic comments) as a task which is more independent of AM, we will not attend to it in this work. The paper is structured as follows: in Section 2 we give a short overview of relevant previous work. We present the dataset provided by the organizers in Section 3. In Section 4 we describe our approach including the developed baselines, and in Section 5 w"
2021.konvens-1.23,C10-2032,0,0.0512882,"tau-b, which is on a scale from -1 to +1 (Kendall, 1945). 3 Related work Conceptual complexity. An earlier study, also ˇ by Stajner and Hulpus, (2018), on the automatic assessment of conceptual complexity uses knowledge graph based features, such as the number of neighbours a node has and the length of the shortest path connecting two nodes. They build on this work by introducing shallow and surface features based on the output of an entity linker, such as the number of unique entities in a sentence or the average distance between consecutive mentions of entities (Stajner and Hulpus, , 2020). Feng et al. (2010) evaluate the features which best predict readability, using magazine articles designed for primary school children of different ages in a classification task. They use “discourse features” such as the density of named entities and proper nouns across a sentence or text, or the length of chains of semantic relations (such as synonym or hypernym) from an entity, based on the hypothesis that the density of named entities and proper nouns introduced in a text relates to the burden placed on the readers’ working memory and therefore the complexity level of a text. For texts in German, Weiß and Meu"
2021.konvens-1.23,P19-1377,0,0.0359573,"Missing"
2021.konvens-1.23,C18-1027,0,0.0290233,"Missing"
2021.konvens-1.23,2020.lrec-1.177,0,0.0634577,"Missing"
2021.konvens-1.23,2020.lrec-1.887,0,0.466958,"called type of decay – and the φ function, which is the function applied to the values which result from the SA process. The distance decay parameter α and the firing threshold β, two parameters which control the amount of nodes activated in each SA step, were not experimented with and the best performing values from the original study were used, 0.25 and 0.01 respectively. The system was then applied to all 885 texts in the lexica corpus. The results can be seen in Table 3: the average accuracy for pairwise classification using the best parameters from the original study (as documented in ˇ (Stajner et al., 2020)) was .86, which is the same as the original system for English texts. The best parameters for the German texts – as can be seen in the right-hand side of Table 3 – increased the average accuracy for the pairwise classification to .89. In both cases the AEoS score provided the best results. 5 https://miniklexikon.zum.de/index. php?title=Hilfe:Regeln&oldid=23440 Level 2 The name Allosaurus is derived from the Greek language and translates to ‘different lizard’. Amsterdam is the capital city and the most populous city in the Kingdom of the Netherlands. Furthermore, astronomy strives to understan"
2021.konvens-1.23,C18-1026,0,0.0168786,"t al. (2010) evaluate the features which best predict readability, using magazine articles designed for primary school children of different ages in a classification task. They use “discourse features” such as the density of named entities and proper nouns across a sentence or text, or the length of chains of semantic relations (such as synonym or hypernym) from an entity, based on the hypothesis that the density of named entities and proper nouns introduced in a text relates to the burden placed on the readers’ working memory and therefore the complexity level of a text. For texts in German, Weiß and Meurers (2018) evaluate a large feature set of complexity indicators on a dataset of news subtitles and scientific articles and their counterparts aimed at children. Some of the most informative features were frequency measures calculated using different lexicons and corpora as well as content overlap within sentences. vor der Br¨uck et al. (2008) develop a readability checker for German texts called DeLite and build so-called semantic networks for sentences, in which the word-class functions of the words and the relations between them are represented as a graph. Using 500 German texts from the municipal do"
2021.konvens-1.23,N16-1078,0,0.0610094,"Missing"
2021.nlp4posimpact-1.2,S16-1003,0,0.0883803,"Missing"
2021.nlp4posimpact-1.2,W17-4205,0,0.0490716,"Missing"
2021.nlp4posimpact-1.2,W16-2603,0,0.0394515,"Missing"
2021.nlp4posimpact-1.2,2020.findings-emnlp.296,0,0.504509,"tsdam/Germany stede@uni-potsdam.de Abstract for humanity and the biosphere around the planet (Cook et al., 2016), public debates on CC and on the policy implications remain highly controversial (see, e.g., (Hulme, 2009)). Natural Language Processing (NLP) is wellpositioned to help study the dynamics of the largescale and complex discourse on CC. Activists and policy-makers need NLP tools through which they can filter, order, and make sense of the vast amount of textual data produced on CC. However, within the NLP community, the amount of work done so far on CC remains limited. In the words of Luo et al. (2020, p. 3296), the topic of climate change ”has received little attention in NLP despite its real world urgency”. This is in contrast to the attention that CC discourses receive in climate and environmental science and in various social sciences. We argue in this paper that the research questions, insights and methods applied in these disciplines can provide useful orientation for NLP practitioners. And conversely, the general advances in NLP can provide more reliable and valid tools to actors aiming at shaping policy and influencing individual behavior. Such tools for monitoring the discourses a"
budzynska-etal-2014-model,W02-0218,0,\N,Missing
budzynska-etal-2014-model,J97-1002,0,\N,Missing
budzynska-etal-2014-model,J00-3003,0,\N,Missing
budzynska-etal-2014-model,reed-etal-2008-language,1,\N,Missing
C02-2027,W96-0411,0,\N,Missing
C02-2027,W98-1404,0,\N,Missing
C02-2027,H89-1022,0,\N,Missing
C04-1061,W02-1704,1,0.891586,"Missing"
C04-1061,P97-1013,0,0.0312971,"he simpler marking of lexical connectives and their relations, and the more difficult decisions on overall tree structure. To this end, we developed an environment of two analysis tools and XML-based declarative resources. Our ConAno tool allows for efficient, interactive annotation of connectives, scopes and relations. This intermediate result is exported to O’Donnell’s ‘RST Tool’, which facilitates completing the tree structure. 1 Introduction A number of approaches tackling the difficult problem of automatic discourse parsing have been proposed in recent years (e.g., (Sumita et al., 1992) (Marcu, 1997), (Schilder, 2002)). They differ in their orientation toward symbolic or statistical information, but they all — quite naturally — share the assumption that the lexical connectives or discourse markers are the primary source of information for constructing a rhetorical tree automatically. The density of discourse markers in a text depends on its genre (e.g., commentaries tend to have more than narratives), but in general, it is clear that only a portion of the relations holding in a text is lexically signalled.1 Furthermore, it is wellknown that discourse markers are often ambiguous; for examp"
C04-1061,W99-0307,0,0.0774814,"Missing"
C04-1061,W04-2703,0,0.0146062,"tors, and clearly specified guidelines on what relation to choose under which circumstances. Nonetheless, rhetorical analysis remains to be in part a rather subjective process (see section 2). In order to eventually arrive at more objective, comparable results, our proposal is to split the annotation process into two parts: 1. Annotation of connectives, their scopes (the two related textual units), and — optionally — the signalled relation 2. Annotation of the remaining (unsignalled) relations between larger segments Step 1 is inspired by work done for English in the Penn Discourse TreeBank2 (Miltsakaki et al., 2004). In our two-step scenario, it is the easier part of the whole task in that connectives can be quite clearly identified, their scopes are often (but not always, see below) transparent, and the coherence relation is often clear. We see the result of step 1 as a corpus resource in its own right (it can be used for training statistical classifiers, for instance) and at the same time as the input for step 2, which “fills the gaps”: now annotators have to decide how the set of small trees produced in step 1 is best arranged in one complete tree, which involves assigning 2 http://www.cis.upenn.edu/∼"
C04-1061,W04-0213,1,0.926822,"that takes the experiences of human annotators into account. “Ideally”, discourse analysis proceeds incrementally from left to right, where for each new segment, an attachment point and a relation (or more than one of each, cf. SDRT) are computed and the discourse structure grows step by step. This view is taken for instance in SDRT (Asher, Lascarides, 2003), which places emphasis on the notion of ‘right frontier’ (also discussed recently by (Webber et al., 2003)). However, when we trained two (experienced) students to annotate the 171 newspaper commentaries of the Potsdam Commentary Corpus (Stede, 2004) and upon completion of the task asked them about their experiences, a very different picture emerged. Both annotators agreed that a strict left-to-right approach is highly impractical, because the intended argumentative structure of the text often becomes clear only in retrospect, after reflecting the possible contributions of the segments to the larger scheme. 3 This assessment of relative difficulty does not carry over to PDTB, where the annotations are more complex than in our step 1 but do not go as far as building rhetorical structures. Thus they very soon settled on a bottom-up approach"
C04-1061,J03-4002,0,\N,Missing
C16-1312,W15-0501,0,0.0195211,"ustomers provide for their judgements. But the general task can be applied in many other domains as well. For example, the early research of Mochales Palau and Moens (2009) sought to identify premises and conclusions in legal text; another early work on finding theses in student essays (Burstein et al., 2003) has recently been extended to detecting more argumentative structure in such essays (Stab and Gurevych, 2014), (Nguyen and Litman, 2015); other genres that are being tackled include online dialogue (Oraby et al., 2015), multi-party dialogue (Budzynska et al., 2013) and scientific papers (Kirschner et al., 2015). Despite the manifold recent activities, the field is still young, and a number of basic issues still require attention. This concerns the design of annotation schemes and possibly finding a consensus on them, but also the more fundamental problem of defining the notion of, and identifying instances of, argumentative text. While it is tempting to assume that earlier work on text-level subjectivity classification can be used to determine whether some (portion of) text is argumentative, we argue below that this is generally not the case. Going further, we posit that ‘argumentativeness’ is a mat"
C16-1312,W15-0503,0,0.0202435,"ments in customer reviews (e.g., (Villalba and Saint-Dizier, 2012)), where the goal is to move beyond finding positive and negative statements by also finding the reasons that customers provide for their judgements. But the general task can be applied in many other domains as well. For example, the early research of Mochales Palau and Moens (2009) sought to identify premises and conclusions in legal text; another early work on finding theses in student essays (Burstein et al., 2003) has recently been extended to detecting more argumentative structure in such essays (Stab and Gurevych, 2014), (Nguyen and Litman, 2015); other genres that are being tackled include online dialogue (Oraby et al., 2015), multi-party dialogue (Budzynska et al., 2013) and scientific papers (Kirschner et al., 2015). Despite the manifold recent activities, the field is still young, and a number of basic issues still require attention. This concerns the design of annotation schemes and possibly finding a consensus on them, but also the more fundamental problem of defining the notion of, and identifying instances of, argumentative text. While it is tempting to assume that earlier work on text-level subjectivity classification can be"
C16-1312,W15-0515,0,0.0314415,"to move beyond finding positive and negative statements by also finding the reasons that customers provide for their judgements. But the general task can be applied in many other domains as well. For example, the early research of Mochales Palau and Moens (2009) sought to identify premises and conclusions in legal text; another early work on finding theses in student essays (Burstein et al., 2003) has recently been extended to detecting more argumentative structure in such essays (Stab and Gurevych, 2014), (Nguyen and Litman, 2015); other genres that are being tackled include online dialogue (Oraby et al., 2015), multi-party dialogue (Budzynska et al., 2013) and scientific papers (Kirschner et al., 2015). Despite the manifold recent activities, the field is still young, and a number of basic issues still require attention. This concerns the design of annotation schemes and possibly finding a consensus on them, but also the more fundamental problem of defining the notion of, and identifying instances of, argumentative text. While it is tempting to assume that earlier work on text-level subjectivity classification can be used to determine whether some (portion of) text is argumentative, we argue below"
C16-1312,W16-2812,1,0.834522,"mental study Features Simple features. Depth of argumentation is unlikely to be detectable with straightforward surface (or ‘near-surface’) features. Nonetheless, we tested two simple features that have been used in subjectivity classification (see above) and that might be relevant here: The presence of modal verbs and of ‘argumentative’ connectives: those that signal a contrastive or causal (in the wide sense) relation. Argumentation structure: depth measure. The Peldszus/Stede annotation scheme had so far been applied by its authors to pro/contra commentaries and to very short ‘microtexts’ (Peldszus and Stede, 2016a). Our present annotation is thus its first application to texts which are not as easy to handle: Portions of text might be irrelevant for the argument, and both the central claim and the attachment points of relations can be hard to identify. We will test if our depth measure reflects differences in ‘argumentativeness’ as perceived by readers. Rhetorical structure. In the literature there is an ongoing and so far not quite conclusive discussion on the relationship between RST analysis and argumentation analysis (e.g., (Azar, 1999), (Peldszus and Stede, 2013), (Green, 2015), (Peldszus and Ste"
C16-1312,J11-2004,0,0.0200737,"t from the ‘objective’ statement, but can do so in quite different ways, such as making speculations on future events, reporting on one’s feelings, attributing content to third parties or to hearsay, or evaluating some entity in terms of valence (positive/negative). The latter is the notion that is mostly used in Computational Linguistics, and it is the central target of most work on subjectivity classification. This task can be applied on sentence or text level, and it has been done with bag-of-words models, PoS tags, or linguistic features that tend to be more roubst against domain changes (Petrenz and Webber, 2011), (Kr¨uger et al., to appear). Importantly, subjectivity is orthogonal to the text types mentioned above: Texts of all types may be predominantly subjective, and most may not (a likely exception being the argumentative type). What has – to the best of our knowledge – not been addressed in depth so far is the distinction between the tasks of classifying subjectivity (versus objectivity) on the one hand and argumentativeness (versus non-argumentativeness) on the other. To see why this is necessary, consider the following examples from the Brexit reader-discussion website of the Irish Times:1 (1)"
C16-1312,prasad-etal-2008-penn,0,0.0685618,"dy in the community. Our next task then is to explain how such judgements arise, i.e., to find features that differentiate texts of different argumentative depths. We show that some simple surface features are not sufficient, and then turn to the underlying pragmatics, as it is – to some extent – captured in analyses according to Rhetorical Structure Theory (RST; (Mann and Thompson, 1988)). It was designed as a tool to make the reasons for a text’s coherence explicit, and the specific notion of coherence relation used in RST (as opposed to other approaches such as the Penn Discourse Treebank (Prasad et al., 2008) or Segmented Discourse Representation Theory (Asher and Lascarides, 2003)) is decidedly intentional. This makes RST a good This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 3308 Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers, pages 3308–3317, Osaka, Japan, December 11-17 2016. candidate for checking whether its text representations are able to predict argumentative depth. The corpus we are using in our study has already been annotated"
C16-1312,C14-1142,0,0.0707544,"field is the mining of arguments in customer reviews (e.g., (Villalba and Saint-Dizier, 2012)), where the goal is to move beyond finding positive and negative statements by also finding the reasons that customers provide for their judgements. But the general task can be applied in many other domains as well. For example, the early research of Mochales Palau and Moens (2009) sought to identify premises and conclusions in legal text; another early work on finding theses in student essays (Burstein et al., 2003) has recently been extended to detecting more argumentative structure in such essays (Stab and Gurevych, 2014), (Nguyen and Litman, 2015); other genres that are being tackled include online dialogue (Oraby et al., 2015), multi-party dialogue (Budzynska et al., 2013) and scientific papers (Kirschner et al., 2015). Despite the manifold recent activities, the field is still young, and a number of basic issues still require attention. This concerns the design of annotation schemes and possibly finding a consensus on them, but also the more fundamental problem of defining the notion of, and identifying instances of, argumentative text. While it is tempting to assume that earlier work on text-level subjecti"
C16-1312,stede-neumann-2014-potsdam,1,0.860546,"nalysis here and omit, inter alia, a justification of defining the boundaries of segments (argumentative discourse units). See (Peldszus and Stede, 2013). 3310 3 Argumentation in Newspapers: Our Corpus Our overall goal is to determine whether argumentative depth can be (i) reliably annotated by humans, and (ii) measured automatically. To this end, we wish to correlate it with different layers of linguistic analysis. In order to start this, we favour a corpus of argumentative text that is annotated with argument structure and also with additional layers. The German ‘Potsdam Commentary Corpus’ (Stede and Neumann, 2014) fulfills this criterion, as it comes with sentence syntax, connectives and their arguments, coreference, and rhetorical structure. We will concentrate here on rhetorical structure; note that our findings do not depend on any specifics of German; a corpus in any other language could be used in the same way. 3.1 The genre of commentary Most newspapers offer ‘opinion’ pages with editorials that comment upon current affairs. The specifics of this genre depend to some extent on the different national traditions, but we expect that classifications like that of Schneider and Raue (1996) for German c"
C18-1318,W17-5115,1,0.938912,"acter, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an app"
C18-1318,C16-1324,1,0.861364,"rsuasion tactics of four types: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authorities, and those that rely on interpersonal factors. These tactics are found in small text spans and could be seen as the local counterpart of the global strategies we consider. To our knowledge, only we have explicitly worked towards a computational analysis of such strategies so far. In particular, we presented a corpus with 300 news editorials whose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2"
C18-1318,D17-1141,1,0.860545,"s: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authorities, and those that rely on interpersonal factors. These tactics are found in small text spans and could be seen as the local counterpart of the global strategies we consider. To our knowledge, only we have explicitly worked towards a computational analysis of such strategies so far. In particular, we presented a corpus with 300 news editorials whose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete argu"
C18-1318,P16-2085,0,0.187524,"ose units are labeled with their roles in the argumentation, such as “testimony” and “common ground” (Al-Khatib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges th"
C18-1318,N16-1166,0,0.0279473,"essed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Similarly, Wang et al. (2017) reveal the importance of selecting the right framing of a discussion topic for winning classical debates. In such dialogical situations, the arguments of opposing parties are usually fragmented into several alternating parts. Our work, in contrast, analyzes the rhetorical strategies of complete monological texts where an author presents his or her entire argumentation. Studying persuasive blog posts, Anand et al. (2011) develop a scheme with 16"
C18-1318,P11-1099,0,0.0515776,"as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for th"
C18-1318,D17-1249,0,0.0183513,"synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al."
C18-1318,D15-1255,0,0.0399843,"ses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argu"
C18-1318,D16-1129,0,0.0156505,". (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Simi"
C18-1318,W17-5102,0,0.0434853,"analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-"
C18-1318,E17-1070,0,0.239536,"s logos, ethos, and pathos. In the realm of the area of computational argumentation, rhetorical strategies are particularly relevant for technologies that synthesize argumentative text and that aim to deliver arguments effectively. Existing argument mining research largely focuses on the logical structure of arguments, identifying their units (premises vs. conclusions) and relations (support vs. attack). Recently, a few studies have tackled strategy-related aspects, such as explicit expressions of ethos (Duthie et al., 2016) and the effects of logical and emotional arguments across audiences (Lukin et al., 2017). So far, however, strategies have not been considered in argumentation synthesis, which altogether has not received much attention (see Section 2). In this paper, we study the role of rhetorical strategies when synthesizing argumentation. In particular, we consider monological argumentative texts where an author seeks to persuade target readers of his or her stance towards a given topic, such as news editorials and persuasive essays. Conceptually, we argue that an author synthesizes a text of such genres in three subsequent steps: 1. Selecting content in terms of argumentative discourse units"
C18-1318,D17-1318,0,0.153505,"tative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhet"
C18-1318,naderi-hirst-2017-classifying,0,0.0521996,"systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously."
C18-1318,D15-1110,1,0.865613,"credibility and good character, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) devel"
C18-1318,D10-1023,0,0.0229266,"t et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exist only for formal argumentation (Rosenfeld and Kraus, 2016). Our synthesis-oriented model covers content, structure, and style properties. For computational purposes, such properties need to be mined before. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political"
C18-1318,W15-0507,0,0.220234,"tib et al., 2016). In (Al-Khatib et al., 2017), we then trained a classifier on this corpus to find sequential role patterns in 30k New York Times editorials. While we observed insightful variances in the use of evidence across editorial topics, it still remains unclear to what extent such patterns really reflect rhetorical strategies. Clearly diverging from previous research, we consider rhetorical strategies in argumentation synthesis, for which related work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their sys"
C18-1318,D15-1050,0,0.134517,"Missing"
C18-1318,P15-4019,0,0.205315,"from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their system pursues similar goals as we do, phrasing an ordered text with multiple arguments. We extend their idea by rhetorical considerations, and we propose a general argumentation synthesis model. It is in line with classical concepts of building natural language generation systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The Debater (Rinott et al., 2015), namely, how to deliver arguments effectively. So far, strategical systems exi"
C18-1318,W14-2110,0,0.024727,"ional way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four"
C18-1318,P17-1011,0,0.136561,"tegy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesiz"
C18-1318,D14-1006,0,0.455036,"stration of the speaker’s credibility and good character, and (3) pathos, the evoking of the right emotions in the target audience. As Aristotle, we target argumentation that aims for persuasion — as opposed to critical discussions where strategic maneuvering is needed to achieve reasonableness (van Eemeren et al., 2014). Joviˇci´c (2006) proposed a procedure to evaluate the effectiveness of persuasive argumentation, though not in a computational way. In natural language processing, most computational argumentation research focuses on the mining of argumentative units and relations from text (Stab and Gurevych, 2014; Peldszus and Stede, 2015; Ajjour et al., 2017). Argument mining infers the logical structure of arguments, but it does not analyze the strategy used to compose arguments. Feng and Hirst (2011) classify the five most common argumentation schemes of Walton et al. (2008). Such a scheme defines a pattern capturing the logical inference from an 3754 argument’s premises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise,"
C18-1318,C16-1158,1,0.888935,"ises to its claim, i.e., it primarily aims at logos only. Song et al. (2014) combine the idea of schemes with strategy-related considerations, and Habernal and Gurevych (2015) annotate pathos in forum comments and blog posts. Likewise, Duthie et al. (2016) develop a corpus and an approach to mine explicit expressions of ethos from political debates, whereas Hidey et al. (2017) even annotate all three means of persuasion for the premises of arguments. The provided data and methods may be helpful for studying persuasive effectiveness, but they have not been employed so far for that purpose. In (Wachsmuth et al., 2016), we use the output of argument mining to assess four quality dimensions of persuasive essays. Some assessed dimensions relate to rhetoric, such as argument strength, which is defined based on how many readers are persuaded. Habernal and Gurevych (2016) examine the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on"
C18-1318,D17-1253,1,0.845016,"e. To capture content, some works identify the different frames under which a topic can be viewed (Naderi and Hirst, 2017), model key aspects of a topic (Menini et al., 2017), or analyze potentially strategy-related topic patterns in campaign speeches (Gautrais et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effect"
C18-1318,Q17-1016,0,0.067982,"e the reasons that make arguments convincing, and Lukin et al. (2017) study how effective logos-oriented and pathosoriented arguments are depending on the target audience. However, none of these works considers the application of rhetorical strategies. Tan et al. (2016) find that the chance to persuade someone in good-faith discussions on Reddit Change My View is increased through multiple interactions and an appropriate linguistic style. Cano-Basave and He (2016) analyze the impact of the semantic framing of arguments (e.g., “taking sides” and “manipulation”) in political debates. Similarly, Wang et al. (2017) reveal the importance of selecting the right framing of a discussion topic for winning classical debates. In such dialogical situations, the arguments of opposing parties are usually fragmented into several alternating parts. Our work, in contrast, analyzes the rhetorical strategies of complete monological texts where an author presents his or her entire argumentation. Studying persuasive blog posts, Anand et al. (2011) develop a scheme with 16 persuasion tactics of four types: those that postulate outcomes of an uptake, those that generalize in some way, those that appeal to external authori"
C18-1318,W15-0512,0,0.182313,"work is generally still scarce. Bilu and Slonim (2016) generate new claims by recycling topics and predicates from existing claims, whereas Reisert et al. (2015) synthesize complete arguments based on the model of Toulmin (1958) where a claim is supported by data that is reasoned by a warrant. Like us, those authors rely on a pool of argument units from which arguments are built. However, they restrict their view to logical argument structure. The same holds for Green (2017) who generates arguments with particular schemes (e.g., “cause to effect”) that are used to teach learners how to argue. Yanase et al. (2015) present a method that arranges the sentences of an argumentative text in a natural way. Sato et al. (2015) build upon this method. Their system pursues similar goals as we do, phrasing an ordered text with multiple arguments. We extend their idea by rhetorical considerations, and we propose a general argumentation synthesis model. It is in line with classical concepts of building natural language generation systems (Reiter and Dale, 1997), but it targets argumentative texts in particular. With that, we seek to contribute a missing aspect to technologies that synthesize arguments such as The D"
C18-1318,D17-1164,0,0.0256897,"et al., 2017). In terms of structure, Persing et al. (2010) learn sequences of discourse functions in essays (e.g., “rebuttal” or “conclusion”) that correlate with a good organization, and we have modeled flows of arbitrary types of information to classify text properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesizes texts such that they have a certain style in terms of sen"
C18-1318,W00-1408,0,0.740009,"t properties, such as stance or quality (Wachsmuth and Stein, 2017). 3755 In (Wachsmuth et al., 2017), we extend this idea to model sequential and hierarchical argumentation structure simultaneously. Regarding style, Lawrence et al. (2017) analyze rhetorical figures in arguments, Song et al. (2017) classify discourse modes in essays, and Zhang et al. (2017) study rhetorical questions in political discourse. All such methods may be relevant when we operationalize our model. Finally, the use of strategies in synthesis scenarios follows up on early work on discourse planning (Young et al., 1994; Zukerman et al., 2000). The computational approaches relied on rule-based techniques to create effective arguments (Carenini and Moore, 2006) for a few selected tactics. More recent research on general text generation uses probabilistic models to employ text structure (Barzilay, 2010), or synthesizes texts such that they have a certain style in terms of sentiment or similar (Hu et al., 2017; Shen et al., 2017). Our model is meant to provide an abstract framework to be exploited in such approaches. 3 Model We now delineate our model of rhetorical strategies for synthesizing a monological argumentative text following"
C98-2197,P97-1011,0,0.0421183,"Missing"
C98-2197,W98-1414,1,0.867014,"Missing"
C98-2197,W97-0401,1,0.771679,"Missing"
C98-2197,W96-0401,0,0.0440875,"Missing"
C98-2197,J95-1002,0,\N,Missing
C98-2197,P97-1013,0,\N,Missing
D15-1110,P05-1012,0,0.0111661,"e not being the central claim, which is capturing the intuition that central claims are unlikely to have outgoing edges: The simple model just described might be able to learn which segment pairs actually attach, i.e., correspond to some argumentative relation in the corpus. However it is not guaranteed to yield predictions that can be combined to a tree structure again. A more appropriate model would enforce global constraints on its predictions. In the simple+MST model, this is achieved by a minimum spanning tree (MST) decoding, which has first been applied for syntactic dependency parsing (McDonald et al., 2005a; McDonald et al., 2005b) and later for discourse parsing (Baldridge et al., 2007; Muller et al., 2012). First, we build a fullyconnected directed graph, with one node for each text segment. The weight of each edge is the attachment probability predicted by the learned classifier for the corresponding pair of source and target segment. We then apply the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to determine the minimum spanning tree, i.e., the subgraph connecting all nodes with minimal total edge cost (in our case highest total edge probability). This resulting tree then re"
D15-1110,H05-1066,0,0.0238636,"e not being the central claim, which is capturing the intuition that central claims are unlikely to have outgoing edges: The simple model just described might be able to learn which segment pairs actually attach, i.e., correspond to some argumentative relation in the corpus. However it is not guaranteed to yield predictions that can be combined to a tree structure again. A more appropriate model would enforce global constraints on its predictions. In the simple+MST model, this is achieved by a minimum spanning tree (MST) decoding, which has first been applied for syntactic dependency parsing (McDonald et al., 2005a; McDonald et al., 2005b) and later for discourse parsing (Baldridge et al., 2007; Muller et al., 2012). First, we build a fullyconnected directed graph, with one node for each text segment. The weight of each edge is the attachment probability predicted by the learned classifier for the corresponding pair of source and target segment. We then apply the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to determine the minimum spanning tree, i.e., the subgraph connecting all nodes with minimal total edge cost (in our case highest total edge probability). This resulting tree then re"
D15-1110,C12-1115,0,0.0493674,"outgoing edges: The simple model just described might be able to learn which segment pairs actually attach, i.e., correspond to some argumentative relation in the corpus. However it is not guaranteed to yield predictions that can be combined to a tree structure again. A more appropriate model would enforce global constraints on its predictions. In the simple+MST model, this is achieved by a minimum spanning tree (MST) decoding, which has first been applied for syntactic dependency parsing (McDonald et al., 2005a; McDonald et al., 2005b) and later for discourse parsing (Baldridge et al., 2007; Muller et al., 2012). First, we build a fullyconnected directed graph, with one node for each text segment. The weight of each edge is the attachment probability predicted by the learned classifier for the corresponding pair of source and target segment. We then apply the Chu-Liu-Edmonds algorithm (Chu and Liu, 1965; Edmonds, 1967) to determine the minimum spanning tree, i.e., the subgraph connecting all nodes with minimal total edge cost (in our case highest total edge probability). This resulting tree then represents the best global attachment structure for a text given the predicted probabilities. 5.5 cci,j ="
D15-1110,P11-1099,0,0.387052,"Missing"
D15-1110,W14-2105,0,0.204117,"Missing"
D15-1110,I13-1191,0,0.0855783,"Missing"
D15-1110,W14-2111,0,0.0344557,"Missing"
D15-1110,D14-1006,0,0.425292,"nearly identical and do not show a significant difference. 7 To the best of our knowledge, this is the first data-driven model of argumentation structure that optimizes argumentation structure globally for the complete sequence of input segments. Furthermore, it is the first model jointly tackling segment type classification, relation identification and relation type classification. Although a direct comparison with results from related work on other corpora is not possible, we can draw indirect comparisons. The first learned model without decoding (simple) is similar to the one presented by Stab and Gurevych (2014). Since it is outperformed by our joint MST decoding model on our data, we assume similar gains could be accomplished on their student essay dataset. Summary and Outlook We introduced a new approach to argumentation mining that we applied to a parallel German/English corpus of 112 short texts. For the purposes of automatic mining, the original more fine-grained annotation in the corpus was reduced to a slightly simplified scheme consisting of support and attack relations among argumentative discourse units. We did not address the segmentation step here but focused on structure prediction, whic"
D15-1110,J02-4002,0,0.0249342,"gumentative function Introduction Argumentation mining is a task that has drawn increased interest in the last years. In its fullfledged version, it seeks to automatically recognize the structure of argumentation in a text by identifying and connecting the central claim of the text, supporting premises, possible objections, and counter-objections to these objections.1 A variety of applications can profit from access to the argumentative structure of text, including the retrieval of relevant court decisions from legal databases (Palau and Moens, 2011), automatic document summarization systems (Teufel and Moens, 2002), the analysis of scientific papers in biomedical text mining (Teufel, 2010; Liakata • Relation identification: building a connected tree- or graph-structure to represent argumentative relations between the ADUs • Relation type classification: determining the type of argumentative relation (e.g. supporting versus attacking relations or more finegrained types) In this paper, we address the last three subtasks: Given a text segmented into relevant ADUs, identify the argumentation structure. We will work with a bilingual corpus of short texts that have been generated in a text production experime"
D15-1110,W14-2112,1,\N,Missing
E93-1055,T87-1046,0,0.0409384,"Missing"
E93-1055,C88-2100,0,0.0663591,"Missing"
E93-1055,W90-0105,0,0.0371025,"Missing"
E93-1055,J93-3002,0,\N,Missing
J11-2001,W10-0731,0,0.080595,"Missing"
J11-2001,E06-1027,0,0.0735695,"Missing"
J11-2001,P08-1034,0,0.113718,"at they are more robust across domains. Their classiﬁers are, in effect, dictionarybased, differing only in the methodology used to build the dictionary. Li et al. (2010) use co-training to incorporate labeled and unlabeled examples, also making use of 3 Blitzer, Dredze, and Pereira (2007) do show some success in transferring knowledge across domains, so that the classiﬁer does not have to be re-built entirely from scratch. 269 Computational Linguistics Volume 37, Number 2 a distinction between sentences with a ﬁrst person subject and with other subjects. Other hybrid methods include those of Andreevskaia and Bergler (2008), Dang, Zhang, and Chen (2010), Dasgupta and Ng (2009), Goldberg and Zhu (2006), or Prabowo and Thelwall (2009). Wan (2009) uses co-training in a method that uses English labeled data and an English classiﬁer to learn a classiﬁer for Chinese. In our approach, we seek methods that operate at a deep level of analysis, incorporating semantic orientation of individual words and contextual valence shifters, yet do not aim at a full linguistic analysis (one that involves analysis of word senses or argument structure), although further work in that direction is possible. In this article, starting in"
J11-2001,C08-2002,0,0.0704027,"Missing"
J11-2001,baccianella-etal-2010-sentiwordnet,0,0.541035,"Missing"
J11-2001,P07-1056,0,0.209864,"Missing"
J11-2001,N07-1039,0,0.0588942,"Missing"
J11-2001,H92-1022,0,0.119727,"only negative when it is a verb, but should not be so in a noun dictionary; novel is a positive adjective, but a neutral noun. To build the system and run our experiments, we use the corpus described in Taboada and Grieve (2004) and Taboada, Anthony, and Voll (2006), which consists of a 400-text collection of Epinions reviews extracted from eight different categories: books, cars, computers, cookware, hotels, movies, music, and phones, a corpus we named “Epinions 1.” Within each collection, the reviews were split into 25 positive and 25 4 To determine part of speech, we use the Brill tagger (Brill 1992). 271 Computational Linguistics Volume 37, Number 2 negative reviews, for a total of 50 in each category, and a grand total of 400 reviews in the corpus (279,761 words). We determined whether a review was positive or negative through the “recommended” or “not recommended” feature provided by the review’s author. 2.2 Nouns, Verbs, and Adverbs In the following example, adapted from Polanyi and Zaenen (2006), we see that lexical items other than adjectives can carry important semantic polarity information. (1) a. The young man strolled+ purposefully+ through his neighborhood+. b. The teenaged mal"
J11-2001,R09-1010,1,0.204264,"Missing"
J11-2001,D09-1030,0,0.0819612,"Missing"
J11-2001,D08-1083,0,0.185675,"y creating separate features, namely, the appearance of good might be either good (no modiﬁcation) not good (negated good), int good (intensiﬁed good), or dim good (diminished good). The classiﬁer, however, cannot determine that these four types of good are in any way related, and so in order to train accurately there must be enough examples of all four in the training corpus. Moreover, we show in Section 2.4 that expanding the scope to two-word phrases does not deal with negation adequately, as it is often a long-distance phenomenon. Recent work has begun to address this issue. For instance, Choi and Cardie (2008) present a classiﬁer that treats negation from a compositional point of view by ﬁrst calculating polarity of terms independently, and then applying inference rules to arrive at a combined polarity score. As we shall see in Section 2, our lexicon-based model handles negation and intensiﬁcation in a way that generalizes to all words that have a semantic orientation value. A middle ground exists, however, with semi-supervised approaches to the problem. Read and Carroll (2009), for instance, use semi-supervised methods to build domainindependent polarity classiﬁers. Read and Carroll built differen"
J11-2001,W10-3110,0,0.0641942,"Missing"
J11-2001,P09-1079,0,0.0588871,"n effect, dictionarybased, differing only in the methodology used to build the dictionary. Li et al. (2010) use co-training to incorporate labeled and unlabeled examples, also making use of 3 Blitzer, Dredze, and Pereira (2007) do show some success in transferring knowledge across domains, so that the classiﬁer does not have to be re-built entirely from scratch. 269 Computational Linguistics Volume 37, Number 2 a distinction between sentences with a ﬁrst person subject and with other subjects. Other hybrid methods include those of Andreevskaia and Bergler (2008), Dang, Zhang, and Chen (2010), Dasgupta and Ng (2009), Goldberg and Zhu (2006), or Prabowo and Thelwall (2009). Wan (2009) uses co-training in a method that uses English labeled data and an English classiﬁer to learn a classiﬁer for Chinese. In our approach, we seek methods that operate at a deep level of analysis, incorporating semantic orientation of individual words and contextual valence shifters, yet do not aim at a full linguistic analysis (one that involves analysis of word senses or argument structure), although further work in that direction is possible. In this article, starting in Section 2, we describe the Semantic Orientation CALcul"
J11-2001,P07-1124,0,0.0611229,"2004; Wiebe et al. 2004; Kennedy and Inkpen 2006; Ng, Dasgupta, and Niaz Ariﬁn 2006; Sokolova and Lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. The results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively. Ours is not the only method that uses linguistic information or dictionaries. Many other systems make use of either the Subjectivity dictionary of Wiebe and colleagues, or of SentiWordNet (Devitt and Ahmad 2007; Thet et al. 2009), and some work relies on Appraisal Theory (Whitelaw, Garg, and Argamon 2005; Bloom, Garg, and Argamon 2007), a theory developed by Martin and White (2005). We also discuss, in Section 2.4, work on incorporating linguistic insights for the treatment of negation (Moilanen and Pulman 2007; Choi and Cardie 2008). 5. Conclusions and Future Research We have presented a word-based method for extracting sentiment from texts. Building on previous research that made use of adjectives, we extend the Semantic Orientation CALculator (SO-CAL) to other parts of speech. We also introduce i"
J11-2001,esuli-sebastiani-2006-sentiwordnet,0,0.230763,"and adverbs (Sokolova and Lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (Zaidan and Eisner 2008). In general, the SO of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). The dictionary can be created in different ways: manually, using existing dictionaries such as the General Inquirer (Stone et al. 1966), or semi-automatically, making use of resources like WordNet (Hu and Liu 2004; Kim 270 Taboada et al. Lexicon-Based Methods for Sentiment Analysis and Hovy 2004; Esuli and Sebastiani 2006). The dictionary may also be produced automatically via association, where the score for each new adjective is calculated using the frequency of the proximity of that adjective with respect to one or more seed words. Seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. In principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. The assoc"
J11-2001,U05-1020,0,0.0907847,"Missing"
J11-2001,W06-3808,0,0.0624214,"Missing"
J11-2001,N09-1057,0,0.0617962,"Missing"
J11-2001,P97-1023,0,0.110973,"classiﬁcation approach involves building classiﬁers from labeled instances of texts or sentences (Pang, Lee, and Vaithyanathan 2002), essentially a supervised classiﬁcation task. The latter approach could also be described as a statistical or machine-learning approach. We follow the ﬁrst method, in which we use dictionaries of words annotated with the word’s semantic orientation, or polarity. Dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also Stone et al. 1966; Tong 2001), or automatically, using seed words to expand the list of words (Hatzivassiloglou and McKeown 1997; Turney 2002; Turney and Littman 2003). Much of the lexicon-based research has focused on using adjectives as indicators of the semantic orientation of text (Hatzivassiloglou and McKeown 1997; Wiebe 2000; Hu and Liu 2004; Taboada, Anthony, and Voll 2006).2 First, a list of adjectives and corresponding SO values is compiled into a dictionary. Then, for any given text, all adjectives are extracted and annotated with their SO value, using the dictionary scores. The SO scores are in turn aggregated into a single score for the text. The majority of the statistical text classiﬁcation research build"
J11-2001,C04-1200,0,0.0680326,"umans (through the use of the Mechanical Turk interface). Section 4 provides comparisons to other work, and Section 5 conclusions. 2. SO-CAL, the Semantic Orientation CALculator Following Osgood, Suci, and Tannenbaum (1957), the calculation of sentiment in SO-CAL begins with two assumptions: that individual words have what is referred to as prior polarity, that is, a semantic orientation that is independent of context; and that said semantic orientation can be expressed as a numerical value. Several lexiconbased approaches have adopted these assumptions (Bruce and Wiebe 2000; Hu and Liu 2004; Kim and Hovy 2004). In this section, we describe the different dictionaries used in SO-CAL, and the incorporation of valence shifters. We conclude the section with tests that show SO-CAL’s performance on different data sets. 2.1 Adjectives Much of the early research in sentiment focused on adjectives or adjective phrases as the primary source of subjective content in a document (Hatzivassiloglou and McKeown 1997; Hu and Liu 2004; Taboada, Anthony, and Voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (Benamara et al. 2007); adjectives and verbs (Kim a"
J11-2001,P10-1043,0,0.0626234,"Missing"
J11-2001,W10-0718,0,0.102582,"Missing"
J11-2001,D09-1063,0,0.0783663,"Missing"
J11-2001,W10-0204,0,0.520523,"Missing"
J11-2001,W04-3253,0,0.113904,"ers trained on n-grams or similar features (Pang, Lee, and Vaithyanathan 2002), and the use of sentiment dictionaries (Esuli and Sebastiani 2006; Taboada, Anthony, and Voll 2006). Support Vector Machine (SVM) classiﬁers have been shown to outperform lexicon-based models within a single domain (Kennedy and Inkpen 2006); they have trouble with cross-domain tasks (Aue and Gamon 2005), however, and some researchers have argued for hybrid classiﬁers (Andreevskaia and Bergler 2008). Although some of the machine-learningbased work makes use of linguistic features for training (Riloff and Wiebe 2003; Mullen and Collier 2004; Wiebe et al. 2004; Kennedy and Inkpen 2006; Ng, Dasgupta, and Niaz Ariﬁn 2006; Sokolova and Lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. The results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively. Ours is not the only method that uses linguistic information or dictionaries. Many other systems make use of either the Subjectivity dictionary of Wiebe and colleagues, or of SentiWordNet (Devit"
J11-2001,P10-1141,0,0.223657,"could pre-process the text and tag spans that are believed to be topic sentences, another module could provide discourse information such as rhetorical relations (Mann and Thompson 1988), and a third module could label the sentences that seem to be subjective. Armed with this information, SO-CAL can disregard or de-emphasize parts of the text that are less relevant to sentiment analysis. This weighting feature is used in Taboada, Brooke, and 17 One of the reviewers points out that this is similar to the use of term frequency (tf-idf) in information retrieval (Salton and McGill 1983). See also Paltoglou and Thelwall (2010) for a use of information retrieval techniques in sentiment analysis. 280 Taboada et al. Lexicon-Based Methods for Sentiment Analysis Stede (2009) to lower the weight of descriptive paragraphs, as opposed to paragraphs that contain mostly commentary. Secondly, SO-CAL allows for multiple cut-offs. Most work in sentiment analysis has focused on binary positive/negative classiﬁcation. Notable exceptions include Koppel and Schler (2005) and Pang and Lee (2005), who each adapted relevant SVM machinelearning algorithms to sentiment classiﬁcation with a three- and four-class system, respectively. Bec"
J11-2001,P04-1035,0,0.263175,"c orientation for the entire sentence.5 In order to make use of this additional information, we created separate noun, verb, and adverb dictionaries, hand-ranked using the same +5 to −5 scale as our adjective dictionary. The enhanced dictionaries contain 2,252 adjective entries, 1,142 nouns, 903 verbs, and 745 adverbs.6 The SO-carrying words in these dictionaries were taken from a variety of sources, the three largest being Epinions 1, the 400-text corpus described in the previous section; a 100-text subset of the 2,000 movie reviews in the Polarity Dataset (Pang, Lee, and Vaithyanathan 2002; Pang and Lee 2004, 2005);7 and positive and negative words from the General Inquirer dictionary (Stone et al. 1966; Stone 1997).8 The sources provide a fairly good range in terms of register: The Epinions and movie reviews represent informal language, with words such as ass-kicking and nifty; at the other end of the spectrum, the General Inquirer was clearly built from much more formal texts, and contributed words such as adroit and jubilant, which may be more useful in the processing of literary reviews (Taboada, Gillies, and McFetridge 2006; Taboada et al. 2008) or other more formal texts. Each of the open-c"
J11-2001,P05-1015,0,0.286207,"points out that this is similar to the use of term frequency (tf-idf) in information retrieval (Salton and McGill 1983). See also Paltoglou and Thelwall (2010) for a use of information retrieval techniques in sentiment analysis. 280 Taboada et al. Lexicon-Based Methods for Sentiment Analysis Stede (2009) to lower the weight of descriptive paragraphs, as opposed to paragraphs that contain mostly commentary. Secondly, SO-CAL allows for multiple cut-offs. Most work in sentiment analysis has focused on binary positive/negative classiﬁcation. Notable exceptions include Koppel and Schler (2005) and Pang and Lee (2005), who each adapted relevant SVM machinelearning algorithms to sentiment classiﬁcation with a three- and four-class system, respectively. Because SO-CAL outputs a numerical value that reﬂects both the polarity and strength of words appearing in the text, it is fairly straightforward to extend the function to any level of granularity required; in particular, the SO-CAL grouping script takes a list of n cut-off values, and classiﬁes texts into n + 1 classes based on text values. The evaluative output gives information about exact matches and also near-misses (when a text is incorrectly classiﬁed"
J11-2001,W02-1011,0,0.061831,"Missing"
J11-2001,E09-1077,0,0.118968,"of the proximity of that adjective with respect to one or more seed words. Seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. In principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. The association is usually calculated following Turney’s method for computing mutual information (Turney 2002; Turney and Littman 2003), but see also Rao and Ravichandran (2009) and Velikovich et al. (2010) for other methods using seed words. Previous versions of SO-CAL (Taboada and Grieve 2004; Taboada, Anthony, and Voll 2006) relied on an adjective dictionary to predict the overall SO of a document, using a simple aggregate-and-average method: The individual scores for each adjective in a document are added together and then divided by the total number of adjectives in that document.4 As we describe subsequently, the current version of SO-CAL takes other parts of speech into account, and makes use of more sophisticated methods to determine the true contribution of"
J11-2001,W03-1014,0,0.063188,"achine-learning classiﬁers trained on n-grams or similar features (Pang, Lee, and Vaithyanathan 2002), and the use of sentiment dictionaries (Esuli and Sebastiani 2006; Taboada, Anthony, and Voll 2006). Support Vector Machine (SVM) classiﬁers have been shown to outperform lexicon-based models within a single domain (Kennedy and Inkpen 2006); they have trouble with cross-domain tasks (Aue and Gamon 2005), however, and some researchers have argued for hybrid classiﬁers (Andreevskaia and Bergler 2008). Although some of the machine-learningbased work makes use of linguistic features for training (Riloff and Wiebe 2003; Mullen and Collier 2004; Wiebe et al. 2004; Kennedy and Inkpen 2006; Ng, Dasgupta, and Niaz Ariﬁn 2006; Sokolova and Lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. The results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively. Ours is not the only method that uses linguistic information or dictionaries. Many other systems make use of either the Subjectivity dictionary of Wiebe and colleagues,"
J11-2001,D08-1027,0,0.0660921,"Missing"
J11-2001,R09-1076,0,0.086209,"hrases as the primary source of subjective content in a document (Hatzivassiloglou and McKeown 1997; Hu and Liu 2004; Taboada, Anthony, and Voll 2006), albeit with some exceptions, especially more recently, which have also included the use of adverbs (Benamara et al. 2007); adjectives and verbs (Kim and Hovy 2004); adjective phrases (Whitelaw, Garg, and Argamon 2005); two-word phrases (Turney 2002; Turney and Littman 2003); adjectives, verbs, and adverbs (Subrahmanian and Reforgiato 2008); the exclusive use of verbs (Sokolova and Lapalme 2008); the use of non-affective adjectives and adverbs (Sokolova and Lapalme 2009a, 2009b); or rationales, words and phrases selected by human annotators (Zaidan and Eisner 2008). In general, the SO of an entire document is the combined effect of the adjectives or relevant words found within, based upon a dictionary of word rankings (scores). The dictionary can be created in different ways: manually, using existing dictionaries such as the General Inquirer (Stone et al. 1966), or semi-automatically, making use of resources like WordNet (Hu and Liu 2004; Kim 270 Taboada et al. Lexicon-Based Methods for Sentiment Analysis and Hovy 2004; Esuli and Sebastiani 2006). The dictio"
J11-2001,N03-1030,0,0.0632353,"ovie ratings). Weighting paragraphs according to this classiﬁcation, with lower weights assigned to description, results in a statistically-signiﬁcant improvement in the polarity classiﬁcation task. The classiﬁcation of paragraphs into comment and description is but one of the many ways in which contextual information can be incorporated into a robust approach to sentiment extraction. In previous work (Voll and Taboada 2007), we showed a prototype for extracting topic sentences, and performing sentiment analysis on those only. We also showed how a sentence-level discourse parser, developed by Soricut and Marcu (2003), could be used to differentiate between main and secondary parts of the text. At 301 Computational Linguistics Volume 37, Number 2 the sentence level, exploring the types of syntactic patterns that indicate subjectivity and sentiment is also a possibility (Greene and Resnik 2009). Syntactic patterns can also be used to distinguish different types of opinion and appraisal (Bednarek 2009). Our current work focuses on developing discourse parsing methods, both general and speciﬁc to the review genre. At the same time, we will investigate different aggregation strategies for the different types o"
J11-2001,S07-1013,0,0.32773,"Missing"
J11-2001,N09-1064,0,0.0915259,"Missing"
J11-2001,taboada-etal-2006-methods,1,0.192451,"Missing"
J11-2001,W09-3909,1,0.0862591,"Missing"
J11-2001,P02-1053,0,0.164585,"1992). In this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. Our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal. There exist two main approaches to the problem of extracting sentiment automatically.1 The lexicon-based approach involves calculating orientation for a document from the semantic orientation of words or phrases in the document (Turney 2002). The text classiﬁcation approach involves building classiﬁers from labeled instances of texts or sentences (Pang, Lee, and Vaithyanathan 2002), essentially a supervised classiﬁcation task. The latter approach could also be described as a statistical or machine-learning approach. We follow the ﬁrst method, in which we use dictionaries of words annotated with the word’s semantic orientation, or polarity. Dictionaries for lexicon-based approaches can be created manually, as we describe in this article (see also Stone et al. 1966; Tong 2001), or automatically, using seed words to expand the list"
J11-2001,J06-3003,0,0.0730056,"is created affects the overall accuracy of subsequent results. In Taboada, Anthony, and Voll (2006) we report on experiments using different search engines and operators in trying to create dictionaries semiautomatically. We found that, although usable, dictionaries created using the Google search engine were unstable. When rerun, the results for each word were subject to change, sometimes by extreme amounts, something that Kilgarriff (2007) also notes, arguing against the use of Google for linguistic research of this type. An alternative would be to use a sufﬁciently large static corpus, as Turney (2006) does to measure relational similarity across word pairs. Automatically or semi-automatically created dictionaries have some advantages. We found many novel words in our initial Google-generated dictionary. For instance, unlistenable was tagged accurately as highly negative, an advantage that Velikovich et al. (2010) point out. However, in light of the lack of stability for automatically generated dictionaries, we decided to create manual ones. These were produced by hand-tagging all adjectives found in our development corpus, a 400-text corpus of reviews (see the following) on a scale ranging"
J11-2001,N10-1119,0,0.119296,"ive with respect to one or more seed words. Seed words are a small set of words with strong negative or positive associations, such as excellent or abysmal. In principle, a positive adjective should occur more frequently alongside the positive seed words, and thus will obtain a positive score, whereas negative adjectives will occur most often in the vicinity of negative seed words, thus obtaining a negative score. The association is usually calculated following Turney’s method for computing mutual information (Turney 2002; Turney and Littman 2003), but see also Rao and Ravichandran (2009) and Velikovich et al. (2010) for other methods using seed words. Previous versions of SO-CAL (Taboada and Grieve 2004; Taboada, Anthony, and Voll 2006) relied on an adjective dictionary to predict the overall SO of a document, using a simple aggregate-and-average method: The individual scores for each adjective in a document are added together and then divided by the total number of adjectives in that document.4 As we describe subsequently, the current version of SO-CAL takes other parts of speech into account, and makes use of more sophisticated methods to determine the true contribution of each word. It is important to"
J11-2001,P09-1027,0,0.127536,"ctionary. Li et al. (2010) use co-training to incorporate labeled and unlabeled examples, also making use of 3 Blitzer, Dredze, and Pereira (2007) do show some success in transferring knowledge across domains, so that the classiﬁer does not have to be re-built entirely from scratch. 269 Computational Linguistics Volume 37, Number 2 a distinction between sentences with a ﬁrst person subject and with other subjects. Other hybrid methods include those of Andreevskaia and Bergler (2008), Dang, Zhang, and Chen (2010), Dasgupta and Ng (2009), Goldberg and Zhu (2006), or Prabowo and Thelwall (2009). Wan (2009) uses co-training in a method that uses English labeled data and an English classiﬁer to learn a classiﬁer for Chinese. In our approach, we seek methods that operate at a deep level of analysis, incorporating semantic orientation of individual words and contextual valence shifters, yet do not aim at a full linguistic analysis (one that involves analysis of word senses or argument structure), although further work in that direction is possible. In this article, starting in Section 2, we describe the Semantic Orientation CALculator (SO-CAL) that we have developed over the last few years. We ﬁrst"
J11-2001,J94-2004,0,0.105406,"uci, and Tannenbaum 1957). When used in the analysis of public opinion, such as the automated interpretation of on-line product reviews, semantic orientation can be extremely helpful in marketing, measures of popularity and success, and compiling reviews. The analysis and automatic extraction of semantic orientation can be found under different umbrella terms: sentiment analysis (Pang and Lee 2008), subjectivity (Lyons 1981; Langacker 1985), opinion mining (Pang and Lee 2008), analysis of stance (Biber and Finegan 1988; Conrad and Biber 2000), appraisal (Martin and White 2005), point of view (Wiebe 1994; Scheibman 2002), evidentiality (Chafe and Nichols 1986), and a few others, without expanding into neighboring disciplines and the study of emotion (Ketal 1975; Ortony, Clore, and Collins 1988) and affect (Batson, Shaw, and Oleson 1992). In this article, sentiment analysis refers to the general method to extract subjectivity and polarity from text (potentially also speech), and semantic orientation refers to the polarity and strength of words, phrases, or texts. Our concern is primarily with the semantic orientation of texts, but we extract the sentiment of words and phrases towards that goal"
J11-2001,J04-3002,0,0.0736342,"similar features (Pang, Lee, and Vaithyanathan 2002), and the use of sentiment dictionaries (Esuli and Sebastiani 2006; Taboada, Anthony, and Voll 2006). Support Vector Machine (SVM) classiﬁers have been shown to outperform lexicon-based models within a single domain (Kennedy and Inkpen 2006); they have trouble with cross-domain tasks (Aue and Gamon 2005), however, and some researchers have argued for hybrid classiﬁers (Andreevskaia and Bergler 2008). Although some of the machine-learningbased work makes use of linguistic features for training (Riloff and Wiebe 2003; Mullen and Collier 2004; Wiebe et al. 2004; Kennedy and Inkpen 2006; Ng, Dasgupta, and Niaz Ariﬁn 2006; Sokolova and Lapalme 2008), it nonetheless still suffers from lack of cross-domain portability. The results presented here suggest that a lexicon-based system could outperform pure or hybrid machine-learning methods in cross-domain situations, though further research would be necessary to establish this point conclusively. Ours is not the only method that uses linguistic information or dictionaries. Many other systems make use of either the Subjectivity dictionary of Wiebe and colleagues, or of SentiWordNet (Devitt and Ahmad 2007; T"
J11-2001,H05-1044,0,0.175407,"Missing"
J11-2001,W10-0723,0,0.223072,"Missing"
J11-2001,D08-1004,0,0.0679194,"Missing"
J11-2001,P06-2079,0,\N,Missing
J11-2001,D09-1017,0,\N,Missing
J98-3003,H89-1022,0,0.0403122,"ned. In the next step, for verbs, the applicable alternations and extensions are computed and added to the set of options. Then a language-specific semantic specification SemSpec (see the example in Figure 3) is constructed in accordance with generation parameters pertaining to brevity, salience, and stylistic features. The SemSpec is then handed over to a surface generator: Penman (Penman group 1989) for English, and a variant developed at FAW Ulm for German. The SemSpec language is a subset of the input representation language that was developed for Penman, the sentence plan language (SPL) (Kasper 1989). An SPL expression consists of variables, types, and case roles; an example was given in Figure 3. Penman and SPL are based on the upper model (UM) (Bateman et al. 1990) introduced 12 Fr6hlich and van de Riet (1997) describe how MOOSE is employed in the generation component of an information system. 419 Computational Linguistics Volume 24, Number 3 Lexicon Domain Model °o& &apos;-.. ..:, . . i .•/ i~: Morphosyntax Morphosyntax Mowhosyntax Alternations Alternations Alternations Partial SemSpec Partial SemSpec Partial SemSpec Connotation Connotation Connotation Denotation Denotation Denotation SitSp"
J98-3003,J88-2003,0,0.170717,"period of time, whereas m o m e n t a n e o u s activities occur in an instant; a &quot;point adverbial&quot; such as at noon serves as a linguistic test. Events are occurrences that have a structure to them; in particular, their result, or their coming to an end is included in them: to destroy a building, to write a book. As their central feature we take them to always involve some change of state: the building loses its integrity, the book comes into existence, or gets finished. While Bach (1986) did not investigate the internal structure of events, others suggested that this needs to be done (e.g., Moens and Steedman 1988; Parsons 1990). Pustejovsky (1991) treated Vendlerian accomplishments and achievements as transitions from a state Q(y) to NOT-Q(y), and suggested that accomplishments in addition have an intrinsic agent performing an activity that brings about the change of state. We follow this line, but modify it in some ways. Basically, we see any event as involving a change of state; an activity responsible for the change can optionally be present. A plain transition is necessarily m o m e n t a n e o u s (The room lit up), whereas a transition-with-activity inherits its p r o t r a c t e d / m o m e n t"
J98-3003,W96-0404,0,0.0642351,"Missing"
J98-3003,C88-2100,0,0.0430552,"g applicable rules in the lexical entries avoids this problem but at the same time raises the question of why rules should be preferable to a simple enumeration of forms. We return to this point in section 5.3. 425 Computational Linguistics Volume 24, Number 3 5.2 Lexicalization in NLG In MOOSE, the lexicon is the central device for mapping between input representations and intermediate sentence-semantic representations. The idea of using the lexicon early in the generation process is not new; it has been realized in several other generators, for example in the frame-oriented system DIOGENES (Nirenburg and Nirenburg 1988). In contrast to earlier systems, however, MOOSE strengthens the role of lexical semantics in the generation process by distinguishing between the SitSpec and SemSpec levels and clearly specifying the relationships between the two (as done with the alternation rules). Furthermore, we have emphasized that lexical choice should be seen as a constraint satisfaction process, similar to Reiter (1991), who focused his attention on nouns, while we have concentrated on verbs. There are several other generators using Penman as a front-end. For example, the DRAFTER system (Paris et al. 1995) builds SPLs"
J98-3003,C94-1055,1,0.877786,"Missing"
J98-3003,C94-1043,0,0.0257859,"y at those questions that Saint-Dizier&apos;s approach proposes to better leave aside. For generation, however, we believe that a system must know about the fine-grained changes in meaning that a verb alternation implies--a generator has to relate some semantic input representation to verb meaning, after all, and that includes alternations. And if the semantic change induced by an alternation can be described by a general rule that covers a whole class of verbs, a useful abstraction is gained. The final point to consider is the question of admitting lexical rules into one&apos;s framework. For example, Sanfilippo (1994) argues against this instrument on the grounds that there is no general control regime on lexical rules that would deterministically restrict any polysemic expansion. Instead, he advocates coding the alternative lexical forms in a hierarchy of typed feature structures, where the underspecified forms subsume the specific ones. His criticism applies to the notion of rules that are triggered automatically and proceed to derive new forms without principled limitations. Our &quot;defensive&quot; approach of listing applicable rules in the lexical entries avoids this problem but at the same time raises the qu"
J98-3003,W94-0318,0,0.0138111,"ve emphasized that lexical choice should be seen as a constraint satisfaction process, similar to Reiter (1991), who focused his attention on nouns, while we have concentrated on verbs. There are several other generators using Penman as a front-end. For example, the DRAFTER system (Paris et al. 1995) builds SPLs and hands them over to Penman; contrary to MoosE, however, the domain model in DRAFTER is subsumed by the upper model, which significantly limits the range of lexical variation, as pointed out above. Working in the framework of systemic-functional grammar (SFG), both Wanner (1992) and Teich and Bateman (1994) employ SPL as an intermediate description, but they emphasize the integration of the SPL construction process into SFG. Wanner uses system networks to make fine-grained lexical choices in line with the three systemic metafunctions. Teich and Bateman develop system networks describing genre and register variation to drive the generation process, and they query an external domain model when building the SPL. In related work, Teich, Firzlaff, and Bateman (1994) present an implementation of Kunze&apos;s theory of semantic emphasis (cf. Section 4.3). From a &quot;basic semantic scheme&quot; annotated with emphas"
K16-2002,J12-2005,1,0.934737,"o an overall account of discourse structure and of having annotation decisions concentrate on the individual instances of discourse relations, rather than on their interactions. Previous work on this task has usually broken it down into a set of sub-problems, which are solved in a pipeline architecture (roughly: identify connectives, then arguments, then discourse senses; Lin et al., 2014). While adopting a similar pipeline approach, the OPT discourse parser also builds on and extends a method that has previously achieved state-of-the-art results for the detection of speculation and negation (Velldal et al., 2012; Read 3 Relation Identification Explicit Connectives Our classifier for detecting explicit discourse connectives extends the work by Velldal et al. (2012) for identifying expressions of speculation and negation. The approach treats the set of connectives observed in the training data as a closed class, and ‘only’ attempts to disambiguate occurrences of these token sequences in new data. Connectives can be single- or multitoken sequences (e.g. ‘as’ vs. ‘as long as’). In cases 20 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 20–26, c Berlin, Germ"
K16-2002,K15-2002,0,0.285154,"r sense classifier described has been developed specifically for OPT. The OPT submission to the Shared Task of the 2016 Conference on Natural Language Learning (CoNLL) implements a ‘classic’ pipeline architecture, combining binary classification of (candidate) explicit connectives, heuristic rules for non-explicit discourse relations, ranking and ‘editing’ of syntactic constituents for argument identification, and an ensemble of classifiers to assign discourse senses. With an end-toend performance of 27.77 F1 on the English ‘blind’ test data, our system advances the previous state of the art (Wang & Lan, 2015) by close to four F1 points, with particularly good results for the argument identification sub-tasks. 1 2 System Architecture Our system overview is shown in Figure 1. The individual modules interface through JSON files which resemble the desired output files of the Task. Each module adds the information specified for it. We will describe them here in thematic blocks, while the exact order of the modules can be seen in the figure. Relation identification (§3) includes the detection of explicit discourse connectives and the stipulation of non-explicit relations. Our argument identification mod"
K16-2002,K15-2001,0,0.136259,"21.6 48.1 27.8 F1 91.8 52.4 75.2 44.0 34.5 64.6 76.4 52.0 21.9 48.2 27.8 Table 3: Per-component breakdown of system performance, compared to top performers in 2015/16. 6 optimizing the primal objective and setting the error penalty term C to 0.3. Experimental Results Overall Results Table 3 summarizes OPT system performance in terms of the metrics computed by the official scorer for the Shared Task, against both the WSJ and ‘blind’ test sets. To compare against the previous state of the art, we include results for the top-performing systems from the 2015 and 2016 competitions (as reported by Xue et al., 2015, and Xue et al., 2016, respectively). Where applicable, best results (when comparing F1 ) are highlighted for each sub-task and -metric. The highlighting makes it evident that the OPT system is competitive to the state of the art across the board, but particularly so on the argument identification sub-task and on the ‘blind’ test data: In terms of the WSJ test data, OPT would have ranked second in the 2015 competition, but on the ‘blind’ data it outperforms the previous state of the art on all but one metric for which contrastive results are provided by Xue et al.. Where earlier systems tend"
K16-2002,P09-2004,0,0.792028,"ve heads only, these are the unit of disambiguation in OPT. Disambiguation is performed as point-wise (‘per-connective’) classification using the support vector machine implementation of the SVMlight toolkit (Joachims, 1999). Tuning of feature configurations and the error-to-margin cost parameter (C) was performed by ten-fold cross validation on the Task training set. candidate configurations against the development data. The model used in the system submission includes n-grams of up to three preceding and following positions, full feature conjunction for the ‘self’ and ‘parent’ categories of Pitler & Nenkova (2009), but limited conjunctions involving their ‘left’ and ‘right’ sibling categories, and none of the ‘connected context’ features suggested by Wang & Lan (2015). This model has some 1.2 million feature types. Non-Explicit Relations According to the PDTB guidelines, non-explicit relations must be stipulated between each pair of sentences iff four conditions hold: two sentences (a) are adjacent; (b) are located in the same paragraph; and (c) are not yet ‘connected’ by an explicit connective; and (d) a coherence relation can be inferred or an entity-based relation holds between them. We proceed stra"
K16-2002,prasad-etal-2008-penn,0,\N,Missing
K16-2002,K15-2003,0,\N,Missing
K16-2002,S12-1041,1,\N,Missing
K16-2002,K16-2001,0,\N,Missing
K19-1072,I11-1120,0,0.549773,"n, each time step is transformed independently using one dense layer for transformation and one dense layer for the final prediction. Relations Figure 2: Baseline architecture proposed by Lin et al. (2014) that has been used throughout the experiments. Section 3.1. The window model for predicting just arguments is explained in Section 3.2, and its extension to also handle connectives in Section 3.3. Thereafter, we turn to more training and evaluation details in Section 4. 3.1 of None, Arg1, Arg2, or Conn1 , as in the work by Hooda and Kosseim (2017) and similar to the window-based approach of Ghosh et al. (2011a). Each word in the input sequence is embedded into lower-dimensional space. Because of the small size of the PDTB corpus, we use pretrained word embeddings, and these are further processed with a bidirectional Long Short-Term Memory network (BiLSTM). LSTMs have shown better performance compared to simple recurrent neural networks due to their higher capacity for storing important information over longer distances. Further improvements are gained through the bidirectional processing of sequential information (Graves and Schmidhuber, 2005). We keep hidden states for each time step and propagat"
K19-1072,prasad-etal-2008-penn,0,0.530758,"Missing"
K19-1072,K16-2012,0,0.46014,"Missing"
K19-1072,K16-2004,0,0.454553,"Missing"
K19-1072,D14-1008,0,0.0434673,"Missing"
K19-1072,K15-2014,0,0.678849,"erved as a model for the vast majority of follow-up work. Recently, however, most work has addressed specifically the last-mentioned task of identifying the senses of implicit relations, which has been found to be by far the most challenging one. The focus of our work, in contrast, is on identifying and delimiting the arguments of relations as well as the disambiguation of connectives. Our aim is to do this without any engineering of linguistic features, so that the approach can be easily applied to new corpora and new languages. With this perspective, we follow in particular the proposals of Wang et al. (2015) and Hooda and Kosseim (2017). The first work applies a recurrent neural network on selected sentences and labels these sentences on a token-level. The second work extends this idea and uses an LSTM on a restricted form of the argument labeling task. The 1. integrating a BiLSTM model into the shallow discourse pipeline architecture, and 2. addressing the problem of jointly predicting connective and arguments with a movingwindow approach for handling overlapping relations in running text. In the following, Section 2 discusses relevant related work, and Section 3 explains our method. The experim"
K19-1072,K16-2013,0,0.0260898,"Missing"
K19-1072,K15-2001,0,0.299998,"Missing"
K19-1072,K16-2001,0,0.360116,"Missing"
K19-1072,K16-2002,1,0.914027,"Missing"
K19-1072,D14-1162,0,0.085506,"Missing"
K19-1072,P09-2004,0,0.142036,"to label each token’s position in such a span . In our work, we extend this idea to make it applicable within the full SDP setting, i.e., on running text rather than on previously extracted individual relations. As a baseline approach, we use our reimplementation of the system of Lin et al. (2014). We study different applications of our neural model and substitute corresponding components for argument extraction from the baseline pipeline: First we address extracting the arguments of connectives that are already given; this is a sensible assumption since models for connective classification (Pitler and Nenkova, 2009) work quite well. Then, we extend this approach by removing the dependency on previously identified connectives. This step is not easy, because the connectives serve to identify the number of explicit relations in a document. Because the number of relations is initially not clear when connectives are missing, we adapt a sliding window approach for decomposing the text in overlapping windows. We then develop a process of identical prediction steps (one for each possible window within a document) and one final aggregation step, which combines the individual results into the final set of predicte"
L16-1160,D15-1264,0,0.0626367,"Missing"
L16-1160,D15-1110,1,0.797482,"nnotating each connective with senses from the new PDTB 3.0 sense hierarchy. We describe our new implementation in the extended DiMLex, which will be available for research purposes. Keywords: connectives, lexicon, semantics 1. Introduction Discourse connectives are closed-class lexical items that are known to provide very useful information for tasks like discourse parsing in the style of RST (e.g., (Hernault et al., 2010)) or PDTB (e.g., (Lin et al., 2014)), relation extraction (e.g., finding causal statements in biomedical text, (van der Horn et al., 2008)), or argumentation mining (e.g., (Peldszus and Stede, 2015)), because they indicate the kind of coherence relation holding between adjacent text spans (e.g., because: causal; although: concessive). While there can be a non-trivial ambiguity problem,1 connectives are generally regarded as highly predictive cues. Having access to a list of connectives for a language, along with information on their syntactic and semantic/pragmatic behavior, is thus an important asset. In addition to the “straightforward” use in a discourse parser, connective lists can moreover help to generate training data for identifying unsignalled (‘implicit’) coherence relations, a"
L16-1160,stede-neumann-2014-potsdam,1,0.844882,"c or lexicographical resources about semantic/pragmatic information on C. Intuitive judgement on raw corpus samples: Obtain samples of corpus instances for C and annotate them with relations (using annotation guidelines and substitution tests). All these methods will by themselves lead only to partial information for our purposes, and the fourth in addition is costly. So, in order to gather information as comprehensively as possible, we used all four options in our work, restricting method 4 to a limited number of samples. The annotated corpus at our disposal is the Potsdam Commentary Corpus (Stede and Neumann, 2014), which has been independently annotated for discourse connectives (without relations) and with RST discourse trees. We extracted each instance of a connective C and automatically associated it with the RST relation that corresponds to C in the text.3 This way, we extracted between one and 19 attested possible relations for 126 connective types that occur in the PCC. About half of the connectives in DiMLex never occur in the PCC, due to the small size of the corpus and the infrequency of many connectives. For an existing lexicon, we used LexConn (Roze et al., 2012), a large lexicon of French c"
L16-1160,P98-2202,1,0.522866,"y (semantically and pragmatically) resemble their sentential origins. The lexicon now contains 275 German connectives that adhere to this definition and are in current use. Having compared the list to that of the extensive work of (Pasch et al., 2003) and used it for text annotation, we confirmed that our list is by and large complete. Current and future work addresses the structure and content of the individual entries. Currently, each entry in the publicly-available version of DiMLex2 specifies: A Lexicon of Discourse Connectives Our work extends DiMLex, the German Discourse Marker Lexicon (Stede and Umbach, 1998; Stede, 2002). We have now partially restructured the lexicon and added almost 100 new connectives. Our underlying definition of discourse connectives in German is based on (Pasch et al., 2003, p. 331): (1) Def.: A discourse connective is a lexical item x that exhibits each of the following five properties: • possible orthographic variants, • ambiguity information (whether the lexical item also has non-connective readings), • examples of non-connective readings, • information on focus particles or correlates that can be associated with the connective, and • syntactic category of the connectiv"
L16-1160,W08-0624,0,0.0297752,"Missing"
L16-1160,P13-4001,0,0.0300166,"Missing"
L16-1167,D15-1109,1,0.887071,"Missing"
L16-1167,D13-1158,0,0.023627,"os et al., 2015; Perret et al., 2016, for example) simplify the underlying structures by a head replacement strategy (HR) that removes nodes representing CDUs from the original hypergraphs and replacing any incoming or outgoing edges on these nodes on the heads of those CDUs, forming thus dependency structures and not hypergraphs. We adapted this strategy as well for the purposes of this paper. An example transformation is provided in Figure 5. The result of the transformation for the example text is shown in Figure 6b. In the case of RST we follow the procedure that was initially proposed by Hirao et al. (2013) and later followed by Li et al. (2014). The first step in this approach includes binarizing the RST trees. In other words we transform all multi-nuclear relations into nested binary relations with the left-most EDU being the head. Dependencies go from nucleus to satellite. For illustration, a dependency structure 1054 1 reason 1 reason Elab. π1 concession Elab. 1 3 e-elab. Elab. 2 4 C. =⇒ e-elab. 2 π2 5 C. 6 2 3 3 4 5 (a) RST Elab. 4 joint C. background 5 C. elaboration 6 1 contrast 2 comment 3 4 5 (b) SDRT Figure 5: An example of SDRT dependency graph transformation support rebut for the RST"
L16-1167,P14-1003,0,0.0399048,"example) simplify the underlying structures by a head replacement strategy (HR) that removes nodes representing CDUs from the original hypergraphs and replacing any incoming or outgoing edges on these nodes on the heads of those CDUs, forming thus dependency structures and not hypergraphs. We adapted this strategy as well for the purposes of this paper. An example transformation is provided in Figure 5. The result of the transformation for the example text is shown in Figure 6b. In the case of RST we follow the procedure that was initially proposed by Hirao et al. (2013) and later followed by Li et al. (2014). The first step in this approach includes binarizing the RST trees. In other words we transform all multi-nuclear relations into nested binary relations with the left-most EDU being the head. Dependencies go from nucleus to satellite. For illustration, a dependency structure 1054 1 reason 1 reason Elab. π1 concession Elab. 1 3 e-elab. Elab. 2 4 C. =⇒ e-elab. 2 π2 5 C. 6 2 3 3 4 5 (a) RST Elab. 4 joint C. background 5 C. elaboration 6 1 contrast 2 comment 3 4 5 (b) SDRT Figure 5: An example of SDRT dependency graph transformation support rebut for the RST tree of Figure 3 is shown in Figure 6a"
L16-1167,C12-1115,1,0.908495,"Missing"
L16-1167,D15-1110,1,0.92578,"rse units (EDUs) as used in RST and SDRT (see below). The argumentation structure scheme then distinguishes between simple support (one ADU provides a justification of another) and linked support, where several ADUs collectively fulfil the role of justification. On the side of attacks, we separate rebutting (denying the validity of a statement) and undercutting (denying the relevance of a statement in supporting another). The scheme is designed in such a way that the fine-grained representations can be reduced to coarser ones that, for example, only distinguish between support and attack (see Peldszus and Stede (2015)), as it is customary in much of the related work on argumentation mining. In Figure 2, we show the representation for the sample text given in Figure 1. The nodes of this graph represent the propositions expressed in text segments (grey boxes), and their shape indicates the role in the dialectical exchange: Round nodes are proponent’s nodes, square ones are opponent’s nodes. The arcs connecting the nodes represent different supporting (arrow-head links) and attacking moves (circle/square-head links). By means of recursive application of relations, representations of relatively complex texts c"
L16-1167,W14-2112,1,0.829844,"ternative medical treatments. Not all practices and approaches that are lumped together under this term may have been proven in clinical trials, yet it’s precisely their positive effect when accompanying conventional ’western’ medical therapies that’s been demonstrated as beneficial. Besides many general practitioners offer such counselling and treatments in parallel anyway - and who would want to question their broad expertise? by two experts, and they apply equally to the English translation. The guidelines are specified in Stede (2016). They have been shown to yield reliable agreement, see Peldszus (2014). The annotated corpus contains 576 ADUs, of which 451 are proponent and 125 opponent ones. The most frequent relation is S UPPORT (263), followed by R EBUT (108), U N DERCUT (63). L INKED relations (21) and support by E X AMPLE (9) occure only rarely. [e1] Health insurance companies should naturally cover alternative medical treatments. Figure 1: Sample text from Microtext Corpus 3. Argumentation structure The initial release of the corpus already incorporated argumentation structures for all texts, following the scheme devised in Peldszus and Stede (2013), which itself is based on Freeman’s"
L16-1167,N16-1013,1,0.877768,"Missing"
L16-1167,prasad-etal-2008-penn,0,0.0854361,"s of discourse structure, and between discourse and argumentation. We converted the three annotation formats to a common dependency tree format that enables to compare the structures, and we describe some initial findings. Keywords: Discourse structure, Argumentation, Multi-layer annotation 1. Introduction In recent years, three approaches to analyzing and representing discourse structure have resulted in various annotated corpora and in implemented discourse parsers: • The Penn Discourse Treebank (PDTB) annotates individual connectives with their coherence relations and their argument spans (Prasad et al., 2008). • Rhetorical Structure Theory (RST) predicts tree structures on the grounds of underlying coherence relations that are mostly defined in terms of speaker intentions (Mann and Thompson, 1988). • Segmented Discourse Representation Theory (SDRT) exploits graphs to model discourse structures and defines coherence relations via their semantic effects on commitments rather than relative to speaker intentions (Asher and Lascarides, 2003; Lascarides and Asher, 2009). Of these, only RST and SDRT aim at predicting a full discourse structure, and our concern in this paper is with these two theories. To"
L16-1167,stede-neumann-2014-potsdam,1,0.83797,"y with other projects, we decided to build two versions of RST trees for texts with embedded EDUs: One version ignores them, while the other splits them off and uses an artificial “Same-Unit” relation to repair the structure (cf. Carlson et al. (2003)). Figure 3: Rhetorical structure of the example text As a result of the finer segmentation, 83 ADUs not directly corresponding with an EDU have been split up, so that the final corpus contains 680 EDUs. 6. 5. RST The RST annotations have been created according to the guidelines (Stede, 2016) that were developed for the Potsdam Commentary Corpus (Stede and Neumann, 2014, in German). The relation set is quite close to the original proposal of Mann and Thompson (1988) and that of the RST website2 , but some relation definitions have been slightly modified to make the guidelines more amenable to argumentative text, as it is found in newspaper commentaries or in the short texts of the corpus we introduce here. Furthermore, the guidelines present the relation set in four different groups: primarily-semantic, primarily-pragmatic, textual, multinuclear. The assignment to ’semantic’ and ’pragmatic’ relations largely agrees with the subject-matter/presentational divi"
L16-1167,W13-4002,1,0.907618,"will see. 7.2. From Discourse Structures to Dependency Structures As pointed out above, SDRT makes use of CDUs to represent larger units of discourse. RST, on the other hand, makes use of some version of the “Nuclearity Principle” to determine what is the exact scope of a discourse relation. The presence of CDUs complicates our translation from SDRT graphs to a common dependency graph format capable of handling most RST trees and SDRT graphs (Perret et al., 2016). But most formulations of the Nuclearity Principle also hinder a structural match between RST trees and SDRT graphs, as detailed in Venant et al. (2013). The authors of this paper axiomatize both RST trees and SDRT graphs in an ecumenical fragment of monadic second order logic, so that precise translation results can be proved concerning the posited structures of the two theories. They show that if one restricts SDRT graphs to those that have just one incoming arc to each node, then one SDRT graph may correspond to several RST trees. On the other hand, the expressive capacities of SDRT outrun those of theories that require tree-like discourse structures, and Afantenos et al. (2015) have shown that we need this expressive capacity for multi-pa"
L16-1167,W01-1605,0,\N,Missing
L16-1271,J08-1001,0,0.0505635,"ural speech and text. Our particular concern here is with topic and its application to written text: The goal is, roughly speaking, to identify a linguisticallymotivated topic in every sentence. So far, our project uses only German text, but the approach will be applicable to typologically-similar languages (certainly to English). In terms of computational application, the primary relevance of topic annotation is for coreference analysis. Topics are locally-prominent entities, and therefore they concern any work revolving around centering theory (Grosz et al., 1995) or entity-based coherence (Barzilay and Lapata, 2008). This research has usually equated prominence with subjecthood, which works to a large extent for English, but not so well for other languages; see, e.g., (Strube and Hahn, 1996). – Other than that, there is a potential connection between sentence topics (in the IS sense) and the (more intuitive) topics of larger stretches of text or of complete documents. Thus, research on breaking down texts into a 1718 sequence (or hierachy) of topic-homogeneous units, which had started with (Hearst, 1994), could benefit from a thorough analyses of topics on the sentence level. In the following, Section 2."
L16-1271,H91-1060,0,0.017999,"a3 , but the guidelines are neutral with respect to the tool. 4.4. Agreement study and PCC annotation 4.4.1. Agreement study We evaluated the discourse segmentation step independently of the topic annotation, which was carried out after creating a gold standard of segments. The data for the IAA study consisted of 10 texts, which amount to 138 discourse segments (in the gold standard). In our evaluation, the segment annotations were produced by two researchers, one being one of the authors of this paper, and the other a graduate student trained with the guidelines. Using the parseeval measure (Black et al., 1991), we achieved a labeled F1 score of 89.13 for the hierarchical segmentation with assignment of syntactic types. More details, including a discussion of various other agreement measures, can be found in (Sidarenka et al., 2015). In the subsequent topic assignment study, two annotators worked on the gold-segmented 10 texts (again one author of this paper and a trained graduate student). There are 3 1721 http://www.exmaralda.org different ways to calculate agreement for this type of annotation. To facilitate comparison with the related work of (Cook and Bildhauer, 2013), mentioned at the end of S"
L16-1271,C00-1021,0,0.220921,"Missing"
L16-1271,P94-1002,0,0.659592,"y work revolving around centering theory (Grosz et al., 1995) or entity-based coherence (Barzilay and Lapata, 2008). This research has usually equated prominence with subjecthood, which works to a large extent for English, but not so well for other languages; see, e.g., (Strube and Hahn, 1996). – Other than that, there is a potential connection between sentence topics (in the IS sense) and the (more intuitive) topics of larger stretches of text or of complete documents. Thus, research on breaking down texts into a 1718 sequence (or hierachy) of topic-homogeneous units, which had started with (Hearst, 1994), could benefit from a thorough analyses of topics on the sentence level. In the following, Section 2. discusses related work on topic annotation in written language. Then, Section 3. provides some background on the data we used for our annotation project, and Section 4. explains our annotation guidelines and the result of an IAA study. Section 5. concludes and sketches directions for future work. 2. Related work Among the three subfields of IS mentioned at the beginning of the paper, information status has received most attention in computational linguistics. Also, the spoken dialogue communi"
L16-1271,prasad-etal-2008-penn,0,0.0685725,"topic or was topic-less (thetic; see Section 4.2.), the agreement was only κ=0.225. Cook and Bildhauer provided a very informative error analysis of problematic cases. 3. The Potsdam Commentary Corpus 2.0 (Stede and Neumann, 2014) is a collection of 175 newspaper commentaries from a regional German newspaper. It was deliberately collected as an “unbalanced” single-genre corpus, so that research questions on subjectivity and argumentation can be addressed. The PCC has been annotated with sentential syntax, nominal coreference, discourse connectives in the spirit of the Penn Discourse Treebank (Prasad et al., 2008), and rhetorical structure in terms of RST (Mann and Thompson, 1988). These annotations and the primary data have been made publicly available. Various other annotation layers have been developed for parts of the corpus but are not yet released; all annotation guidelines are published in (Stede, 2016). The layers are being annotated largely independently (i.e., directly on the source text), the exceptions being that (i) a layer of nominal referring expressions is the basis for both coreference and topic annotation, and (ii) a layer of discourse segments is the basis for rhetorical structure, a"
L16-1271,ritz-etal-2008-annotation,0,0.027472,"Missing"
L16-1271,stede-neumann-2014-potsdam,1,0.85688,"esented here, the annotator agreement for topic was κ=0.44. More recently, (Cook and Bildhauer, 2013) used a revised version of the G¨otze et al. guidelines and conducted a new study on newspaper texts. Four annotators labeled 56 sentences and achieved a Fleiss-κ of 0.447 on topic assignment – essentially the same value as that of the Ritz et al. study. For deciding whether a sentence had a topic or was topic-less (thetic; see Section 4.2.), the agreement was only κ=0.225. Cook and Bildhauer provided a very informative error analysis of problematic cases. 3. The Potsdam Commentary Corpus 2.0 (Stede and Neumann, 2014) is a collection of 175 newspaper commentaries from a regional German newspaper. It was deliberately collected as an “unbalanced” single-genre corpus, so that research questions on subjectivity and argumentation can be addressed. The PCC has been annotated with sentential syntax, nominal coreference, discourse connectives in the spirit of the Penn Discourse Treebank (Prasad et al., 2008), and rhetorical structure in terms of RST (Mann and Thompson, 1988). These annotations and the primary data have been made publicly available. Various other annotation layers have been developed for parts of t"
L16-1271,P96-1036,0,0.180566,"y sentence. So far, our project uses only German text, but the approach will be applicable to typologically-similar languages (certainly to English). In terms of computational application, the primary relevance of topic annotation is for coreference analysis. Topics are locally-prominent entities, and therefore they concern any work revolving around centering theory (Grosz et al., 1995) or entity-based coherence (Barzilay and Lapata, 2008). This research has usually equated prominence with subjecthood, which works to a large extent for English, but not so well for other languages; see, e.g., (Strube and Hahn, 1996). – Other than that, there is a potential connection between sentence topics (in the IS sense) and the (more intuitive) topics of larger stretches of text or of complete documents. Thus, research on breaking down texts into a 1718 sequence (or hierachy) of topic-homogeneous units, which had started with (Hearst, 1994), could benefit from a thorough analyses of topics on the sentence level. In the following, Section 2. discusses related work on topic annotation in written language. Then, Section 3. provides some background on the data we used for our annotation project, and Section 4. explains"
L16-1271,J86-3001,0,\N,Missing
L18-1258,W16-2803,0,0.0783999,"akers and later professionally translated into English. The argument structure was annotated according to the scheme proposed by Peldszus and Stede (2013), which builds on the ideas of Freeman (2011). Briefly, the texts are segmented into argumentative units, each unit has an argumentative role and is related to another unit, except for the single main claim (resulting overall in a tree structure). Furthermore, the corpus was enriched with discourse structure information based on 1629 – if the effect is the case, the cause is probably the case RST and SDRT theories by Stede et al. (2016), and Becker et al. (2016) have provided the additional layer of situation entity types (Becker et al., 2016). Annotating the argument schemes, covering the underlying inferential moves, will provide a valuable annotation layer for studying the mechanics of argumentation from a theoretical, yet empirically grounded perspective, and for argumentation mining. 2.2. – if a quality characterizes the cause, then such quality characterizes the effect too – if the realization of the goal necessitates the means x, x must be adopted – if an action does not allow to achieve the goal, it should not be undertaken Annotation guideli"
L18-1258,P11-1099,0,0.0258086,"d corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an annotation project that adds information about inferential rules, in the shape of argument schemes, to an existing corpus that already holds annotations of argumentation structure as well as discourse structure based on Rhetorical Structure Theory (RST) and Segmented Discourse Representation Theory(SDRT) (Stede et al., 2016). Our emphasis thus is on a multi-layer resource that allows for correlating different lev"
L18-1258,W14-2106,1,0.881402,"mpasses the following steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this"
L18-1258,W17-5109,0,0.0169349,"s and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an annotation project that adds information about inferential rules, in the shape of argument schemes, to an existing corpus that already holds annotations of argumentation structure as well as discourse structure based on Rhetorical Structure Theory (RST) and Segmented Discourse Representation Theory(SDRT) (Stede et al., 2016). Our emphasis thus is on a multi-layer resource that allows for correlating different levels and for studying dependencies between discourse relations and argument str"
L18-1258,J17-1004,0,0.101946,"schemes both for support and attack relations, and a new user-friendly annotation tool. The multi-layer annotated corpus allows us to conduct an initial study of dependencies between discourse relations (according to Rhetorical Structure Theory (Mann and Thompson, 1988)) and argument schemes. Our main contribution is that of offering the first resource for the combined study of (argumentative) discourse relations and inferential moves. Keywords: argumentation mining, argument schemes, discourse relations 1. Introduction Recent interest in Argumentation Mining (e.g., (Lippi and Torroni, 2016; Habernal and Gurevych, 2017) has brought to the fore the need for corpora annotated with argument information, which can be used as training data. Generally, the automatic search for arguments encompasses the following steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far pr"
L18-1258,W15-0501,0,0.0196503,"earch for arguments encompasses the following steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-avai"
L18-1258,W16-2810,1,0.894866,", it should not be terminated The annotation consists of two subtasks: 1) given a SUPPORT or REBUT relation, identify the argument scheme among the 8 middle level schemes (DEFINI TIONAL , CAUSAL , MEREOLOGICAL , ANALOGY , OPPOSI TION , PRACTICAL EVALUATION , AUTHORITY ) or NONE if no reasoning is present; and 2) identify the associated inference rule. An early pilot annotation testing the first version of guidelines had been carried out on top of 30 persuasive essays from the corpus of Stab and Gurevych (2014), obtaining a fair inter-annotator agreement with trained but non-expert annotators (Musi et al., 2016). The guidelines contain identification questions, linguistic clues and inferential rules for each argument scheme. Annotators are asked to first browse the identification yes/no questions and check whether inferential rules apply and linguistic clues are indeed present. The description of CAUSAL argument schemes contains, for example, the following information: – if something has a positive value, it should be supported/continued/promoted/maintained • Identification Question: is x a cause/effect of y or is it a means to obtain y? • Other clues: Evaluations about actions play a role as common"
L18-1258,C14-1142,0,0.118212,"g steps (Peldszus and Stede, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an an"
L18-1258,L16-1167,1,0.572851,"Argument Schemes to Discourse Relations Elena Musi§ , Tariq Alhindi§ , Manfred Stede † , Leonard Kriese † , Smaranda Muresan § , Andrea Rocci Columbia University§ , Potsdam University† , Universita della Svizzera italiana 475 Riverside Drive New York (USA) § , Karl-Liebknecht-Str. 24-25 Potsdam (Germany) † , Via Buffi 13 Lugano (Switzerland) em3202/ta2509/sm761@columbia.edu § , stede@uni-potsdam.de, leonard.kriese@hotmail.de, andrea.rocci@usi.ch Abstract We present a multi-layer annotated corpus of 112 argumentative microtexts encompassing not only argument structure and discourse relations (Stede et al., 2016), but also argument schemes — the inferential relations linking premises to claims. We propose a set of guidelines for the annotation of argument schemes both for support and attack relations, and a new user-friendly annotation tool. The multi-layer annotated corpus allows us to conduct an initial study of dependencies between discourse relations (according to Rhetorical Structure Theory (Mann and Thompson, 1988)) and argument schemes. Our main contribution is that of offering the first resource for the combined study of (argumentative) discourse relations and inferential moves. Keywords: argu"
L18-1258,walker-etal-2012-corpus,0,0.0626493,"de, 2013): 1. the segmentation of texts into argumentative discourse units(ADUs); 2. the classification of the role (e.g., claim, premise) played by each ADU; 3. the analysis of the relations linking ADUs (e.g., support, attack); and 4. the identification argument schemes, namely the implicit and explicit inferential relations within and across ADUs. Annotation efforts have so far provided corpora that focus on the first three steps, with genres ranging from persuasive essays and scientific articles to online debates (e.g., (Kirschner et al., 2015; Ghosh et al., 2014; Stab and Gurevych, 2014; Walker et al., 2012). Step 4, which builds a bridge to reasoning, has not received nearly as much attention as the others. A notable exception is the Araucaria corpus (Reed and Rowe, 2004), which provides annotations based on Walton et al. (2008a) argument schemes. This annotated corpus has led to work on automatically classifying argument schemes focusing on the five most frequent ones (Feng and Hirst, 2011). Other projects looked at restricted subsets of argument schemes (Green, 2017; Schneider et al., 2013) and have not yet led to publicly-available data. In this paper, we report on an annotation project that"
L18-1288,W17-3602,1,0.871284,"v, 2000), which explains how text passages from different topics on the same topic are related to each other. 1833 4 in Bangla, and in this respect, the present Bangla RST-DT is going to be the first data set of its kind. 4. Annotation Guidelines The success of a corpus annotation task depends much on the reliability of the guidelines to be followed in the annotation. Our annotation guidelines for the Bangla RST-DT5 are based on the guidelines used to annotate the Potsdam Commentary Corpus or PCC (Stede, 2016)6 , and are more closely related to an updated version of the PCC guidelines used in Das et al. (2017)7 . In the present project, we adopt a modified version of these guidelines for annotating texts in Bangla, because these original guidelines (although used for German and English texts) are based on RST, which is essentially a language-independent theory. those guidelines (or even develop some new segmentation strategies) to account for certain constructions in Bangla. We enumerate the most significant segmentation principles followed in our annotation below. For more information about the segmentation guidelines, see (Das, under review). 1. Clausal subjects, represented as verbal nouns or co"
L18-1288,P14-1048,0,0.0719969,"Missing"
L18-1288,D13-1090,0,0.0427374,"Missing"
L18-1288,W00-1434,0,0.262501,"Missing"
L18-1288,W04-2322,0,0.118966,"Missing"
L18-1288,W00-1009,0,0.15591,"ailable for Bangla, mainly either transcribed for speech (Das et al., 2011; Bills et al., 2016), or annotated for lemmatization, POS tags or similar phenomena (Bali et al., 2010; Chaudhury et al., 2017; Ekbal and Bandyopadhyay, 2008), or unannotated (Al Mumin et al., 2014). To our knowledge, there is no discourse-annotated text corpus available 3 2 The RST diagram is created by RSTTool (O’Donnell, 2000) which provides a graphical representation of the RST analysis of a text in the form of tree diagrams. http://www.sfu.ca/rst/ CSTNews is also annotated based on Cross-document Structure Theory (Radev, 2000), which explains how text passages from different topics on the same topic are related to each other. 1833 4 in Bangla, and in this respect, the present Bangla RST-DT is going to be the first data set of its kind. 4. Annotation Guidelines The success of a corpus annotation task depends much on the reliability of the guidelines to be followed in the annotation. Our annotation guidelines for the Bangla RST-DT5 are based on the guidelines used to annotate the Potsdam Commentary Corpus or PCC (Stede, 2016)6 , and are more closely related to an updated version of the PCC guidelines used in Das et a"
L18-1288,P09-2020,0,0.0229937,"as think, know, estimate or wonder in English). Attribution clauses are not considered to form EDUs. An RST annotation of a text comprises three steps: (1) segmenting the text into EDUs (elementary discourse units), (2) assigning the relations between EDUs and larger spans, and finally (3) building the hierarchical RST tree, comprising all the connected spans stemming from a single root node at the top of the tree. Our segmentation guidelines closely follow those used for German texts in the PCC (Stede, 2016) and for English texts in SLSeg (syntactic and lexically based discourse segmenter) (Tofiloski et al., 2009). These guidelines were also used in Das et al. (2017) for segmenting German and English texts, respectively. Both PCC and SLSeg guidelines closely adhere to the original definition of spans in RST (Mann and Thompson, 1988), which specifies that only adjunct clauses (rather than complement clauses) are considered to constitute legitimate EDUs. According to this principle, every EDU must contain a verb, either finite or non-finite. Broadly, we consider coordinated clauses (but not coordinated verb phrases), adjunct clauses and non-restrictive relative clauses to establish legitimate EDUs. In al"
L18-1288,W17-3610,0,0.0457469,"rent specialized domains (Astrophysics, Law, Mathematics, Psychology, etc.). The Basque version of the RST corpus called the RST Basque Treebank (Iruskieta et al., 2013) is annotated not only for coherence relations, but for their signals as well. For German, the Potsdam Commentary Corpus or PCC (Stede, 2016) is built over a collection of 170 newspaper commentaries. The texts in PCC are annotated for RST relations, and also for five other different layers of annotation, such as syntax, co-reference and information structure. Initiatives to develop RST corpora have also been taken for Chinese (Cao et al., 2017) and Russian (Toldova et al., 2017), and those corpora are currently under production. We chose to develop an RST corpus for Bangla. Bangla is an Indo-Aryan language spoken in India and Bangladesh, with an estimated 177 million speakers in the Indian subcontinent (leaving aside the diasporic Bangla speakers living elsewhere) (Dasgupta, 2003). While Bangla has remained a relatively well-studied language, unfortunately there are only a handful of linguistic corpora available for Bangla, mainly either transcribed for speech (Das et al., 2011; Bills et al., 2016), or annotated for lemmatization, P"
L18-1288,W11-0401,0,0.0730796,"in other languages. Taboada and Renkema develop the Discourse Relations Reference Corpus in English (Taboada and Renkema, 2008), annotating a set of 65 texts taken from the RST website3 , RST-DT (Carlson et al., 2002) and SFU Review Corpus (Taboada, 2008). In Dutch, an RST corpus is developed with annotation of discourse structure and also lexical cohesion (van der Vliet et al., 2011). For Brazilian Portuguese, RST is used to create the CSTNews corpus (Cardoso et al., 2011), which includes annotation of news texts and single/multi-document summaries4 . In Spanish, da Cunha and colleagues (da Cunha et al., 2011a; da Cunha et al., 2011b) develop the RST Spanish Discourse Treebank, which includes a collection of over 250 RST-annotated texts from different specialized domains (Astrophysics, Law, Mathematics, Psychology, etc.). The Basque version of the RST corpus called the RST Basque Treebank (Iruskieta et al., 2013) is annotated not only for coherence relations, but for their signals as well. For German, the Potsdam Commentary Corpus or PCC (Stede, 2016) is built over a collection of 170 newspaper commentaries. The texts in PCC are annotated for RST relations, and also for five other different layers"
L18-1288,R11-1101,0,0.0430512,"Missing"
L18-1288,miltsakaki-etal-2004-penn,0,0.179873,"Missing"
L18-1288,W17-3604,0,0.0201042,"physics, Law, Mathematics, Psychology, etc.). The Basque version of the RST corpus called the RST Basque Treebank (Iruskieta et al., 2013) is annotated not only for coherence relations, but for their signals as well. For German, the Potsdam Commentary Corpus or PCC (Stede, 2016) is built over a collection of 170 newspaper commentaries. The texts in PCC are annotated for RST relations, and also for five other different layers of annotation, such as syntax, co-reference and information structure. Initiatives to develop RST corpora have also been taken for Chinese (Cao et al., 2017) and Russian (Toldova et al., 2017), and those corpora are currently under production. We chose to develop an RST corpus for Bangla. Bangla is an Indo-Aryan language spoken in India and Bangladesh, with an estimated 177 million speakers in the Indian subcontinent (leaving aside the diasporic Bangla speakers living elsewhere) (Dasgupta, 2003). While Bangla has remained a relatively well-studied language, unfortunately there are only a handful of linguistic corpora available for Bangla, mainly either transcribed for speech (Das et al., 2011; Bills et al., 2016), or annotated for lemmatization, POS tags or similar phenomena (Bali"
L18-1693,Y16-3017,0,0.0196758,"ltracco et al, 2016). For French, there is LEXCONN, a large lexicon with 328 connectives, with information on their syntactic category and their discourse relation, based on SDRT (Roze et al., 2012). The DPDE is an online dictionary of Spanish DMs with 210 entries in html format. The DMs are not labelled with a rhetorical sense, but a definition is provided, together with detailed information on each connective, such as register, prosody, formulae and comparable DMs (Briz et al, 2003). Recently, the design of a Czech lexicon of DMs that exploits the Prague Dependency Treebank was presented in Mírovský et al. (2016). Lexical resources available for Portuguese deal essentially with content words and even those focusing on multi word expressions favour content expressions. However, the DPDE online does provide a Portuguese equivalent to the set of Spanish discourse particles, and an experiment in the fully automatic identification of multilingual lexica including Portuguese has been reported (Lopes et al., 2015). In this context, the LDM-PT lexicon provides a new resource for discourse studies in Portuguese. 4379 3. The acquisition of DMs The identification of DMs comes from several sources. First of all,"
L18-1693,prasad-etal-2008-penn,0,0.314833,"trictive perspective, there are differences in the set of categories included in lexicons. The question is additionally related to the acquisition method: while a lexicon that is compiled manually and is informed mainly by grammars and dictionaries will be more restrictive in terms of the categories and items listed, a lexicon (semi-) automatically derived from a discourse treebank will typically include a larger set of devices that the annotators have found to fulfil a cohesive function. Example of such cases are the Alternative Lexicalizations included in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008; Prasad et al., 2010) and the secondary connectives (and free connective phrases) in the Prague Discourse Treebank (Rysová and Rysová, 2015), that fall outside the traditional categories associated to discourse connectives. There are few lexicons of DMs currently available, although recent initiatives are reported for several languages. The German lexicon DiMLex (Stede, 2002) includes 275 connectives and provides information on orthographic variants, non-connective readings, focus particle and syntactic category. The association of discourse relations to each connective in DiMLex is described"
L18-1693,C10-2118,0,0.688748,"course Markers for Portuguese – LDM-PT Amália Mendes1, Iria del Rio1, Manfred Stede2, Felix Dombek2 1 University of Lisbon, Centre of Linguistics, Portugal 2 University of Potsdam, Germany amaliamendes@letras.ulisboa.pt, igayo@gmail.com, stede@uni-potsdam.de, felix_dombek@hotmail.com Abstract We present LDM-PT, a lexicon of discourse markers for European Portuguese, composed of 252 pairs of discourse marker/rhetorical sense. The lexicon covers conjunctions, prepositions, adverbs, adverbial phrases and alternative lexicalizations with a connective function, as in the PDTB (Prasad et al., 2008; Prasad et al., 2010). For each discourse marker in the lexicon, there is information regarding its type, category, mood and tense restrictions over the sentence it introduces, rhetorical sense, following the PDTB 3.0 sense hierarchy (Webber et al., 2016), as well as a link to an English near-synonym and a corpus example. The lexicon is compiled in a single excel spread sheet that is later converted to an XML scheme compatible with the DiMLex format (Stede, 2002). We give a detailed description of the contents and format of the lexicon, and discuss possible applications of this resource for discourse studies and d"
L18-1693,J14-4007,0,0.0205278,"nctions and prepositions, compiled during the preparatory work for the POS annotation of the Reference Corpus of Contemporary Portuguese (Généreux et al., 2012). We also automatically identify the DMs that are labelled as connectives in the Portuguese part of the TED-MDB corpus (Zeyrek et al., 2018). The TED-Multilingual Discourse Bank, or TED-MDB, is a parallel corpus of English TED talks transcripts and their translations in 5 languages (German, Russian, Polish, Portuguese and Turkish). The transcripts are manually annotated at the discourse level following the goals and principles of PDTB (Prasad et al., 2014). For each language, trained or experienced annotators go through each transcribed talk and proceed sentence by sentence, by identifying the type of relation (e.g. explicit, implicit, AltLex), the sense (using PDTB 3.0 sense hierarchy) and the arguments. The annotations are then discussed in multilingual group meetings where all TED-MDB members are physically present, to check annotation consistency. We refer to Zeyrek et al. (2018) for a detailed account of the annotation process. To populate the lexicon, we retrieve the list of explicit and implicit connectives and the alternative lexicaliza"
L18-1693,W15-2703,0,0.0140529,"connective. Again, the acquisition method informs the results: the annotators of a discourse treebank will frequently choose different rhetorical senses for a single connective according to the context and this will be reflected in a treebank-driven lexicon. In our case, many of the DMs that are included in the lexicon are acquired from our work on TED-MDB. Here, the method followed the proposal of the PDTB: when the contexts lead to infer an additional sense, the explicit DM is labelled with its prototypical sense and an implicit connective is proposed and annotated with the inferred sense (Rohde et al., 2015). One example of such annotation in the Portuguese section of the TED-MDB Treebank is provided in (1): the explicit coordinate conjunction (underlined) is labelled with the sense Expansion:Conjunction (cf. 1a) and an additional implicit DM (underlined and in parentheses) accounts for the inferred sense Contingency:Cause:Result (cf. 1b). (1) a. Estas iniciativas criam um ambiente de trabalho mais móvel e reduzem a nossa pegada imobiliária. (TED talk 1927) ‘These initiatives create a more mobile work environment and reduce our housing footprint.’ b. Estas iniciativas criam um ambiente de trabalh"
L18-1693,W15-2132,0,0.739823,"cquisition method: while a lexicon that is compiled manually and is informed mainly by grammars and dictionaries will be more restrictive in terms of the categories and items listed, a lexicon (semi-) automatically derived from a discourse treebank will typically include a larger set of devices that the annotators have found to fulfil a cohesive function. Example of such cases are the Alternative Lexicalizations included in the Penn Discourse Treebank (PDTB) (Prasad et al., 2008; Prasad et al., 2010) and the secondary connectives (and free connective phrases) in the Prague Discourse Treebank (Rysová and Rysová, 2015), that fall outside the traditional categories associated to discourse connectives. There are few lexicons of DMs currently available, although recent initiatives are reported for several languages. The German lexicon DiMLex (Stede, 2002) includes 275 connectives and provides information on orthographic variants, non-connective readings, focus particle and syntactic category. The association of discourse relations to each connective in DiMLex is described in Scheffler and Stede (2016). The Italian lexicon LiCO contains 173 connectives and follows closely the DiMLex structure (Feltracco et al,"
L18-1693,L16-1160,1,0.403068,"asad et al., 2010) and the secondary connectives (and free connective phrases) in the Prague Discourse Treebank (Rysová and Rysová, 2015), that fall outside the traditional categories associated to discourse connectives. There are few lexicons of DMs currently available, although recent initiatives are reported for several languages. The German lexicon DiMLex (Stede, 2002) includes 275 connectives and provides information on orthographic variants, non-connective readings, focus particle and syntactic category. The association of discourse relations to each connective in DiMLex is described in Scheffler and Stede (2016). The Italian lexicon LiCO contains 173 connectives and follows closely the DiMLex structure (Feltracco et al, 2016). For French, there is LEXCONN, a large lexicon with 328 connectives, with information on their syntactic category and their discourse relation, based on SDRT (Roze et al., 2012). The DPDE is an online dictionary of Spanish DMs with 210 entries in html format. The DMs are not labelled with a rhetorical sense, but a definition is provided, together with detailed information on each connective, such as register, prosody, formulae and comparable DMs (Briz et al, 2003). Recently, the"
L18-1693,W16-1704,0,0.26126,"om, stede@uni-potsdam.de, felix_dombek@hotmail.com Abstract We present LDM-PT, a lexicon of discourse markers for European Portuguese, composed of 252 pairs of discourse marker/rhetorical sense. The lexicon covers conjunctions, prepositions, adverbs, adverbial phrases and alternative lexicalizations with a connective function, as in the PDTB (Prasad et al., 2008; Prasad et al., 2010). For each discourse marker in the lexicon, there is information regarding its type, category, mood and tense restrictions over the sentence it introduces, rhetorical sense, following the PDTB 3.0 sense hierarchy (Webber et al., 2016), as well as a link to an English near-synonym and a corpus example. The lexicon is compiled in a single excel spread sheet that is later converted to an XML scheme compatible with the DiMLex format (Stede, 2002). We give a detailed description of the contents and format of the lexicon, and discuss possible applications of this resource for discourse studies and discourse processing tools for Portuguese. Keywords: Discourse markers, Lexicon, Discourse treebank 1. Introduction The Lexicon of Discourse Markers (LDM-PT) provides a set of lexical items in Portuguese that have the function of struc"
L18-1693,L18-1301,1,0.804046,"reported (Lopes et al., 2015). In this context, the LDM-PT lexicon provides a new resource for discourse studies in Portuguese. 4379 3. The acquisition of DMs The identification of DMs comes from several sources. First of all, we used a list of single and phrasal elements belonging to grammatical classes, such as conjunctions and prepositions, compiled during the preparatory work for the POS annotation of the Reference Corpus of Contemporary Portuguese (Généreux et al., 2012). We also automatically identify the DMs that are labelled as connectives in the Portuguese part of the TED-MDB corpus (Zeyrek et al., 2018). The TED-Multilingual Discourse Bank, or TED-MDB, is a parallel corpus of English TED talks transcripts and their translations in 5 languages (German, Russian, Polish, Portuguese and Turkish). The transcripts are manually annotated at the discourse level following the goals and principles of PDTB (Prasad et al., 2014). For each language, trained or experienced annotators go through each transcribed talk and proceed sentence by sentence, by identifying the type of relation (e.g. explicit, implicit, AltLex), the sense (using PDTB 3.0 sense hierarchy) and the arguments. The annotations are then"
N03-2011,P99-1047,0,0.0159081,"t:mc}) which ensures the desired level shifting. Because of the inherent relational nature of RST trees this solution is blocked. Instead we use an inheritance hierarchy like that in fig. 3 and replace rule (4) with the following one, which is underspecified w.r.t to the category feature. (5) rst({cat:macro_seg, rel:[sequence,elaboration], dp:no_dp, type:nn}) ---> rst({cat:rst_tree, role:nuc, dp:no_dp}), rst({cat:rst_tree, role:nuc, dp:no_dp}). segment rst_tree mc macro_seg Related work Similar to Marcu (2000) we assume discourse markers as indicators for rhetorical relations. But contrary to Marcu (1999) and also to Schilder (2002) we use a full-fledged discourse grammar and a standard parsing algorithm, which makes it, in our opinion, unnecessary to propose special rhetorical tree building operations, as suggested e.g. by Marcu (1999). By using the chart parsing algorithm combined with the construction of an underspecified parse forest, it can easily be shown that our method is of cubic complexity. This is a crucial property, because it is commonly assumed that the number of distinct structures that can be constructed over a sequence of n discourse units is exponential in n, (as it is for ex"
N03-2011,J00-3005,0,0.136537,"inguistics University of Potsdam P.O. Box 601553 14415 Potsdam, Germany tom@ling.unipotsdam.de Silvan Heintze Dept. of Linguistics University of Potsdam P.O. Box 601553 14415 Potsdam, Germany heintze@ling.unipotsdam.de Abstract We combine a surface based approach to discourse parsing with an explicit rhetorical grammar in order to efficiently construct an underspecified representation of possible discourse structures. 1 Introduction The task of rhetorical parsing, i.e., automatically determining discourse structure, has been shown to be relevant, inter alia, for automatic summarization (e.g., Marcu, 2000). Not surprisingly, though, the task is very difficult. Previous approaches have thus emphasized the need for heuristic or probabilistic information in the process of finding the best or most likely rhetorical tree. As an alternative, we explore the idea of strictly separating “high-confidence” information from hypothetical reasoning and of working with underspecified trees as much as possible. We create a parse forest on the basis of surface cues found in the text. This forest can then be subject to further processing. Depending on the application, such further steps can either calculate the"
P19-2010,orozco-arroyave-etal-2014-new,1,0.890275,"Missing"
P98-2202,P97-1011,0,0.0422508,"Missing"
P98-2202,W98-1414,1,0.865765,"Missing"
P98-2202,W97-0401,1,0.778065,"Missing"
P98-2202,W96-0401,0,0.0453173,"Missing"
P98-2202,J95-1002,0,\N,Missing
P98-2202,P97-1013,0,\N,Missing
sonntag-stede-2014-grapat,burchardt-etal-2006-salto,0,\N,Missing
sonntag-stede-2014-grapat,W13-2324,1,\N,Missing
sonntag-stede-2014-grapat,J05-2005,0,\N,Missing
sonntag-stede-2014-grapat,clematide-etal-2012-mlsa,0,\N,Missing
sonntag-stede-2014-grapat,E12-2021,0,\N,Missing
sonntag-stede-2014-grapat,P13-4001,0,\N,Missing
stede-neumann-2014-potsdam,W00-1434,0,\N,Missing
stede-neumann-2014-potsdam,W07-1501,0,\N,Missing
stede-neumann-2014-potsdam,W04-0213,1,\N,Missing
stede-neumann-2014-potsdam,C04-1061,1,\N,Missing
stede-neumann-2014-potsdam,prasad-etal-2008-penn,0,\N,Missing
varges-etal-2012-semscribe,W94-0319,0,\N,Missing
varges-etal-2012-semscribe,W09-0602,0,\N,Missing
varges-etal-2012-semscribe,J03-1003,0,\N,Missing
varges-etal-2012-semscribe,P98-1116,0,\N,Missing
varges-etal-2012-semscribe,C98-1112,0,\N,Missing
varges-etal-2012-semscribe,J02-1003,0,\N,Missing
varges-etal-2012-semscribe,P01-1057,0,\N,Missing
varges-etal-2012-semscribe,J04-3003,0,\N,Missing
varges-etal-2012-semscribe,P97-1026,0,\N,Missing
varges-etal-2012-semscribe,W10-4209,0,\N,Missing
W00-1413,P93-1031,0,\N,Missing
W02-1704,W98-1414,1,0.903947,"Missing"
W02-1704,C02-2027,1,0.877241,"Missing"
W02-1704,W96-0401,0,0.0721498,"Missing"
W02-1704,P97-1013,0,\N,Missing
W03-0909,P97-1013,0,0.0267154,"esent example, things happen to work out quite well, but in general, an explicit topic identification step will be needed. And finally, the rhetorical tree does not have information on illocution types (1-place rhetorical relations, so to speak) that distinguish reported facts (e.g., segments 3 and 4) from author’s opinion (e.g., segment 7). We will return to these issues in Section 6, but first consider the chances for building up rhetorical trees automatically. 5 Prospects for Rhetorical Parsing Major proponents of rhetorical parsing have been (Sumita et al., 1992), (Corston-Oliver, 1998), (Marcu, 1997), and (Schilder, 2002). All these approaches emphasise their membership in the “shallow analysis” family; they are based solely on surface cues, none tries to work with semantic / domain / world knowledge. (Corston-Oliver and Schilder use some genre-specific heuristics for preferential parsing, though.) In general, our sample text belongs to a rather “friendly” genre for rhetorical parsing, as commentaries are relatively rich in connectives, which are the most important source of information for making decisions — but not the only one: Corston-Oliver, for example, points out that certain lingu"
W03-0909,W03-2411,1,0.40611,"pletely understood. For example, a sentence adverbial might be unknown and thus the modality of the sentence not be determined. The ABox then should reflect this partiality accordingly, and allow for appropriate inferences on the different levels of representation. The notion of mixed depth is relevant not only for the tree’s leaves: Sometimes, it might not be possible to derive a unique rhetorical relation between two segments, in which case a set of candidates can be given, or none at all, or just an assignment of nucleus and satellite segments, if there are cues allowing to infer this. In (Reitter and Stede, 2003) we suggest an XML-based format for representing such underspecified rhetorical structures. Projecting this onto the terminological logic scheme, and adding the treatment of leaves, we need to provide the TBox not only with concepts representing entities of “the world” but also with those representing linguistic objects, such as clause or noun group, and for the case of unanalyzed material, string. To briefly elaborate the noun group example, consider Reiches Ministerkollegen (‘Reiche’s colleagues’) in sentence 6. Shallow analysis will identify Reiche as some proper name and thus the two words"
W03-0909,N03-2011,1,\N,Missing
W03-2411,W00-1434,0,\N,Missing
W03-2411,W01-1605,0,\N,Missing
W03-2411,J86-3001,0,\N,Missing
W03-2411,P89-1018,0,\N,Missing
W04-0213,N03-2011,1,0.852558,"Missing"
W04-0213,W03-0909,1,0.819665,"een assembled and annotated with different information (and currently, to different degrees): part-of-speech, syntax, rhetorical structure, connectives, co-reference, and information structure. The paper explains the design decisions taken in the annotations, and describes a number of applications using this corpus with its multi-layer annotation. 1 Introduction A corpus of German newspaper commentaries has been assembled at Potsdam University, and annotated with different linguistic information, to different degrees. Two aspects of the corpus have been presented in previous papers ((Reitter, Stede 2003) on underspecified rhetorical structure; (Stede 2003) on the perspective of knowledge-based summarization). This paper, however, provides a comprehensive overview of the data collection effort and its current state. At present, the ‘Potsdam Commentary Corpus’ (henceforth ‘PCC’ for short) consists of 170 commentaries from M¨ arkische Allgemeine Zeitung, a German regional daily. The choice of the genre commentary resulted from the fact that an investigation of rhetorical structure, its interaction with other aspects of discourse structure, and the prospects for its automatic derivation are the k"
W04-0213,C04-1061,1,0.873591,"Missing"
W04-0213,J03-4002,0,\N,Missing
W04-0213,W03-2411,1,\N,Missing
W04-0607,W02-1706,0,0.139049,"patial dimensions) which can be recognised easily but would require very specialised grammar rules later on.6 Then, the domain-specific lexicon is accessed, which maps “concept names” (nouns, or phrases as recognised in the previous step) to the concept IDs used in the ontology.7 Tokens for which there is no entry in that lexicon, and which are hence deemed ‘irrelevant’ for the domain, are assigned a ‘dummy’ semantics appropriate for their part of speech, so that they do not confuse the later parsing stage. (More details about this kind of robustness will be given shortly.) 6 See for example (Grover et al., 2002) for a discussion of the utility of a named entitiy recognition preprocessing stage for robust symbolic parsing. 7 Note that this lexicon is one single resource out of which also the domain specfic additions to the morphology-lexicon and the list of multi-word expressions are compiled. 3.3 Chunk Parsing Next, the analyses of the tokens are transformed into a feature structure format, and are passed to the parsing component.8 The output of this stage is an intermediate semantic representation of (aspects of) the content (of which the notation shown in 1 is a variant). This format is akin to tra"
W04-0607,W03-2106,1,0.833333,"nt means certain limitations have to be accepted. Being a fragment of FOL, it is not expressive enough to represent certain finer semantic details, as will be discussed below. However, the advantage of using an emerging standard for delivering and sharing information outweighs these drawbacks. 3 Implementation 3.1 Overview As mentioned above, most of the sentences in our corpus do not contain a finite verb; i.e., according to standard rules of grammar they are elliptical. While a theoretically motivated approach should strive to resolve this ellipsis contextually (for example as described in (Schlangen, 2003)), in view of the intended application and for reasons of robustness we have decided to focus only on extracting information about the entities introduced in the reports— that is, on recognising nominal phrases, leaving aside the question of how verbal meanings are to be resolved. Our strategy is to combine a “shallow” preprocessing stage (based on finite-state methods and statistical approaches) with a symbolic phase, in which the semantics of the NPs is assembled.5 A requirement for the processing is that it must be robust, in two ways: it must be able to deal with unknown tokens (i.e., “out"
W07-1530,W05-0305,1,0.887762,"Missing"
W07-1530,J93-2004,0,0.0279621,"Missing"
W07-1530,W04-2703,1,0.832896,"Missing"
W07-1530,W04-0212,1,0.90064,"Missing"
W07-1530,J05-2005,0,0.0850358,"Missing"
W07-1530,W04-0213,1,0.81694,"d projects/muc/. 2 The Automated Content Extraction program, www.nist.gov/speech/tests/ace/. 192 genre-specific corpus of German newspaper commentaries, taken from the daily papers M¨arkische Allgemeine Zeitung and Tagesspiegel. One central aim is to provide a tool for studying mechanisms of argumentation and how they are reflected on the linguistic surface. The corpus on the one hand is a collection of “raw” data, which is used for genreoriented statistical explorations. On the other hand, we have identified two sub-corpora that are subject to a rich multi-level annotation (MLA). The PCC176 (Stede, 2004) is a sub-corpus that is available upon request for research purposes. It consists of 176 relatively short commentaries (1215 sentences), with 33.000 tokens in total. The sentences have been PoS-tagged automatically (and manually checked); sentence syntax was annotated semi-automatically using the TIGER scheme (Brants et al., 2002) and Annotate3 tool. In addition, we annotated coreference (PoCos (Krasavina and Chiarcos, 2007)) and rhetorical structure according to RST (Mann and Thompson, 1988). Our annotation software architecture consists of a variety of standard, external tools that can be u"
W07-1530,J02-4002,1,0.73024,"a single, very complex annotation step; • end up with less ambiguity in the annotations, since the reasons for specific decisions can be made explicit (by annotations on “simpler” levels); • be more explicit than a single tree can be: if a discourse fulfills, for example, a function both for thematic development and for the writer’s intention, they can both be accounted for; • provide the central information that a “traditional” rhetorical tree conveys, without loosing essential information. 5 AZ Corpus (Simone Teufel, Cambridge) The Argumentative Zoning (AZ) annotation scheme (Teufel, 2000; Teufel and Moens, 2002) is concerned with marking argumentation steps in scientific articles. One example for an argumentation step is the description of the research goal, another an overt comparison of the authors’ work with rival approaches. In our scheme, these argumentation steps have to be associated with text spans (sentences or sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teu"
W07-1530,E99-1015,1,0.662031,"02) is concerned with marking argumentation steps in scientific articles. One example for an argumentation step is the description of the research goal, another an overt comparison of the authors’ work with rival approaches. In our scheme, these argumentation steps have to be associated with text spans (sentences or sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teufel et al., 1999) reports on interannotator agreement studies with this scheme. There is a strong interrelationship between the argumentation in a paper, and the citations writers use to support their argument. Therefore, a part of the computational linguistics corpus has a second layer of annotation, called CFC (Teufel et al., 2006) or Citation Function Classification. CFC– annotation records for each citation which rhetorical function it plays in the argument. This is following the spirit of research in citation content analysis (e.g., (Moravcsik and Murugesan, 1975)). An example for a ci193 tation function"
W07-1530,W06-1312,1,0.772883,"sequences of sentences). AZ–Annotation is the labelling of each sentence in the text with one of these labels (7 in the original scheme in (Teufel, 2000)). The AZ labels are seen as relations holding between the meanings of these spans, and the rhetorical act of the entire paper. (Teufel et al., 1999) reports on interannotator agreement studies with this scheme. There is a strong interrelationship between the argumentation in a paper, and the citations writers use to support their argument. Therefore, a part of the computational linguistics corpus has a second layer of annotation, called CFC (Teufel et al., 2006) or Citation Function Classification. CFC– annotation records for each citation which rhetorical function it plays in the argument. This is following the spirit of research in citation content analysis (e.g., (Moravcsik and Murugesan, 1975)). An example for a ci193 tation function would be “motivate that the method used is sound”. The annotation scheme contains 12 functions, clustered into “superiority”, “neutral comparison/contrast”, “praise or usage” and “neutral”. One type of research we hope to do in the future is to study the relationship between these rhetorical phonemena with more tradi"
W08-2218,P94-1002,0,0.114993,"nces of minimal units from layer (5). Given a reliable syntactic analysis, scope determination is usually straightforward for coordinating and subordinating conjunctions. For adverbials, we hypothesize different solutions and rank them according to size: The most narrow interpretation is taken as most likely. In this step, we also consider the layer of logical document structure in order to avoid segments that would stretch across paragraphs or other kinds of boundaries. Similarly, a layer with the results of “text tiling” (breakdown of the text in terms of thematic units, in the tradition of Hearst (1994)) could be used for this purpose, as well as as an ‘attribution’ layer that identifies those modal contexts that attribute a span of text to a particular source (as in indirect speech). In this way, the module will generate hypotheses of coherence relations and related spans, for the time being solely on the basis of connectives occurring in the text. As explained, this information is represented in two additional analysis layers. Modules following in the processing chain may combine the various hypotheses into the most likely overall relational tree structure for the paragraph (or a set of su"
W08-2218,W04-2322,0,0.0254644,"Missing"
W08-2218,W03-2411,1,0.796994,"purpose, as well as as an ‘attribution’ layer that identifies those modal contexts that attribute a span of text to a particular source (as in indirect speech). In this way, the module will generate hypotheses of coherence relations and related spans, for the time being solely on the basis of connectives occurring in the text. As explained, this information is represented in two additional analysis layers. Modules following in the processing chain may combine the various hypotheses into the most likely overall relational tree structure for the paragraph (or a set of such tree structures, see Reitter and Stede (2003)), or they may use the hypotheses directly for some application that does not rely on a spanning tree. 7 Discussion The central idea behind the separation of the declarative DIMLEX resource and the (ongoing) implementation of an analysis procedure is to facilitate a smooth extensibility of the overall approach towards further kinds of connectives and coherence relations. When the lexicon is extended — while the underlying scheme remains unchanged — coverage of the analyzer grows without adaptations to the analysis procedure. An important benefit of the XML-based organization of the lexicon is"
W08-2218,J03-4002,0,0.0241894,"easons / for all these very good reasons / .... For German, we have dealt with the issue of differentiating between types of multitoken connectives in a separate paper Stede and Irsig (2008). Discourse structure. As is well-known, the structural description of a text can also be more complicated than in our “well-formed XML” example shown at the beginning. For one thing, discourse units can be embedded into one another, using parenthetical material or appositions. Further, connectives can occasionally link text segments that are non-adjacent — a phenomenon that has been studied intensively by Webber et al. (2003) and also by Wolf and Gibson (2005). An example from Webber et al.: John loves Barolo. So he ordered three cases of the ’97. But he had to cancel the order because then he discovered he was broke. Here, the then is to be understood as linking the discovery event back to the ordering event rather than to the (adjacent) canceling. In German, many adverbial connectives have an overt anaphoric affix (e.g., deswegen, daher, trotzdem), and the ability to link non-adjacent segments appears to be restricted to these. Non-adjacency also leads to the issue of crossing dependencies, which is also discuss"
W08-2218,J05-2005,0,0.0237235,"reasons / .... For German, we have dealt with the issue of differentiating between types of multitoken connectives in a separate paper Stede and Irsig (2008). Discourse structure. As is well-known, the structural description of a text can also be more complicated than in our “well-formed XML” example shown at the beginning. For one thing, discourse units can be embedded into one another, using parenthetical material or appositions. Further, connectives can occasionally link text segments that are non-adjacent — a phenomenon that has been studied intensively by Webber et al. (2003) and also by Wolf and Gibson (2005). An example from Webber et al.: John loves Barolo. So he ordered three cases of the ’97. But he had to cancel the order because then he discovered he was broke. Here, the then is to be understood as linking the discovery event back to the ordering event rather than to the (adjacent) canceling. In German, many adverbial connectives have an overt anaphoric affix (e.g., deswegen, daher, trotzdem), and the ability to link non-adjacent segments appears to be restricted to these. Non-adjacency also leads to the issue of crossing dependencies, which is also discussed by the two teams of authors ment"
W08-2218,J93-3003,0,\N,Missing
W09-3005,A00-1031,0,0.0794563,"earch interface. Integrative representation. All annotations that are consistent with the merged tokenization should refer to the merged tokenization. This is necessary in order to query across multiple annotations orig35 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP inating from different annotation layers or tools. Unsupervised merging. The integration of conflicting tokenizations should not require manual interference. 1.2 (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the term ‘word’. For a shallow morphosyntactic analysis (part of speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store b. Herzog-von der Heide1 c. Red Cross/Red Cre"
W09-3005,W06-2709,0,0.0505607,"Missing"
W09-3005,C00-2157,0,0.690933,"Missing"
W09-3005,J93-2004,0,0.0357686,"lating queries for words, e.g. in a corpus search interface. Integrative representation. All annotations that are consistent with the merged tokenization should refer to the merged tokenization. This is necessary in order to query across multiple annotations orig35 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP inating from different annotation layers or tools. Unsupervised merging. The integration of conflicting tokenizations should not require manual interference. 1.2 (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the term ‘word’. For a shallow morphosyntactic analysis (part of speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store b. Herzo"
W09-3005,J97-4004,0,0.0394198,"to the merged tokenization. This is necessary in order to query across multiple annotations orig35 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 35–43, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP inating from different annotation layers or tools. Unsupervised merging. The integration of conflicting tokenizations should not require manual interference. 1.2 (4) doesn’t a. [does][n’t] (Marcus et al., 1993, PTB) b. [doesn][’][t] (Brants, 2000, TnT) Tokenization Tokenization is the process of mapping sequences of characters to sequences of words (cf. Guo 1997). However, different research questions or applications induce different conceptions of the term ‘word’. For a shallow morphosyntactic analysis (part of speech tagging), a ‘simple’ tokenization using whitespaces and punctation symbols as delimiters seems acceptable for the examples in (1). A full syntactic analysis (parsing), however, could profit from the aggregation of complex nominals into one token each. (1) a. department store b. Herzog-von der Heide1 c. Red Cross/Red Crescent movement Similarly, examples (2a) and (2b) can be argued to be treated as one token for (morpho)syntactic analyse"
W09-3005,kingsbury-palmer-2002-treebank,0,\N,Missing
W09-3005,W04-2705,0,\N,Missing
W09-3005,poesio-artstein-2008-anaphoric,0,\N,Missing
W09-3005,M95-1005,0,\N,Missing
W09-3005,W07-1501,0,\N,Missing
W09-3005,W01-1605,0,\N,Missing
W09-3005,W03-1309,0,\N,Missing
W09-3005,N06-2015,0,\N,Missing
W09-3005,J95-4004,0,\N,Missing
W09-3005,J05-2005,0,\N,Missing
W09-3005,prasad-etal-2008-penn,0,\N,Missing
W09-3005,P02-1022,0,\N,Missing
W09-3909,P08-1034,0,0.133947,"Missing"
W09-3909,2007.sigdial-1.16,1,0.921162,"the three raters). The full kappa values for all the distinctions are provided in Appendix B. After the reliability test, one of the authors performed the full annotation for all 100 texts. Table 1 shows the breakdown of high-level stages for the 100 texts. Stages in movie reviews Within the larger review genre, we focus on movie reviews. Movie reviews are particularly difficult to classify (Turney, 2002), because large portions of the review contain description of the plot, the characters, actors, director, etc., or background information about the film. Our approach is based on the work of Bieler et al. (2007), who identify formal and functional zones (stages) within German movie reviews. Formal zones are parts of the text that contribute factual information about the cast and the credits, and also about the review itself (author, date of publication and the reviewer’s rating of the movie). Functional zones contain the main gist of the review, and can be divided roughly into description and comment. Bieler et al. showed that functional zones could be identified using 5-gram SVM classifiers built from an annotated German corpus. 2.1 Manual annotation Taxonomy In addition to the basic Describe/Commen"
W09-3909,taboada-etal-2006-methods,1,0.884663,"Missing"
W09-3909,esuli-sebastiani-2006-sentiwordnet,0,0.0413206,"Missing"
W09-3909,P02-1053,0,0.00741137,"for the distinction between formal and functional zones (.84 for the 3-rater kappa). The lowest reliability was for the 3-way distinction in the functional zones (.68 for the first two raters, and .54 for the three raters). The full kappa values for all the distinctions are provided in Appendix B. After the reliability test, one of the authors performed the full annotation for all 100 texts. Table 1 shows the breakdown of high-level stages for the 100 texts. Stages in movie reviews Within the larger review genre, we focus on movie reviews. Movie reviews are particularly difficult to classify (Turney, 2002), because large portions of the review contain description of the plot, the characters, actors, director, etc., or background information about the film. Our approach is based on the work of Bieler et al. (2007), who identify formal and functional zones (stages) within German movie reviews. Formal zones are parts of the text that contribute factual information about the cast and the credits, and also about the review itself (author, date of publication and the reviewer’s rating of the movie). Functional zones contain the main gist of the review, and can be divided roughly into description and"
W09-3909,W03-2120,0,0.0158509,"n newspapers or on-line magazines. We restricted the texts to “Top Critics” because we wanted well-structured, polished texts, unlike those found in some on-line review sites. Future work will address those more informal reviews. The 100 reviews contain 83,275 words and 1,542 paragraphs. The annotation was performed at the paragraph level. Although stages may span across paragraphs, and paragraphs may contain more than one stage, there is a close relationship between paragraphs and stages. The restriction also resulted in a more reliable annotation, performed with the PALinkA annotation tool (Orasan, 2003). The annotation was performed by one of the authors, and we carried out reliability tests with two other annotators, one another one of the authors, who helped develop the taxonomy, and the third one a project member who read the annotation guidelines 1 , and received a few hours’ training in the labels and software. We used Fleiss’ kappa (Fleiss, 1971), which extends easily to the case of multiple raters (Di Eugenio and Glass, 2004). We all annotated four texts. The results of the reliability tests show a reasonable agreement level for the distinction between formal and functional zones (.84"
W09-3909,P04-1035,0,0.458038,"Missing"
W09-3909,W02-1011,0,0.0172322,"Missing"
W13-1611,C10-2005,0,0.110498,"Missing"
W13-1611,D11-1052,0,0.027674,"Missing"
W13-1611,P11-1016,0,0.0728055,"Missing"
W13-1611,P11-4022,0,0.0626528,"Missing"
W13-1611,pak-paroubek-2010-twitter,0,0.0614628,"Missing"
W13-1611,J11-2001,1,0.221765,"widespread use, and because Twitter communication, in response to emerging issues, is fast and especially ad hoc, making it an effective platform for the sharing and discussion of crisisrelated information (Bruns/Burgess, 2011). Furthermore, Twitter is characterized by a high topicality of content (Milstein al., 2008). Specifically, we present experiments involving two sentiment analysis systems that both employ a combination of polarity lexicon and sentiment composition rules: (i) SentiStrength (Thelwall et al., 2012), a system that is geared toward short social-media text, and (ii) SO-CAL (Taboada et al., 2011), ‘Semantic Orientation Calculator’, a generalpurpose system that was designed primarily to work on the level of complete texts. While both are lexicon-based approaches, there are certain differences in the roles of the various submodules. For our purposes here, it is important that SentiStrength was designed to cope specifically with “user-generated content”. Among the features of the system, as stated by Thelwall et al., the following four are especially important for tweets: (i) a simple spelling correction algorithm deletes repeated letters when the word is not found in the dictionary; (ii"
W13-2312,basile-etal-2012-developing,0,0.0178675,"ovides access to multiple annotation layers in the corpus. This access provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications. 1 Introduction Over the past decade, corpora with multiple layers of linguistic annotation have been developed in order to extend the range of empirically-based linguistic research and enable study of inter-layer interactions. Recently created corpora include OntoNotes (Pradhan et al., 2007), the Groningen Meaning Bank (Basile et al., 2012), and the Manually Annotated Sub-Corpus (MASC)1 (Ide et al., 2010). Typically, such corpora are represented in idiosyncratic in-house formats, and developers provide special software to access and query the annotations (for example, the OntoNotes “db tool” and Groningen’s GMB Explorer). Access without the use of developer-supplied software often requires significant programming expertise, and as a result, it is not easy–or even possible–for others to add to or modify data and annotations in the resource. This paper describes the importation of MASC data and annotations into the linguistic data"
W13-2312,W07-1501,1,0.750868,"Importing MASC into the ANNIS linguistic database: A case study of mapping GrAF Arne Neumann Nancy Ide Manfred Stede EB Cognitive Science and SFB 632 Department of Computer Science EB Cognitive Science and SFB 632 University of Potsdam Vassar College neumana@uni-potsdam.de ide@cs.vassar.edu ANNIS2 (Chiarcos et al., 2008; Zeldes et al., 2009), which was designed to visualize and query linguistically-annotated corpora. Unlike most other corpora with multi-layer annotations, no special software has been developed for access to MASC. Instead, all MASC data and annotations are represented in GrAF (Ide and Suderman, 2007), the XML serialization of the abstract model for annotations defined by ISO TC37 SC4’s Linguistic Annotation Framework (ISO/LAF) (Ide and Suderman, In press). GrAF is intended to serve as a generic “pivot” format that is isomorphic to annotation schemes conforming to the abstract model and therefore readily mappable to schemes used in available systems. We outline the process of mapping GrAF to ANNIS’s internal format relANNIS and demonstrate how the system provides access to multiple annotation layers in MASC. Abstract This paper describes the importation of Manually Annotated Sub-Corpus (MA"
W13-2312,P10-2013,1,0.889118,"s provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications. 1 Introduction Over the past decade, corpora with multiple layers of linguistic annotation have been developed in order to extend the range of empirically-based linguistic research and enable study of inter-layer interactions. Recently created corpora include OntoNotes (Pradhan et al., 2007), the Groningen Meaning Bank (Basile et al., 2012), and the Manually Annotated Sub-Corpus (MASC)1 (Ide et al., 2010). Typically, such corpora are represented in idiosyncratic in-house formats, and developers provide special software to access and query the annotations (for example, the OntoNotes “db tool” and Groningen’s GMB Explorer). Access without the use of developer-supplied software often requires significant programming expertise, and as a result, it is not easy–or even possible–for others to add to or modify data and annotations in the resource. This paper describes the importation of MASC data and annotations into the linguistic database 1 University of Potsdam stede@uni-potsdam.de 2 The ANNIS Infr"
W13-2324,J08-4004,0,0.21408,"s α (Krippendorff, 1980) as a weighted measure of agreement. We use the distance between two tags in the tag hierarchy to weigh the confusion (similar to Geertzen and Bunt (2006)), in order to capture the intuition that confusing, e.g., PSNC with PSNS is less severe than confusing it with OAUS. According to the scale of Krippendorff (1980), 4 Notice that this hierarchy is implicit in the annotation process, yet the annotators were neither confronted with a decision-tree version nor the labels of this tag hierarchy. 5 A generalisation of Scott’s π (Scott, 1955) for more than two annotators, as Artstein and Poesio (2008) pointed out. 3 This is roughly equivalent to Freeman’s ‘linked premises’. 198 Figure 2: The hierarchy of segment labels. level role typegen type comb target role+typegen role+type role+type+comb role+type+comb+target #cats 2 3 5 2 (9) 5 9 15 (71) κ 0.521 0.579 0.469 0.458 0.490 0.541 0.450 0.392 0.384 AO 0.78 0.72 0.61 0.73 0.58 0.66 0.56 0.49 0.44 AE 0.55 0.33 0.26 0.50 0.17 0.25 0.20 0.16 0.08 α DO DE 0.534 0.500 0.469 0.425 0.28 0.33 0.38 0.45 0.60 0.67 0.71 0.79 Table 1: Agreement for all 26 annotators on 115 items for the different levels. The number of categories on each level (without"
W13-2324,W10-1806,0,0.0751677,"s conceived as finding the corresponding critical question of the challenger that is answered by a particular segment of the text. Since the focus of this paper is on the evaluation methodology, we provide here only a brief sketch of the scheme; for a detailed description with many examples, see Peldszus and Stede (to appear). Premises and conclusions are propositions expressed in the text segments. We can graphically present an argument as an argument diagram, with propositions as nodes and the various relations as arrows linking either two nodes or 1 See, for instance, Snow et al. (2008) or Bhardwaj et al. (2010) for strategies to analyse and cope with diverging performance of annotators in that scenario. 196 Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 196–204, c Sofia, Bulgaria, August 8-9, 2013. 2013 Association for Computational Linguistics mer (segment 5) denies the relevance of a stated relation, here: the support that 4 lends to 1=8. The opponent does not dispute the truth of 4 itself but challenges the idea that it can in fact lend support to 1=8. We draw it as an attack arrow pointing at the relation in question. In contrast, a rebuttal direct"
W13-2324,E12-1085,0,0.0774021,"Missing"
W13-2324,W06-1318,0,0.047072,"ombined’-level.3 The target is finally specified by the segment identifier (1 . . . 5) or relation identifier (a . . . d) on the ‘target’-level. The labels of each separate level can be merged to form a complex tagset. We interpret the result 4.2 IAA over all annotators The agreement in terms of Fleiss’s κ (Fleiss, 1971)5 of all annotators on the different levels is shown in Table 1. For the complex levels we additionally report Krippendorff’s α (Krippendorff, 1980) as a weighted measure of agreement. We use the distance between two tags in the tag hierarchy to weigh the confusion (similar to Geertzen and Bunt (2006)), in order to capture the intuition that confusing, e.g., PSNC with PSNS is less severe than confusing it with OAUS. According to the scale of Krippendorff (1980), 4 Notice that this hierarchy is implicit in the annotation process, yet the annotators were neither confronted with a decision-tree version nor the labels of this tag hierarchy. 5 A generalisation of Scott’s π (Scott, 1955) for more than two annotators, as Artstein and Poesio (2008) pointed out. 3 This is roughly equivalent to Freeman’s ‘linked premises’. 198 Figure 2: The hierarchy of segment labels. level role typegen type comb t"
W13-2324,D08-1027,0,0.236924,"Missing"
W13-2324,W01-1605,0,\N,Missing
W13-2708,C10-1011,0,0.0287056,"d a system to identify direct and indirect speech and a sentiment system to determine the orientation of the statement. These systems in turn need various kinds of preprocessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide range of possible features — including high level information like sentiment, text type,"
W13-2708,J96-2004,0,0.103544,"ists as the separation of metadata and 6 Conclusion and Outlook We developed and implemented a pipeline of various text processing tools which is designed to assist political scientists in finding specific, complex concepts within large amounts of text. Our case studies showed that our approach can provide beneficial assistance for the research of political scientists as well as researcher from other social sciences and the humanities. A future aspect will be to find metrics to evaluate our pipeline. In recently started annotation experiments on topic classification Cohen’s kappa coefficient (Carletta, 1996) is mediocre. It may very well be possible that the complex concepts, like multiple collective identities, are intrinsically hard to detect, and the annotations cannot be improved substantially. The extension of the NLP pipeline will be another major working area in the future. Examples are sentiment analysis for German, adding world knowledge about named entities (e.g. persons and events), identification of relations between entities. Finally, all these systems need to be evaluated not only in terms of f-score, precision and recall, but also in terms of usability for the political scientists."
W13-2708,C08-1098,0,0.0274623,"tician who tries to rally support for his political party. In order to detect such text, we need a system to identify direct and indirect speech and a sentiment system to determine the orientation of the statement. These systems in turn need various kinds of preprocessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide ra"
W13-2708,N12-1066,0,0.0162412,"pic filter Concept detection Web-based Userinterface Data cleaning is important for the data-driven studies. Not only duplicate articles have a negative impact, also articles which are not of interest for the given topic have to be filtered out. There are different approaches to classify articles into a range of predefined topics. In the last years LDA (Blei et al., 2003; Niekler and J¨ahnichen, 2012) is one of the most successful methods to find topics in articles. But for social scientists the categories typically used in LDA are not sufficient. We follow the idea of Dualist (Settles, 2011; Settles and Zhu, 2012) which is an interactive method for classification. The architecture of Dualist is based on MALLET (McCallum, 2002) which is easily integrable into our architecture. Our goal is to design the correct feature to find relevant articles for a given topic. Word features are not sufficient since we have to model more complex features (cf. Section 2.1). Figure 2: Overview of the complete processing chain. We split the workflow for the user into two parts: The first part is only used if the user imports new data into the repository. For that he can use the exploration workbench (Section 3.1). Secondl"
W13-2708,P10-4005,0,0.0309161,"ation is gathered and visualised as well. Major differences between the EMM and our approach are the user group and the domain of the corpus. The complex concepts political scientists are interested in are much more nuanced than the concepts relevant for topic detection and the construction of social networks. Additionally, the EMM does not allow its users to look for their own concepts and issues, while this interactivity is a central contribution of our approach (cf. Sections 1, 2.1 and 3.2). The CLARIN-D project also provides a webbased platform to create NLP-chains. It is called WebLicht (Hinrichs et al., 2010), but in its current form, the tool is not immediately usable for social scientists as the separation of metadata and 6 Conclusion and Outlook We developed and implemented a pipeline of various text processing tools which is designed to assist political scientists in finding specific, complex concepts within large amounts of text. Our case studies showed that our approach can provide beneficial assistance for the research of political scientists as well as researcher from other social sciences and the humanities. A future aspect will be to find metrics to evaluate our pipeline. In recently sta"
W13-2708,D11-1136,0,0.0485983,"cept Builder Topic filter Concept detection Web-based Userinterface Data cleaning is important for the data-driven studies. Not only duplicate articles have a negative impact, also articles which are not of interest for the given topic have to be filtered out. There are different approaches to classify articles into a range of predefined topics. In the last years LDA (Blei et al., 2003; Niekler and J¨ahnichen, 2012) is one of the most successful methods to find topics in articles. But for social scientists the categories typically used in LDA are not sufficient. We follow the idea of Dualist (Settles, 2011; Settles and Zhu, 2012) which is an interactive method for classification. The architecture of Dualist is based on MALLET (McCallum, 2002) which is easily integrable into our architecture. Our goal is to design the correct feature to find relevant articles for a given topic. Word features are not sufficient since we have to model more complex features (cf. Section 2.1). Figure 2: Overview of the complete processing chain. We split the workflow for the user into two parts: The first part is only used if the user imports new data into the repository. For that he can use the exploration workbenc"
W13-2708,W04-0213,1,0.689214,"ally by the political scientists. The following are further applications of the identified indirect speeches a) using the frequency of speeches per text as a feature for classification; e.g. a classification system for news reports/commentaries as described in Section 4.4 b) a project-goal is to find texts in which collective A useful distinction for political scientists dealing with newspaper articles is the distinction between articles that report objectively on events or backgrounds and editorials or press commentaries. We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz et al., 2010). Some texts were removed in order to balance the corpus. The balanced corpus contains 2848 documents and has been split into a development and a training and test set. 570 documents were used for the manual creation of features. The remaining 2278 documents were used to train and evaluate classifiers using 10-fold cross-validation with the WEKA machine learning toolkit (Hall et al., 2009) and various classifiers (cf. Table 1). The challenge is that the newspaper articles from the training and evaluation corpus come from different newspapers and, of course, from differen"
W13-2708,kupietz-etal-2010-german,0,0.0200728,"olitical scientists. The following are further applications of the identified indirect speeches a) using the frequency of speeches per text as a feature for classification; e.g. a classification system for news reports/commentaries as described in Section 4.4 b) a project-goal is to find texts in which collective A useful distinction for political scientists dealing with newspaper articles is the distinction between articles that report objectively on events or backgrounds and editorials or press commentaries. We first extracted opinionated and objective texts from DeReKo corpus (Stede, 2004; Kupietz et al., 2010). Some texts were removed in order to balance the corpus. The balanced corpus contains 2848 documents and has been split into a development and a training and test set. 570 documents were used for the manual creation of features. The remaining 2278 documents were used to train and evaluate classifiers using 10-fold cross-validation with the WEKA machine learning toolkit (Hall et al., 2009) and various classifiers (cf. Table 1). The challenge is that the newspaper articles from the training and evaluation corpus come from different newspapers and, of course, from different authors. Commentaries"
W13-2708,J01-4002,0,0.0247788,"to determine the orientation of the statement. These systems in turn need various kinds of preprocessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide range of possible features — including high level information like sentiment, text type, relations to other texts, etc. — that can be used by non-experts for semia"
W13-2708,J11-2001,1,0.14676,"cessing starting from tokenization over syntactic parsing up to coreference resolution. The Complex Concept Builder is the collection of all these systems with the goal to assist the political scientists. So far, the Complex Concept Builder implements tokenization (Schmid, 2009), lemmatisation (Schmid, 1995), part-of-speech tagging (Schmid and Laws, 2008), named entity detection (Faruqui and Pad´o, 2010), syntactical parsing (Bohnet, 2010), coreference analysis for German (Lappin and Leass, 1994; Stuckardt, 2001), relation extraction (Blessing et al., 2012) and sentiment analysis for English (Taboada et al., 2011). It is important for a researcher of the humanities to be able to adapt existing classification systems according to his own needs. A common procedure in both, NLP and political sciences, is to annotate data. Therefore, one major goal of the project and the Complex Concept Builder is to provide machine learning systems with a wide range of possible features — including high level information like sentiment, text type, relations to other texts, etc. — that can be used by non-experts for semiautomatic annotation and text selection. Active learning is used to provide immediate results that 3.3 I"
W13-2708,J94-4002,0,\N,Missing
W14-2906,P10-1143,0,0.440983,"d use a minimal linguistic procedure to extract values from text for every event attribute. This system is being developed to work within a pipeline annotation project where incremental clustering performs efficiently on large-scale data. In order to evaluate our proposed method, we have manually annotated a random selection of event mentions in the AQUAINT TimeML corpus (UzZaman et al., 2013). Performance of the automatic system in pair-wise coreference resolution is comparable to that of more sophisticated clustering methods, which at the same time consider a variety of linguistic features (Bejan and Harabagiu, 2010). The differences between the human annotator pair-wise decisions and the output of our clustering algorithm reveal interesting cases where coreference labeling is performed based upon the adapted semantic convention rather than information available in the text about time, location and participants of an event instance. In the following, we provide an overview of the adapted ontology, background on event coreference, and finally our implementation and experiments within the proposed framework on real data as well as the annotated corpus. We point to related work at the various appropriate pla"
W14-2906,W13-2708,1,0.785854,"emantic level. For example, multi-assignment would allow plural mentions to take part in several different clusters, each representative of one event instance. 38 tributes including Timestamps and Related Entities. While being very coarse-grained, this way of attribution is quite intuitive: events are identified by times, places and participants directly or vaguely attached to them. Temporal expressions are extracted also by ClearTK and normalized using SUTime (Chang and Manning, 2012). Named entities of all types except Date are used which are obtained from previous work on the same dataset (Blessing et al., 2013). into the feature representation of the assigned cluster in a compressed format. 4 Event Coreference System The original data in our study is a text corpus automatically annotated with several layers of syntactic and semantic information (Blessing et al., 2013). The English portion includes news and commentary articles of several British and American publishers from 1990 to 2012. An approximate average of 100 event mentions per document with the large number of total documents per month (avg. 1200) requires us to think of different ways to reduce the search space and also design a low-complex"
W14-2906,W13-1200,0,0.548773,"(CNN) -- A strong earthquake that struck the southwestern Chinese province of Sichuan this weekend has killed 186 people, sent nearly 8,200 to hospitals and created a dire dearth of drinking water, Chinese state-run Xinhua reported Sunday. Earlier reports had said as many as 11,200 people were injured. ” Mention 4 “ Shortly after noon on November 22, 1963, President John F. Kennedy was assassinated as he rode in a motorcade through Dealey Plaza. “ q m Figure 1: A three-layer ontology of events: classes, instances and mentions ality events occur with possibly infinite number of attributes. of Hovy et al. (2013b) on different types of identity according to lexicosyntactic similarity, synonymy and paraphrasing indicate that the modelers have a wide choice of identity definition for event types. In section 4.3 we explain how to adapt an extended version of synonymy in order to define event classes prior to similarity-based clustering of the mentions. 2.2 2.3 Event Mentions Facing an event mention in the text, one should first determine its class and then the unique event instance, to which the mention points. Detection of the class depends on the semantic layer definitions, while discovering the parti"
W14-2906,W13-1203,0,0.373732,"(CNN) -- A strong earthquake that struck the southwestern Chinese province of Sichuan this weekend has killed 186 people, sent nearly 8,200 to hospitals and created a dire dearth of drinking water, Chinese state-run Xinhua reported Sunday. Earlier reports had said as many as 11,200 people were injured. ” Mention 4 “ Shortly after noon on November 22, 1963, President John F. Kennedy was assassinated as he rode in a motorcade through Dealey Plaza. “ q m Figure 1: A three-layer ontology of events: classes, instances and mentions ality events occur with possibly infinite number of attributes. of Hovy et al. (2013b) on different types of identity according to lexicosyntactic similarity, synonymy and paraphrasing indicate that the modelers have a wide choice of identity definition for event types. In section 4.3 we explain how to adapt an extended version of synonymy in order to define event classes prior to similarity-based clustering of the mentions. 2.2 2.3 Event Mentions Facing an event mention in the text, one should first determine its class and then the unique event instance, to which the mention points. Detection of the class depends on the semantic layer definitions, while discovering the parti"
W14-2906,chang-manning-2012-sutime,0,0.0232838,"coreference whenever 3 4 The other type of quasi-identity discussed by Hovy et al. (2013b) engaged with sub-events is handled in the semantic level. For example, multi-assignment would allow plural mentions to take part in several different clusters, each representative of one event instance. 38 tributes including Timestamps and Related Entities. While being very coarse-grained, this way of attribution is quite intuitive: events are identified by times, places and participants directly or vaguely attached to them. Temporal expressions are extracted also by ClearTK and normalized using SUTime (Chang and Manning, 2012). Named entities of all types except Date are used which are obtained from previous work on the same dataset (Blessing et al., 2013). into the feature representation of the assigned cluster in a compressed format. 4 Event Coreference System The original data in our study is a text corpus automatically annotated with several layers of syntactic and semantic information (Blessing et al., 2013). The English portion includes news and commentary articles of several British and American publishers from 1990 to 2012. An approximate average of 100 event mentions per document with the large number of t"
W14-2906,W09-3208,0,0.0320251,"a general advantage of cluster analysis is that it provides an exploratory framework to assess the nature of similar input records, and at the end it results in a global distributional representation. This is specially desired here, since computational research on event coreference is in its early ages. Evaluation corpora and methodology are still not established, thus, the problem is not yet in the phase of “look for higher precision”! Towards Coreference Analysis In terms of method, two different approaches have been tried in the literature under the notion of event coreference resolution (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Hovy et al., 2013b). The first and most theoretically founded strategy is to decide for every pair of event mentions, whether or not they refer to the same event instance. Since in this approach decisions are independently made for every pair of event mentions, a clear formalism is needed to determine exactly what types of coreference are possible and how they are detected by looking at textual mentions (Chen and Ji, 2009; Hovy et al., 2013b). Some related work on predicate alignment also fit into this category of research (Roth and Frank, 2012; W"
W14-2906,D12-1045,0,0.142279,"at it provides an exploratory framework to assess the nature of similar input records, and at the end it results in a global distributional representation. This is specially desired here, since computational research on event coreference is in its early ages. Evaluation corpora and methodology are still not established, thus, the problem is not yet in the phase of “look for higher precision”! Towards Coreference Analysis In terms of method, two different approaches have been tried in the literature under the notion of event coreference resolution (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Hovy et al., 2013b). The first and most theoretically founded strategy is to decide for every pair of event mentions, whether or not they refer to the same event instance. Since in this approach decisions are independently made for every pair of event mentions, a clear formalism is needed to determine exactly what types of coreference are possible and how they are detected by looking at textual mentions (Chen and Ji, 2009; Hovy et al., 2013b). Some related work on predicate alignment also fit into this category of research (Roth and Frank, 2012; Wolfe et al., 2013). Alternatively, in automat"
W14-2906,R13-1021,0,0.0186596,"y constructed clusters with respect to the attribute values that are extractable from its context. In order to fill the Timestamps attribute we have employed a back-off strategy: first we look at all time expressions in the same paragraph where the event mention appears, if we found enough temporal information, that would suffice. Otherwise, we look into the content of the entire article for temporal expressions. The Related Entities atPartitioning In cross-document analysis, typically, a topicbased document partitioning is performed prior to the coreference chain detection (Lee et al., 2012; Cybulska and Vossen, 2013). Since we are interested to track discussions about a certain event possibly appearing in different contexts, this technique is not desired as coreference between mentions of a single real word event in two different topics would remain unknown. For example, when an articles reviews several instances of a certain event type such as different attacks that has happened in a wide temporal range and in different locations, such articles would not be included in any of the individual topics each focused on one event instance. As an alternative to the previous approach, we perform a time-window par"
W14-2906,W13-1204,0,0.0258693,"tities, e.g.: participants, time and location. Note, however, that structural and semantic differences exist among events of different natures, even if these complex phenomena are reduced into something more familiar and tangible such as verb frames (Fillmore et al., 2003). For example, a KILLING event is essentially attributed with its Agent and Patient, while salient attributes of an EARTHQUAKE include Location, Magnitude, Time and Human Impacts, in a typical news context. This becomes even more clear when event types are taken and compared against one another from different genres of text (Pivovarova et al., 2013; Shaw, 2013). A scientific attitude toward the analysis of EARTHQUAKE events might characterize them with Natural Impacts rather than Human Impacts. Thus, the first layer of the model needs to be designed with respect to the specific information extraction goals of the particular study, be it a pure linguistic or an applicationoriented one. Ambiguities about the granularity of attributes, subevent-ness, scope and most importantly, identity between event instances are dealt with at the definition layer for and between classes. For example, if the modeler wants to allow coreference between inst"
W14-2906,C10-2121,0,0.061261,"Missing"
W14-2906,W14-0111,0,0.016198,"ords enables us to specify a clear semantic convention for the coreference system. In addition to the mentions coming from the same synset, we allow coreference between events belonging to two different synsets that are directly connected via hypernymy or morphosemantic links. While every WordNet synset comprises words only from a single part of speech, morphosemantic relations allow the model to establish cross-PoS identity among words sharing a stem with the same meaning which is desired here: observe (verb) and observation (noun)6 . A Java library is employed to access WordNet annotations (Finlayson, 2014). 2) Similarity-based clustering: A mention is compared against previously constructed clusters with respect to the attribute values that are extractable from its context. In order to fill the Timestamps attribute we have employed a back-off strategy: first we look at all time expressions in the same paragraph where the event mention appears, if we found enough temporal information, that would suffice. Otherwise, we look into the content of the entire article for temporal expressions. The Related Entities atPartitioning In cross-document analysis, typically, a topicbased document partitioning"
W14-2906,S12-1030,0,0.0143093,"tion (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Hovy et al., 2013b). The first and most theoretically founded strategy is to decide for every pair of event mentions, whether or not they refer to the same event instance. Since in this approach decisions are independently made for every pair of event mentions, a clear formalism is needed to determine exactly what types of coreference are possible and how they are detected by looking at textual mentions (Chen and Ji, 2009; Hovy et al., 2013b). Some related work on predicate alignment also fit into this category of research (Roth and Frank, 2012; Wolfe et al., 2013). Alternatively, in automatic event clustering, the objective is basically discovering event instances: all we know about an event in the world is the collective information obtained from mentions referring to that in a text corpus. Each cluster in the end ideally represents a unique event in reality with all its attribute values (Bejan and Harabagiu, 2010; Lee et al., 2012). Some formal and technical differences exist between the two approaches. The method we are going to propose in the next section combines a rule-based initial stage with a similarity-based clustering pr"
W14-2906,W13-1205,0,0.0461165,"s, time and location. Note, however, that structural and semantic differences exist among events of different natures, even if these complex phenomena are reduced into something more familiar and tangible such as verb frames (Fillmore et al., 2003). For example, a KILLING event is essentially attributed with its Agent and Patient, while salient attributes of an EARTHQUAKE include Location, Magnitude, Time and Human Impacts, in a typical news context. This becomes even more clear when event types are taken and compared against one another from different genres of text (Pivovarova et al., 2013; Shaw, 2013). A scientific attitude toward the analysis of EARTHQUAKE events might characterize them with Natural Impacts rather than Human Impacts. Thus, the first layer of the model needs to be designed with respect to the specific information extraction goals of the particular study, be it a pure linguistic or an applicationoriented one. Ambiguities about the granularity of attributes, subevent-ness, scope and most importantly, identity between event instances are dealt with at the definition layer for and between classes. For example, if the modeler wants to allow coreference between instances of KILL"
W14-2906,S13-2001,0,0.0547746,"Missing"
W14-2906,P13-2012,0,0.0269427,"Missing"
W14-2906,W13-1202,0,\N,Missing
W15-0513,P11-1099,0,0.053369,"be more difficult. After looking at related work, Section 3 describes our corpora and the machine learning experiments. In Section 4, we evaluate the results and discuss the most common problems with the ProCon texts, and Section 5 concludes. 2 Related work The majority of work on text-oriented argumentation mining concentrates on identifying just the “gist” of arguments, i.e., premises and conclusions. This holds, for example, for the well-known early approach of Mochales Palau and Moens (2009), and for the follow-up step on scheme classification (on top of detected premises/conclusions) by Feng and Hirst (2011). Among the few approaches that do consider counter-considerations, Kang and Saint-Dizier (2014) analyze technical douments (largely instructional text), where the notion of exception to an argument plays a role, but its function is quite different from the perspective-switching that we discuss here. Ong et al. (2014) work on student essays, which are somewhat more similar to “our” genres. Their task includes the recognition of sentence types (CurrentStudy, Hypothesis, Claim, Citation) and of support and oppose relations between sentences. For the complete task, the authors use eight hand-code"
W15-0513,W14-2104,0,0.0363126,"ntrates on identifying just the “gist” of arguments, i.e., premises and conclusions. This holds, for example, for the well-known early approach of Mochales Palau and Moens (2009), and for the follow-up step on scheme classification (on top of detected premises/conclusions) by Feng and Hirst (2011). Among the few approaches that do consider counter-considerations, Kang and Saint-Dizier (2014) analyze technical douments (largely instructional text), where the notion of exception to an argument plays a role, but its function is quite different from the perspective-switching that we discuss here. Ong et al. (2014) work on student essays, which are somewhat more similar to “our” genres. Their task includes the recognition of sentence types (CurrentStudy, Hypothesis, Claim, Citation) and of support and oppose relations between sentences. For the complete task, the authors use eight hand-coded rules performing string matching using word lists and numbers (for identifying the year of a citation); thus the approach is geared toward finding relationships specifically between citations and will not generalize well to the broad class of counterconsiderations. A support/oppose distinction is also made by Stab a"
W15-0513,W14-2112,1,0.826007,"Missing"
W15-0513,C14-1142,0,0.0961901,"(2014) work on student essays, which are somewhat more similar to “our” genres. Their task includes the recognition of sentence types (CurrentStudy, Hypothesis, Claim, Citation) and of support and oppose relations between sentences. For the complete task, the authors use eight hand-coded rules performing string matching using word lists and numbers (for identifying the year of a citation); thus the approach is geared toward finding relationships specifically between citations and will not generalize well to the broad class of counterconsiderations. A support/oppose distinction is also made by Stab and Gurevych (2014), who annotated a corpus of 90 essays (1673 sentences) with the central claim of the text (90 instances), claims of paragraph-size units (429), and premises (1033). Claims are marked with an attribute ‘for’ (365) or ‘against’ (64), but the authors do not report numbers on the stance of premises. Note however, that the stance of premises could be inferred by the relation structure, i.e. the sequence of supposing and opposing relations. Of the 1473 relations in the corpus, 161 are opposing. As the proportion of ‘against’ claims is also relatively low, the authors restrict their classification 10"
W15-0513,W07-1515,0,\N,Missing
W15-3403,R11-1017,0,0.0485526,"Missing"
W15-3403,P14-5002,0,0.0488,"Missing"
W15-3403,guillou-etal-2014-parcor,0,0.0527157,"), and (ii) facilitate extension to further languages. Regarding English, our guidelines are of similar length and quite compatible with the scheme used for OntoNotes - the largest annotated coreference corpus for the English language (Hovy et al., 2006). One exception is that we handle only NPs and do not annotate verbs that are coreferent with NPs. Our guidelines borrow many decisions from the (relatively language-neutral) Potsdam Coreference Scheme (PoCoS) (Krasavina and Chiarcos, 2007), and we also considered the recently developed guidelines for thr English-German parallel corpus ParCor (Guillou et al., 2014). But it considers only pairwise annotation of anaphoric pronouns and their antecedents, whereas we annotate all REs appearing in a coreference chain (i.e. that are mentioned in the text at least twice). For the time being, our annotation is restricted to the referential identity; we thus exclude cases of ‘bridging’ (also called ’indirect anaphora’) or near-identity. The following types of REs are considered as markables: full NPs, proper names, and pronouns (personal, demonstrative, relative, reflexive, and pronominal adverbs). As in OntoNotes, generic nouns can corefer with definite full NPs"
W15-3403,A00-1020,0,0.175705,"Missing"
W15-3403,N06-2015,0,0.147341,"swire texts, and the coreference chains of the stories tend to be much longer. 3.2 Usually, coreference annotation guidelines have been designed with one target language in mind. In contrast, our goal was to have common guidelines for the three languages, in order to (i) obtain uniform nominal coreference annotations in our corpus (supporting the projection task), and (ii) facilitate extension to further languages. Regarding English, our guidelines are of similar length and quite compatible with the scheme used for OntoNotes - the largest annotated coreference corpus for the English language (Hovy et al., 2006). One exception is that we handle only NPs and do not annotate verbs that are coreferent with NPs. Our guidelines borrow many decisions from the (relatively language-neutral) Potsdam Coreference Scheme (PoCoS) (Krasavina and Chiarcos, 2007), and we also considered the recently developed guidelines for thr English-German parallel corpus ParCor (Guillou et al., 2014). But it considers only pairwise annotation of anaphoric pronouns and their antecedents, whereas we annotate all REs appearing in a coreference chain (i.e. that are mentioned in the text at least twice). For the time being, our annot"
W15-3403,W07-1525,0,0.0346295,"s for the three languages, in order to (i) obtain uniform nominal coreference annotations in our corpus (supporting the projection task), and (ii) facilitate extension to further languages. Regarding English, our guidelines are of similar length and quite compatible with the scheme used for OntoNotes - the largest annotated coreference corpus for the English language (Hovy et al., 2006). One exception is that we handle only NPs and do not annotate verbs that are coreferent with NPs. Our guidelines borrow many decisions from the (relatively language-neutral) Potsdam Coreference Scheme (PoCoS) (Krasavina and Chiarcos, 2007), and we also considered the recently developed guidelines for thr English-German parallel corpus ParCor (Guillou et al., 2014). But it considers only pairwise annotation of anaphoric pronouns and their antecedents, whereas we annotate all REs appearing in a coreference chain (i.e. that are mentioned in the text at least twice). For the time being, our annotation is restricted to the referential identity; we thus exclude cases of ‘bridging’ (also called ’indirect anaphora’) or near-identity. The following types of REs are considered as markables: full NPs, proper names, and pronouns (personal,"
W15-3403,N12-1090,0,0.32708,"Missing"
W15-3403,H05-1004,0,0.104933,"ts are somewhat lower, probably due to the much smaller training set. Padó (2007) Spreyer (2011) Our alignment Bisentences 1 029 400 1 314 944 205 208 Prec. 98.6 94.88 92.95 Recall 52.9 62.04 51.23 90 70 60 50 F-m. 68.86 75.02 66.05 40 30 Table 4: Evaluation of the automatic word alignment 20 To simplify subsequent processing, we converted the corpus annotations into the CoNLL table format6 using discoursegraphs converter (Neumann, 2015). 0 10 We evaluate all the projected coreference chains against gold chains using the standard coreference evaluation metrics MUC (Vilain et al., 1995), CEAF (Luo, 2005) and B3 (Bagga and Baldwin, 1998) to get complete performance characteristics. We also use strict matching as in the evaluation of the identification of mentions and evaluate the projected markables against all the markables of the gold standard. These scores depend on the identification of mentions evaluated in the previous step. We report the micro-averaged Precision, Recall and F-1 scores in Table 6. In addition, Figure 1 shows the distribution of macro-averaged F1-scores for two of the metrics (MUC and B3 ) for both language pairs as boxplots. Evaluation We evaluate both the quality of the"
W15-3403,P07-1123,0,0.117006,"Missing"
W15-3403,W15-1843,0,0.0520453,"Missing"
W15-3403,J03-1002,0,0.0154821,"Missing"
W15-3403,C14-1175,0,0.0291915,"Missing"
W15-3403,W06-2008,0,0.0694807,"Missing"
W15-3403,E06-1020,0,0.0289032,"s: a. [a fat lady]1 [who]1 wore a fur around her neck b. [eine dicke Dame mit einer Pelzstola]1 (‘a fat lady with a a fur’) 4.4 Discussion Comparing the genres According to Table 6 and Figure 1, we see that newswire texts get the lowest scores, the reason most likely being the more complicated NPs. In setting 2 (evaluation of minimal spans), both newswire texts and stories obtain closer F1-scores, but the stories still have better precision scores. 9 The COWAL word aligner is a lexical aligner which is adjusted only for Romanian-English and requires a corpus with morpho-syntactic annotations (Tufis et al., 2006). 20 that way. Still, our emphasis remains on devising procedures that are generalizable to other lowresourced languages, so we will do these extensions in small steps only. Our annotation guidelines and other material will be made available via our website http://www.ling.uni-potsdam.de/acl-lab/. the English-Romanian; the Russian REs were extracted slightly more accurately due to the structural differences in NPs. We also observed different scores for newswire texts, stories and medical leaflets, while Postolache et al. only used texts of one genre and in fact one author (different chapters o"
W15-3403,H05-1108,0,0.11986,"Missing"
W15-3403,M95-1005,0,0.572901,"Europarl dataset. Our results are somewhat lower, probably due to the much smaller training set. Padó (2007) Spreyer (2011) Our alignment Bisentences 1 029 400 1 314 944 205 208 Prec. 98.6 94.88 92.95 Recall 52.9 62.04 51.23 90 70 60 50 F-m. 68.86 75.02 66.05 40 30 Table 4: Evaluation of the automatic word alignment 20 To simplify subsequent processing, we converted the corpus annotations into the CoNLL table format6 using discoursegraphs converter (Neumann, 2015). 0 10 We evaluate all the projected coreference chains against gold chains using the standard coreference evaluation metrics MUC (Vilain et al., 1995), CEAF (Luo, 2005) and B3 (Bagga and Baldwin, 1998) to get complete performance characteristics. We also use strict matching as in the evaluation of the identification of mentions and evaluate the projected markables against all the markables of the gold standard. These scores depend on the identification of mentions evaluated in the previous step. We report the micro-averaged Precision, Recall and F-1 scores in Table 6. In addition, Figure 1 shows the distribution of macro-averaged F1-scores for two of the metrics (MUC and B3 ) for both language pairs as boxplots. Evaluation We evaluate both"
W15-3403,H01-1035,0,0.495427,"report on experiments with projecting nominal coreference chains across bilingual corpora. Our goal is to see how well a knowledge-lean projection algorithm works for two relatively similar languages (English-German) and for less similar languages (English-Russian). Furthermore, we are interested in differences incurred by the text genre and 2 Related work A projection approach is used to automatically transfer different types of linguistic annotation from one language to another. The idea of mapping from well-studied languages to lowresourced languages was initially introduced in the work of Yarowsky et al. (2001), who studied the induction of PoS and NE taggers, NP chunkers and morphological analyzers for different languages using annotation projection. Thereafter, the technique has been used for a variety of 14 Proceedings of the Eighth Workshop on Building and Using Comparable Corpora, pages 14–22, c Beijing, China, July 30, 2015. 2015 Association for Computational Linguistics tasks, including PoS tagging and syntactic parsing (Hwa et al., 2005; Ozdowska, 2006; Tiedemann, 2014), semantic role labelling (Padó and Lapata, 2005), sentiment analysis (Mihalcea et al., 2007), mention detection (Zitouni an"
W15-3403,postolache-etal-2006-transferring,0,0.522466,"n et al., 2011) and CONLL-2012 shared tasks (Pradhan et al., 2012), so that we score only those projected markable spans that are exactly the same as the gold ones. The values for English-German and English-Russian are given in Table 6 as mentions. 3. Evaluation of coreference chains with minimal spans Finally, we evaluate using just minimal spans of the REs, i.e., syntactic heads. This indicates how well the REs can be projected, not punishing the algorithm for detecting only partially correct REs. We manually annotated syntactic heads of the gold and projected REs. Following the approach of Postolache et al. (2006), we select the leftmost 2. Evaluation of coreference chains 6 7 -mucews-bcub ries-muc ries-bcumb ed-muc ed-bcub m n sto sto news Figure 1: Comparison of English-German and English-Russian projections: boxplots of the macro-averaged F1 scores (MUC and B-cubed) for different genres Extraction of REs and transfer of coreference chains. For each RE in the source language we extract the corresponding RE in the target language, together with its coreference set number. Following the approach of Postolache et al. (2006), for each word span representing an RE in the source language, we extract the co"
W15-3403,W11-1901,0,0.0367055,"es depend on the identification of mentions evaluated in the previous step. We report the micro-averaged Precision, Recall and F-1 scores in Table 6. In addition, Figure 1 shows the distribution of macro-averaged F1-scores for two of the metrics (MUC and B3 ) for both language pairs as boxplots. Evaluation We evaluate both the quality of the identification of mentions and the extraction of coreference chains using the CoNLL scorer7 . 1. Evaluation of the identification of mentions. We compute the scores for the identification of mentions using the strict mention matching as in the CoNLL-2011 (Pradhan et al., 2011) and CONLL-2012 shared tasks (Pradhan et al., 2012), so that we score only those projected markable spans that are exactly the same as the gold ones. The values for English-German and English-Russian are given in Table 6 as mentions. 3. Evaluation of coreference chains with minimal spans Finally, we evaluate using just minimal spans of the REs, i.e., syntactic heads. This indicates how well the REs can be projected, not punishing the algorithm for detecting only partially correct REs. We manually annotated syntactic heads of the gold and projected REs. Following the approach of Postolache et a"
W15-3403,D08-1063,0,0.0503692,"Missing"
W15-3403,W12-4501,0,0.0636028,"ed in the previous step. We report the micro-averaged Precision, Recall and F-1 scores in Table 6. In addition, Figure 1 shows the distribution of macro-averaged F1-scores for two of the metrics (MUC and B3 ) for both language pairs as boxplots. Evaluation We evaluate both the quality of the identification of mentions and the extraction of coreference chains using the CoNLL scorer7 . 1. Evaluation of the identification of mentions. We compute the scores for the identification of mentions using the strict mention matching as in the CoNLL-2011 (Pradhan et al., 2011) and CONLL-2012 shared tasks (Pradhan et al., 2012), so that we score only those projected markable spans that are exactly the same as the gold ones. The values for English-German and English-Russian are given in Table 6 as mentions. 3. Evaluation of coreference chains with minimal spans Finally, we evaluate using just minimal spans of the REs, i.e., syntactic heads. This indicates how well the REs can be projected, not punishing the algorithm for detecting only partially correct REs. We manually annotated syntactic heads of the gold and projected REs. Following the approach of Postolache et al. (2006), we select the leftmost 2. Evaluation of"
W16-0706,C10-1011,0,0.0201701,"ing demzufolge and the 39 similar German words involves two subproblems: Disambiguate the reading (connective or non-connective), and resolve the argument(s) – either the antecedent of the NPanaphor, or the two arguments of the connective. For disambiguation, before embarking on fullfledged feature-based classification, it is advisable to check whether standard POS tagging can (partially) solve the problem. To this end, we experimented with two German taggers on the kernel90 set: clevertagger9 , which is integrated in the ParZu parser (Sennrich et al., 2009), and the tagger of the MATE tools (Bohnet, 2010). Both were used with their standard models, which for ParZu was trained on the TüBa-D/Z treebank10 and for MATE on a dependency-converted version of the TIGER treebank11 . They both make use of the STTS tagset12 but in different versions. For our purposes, it is relevant that they use PROAV and PROP, respectively, for the German pronominal adverbs (contractions of a pronominal form and a preposition). Table 1 shows the tag distribution for the four groups of antecedent types; in each group, the top line gives the MATE results and the bottom line those of ParZu. The &quot;other&quot; column conflates a"
W16-0706,C14-1058,0,0.0301801,"Missing"
W16-0706,W03-2608,0,0.100437,"Missing"
W16-0706,P09-2004,0,0.0562667,"Missing"
W16-0706,prasad-etal-2008-penn,0,0.0496228,"Missing"
W16-0706,J03-4002,0,0.172069,"Missing"
W16-0706,D07-1010,0,0.0930253,"Missing"
W16-2812,P14-1002,0,0.0476093,"tions are being defined largely in terms of speaker intentions, so that the analysis is meant to capture the “plan” the author devised to influence his or her audience. The developers of RST had not explicitly targeted one particular text type or discourse mode (instructive, argumentative, descriptive, narrative, expository), but when we assume that the text is argumentative, the very nature of the RST approach suggests that it might in fact capture the underlying argumentation quite well. Systems for automatic RST parsing have been built since the early 00s, with recent approaches including (Ji and Eisenstein, 2014) and (Joty et • We provide a qualitative analysis that examines the commonalities and differences between the two levels of representation in the corpus, and seeks explanations for them. • We report on experiments in automatically mapping RST trees to argumentation structures, for now on the basis of the manuallyannotated “gold” RST trees. Following the discussion of related work in Section 2, Section 3 gives a brief introduction to the corpus and the annotation schemes that are used for argumentation and for RST. Then, Section 4 presents our qualitative (comparative) analysis, and Section 5 t"
W16-2812,J15-3002,0,0.141106,"Missing"
W16-2812,W15-0501,0,0.0607759,"gle node in the RST tree (representing the central claim), which is in conflict with a basic principle of RST. This indicates that Azar borrowed certain aspects from RST but ignored others. In our earlier work (Peldszus and Stede, 2013), we posited that the underlying phenomenon of non-adjacency creates a problem for RST-argumentation mapping in general, i.e., it is not limited to discontinuous claims: Both Support and Attack moves can be directed to material that occurs in non-adjacent segments. A small portion of RST’s ideas was incorporated into the annotation of argumentation performed by Kirschner et al. (2015) on student essays. The authors used standard argumentative Support and Attack relations, and to these added the coherence relations Sequence and Detail for capturing specific argumentative moves; the relation definitions are inspired by those used in RST. Green (2010) proposed a “hybrid” tree representation called ArgRST, which combines RST’s nuclearity principle and some of its relation definitions with additional annotations capturing aspects of argumentation: The analyst can add implicit statements to the tree (enthymemes in the argumentation), and in parallel to RST relations, the links b"
W16-2812,J92-4007,0,0.675238,"Missing"
W16-2812,D15-1110,1,0.898942,"an be theoretically adequate. Of central importance is the correspondence between RSTnuc and ARGcc; we found that for all the mismatches in the corpus, it is possble to construct a plausible alternative RST tree such that the two are identical or at least overlapping (when the granularities of the analyses don’t match exactly). Another issue is the presence of crossing edges, which occur in seven ARG graphs in the corpus. Since this is likely to occur more often in longer texts, it remains a fundamental issue; we will return to it at the end. In our study, we follow the experimental setup of (Peldszus and Stede, 2015). We use the same train-test splits, resulting from 10 iterations of 5-fold cross validation, and adopt their evaluation procedure, where the correctness of predicted structures is assessed in four subtasks: 5 Note that the argumentative role of each segment is not explicitly coded in the structures we predict below, but is inferred from the chain of supporting (role preserving) and attacking (role switching) relations from the central claim (by definition in proponent’s voice) to the segment of interest. • attachment (at): Given a pair of EDUs, are they connected? [yes, no] • central claim (c"
W16-2812,L16-1167,1,0.660155,"olve an RST parser as an early step that accomplishes a good share of the overall task. How feasible this is has so far not been determined, though. On the theoretical side, different opinions have been voiced in the literature on the role of RST trees for argumentation analysis; we summarize the situation below in Section 2. All these opinions were based on the experiences that their authors had made with manually applying RST and with analyzing argumentation, but they were not based on systematic empirical evidence. In contrast, in this paper we use a new resource that we recently released (Stede et al., 2016), which offers annotations of both RST and argumentation structure analyses on a corpus of 112 short texts. Our previous paper presented a first rough analysis of the correlations between RST and argumentation. The present paper builds on those preliminary results and makes two contributions: On the basis of a new corpus of short “microtexts” with parallel manual annotations, we study the mapping from discourse structure (in terms of Rhetorical Structure Theory, RST) to argumentation structure. We first perform a qualitative analysis and discuss our findings on correspondence patterns. Then we"
W16-4309,P08-1034,0,0.0294058,"proaches in Section 5. Finally, after estimating the impact of different seed sets on the automatic methods and performing a qualitative analysis of their entries, we draw our conclusions and outline directions for future research in the final part of this paper. To avoid unnecessary repetitions, we deliberately omit a summary of related work, since most of the popular SLG algorithms will be referenced in the respective evaluation sections anyway. We should, however, note that, apart from the research on the automatic lexicon generation, our study is also closely related to the experiments of Andreevskaia and Bergler (2008) and the “Sentiment Analysis in Twitter” track of the SemEval competition (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). In contrast to the former work, however, where the authors trained a supervised classifier on one domain and applied it to another in order to determine the polarities of the sentences, we explicitly model a situation where no annotated training data are available, thus looking for the most general unsupervised SLG strategy which performs best regardless of the target domain, and we also evaluate these strategies on the level of lexical phrases only. F"
W16-4309,P10-1041,0,0.350537,"deal with an extreme noisiness of their input. Since it was unclear which of these properties would have a stronger impact on the net results, we decided to reimplement the most commonly used algorithms from both of these paradigms and evaluate them on the PotTS corpus. 5.1 Dictionary-Based Approaches For dictionary-based methods, we adopted the systems proposed by Hu and Liu (2004), BlairGoldensohn et al. (2008), Kim and Hovy (2004), Esuli and Sebastiani (2006), as well as the min-cut and label-propagation approaches of Rao and Ravichandran (2009), and the random-walk algorithm described by Awadallah and Radev (2010). The first of these works (Hu and Liu, 2004) expanded a given set of seed terms with known semantic orientations by propagating polarity values of these terms to their W ORD N ET synonyms and passing reversed polarity scores to the antonyms of these words. Later on, this idea was further refined by BlairGoldensohn et al. (2008), who obtained polarity labels for new terms by multiplying a score vector ~v containing the orientation scores of the known seed words (-1 for negative expressions and 1 for positive ones) with an adjacency matrix A constructed for the W ORD N ET graph. With various mo"
W16-4309,esuli-sebastiani-2006-sentiwordnet,0,0.406996,"lexical database, but lack any domain specificity. Corpusbased methods, on the other hand, can operate directly on unannotated in-domain data, but often have to deal with an extreme noisiness of their input. Since it was unclear which of these properties would have a stronger impact on the net results, we decided to reimplement the most commonly used algorithms from both of these paradigms and evaluate them on the PotTS corpus. 5.1 Dictionary-Based Approaches For dictionary-based methods, we adopted the systems proposed by Hu and Liu (2004), BlairGoldensohn et al. (2008), Kim and Hovy (2004), Esuli and Sebastiani (2006), as well as the min-cut and label-propagation approaches of Rao and Ravichandran (2009), and the random-walk algorithm described by Awadallah and Radev (2010). The first of these works (Hu and Liu, 2004) expanded a given set of seed terms with known semantic orientations by propagating polarity values of these terms to their W ORD N ET synonyms and passing reversed polarity scores to the antonyms of these words. Later on, this idea was further refined by BlairGoldensohn et al. (2008), who obtained polarity labels for new terms by multiplying a score vector ~v containing the orientation scores"
W16-4309,W97-0802,0,0.509653,"ger application such as a supervised classifier which utilizes lexicon’s entries as features). 1 We use version 0.1.0 of this corpus. The annotators had been asked to judge the semantic orientation of a term irrespective of its possible negations. They could, however, consider the context for determining whether a particular reading of a polysemous word in the text was subjective or not. 3 A detailed inter-annotator agreement study of this corpus is provided in (Sidarenka, 2016). 4 That way, we only used the labeled corpus for evaluation or parameter optimization, other resources—G ERMA N ET (Hamp and Feldweg, 1997) and the German Twitter Snapshot (Scheffler, 2014)—were used for training the methods. 2 81 Traditionally, intrinsic evaluation of English sentiment lexicons amounts to comparing these polarity lists with the General Inquirer (GI; Stone, 1966)—a manually compiled set of 11,895 words annotated with their semantic categories—by taking the intersection of the two resources and estimating the percentage of matches in which automatically induced polar terms have the same polarity as the GI entries. This evaluation method, however, is somewhat problematic: First of all, it is not easily transferable"
W16-4309,P97-1023,0,0.113064,"rmation obtained from a corpus of 10,200 customer reviews and the German Collocation Dictionary (Quasthoff, 2010). Finally, the Zurich Polarity List features 8,000 subjective entries taken from G ERMA N ET synsets (Hamp and Feldweg, 1997). These synsets were manually annotated with their prior polarities by human experts. Since the authors, however, found the number of polar adjectives obtained that way insufficient for running further classification experiments, they automatically enriched this lexicon with more attributive terms by analyzing conjoined corpus collocations using the method of Hatzivassiloglou and McKeown (1997). 5 Recall that the annotators of the PotTS data set were asked to annotate a polar expression iff its actual sense in the respective context was polar. 6 In other words, we successively compare lexicon entries with the occurrences of corpus tokens in the same linear order as these occurrences appear in the text. 7 We use the T REE TAGGER of Schmid (1995) for lemmatization. 82 Lexicon GPC SWS ZPL GPC ∩ SWS ∩ ZPL GPC ∪ SWS ∪ ZPL Positive Expressions Negative Expressions Neutral Terms Precision Recall F1 Precision Recall F1 Precision Recall F1 0.209 0.335 0.411 0.527 0.202 0.535 0.435 0.424 0.37"
W16-4309,C04-1200,0,0.639048,"lly manually labeled lexical database, but lack any domain specificity. Corpusbased methods, on the other hand, can operate directly on unannotated in-domain data, but often have to deal with an extreme noisiness of their input. Since it was unclear which of these properties would have a stronger impact on the net results, we decided to reimplement the most commonly used algorithms from both of these paradigms and evaluate them on the PotTS corpus. 5.1 Dictionary-Based Approaches For dictionary-based methods, we adopted the systems proposed by Hu and Liu (2004), BlairGoldensohn et al. (2008), Kim and Hovy (2004), Esuli and Sebastiani (2006), as well as the min-cut and label-propagation approaches of Rao and Ravichandran (2009), and the random-walk algorithm described by Awadallah and Radev (2010). The first of these works (Hu and Liu, 2004) expanded a given set of seed terms with known semantic orientations by propagating polarity values of these terms to their W ORD N ET synonyms and passing reversed polarity scores to the antonyms of these words. Later on, this idea was further refined by BlairGoldensohn et al. (2008), who obtained polarity labels for new terms by multiplying a score vector ~v cont"
W16-4309,S13-2053,0,0.0456769,"raged F1 -score of 0.589), and that dictionary-based techniques produce much better polarity lists as compared to corpus-based approaches (whose best F1 -scores run up to 0.479 and 0.419 respectively) even for the non-standard Twitter genre. All reimplementations of the compared systems and the resulting lexicons of these methods are available online at https://github.com/WladimirSidorenko/SentiLex. 1 Introduction Sentiment lexicons play a crucial role in many existing and emerging opinion mining applications. Not only do they serve as a valuable source of features for supervised classifiers (Mohammad et al., 2013; Zhu et al., 2014) but they also achieve competitive results when used as the main component of a sentiment analysis system (Taboada et al., 2011). Due to this high impact and tremendous costs of building such lexicons manually, devising new algorithms for an automatic generation of polarity lists has always been an area of active research in the sentiment analysis literature (Liu, 2012, pp. 79-91). Nevertheless, despite some obvious progress in this field (Cambria et al., 2016), the applicability of these approaches to other languages and text genres still raises questions: It is, for instan"
W16-4309,S13-2052,0,0.0987203,"Missing"
W16-4309,E09-1077,0,0.148376,"d, can operate directly on unannotated in-domain data, but often have to deal with an extreme noisiness of their input. Since it was unclear which of these properties would have a stronger impact on the net results, we decided to reimplement the most commonly used algorithms from both of these paradigms and evaluate them on the PotTS corpus. 5.1 Dictionary-Based Approaches For dictionary-based methods, we adopted the systems proposed by Hu and Liu (2004), BlairGoldensohn et al. (2008), Kim and Hovy (2004), Esuli and Sebastiani (2006), as well as the min-cut and label-propagation approaches of Rao and Ravichandran (2009), and the random-walk algorithm described by Awadallah and Radev (2010). The first of these works (Hu and Liu, 2004) expanded a given set of seed terms with known semantic orientations by propagating polarity values of these terms to their W ORD N ET synonyms and passing reversed polarity scores to the antonyms of these words. Later on, this idea was further refined by BlairGoldensohn et al. (2008), who obtained polarity labels for new terms by multiplying a score vector ~v containing the orientation scores of the known seed words (-1 for negative expressions and 1 for positive ones) with an a"
W16-4309,remus-etal-2010-sentiws,0,0.613924,"inions, Personality, and Emotions in Social Media, pages 80–90, Osaka, Japan, December 12 2016. (most of which were semi-automatically translated from popular English resources) with the results of common automatic dictionary- and corpus-based SLG approaches. We begin our study by describing the data set which will be used in our evaluation. Afterwards, in Section 3, we introduce the metrics with which we will assess the quality of various polarity lists. Then, in Section 4, we evaluate three most popular existing German sentiment lexicons—the German Polarity Clues (Waltinger, 2010), SentiWS (Remus et al., 2010), and Zurich Polarity List of Clematide and Klenner (2010), subsequently comparing them with popular automatic SLG approaches in Section 5. Finally, after estimating the impact of different seed sets on the automatic methods and performing a qualitative analysis of their entries, we draw our conclusions and outline directions for future research in the final part of this paper. To avoid unnecessary repetitions, we deliberately omit a summary of related work, since most of the popular SLG algorithms will be referenced in the respective evaluation sections anyway. We should, however, note that,"
W16-4309,S14-2009,0,0.0168668,"ualitative analysis of their entries, we draw our conclusions and outline directions for future research in the final part of this paper. To avoid unnecessary repetitions, we deliberately omit a summary of related work, since most of the popular SLG algorithms will be referenced in the respective evaluation sections anyway. We should, however, note that, apart from the research on the automatic lexicon generation, our study is also closely related to the experiments of Andreevskaia and Bergler (2008) and the “Sentiment Analysis in Twitter” track of the SemEval competition (Nakov et al., 2013; Rosenthal et al., 2014; Rosenthal et al., 2015). In contrast to the former work, however, where the authors trained a supervised classifier on one domain and applied it to another in order to determine the polarities of the sentences, we explicitly model a situation where no annotated training data are available, thus looking for the most general unsupervised SLG strategy which performs best regardless of the target domain, and we also evaluate these strategies on the level of lexical phrases only. Furthermore, unlike in the SemEval track, where the organizers also provided participants with sufficient labeled in-d"
W16-4309,S15-2078,0,0.171325,"Approaches An alternative way to generate polarity lists is to use corpus-based approaches. In contrast to dictionarybased methods, these systems typically operate immediately on raw texts and are, therefore, virtually independent of any manually annotated linguistic resources. This flexibility, however, might come at the cost of a reduced accuracy due to an inherent noisiness of the unlabeled data. The most prominent representatives of this class of algorithms are the approaches proposed by Takamura et al. (2005), Velikovich et al. (2010), Kiritchenko et al. (2014), and Severyn and Moschitti (2015), which we briefly describe in this section. 8 All translated seed sets are provided along with the source code for this paper. 84 Drawing on the pioneering work of Hatzivassiloglou and McKeown (1997), in which the authors expanded an initial list of polar adjectives by analyzing coordinately conjoined terms from a text corpus, Takamura et al. (2005) enhanced this algorithm, extending it to other parts of speech and also incorporating semantic links from W ORD N ET in addition to the co-occurrence statistics extracted from the corpus. After representing the final set of terms as an electron la"
W16-4309,scheffler-2014-german,0,0.103183,"lizes lexicon’s entries as features). 1 We use version 0.1.0 of this corpus. The annotators had been asked to judge the semantic orientation of a term irrespective of its possible negations. They could, however, consider the context for determining whether a particular reading of a polysemous word in the text was subjective or not. 3 A detailed inter-annotator agreement study of this corpus is provided in (Sidarenka, 2016). 4 That way, we only used the labeled corpus for evaluation or parameter optimization, other resources—G ERMA N ET (Hamp and Feldweg, 1997) and the German Twitter Snapshot (Scheffler, 2014)—were used for training the methods. 2 81 Traditionally, intrinsic evaluation of English sentiment lexicons amounts to comparing these polarity lists with the General Inquirer (GI; Stone, 1966)—a manually compiled set of 11,895 words annotated with their semantic categories—by taking the intersection of the two resources and estimating the percentage of matches in which automatically induced polar terms have the same polarity as the GI entries. This evaluation method, however, is somewhat problematic: First of all, it is not easily transferable to other languages, since even a manual translati"
W16-4309,N15-1159,0,0.467685,"on). 5.2 Corpus-Based Approaches An alternative way to generate polarity lists is to use corpus-based approaches. In contrast to dictionarybased methods, these systems typically operate immediately on raw texts and are, therefore, virtually independent of any manually annotated linguistic resources. This flexibility, however, might come at the cost of a reduced accuracy due to an inherent noisiness of the unlabeled data. The most prominent representatives of this class of algorithms are the approaches proposed by Takamura et al. (2005), Velikovich et al. (2010), Kiritchenko et al. (2014), and Severyn and Moschitti (2015), which we briefly describe in this section. 8 All translated seed sets are provided along with the source code for this paper. 84 Drawing on the pioneering work of Hatzivassiloglou and McKeown (1997), in which the authors expanded an initial list of polar adjectives by analyzing coordinately conjoined terms from a text corpus, Takamura et al. (2005) enhanced this algorithm, extending it to other parts of speech and also incorporating semantic links from W ORD N ET in addition to the co-occurrence statistics extracted from the corpus. After representing the final set of terms as an electron la"
W16-4309,L16-1181,1,0.890226,"strategy which performs best regardless of the target domain, and we also evaluate these strategies on the level of lexical phrases only. Furthermore, unlike in the SemEval track, where the organizers also provided participants with sufficient labeled in-domain training sets and then asked them to predict the contextual polarity of pre-annotated polar expressions in the test data, we simultaneously try to predict polar terms and their prior polarities, learning both of them without supervision. 2 Data We perform our evaluation on the publicly available Potsdam Twitter Sentiment corpus (PotTS; Sidarenka, 2016).1 This collection comprises 7,992 microblogs pertaining to the German federal elections, general political life, papal conclave 2013, as well as casual everyday conversations. Two human experts annotated these posts with polar terms and their prior polarities,2 reaching a substantial agreement of 0.75 binary κ (Cohen, 1960).3 We used the complete data set labeled by one of the annotators as our test corpus, getting a total of 6,040 positive and 3,055 negative terms including multi-word expressions. However, since many of these expressions were emoticons, which, on the one hand, were a priori"
W16-4309,J11-2001,1,0.340481,"st F1 -scores run up to 0.479 and 0.419 respectively) even for the non-standard Twitter genre. All reimplementations of the compared systems and the resulting lexicons of these methods are available online at https://github.com/WladimirSidorenko/SentiLex. 1 Introduction Sentiment lexicons play a crucial role in many existing and emerging opinion mining applications. Not only do they serve as a valuable source of features for supervised classifiers (Mohammad et al., 2013; Zhu et al., 2014) but they also achieve competitive results when used as the main component of a sentiment analysis system (Taboada et al., 2011). Due to this high impact and tremendous costs of building such lexicons manually, devising new algorithms for an automatic generation of polarity lists has always been an area of active research in the sentiment analysis literature (Liu, 2012, pp. 79-91). Nevertheless, despite some obvious progress in this field (Cambria et al., 2016), the applicability of these approaches to other languages and text genres still raises questions: It is, for instance, unclear whether simply translating the existing English sentiment resources would produce better results than applying the methods that were in"
W16-4309,P05-1017,0,0.763022,", and F1 -score for each particular polarity class (positive, negative, and neutral), considering all words absent in the lexicons (not annotated in the corpus) as neutral. 4 Semi-Automatic Lexicons We first apply the above metric to estimate the quality of the existing German resources: the German Polarity Clues (GPC; Waltinger, 2010), SentiWS (SWS; Remus, 2010), and the Zurich Polarity List (ZPL) of Clematide and Klenner (2010). The GPC set comprises 10,141 subjective entries automatically translated from the English sentiment lexicons Subjectivity Clues (Wilson et al., 2005) and SentiSpin (Takamura et al., 2005), with a subsequent manual correction of these translations, and several synonyms and negated terms added by the authors. The SWS lexicon includes 1,818 positively and 1,650 negatively connoted terms, also providing their part-of-speech tags and inflections (resulting in a total of 32,734 word forms). Similarly to the GPC, the authors used an English sentiment resource—the GI lexicon of Stone et al. (1966)—to bootstrap their polarity list, manually revising these automatic translations afterwards. In addition to that, Remus et al. (2010) also expanded their set with words and phrases frequentl"
W16-4309,N10-1119,0,0.754735,"matic decrease of method’s scores after the fifth iteration). 5.2 Corpus-Based Approaches An alternative way to generate polarity lists is to use corpus-based approaches. In contrast to dictionarybased methods, these systems typically operate immediately on raw texts and are, therefore, virtually independent of any manually annotated linguistic resources. This flexibility, however, might come at the cost of a reduced accuracy due to an inherent noisiness of the unlabeled data. The most prominent representatives of this class of algorithms are the approaches proposed by Takamura et al. (2005), Velikovich et al. (2010), Kiritchenko et al. (2014), and Severyn and Moschitti (2015), which we briefly describe in this section. 8 All translated seed sets are provided along with the source code for this paper. 84 Drawing on the pioneering work of Hatzivassiloglou and McKeown (1997), in which the authors expanded an initial list of polar adjectives by analyzing coordinately conjoined terms from a text corpus, Takamura et al. (2005) enhanced this algorithm, extending it to other parts of speech and also incorporating semantic links from W ORD N ET in addition to the co-occurrence statistics extracted from the corpus"
W16-4309,waltinger-2010-germanpolarityclues,0,0.262379,"nal Modeling of People’s Opinions, Personality, and Emotions in Social Media, pages 80–90, Osaka, Japan, December 12 2016. (most of which were semi-automatically translated from popular English resources) with the results of common automatic dictionary- and corpus-based SLG approaches. We begin our study by describing the data set which will be used in our evaluation. Afterwards, in Section 3, we introduce the metrics with which we will assess the quality of various polarity lists. Then, in Section 4, we evaluate three most popular existing German sentiment lexicons—the German Polarity Clues (Waltinger, 2010), SentiWS (Remus et al., 2010), and Zurich Polarity List of Clematide and Klenner (2010), subsequently comparing them with popular automatic SLG approaches in Section 5. Finally, after estimating the impact of different seed sets on the automatic methods and performing a qualitative analysis of their entries, we draw our conclusions and outline directions for future research in the final part of this paper. To avoid unnecessary repetitions, we deliberately omit a summary of related work, since most of the popular SLG algorithms will be referenced in the respective evaluation sections anyway. W"
W16-4309,H05-1044,0,0.111111,"y, we estimate the precision, recall, and F1 -score for each particular polarity class (positive, negative, and neutral), considering all words absent in the lexicons (not annotated in the corpus) as neutral. 4 Semi-Automatic Lexicons We first apply the above metric to estimate the quality of the existing German resources: the German Polarity Clues (GPC; Waltinger, 2010), SentiWS (SWS; Remus, 2010), and the Zurich Polarity List (ZPL) of Clematide and Klenner (2010). The GPC set comprises 10,141 subjective entries automatically translated from the English sentiment lexicons Subjectivity Clues (Wilson et al., 2005) and SentiSpin (Takamura et al., 2005), with a subsequent manual correction of these translations, and several synonyms and negated terms added by the authors. The SWS lexicon includes 1,818 positively and 1,650 negatively connoted terms, also providing their part-of-speech tags and inflections (resulting in a total of 32,734 word forms). Similarly to the GPC, the authors used an English sentiment resource—the GI lexicon of Stone et al. (1966)—to bootstrap their polarity list, manually revising these automatic translations afterwards. In addition to that, Remus et al. (2010) also expanded thei"
W16-4309,S14-2077,0,0.0136918,"9), and that dictionary-based techniques produce much better polarity lists as compared to corpus-based approaches (whose best F1 -scores run up to 0.479 and 0.419 respectively) even for the non-standard Twitter genre. All reimplementations of the compared systems and the resulting lexicons of these methods are available online at https://github.com/WladimirSidorenko/SentiLex. 1 Introduction Sentiment lexicons play a crucial role in many existing and emerging opinion mining applications. Not only do they serve as a valuable source of features for supervised classifiers (Mohammad et al., 2013; Zhu et al., 2014) but they also achieve competitive results when used as the main component of a sentiment analysis system (Taboada et al., 2011). Due to this high impact and tremendous costs of building such lexicons manually, devising new algorithms for an automatic generation of polarity lists has always been an area of active research in the sentiment analysis literature (Liu, 2012, pp. 79-91). Nevertheless, despite some obvious progress in this field (Cambria et al., 2016), the applicability of these approaches to other languages and text genres still raises questions: It is, for instance, unclear whether"
W17-1506,P15-1039,0,0.0242451,"Missing"
W17-1506,postolache-etal-2006-transferring,0,0.320523,"NP chunkers and morphological analyzers for different languages. In particular, they used labelled English data and an aligned parallel corpus to automatically create mappings between the annotations from the source side and the corresponding aligned words on the target side, and exploited the resulting annotations to train their systems. Thereafter, projection has been widely used as a method in cross-lingual NLP, and several studies on annotation projection targeted cross-lingual coreference resolution. In particular, automatic annotation transfer was first applied to coreference chains by Postolache et al. (2006) who used a projection method and filtering heuristics to support the creation of a coreference corpus in a new language. The evaluation of projected annotations against a small manually annotated corpus exhibited promising 63.88 and 82.6 MUC and Bcubed scores respectively. Subsequently, Souza and Or˘asan (2011) went one step further and made an attempt to project automatically produced annotations, and used projected data to train a new coreference resolver, which, however, resulted in a poor coreference resolution quality due to lowquality annotations on the source side. The next steps in pr"
W17-1506,N06-2015,0,0.184696,"Missing"
W17-1506,W15-3403,1,0.805867,"ge, on which coreference resolution is performed; after that, the source coreference chains can be projected back to the target side. This approach was used, for example, by Rahman and Ng (2012) to train coreference resolvers for Spanish and Italian using English as the source language, achieving an average F1 of 37.6 and 21.4 for Spanish and Italian respectively in a low-resource scenario, and much better scores of 46.8 and 54.9 F1 using only a mention extractor. 3 Data For our experiments, we have chosen a trilingual parallel annotated coreference corpus of English, German and Russian from (Grishina and Stede, 2015). This corpus was annotated with coreference chains according to the guidelines described in (Grishina and Stede, 2016) which are largely compatible to the coreference annotations of the OntoNotes corpus (Pradhan and Xue, 2009). The corpus is annotated with full coreference chains, excluding singletons1 . The major differences to OntoNotes are: (a) annotation of NPs only, but not of verbs that are coreferent with NPs, (b) inclusion of appositions into the markable span and not marking them as a separate relation, (c) marking relative pronouns as separate markables, and (d) 1 42 Mentions of the"
W17-1506,W12-4501,0,0.0642466,"lia Grishina and Manfred Stede Applied Computational Linguistics FSP Cognitive Science University of Potsdam grishina|stede@uni-potsdam.de Abstract formed quite well cross-lingually, e.g. for semantic role labelling (Akbik et al., 2015) or syntactic parsing (Lacroix et al., 2016). At the same time, several recent studies on annotation projection for coreference have proven it to be a more difficult task than POS tagging or syntactic parsing, which is hard to be tackled by projection algorithms. These works are limited to the existing multilingual resources (mostly newswire, mostly CoNLL 2012 (Pradhan et al., 2012)) and, surprisingly, are not even able to beat a threshold of 40.0 F1 for coreference resolvers trained on projections only. The best-performing system based on projection achieves 38.82 for English-Spanish and 37.23 for English-Portuguese F1-score (Martins, 2015), while state-of-the-art monolingual coreference systems are already able to achieve 64.21 Fscore for English (Wiseman et al., 2016). While being quite powerful for other tasks, annotation projection is less successful for coreference resolution. Therefore, our question is, how can the quality of annotation projection be improved for"
W17-1506,P16-2091,0,0.0418886,"Missing"
W17-1506,N12-1090,0,0.303333,"step further and made an attempt to project automatically produced annotations, and used projected data to train a new coreference resolver, which, however, resulted in a poor coreference resolution quality due to lowquality annotations on the source side. The next steps in projecting coreference included several translation-based approaches. The difference is that the target text is first translated into the source language, on which coreference resolution is performed; after that, the source coreference chains can be projected back to the target side. This approach was used, for example, by Rahman and Ng (2012) to train coreference resolvers for Spanish and Italian using English as the source language, achieving an average F1 of 37.6 and 21.4 for Spanish and Italian respectively in a low-resource scenario, and much better scores of 46.8 and 54.9 F1 using only a mention extractor. 3 Data For our experiments, we have chosen a trilingual parallel annotated coreference corpus of English, German and Russian from (Grishina and Stede, 2015). This corpus was annotated with coreference chains according to the guidelines described in (Grishina and Stede, 2016) which are largely compatible to the coreference a"
W17-1506,N16-1121,0,0.0355168,"Missing"
W17-1506,D15-1039,0,0.114333,"e resolution systems trained on projections for Spanish and 37.23 for Portuguese, as compared to the performance of fully supervised systems: 43.93 and 39.83 respectively. The idea of using multiple sources for annotation projection was also initially considered by Yarowsky et al. (2001) who used multiple translations of the same text to improve the performance of the projected annotations for several NLP tasks. Furthermore, multi-source projection has been extensively explored for multilingual syntactic parsing. The best unsupervised dependency parsers nowadays rely on annotation projection (Rasooli and Collins, 2015; Johannsen et al., 2016). To our knowledge, there has been no attempt to apply multi-source annotation projection to the task of coreference resolution so far. for our experiments is explained in Section 4. We then analyse the projection errors and evaluate the target annotations (Section 5). Finally, Section 6 summarises the outcomes of this study, and Section 7 concludes. 2 Related work Annotation projection is a method of automatically transferring linguistic annotations from one language to the other in a parallel corpus. It was first applied in the pilot work of Yarowski et al. (2001) wh"
W17-1506,P15-1138,0,0.408646,"l., 2016). At the same time, several recent studies on annotation projection for coreference have proven it to be a more difficult task than POS tagging or syntactic parsing, which is hard to be tackled by projection algorithms. These works are limited to the existing multilingual resources (mostly newswire, mostly CoNLL 2012 (Pradhan et al., 2012)) and, surprisingly, are not even able to beat a threshold of 40.0 F1 for coreference resolvers trained on projections only. The best-performing system based on projection achieves 38.82 for English-Spanish and 37.23 for English-Portuguese F1-score (Martins, 2015), while state-of-the-art monolingual coreference systems are already able to achieve 64.21 Fscore for English (Wiseman et al., 2016). While being quite powerful for other tasks, annotation projection is less successful for coreference resolution. Therefore, our question is, how can the quality of annotation projection be improved for the task of coreference resolution? In our opinion, projection from multiple source languages can be a long-term solution, assuming that we have access to two or more reliable coreference resolvers on the source sides. Our idea is that multi-source annotation proj"
W17-1506,J03-1002,0,0.0180379,"Missing"
W17-1506,N16-1114,0,0.03085,"Missing"
W17-1506,H01-1035,0,0.139543,"transferring automatically produced coreference chains from English to Spanish and Portuguese, and subsequently trained target coreference resolvers on the projected data, combining projection with posterior regularization. His approach shows competitive results in a low-resource setting, with the average of 38.82 F1 for coreference resolution systems trained on projections for Spanish and 37.23 for Portuguese, as compared to the performance of fully supervised systems: 43.93 and 39.83 respectively. The idea of using multiple sources for annotation projection was also initially considered by Yarowsky et al. (2001) who used multiple translations of the same text to improve the performance of the projected annotations for several NLP tasks. Furthermore, multi-source projection has been extensively explored for multilingual syntactic parsing. The best unsupervised dependency parsers nowadays rely on annotation projection (Rasooli and Collins, 2015; Johannsen et al., 2016). To our knowledge, there has been no attempt to apply multi-source annotation projection to the task of coreference resolution so far. for our experiments is explained in Section 4. We then analyse the projection errors and evaluate the"
W17-3602,prasad-etal-2008-penn,0,0.031169,"English, we present an analysis of such possible disagreements, and propose an underspeciﬁed representation that captures the disagreements. 1 Introduction The past ten years have seen continuous interest in RST-oriented discourse parsing, which aims at automatically deriving a complete and well-formed tree representation over coherence relations assigned to adjacent spans of text. For various downstream applications (e.g., summarization, essay scoring), such a complete structure is more useful than the purely localized assignment of individual relations, as it is done in PDTB-style analysis (Prasad et al., 2008). At the same time, it is well known that RST parsing is difﬁcult, and furthermore, it is more difﬁcult to achieve good human agreement on RST trees, as compared to PDTB annotation. This latter problem has not been in the spotlight of attention, though, while the computational linguistics community developed a series of parsing approaches over the years (Hernault et al., 2010; Ji and Eisenstein, 2013; Feng and Hirst, 2014; Braud et al., 2016). Part of the reason for the focus on dataManfred Stede Dept. of Linguistics University of Potsdam Potsdam, Germany stede@uni-potsdam.de oriented automati"
W17-3602,C16-1179,0,0.0156899,"y scoring), such a complete structure is more useful than the purely localized assignment of individual relations, as it is done in PDTB-style analysis (Prasad et al., 2008). At the same time, it is well known that RST parsing is difﬁcult, and furthermore, it is more difﬁcult to achieve good human agreement on RST trees, as compared to PDTB annotation. This latter problem has not been in the spotlight of attention, though, while the computational linguistics community developed a series of parsing approaches over the years (Hernault et al., 2010; Ji and Eisenstein, 2013; Feng and Hirst, 2014; Braud et al., 2016). Part of the reason for the focus on dataManfred Stede Dept. of Linguistics University of Potsdam Potsdam, Germany stede@uni-potsdam.de oriented automatic parsing is the availability of the RST Discourse Treebank (Carlson et al., 2003), a corpus large enough to supply training/test data in supervised machine learning (ML). The central thesis of our paper is that the fundamental questions of RST annotation and agreement deserve to be re-opened. With powerful ML and parsing technology in place, it is timely to give more attention to the nature of the underlying data, and to its descriptive and"
W17-3602,W03-2411,1,0.715598,"framework for technically representing alternative analyses in Section 5. A brief summary (Section 6) concludes the paper. 2 The problem of ambiguity was again studied by Schilder (2002), who worked in the framework of Segmented Discourse Representation Theory or SDRT (Asher and Lascarides, 2003) and approached the problem from a semantic viewpoint. He proposed that certain aspects of the analysis could be left unannotated. For instance, nuclearity may be assigned, but the speciﬁc relation between nucleus and satellite may be left blank, if a decision cannot be reached. Around the same time, Reitter and Stede (2003) proposed the Underspeciﬁed Rhetorical Markup Language (URML), an XML language for encoding competing analyses in a single representation. We will describe this in more detail in Section 5. More recently, Iruskieta et al. (2015) proposed a qualitative method for analysis comparison, teasing apart constituency, relation, and attachment. The most important aspect of their comparison method is that nuclearity and relation label are separated, unlike in Marcu’s quantitative agreement metric. Related work In Computational Linguistics, a discussion on ambiguity in RST started shortly after Mann and"
W17-3602,P14-1048,0,0.0351796,"., summarization, essay scoring), such a complete structure is more useful than the purely localized assignment of individual relations, as it is done in PDTB-style analysis (Prasad et al., 2008). At the same time, it is well known that RST parsing is difﬁcult, and furthermore, it is more difﬁcult to achieve good human agreement on RST trees, as compared to PDTB annotation. This latter problem has not been in the spotlight of attention, though, while the computational linguistics community developed a series of parsing approaches over the years (Hernault et al., 2010; Ji and Eisenstein, 2013; Feng and Hirst, 2014; Braud et al., 2016). Part of the reason for the focus on dataManfred Stede Dept. of Linguistics University of Potsdam Potsdam, Germany stede@uni-potsdam.de oriented automatic parsing is the availability of the RST Discourse Treebank (Carlson et al., 2003), a corpus large enough to supply training/test data in supervised machine learning (ML). The central thesis of our paper is that the fundamental questions of RST annotation and agreement deserve to be re-opened. With powerful ML and parsing technology in place, it is timely to give more attention to the nature of the underlying data, and to"
W17-3602,P09-2020,1,0.864988,"t critical for the purposes of this paper. For the German experiment, we used the annotation guidelines developed for the Potsdam Commentary Corpus (Stede, 2016) and which are publicly available. Then, for annotating the English texts, we produced an English version of those guidelines and made minimal changes to the descriptions of relations (clariﬁcations on how to distinguish between certain contrastive and argumentative relations). Further, we used languagespeciﬁc segmentation guidelines that we borrowed from the implementation of SLSeg (syntactic and lexically based discourse segmenter) (Toﬁloski et al., 2009).2 In addition to many individual examples for the relations, the guidelines ﬁnish with a sample analysis of a complete text with 14 elementary discourse units (EDUs). The guidelines merely guide the annotators in their task. They could in principle be written in such a way as to “strongly encourage” agreement when cases of ambiguity arise (e.g., by specifying preference hierarchies), but they make only minimal use of that move. The interesting issue from a theoretical viewpoint is that the same general guidelines can give rise to what we consider as legitimate disagreements. 3.2 forth: GE1 an"
W17-3602,D13-1090,0,0.0269534,"nstream applications (e.g., summarization, essay scoring), such a complete structure is more useful than the purely localized assignment of individual relations, as it is done in PDTB-style analysis (Prasad et al., 2008). At the same time, it is well known that RST parsing is difﬁcult, and furthermore, it is more difﬁcult to achieve good human agreement on RST trees, as compared to PDTB annotation. This latter problem has not been in the spotlight of attention, though, while the computational linguistics community developed a series of parsing approaches over the years (Hernault et al., 2010; Ji and Eisenstein, 2013; Feng and Hirst, 2014; Braud et al., 2016). Part of the reason for the focus on dataManfred Stede Dept. of Linguistics University of Potsdam Potsdam, Germany stede@uni-potsdam.de oriented automatic parsing is the availability of the RST Discourse Treebank (Carlson et al., 2003), a corpus large enough to supply training/test data in supervised machine learning (ML). The central thesis of our paper is that the fundamental questions of RST annotation and agreement deserve to be re-opened. With powerful ML and parsing technology in place, it is timely to give more attention to the nature of the u"
W17-3602,J92-4007,0,0.48429,"in a single representation. We will describe this in more detail in Section 5. More recently, Iruskieta et al. (2015) proposed a qualitative method for analysis comparison, teasing apart constituency, relation, and attachment. The most important aspect of their comparison method is that nuclearity and relation label are separated, unlike in Marcu’s quantitative agreement metric. Related work In Computational Linguistics, a discussion on ambiguity in RST started shortly after Mann and Thompson (1988) was published, mostly in the Natural Language Generation community. The well-known proposal by Moore and Pollack (1992) argued that certain text passages can systematically have two different analyses, one drawing on the intentional, the other on the subjectmatter (informational) subset of coherence relations. In a pair of two sentences, for example, when the ﬁrst states a subjective claim, the second might be interpreted as E VIDENCE for the ﬁrst, or as merely providing E LABORATION. Moore and Pollack also gave examples where the alternative analyses coincide with conﬂicting nuclearity assignments. These questions were never really resolved; instead, with the availability of the RST Discourse Treebank (RST-DT"
W17-3602,W00-1434,0,0.424503,"Missing"
W17-3602,W01-1605,0,\N,Missing
W17-5801,W11-1701,0,0.0254572,"ducted. For constructing the corpus used here, a number of decisions had to be made. In the following sections we reflect on these decisions and make a number of suggestions for future studies. 5.1 against for undec. Classified as: against for 275 148 137 240 101 89 against for 0.54 0.53 0.56 0.50 thread. Taking the context into account for determining the stance would, however, also entail a more complex classification task. At least in a first step in future work, we will therefore concentrate on the types of debate posts that can be interpreted without context. Research on online forums by Anand et al. (2011) has shown that the incorporation of features from the previous post can improve the performance of a stance classifier for posts that express rebuttals. This work, however, used meta-data of the debaters’ stance for training the classifiers and did not provide the option to classify a post as undecided when its stance could not be determined without its debate context. Another possible approach to take would be to classify debaters according to the stance they take, instead of classifying each individual post into a stance. The set of debate posts written by one debater would then be treated"
W17-5801,J08-4004,0,0.0507291,"f posts in online forums can be improved by also taking classifier output from other posts by the same author into account (Hasan and Ng, 2013). On the same assumption that debaters do not change their stance, it might also be possible to give some kind of measure of the validity of the stance annotations. If the annotations are to be considered valid, posts from the same debater should ideally always take the same stance, or at least only exceptionally take the opposite stance. Such a measure could be used as a complement to annotator agreement that measures reliability rather than validity (Artstein and Poesio, 2008). Annotator agreement should, however, also be measured. Previous studies on stance do not reason to a Annotation decisions The decision to treat each debate post as one independent unit without taking its context into account is not self-evident, as a post is more meaningful to interpret in the context of its discussion 5 5.2 large extent on where to draw the line between the categories against and for the target. In the case of previous vaccination stance studies, this might be explained by that these studies have focused on attitudes towards one specific type of vaccine; whereas stance towa"
W17-5801,D16-1084,0,0.0259379,"Missing"
W17-5801,W02-0109,0,0.0680965,"r to indicate that a comment to this previous post is made (the posts are all posted on the same level, and there is no functionality for posting an answer to a specific previous post). Although the debaters do not use a uniform approach to indicate that text has been copied from another debater, we devised a simple method for removing as many instances as possible of copied text. Paragraphs that were exclusively constructed of sentences that had occurred in previous posts were removed, using the standard sentence segmentation included in NLTK (Natural Language Toolkit) for sentence matching (Bird, 2002). In addition, text chunks longer than three words that were marked in bold or by double quotation were removed, in order to exclude citations in general, from other debaters as well as from external sources. Also names of opponents are sometimes mentioned in the posts, as a means to indicate that the content of the post is addressed to a specific opponent debater. These names were also automatically removed. Similar to previous vaccination studies on tweets, we considered the content of each debate post without the context of surrounding posts. To increase the likelihood that the debater’s st"
W17-5801,N16-1138,0,0.0360506,"Missing"
W17-5801,I13-1191,0,0.198169,"nual analyses are not enough. Instead, the functionality that we investigate here is required, that is to be able to automatically detect stance towards vaccination. While previous studies on detection of stance/sentiment towards specific types of vaccines in tweets have have been carried out, we here aim to investigate the possibility of automatic vaccine stance detection in the important genre of online discussion forums. 1 For these two studies, F-score refers to macro F-score calculated over five and four different stance targets, respectively. This figure was not reported in the paper by Hasan and Ng (2013), but was calculated here from the best results reported for each target. From experiments by Hasan and Ng (2013) that included non-textual features and information from other posts from the same debater, better results than these have been reported. A corpus was first compiled and pre-processed, and thereafter annotated for stance towards vaccination. The annotated corpus was then used to train models to detect the stance categories.2 3 Method 2 The code used for the experiments, as well as the annotation and post meta-data (Mumsnet ID, debater, debate thread) 2 3.1 Corpus selection their div"
W17-5801,W10-0214,0,0.0996146,"Missing"
W17-5801,N12-1072,0,0.0495109,"Missing"
W17-5801,P06-4018,0,\N,Missing
W17-6802,P07-1056,0,0.0802351,"ot include context-independent polar words that belong in standard sentiment (polarity) dictionaries. With a polar fact, an author gives a description of some state of affairs, which prima facie appears to be an objective statement, but for the particular target at hand (or more precisely, all targets of its class) entails a polar opinion. They have been studied for product reviews (Toprak et al., 2010) and minutes of meetings (Wilson, 2008), but are also relevant in many other genres such as political or legal discourse. By their nature, polar fact expressions are domain-specific (see, e.g., Blitzer et al. (2007)). Therefore, it is important to be able to acquire the vocabulary for a domain when high-quality sentiment analysis is to be applied. In this paper, we provide a comparison of several variants of a seed-list approach to generating lists of such lexical items. The starting point is a list of standard opinion words, which we use as seeds to extract collocating polar fact words and phrases from their contexts. The genre we tackle is customer emails, and our data comes from four different content domains, which we illustrate with examples: • Fashion: “The seam’s coming undone.” • Food: “Those coo"
W17-6802,S14-2076,0,0.0276076,"exicons and contributed significantly to the performance in the overall task of computing aspect-based sentiment (lexicons as a whole increased the F-score by 8 points). Participating in the same SemEval tasks on Twitter sentiment, Severyn and Moschitti (2015) use a distant supervision method: A large Twitter corpus with noisy polarity labels (inferred from hashtags and emoticons) is mapped to a lexicon of labeled unigrams and bigrams from those Tweets. Then an SVM classifier is trained on these lexical features. The authors show that their method outperforms the PMI-induced lexicon method of Kiritchenko et al. (2014) on the same datasets. Similarly, Vo and Zhang (2016) show that a prediction-based neural network implementation yields a better accuracy than the counting-based method of Mohammad et al. (2013) when comparing to an existing (manually-annotated) “gold” lexicon. For German, Sidarenka and Stede (2016) compare two families of methods for generating sentiment lexicons and evaluate them on Twitter data: dictionary-based methods starting from WordNet, and corpus-based methods, including the two mentioned above. They found that dictionary-based methods generally outperform corpus-based ones, and that"
W17-6802,S13-2053,0,0.0311284,"emEval tasks on Twitter sentiment, Severyn and Moschitti (2015) use a distant supervision method: A large Twitter corpus with noisy polarity labels (inferred from hashtags and emoticons) is mapped to a lexicon of labeled unigrams and bigrams from those Tweets. Then an SVM classifier is trained on these lexical features. The authors show that their method outperforms the PMI-induced lexicon method of Kiritchenko et al. (2014) on the same datasets. Similarly, Vo and Zhang (2016) show that a prediction-based neural network implementation yields a better accuracy than the counting-based method of Mohammad et al. (2013) when comparing to an existing (manually-annotated) “gold” lexicon. For German, Sidarenka and Stede (2016) compare two families of methods for generating sentiment lexicons and evaluate them on Twitter data: dictionary-based methods starting from WordNet, and corpus-based methods, including the two mentioned above. They found that dictionary-based methods generally outperform corpus-based ones, and that – more importantly for our purposes here – the results of corpus methods depend heavily on the specific seed sets employed in the various approaches. One of the German lexicons available today,"
W17-6802,remus-etal-2010-sentiws,0,0.0230729,"s, and one of six different lexical association measures, in order to find the overall most promising combination. In the absence of a large amount of test data, we used human judgements of interim results (the union of the top 350 items suggested by the best-performing combinations) in order to produce the confirmed lexical items (many of them domain-specific) that were then used to evaluate the approach. Although not directly comparable, because different corpora are used, our method, with accuracies of 0.53, seems to be competitive to the earlier work on generating German negative words by Remus et al. (2010) (see Section 2), which achieved an accuracy of 0.495. A further difference to earlier work is our use of tuples of verb and dependents in addition to individual lexical tokens (or bigrams). These tuples represent a step toward syntactically-motivated analysis while avoiding the full parsing problem (which we found to be quite hard for the email genre). The lists generated by our best-performing settings contain many such tuples, which indicates the potential of this approach. Further improving this minimal syntactic analysis (e.g., by handling verb suffixes) is one of our goals for the future"
W17-6802,N15-1159,0,0.0463364,"Missing"
W17-6802,W16-4309,1,0.853767,"et and provide evaluations only for that. The basis for our approach is a seed list of negative words, and we experiment with two variants here: The first is a list of 170 domain-independent negative German words (lemmas) that we compiled manually from various sources, targeting specifically the customer-care email genre. Some translated examples from this list are: unsatisfactory, unpleasant, dirty, pointless, weak, fault, unfortunately, unreliable, sad. The second was obtained from automatically computing the intersection of negative entries in three existing German sentiment lexicons, see (Sidarenka and Stede, 2016). It consists of 9004 words. Henceforth, we abbreviate these lists as NEG-170 and NEG-INTER, respectively. For computing the polarity of segments, we also use a list of manually-compiled of positive words, POS-100. The central goal of the experiments is to assess the influence of • two different ways of obtaining a seed list for negative words, • different notions of “minimal unit” for the polarity analysis (in the related work above, these were always complete Tweets; we need a more elaborate definition), and • different measures for computing lexical association. 4.1 Preprocessing Our pipeli"
W17-6802,P10-1059,0,0.0274373,"correlation tests. 1 Introduction One interesting and difficult subtask of sentiment analysis is the automatic recognition of so-called implicit opinions or polar facts: Statements that express a valuation yet do not include context-independent polar words that belong in standard sentiment (polarity) dictionaries. With a polar fact, an author gives a description of some state of affairs, which prima facie appears to be an objective statement, but for the particular target at hand (or more precisely, all targets of its class) entails a polar opinion. They have been studied for product reviews (Toprak et al., 2010) and minutes of meetings (Wilson, 2008), but are also relevant in many other genres such as political or legal discourse. By their nature, polar fact expressions are domain-specific (see, e.g., Blitzer et al. (2007)). Therefore, it is important to be able to acquire the vocabulary for a domain when high-quality sentiment analysis is to be applied. In this paper, we provide a comparison of several variants of a seed-list approach to generating lists of such lexical items. The starting point is a list of standard opinion words, which we use as seeds to extract collocating polar fact words and ph"
W17-6802,P16-2036,0,0.0469607,"Missing"
W17-6802,wilson-2008-annotating,0,0.0396809,"ing and difficult subtask of sentiment analysis is the automatic recognition of so-called implicit opinions or polar facts: Statements that express a valuation yet do not include context-independent polar words that belong in standard sentiment (polarity) dictionaries. With a polar fact, an author gives a description of some state of affairs, which prima facie appears to be an objective statement, but for the particular target at hand (or more precisely, all targets of its class) entails a polar opinion. They have been studied for product reviews (Toprak et al., 2010) and minutes of meetings (Wilson, 2008), but are also relevant in many other genres such as political or legal discourse. By their nature, polar fact expressions are domain-specific (see, e.g., Blitzer et al. (2007)). Therefore, it is important to be able to acquire the vocabulary for a domain when high-quality sentiment analysis is to be applied. In this paper, we provide a comparison of several variants of a seed-list approach to generating lists of such lexical items. The starting point is a list of standard opinion words, which we use as seeds to extract collocating polar fact words and phrases from their contexts. The genre we"
W18-0701,P15-1136,0,0.0662016,"ys) interact with each other across several turns.1 Hence, anaphora resolution needs to attend both to the general and well-known problems of handling Twitter language, and potentially to aspects of conversation structure. In order to study the properties of coreference relations in these conversations, we built a corpus that is designed to represent a number of different relevant phenomena, which we selected carefully. We annotated pronouns and their antecedents, so that the data can be used for systematically testing anaphora resolvers, and we conducted experiments with the Stanford system (Clark and Manning, 2015). The paper is structured as follows: Section 2 introduces general phenomena found in Twitter conversations and describes earlier research. Section 3 discusses our approach to corpus construction 1. Pronouns referring to speakers 2. Other exophoric reference 3. Conversation structure as a factor for antecedent selection 4. Phenomena specific to spoken conversation 5. Phenomena specific to social media text Obviously, not all of these phenomena are equally relevant in all interactive dialogue settings — in fact, certain settings basically do not require attending to such phenomena. For instance"
W18-0701,W15-3403,1,0.930183,"Ls and, therefore, divides them into smaller tokens. However, for our purposes, over-tokenization (i.e., producing too many tokens) is preferred to insufficient generation of token boundaries, because annotation tools (see below) can handle markables containing more than one token, but they do not allow for selecting a substring of a token as a markable. 3.3 Data Preparation Annotation In our annotation scheme, we so far consider only the identity relation. With tweets being structurally relatively simple, we were interested in lean annotation guidelines, and followed the strategy defined in (Grishina and Stede, 2015), with some modifications in the treatment of predicative nouns and appositives. In our scheme, predicative nouns and appositions are considered as markables indicating reference identity. We defined additional attributes to differentiate these markables (i.e., copula constructions and appositives) from the other mentions. Also, we annotate the structural relation (anaphora, cataphora and exophora) of the pronouns, in order to cover the phenomena we will explain in Section 4. For exophora, additional more fine-grained categories are used: It is well known that tokenization is a crucial prepara"
W18-0701,D11-1141,0,0.252735,"Missing"
W18-0701,W04-2310,0,0.0764438,"eraction features. 2.1 Not much work has been done on speech-specific features for anaphora resolution; we mention here the influence of hesitations that Schlangen et al. (2009) studied for referring to Pentomino pieces. The potential connection to Twitter is the fact that Twitter users often borrow from speech, for example emphasis markers such as vowel lengthening (honeyyyy) and hesitation markers (hmm). Reference to Speakers In addition to using proper names, speakers can refer to one another using pronouns, and several early systems implemented simple rules for resolving I and you (e.g., (Jain et al., 2004)). In multilogue, it is also possible that third-person pronouns he/she refer to conversation participants; we are not aware of systems addressing this. 2.2 2.5 Social media text The need for pre-processing Twitter text is widely known and not specific to anaphora resolution. As just one example, Ritter et al. (2011) worked on Named-Entity Recognition on Tweets. They show that performance can be significantly improved when a dedicated preprocessing pipeline is employed. But we are not aware of Twitter-specific work on coreference or anaphora. Finally, we mention an early study on threaded data"
W18-0701,W09-3905,0,0.0347636,"For an overview of constructing corpora of this kind and some annotation tasks, see (Scheffler, 2017). 1 Proceedings of the Workshop on Computational Models of Reference, Anaphora and Coreference, pages 1–10 c New Orleans, Louisiana, June 6, 2018. 2018 Association for Computational Linguistics 2.4 could carry over to conversational data, but the authors focused on non-nominal antecedents and did not emphasize the need for using additional interaction features. 2.1 Not much work has been done on speech-specific features for anaphora resolution; we mention here the influence of hesitations that Schlangen et al. (2009) studied for referring to Pentomino pieces. The potential connection to Twitter is the fact that Twitter users often borrow from speech, for example emphasis markers such as vowel lengthening (honeyyyy) and hesitation markers (hmm). Reference to Speakers In addition to using proper names, speakers can refer to one another using pronouns, and several early systems implemented simple rules for resolving I and you (e.g., (Jain et al., 2004)). In multilogue, it is also possible that third-person pronouns he/she refer to conversation participants; we are not aware of systems addressing this. 2.2 2."
W18-0701,N09-2051,0,0.052318,"Missing"
W18-0701,P03-1022,0,0.377231,"Missing"
W18-0701,P14-5010,0,0.0041768,"ositions are considered as markables indicating reference identity. We defined additional attributes to differentiate these markables (i.e., copula constructions and appositives) from the other mentions. Also, we annotate the structural relation (anaphora, cataphora and exophora) of the pronouns, in order to cover the phenomena we will explain in Section 4. For exophora, additional more fine-grained categories are used: It is well known that tokenization is a crucial preparatory step for doing any kind of NLP on tweets. We experimented with two different tokenizers: the Stanford PTBTokenizer (Manning et al., 2014) and Twokenizer (Gimpel et al., 2011). It turned out that these systems have different strengths in handling the variety of challenges, such as: • PTBTokenizer decides whether to split at apostrophes (whereas Twokenizer does not). For example: 3 Sharing a tweet by adding new content ”on top” of it: https://developer.twitter.com/en/docs/tweets/datadictionary/overview/entities-object 3 Threads: Coreference chains: Annotated mentions: Annotated pronouns: Annotated predicative NPs: Length of longest coreference chain: Average length of coreference chains: Median length of coreference chains: Intra"
W18-5037,prasad-etal-2008-penn,0,0.263218,"tively small) German annotated corpus, the Potsdam Commentary Corpus. We introduce new features and experiment with including additional training data obtained through annotation projection and achieve an f-score of 83.89. 1 2 Introduction Related Work Early attempts at formalizing discourse parsing procedures for English are described in (Soricut and Marcu, 2003), among others. Pitler and Nenkova (2009) experiment with syntactically motivated features for the binary classification of discourse connectives (connective or nonconnective reading) and report an f-score of 94.19 for the PDTB data (Prasad et al., 2008). The SDP pipeline architecture is adopted from Lin et al. (2014) and is also used in the best-scoring systems of the 2015 and 2016 CONLL shared tasks, (Wang and Lan, 2015) and (Oepen et al., 2016) respectively. Oepen et al. (2016) achieve an overall f-score of 27.77 for full SDP, but 91.79 for identifying explicit connectives. The best-scoring system for this subtask (Li et al., 2016) achieved an impressive 98.38. A notable drawback of the pipeline architecture is the possibility of error propagation. This is addressed by (Biran and McKeown, 2015), who use A task central to the field of Disco"
W18-5037,W08-1006,0,0.423636,"Missing"
W18-5037,W15-4612,0,0.0197679,"and report an f-score of 94.19 for the PDTB data (Prasad et al., 2008). The SDP pipeline architecture is adopted from Lin et al. (2014) and is also used in the best-scoring systems of the 2015 and 2016 CONLL shared tasks, (Wang and Lan, 2015) and (Oepen et al., 2016) respectively. Oepen et al. (2016) achieve an overall f-score of 27.77 for full SDP, but 91.79 for identifying explicit connectives. The best-scoring system for this subtask (Li et al., 2016) achieved an impressive 98.38. A notable drawback of the pipeline architecture is the possibility of error propagation. This is addressed by (Biran and McKeown, 2015), who use A task central to the field of Discourse Processing is the uncovering of coherence relations that hold between individual (elementary) units of a text. When discourse relations are explicitly signaled in a text, the explicit markers are called (discourse) connectives. Connectives can be two-way ambiguous in the sense of having either a discourse or a sentential reading, and if they have a discourse reading, many can assign multiple senses. Further, connectives form a syntactically heterogeneous group and include coordinating and subordinating conjunctions, adverbials, and depending o"
W18-5037,L16-1160,1,0.855508,"es 327–331, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics a tagging-based approach and divide the task into processing intra-sentential and inter-sentential relations (as opposed to the more typical division into explicit and implicit relations) and report a final f-score of 39.33. This is based on a more lenient scoring system though, and Oepen et al. (2016) achieve 44.20 using a similar partial matching scoring system. The main resources available for German are DiMLex, a lexicon of German discourse connectives containing 275 entries (Stede, 2002), (Scheffler and Stede, 2016) and the Potsdam Commentary Corpus (PCC) (Stede and Neumann, 2014), described in more detail in Section 3. We experiment with generating extra training data through annotation projection. This approach is inspired by Versley (2010), who attempts to disambiguate German connectives using a parallel EnglishGerman corpus. Earlier work on connective identification for German is done by (Dipper and Stede, 2006), who train the Brill Tagger using a modified tag set and consider only 9 of the 42 ambiguous entries in DiMLex, reporting an f-score of 90.20. In our present study, we deal with the full set"
W18-5037,W08-0509,0,0.0184212,"Missing"
W18-5037,N03-1030,0,0.359479,"end Shallow Discourse Parsing system for German and in this paper focus on the first subtask: the identification of explicit connectives. Starting with the feature set from an English system and a Random Forest classifier, we evaluate our approach on a (relatively small) German annotated corpus, the Potsdam Commentary Corpus. We introduce new features and experiment with including additional training data obtained through annotation projection and achieve an f-score of 83.89. 1 2 Introduction Related Work Early attempts at formalizing discourse parsing procedures for English are described in (Soricut and Marcu, 2003), among others. Pitler and Nenkova (2009) experiment with syntactically motivated features for the binary classification of discourse connectives (connective or nonconnective reading) and report an f-score of 94.19 for the PDTB data (Prasad et al., 2008). The SDP pipeline architecture is adopted from Lin et al. (2014) and is also used in the best-scoring systems of the 2015 and 2016 CONLL shared tasks, (Wang and Lan, 2015) and (Oepen et al., 2016) respectively. Oepen et al. (2016) achieve an overall f-score of 27.77 for full SDP, but 91.79 for identifying explicit connectives. The best-scoring"
W18-5037,2005.mtsummit-papers.11,0,0.112871,"Missing"
W18-5037,K16-2008,0,0.126638,"Nenkova (2009) experiment with syntactically motivated features for the binary classification of discourse connectives (connective or nonconnective reading) and report an f-score of 94.19 for the PDTB data (Prasad et al., 2008). The SDP pipeline architecture is adopted from Lin et al. (2014) and is also used in the best-scoring systems of the 2015 and 2016 CONLL shared tasks, (Wang and Lan, 2015) and (Oepen et al., 2016) respectively. Oepen et al. (2016) achieve an overall f-score of 27.77 for full SDP, but 91.79 for identifying explicit connectives. The best-scoring system for this subtask (Li et al., 2016) achieved an impressive 98.38. A notable drawback of the pipeline architecture is the possibility of error propagation. This is addressed by (Biran and McKeown, 2015), who use A task central to the field of Discourse Processing is the uncovering of coherence relations that hold between individual (elementary) units of a text. When discourse relations are explicitly signaled in a text, the explicit markers are called (discourse) connectives. Connectives can be two-way ambiguous in the sense of having either a discourse or a sentential reading, and if they have a discourse reading, many can assi"
W18-5037,stede-neumann-2014-potsdam,1,0.861942,"n for Computational Linguistics a tagging-based approach and divide the task into processing intra-sentential and inter-sentential relations (as opposed to the more typical division into explicit and implicit relations) and report a final f-score of 39.33. This is based on a more lenient scoring system though, and Oepen et al. (2016) achieve 44.20 using a similar partial matching scoring system. The main resources available for German are DiMLex, a lexicon of German discourse connectives containing 275 entries (Stede, 2002), (Scheffler and Stede, 2016) and the Potsdam Commentary Corpus (PCC) (Stede and Neumann, 2014), described in more detail in Section 3. We experiment with generating extra training data through annotation projection. This approach is inspired by Versley (2010), who attempts to disambiguate German connectives using a parallel EnglishGerman corpus. Earlier work on connective identification for German is done by (Dipper and Stede, 2006), who train the Brill Tagger using a modified tag set and consider only 9 of the 42 ambiguous entries in DiMLex, reporting an f-score of 90.20. In our present study, we deal with the full set of connectives for which we have training data. 3 non-connective r"
W18-5037,K16-2002,1,0.77527,"Missing"
W18-5037,K15-2002,0,0.147826,"ation projection and achieve an f-score of 83.89. 1 2 Introduction Related Work Early attempts at formalizing discourse parsing procedures for English are described in (Soricut and Marcu, 2003), among others. Pitler and Nenkova (2009) experiment with syntactically motivated features for the binary classification of discourse connectives (connective or nonconnective reading) and report an f-score of 94.19 for the PDTB data (Prasad et al., 2008). The SDP pipeline architecture is adopted from Lin et al. (2014) and is also used in the best-scoring systems of the 2015 and 2016 CONLL shared tasks, (Wang and Lan, 2015) and (Oepen et al., 2016) respectively. Oepen et al. (2016) achieve an overall f-score of 27.77 for full SDP, but 91.79 for identifying explicit connectives. The best-scoring system for this subtask (Li et al., 2016) achieved an impressive 98.38. A notable drawback of the pipeline architecture is the possibility of error propagation. This is addressed by (Biran and McKeown, 2015), who use A task central to the field of Discourse Processing is the uncovering of coherence relations that hold between individual (elementary) units of a text. When discourse relations are explicitly signaled in a te"
W18-5042,L18-1693,1,0.734045,"Missing"
W18-5042,W11-2022,0,0.0904396,"syntactic categories, discourse semantics and non-connective uses (if any). We report on the development steps and discuss design decisions encountered in the lexicon expansion phase. The resource is freely available for use in studies of discourse structure and computational applications. 1 Introduction Discourse connectives are generally considered to be the most reliable signals of coherence relations, and they are widely used in a variety of NLP tasks involving the processing of coherence relations, such as discourse parsing (Hernault et al., 2010; Lin et al., 2014), machine translation (Meyer et al., 2011), text summarization (Alemany, 2005), or argumentation mining (Kirschner et al., 2015). Accordingly, corpora annotated for discourse connectives and coherence relations have been developed for different languages. In addition to discourse-annotated corpora, a lexicon of discourse connectives, giving the list of connectives for a language, along with useful information about their syntactic and semanticpragmatic properties, can also serve as a valuable resource. Such lexicons were developed and are becoming more and more available in different languages, beginning with German (Stede 2 Sources o"
W18-5042,prasad-etal-2008-penn,0,0.93252,"s UFS Cognitive Sciences University of Potsdam / Germany firstname.lastname@uni-potsdam.de Abstract and Umbach, 1998), later for Spanish (Briz et al., 2008) and French (Roze et al., 2010), and more recently for Italian (Feltracco et al., 2016), Portuguese (Mendes et al., 2018) and Czech (M´ırovsk´y et al., 2017). We present a lexicon of English discourse connectives called DiMLex-Eng, which is developed as a part of the Connective-Lex database1 at the University of Potsdam. It includes 149 connectives, a large part of which was compiled from the annotations of the Penn Discourse Treebank 2.0 (Prasad et al., 2008). We expanded that list to include additional connectives from the RST Signalling Corpus (Das et al., 2015) and relational indicators from a list supplied by Biran and Rambow (2011). For organizing the entries in the lexicon, we use the format of DiMLex, a lexicon of German connectives (Stede and Umbach, 1998; Scheffler and Stede, 2016). For each entry in DiMLex-Eng, we provide information on the possible orthographic variants of the connective, its syntactic category, non-connective usage (if any), and the set of discourse relations indicated by the connective (with examples from corpora). We"
W18-5042,L16-1160,1,0.694377,"con of English discourse connectives called DiMLex-Eng, which is developed as a part of the Connective-Lex database1 at the University of Potsdam. It includes 149 connectives, a large part of which was compiled from the annotations of the Penn Discourse Treebank 2.0 (Prasad et al., 2008). We expanded that list to include additional connectives from the RST Signalling Corpus (Das et al., 2015) and relational indicators from a list supplied by Biran and Rambow (2011). For organizing the entries in the lexicon, we use the format of DiMLex, a lexicon of German connectives (Stede and Umbach, 1998; Scheffler and Stede, 2016). For each entry in DiMLex-Eng, we provide information on the possible orthographic variants of the connective, its syntactic category, non-connective usage (if any), and the set of discourse relations indicated by the connective (with examples from corpora). We describe our criteria for filtering connective candidates for inclusion in the lexicon, and give an outlook on the relationship between connectives and the broader range of ‘cue phrases’ or ‘AltLex’ expressions in language. We present a new lexicon of English discourse connectives called DiMLex-Eng, built by merging information from tw"
W18-5042,P98-2202,1,0.762562,"2017). We present a lexicon of English discourse connectives called DiMLex-Eng, which is developed as a part of the Connective-Lex database1 at the University of Potsdam. It includes 149 connectives, a large part of which was compiled from the annotations of the Penn Discourse Treebank 2.0 (Prasad et al., 2008). We expanded that list to include additional connectives from the RST Signalling Corpus (Das et al., 2015) and relational indicators from a list supplied by Biran and Rambow (2011). For organizing the entries in the lexicon, we use the format of DiMLex, a lexicon of German connectives (Stede and Umbach, 1998; Scheffler and Stede, 2016). For each entry in DiMLex-Eng, we provide information on the possible orthographic variants of the connective, its syntactic category, non-connective usage (if any), and the set of discourse relations indicated by the connective (with examples from corpora). We describe our criteria for filtering connective candidates for inclusion in the lexicon, and give an outlook on the relationship between connectives and the broader range of ‘cue phrases’ or ‘AltLex’ expressions in language. We present a new lexicon of English discourse connectives called DiMLex-Eng, built by"
W18-5218,D17-1143,0,0.189781,"om web-text corpora that have been collected for argumentation mining purposes, such as the Internet Argument Corpus (Abbott et al., 2016), the ABCD corpus (Rosenthal and McKeown, 2015) and others. 2.2 2.3 Annotation scheme The argumentation structure of every text was annotated according to a scheme proposed by Peldszus and Stede (2013), which in turn had been based on Freeman’s theory of argumentation structures (Freeman, 2011). This annotation scheme has already been proven to yield reliable structures in annotation and classification experiments, for instance by (Peldszus and Stede, 2015; Potash et al., 2017). (Stab and Gurevych, 2017) use a similar scheme for their corpus of persuasive essay, and they also provide classification results for the 2 http://angcl.ling.uni-potsdam.de/resources/argmicro.html 156 Other annotation layers In contrast to other argumentation corpora, the microtext corpus is unique in that it is already annotated with further layers of linguistic information, which makes it usable for systematic correlation studies. Stede et al. (2016) described the annotation of discourse structure according to RST and SDRT, and Becker et al. (2016) added information on situation entity typ"
W18-5218,L16-1704,0,0.0213348,"). Finally, all texts have been checked for spelling and grammatical problems, which have been corrected by the annotators. The reason underlying this decision was the intended role of the corpus as a resource for studying argumentation in connection with other linguistic phenomena (see Section 2.3), where plain errors can lead to undesired complications for parsers, etc. Hence, “authenticity” on this level was considered as less important. In this respect the corpus differs from web-text corpora that have been collected for argumentation mining purposes, such as the Internet Argument Corpus (Abbott et al., 2016), the ABCD corpus (Rosenthal and McKeown, 2015) and others. 2.2 2.3 Annotation scheme The argumentation structure of every text was annotated according to a scheme proposed by Peldszus and Stede (2013), which in turn had been based on Freeman’s theory of argumentation structures (Freeman, 2011). This annotation scheme has already been proven to yield reliable structures in annotation and classification experiments, for instance by (Peldszus and Stede, 2015; Potash et al., 2017). (Stab and Gurevych, 2017) use a similar scheme for their corpus of persuasive essay, and they also provide classific"
W18-5218,W16-2803,0,0.0136127,"or instance by (Peldszus and Stede, 2015; Potash et al., 2017). (Stab and Gurevych, 2017) use a similar scheme for their corpus of persuasive essay, and they also provide classification results for the 2 http://angcl.ling.uni-potsdam.de/resources/argmicro.html 156 Other annotation layers In contrast to other argumentation corpora, the microtext corpus is unique in that it is already annotated with further layers of linguistic information, which makes it usable for systematic correlation studies. Stede et al. (2016) described the annotation of discourse structure according to RST and SDRT, and Becker et al. (2016) added information on situation entity types, which Smith (2003) had proposed as a linguistic tool for identifying different ‘discourse modes’, viz. Narrative, Description, Report, Information, and Argument. Reisert et al. (2017) annotated part of the corpus with information on argumentation schemes, in the spirit of Walton et al. (2008). Also, an alternative approach to schemes, that of Rigotti and Greco Morasso (2010), was annotated on the microtexts by Musi et al. (2018). Given these extra layers, we regard the extension of the microtext corpus as especially useful, as the annotations of th"
W18-5218,W15-4625,0,0.0231569,"for spelling and grammatical problems, which have been corrected by the annotators. The reason underlying this decision was the intended role of the corpus as a resource for studying argumentation in connection with other linguistic phenomena (see Section 2.3), where plain errors can lead to undesired complications for parsers, etc. Hence, “authenticity” on this level was considered as less important. In this respect the corpus differs from web-text corpora that have been collected for argumentation mining purposes, such as the Internet Argument Corpus (Abbott et al., 2016), the ABCD corpus (Rosenthal and McKeown, 2015) and others. 2.2 2.3 Annotation scheme The argumentation structure of every text was annotated according to a scheme proposed by Peldszus and Stede (2013), which in turn had been based on Freeman’s theory of argumentation structures (Freeman, 2011). This annotation scheme has already been proven to yield reliable structures in annotation and classification experiments, for instance by (Peldszus and Stede, 2015; Potash et al., 2017). (Stab and Gurevych, 2017) use a similar scheme for their corpus of persuasive essay, and they also provide classification results for the 2 http://angcl.ling.uni-p"
W18-5218,J17-1004,0,0.219709,"umentation structure: We first replicated the approach of an earlier study on the original corpus, and compare the performance to various settings involving the extension. 1 Introduction As with most areas in NLP, progress on Argumentation Mining hinges on the availability of data, and in the case of this field, this is generally taken to be annotated data. Up to now, only few corpora labelled with full argumentation structure (i.e., argument components and relations between them) are available; prominent ones are the persuasive essay corpus of Stab and Gurevych (2014), the web text corpus of Habernal and Gurevych (2017), and the argumentative microtext corpus of Peldszus and Stede (2016).1 The latter is interesting because it has in parallel been annotated with various other linguistic layers, as will be described in Section 2. The microtexts are relatively “clean” text, and the annotation of argumentation structure was generally easy, leading to reasonable annotator agreement, as reported by Peldszus and 1 Many other corpora are available with more lean or more specific annotations; see Section 4 of (Lippi and Torroni, 2016). 155 Proceedings of the 5th Workshop on Argument Mining, pages 155–163 c Brussels,"
W18-5218,sonntag-stede-2014-grapat,1,0.830284,"e its risks? Fracking has uncovered cheap natural gas. The aggregate savings to the American household are then passed on to the economy in the way of spending. Also, the coal industry has imploded as a consequence which is more of a pollutant than natural gas. The potential contamination damage caused by the fracking process is outweighed by the reduction of energy costs to the American household. Yes, we need fracking despite its risks. Annotating the crowdsourced texts We applied the annotation guidelines (mentioned in Section 2) and used the freely available graph annotation tool GraPAT3 (Sonntag and Stede, 2014) to annotate the 171 texts that passed our filtering step. Two annotators (one of them being a co-author of this paper) shared the work, and a third person (another co-author) joined in discussions of difficult cases. At the present stage, we did not run an inter-annotator agreement study, because this had already been done on the original corpus and guidelines (see Peldszus and Stede (2016)), thereby verifying the usability of the scheme. However, the annotation process was not entirely straightforward. In the following, we describe specifically the challenges posed by the different type of t"
W18-5218,L18-1258,1,0.802922,"correlation studies. Stede et al. (2016) described the annotation of discourse structure according to RST and SDRT, and Becker et al. (2016) added information on situation entity types, which Smith (2003) had proposed as a linguistic tool for identifying different ‘discourse modes’, viz. Narrative, Description, Report, Information, and Argument. Reisert et al. (2017) annotated part of the corpus with information on argumentation schemes, in the spirit of Walton et al. (2008). Also, an alternative approach to schemes, that of Rigotti and Greco Morasso (2010), was annotated on the microtexts by Musi et al. (2018). Given these extra layers, we regard the extension of the microtext corpus as especially useful, as the annotations of the other layers may now also be added, resulting in a much more valuable re[e1] Helicopter parenting has proven to be detrimental to the success of children. [e2] Over involvement in the work, production, and affairs of children impact the child&apos;s ability to face consequences and experiences good and bad decisions. [e3] While one can argue that this type of parenting style benefits a child due to the active involvement and guidance by a parent [e4] there are few arguments to"
W18-5218,C14-1142,0,0.17746,"eriment with the automatic prediction of this argumentation structure: We first replicated the approach of an earlier study on the original corpus, and compare the performance to various settings involving the extension. 1 Introduction As with most areas in NLP, progress on Argumentation Mining hinges on the availability of data, and in the case of this field, this is generally taken to be annotated data. Up to now, only few corpora labelled with full argumentation structure (i.e., argument components and relations between them) are available; prominent ones are the persuasive essay corpus of Stab and Gurevych (2014), the web text corpus of Habernal and Gurevych (2017), and the argumentative microtext corpus of Peldszus and Stede (2016).1 The latter is interesting because it has in parallel been annotated with various other linguistic layers, as will be described in Section 2. The microtexts are relatively “clean” text, and the annotation of argumentation structure was generally easy, leading to reasonable annotator agreement, as reported by Peldszus and 1 Many other corpora are available with more lean or more specific annotations; see Section 4 of (Lippi and Torroni, 2016). 155 Proceedings of the 5th Wo"
W18-5218,J17-3005,0,0.111313,"t have been collected for argumentation mining purposes, such as the Internet Argument Corpus (Abbott et al., 2016), the ABCD corpus (Rosenthal and McKeown, 2015) and others. 2.2 2.3 Annotation scheme The argumentation structure of every text was annotated according to a scheme proposed by Peldszus and Stede (2013), which in turn had been based on Freeman’s theory of argumentation structures (Freeman, 2011). This annotation scheme has already been proven to yield reliable structures in annotation and classification experiments, for instance by (Peldszus and Stede, 2015; Potash et al., 2017). (Stab and Gurevych, 2017) use a similar scheme for their corpus of persuasive essay, and they also provide classification results for the 2 http://angcl.ling.uni-potsdam.de/resources/argmicro.html 156 Other annotation layers In contrast to other argumentation corpora, the microtext corpus is unique in that it is already annotated with further layers of linguistic information, which makes it usable for systematic correlation studies. Stede et al. (2016) described the annotation of discourse structure according to RST and SDRT, and Becker et al. (2016) added information on situation entity types, which Smith (2003) had"
W18-5218,L16-1167,1,0.785654,"as already been proven to yield reliable structures in annotation and classification experiments, for instance by (Peldszus and Stede, 2015; Potash et al., 2017). (Stab and Gurevych, 2017) use a similar scheme for their corpus of persuasive essay, and they also provide classification results for the 2 http://angcl.ling.uni-potsdam.de/resources/argmicro.html 156 Other annotation layers In contrast to other argumentation corpora, the microtext corpus is unique in that it is already annotated with further layers of linguistic information, which makes it usable for systematic correlation studies. Stede et al. (2016) described the annotation of discourse structure according to RST and SDRT, and Becker et al. (2016) added information on situation entity types, which Smith (2003) had proposed as a linguistic tool for identifying different ‘discourse modes’, viz. Narrative, Description, Report, Information, and Argument. Reisert et al. (2017) annotated part of the corpus with information on argumentation schemes, in the spirit of Walton et al. (2008). Also, an alternative approach to schemes, that of Rigotti and Greco Morasso (2010), was annotated on the microtexts by Musi et al. (2018). Given these extra la"
W18-5218,D15-1110,1,0.928361,"pect the corpus differs from web-text corpora that have been collected for argumentation mining purposes, such as the Internet Argument Corpus (Abbott et al., 2016), the ABCD corpus (Rosenthal and McKeown, 2015) and others. 2.2 2.3 Annotation scheme The argumentation structure of every text was annotated according to a scheme proposed by Peldszus and Stede (2013), which in turn had been based on Freeman’s theory of argumentation structures (Freeman, 2011). This annotation scheme has already been proven to yield reliable structures in annotation and classification experiments, for instance by (Peldszus and Stede, 2015; Potash et al., 2017). (Stab and Gurevych, 2017) use a similar scheme for their corpus of persuasive essay, and they also provide classification results for the 2 http://angcl.ling.uni-potsdam.de/resources/argmicro.html 156 Other annotation layers In contrast to other argumentation corpora, the microtext corpus is unique in that it is already annotated with further layers of linguistic information, which makes it usable for systematic correlation studies. Stede et al. (2016) described the annotation of discourse structure according to RST and SDRT, and Becker et al. (2016) added information o"
W18-5902,W17-5801,1,0.891517,"Missing"
W18-5902,W15-0509,0,0.0319475,"y topic modelling from two different corpora, one discussion forum corpus and one tweet corpus. For some of the topics extracted, their most closely associated documents contained a proportion of vaccine stance-taking texts that exceeded the corpus average by a large margin. These extracted document sets would, therefore, form a useful resource in a process for computer-assisted analysis of argumentation on the subject of vaccination. 1 Background There are previous studies that use topic modelling in computer-assisted processes to find frequently occurring arguments in a document collection (Sobhani et al., 2015; Skeppstedt et al., 2018). Documents that had been manually annotated as not containing argumentation/stance-taking were, however, removed in those two previous studies, i.e., no evaluation of the effects of including neutral documents when performing topic modelling was carried out. For most types of document collections, it is not known beforehand in which documents a stance towards the target of interest is taken or not. Therefore, the setting used here is more widely applicable, i.e., to use topic modelling on an entire text collection, without removing documents in which no stance is tak"
W19-2707,prasad-etal-2008-penn,0,0.922001,"us of 185 threads and annotation, including an inter-annotator agreement study. We discuss our observations as to how Twitter discourses differ from written news text wrt. discourse connectives and relations. We confirm our hypothesis that discourse relations in written social media conversations are expressed differently than in (news) text. We find that in Twitter, connective arguments frequently are not full syntactic clauses, and that a few general connectives expressing E XPANSION and C ONTINGENCY make up the majority of the explicit relations in our data. 1 Introduction The PDTB corpus (Prasad et al., 2008) is a wellknown resource of discourse-level annotations, and the general idea of lexically signalled discourse structure annotation has over the years been applied to a variety of languages. A shallow approach to discourse structure in the PDTB style can also be adapted to different genres. In this paper, we consider English conversations on Twitter, and describe the first phase of our annotation, viz. that of explicit connectives whose arguments are within a single tweet. We explain the collection of the data and our annotation procedure, and the results of an inter-annotator-agreement study."
W19-2707,L16-1165,0,0.0137882,". We explain the collection of the data and our annotation procedure, and the results of an inter-annotator-agreement study. We present our analysis of the specific features of this genre of conversation wrt. discourse structure, as well as corpus statistics, which we compare to the distributions in the original PDTB corpus. We show that explicit discourse relations are frequent in English Twitter conversations, and that 2 Twitter and Discourse Relations Recent studies indicate significant differences in the use of discourse connectives and discourse relations between written and spoken data (Rehbein et al., 2016; Crible and Cuenca, 2017). Though the PDTB approach has been applied to different text types, conversational data has not been systematically analysed yet. There is recent work on annotating spoken TED talks in several languages (Zeyrek et al., 2018), but these planned monologues do not exhibit spontaneous interaction. To our knowledge, only (Tonelli et al., 2010; Riccardi et al., 2016) have constructed PDTB annotations 50 Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 50–55 c Minneapolis, MN, June 6, 2019. 2019 Association for Computational Linguistics for spok"
W19-2707,P18-1210,0,0.0420928,"Missing"
W19-2707,scheffler-2014-german,1,0.825427,"connectives from lexical and acoustic features in help desk dialogs. In contrast, we investigate open topic spontaneous conversations in computer mediated communication, to abstract away from the speech mode, but retain the conversational properties. Twitter1 is a social media platform that publishes short “microposts” by registered users. In addition to textual content, these posts may contain embedded images or videos. Twitter users can interact by directly replying to each other’s messages. Such replies are quite frequent and the resulting conversations often contain discourse connectives (Scheffler, 2014). There is some evidence that the types of relations and connectives found in Twitter conversations differs markedly from edited news text and reflects some features of spoken conversations (Scheffler, 2014; Scheffler and Stede, 2016). Here, we introduce an annotated corpus of explicit connectives in English tweets, which allows us to test genre differences between the discourse structure of newspaper texts (PDTB) and conversational writing (our Twitter corpus). 3 We primarily used the list of 100 explicit connectives from the PDTB corpus (Prasad et al., 2008) to identify connectives. Addition"
W19-2707,tonelli-etal-2010-annotation,0,0.111323,"ns are frequent in English Twitter conversations, and that 2 Twitter and Discourse Relations Recent studies indicate significant differences in the use of discourse connectives and discourse relations between written and spoken data (Rehbein et al., 2016; Crible and Cuenca, 2017). Though the PDTB approach has been applied to different text types, conversational data has not been systematically analysed yet. There is recent work on annotating spoken TED talks in several languages (Zeyrek et al., 2018), but these planned monologues do not exhibit spontaneous interaction. To our knowledge, only (Tonelli et al., 2010; Riccardi et al., 2016) have constructed PDTB annotations 50 Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 50–55 c Minneapolis, MN, June 6, 2019. 2019 Association for Computational Linguistics for spoken conversations, and they work on Italian dialogs. Riccardi et al. (2016) focus on the detection of discourse connectives from lexical and acoustic features in help desk dialogs. In contrast, we investigate open topic spontaneous conversations in computer mediated communication, to abstract away from the speech mode, but retain the conversational properties. Twit"
W19-2712,W17-3602,1,0.811069,"Theory (Mann and Thompson, 1988) is intended to describe discourse structure and text organization by labeling the discourse relations that hold between elementary discourse units (EDU) or between larger spans of text. It is widely used throughout the discourse community as a theory for discourse analysis. RST is defined as the reconstruction of the author’s plan from the perspective of the reader (Stede, 2017), that is to say it implies a certain subjectivity. According to this view, different annotators might very well produce different analyses, which can nonetheless be equally legitimate (Das et al., 2017). However, differences in the analysis based on the legitimate scope of explication ought to be distinguished from unexpected errors or ambiguities resulting from unclear annotation guidelines. In 88 Proceedings of Discourse Relation Parsing and Treebanking (DISRPT2019), pages 88–96 c Minneapolis, MN, June 6, 2019. 2019 Association for Computational Linguistics Example [1] [2] [3] [4] CS1 1 23 17-18|26 10|15 CS2 1 23|24 17-18|26 11|15 Table 1: Examples of matching central subconstiutents (extracted from CMN_008, RST German Learner Treebank8 ) (a) Annotator I Figure 1: Annotations from two anno"
W19-2712,J00-3005,0,0.483226,"Missing"
W19-3015,N18-1049,0,0.0430097,"Missing"
W19-3015,D14-1162,0,0.0891571,"thus to be conceived of as a kind of epiphenomenon of more general principles of communication. Baseline coherence model: The first step of speech analysis aims to test the Incoherence and Tangentiality model on the raw dataset. No filtering of stop words or fillers was performed except for the unavoidable loss of words not covered by vocabulary of the respective models. Baseline models use mean vector sentence embeddings, i.e. the mean of all word vectors per sentence or window of tokens (Iter et al., 2018). The vectors are given by a word2vec model (Mikolov et al., 2013) and a GloVe model (Pennington et al., 2014) trained on German data. The Tangentiality model at baseline uses a fixedsize window of four tokens. In contrast to Iter et al. (2018), we refrained from using LSA in our analysis due to the lack of availability of such a model that has not already performed a TF-IDF-weighting (Lintean et al., 2010) at the stage of training. Additionally, the weighting scheme used at the training of the model at hand differs from that adopted by Iter et al. (2018). Consequently, in order to preserve a certain level of comparability, we decided not to use the available LSA model. However, the use of word2vec fo"
W19-3015,Q15-1016,0,0.026656,"ols replicating the LSA-based models used in Bedi et al. (2015) and Elvevag et al. (2007). They point out three major shortcomings of the models: (1) the misinterpretation of verbal fillers as incoherent speech, (2) a bias to judging longer sentences as more coherent than short ones, as well as (3) a bias to judging repetitions as more coherent. Iter et al. (2018) were able to improve coherence models by Elvevag et al. (2007) and Bedi et al. (2015) by preprocessing their dataset and using modern word and sentence embedding techniques which have been shown to outperform LSA (Fang et al., 2016; Levy et al., 2015). Moreover, they credit the mentioned observations of referential problems in schizophrenia and propose a referential coherence model based on classifying ambiguous pronoun use to further improve the predictive value of their results. Our study aims to (1) assess whether the models used by Iter et al. (2018) can be transferred to the German language, and (2) to apply them to a larger sample of patients of varying stability. Specifically, we aim to examine (1) whether schizophrenia patients and controls can be differentiated based on automated coherence analysis, and (2) whether schizophrenia p"
W19-4512,W15-4612,0,0.0247207,"t point of segments) can clearly benefit from these features. Our project is a continuation of that study, as we essentially replicate the model, but use automatically parsed RST trees instead of the gold annotations, in order to assess a “real world” scenario. thermore, various other layers of annotation have been produced, including RST trees (Stede et al., 2016). Later, Musi et al. (2018) conducted a study comparing the RST trees to annotations of argumentation schemes. 3 We parsed a subset of the corpus with various parsers (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Lin et al., 2014; Biran and McKeown, 2015), and after a manual analysis of the results, chose the systems of Feng and Hirst (2014) and Lin et al. (2014). These were used “out of the box”, without having been trained on our data, to produce the automatic RST- and PDTB-parses for our study in a domain-independent way. In a small pilot study, we compared the RST parser output to the gold argumentation structures for 10 texts of the corpus. We observed that the parser sometimes produced different segmentations, either combining segments, or using completely new boundaries. We also noted that the central claims matched the most-nuclear RST"
W19-4512,D15-1110,1,0.927094,"e box”, without having been trained on our data, to produce the automatic RST- and PDTB-parses for our study in a domain-independent way. In a small pilot study, we compared the RST parser output to the gold argumentation structures for 10 texts of the corpus. We observed that the parser sometimes produced different segmentations, either combining segments, or using completely new boundaries. We also noted that the central claims matched the most-nuclear RST seg3.2 Various researchers have used slightly different approaches to automatically parse the argumentation structure in the microtexts. Peldszus and Stede (2015) decompose the problem into the four subtasks of finding the central claim (cc) segment, and for each other segment its role (ro: proponent or opponent), its function (fu: support, rebut, undercut), and the segment it attaches to (at). They use a minimum spanning tree (MST) decoder on a so-called ‘evidence graph’ that combines the probabilities computed for the four subtasks. Stab and Gurevych (2016) achieved slightly better results for some subtasks using Integer Linear Programming. Potash et al. (2017) use a bidirectional LSTM encoder and achieve competitive results on the microtexts, but th"
W19-4512,D15-1267,0,0.0231988,"mple’, which can be defined as when the second argument offers a summary or a conclusion based on the first argument. Generally, the presence of connectives or other discourse markers has often been employed 98 Proceedings of the 6th Workshop on Argument Mining, pages 98–103 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics for detecting argument components and relations between them. Stab and Gurevych (2014) compile a list of 55 discourse markers which indicate argumentative discourse and use these as features to detect the argumentative role in German essays. Eckle-Kohler et al. (2015) instead look at German news items which are annotated with the argumentative roles ‘claim’ and ‘premise’ (with various sub-categories). They found that both single discourse markers and semantic groups of such markers occurred in significant correlation with claims or premises. Turning specifically to RST, Green (2010) proposes the ArgRST annotation scheme, which represents both argumentation and discourse analysis in the same structure. Inter alia, she finds parallels between the RST relation ‘evidence’ and the premise and claim of an argument. Finally, Peldszus and Stede (2016b) present a q"
W19-4512,P14-1048,0,0.0270729,"ding the central claim and the attachment point of segments) can clearly benefit from these features. Our project is a continuation of that study, as we essentially replicate the model, but use automatically parsed RST trees instead of the gold annotations, in order to assess a “real world” scenario. thermore, various other layers of annotation have been produced, including RST trees (Stede et al., 2016). Later, Musi et al. (2018) conducted a study comparing the RST trees to annotations of argumentation schemes. 3 We parsed a subset of the corpus with various parsers (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Lin et al., 2014; Biran and McKeown, 2015), and after a manual analysis of the results, chose the systems of Feng and Hirst (2014) and Lin et al. (2014). These were used “out of the box”, without having been trained on our data, to produce the automatic RST- and PDTB-parses for our study in a domain-independent way. In a small pilot study, we compared the RST parser output to the gold argumentation structures for 10 texts of the corpus. We observed that the parser sometimes produced different segmentations, either combining segments, or using completely new boundaries. We also noted that the"
W19-4512,W16-2812,1,0.643432,"ion 2 discusses related work, and Section 3 describes the corpus and the discourse parsers we used. Initial analyses of parser results are given in Section 4, and the experiments on predicting argumentation structure are reported in Section 5. The paper closes with some conclusions in Section 6. Research on argumentation mining from text has frequently discussed relationships to discourse parsing, but few empirical results are available so far. One corpus that has been annotated in parallel for argumentation structure and for discourse structure (RST, SDRT) are the ‘argumentative microtexts’ (Peldszus and Stede, 2016a). While results on perusing the gold RST annotations for predicting argumentation have been published (Peldszus and Stede, 2016b), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the microtexts, we find that PDTB features can indeed improve its performan"
W19-4512,D17-1143,0,0.0631875,"rent approaches to automatically parse the argumentation structure in the microtexts. Peldszus and Stede (2015) decompose the problem into the four subtasks of finding the central claim (cc) segment, and for each other segment its role (ro: proponent or opponent), its function (fu: support, rebut, undercut), and the segment it attaches to (at). They use a minimum spanning tree (MST) decoder on a so-called ‘evidence graph’ that combines the probabilities computed for the four subtasks. Stab and Gurevych (2016) achieved slightly better results for some subtasks using Integer Linear Programming. Potash et al. (2017) use a bidirectional LSTM encoder and achieve competitive results on the microtexts, but they solve only part of the problem (no support/attack distinction). Finally, Afantenos et al. (2018) compare ILP and MST by training a classifier for each subtask (cc, ro, fu, at) and use this combined distribution as input to the decoders. Their best model is a replication of the evidence graph model with MST decoding from Peldszus and Stede (2015) with some additional features, including discourse connectives for English. As this is this the model with best results for the complete problem, we will repl"
W19-4512,J17-1004,0,0.0494463,"ove its performance. 1 Introduction The argumentative structure of texts, as captured, for instance, by schemata from Peldszus and Stede (2013) or Stab and Gurevych (2014), is represented by tree structures that suggest a certain similarity to accounts of discourse structure, such as in Rhetorical Structure Theory (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (Asher and Lascarides, 2003). These approaches aim at accounting for the coherence of texts, which is clearly related – though not identical – to the structure of complex arguments. This is not a new observation (Habernal and Gurevych, 2017), but we are not aware of many empiricallygrounded studies of the correspondences between the two realms. A corpus that facilitates such experiments is the ‘argumentative microtext corpus’ (Peldszus and Stede, 2016a), as it offers annotation not only for argumentation but also for 2 Related work A number of researchers have studied connections between discourse structure and argumentation. Cabrio et al. (2013) look at the link between PDTB relations and the argumentation schemes from Walton et al. (2008). They find, for example, that the PDTB relation ‘expansion’ corresponds to the ‘Argument b"
W19-4512,D14-1006,0,0.332856,"), the step to automatic discourse parsing has not yet been taken. In this paper, we run various discourse parsers (RST, PDTB) on the corpus, compare their results to the gold annotations (for RST) and then assess the contribution of automatically-derived discourse features for argumentation parsing. After reproducing the state-of-the-art Evidence Graph model from Afantenos et al. (2018) for the microtexts, we find that PDTB features can indeed improve its performance. 1 Introduction The argumentative structure of texts, as captured, for instance, by schemata from Peldszus and Stede (2013) or Stab and Gurevych (2014), is represented by tree structures that suggest a certain similarity to accounts of discourse structure, such as in Rhetorical Structure Theory (Mann and Thompson, 1988) or Segmented Discourse Representation Theory (Asher and Lascarides, 2003). These approaches aim at accounting for the coherence of texts, which is clearly related – though not identical – to the structure of complex arguments. This is not a new observation (Habernal and Gurevych, 2017), but we are not aware of many empiricallygrounded studies of the correspondences between the two realms. A corpus that facilitates such experi"
W19-4512,P14-1002,0,0.0997516,"ng in the microtexts (finding the central claim and the attachment point of segments) can clearly benefit from these features. Our project is a continuation of that study, as we essentially replicate the model, but use automatically parsed RST trees instead of the gold annotations, in order to assess a “real world” scenario. thermore, various other layers of annotation have been produced, including RST trees (Stede et al., 2016). Later, Musi et al. (2018) conducted a study comparing the RST trees to annotations of argumentation schemes. 3 We parsed a subset of the corpus with various parsers (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Lin et al., 2014; Biran and McKeown, 2015), and after a manual analysis of the results, chose the systems of Feng and Hirst (2014) and Lin et al. (2014). These were used “out of the box”, without having been trained on our data, to produce the automatic RST- and PDTB-parses for our study in a domain-independent way. In a small pilot study, we compared the RST parser output to the gold argumentation structures for 10 texts of the corpus. We observed that the parser sometimes produced different segmentations, either combining segments, or using completely new boundaries."
W19-4512,J15-3002,0,0.0262923,"Besides comparing RST parses to argumentative structures, we were also interested in evaluating the RST parser on the microtexts, i.e., on their gold RST trees. To this end, we converted the gold annotations to a comparable format, which involved converting the ‘span’ relations (which were not present in the parser’s output), adjusting the segment IDs so that they were in ascending order, and converting the more fine-grained relations to the smaller set used by the parser (using the taxonomy in Das and Taboada, 2014). We adapted the metrics to evaluate the parser output from those proposed by Joty et al. (2015); our results are given in Table 3. F1 Span 0.338 Nuclearity 0.264 RST parser output Best performing ARG model output [2, 1, ‘join’], [3, 1, ‘rebut’], [4, 3, ‘undercut’], [5, 1, ‘support’] Gold ARG annotation [2, 1, ‘support’], [3, 2, ‘undercut’], [4, 3, ‘undercut’], [5, 4, ‘support’] Figure 1: Parser and model output for microtext b005. The numbers refer to the segments. RST tree created using RSTTool (O’Donnell, 2000). Relation 0.115 fier whose highest score, 0.750, was achieved with the combination of all features for the simple relations. Even though the statistical analysis of the PDTB ou"
W19-4512,L16-1167,1,0.894537,"whether a segment has incoming or outgoing edges, and the type of RST relation between segments, amongst others. They showed that especially two subtasks of argumentation structure parsing in the microtexts (finding the central claim and the attachment point of segments) can clearly benefit from these features. Our project is a continuation of that study, as we essentially replicate the model, but use automatically parsed RST trees instead of the gold annotations, in order to assess a “real world” scenario. thermore, various other layers of annotation have been produced, including RST trees (Stede et al., 2016). Later, Musi et al. (2018) conducted a study comparing the RST trees to annotations of argumentation schemes. 3 We parsed a subset of the corpus with various parsers (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Lin et al., 2014; Biran and McKeown, 2015), and after a manual analysis of the results, chose the systems of Feng and Hirst (2014) and Lin et al. (2014). These were used “out of the box”, without having been trained on our data, to produce the automatic RST- and PDTB-parses for our study in a domain-independent way. In a small pilot study, we compared the RST parser output to the go"
W19-4512,L18-1258,1,0.655182,"ing or outgoing edges, and the type of RST relation between segments, amongst others. They showed that especially two subtasks of argumentation structure parsing in the microtexts (finding the central claim and the attachment point of segments) can clearly benefit from these features. Our project is a continuation of that study, as we essentially replicate the model, but use automatically parsed RST trees instead of the gold annotations, in order to assess a “real world” scenario. thermore, various other layers of annotation have been produced, including RST trees (Stede et al., 2016). Later, Musi et al. (2018) conducted a study comparing the RST trees to annotations of argumentation schemes. 3 We parsed a subset of the corpus with various parsers (Ji and Eisenstein, 2014; Feng and Hirst, 2014; Lin et al., 2014; Biran and McKeown, 2015), and after a manual analysis of the results, chose the systems of Feng and Hirst (2014) and Lin et al. (2014). These were used “out of the box”, without having been trained on our data, to produce the automatic RST- and PDTB-parses for our study in a domain-independent way. In a small pilot study, we compared the RST parser output to the gold argumentation structures"
W19-4512,W00-1434,0,0.0786865,"Missing"
W19-8607,W18-5215,0,0.0251713,"nalysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference on Natural Language Generation, pages 54–64, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics composed of meaningful units, a property that neural generation methods barely achieve so far. In our evaluation, we utilize the dataset of Wachsmuth et al. (2018). This dataset contains 260 argumentative"
W19-8607,P16-2085,0,0.137945,", independent of their finances what is the good of a wonderfully outfitted university if it doesn’t actually allow the majority of clever people to broaden their horizons with all that great equipment p5 p6 p7 p8 p9 p10 p11 p12 Topic Should all universities in Germany charge tuition fees? Stance Con Table 1: The candidate thesis, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical co"
W19-8607,D17-1144,0,0.0659392,"arranges the structure of the types based on the units’ argumentative roles, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific ro"
W19-8607,W16-2816,0,0.0845706,"es in Germany charge tuition fees? Stance Con Table 1: The candidate thesis, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical considerations in the synthesis process. 3 3. Elocutio ∼ Phrasing the arranged ADUs by adding connectives at unit-initial or unit-final positions. Specifically, Wachsmuth et al. (2018) selected a pool of 200 ADUs for 10 pairs of controversial topic and s"
W19-8607,D14-1162,0,0.0830054,"Missing"
W19-8607,P16-1150,0,0.0216463,"es, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference"
W19-8607,W15-0507,0,0.0710582,"is, con, and pro units for one topic-stance pair in the dataset of Wachsmuth et al. (2018). 2. Dispositio ∼ Arranging the selected ADUs in a sequential order. Some other approaches have been proposed that recompose existing text segments in new arguments. In particular, Bilu and Slonim (2016) generated new claims by “recycling” topics and predicates that were found in a database of claims. Claim selection involves preferring predicates that are generally amenable to claim units and that are relevant for the target topic. Egan et al. (2016) created summaries of the main points in a debate, and Reisert et al. (2015) synthesized complete arguments from a set of manually curated topic-stance relations based on the fine-grained argument model of Toulmin (1958). However, we are not aware of any approach that synthesizes arguments fully automatically, let alone that follows rhetorical considerations in the synthesis process. 3 3. Elocutio ∼ Phrasing the arranged ADUs by adding connectives at unit-initial or unit-final positions. Specifically, Wachsmuth et al. (2018) selected a pool of 200 ADUs for 10 pairs of controversial topic and stance from the English version of the arg-microtexts corpus (Peldszus and St"
W19-8607,P18-1152,0,0.0201934,"2 2 Related Work Recently, some researchers have tackled argumentation synthesis statistically with neural networks. For instance, Wang and Ling (2016) employed a sequence-to-sequence model to generate summaries of argumentative texts, and Hua and Wang (2018) did similar to generate counterarguments. Using neural methods in text generation, it is possible to achieve output that is on topic and grammatically (more or less) correct. However, when the desired text is to span multiple sentences, the generated text regularly suffers from incoherence and repetitiveness, as for instance discussed by Holtzman et al. (2018) who examine texts that were produced by RNNs in various domains. While these problems may be tolerable to some extent in some applications, such as chatbots, bad text cannot be accepted in an argumentative or debating scenario, where the goal is to convince or persuade a reader (rather than to merely inform or entertain). Holtzman et al. (2018) propose to alleviate incoherence and repetitiveness by training a set of discriminators, which aim to ensure that a text respects the Gricean maxims of quantity, quality, relation, and manner (Grice, 1975). To this end, they Most related to our approac"
W19-8607,P18-1021,0,0.100511,"view of argumentation synthesis that represents argumentative and rhetorical considerations with language modeling. 2. A novel approach that selects, arranges, and phrases ADUs to synthesize strategy-specific arguments for any topic and stance. 3. First experimental evidence that arguments with basic rhetorical strategies can be synthesized computationally.2 2 Related Work Recently, some researchers have tackled argumentation synthesis statistically with neural networks. For instance, Wang and Ling (2016) employed a sequence-to-sequence model to generate summaries of argumentative texts, and Hua and Wang (2018) did similar to generate counterarguments. Using neural methods in text generation, it is possible to achieve output that is on topic and grammatically (more or less) correct. However, when the desired text is to span multiple sentences, the generated text regularly suffers from incoherence and repetitiveness, as for instance discussed by Holtzman et al. (2018) who examine texts that were produced by RNNs in various domains. While these problems may be tolerable to some extent in some applications, such as chatbots, bad text cannot be accepted in an argumentative or debating scenario, where th"
W19-8607,2007.sigdial-1.5,0,0.256035,"each ADU is represented by a cluster label (A–F in Figure 2), where each label represents one ADU type. Now, for each of the strategies, we map each manually-generated sequence of ADUs to a sequence of cluster labels. Using these sequences of labels, we train one separated selection language model for each strategy. For clustering, we rely on topic-independent features that we expect to implicitly encode logical and emotional strategies: (1) psychological meaningfulness (Pennebaker et al., 2015), (2) eight basic emotions (Plutchik, 1980; Mohammad and Turney, 2013), and (3) argumentativeness (Somasundaran et al., 2007). In the following, we elaborate on the concrete features that we extract: Selection Language Model This model handles the selection of a set of n ADUs for a topic-stance pair x and a rhetorical strategy. We approach the selection as a language modeling task where each ADU is a “word” of our language model and each argument a “sentence”. To abstract from topic, the model actually selects ADU types, as explained in the following. 57 Argument corpus Topic+stance 1 … Argument1,1 ... 1-grams P( ) = 0.20 ... P( ) = 0.60 E F 3. Phrasing Regression Model → 2. Arrangement Language Model D A Argumentm,"
W19-8607,D14-1006,0,0.0583418,"t of unit types according to a basic rhetorical strategy (logos vs. pathos), arranges the structure of the types based on the units’ argumentative roles, and finally “phrases” an argument by instantiating the structure with semantically coherent units from the pool. Our evaluation suggests that the model can, to some extent, mimic the human synthesis of strategy-specific arguments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We conside"
W19-8607,C18-1318,1,0.571429,"Missing"
W19-8607,N16-1007,0,0.250645,"uments. 1 Introduction Existing research on computational argumentation largely focuses on the analysis side. Various analysis tasks are widely studied including identifying the claims along with their supporting premises (Stab and Gurevych, 2014), finding the relation between argumentative units (Cocarascu and Toni, 2017), and assessing the persuasiveness of arguments (Habernal and Gurevych, 2016). Diverse downstream applications, however, necessitate the development of argumentation synthesis technologies. For example, synthesis is needed to produce a summary of arguments for a given topic (Wang and Ling, 2016) or to build a debating system where new arguments are exchanged between the users and the system (Le et al., 2018). As a result, a number of recent studies addresses the argumentation synthesis task. These studies 1 We consider a single argument to be a sequence of ADUs where each ADU has a specific role: thesis, con, or pro. 54 Proceedings of The 12th International Conference on Natural Language Generation, pages 54–64, c Tokyo, Japan, 28 Oct - 1 Nov, 2019. 2019 Association for Computational Linguistics composed of meaningful units, a property that neural generation methods barely achieve so"
W19-8607,W15-0512,0,0.148233,"ricean maxims of quantity, quality, relation, and manner (Grice, 1975). To this end, they Most related to our approach is the system of Sato et al. (2015), where a user can enter a claimlike topic along with a stance. The system then generates argumentative paragraphs on specific aspects of the topic by selecting sentences from 10 million news texts of the Gigaword corpus. Potentially relevant aspects are those that trigger evaluative judgment in the reader. The sentences are arranged so that the text starts with a claim sentence and is followed by support sentences, employing the approach of Yanase et al. (2015). The support sentences are ordered by maximizing the semantic connectivity between sentences. Finally, some rephrasing is done in terms of certain aspects of surface realization. In a manual evaluation, however, no text was seen as sounding natural, underlining the difficulty of the task. In contrast to Sato et al. (2015), we learn directly from input data what argumentative discourse units to combine and how to arrange them. We leave surface realization aside to keep the focus on the argument composition. 2 The code for running the experiments is available here: https://github.com/webis-de/"
W19-8607,W00-1408,0,0.695119,"Missing"
W96-0415,H89-1022,0,0.0437964,"Missing"
W96-0415,C88-2100,0,0.04333,"Missing"
W96-0415,C94-1055,1,0.896723,"Missing"
W97-0401,J93-3003,0,0.0892089,"Missing"
W98-1414,C96-1050,0,0.133875,"Missing"
W98-1414,P97-1011,0,0.1835,"Missing"
W98-1414,C90-3018,0,0.451294,"Missing"
W98-1414,H89-1022,0,0.0587414,"Missing"
W98-1414,A92-1006,0,0.0186973,"Missing"
W98-1414,P94-1007,0,0.0334931,"Missing"
W98-1414,W96-0401,0,0.122922,"Missing"
W98-1414,W94-0315,0,\N,Missing
