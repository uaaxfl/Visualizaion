2020.coling-main.198,D16-1216,0,0.286197,"identify the different contexts in which the word appears and how they correlate with the categories of the task being studied. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 2181 Proceedings of the 28th International Conference on Computational Linguistics, pages 2181–2190 Barcelona, Spain (Online), December 8-13, 2020 We use politeness as case study of a linguistic phenomenon. Existing computational work on politeness developed feature-based (Danescu-Niculescu-Mizil et al., 2013) and neural (Aubakirova & Bansal, 2016, Niu & Bansal, 2018) models which detect if a natural language request (e.g. Can you please tell me how to do that?) is polite or impolite. Danescu-Niculescu-Mizil et al. (2013) developed a computational tool driven by existing theories in the literature on politeness (Brown & Levinson, 1987). These theories highlighted linguistic constructions that speakers use to reduce the burden on the addressee by sounding indirect (e.g. Could you please [...]). Danescu-Niculescu-Mizil et al. (2013) showed further that, for some words, their position in the request plays a role in whether the request wil"
2020.coling-main.198,P13-1025,0,0.149633,"Our model further discovers novel finer-grained patterns associated with (im)polite language. For example, the word please can occur in impolite contexts that are predictable from BERT clustering. The approach proposed here is validated by showing that features based on fine-grained patterns inferred from the clustering improve over politeness-word baselines. 1 Introduction Linguistic analyses have often been computationally performed around the static notion of words or word categorization methods (e.g. LIWC) where the context of words is not taken into account. Previous work on politeness (Danescu-Niculescu-Mizil et al., 2013) and gender (Bamman et al., 2014) have focused on comparing the use of non-contextual broad word categories such as personal pronouns across different categories/groups. For example, Bamman et al. (2014) found that women use more pronoun words than men. Danescu-Niculescu-Mizil et al. (2013) found that requests which contain a hedge word (e.g. think) are more likely to be perceived as polite than impolite. However, words often occur in many different contexts and thus analyzing them statically hides cues which can potentially enrich our understanding of the phenomenon being studied. As opposed"
2020.coling-main.198,Q18-1027,0,0.0179448,"exts in which the word appears and how they correlate with the categories of the task being studied. This work is licensed under a Creative Commons Attribution 4.0 International License. http://creativecommons.org/licenses/by/4.0/. License details: 2181 Proceedings of the 28th International Conference on Computational Linguistics, pages 2181–2190 Barcelona, Spain (Online), December 8-13, 2020 We use politeness as case study of a linguistic phenomenon. Existing computational work on politeness developed feature-based (Danescu-Niculescu-Mizil et al., 2013) and neural (Aubakirova & Bansal, 2016, Niu & Bansal, 2018) models which detect if a natural language request (e.g. Can you please tell me how to do that?) is polite or impolite. Danescu-Niculescu-Mizil et al. (2013) developed a computational tool driven by existing theories in the literature on politeness (Brown & Levinson, 1987). These theories highlighted linguistic constructions that speakers use to reduce the burden on the addressee by sounding indirect (e.g. Could you please [...]). Danescu-Niculescu-Mizil et al. (2013) showed further that, for some words, their position in the request plays a role in whether the request will be perceived as pol"
2020.coling-main.198,D14-1162,0,0.0923898,"tegories/groups. For example, Bamman et al. (2014) found that women use more pronoun words than men. Danescu-Niculescu-Mizil et al. (2013) found that requests which contain a hedge word (e.g. think) are more likely to be perceived as polite than impolite. However, words often occur in many different contexts and thus analyzing them statically hides cues which can potentially enrich our understanding of the phenomenon being studied. As opposed to static word embeddings which provide the same representation for a word regardless of its context (i.a. Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014), the BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models provide methods for extracting pre-trained contextualized word representations. By leveraging contextualized representations, linguistic theories can be validated and enriched. Given a dataset annotated for a downstream task, we build a model which automatically discovers fine-grained context patterns of words. Discovering such patterns provides insight into the phenomenon the task attempts to model. We use pre-trained BERT embeddings and exploit the fact that words which occur in similar contexts tend to have similar repre"
2020.coling-main.198,N18-1202,0,0.0520276,"use more pronoun words than men. Danescu-Niculescu-Mizil et al. (2013) found that requests which contain a hedge word (e.g. think) are more likely to be perceived as polite than impolite. However, words often occur in many different contexts and thus analyzing them statically hides cues which can potentially enrich our understanding of the phenomenon being studied. As opposed to static word embeddings which provide the same representation for a word regardless of its context (i.a. Mikolov et al., 2013a; Mikolov et al., 2013b; Pennington et al., 2014), the BERT (Devlin et al., 2018) and ELMo (Peters et al., 2018) models provide methods for extracting pre-trained contextualized word representations. By leveraging contextualized representations, linguistic theories can be validated and enriched. Given a dataset annotated for a downstream task, we build a model which automatically discovers fine-grained context patterns of words. Discovering such patterns provides insight into the phenomenon the task attempts to model. We use pre-trained BERT embeddings and exploit the fact that words which occur in similar contexts tend to have similar representations. Our model takes as input contextualized representat"
2020.sigdial-1.24,D14-1181,0,0.0128087,"ponse can be returned to the user to achieve the desired pedagogical objectives.1 Many of the classes in this task are distinguished in subtle ways, e.g., in degree of specificity (“Are you married?” vs. “Are you in a relationship?”) or temporal aspect (“Do you [currently] have any medical conditions?” vs. “Have you ever had a serious illness?”). A few classes are very frequent, but many appear only once in the data set, with almost three quarters of the classes comprising only 20 percent of the examples (Jin et al., 2017). The current best approach to this task uses an ensemble of Text CNNs (Kim, 2014) combined with a rule-based dialogue manager (Wilcox, 2019) via a logistic regression model, to leverage complementary performance characteristics of each system on the rare classes (Jin et al., 2017). This approach na¨ıvely treats all classes as orthogonal, so the semantic similarity of the classes above can be problematic. Ideally, a model should be able to learn the semantic contributions of common linguistic substructures from frequent classes, and use that knowledge to improve performance when those structures appear in infrequent classes. We hypothesize that multi-headed attention mechan"
2020.sigdial-1.24,N19-1319,0,0.204077,"splitting different aspects of the semantics of a single word across multiple attention heads. 1 Introduction Many semantic classification tasks have seen a huge boost in performance in recent years (Wang et al., 2018, 2019), thanks to the power of contextualized language models such as BERT (Devlin et al., 2019), which uses a Transformer (Vaswani et al., 2017) architecture to produce context-specific word embeddings for use in downstream classification tasks. These large, datahungry models are not always well suited to tasks that have a large number of classes or relatively small data sets (Mahabal et al., 2019). One task having both of these inauspicious properties is the Virtual Patient corpus (Jin et al., 2017), a collection of dialogues between medical students and a virtual patient experiencing back pain. The corpus contains examples of nearly 350 questions that the virtual patient knows how to answer, and the interaction is modeled as a text-based conversation in which the human, as the interviewer of the patient, always has the conversational initiative. Thus, the corpus represents a question identification task from the perspective of the dialogue agent, in which natural language inputs must"
2020.sigdial-1.24,D14-1162,0,0.0874106,"ports the interpretation that small numbers of examples are inadequate to train a classifier to handle the variation in representations that come out of a contextualized model. This would be consistent with other research showing poor performance of BERT in low-data regimes (Mahabal et al., 2019). Some of the discrepancy may also be explained by a domain mismatch. The BERT base model is trained on book and encyclopedia data (Devlin et al., 2019), to provide long, contiguous sequences of text. In contrast, our inputs are short, conversational, and full of typos. GloVe.42B, trained on web data (Pennington et al., 2014), may simply be a better fit for https://github.com/ExplorerFreda/ Structured-Self-Attentive-SentenceEmbedding 4 We only tested on the baseline and best system in this paper to minimize use of the test set for future work. 198 that the attention does play a role. To better understand the behavior of the selfattentive RNN, we employ a relatively novel method of analyzing attention: we insert bottleneck layers of just eight dimensions after each attention head, with sigmoid activations and no dropout. This adds another nonlinearity into the model, but reduces the total number of parameters subst"
2020.sigdial-1.24,P19-1452,0,0.0293362,"a virtual patient dialogue task (Danforth et al., 2013; Jaffe et al., 2015), which has many classes and scarce data. Previous authors have used memory networks (Weston et al., 2015) to improve performance on rare classes for this task (Jin et al., 2018). Despite the contrast presented above, our selfattentive model actually shares characteristics with the work by Mahabal et al. (2019), as we find that individual word tokens carry parallel meanings. We present a detailed analysis of our model’s behavior using clustering and visualization techniques; this bears a resemblance to the analysis by Tenney et al. (2019), although they use internal representations to make predictions for linguistic tasks, rather than examining correlations between representations and individual input tokens. 3 Task and Data As described above, our task is a text-based question-answering task for an agent that has a fixed set of responses. The goal is to classify input queries as paraphrases of canonical questions that the agent knows how to answer, so we call this a question identification task. Data are collected from actual user interactions with a virtual patient, which is a graphical avatar with a text input interface and"
2020.sigdial-1.24,W18-5446,0,0.0209544,"arch has used attention weights to illustrate what input is important for a task, the complexities of our dialogue corpus offer a unique opportunity to examine how the model represents what it attends to, and we offer a detailed analysis of how that contributes to improved performance on rare classes. A particularly interesting phenomenon we observe is that the model picks up implicit meanings by splitting different aspects of the semantics of a single word across multiple attention heads. 1 Introduction Many semantic classification tasks have seen a huge boost in performance in recent years (Wang et al., 2018, 2019), thanks to the power of contextualized language models such as BERT (Devlin et al., 2019), which uses a Transformer (Vaswani et al., 2017) architecture to produce context-specific word embeddings for use in downstream classification tasks. These large, datahungry models are not always well suited to tasks that have a large number of classes or relatively small data sets (Mahabal et al., 2019). One task having both of these inauspicious properties is the Virtual Patient corpus (Jin et al., 2017), a collection of dialogues between medical students and a virtual patient experiencing back"
2021.acl-long.525,K15-2006,0,0.0144595,"proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum, 6738 2012). In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016). Finally, Do et al. (2017) explored the full probability space of semantic arguments; however, the method does not scale well. 3 Task Formulation: Material State Transfer Graph A (I-I) Dilute 5x buffer to 1x (???). Store (???) in a tube. PROD (I-E) Dilute 5x buffer to 1x (???). Store the Sol in a tube. A (E-I) Dilute 5x buffer to 1x Sol . Store (???) in a tube. COREF (E-E) Dilute 5x buffer to 1x Sol. Store the Sol in a tube. Temporal Relation Sub-Groups (a) Four different cases of implicit arguments Distance in #Sentences 3 4 5 1 2 (I-I) 93.1 4.3 1.4 0.0 0.0 1.2 (I-E) 79"
2021.acl-long.525,P13-1116,0,0.0197349,"2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum, 6738 2012). In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016). Finally, Do et al. (2017) explored the full probability space of semantic arguments; however, the method does not scale well. 3 Task Formulation: Material State Transfer Graph A (I-I) Dilute 5x buffer to 1x (???). Store (???) in a tube. PROD (I-E) Dilute 5x buffer to 1x (???). Store the Sol in a tube. A (E-I) Dilute 5x buffer to 1x Sol . Store (???) in a tube. COREF (E-E) Dilute 5x buffer to 1x Sol. Store the Sol in a tube. Temp"
2021.acl-long.525,I17-1010,0,0.0127664,"aphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum, 6738 2012). In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016). Finally, Do et al. (2017) explored the full probability space of semantic arguments; however, the method does not scale well. 3 Task Formulation: Material State Transfer Graph A (I-I) Dilute 5x buffer to 1x (???). Store (???) in a tube. PROD (I-E) Dilute 5x buffer to 1x (???). Store the Sol in a tube. A (E-I) Dilute 5x buffer to 1x Sol . Store (???) in a tube. COREF (E-E) Dilute 5x buffer to 1x Sol. Store the Sol in a tube. Temporal Relation Sub-Groups (a) Four different cases of implicit arguments Distance in #Sentences 3 4 5 1 2 (I-I) 93.1 4.3 1.4 0.0 0.0 1.2 (I-E) 79.6 12.8 2.8 1.5 0.2 3.2 (E-I) 76.0 12.0 12.0 0.0"
2021.acl-long.525,D17-1018,0,0.0208217,"w (sij )]) (1) where, ti and tj are the first and last token representation. Note, φsh (sij ) is a soft head representation (Bahdanau et al., 2014) and, φw (sij ) is a learnt span width embedding respectively. Further, φpos (sij ) and φstep (sij ) are two positional embeddings, the former for within sentence while the latter defines the step position within the protocol respectively. Hence, host culture and host culture and are two valid spans that are enumerated through this process. Span Pruning: Next, low scoring spans are filtered out during both training and evaluation phases. Following (Lee et al., 2017), the scoring function is implemented as a feed-forward network φs (eij ) = wsT FFNNs (eij ). We rank and pick a number of top scoring spans per sentence by using a combination of (i): a maximum fraction λp = 0.1 of spans per sentence, and (ii): a minimum score threshold λt = 0.5. Thus, the span host culture receives a significantly higher score than host culture and, indicating that the former is the correct reagent entity in the prescribed step. These span candidates are then passed to the transcoder block. 4.2 Transcoder Block In the transcoder block, we propose a novel architecture to impr"
2021.acl-long.525,P10-1160,0,0.0342359,"Long range relations are understudied in literature. Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum, 6738 2012). In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016). Finally, Do et al. (2017) explored the full probability space of semantic arguments; however, the method does not scale well. 3 Task Formulation: Material State Transfer Graph A (I-I) Dilute 5x buffer to 1x (???). Store (???) in a tube. PROD (I-E) Dilute 5x buffer to 1x (???"
2021.acl-long.525,J12-4003,0,0.0371899,"entences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum, 6738 2012). In contrast, a few notable studies used unlabeled training data to resolve implicit arguments (Chiarcos and Schenk, 2015; Schenk et al., 2016). Finally, Do et al. (2017) explored the full probability space of semantic arguments; however, the method does not scale well. 3 Task Formulation: Material State Transfer Graph A (I-I) Dilute 5x buffer to 1x (???). Store (???) in a tube. PROD (I-E) Dilute 5x buffer to 1x (???). Store the Sol in a tube. A (E-I) Dilute 5x buffer to 1x Sol . Store (???) in a tube. COREF (E-E) Dilute 5x buffer to 1x Sol. Stor"
2021.acl-long.525,P19-1024,0,0.0182471,"and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation Extraction: Long range relations are understudied in literature. Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum, 6738 2012). In contrast,"
2021.acl-long.525,D19-1041,0,0.0150994,"on latent structures to resolve implicit arguments and long-range relations spanning multiple sentences. In Section 2, we describe related works and in Section 3, we introduce MSTGs highlighting the two challenges. Next, we describe our proposed model in Section 4 and demonstrate its performance in Section 5. 2 Related Work Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017). Neural-networkbased methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018). Notably, Han et al. (2019) use neural support vector machine which can be difficult to train. Early methods for extracting causal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation Extraction: Long range relations are understudied in lit"
2021.acl-long.525,N18-2016,1,0.928724,"nd distant parts, and built upon implicit information that were referenced earlier or omitted entirely. Lack of careful documentation has led to a reproducibility crisis (Baker, 2016) in the biosciences and also poses considerable challenges for automation of laboratory procedures: gleaning the effect and semantics of actions requires understanding the underlying experiment, the sentence structure and rationale behind implicitly stated arguments. Currently, there is a dearth of annotated resources for natural language instructions in laboratory protocols. The WLP corpus initially collected by Kulkarni et al. (2018) and later updated by Tabassum et al. (2020) focused solely on relations within sentences. However, actions in WLPs are more complex, containing additional relations between actions (e.g., temporal and causal rela6737 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6737–6750 August 1–6, 2021. ©2021 Association for Computational Linguistics tions). We propose using material state transfer graphs (MSTG), which are a natural extension of Action Graphs (Kulkarni et al., 2018)."
2021.acl-long.525,N18-2108,0,0.0212001,"Missing"
2021.acl-long.525,E17-1108,0,0.0597792,"Missing"
2021.acl-long.525,P18-1049,0,0.0127559,"d (ii): a novel model that builds upon latent structures to resolve implicit arguments and long-range relations spanning multiple sentences. In Section 2, we describe related works and in Section 3, we introduce MSTGs highlighting the two challenges. Next, we describe our proposed model in Section 4 and demonstrate its performance in Section 5. 2 Related Work Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017). Neural-networkbased methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018). Notably, Han et al. (2019) use neural support vector machine which can be difficult to train. Early methods for extracting causal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation Extraction: Long range relat"
2021.acl-long.525,D17-1092,0,0.0116238,"al relationships and (ii): a novel model that builds upon latent structures to resolve implicit arguments and long-range relations spanning multiple sentences. In Section 2, we describe related works and in Section 3, we introduce MSTGs highlighting the two challenges. Next, we describe our proposed model in Section 4 and demonstrate its performance in Section 5. 2 Related Work Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017). Neural-networkbased methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018). Notably, Han et al. (2019) use neural support vector machine which can be difficult to train. Early methods for extracting causal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation E"
2021.acl-long.525,2020.wnut-1.36,0,0.0439106,"Missing"
2021.acl-long.525,D17-1108,0,0.0243057,"et al., 2018) by including intra- and cross-sentence temporal and causal relationships and (ii): a novel model that builds upon latent structures to resolve implicit arguments and long-range relations spanning multiple sentences. In Section 2, we describe related works and in Section 3, we introduce MSTGs highlighting the two challenges. Next, we describe our proposed model in Section 4 and demonstrate its performance in Section 5. 2 Related Work Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017). Neural-networkbased methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018). Notably, Han et al. (2019) use neural support vector machine which can be difficult to train. Early methods for extracting causal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such met"
2021.acl-long.525,D19-1585,0,0.300064,"blish a temporal relation between Step 1 and Step 5). To automate the generation of MSTGs, we must overcome two distinct challenges prevalent in WLPs. First, the result of a preceding step may not be immediately used by the next step, resulting in long-range dependencies. Second, an action may involve implicit information, which is either mentioned earlier or omitted entirely. Current models usually fail to make accurate predictions for long-range relations, as seen in Figure 1 when establishing a temporal relation between Step 1 and Step 5. These methods rely on relation propagation (DyGIE++ Wadden et al. (2019)) or use contextual embeddings (spERT Eberts and Ulges (2019)). Furthermore, neither successfully establish complex relations involving implicit arguments. In Step 5, the host culture and viral concentrate must be added to the tube containing soft agar that was removed in Step 4. However, the location tube in Step 5 is implicit and has to be correctly inferred to make the Site relation between Remove and Add. We propose a novel and effective neural network model that: (i): uses a series of relational convolutions to learn from relations within and across multiple action phrases and (ii): itera"
2021.acl-long.525,Q17-1008,0,0.0213818,"ausal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation Extraction: Long range relations are understudied in literature. Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Lapa"
2021.acl-long.525,P15-1061,0,0.0142804,"rformance in Section 5. 2 Related Work Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017). Neural-networkbased methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018). Notably, Han et al. (2019) use neural support vector machine which can be difficult to train. Early methods for extracting causal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation Extraction: Long range relations are understudied in literature. Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful"
2021.acl-long.525,C14-1220,0,0.0123472,"osed model in Section 4 and demonstrate its performance in Section 5. 2 Related Work Temporal and Causal Relation Extraction: Prior efforts have shown great promise in learning local and global features (Leeuwenberg and Moens, 2017; Ning et al., 2017). Neural-networkbased methods have proven effective (Meng et al., 2017; Meng and Rumshisky, 2018). Notably, Han et al. (2019) use neural support vector machine which can be difficult to train. Early methods for extracting causal relations resorted to feature engineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation Extraction: Long range relations are understudied in literature. Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden e"
2021.acl-long.525,K16-2005,0,0.0224523,"Missing"
2021.acl-long.525,2020.wnut-1.38,0,0.0268541,"Missing"
2021.acl-long.525,D18-1246,0,0.0212459,"ngineering (Bethard and Martin, 2008; Yang and Mao, 2014). Recently several researchers (Zeng et al., 2014; Nguyen and Grishman, 2015; Santos et al., 2015) used convolutional neural networks (CNNs) for extracting causal features. Notably, Li and Mao (2019) addressed scarcity of training data thorough knowledge-based CNN. However, such methods are not scalable to multiple sentences. Cross Sentence Relation Extraction: Long range relations are understudied in literature. Prior work focused on relations within a sentence or at best between pairs of sentences (Peng et al., 2017; Lee et al., 2018; Song et al., 2018; Guo et al., 2019). In addition to joint entity and relation extraction models, Wadden et al. (2019) proposed a model that passes useful information across graphs over cross-sentence contexts while Eberts and Ulges (2019) encoded per sentence contextual information for relation extraction over longer sentences. Implicit Arguments: Early methods selected specific features to build linear classifiers (Gerber and Chai, 2010, 2012). Others incorporated additional, manually-constructed resources like named entity taggers and WordNet (Gerber and Chai, 2012; Laparra and Rigau, 2013; Fellbaum, 6738 2"
2021.naacl-demos.13,2020.acl-main.51,0,0.450589,"e fascinating future research. 3 Identifying Stable Embeddings for Analysis While embeddings are a well-established means of capturing syntax and semantics from natural language text (Boleda, 2020), the problem of comparing multiple sets of embeddings remains an active area of research. The typical approach is to consider the nearest neighbors of specific points, consistent with the “similar items have similar representations” intuition of embeddings. This method also avoids the conceptual difficulties and low replicability of comparing embedding spaces numerically (e.g. by cosine distances) (Gonen et al., 2020). However, even nearest neighborhoods are often unstable, and vary dramatically across runs of the same embedding algorithm on the same corpus (Wendlandt et al., 2018; Antoniak and Mimno, 2018). In a setting such as our case study, the relatively small sub-corpora we use (typically less than 100 million tokens each) exacerbate this instability. Therefore, to quantify variation across embedding replicates and identify informative concepts, we introduce a measure of embedding confidence.2 We define embedding confidence as the mean overlap between the top k nearest neighbors of a 2 An embedding r"
2021.naacl-demos.13,P16-1141,0,0.167616,"text corpora can act as tating inter-corpus analysis (Liu et al., 2012; Weiss, a lens into the social and cultural context in which 2014; Liu et al., 2019). those corpora were produced (Nguyen et al., 2020). We introduce TextEssence, a novel tool that com- Diachronic word embeddings have been shown to bines the strengths of these prior lines of research reflect important context behind the corpora they 106 Proceedings of NAACL-HLT 2021: Demonstrations, pages 106–115 June 6–11, 2021. ©2021 Association for Computational Linguistics are trained on, such as cultural shifts (Kulkarni et al., 2015; Hamilton et al., 2016; Garg et al., 2018), world events (Kutuzov et al., 2018), and changes in scientific and professional practice (Vylomova et al., 2019). However, these analyses have proceeded independently of work on interactive tools for exploring embeddings, which are typically limited to visual projections (Zhordaniya et al.; Warmerdam et al., 2020). TextEssence combines these directions into a single general-purpose tool for interactively studying differences between any set of corpora, whether categorical or diachronic. 2.1 From words to domain concepts When corpora of interest are drawn from specialized"
2021.naacl-demos.13,2020.acl-demos.22,0,0.0405239,"focus entirely on the characteristics of a given corpus. Second, the scope of what static embedding methods are able to capture from a given corpus has been well-established in the literature, but is an area of current investigation for contextualized models (Jawahar et al., 2019; Zhao and Bethard, 2020). Finally, the nature of contextualized representations makes them best suited for context-sensitive tasks, while static embeddings capture aggregate patterns that lend themselves to corpus-level analysis. Nevertheless, as work on qualitative and visual analysis of contextualized models grows (Hoover et al., 2020), new opportunities for comparative analysis of local contexts will provide fascinating future research. 3 Identifying Stable Embeddings for Analysis While embeddings are a well-established means of capturing syntax and semantics from natural language text (Boleda, 2020), the problem of comparing multiple sets of embeddings remains an active area of research. The typical approach is to consider the nearest neighbors of specific points, consistent with the “similar items have similar representations” intuition of embeddings. This method also avoids the conceptual difficulties and low replicabil"
2021.naacl-demos.13,P19-1356,0,0.0221972,"dings from knowledge graph structure is omitted here for brevity. NLP than static (i.e., non-contextualized) embeddings. However, static embeddings have several advantages for this comparative use case. First, they are less resource-intensive than contextualized models, and can be efficiently trained several times without pre-training to focus entirely on the characteristics of a given corpus. Second, the scope of what static embedding methods are able to capture from a given corpus has been well-established in the literature, but is an area of current investigation for contextualized models (Jawahar et al., 2019; Zhao and Bethard, 2020). Finally, the nature of contextualized representations makes them best suited for context-sensitive tasks, while static embeddings capture aggregate patterns that lend themselves to corpus-level analysis. Nevertheless, as work on qualitative and visual analysis of contextualized models grows (Hoover et al., 2020), new opportunities for comparative analysis of local contexts will provide fascinating future research. 3 Identifying Stable Embeddings for Analysis While embeddings are a well-established means of capturing syntax and semantics from natural language text (Bo"
2021.naacl-demos.13,W19-5034,0,0.0164237,"d of October 2020; while additions of new sources over time led to occasional jumps 4.3 Compare: tracking pair similarity in corpus volumes, all are sufficiently large for emThe Compare view facilitates analysis of the chang- bedding training. We created disjoint sub-corpora ing relationship between two or more concepts containing the new articles indexed in CORD-19 across corpora (e.g. from month to month). This each month for our case study. view displays paired nearest neighbor tables, one CORD-19 monthly corpora were tokenized usper corpus, showing the aggregate nearest neighing ScispaCy (Neumann et al., 2019), and conbors of each of the concepts being compared. An cept embeddings were trained using JET (Newmanadjacent line graph depicts the similarity between Griffis et al., 2018), a weakly-supervised concept the concepts in each corpus, with one concept specembedding method that does not require explicit ified as the reference item and the others serving as corpus annotations. We used SNOMED Clinical comparison items (similar to Figure 3). Similarity Terms (SNOMED CT), a widely-used reference between two concepts for a specific corpus is calrepresenting concepts used in clinical reporting, as cul"
2021.naacl-demos.13,D19-6218,1,0.88945,"Missing"
2021.naacl-demos.13,C18-1117,0,0.0247702,"et al., 2012; Weiss, a lens into the social and cultural context in which 2014; Liu et al., 2019). those corpora were produced (Nguyen et al., 2020). We introduce TextEssence, a novel tool that com- Diachronic word embeddings have been shown to bines the strengths of these prior lines of research reflect important context behind the corpora they 106 Proceedings of NAACL-HLT 2021: Demonstrations, pages 106–115 June 6–11, 2021. ©2021 Association for Computational Linguistics are trained on, such as cultural shifts (Kulkarni et al., 2015; Hamilton et al., 2016; Garg et al., 2018), world events (Kutuzov et al., 2018), and changes in scientific and professional practice (Vylomova et al., 2019). However, these analyses have proceeded independently of work on interactive tools for exploring embeddings, which are typically limited to visual projections (Zhordaniya et al.; Warmerdam et al., 2020). TextEssence combines these directions into a single general-purpose tool for interactively studying differences between any set of corpora, whether categorical or diachronic. 2.1 From words to domain concepts When corpora of interest are drawn from specialized domains, such as medicine, it is often necessary to shift"
2021.naacl-demos.13,W18-3026,1,0.698922,"he shared knowledge that underpins discourse within these communities. Reified domain concepts may be referred to by multi-word surface forms (e.g., “Lou Gehrig’s disease”) and multiple distinct surface forms (e.g., “Lou Gehrig’s disease” and “amyotrophic lateral sclerosis”), making them more semantically powerful but also posing distinct challenges from traditional word-level representations. A variety of embedding algorithms have been developed for learning representations of domain concepts and real-world entities from text, including weakly-supervised methods requiring only a terminology (Newman-Griffis et al., 2018); methods using pre-trained NER models for noisy annotation (De Vine et al., 2014; Chen et al., 2020); and methods leveraging explicit annotations of concept mentions (as in Wikipedia) (Yamada et al., 2020).1 These algorithms capture valuable patterns about concept types and relationships that can inform corpus analysis (Runge and Hovy, 2020). TextEssence only requires pre-trained embeddings as input, so it can accommodate any embedding algorithm suiting the needs and characteristics of specific corpora (e.g. availability of annotations or knowledge graph resources). Furthermore, while the rem"
2021.naacl-demos.13,2020.blackboxnlp-1.20,0,0.030198,"from traditional word-level representations. A variety of embedding algorithms have been developed for learning representations of domain concepts and real-world entities from text, including weakly-supervised methods requiring only a terminology (Newman-Griffis et al., 2018); methods using pre-trained NER models for noisy annotation (De Vine et al., 2014; Chen et al., 2020); and methods leveraging explicit annotations of concept mentions (as in Wikipedia) (Yamada et al., 2020).1 These algorithms capture valuable patterns about concept types and relationships that can inform corpus analysis (Runge and Hovy, 2020). TextEssence only requires pre-trained embeddings as input, so it can accommodate any embedding algorithm suiting the needs and characteristics of specific corpora (e.g. availability of annotations or knowledge graph resources). Furthermore, while the remainder of this paper primarily refers to concepts, TextEssence can easily be used for word-level embeddings in addition to concepts. 2.2 Why static embeddings? Contextualized, language model-based embeddings can provide more discriminative features for 1 The significant literature on learning embeddings from knowledge graph structure is omitt"
2021.naacl-demos.13,2020.semeval-1.1,0,0.0262514,"e. of embeddings as a lens to investigate those reguThe remainder of the paper is organized as follarities, and what they reveal about different text lows. §2 lays out the conceptual background becorpora, has been fairly limited. Prior work using hind TextEssence and its utility as a corpus analyembeddings to study language shifts, such as the use of diachronic embeddings to measure seman- sis tool. In §3 and §4, we describe the nearestneighbor analysis and user interface built into tic change in specific words over time (Hamilton TextEssence. §5 describes our case study on scienet al., 2016; Schlechtweg et al., 2020), has focused primarily on quantitative measurement of change, tific literature related to COVID-19, and §6 highrather than an interactive exploration of its quali- lights key directions for future research. tative aspects. On the other hand, prior work on 2 Background interactive analysis of text collections has focused on analyzing individual corpora, rather than facili- Computational analysis of text corpora can act as tating inter-corpus analysis (Liu et al., 2012; Weiss, a lens into the social and cultural context in which 2014; Liu et al., 2019). those corpora were produced (Nguyen et al"
2021.naacl-demos.13,W19-4704,0,0.0191509,"014; Liu et al., 2019). those corpora were produced (Nguyen et al., 2020). We introduce TextEssence, a novel tool that com- Diachronic word embeddings have been shown to bines the strengths of these prior lines of research reflect important context behind the corpora they 106 Proceedings of NAACL-HLT 2021: Demonstrations, pages 106–115 June 6–11, 2021. ©2021 Association for Computational Linguistics are trained on, such as cultural shifts (Kulkarni et al., 2015; Hamilton et al., 2016; Garg et al., 2018), world events (Kutuzov et al., 2018), and changes in scientific and professional practice (Vylomova et al., 2019). However, these analyses have proceeded independently of work on interactive tools for exploring embeddings, which are typically limited to visual projections (Zhordaniya et al.; Warmerdam et al., 2020). TextEssence combines these directions into a single general-purpose tool for interactively studying differences between any set of corpora, whether categorical or diachronic. 2.1 From words to domain concepts When corpora of interest are drawn from specialized domains, such as medicine, it is often necessary to shift analysis from individual words to domain concepts, which serve to reify the"
2021.naacl-demos.13,2021.naacl-demos.8,0,0.0264654,"Missing"
2021.naacl-demos.13,N18-1190,0,0.0948847,"tural language text (Boleda, 2020), the problem of comparing multiple sets of embeddings remains an active area of research. The typical approach is to consider the nearest neighbors of specific points, consistent with the “similar items have similar representations” intuition of embeddings. This method also avoids the conceptual difficulties and low replicability of comparing embedding spaces numerically (e.g. by cosine distances) (Gonen et al., 2020). However, even nearest neighborhoods are often unstable, and vary dramatically across runs of the same embedding algorithm on the same corpus (Wendlandt et al., 2018; Antoniak and Mimno, 2018). In a setting such as our case study, the relatively small sub-corpora we use (typically less than 100 million tokens each) exacerbate this instability. Therefore, to quantify variation across embedding replicates and identify informative concepts, we introduce a measure of embedding confidence.2 We define embedding confidence as the mean overlap between the top k nearest neighbors of a 2 An embedding replicate here refers to the embedding matrix output by running a specific embedding training algorithm on a specific corpus. Ten runs of word2vec on a given Wikipedia"
2021.naacl-demos.13,2020.emnlp-demos.4,0,0.135264,", “Lou Gehrig’s disease” and “amyotrophic lateral sclerosis”), making them more semantically powerful but also posing distinct challenges from traditional word-level representations. A variety of embedding algorithms have been developed for learning representations of domain concepts and real-world entities from text, including weakly-supervised methods requiring only a terminology (Newman-Griffis et al., 2018); methods using pre-trained NER models for noisy annotation (De Vine et al., 2014; Chen et al., 2020); and methods leveraging explicit annotations of concept mentions (as in Wikipedia) (Yamada et al., 2020).1 These algorithms capture valuable patterns about concept types and relationships that can inform corpus analysis (Runge and Hovy, 2020). TextEssence only requires pre-trained embeddings as input, so it can accommodate any embedding algorithm suiting the needs and characteristics of specific corpora (e.g. availability of annotations or knowledge graph resources). Furthermore, while the remainder of this paper primarily refers to concepts, TextEssence can easily be used for word-level embeddings in addition to concepts. 2.2 Why static embeddings? Contextualized, language model-based embedding"
2021.naacl-demos.13,2020.acl-main.429,0,0.0271314,"raph structure is omitted here for brevity. NLP than static (i.e., non-contextualized) embeddings. However, static embeddings have several advantages for this comparative use case. First, they are less resource-intensive than contextualized models, and can be efficiently trained several times without pre-training to focus entirely on the characteristics of a given corpus. Second, the scope of what static embedding methods are able to capture from a given corpus has been well-established in the literature, but is an area of current investigation for contextualized models (Jawahar et al., 2019; Zhao and Bethard, 2020). Finally, the nature of contextualized representations makes them best suited for context-sensitive tasks, while static embeddings capture aggregate patterns that lend themselves to corpus-level analysis. Nevertheless, as work on qualitative and visual analysis of contextualized models grows (Hoover et al., 2020), new opportunities for comparative analysis of local contexts will provide fascinating future research. 3 Identifying Stable Embeddings for Analysis While embeddings are a well-established means of capturing syntax and semantics from natural language text (Boleda, 2020), the problem"
2021.naacl-demos.13,2020.nlposs-1.8,0,0.0341947,"or lines of research reflect important context behind the corpora they 106 Proceedings of NAACL-HLT 2021: Demonstrations, pages 106–115 June 6–11, 2021. ©2021 Association for Computational Linguistics are trained on, such as cultural shifts (Kulkarni et al., 2015; Hamilton et al., 2016; Garg et al., 2018), world events (Kutuzov et al., 2018), and changes in scientific and professional practice (Vylomova et al., 2019). However, these analyses have proceeded independently of work on interactive tools for exploring embeddings, which are typically limited to visual projections (Zhordaniya et al.; Warmerdam et al., 2020). TextEssence combines these directions into a single general-purpose tool for interactively studying differences between any set of corpora, whether categorical or diachronic. 2.1 From words to domain concepts When corpora of interest are drawn from specialized domains, such as medicine, it is often necessary to shift analysis from individual words to domain concepts, which serve to reify the shared knowledge that underpins discourse within these communities. Reified domain concepts may be referred to by multi-word surface forms (e.g., “Lou Gehrig’s disease”) and multiple distinct surface for"
2021.naacl-demos.13,W14-3108,0,0.0704943,"Missing"
D17-1302,C16-1126,0,0.0258994,"Missing"
D17-1302,P13-2112,0,0.0215578,"mbeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can"
D17-1302,W04-3229,0,0.125229,"Missing"
D17-1302,D14-1181,0,0.00177753,"the i-th sentence in the minibatch, and pˆi,j is the predicted tag. In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections. Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016). First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014). The encoder is with three convolution filters whose sizes are 3, 4, and 5. For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector. Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer. The discriminator is implemented 1 We also tried isolated character-level modules but the overall performance was worse. 2833 as a fully-connected neural network w"
D17-1302,D15-1150,1,0.847344,"ter embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can be shared for kno"
D17-1302,C16-1038,1,0.73952,"eddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages. Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets. In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings. Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al2832 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics Softmax Output Language Discriminator ?1 Gradient Reversal ?2 … ?? Bidirectional Language Model CNN/MaxPool Encoder ?2 ?3 ???? ?1 ???? … ??−1 Common BLSTM ℎ1?? ?? ℎ1 ? ℎ1 ℎ2?? ?? … ℎ2 Private BLSTM ?? ?? ℎ? ℎ1 ℎ? ?? ℎ1 ? ℎ1 ? ℎ2 ℎ? ?? ℎ2 ?? ?? ℎ2 ? ? ℎ2 ?? … ℎ? ?? ℎ? ? ℎ? Wor"
D17-1302,P15-1046,1,0.653527,"ter embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can be shared for kno"
D17-1302,D15-1176,0,0.0797098,"Missing"
D17-1302,P16-2067,0,0.365392,"aluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language. 1 Introduction Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic r"
D17-1302,P17-1194,0,0.011091,"passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic. The loss function of the language classifier is formulated as: La = − S X li log lˆi , (2) i=1 where S is the number of sentences, li is the language of the i-th sentence, and lˆi is the softmax output of the tagging. Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic. Bidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word. For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy &lt;eos&gt;” and the backward LSTM predicts “&lt;bos&gt; I am”. This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017). We adopted the bidirectional languag"
D17-1302,Q13-1001,0,0.0992589,"Missing"
D17-1302,D14-1187,0,0.149168,"Missing"
D17-1302,N16-1156,0,0.0461915,"e shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017). Given insufficient training examples, we can improve the POS tagging performance by crosslingual POS tagging, which exploits affluent POS tagging corpora from other source languages. This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (T¨ackstr¨om et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016). Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead. Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space. When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collo"
D19-3015,W18-2301,1,0.341468,"sessment and reassessment notes from the Rehabilitation Medicine Department of the NIH Clinical Center. These text documents have been annotated at the token level for descriptions and assessments of patient mobility status. Further information on this dataset is given in Table 1. We use ten-fold cross validation for our experiments, splitting into folds at the document level. 3.1 Ranking Figure 1: HARE workflow for working with a set of documents; outlined boxes indicate automated components, and gray boxes signify user interfaces. and semantically complex, and difficult to extract reliably (Newman-Griffis and Zirikly, 2018; Newman-Griffis et al., 2019). Some characterization of mobility-related terms has been performed as part of larger work on functioning (Skube et al., 2018), but a lack of standardized terminologies limits the utility of vocabulary-driven clinical NLP tools such as CLAMP (Soysal et al., 2018) or cTAKES (Savova et al., 2010). Thus, it forms a useful test case for HARE. 3 Viewer Feature Extraction 3.1.2 Feature extraction Our system supports feature extraction for individual tokens in input documents using both static and contextualized word embeddings. Static embeddings Using static (i.e., non"
D19-3015,N18-1202,0,0.0199775,"t forms a useful test case for HARE. 3 Viewer Feature Extraction 3.1.2 Feature extraction Our system supports feature extraction for individual tokens in input documents using both static and contextualized word embeddings. Static embeddings Using static (i.e., noncontextualized) embeddings, we calculate input features for each token as the mean embedding of the token and 10 words on each side (truncated at sentence/line breaks). We used FastText (Bojanowski et al., 2017) embeddings trained on a 10year collection of physical and occupational therapy records from the NIH Clinical Center. ELMo (Peters et al., 2018) ELMo features are calculated for each token by taking the hidden states of the two bLSTM layers and the token layer, multiplying each vector by learned weights, and summing to produce a final embedding. Combination weights are trained jointly with the token annotation model. We used a 1024-dimensional Relevance tagging workflow All hyperparameters discussed in this section were tuned on held-out development data in cross3 86 https://arxiv.org/abs/1908.11302 (a) No collapsing Figure 2: Precision, recall, and F-2 when varying binarization threshold from 0 to 1, using ELMo embeddings. The thresh"
D19-3015,W19-1909,0,0.0216342,"Missing"
D19-3015,Q17-1010,0,0.0076131,"erminologies limits the utility of vocabulary-driven clinical NLP tools such as CLAMP (Soysal et al., 2018) or cTAKES (Savova et al., 2010). Thus, it forms a useful test case for HARE. 3 Viewer Feature Extraction 3.1.2 Feature extraction Our system supports feature extraction for individual tokens in input documents using both static and contextualized word embeddings. Static embeddings Using static (i.e., noncontextualized) embeddings, we calculate input features for each token as the mean embedding of the token and 10 words on each side (truncated at sentence/line breaks). We used FastText (Bojanowski et al., 2017) embeddings trained on a 10year collection of physical and occupational therapy records from the NIH Clinical Center. ELMo (Peters et al., 2018) ELMo features are calculated for each token by taking the hidden states of the two bLSTM layers and the token layer, multiplying each vector by learned weights, and summing to produce a final embedding. Combination weights are trained jointly with the token annotation model. We used a 1024-dimensional Relevance tagging workflow All hyperparameters discussed in this section were tuned on held-out development data in cross3 86 https://arxiv.org/abs/1908"
D19-3015,N19-1423,0,0.00996208,"orkflow All hyperparameters discussed in this section were tuned on held-out development data in cross3 86 https://arxiv.org/abs/1908.11302 (a) No collapsing Figure 2: Precision, recall, and F-2 when varying binarization threshold from 0 to 1, using ELMo embeddings. The threshold corresponding to the best F-2 is marked with a dotted vertical line. (b) Collapse one blank Figure 3: Collapsing adjacent segments illustration. old of 1e-05 on this F-2 score, with a patience of 5 epochs and a maximum of 50 epochs of training. ELMo model pretrained on PubMed data4 for our mobility experiments. BERT (Devlin et al., 2019) For BERT features, we take the hidden states of the final k layers of the model; as with ELMo embeddings, these outputs are then multiplied by a learned weight vector, and the weighted layers are summed to create the final embedding vectors.5 We used the 768-dimensional clinicalBERT (Alsentzer et al., 2019) model6 in our experiments, extracting features from the last 3 layers. 3.2 Post-processing methods Given a set of token-level relevance annotations, HARE provides three post-processing techniques for analyzing and improving annotation results. Decision thresholding The threshold for binari"
D19-3015,1983.tc-1.13,0,0.150111,"Missing"
D19-6218,drouin-2004-detection,0,0.01578,"number of terminology string match instances and number of unique concepts embedded, respectively, using SNOMED-CT and LOINC vocabularies from UMLS 2017AB release. The number of highconfidence concepts identified for each document type is given with their mean consistency. use between different expert communities. (Denecke, 2014), as well as more structural information such as document section patterns and syntactic features (Zeng et al., 2011; Temnikova et al., 2014). The use of terminologies to assess conceptual features of a sublanguage corpus was proposed by Walker and Amsler (1986), and Drouin (2004); Gr¨on et al. (2019) used sublanguage features to expand existing terminologies, but largescale characterization of concept usage in sublanguage has remained a challenging question. 3 Data and preprocessing We use free text notes from the MIMIC-III critical care database (Johnson et al., 2016) for our analysis. This includes approximately 2 million text records from hospital admissions of almost 50 thousand patients to the critical care units of Beth Israel Deaconess Medical Center over a 12-year period. Each document belongs to one of 15 document types, listed in Table 1. As sentence segment"
D19-6218,L16-1733,0,0.0363198,"Missing"
D19-6218,W18-3026,1,0.633456,"Missing"
D19-6218,P19-1317,0,0.0630166,"Missing"
D19-6218,W19-5022,0,0.0570439,"Missing"
D19-6218,temnikova-etal-2014-sublanguage,0,0.0300911,"3 0 – 122 57 599 63 9 62 5 63 0 – Table 1: Document type subcorpora in MIMIC-III. Tokenization was performed with SpaCy; Matches and Concepts refer to number of terminology string match instances and number of unique concepts embedded, respectively, using SNOMED-CT and LOINC vocabularies from UMLS 2017AB release. The number of highconfidence concepts identified for each document type is given with their mean consistency. use between different expert communities. (Denecke, 2014), as well as more structural information such as document section patterns and syntactic features (Zeng et al., 2011; Temnikova et al., 2014). The use of terminologies to assess conceptual features of a sublanguage corpus was proposed by Walker and Amsler (1986), and Drouin (2004); Gr¨on et al. (2019) used sublanguage features to expand existing terminologies, but largescale characterization of concept usage in sublanguage has remained a challenging question. 3 Data and preprocessing We use free text notes from the MIMIC-III critical care database (Johnson et al., 2016) for our analysis. This includes approximately 2 million text records from hospital admissions of almost 50 thousand patients to the critical care units of Beth Isra"
D19-6218,P16-1141,0,0.343161,"r-lussier.1} @ osu.edu Abstract (e.g., “ALS” and “Lou Gehrig’s disease”), making them difficult to analyze using lexical occurrence alone. Understanding how these concepts differ between document types can not only augment recent methods for sublanguage-based text categorization (Feldman et al., 2016), but also inform the perennial challenge of medical concept normalization (Luo et al., 2019): “depression” is much easier to disambiguate if its occurrence is known to be in a social work note or an abdominal exam. Inspired by recent technological advances in modeling diachronic language change (Hamilton et al., 2016; Vashisth et al., 2019), we characterize concept usage differences within clinical sublanguages using nearest neighborhood structures of clinical concept embeddings. We show that overlap in nearest neighborhoods can reliably distinguish between document types while controlling for noise in the embedding process. Qualitative analysis of these nearest neighborhoods demonstrates that these distinctions are semantically relevant, highlighting sublanguage-sensitive relationships between specific concepts and between concepts and related surface forms. Our findings suggest that the structure of con"
D19-6218,W19-5037,0,0.138696,"Abstract (e.g., “ALS” and “Lou Gehrig’s disease”), making them difficult to analyze using lexical occurrence alone. Understanding how these concepts differ between document types can not only augment recent methods for sublanguage-based text categorization (Feldman et al., 2016), but also inform the perennial challenge of medical concept normalization (Luo et al., 2019): “depression” is much easier to disambiguate if its occurrence is known to be in a social work note or an abdominal exam. Inspired by recent technological advances in modeling diachronic language change (Hamilton et al., 2016; Vashisth et al., 2019), we characterize concept usage differences within clinical sublanguages using nearest neighborhood structures of clinical concept embeddings. We show that overlap in nearest neighborhoods can reliably distinguish between document types while controlling for noise in the embedding process. Qualitative analysis of these nearest neighborhoods demonstrates that these distinctions are semantically relevant, highlighting sublanguage-sensitive relationships between specific concepts and between concepts and related surface forms. Our findings suggest that the structure of concept embedding spaces no"
D19-6218,W19-4704,0,0.183984,"f clinical text is often optimized for specific document types (Griffis et al., 2016), we segmented our documents at linebreaks and tokenized using SpaCy (version 2.1.6; Honnibal and Montani 2017). All tokens were lowercased, but punctuation and deidentifier strings were retained, and no stopwords were removed. Word embedding techniques have been utilized to describe diachronic language change in a number of recent studies, from evaluating broad changes over decades (Hamilton et al., 2016; Vashisth et al., 2019) to detecting fine-grained shifts in conceptualizations of psychological concepts (Vylomova et al., 2019). Embedding techniques have also been used as a mirror to analyze social biases in language data (Garg et al., 2018). Similar to our work, Ye and Fabbri (2018) investigate document type-specific embeddings from clinical data as a tool for medical language analysis. However, our approach has two significant differences: Ye and Fabbri (2018) used word embeddings only, while we utilize concept embeddings to capture concepts across multiple surface forms; more importantly, their work investigated multiple document types as a way to control for specific usage patterns within sublanguages in order t"
D19-6218,N18-1190,0,0.0603293,"ngs with JET. Due to the size of our subcorpora, we used a window size of 5, minimum frequency of 5, embedding dimensionality of 100, initial learning rate of 2 We chose five nearest neighbors for our analyses based on qualitative review of neighborhoods for concepts within different document types. We found nearest neighborhoods for concept embeddings to vary more than for word embeddings, often introducing noise beyond the top five nearest neighbors; we therefore set a conservative baseline for reliability by focusing on the closest and most stable neighbors. However, using 10 neighbors, as Wendlandt et al. (2018) did, or more could yield different qualitative patterns in document type comparisons and bears exploration. 1 We used the versions distributed in the 2017AB release of the UMLS (Bodenreider, 2004). 148 (a) Number of concepts analyzed (b) Reference set self-consistency (d) Cross-type consistency (c) Comparison set self-consistency (e) Consistency deltas Figure 2: Comparison of concept neighborhood consistency statistics across document types, using highconfidence concepts from the reference type. Figure 2a provides the number of concepts shared between the high-confidence reference set and the"
H05-1072,P89-1011,0,0.325688,"ment of Computer Science and Engineering Department of Linguistics Department of Linguistics Ohio State University Ohio State University Columbus, OH 43210 Columbus, OH 43210 rytting@ling.ohio-state.edu fosler@cse.ohio-state.edu Abstract In the past decade, several researchers have started reinvestigating the use of sub-phonetic models for lexical representations within automatic speech recognition systems. Lest history repeat itself, it may be instructive to mine the further past for models of lexical representations in the lexical access literature. In this work, we re-evaluate the model of Briscoe (1989), in which a hybrid strategy of lexical representation between phones and manner classes is promoted. While many of Briscoe’s assumptions do not match up with current ASR processing models, we show that his conclusions are essentially correct, and that reconsidering this structure for ASR lexica is an appropriate avenue for future ASR research. 1 Introduction Almost every state-of-the-art large vocabulary automatic speech recognition (ASR) system requires the sharing of sub-word units in order to achieve the desired vocabulary coverage. Traditionally, these sub-word units are determined by the"
iosif-etal-2012-associative,J90-1003,0,\N,Missing
iosif-etal-2012-associative,W06-2501,0,\N,Missing
iosif-etal-2012-associative,C92-2082,0,\N,Missing
iosif-etal-2012-associative,P99-1016,0,\N,Missing
iosif-etal-2012-associative,O97-1002,0,\N,Missing
iosif-etal-2012-associative,J92-4003,0,\N,Missing
iosif-etal-2012-associative,P94-1019,0,\N,Missing
iosif-etal-2012-associative,J06-1003,0,\N,Missing
iosif-etal-2012-associative,P08-1017,0,\N,Missing
iosif-etal-2012-associative,C08-1114,0,\N,Missing
iosif-etal-2012-associative,N03-1011,0,\N,Missing
N06-2040,J96-2004,0,0.00577965,"with a defined spatial position (buttons, doors and cabinets). We excluded plural referring expressions, since their spatial properties are more complex, and also expressions annotated as vague or abandoned. Overall, the corpus contains 1736 markable items, of which 87 were annotated as vague, 84 abandoned and 228 sets. We annotated each referring expression with a boolean feature called Locate that indicates whether the expression is the first one that allowed the follower to identify the object in the world, in other words, the point at which joint spatial reference was achieved. The kappa (Carletta, 1996) obtained on this feature was 0.93. There were 466 referring expressions in the 15-dialog corpus that were annotated TRUE for this feature. The dataset used in the experiments is a consensus version on which both annotators agreed on the set of markables. Due to the constraints introduced by the task, referent annotation achieved almost perfect agreement. Annotators were allowed to look ahead in the dialog to assign the referent. The data used in the current study is only the DG’s language. 3 Algorithm Development The generation module receives as input a route plan produced by a planning modu"
N06-2040,P02-1048,0,0.208424,"Missing"
N06-2040,C92-1038,0,0.675169,"Missing"
N10-1110,N03-1028,0,0.0176261,"cs weighted feature functions, p(Q|X) = exp P P t j λj sj (qt , X) + P j µj fj (qt−1 , qt , X) Z(X) (1) where sj (·) and fj (·) are known as state feature functions and transition feature functions respectively, and λj and µj are the associated weights. Z(X) is a normalization term that ensures a valid probability distribution. Given a set of labeled examples, the CRF is trained to maximize the conditional log-likelihood of the training set. The log-likelihood is concave over the entire parameter space, and can be maximized using standard convex optimization techniques (Lafferty et al., 2001; Sha and Pereira, 2003). The local posterior probability of a particular label can be computed via a forwardbackward style algorithm. Mathematically, p(qt = q|X) = αt (q|X)βt (q|X) Z(X) (2) where αt (q|X) and βt (q|X) accumulate contributions associated with possible assignments of labels before and after the current time-step t. The Crandem system utilizes these local posterior values from the CRF analogously to the way in which MLP-posteriors are treated in the Tandem framework (Hermansky et al., 2000), by applying a log transformation to the posteriors. These transformed outputs are then decorrelated using a KL-t"
N12-1063,N04-1025,0,0.310084,"hers may have standard guidelines for content providers on visual layout, these guidelines likely differ from publisher to publisher and are not available for the general public. Moreover, in the digital age teachers are also content providers who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and shor"
N12-1063,C10-2032,0,0.228161,"l layout, these guidelines likely differ from publisher to publisher and are not available for the general public. Moreover, in the digital age teachers are also content providers who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and short passages, or articles from educational newspape"
N12-1063,N07-1058,0,0.630611,"oviders who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and short passages, or articles from educational newspapers. Since the datasets used are text-intensive, many efforts have been made to investigate text properties at a higher linguistic level, such as discourse analysis, language m"
N12-1063,P05-1065,0,0.3464,"or content providers on visual layout, these guidelines likely differ from publisher to publisher and are not available for the general public. Moreover, in the digital age teachers are also content providers who do not have access to these guidelines, so our proposed ranking system would be very helpful as they create reading materials such as worksheets, web pages, etc. 2 Related Work Due to the limitations of traditional approaches, more advanced methods which use statistical language processing techniques have been introduced by recent work in this area (Collins-Thompson and Callan, 2004; Schwarm and Ostendorf, 2005; Feng et al., 2010). Collins-Thompson and Callan (2004) used a smoothed unigram language model to predict the grade reading levels of web page documents and short passages. Heilman et al. (2007) combined a language modeling approach with grammarbased features to improve readability assessment for first and second language texts. Schwarm/Petersen and Ostendorf (2005; 2009) used a support vector machine to combine surface features with language models and parsed features. The datasets used in these previous related works mostly consist of web page documents and short passages, or articles from"
N12-1091,W11-0219,0,0.0618673,"Missing"
N12-1091,P02-1045,0,0.0273682,"entities that fall into five medical semantic categories commonly appearing in discharge summaries. However, we focus on feature extraction to determine the similarity between medical concepts, both in terms of meaning and time of occurrence, for resolving coreferences within and across all types of clinical narratives. A disadvantage of supervised machine learning approaches is the need for an unknown amount of annotated training data for optimal performance. Researchers then began to experiment with weakly supervised machine learning algorithms such as cotraining (Blum and Mitchell, 1998). Muller et al. (2002) investigate the practical applicability of cotraining for the task of building a classifier for coreference resolution and observed that the results were 2 https://www.i2b2.org/NLP/Coreference/ https://www.i2b2.org/NLP/Coreference/assets/ CoreferenceGuidelines.pdf 1 3 https://cabig-kc.nci.nih.gov/Vocab/KC/ index.php/OHNLP 732 mostly negative for their dataset. Ganchev et al. (2010) propose a posterior regularization framework for weakly supervised learning to derive a multi-view learning algorithm. Multi-view methods typically begin by assuming that each view alone can yield a good predictor."
N12-1091,P10-1142,0,0.0247941,"g temporal relations between medical events in clinical text include work by Jung et al. (2011) and Zhou et al. (2006). Gaizauskas et al. (2006) learn the temporal relations before, after, is included between events from a corpus of clinical text much like the event-event relation tlink learning in Timebank (Pustejovsky et al., 2003). A comprehensive survey of temporal reasoning in medical data is provided by Zhou and Hripcsak (2007). Chapman et al. (2011) discuss barriers to NLP development in the clinical domain. Coreference resolution is a well-studied problem in computational linguistics (Ng, 2010; Raghunathan et al., 2010). Supervised machine learning algorithms have been previously used for noun phrase coreference resolution with fairly good results (Soon et al., 2001; Raghunathan et al., 2010). Recently, the i2b2 challenge2 on coreference resolution examined coreference resolution in clinical data. The problem addressed in our paper is similar to the task described in the i2b2 challenge.3 Besides the i2b2 challenge, there has not been significant work in MCCR. This may be due to various privacy concerns and the efforts required to anonymize and annotate massive amounts of patient na"
N12-1091,D10-1048,0,0.0458664,"relations between medical events in clinical text include work by Jung et al. (2011) and Zhou et al. (2006). Gaizauskas et al. (2006) learn the temporal relations before, after, is included between events from a corpus of clinical text much like the event-event relation tlink learning in Timebank (Pustejovsky et al., 2003). A comprehensive survey of temporal reasoning in medical data is provided by Zhou and Hripcsak (2007). Chapman et al. (2011) discuss barriers to NLP development in the clinical domain. Coreference resolution is a well-studied problem in computational linguistics (Ng, 2010; Raghunathan et al., 2010). Supervised machine learning algorithms have been previously used for noun phrase coreference resolution with fairly good results (Soon et al., 2001; Raghunathan et al., 2010). Recently, the i2b2 challenge2 on coreference resolution examined coreference resolution in clinical data. The problem addressed in our paper is similar to the task described in the i2b2 challenge.3 Besides the i2b2 challenge, there has not been significant work in MCCR. This may be due to various privacy concerns and the efforts required to anonymize and annotate massive amounts of patient narratives. Zheng et al. (201"
N12-1091,J01-4004,0,0.0456209,"ations before, after, is included between events from a corpus of clinical text much like the event-event relation tlink learning in Timebank (Pustejovsky et al., 2003). A comprehensive survey of temporal reasoning in medical data is provided by Zhou and Hripcsak (2007). Chapman et al. (2011) discuss barriers to NLP development in the clinical domain. Coreference resolution is a well-studied problem in computational linguistics (Ng, 2010; Raghunathan et al., 2010). Supervised machine learning algorithms have been previously used for noun phrase coreference resolution with fairly good results (Soon et al., 2001; Raghunathan et al., 2010). Recently, the i2b2 challenge2 on coreference resolution examined coreference resolution in clinical data. The problem addressed in our paper is similar to the task described in the i2b2 challenge.3 Besides the i2b2 challenge, there has not been significant work in MCCR. This may be due to various privacy concerns and the efforts required to anonymize and annotate massive amounts of patient narratives. Zheng et al. (2011) review heuristic-based, supervised and unsupervised methods for coreference resolution in the context of the clinical domain. He (2007) studied co"
N15-1051,P98-1013,0,0.0598898,"Sheinman et al., 2013; Schulam and Fellbaum, 2010). de Melo and Bansal (2013) propose a novel Mixed Integer Linear Programming (MILP) based approach, publish a gold standard dataset and report the best performance on ordering scalar adjectives on this dataset. However, these approaches are limited in two ways. First, they depend on a manually created resource, such 483 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 483–493, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics as WordNet or FrameNet (Baker et al., 1998). Lexical patterns (e.g., ‘not just x but y’) are used both to extract words that belong to the same scale and to determine the direction of the ordering (e.g., in the above pattern, x is weaker than y). However, this extraction process gives noisy results that require filtering using an electronic thesaurus. The domain of application is thus restricted to words that exist in an electronic thesaurus. Second, previous work is limited to the study of adjectives. In this paper, we propose a fully automated pipeline that uses structural patterns to extract gradable terms from a corpus, cluster the"
N15-1051,P14-2131,0,0.0153494,"t work shows promise for context vectors embedded in a compressed semantic space that are derived using neural networks: Baroni et al. (2014) compare standard context vectors with embedded vectors for a wide range of lexical semantic tasks and found embedded vectors to yield better results. We therefore generate context vectors and compare the utility of both skip-gram and continuous bag of words (CBOW) representations using the word2vec tool (Mikolov et al., 2013) for our task. These two representations have demonstrated varying degrees of success in different NLP tasks (Baroni et al., 2014; Bansal et al., 2014). Given a 2 The choice of a hard-clustering algorithm was mostly for implementational convenience, but carries with it the issue that polysemous words can only appear in one semantic cluster. We leave the issue of deriving a soft clustering approach that works with context vectors, a separate research problem in its own right, to future work. window size w, the CBOW model predicts the current word given the neighboring words as context. In contrast, the skip-gram model predicts the neighboring words given the current word. We used w = 5 and found CBOW to yield better results for our task. Thus"
N15-1051,P14-1023,0,0.225147,". As the clustering algorithm, we use the Matlab (2014) implementation of K-means++ (Arthur and Vassilvitskii, 2007), a hard clustering algorithm2 with cosine similarity as a distance metric. Following Hatzivassiloglou and McKeown (1993), we use context vectors to represent the words to cluster. They make use of standard context vectors for clustering adjectives, where context for every adjective comprises of nouns it modifies across all sentences in a corpus. However, recent work shows promise for context vectors embedded in a compressed semantic space that are derived using neural networks: Baroni et al. (2014) compare standard context vectors with embedded vectors for a wide range of lexical semantic tasks and found embedded vectors to yield better results. We therefore generate context vectors and compare the utility of both skip-gram and continuous bag of words (CBOW) representations using the word2vec tool (Mikolov et al., 2013) for our task. These two representations have demonstrated varying degrees of success in different NLP tasks (Baroni et al., 2014; Bansal et al., 2014). Given a 2 The choice of a hard-clustering algorithm was mostly for implementational convenience, but carries with it th"
N15-1051,P10-1018,1,0.761133,"Missing"
N15-1051,Q13-1023,0,0.252435,"Missing"
N15-1051,D13-1169,1,0.771925,"Missing"
N15-1051,P03-1054,0,0.00437118,"t)$ (ADJP<JJ)).’ Similarly, a structural pattern for adverbs can be written as ‘ADVP< ((ADVP<RB) $ (CC<but)$(RB<not)$ (ADVP<RB)).’ These patterns are available for download1 . 1 http://web.cse.ohio-state.edu/˜shivade/naacl2015 485 Introducing tree patterns requires parsing a corpus: while this additional step in the pipeline might lead to error propagation, the advantages of the structural patterns are that (i) they are more robust than the lexical ones and (ii) restricting results to a desired part-of-speech comes for free. In the experiments reported here, we use the Stanford parser v3.3.1 (Klein and Manning, 2003). 3.2 Automatic clustering In order to determine a ranking of words based on their semantic intensity, the first step is to determine words that belong to the same scale of meaning. As pointed out earlier, previous work (de Melo and Bansal, 2013; Sheinman et al., 2013) use WordNet dumbbells, and this restricts the utility of these approaches to the scope of a manually created lexical resource. We overcome this limitation by automatically clustering words that belong to the same scale. As the clustering algorithm, we use the Matlab (2014) implementation of K-means++ (Arthur and Vassilvitskii, 2"
N15-1051,levy-andrew-2006-tregex,0,0.0128554,"proaches suffer from a coverage issue. This is because these patterns consist of longer n-grams, which are sparsely found in a small dataset. Therefore, Sheinman et al. (2013) use the Web as their corpus, and de Melo & Bansal use Google N-grams (Brants and Franz, 2006). However, this results in a large number of instances where satisfied lexical patterns do not correspond to adjectives (e.g., sometimes but not always). Moreover, since the Google N-grams corpus is limited to 5-grams, adjective pairs of interest beyond a five-word window are lost. To deal with these shortcomings, we use Tregex (Levy and Andrew, 2006), which enables pattern matching on parse trees based on syntactic relationships and regular expression matches on nodes. Using Tregex, we transform de Melo and Bansal’s weak-strong and strong-weak lexical patterns into structural patterns. For example, one way of expanding the lexical pattern ‘∗ but not ∗’ into a structural Tregex pattern for adjectives is ‘ADJP< ((ADJP<JJ) $ (CC<but)$(RB<not)$ (ADJP<JJ)).’ Similarly, a structural pattern for adverbs can be written as ‘ADVP< ((ADVP<RB) $ (CC<but)$(RB<not)$ (ADVP<RB)).’ These patterns are available for download1 . 1 http://web.cse.ohio-state.e"
N15-1051,W14-1618,0,0.0123674,"ty can be successful established between regular terms, doing so for domain-specific terms requires knowledge of context. We plan to expand the structure patterns derived from the lexical patterns of de Melo and Bansal (2013), looking for new patterns that could be more suited for adverbs. We also plan to investigate soft clustering algorithms such as (Pereira et al., 1993) that may allow us to model polysemous words better. Furthermore, recent studies have compared traditional vectors against embedded vectors (such as the CBOW vectors used in this study) for different lexical semantic tasks (Levy and Goldberg, 2014; Baroni et al., 2014), which suggests that such a comparison for our clustering task could be insightful. Our experimental results show that automatic clustering of gradable words produces promising results. However, we also observe that with domainspecific words, context is important for establishing a ranking between words that is based on semantic intensity. Thus, rather than clustering adjectives or adverbs in isolation, a joint with the clustering of nouns or verbs with which they occur is a possible direction of research. Finally, studies deriving a ranking based on semantic intensities"
N15-1051,P93-1024,0,0.559226,"aces lexical patterns with structural patterns, and show that the approach has utility for not only discovering adjective patterns but also adverb patterns in biomedical text. We observe that while automatic ranking based 491 on semantic intensity can be successful established between regular terms, doing so for domain-specific terms requires knowledge of context. We plan to expand the structure patterns derived from the lexical patterns of de Melo and Bansal (2013), looking for new patterns that could be more suited for adverbs. We also plan to investigate soft clustering algorithms such as (Pereira et al., 1993) that may allow us to model polysemous words better. Furthermore, recent studies have compared traditional vectors against embedded vectors (such as the CBOW vectors used in this study) for different lexical semantic tasks (Levy and Goldberg, 2014; Baroni et al., 2014), which suggests that such a comparison for our clustering task could be insightful. Our experimental results show that automatic clustering of gradable words produces promising results. However, we also observe that with domainspecific words, context is important for establishing a ranking between words that is based on semantic"
N15-1051,E14-4023,0,0.208288,"ity: although words such as small and minuscule illustrate varying degrees of size, they are listed as synonyms in WordNet. Introduction Gradability (Sapir, 1944) is a property of words that identifies different degrees of the quality the word denotes. For example, adjectives such as large, huge and gigantic present different degrees of size or volume. Similarly, adverbs such as approximately, almost and roughly present different degrees of how Recently, there has been a lot of interest in exploring different approaches to derive an ordering among gradable adjectives based on their semantics (Ruppenhofer et al., 2014; Sheinman et al., 2013; Schulam and Fellbaum, 2010). de Melo and Bansal (2013) propose a novel Mixed Integer Linear Programming (MILP) based approach, publish a gold standard dataset and report the best performance on ordering scalar adjectives on this dataset. However, these approaches are limited in two ways. First, they depend on a manually created resource, such 483 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 483–493, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics as WordNet or FrameNet"
N15-1051,D08-1027,0,0.0330722,"Missing"
N15-1051,C98-1013,0,\N,Missing
N15-1051,P93-1023,0,\N,Missing
P03-1071,A00-2004,0,0.864903,"the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001)"
P03-1071,J86-3001,0,0.614029,"mber of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25 meetings to be segmented, each by at least three subjects. We opted for a linear representation of discourse, since finer-grained discourse structures (e.g. (Grosz and Sidner, 1986)) are generally considered to be difficult to mark reliably. Subjects were asked to mark each speaker change (potential boundary) as either boundary or non-boundary. In the resulting annotation, the agreed segmentation based on majority 1 While it would be desirable to have a broader variety of meetings, we hope that experiments on this corpus will still carry some generality. opinion contained 7.5 segments per meeting on average, while the average number of potential boundaries is 770. We used Cochran’s Q (1950) to evaluate the agreement among annotators. Cochran’s test evaluates the null hyp"
P03-1071,P98-2145,0,0.0135692,"are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corpus (Janin et al., 2003). This corpus is one of a growing number of corpora with human-to-human multi-party conversations. In this corpus, recordings of meetings ranged primarily over three different recurring meeting types, all of which concerned speech or language research.1 The average duration is 60 minutes, with an average of 6.5 participants. They were transcribed, and each conversation turn was marked with the speaker, start time, end time, and word content. From the corpus, we selected 25"
P03-1071,P94-1002,0,0.983157,"ing approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg a"
P03-1071,P96-1038,0,0.663903,"91; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997; T¨ur et al., 2001) exponential models (Beeferman et al., 1999) or other probabilistic models (Hajime et al., 1998; Reynar, 1999). 3 The ICSI Meeting Corpus We have evaluated our segmenter on the ICSI Meeting corp"
P03-1071,W98-1123,1,0.766858,"ule. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses,"
P03-1071,P93-1041,0,0.823497,"r features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Na"
P03-1071,P95-1015,0,0.0924914,"e counted the number of its occurrences near any topic boundary, and its number of appearances overall. Then, we performed χ2 significance tests (e.g. figure 2 for okay) under the null hypothesis that no correlation exists. We selected terms whose χ2 value rejected the hypothesis under a 0.01-level confidence (the rejection criterion is χ2 ≥ 6.635). Finally, induced cue phrases whose usage has never been described in other work were removed (marked with ∗ in Table 3). Indeed, there is a risk that the automatically derived list of cue phrases could be too specific to the word usage in 9 As in (Litman and Passonneau, 1995), we restrict ourselves to the first lexical item of any utterance, plus the second one if the first item is also a cue word. okay Other Near boundary 64 657 Distant 740 25896 Table 2: okay (χ2 = 89.11, df = 1, p &lt; 0.01). okay shall ∗ anyway we’re ∗ alright let’s ∗ 93.05 27.34 23.95 17.67 16.09 14.54 but so and should ∗ good ∗ 13.57 11.65 10.99 10.21 7.70 to the discussion can greatly change from one discourse unit to the next. We try to capture significant changes in speakership by measuring the dissimilarity between two analysis windows. For each potential boundary, we count for each speaker"
P03-1071,J91-1002,0,0.963738,"speaker change, and other features like cue phrases. We present a machine learning approach for integrating these conversational features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hi"
P03-1071,P93-1020,0,0.0314282,"wo analysis windows. For each potential boundary, we count for each speaker i the number of words that are uttered before (Li ) and after (Ri ) the potential boundary (we limit our analysis to a window of fixed size). The two distributions are normalized to form two probability distributions l and r, and significant changes of speakership are detected by computing their Jensen-Shannon divergence: JS(l, r) = 12 [D(l||avgl,r ) + D(r||avgl,r )] Table 3: Automatically selected cue phrases. these meetings. Silences: previous work has found that major shifts in topic typically show longer silences (Passonneau and Litman, 1993; Hirschberg and Nakatani, 1996). We investigated the presence of silences in meetings and their correlation with topic boundaries, and found it necessary to make a distinction between pauses and gaps (Levinson, 1983). A pause is a silence that is attributable to a given party, for example in the middle of an adjacency pair, or when a speaker pauses in the middle of her speech. Gaps are silences not attributable to any party, and last until a speaker takes the initiative of continuing the discussion. As an approximation of this distinction, we classified a silence that follows a question or in"
P03-1071,J97-1005,0,0.950475,"roadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range in F0 and intensity, and higher maximum accent peak. These approaches use different learning mechanisms to combine features, including decision trees (Grosz and Hirschberg, 1992; Passonneau and Litman, 1997;"
P03-1071,J02-1002,0,0.625567,"ng the average (µ) and standard deviation (σ) of the p(mi ) values, and each potential boundary mi above the threshold µ−α·σ is hypothesized as a real boundary. 4.2 Evaluation We evaluate LCseg against two state-of-the-art segmentation algorithms based on lexical cohesion (Choi, 2000; Utiyama and Isahara, 2001). We use the error metric Pk proposed by Beeferman et al. (1999) to evaluate segmentation accuracy. It computes the probability that sentences k units (e.g. sentences) apart are incorrectly determined as being either in different segments or in the same one. Since it has been argued in (Pevzner and Hearst, 2002) that Pk has some weaknesses, we also include results according to the WindowDiff (WD) metric (which is described in the same work). A test corpus of concatenated6 texts extracted from the Brown corpus was built by Choi (2000) to evaluate several domain-independent segmentation algorithms. We reuse the same test corpus for our evaluation, in addition to two other test corpora we constructed to test how segmenters scale across genres and how they perform with texts with various 6 Concatenated documents correspond to reference segments. number of segments.7 We designed two test corpora, each of"
P03-1071,P94-1050,0,0.200681,"features with the text-based segmentation module. Experimental results show a marked improvement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur"
P03-1071,P99-1046,0,0.0706811,"provement in meeting segmentation with the incorporation of both sets of features. We close with discussions and conclusions. 2 Related Work Existing approaches to textual segmentation can be broadly divided into two categories. On the one hand, many algorithms exploit the fact that topic segments tend to be lexically cohesive. Embodiments of this idea include semantic similarity (Morris and Hirst, 1991; Kozima, 1993), cosine similarity in word vector space (Hearst, 1994), inter-sentence similarity matrix (Reynar, 1994; Choi, 2000), entity repetition (Kan et al., 1998), word frequency models (Reynar, 1999), or adaptive language models (Beeferman et al., 1999). Other algorithms exploit a variety of linguistic features that may mark topic boundaries, such as referential noun phrases (Passonneau and Litman, 1997). In work on segmentation of spoken documents, intonational, prosodic, and acoustic indicators are used to detect topic boundaries (Grosz and Hirschberg, 1992; Nakatani et al., 1995; Hirschberg and Nakatani, 1996; Passonneau and Litman, 1997; Hirschberg and Nakatani, 1998; Beeferman et al., 1999; T¨ur et al., 2001). Such indicators include long pauses, shifts in speaking rate, great range"
P03-1071,J01-1002,0,0.457388,"Missing"
P03-1071,P01-1064,0,0.942001,"es lexical chains, which are thought to mirror the discourse structure of the underlying text (Morris and Hirst, 1991). We ignore synonymy and other semantic relations, building a restricted model of lexical chains consisting of simple term repetitions, hypothesizing that major topic shifts are likely to occur where strong term repetitions start and end. While other relations between lexical items also work as cohesive factors (e.g. between a term and its super-ordinate), the work on linear topic segmentation reporting the most promising results account for term repetitions alone (Choi, 2000; Utiyama and Isahara, 2001). The preprocessing steps of LCseg are common to many segmentation algorithms. The input document is first tokenized, non-content words are removed, 2 Four other meetings failed short the significance test, while there was little agreement on the two last ones (p > 0.1). and remaining words are stemmed using an extension of Porter’s stemming algorithm (Xu and Croft, 1998) that conflates stems using corpus statistics. Stemming will allow our algorithm to more accurately relate terms that are semantically close. The core algorithm of LCseg has two main parts: a method to identify and weight stro"
P03-1071,J93-3003,0,\N,Missing
P03-1071,C98-2140,0,\N,Missing
P12-2014,P07-2044,0,0.176103,"finishes e1 e1.start e2.start e1.stop; e2.stop Figure 1: Excerpt from a sanitized clinical narrative (history & physical report) with medical events underlined. Table 1: Allen’s temporal relations between medical events can be realized by ordering the starts and stops 2 Related Work The Timebank corpus provides hand-tagged features, including tense, aspect, modality, polarity and event class. There have been significant efforts in machine learning of temporal relations between events using these features and a wide range of other features extracted from the Timebank corpus (Mani et al., 2006; Chambers et al., 2007; Lapata and Lascarides, 2011). The SemEval/TempEval (Verhagen et al., 2009) challenges have often focused on temporal relation learning between different types of events from Timebank. Zhou and Hripcsak (2007) provide a comprehensive survey of temporal reasoning with clinical data. There has also been some work in generating annotated corpora of clinical text for temporal relation learning (Roberts et al., 2008; Savova et al., 2009). However, none of these corpora are freely available. Zhou et al. (2006) propose a Temporal Constraint Structure (TCS) for medical events in discharge summaries."
P12-2014,P06-1095,0,0.421485,"mporal relation learning: clinical narratives may have different language attributes corresponding to temporal ordering relative to Timebank, implying that the field may need to look at a wider range of domains to fully understand the nature of temporal ordering. 1 Introduction There has been considerable research on learning temporal relations between events in natural language. Most learning problems try to classify event pairs as related by one of Allen’s temporal relations (Allen, 1981) i.e., before, simultaneous, includes/during, overlaps, begins/starts, ends/finishes and their inverses (Mani et al., 2006). The Timebank corpus, widely used for temporal relation learning, consists of newswire text annotated for events, temporal expressions, and temporal relations between events using TimeML (Pustejovsky et al., 2003). In Timebank, the notion of an “event” primarily consists of verbs or phrases that denote change in state. However, there may be a need to rethink how we learn temporal relations between events in different domains. Timebank, its features, and established learning techniques like classification, may not work optimally in many real-world problems where temporal relation learning is o"
P14-1094,J05-3002,0,0.0367172,"ork in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text. Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text. There is limited prior work in learning relations across documents. Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion. This involves multisequence dependency tree alignment to identify phrases conveying sim3 Problem Description Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient. We represent medical events by splitting each event into a start and a stop. When there is insufficient information to discern the start or stop of an event, it is represented as a single concept. If"
P14-1094,W06-1623,0,0.0874456,"Missing"
P14-1094,P09-1068,0,0.0394062,"ple sequence alignment. Related Work In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010). In this paper, we focus on temporal ordering of information, as discussed next. Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011). Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal classification to induce partial order. The task was carried out on the Timebank newswire corpus, but was limited to an intra-document setting. More recently, (Do et al., 2012) proposed an ILP-based method to combine the outputs of an event-interval and an event-event classifier for timeline construction on the ACE 2005 corpus. However, this approach is also restricted"
P14-1094,D12-1062,0,0.287775,"e problem as a sequence alignment task and propose solving this using two approaches. First, we use weighted finite state machines to represent medical events sequences, thus enabling composition and search to obtain the most probable combined sequence of medical events. As a contrast, we adapt dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) used to produce global and local alignments for aligning sequences of medical events across narratives. We also compare the proposed methods with an Integer Linear Programming (ILP) based method for timeline construction (Do et al., 2012). The cross-narrative coreference and temporal relation scores used in both these approaches are learned from a corpus of patient narratives from The Ohio State University Wexner Medical Center. Introduction Discourse structure, logical flow of sentences, and context play a large part in ordering medical events based on temporal relations within a clinical narrative. However, cross-narrative temporal relation ordering is a challenging task as it is difficult to learn temporal relations among medical events which are not part of the logically coherent discourse of a single narrative. Resolving"
P14-1094,P08-1030,0,0.0295998,"hods for timeline creation from longitudinal clinical narratives to such an ILP-based approach in Section 7. While a lot of this work has been done in the news domain, there is also some recent work in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text. Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text. There is limited prior work in learning relations across documents. Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion. This involves multisequence dependency tree alignment to identify phrases conveying sim3 Problem Description Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient."
P14-1094,lacatusu-etal-2004-multi,0,0.0355342,"edge from resources like WordNet to find similar sentences. In case of clinical narratives and medical event alignment, the objective is to identify a unique sequence of temporally ordered medical events from across longitudinal clinical data. To the best of our knowledge, there is no prior work on cross-document alignment of event sequences. Multiple sequence alignment is a problem that arises in a variety of domains including gene/protein alignments in bioinformatics (Notredame, 2002), word alignments in machine translation (Kumar and Byrne, 2003), and sentence alignments for summarization (Lacatusu et al., 2004). Dynamic programming algorithms have been popularly leveraged to produce pairwise and global genetic alignments, where edit distance based metrics are used to compute the cost of insertions, deletions and substitutions. We use dynamic programming to compute the best alignment, given the temporal and coreference information between medical events across these sequences. More importantly, we propose a cascaded WFST-based framework for crossdocument temporal ordering of medical event sequences. Composition and search operations can be used to build a single transducer that integrates these compo"
P14-1094,P03-1069,0,0.0435659,"FSTs have seen varied applications in machine translation (Kumar and Byrne, 2003), morphology (Sproat, 2006), named entity recognition (Krstev et al., 2011) and biological sequence alignment / generation (Whelan et al., 2010) among others. We demonstrate that the WFST-based approach outperforms popularly used dynamic programming algorithms for multiple sequence alignment. Related Work In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010). In this paper, we focus on temporal ordering of information, as discussed next. Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011). Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal clas"
P14-1094,P06-1095,0,0.0392668,"oach outperforms popularly used dynamic programming algorithms for multiple sequence alignment. Related Work In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010). In this paper, we focus on temporal ordering of information, as discussed next. Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011). Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal classification to induce partial order. The task was carried out on the Timebank newswire corpus, but was limited to an intra-document setting. More recently, (Do et al., 2012) proposed an ILP-based method to combine the outputs of an event-interval and an event-event classifier for time"
P14-1094,P98-2147,0,0.0129689,"farction/0.1 c+t is a WFST representation used for mapping medical events between N1 and N2 (from Figure 7: M12 Figure 2) and is weighted by both the coreference and temporal relation probabilities all narrative chains belonging to the same patient, the composition cascade to build the final combined sequence will be as, i i i Df = N1 ◦M12 ◦N2 ◦M23 ◦N3 ◦M34 ...◦Nn (2) where i = c or i = c + t and n is the number of medical event sequences corresponding to clinical narratives for a patient. During composition i utilizing the we retain intermediate paths like M23 ability to do lazy composition (Mohri and Pereira, 1998) in order to facilitate beam search through the multi-alignment. The best hypothesis corresponds to the highest scoring path which can be obtained using shortest path algorithms like Djikstra’s algorithm. The best path corresponds to the best alignment across all medical event sequences based on the joint probability of cross-narrative medical event coreferences and temporal relations across the narrative sequences. The complexity of decoding increases exponentially with the number of narrative sequences in the composition, and exact decoding becomes infeasible. One solution to this problem is"
P14-1094,W11-4407,0,0.0205344,"Missing"
P14-1094,N03-1019,0,0.0484075,"into a sentence. Along with syntactic features, they combine knowledge from resources like WordNet to find similar sentences. In case of clinical narratives and medical event alignment, the objective is to identify a unique sequence of temporally ordered medical events from across longitudinal clinical data. To the best of our knowledge, there is no prior work on cross-document alignment of event sequences. Multiple sequence alignment is a problem that arises in a variety of domains including gene/protein alignments in bioinformatics (Notredame, 2002), word alignments in machine translation (Kumar and Byrne, 2003), and sentence alignments for summarization (Lacatusu et al., 2004). Dynamic programming algorithms have been popularly leveraged to produce pairwise and global genetic alignments, where edit distance based metrics are used to compute the cost of insertions, deletions and substitutions. We use dynamic programming to compute the best alignment, given the temporal and coreference information between medical events across these sequences. More importantly, we propose a cascaded WFST-based framework for crossdocument temporal ordering of medical event sequences. Composition and search operations c"
P14-1094,P12-2014,1,0.889953,"Missing"
P14-1094,P95-1026,0,0.138015,"o such an ILP-based approach in Section 7. While a lot of this work has been done in the news domain, there is also some recent work in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text. Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text. There is limited prior work in learning relations across documents. Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents. Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion. This involves multisequence dependency tree alignment to identify phrases conveying sim3 Problem Description Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient. We represent medical events by splitting each event into a"
P14-1094,N12-1091,1,\N,Missing
P14-1094,C98-2142,0,\N,Missing
stoia-etal-2008-scare,byron-fosler-lussier-2006-osu,1,\N,Missing
stoia-etal-2008-scare,P02-1048,0,\N,Missing
W05-1529,A00-2018,0,\N,Missing
W05-1529,J04-4004,0,\N,Missing
W05-1529,C00-2100,0,\N,Missing
W05-1529,C94-1042,0,\N,Missing
W05-1529,A97-1052,0,\N,Missing
W05-1529,J93-2002,0,\N,Missing
W05-1529,P98-2184,0,\N,Missing
W05-1529,C98-2179,0,\N,Missing
W06-1412,P02-1048,0,0.0174474,"Missing"
W06-1412,W00-1411,0,0.0640637,"Missing"
W06-1412,C92-1038,0,0.0344963,"l Linguistics spatial position, and the presence of similar items from which the target referent must be distinguished, have all been found to cause changes to the lexical properties chosen for a particular referring expression (i.e. (Gundel et al., 1993; Prince, 1981; Grosz et al., 1995)). This variation is expressed in terms of the determiner chosen (e.g. that/a), the head noun (e.g. that/door/one), and the presence of additional modifiers such as prenominal adjectives or prepositional phrases. In natural language generation, the process of generating referring expressions occurs in stages (Reiter and Dale, 1992). The process we explore in this paper is the sentence planning stage, which determines whether the context supports generating a particular referring expression as a pronoun, description, one-anaphor, etc. There has been extensive research in both automatic route description and on general noun phrase (NP) generation, but few projects consider extra-linguistic information as part of the context that influences dialog behavior. (Poesio et al., 1999) applies statistical techniques for the problem of NP generation. However, even though the corpus used in that study contains descriptions of museu"
W06-1412,P98-2241,1,0.86179,"Missing"
W06-1412,N01-1002,0,0.1806,"rch in both automatic route description and on general noun phrase (NP) generation, but few projects consider extra-linguistic information as part of the context that influences dialog behavior. (Poesio et al., 1999) applies statistical techniques for the problem of NP generation. However, even though the corpus used in that study contains descriptions of museum items visually accessible to the user, the features used in generation were mostly linguistic, and included little information about the visual or spatial properties of the referent. Another related study in statistical NP generation (Cheng et al., 2001) focuses on choosing the modifiers to be included. Again, no features derived from the situated world were used in that study. (Maass et al., 1995) use features from the world, including objects’ color, height, width, and visibility, as well as the user’s direction of travel and distance from objects, for generating instructions in a situated task. However, their focus is on selecting landmarks and descriptions under time pressure, rather than selecting the linguistic form to be produced. DG: DF: DG: DF: DG: DF: DG: DF: DG: you can currently see three buttons... there’s actually a fourth butto"
W06-1412,W05-1627,0,\N,Missing
W06-1412,E06-1045,0,\N,Missing
W06-1412,E06-1040,0,\N,Missing
W06-1412,W05-1608,0,\N,Missing
W06-1412,W06-1410,0,\N,Missing
W06-1412,W07-2305,0,\N,Missing
W06-1412,T78-1009,0,\N,Missing
W06-1412,J07-2004,0,\N,Missing
W06-1412,J03-1003,0,\N,Missing
W06-1412,C98-2236,0,\N,Missing
W06-1412,J02-1003,0,\N,Missing
W06-1412,P88-1014,0,\N,Missing
W06-1412,J95-2003,0,\N,Missing
W06-1412,P07-1043,0,\N,Missing
W06-1412,P06-1131,0,\N,Missing
W06-1412,P06-1140,0,\N,Missing
W06-1412,P02-1012,0,\N,Missing
W06-1412,P89-1009,0,\N,Missing
W06-1412,J86-3001,0,\N,Missing
W06-1412,P04-1019,0,\N,Missing
W06-1412,byron-fosler-lussier-2006-osu,1,\N,Missing
W06-1412,W05-1609,0,\N,Missing
W06-1412,J78-3015,0,\N,Missing
W06-1412,N06-2040,1,\N,Missing
W09-3304,J06-1003,0,0.0316139,"n had to be built using co-occurrence statistics of collected corpora (Higgins, 2004) or expensive, expert-created resources such as WordNet or Roget’s Thesaurus (Jarmasz and Szpakowicz, 2003). Here, we evaluate the effectiveness of Wiktionary, a collaboratively constructed resource, as a source of semantic relatedness information for the synonym detection problem. Researching these metrics is important because they have been empirically shown to improve performance in a variety of NLP applications, including word sense disambiguation (Turdakov and Velikhov, 2008), real-world spelling errors (Budanitsky and Hirst, 2006) and coreference resolution (Strube and Ponzetto, 2006). Synonym detection is a recognized testbed for comparing semantic relatedness metrics (e.g (Zesch et al., 2008)). In this task, a target word or phrase is presented to the system, which is then presented with four alternative words or phrases. The goal of the system is to pick the alternative most related to the target. Example questions can be found in Figure 1. Both Wikipedia and Wiktionary are organized around a basic “page” unit, containing information about an individual word, phrase or entity in the world – definitions, thesaurus en"
W12-2208,W10-1001,0,0.0126169,"s more robust for web documents and passages. Heilman et al. (2007) studied the impact of grammar-based features combined with language modeling approach for readability assessment of first and second language texts. They argued that grammar-based features are more pertinent for second language learners than for the first language readers. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) both used a support vector machine to classify texts based on the reading level. They combined traditional methods 59 of readability assessment and the features from language models and parsers. Aluisio et al. (2010) have developed a tool for text simplification for the authoring process which addresses lexical and syntactic phenomena to make text readable but their assessment takes place at more coarse levels of literacy instead of finer-grained levels used for children’s books. A detailed analysis of various features for automatic readability assessment has been done by Feng et al. (2010). Most of the previous work has used web page documents, short passages or articles from educational newspapers as their datasets; typically the task is to assess reading level at a whole-grade level. In contrast, early"
W12-2208,N04-1025,0,0.257393,"ons in Section 3. The extended features will be covered in Section 4, followed by experimental analysis in Section 5, in which we will compare the results between human annotations and automatic annotations. We will also report the system performance after incorporating the rich text features (structural features). Conclusions follow in Section 6. 2 Related Work Since 1920, approximately 200 readability formulas have been reported in the literature (DuBay, 2004); statistical language processing techniques have recently entered into the fray for readability assessment. Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages. Heilman et al. (2007) studied the impact of grammar-based features combined with language modeling approach for readability assessment of first and second language texts. They argued that grammar-based features are more pertinent for second language learners than for the first language readers. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) both used a support vector machine to classify texts based on the reading level. They combined traditional methods 59 of readability assessment and"
W12-2208,C10-2032,0,0.162887,"xts. The history of assessing readability using simple arithmetic metrics dates back to the 1920s when Thorndike (1921) has measured difficulty of texts by tabulating words according to the frequency of their use in general literature. Most of the traditional readability formulas were also based on countable features of text, such as syllable counts (Flesch, 1948). More advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction (CollinsThompson and Callan, 2004; Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; Feng et al., 2010); such works are described in further detail in the next Section 2. In recent work (Ma et al., 2012), we approached the problem of fine-grained leveling of books, demonstrating that a ranking approach to predicting reading level outperforms both classification and regression approaches in that domain. A further finding was that visually-oriented features that consider the visual layout of the page (e.g. number of text lines per annotated text region, text region area compared to the whole page area and font size etc.) play an important role in predicting the reading levels of children’s books"
W12-2208,N07-1058,0,0.177656,"we will compare the results between human annotations and automatic annotations. We will also report the system performance after incorporating the rich text features (structural features). Conclusions follow in Section 6. 2 Related Work Since 1920, approximately 200 readability formulas have been reported in the literature (DuBay, 2004); statistical language processing techniques have recently entered into the fray for readability assessment. Si and Callan (2001) and Collins-Thompson and Callan (2004) have demonstrated the use of language models is more robust for web documents and passages. Heilman et al. (2007) studied the impact of grammar-based features combined with language modeling approach for readability assessment of first and second language texts. They argued that grammar-based features are more pertinent for second language learners than for the first language readers. Schwarm and Ostendorf (2005) and Petersen and Ostendorf (2009) both used a support vector machine to classify texts based on the reading level. They combined traditional methods 59 of readability assessment and the features from language models and parsers. Aluisio et al. (2010) have developed a tool for text simplification"
W12-2208,P03-1054,0,0.00618055,"d versions of PDFs. Once the XMLs for each children’s book are generated, we could proceed to the feature extraction step. The set of features we use in the experiments are described in the following Section 4. Surface-level Features and Visually-oriented Features 4.2 Structural Features Since our previous work only uses surface level of text features, we are interested in investigating the contribution of high-level structural features to the current system. Feng et al. (2010) found several parsing-based features and part-of-speech based features to be useful. We utilize the Stanford Parser (Klein and Manning, 2003) to extract the following features from the XML files based on those used in (Feng et al., 2010): • Parsed Syntactic Features for NPs and VPs 4 Features For surface-level features and visual features, we utilize similar features proposed in our previous study.3 For completeness’ sake, we list these two sets of features as follows in Section 4.1: 3 We discard two visual features in both the human and automatic annotation that require the annotation of the location of images on the page, as these were features that the Adobe Acrobat JavaScript API could not directly access. 61 1. Number of the N"
W12-2208,N12-1063,1,0.75378,"nt Robert Lofthus Yi Ma, Ritu Singh, Eric Fosler-Lussier Xerox Corporation Dept. of Computer Science & Engineering Rochester, NY 14604, USA The Ohio State University Robert.Lofthus@xerox.com Columbus, OH 43210, USA may,singhri,fosler@cse.ohio-state.edu Abstract Early primary children’s literature poses some interesting challenges for automated readability assessment: for example, teachers often use fine-grained reading leveling systems for determining appropriate books for children to read (many current systems approach readability assessment at a coarser whole grade level). In previous work (Ma et al., 2012), we suggested that the fine-grained assessment task can be approached using a ranking methodology, and incorporating features that correspond to the visual layout of the page improves performance. However, the previous methodology for using “found” text (e.g., scanning in a book from the library) requires human annotation of the text regions and correction of the OCR text. In this work, we ask whether the annotation process can be automated, and also experiment with richer syntactic features found in the literature that can be automatically derived from either the humancorrected or raw OCR te"
W12-2208,P05-1065,0,0.472909,"—reading level assessment is required in a variety of contexts. The history of assessing readability using simple arithmetic metrics dates back to the 1920s when Thorndike (1921) has measured difficulty of texts by tabulating words according to the frequency of their use in general literature. Most of the traditional readability formulas were also based on countable features of text, such as syllable counts (Flesch, 1948). More advanced machine learning techniques such as classification and regression have been applied to the task of reading level prediction (CollinsThompson and Callan, 2004; Schwarm and Ostendorf, 2005; Petersen and Ostendorf, 2009; Feng et al., 2010); such works are described in further detail in the next Section 2. In recent work (Ma et al., 2012), we approached the problem of fine-grained leveling of books, demonstrating that a ranking approach to predicting reading level outperforms both classification and regression approaches in that domain. A further finding was that visually-oriented features that consider the visual layout of the page (e.g. number of text lines per annotated text region, text region area compared to the whole page area and font size etc.) play an important role in"
W12-2404,J96-1002,0,0.0289869,"Missing"
W12-2404,W11-0219,0,0.0654509,"Missing"
W15-0611,Q13-1005,0,0.0143705,"l patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical students were asked to complete a focused history of pre"
W15-0611,P05-1074,0,0.115102,"Missing"
W15-0611,P14-1133,0,0.0129923,"The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical students were asked to complete a focused history of present illness of a patient"
W15-0611,P09-1053,0,0.0458946,"Missing"
W15-0611,D08-1069,0,0.0752837,"Missing"
W15-0611,W11-2107,0,0.0343506,"Missing"
W15-0611,C04-1051,0,0.0143944,"Missing"
W15-0611,E12-1048,0,0.0229144,"nce, allowing them to “suspend disbelief” and behave as if the virtual patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of t"
W15-0611,W13-1738,0,0.0236171,"uspend disbelief” and behave as if the virtual patient is a real patient. The virtual world platform can be run in a variety of environments; here we focus on text-based interaction for laptops and mobile devices. The current task is a question matching paradigm where user input is mapped to a set of predefined questions, which have scripted answers created by content authors, as in much previous work on question answering systems (Leuski and Traum, 2011). This approach allows for easier authoring than, for example, systems that use deep natural language understanding (Dzikovska et al., 2012; Dzikovska et al., 2013) or semantic parsing (Artzi and Zettlemoyer, 2013; Berant and Liang, 2014), and yet still achieves the desired learning objectives of the virtual patient system. To date, the VSP system has been based on the 86 Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educational Applications, 2015, pages 86–96, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ChatScript1 pattern matching engine, which offers a low cost and straightforward approach for initial dialogue system development. In an evaluation where a group of third-year medical student"
W15-0611,D13-1090,0,0.0291762,"Missing"
W15-0611,N12-1019,0,0.0727964,"Missing"
W15-0611,P02-1040,0,0.0915643,"Missing"
W15-0611,W03-1209,0,0.289668,"Missing"
W15-0611,U06-1019,0,0.0515201,"Missing"
W15-0611,Q14-1034,0,0.0577806,"Missing"
W15-1305,D13-1099,0,0.0210921,"lts without the need for annotated data. This is an important result especially in the clinical domain where available annotated data is sparse and extremely costly to generate. 6 Dependency Tree Kernels Dependency tree kernels have been showed to be effective for NLP tasks in the past. Culotta et al. (2004) showed that although tree kernels by themselves may not be effective for relation extraction, combining a tree kernel with a bag of words kernel showed promising results. Dependency tree kernels have also been explored in the context of negation extraction in the medical domain. Recently, Bowei et al. (2013) demonstrated the use of tree kernel based approaches in detecting the scope of negations and speculative sentences using the BioScope corpus (Szarvas et al., 2008). However, the task of negation scope detection task is different than that of negation detection. Among other differences, an important one being the presence of annotations for negation cues in the Bioscope corpus. Sohn et al. (2012) developed hand crafted rules representing subtrees of dependency parses of negated sentences and showed that they were effective on a dataset from their institution. 44 Therefore, we implemented a dep"
W15-1305,P04-1054,0,0.770769,"detecting the scope of negations and speculative sentences using the BioScope corpus (Szarvas et al., 2008). However, the task of negation scope detection task is different than that of negation detection. Among other differences, an important one being the presence of annotations for negation cues in the Bioscope corpus. Sohn et al. (2012) developed hand crafted rules representing subtrees of dependency parses of negated sentences and showed that they were effective on a dataset from their institution. 44 Therefore, we implemented a dependency tree kernel similar to the approach described in Culotta and Sorensen (2004) to automatically capture the structural patterns in negated assertions. We used the Stanford dependencies parser (version 2.0.4) (de Marneffe et al., 2006) to get the dependency parse for every assertion. As per their representation (de Marneffe and Manning, 2008) every dependency is a triple, consisting of a governor, a dependent and a dependency relation. In this triple, the governor and dependent are words from the input sentence. Thus, the tree kernel comprised of nodes corresponding to every word and every dependency relation in the parse. Node similarity was computed based on features s"
W15-1305,W08-1301,1,0.7412,"Missing"
W15-1305,de-marneffe-etal-2006-generating,1,0.112784,"Missing"
W15-1305,W08-0606,0,0.0340525,"ly to generate. 6 Dependency Tree Kernels Dependency tree kernels have been showed to be effective for NLP tasks in the past. Culotta et al. (2004) showed that although tree kernels by themselves may not be effective for relation extraction, combining a tree kernel with a bag of words kernel showed promising results. Dependency tree kernels have also been explored in the context of negation extraction in the medical domain. Recently, Bowei et al. (2013) demonstrated the use of tree kernel based approaches in detecting the scope of negations and speculative sentences using the BioScope corpus (Szarvas et al., 2008). However, the task of negation scope detection task is different than that of negation detection. Among other differences, an important one being the presence of annotations for negation cues in the Bioscope corpus. Sohn et al. (2012) developed hand crafted rules representing subtrees of dependency parses of negated sentences and showed that they were effective on a dataset from their institution. 44 Therefore, we implemented a dependency tree kernel similar to the approach described in Culotta and Sorensen (2004) to automatically capture the structural patterns in negated assertions. We used"
W15-1520,W14-1505,0,0.012627,"itchboard corpus (Godfrey et al., 1992). Switchboard is a collection of about 2400 telephone dialogs among 543 speakers in the United States. Each utterance is assigned one of 42 dialog-act tags, which summarize syntactic, semantic and pragmatic information about the turns (e.g., yes/no question, yes answer, agree).8 The minimum, mean, and maximum lengths of the phrases in the training set are 0, 34.1, and 549, respectively. Zero length phrases exist because of the preprocessing, and they are ignored. The task in this section is identifying the dialog act tags from given utterances. Following Milajevs and Purver (2014); Milajevs et al. (2014), we used the first 1115 utterances as the training set and the 8 The tags are described in http://web.stanford. edu/˜jurafsky/ws97/manual.august1.html. Task Method (Milajevs et al., 2014) mean mult mean + mult concat {mean,mult} mean + nbr prj nbr outer prj mean + nbr outer prj Paraphrase detection Addition Multiplication 0.73 0.42 0.686 0.393 0.665 0.652 0.690 0.388 0.688 0.587 0.689 0.387 0.684 0.412 0.688 0.371 Dialog act tagging Addition Multiplication 0.63 0.58 0.638 0.522 0.636 0.606 0.633 0.593 0.636 0.515 0.636 0.581 0.565 0.573 0.626 0.598 Table 4: Accuracies"
W15-1520,D14-1079,0,0.058763,"g, ‡ Department of Linguistics, The Ohio State University, Columbus, OH 43210, USA kimjook@cse.ohio-state.edu, mcdm@ling.ohio-state.edu, fosler@cse.ohio-state.edu Abstract their efficient but still effective architectures. The CBOW model takes the mean vector of projections of the context words and use it to predict the target word as the following objective function:2 Categorical compositional distributional models unify compositional formal semantic models and distributional models by composing phrases with tensor-based methods from vector representations. For the tensor-based compositions, Milajevs et al. (2014) showed that word vectors obtained from the continuous bag-of-words (CBOW) model are competitive with those from co-occurrence based models. However, because word vectors from the CBOW model are trained assuming additive interactions between context words, the word composition used for the training mismatches to the tensor-based methods used for evaluating the actual compositions including pointwise multiplication and tensor product of context vectors. In this work, we show whether the word embeddings from extended CBOW models using multiplication or tensor product between context words, refle"
W15-1520,P08-1028,0,0.120063,"ative interactions between word projections to obtain word embeddings more suitable for the tensor-based compositions. For four datasets, evaluating different types of compositions, we show that those extensions of the CBOW model improve the performance of the actual composition tasks with multiplication or tensor product operations. 2 Tensor-based compositions Prior to discussing the modification to the CBOW algorithm, we review different composition methods used in the literature (Table 1). Addition and Multiplication are compositions by point-wise addition and multiplication, respectively (Mitchell and Lapata, 2008). They can be done simply without any other information, but they cannot reflect word orders and grammatical structures. 144 Mitchell and Lapata (2008, 2009) showed that composition by multiplication can be more effective than composition by addition because additive models compose by considering the content altogether whereas multiplicative models focus on the content relevant to the composition by scaling each element of one with the strength of the corresponding element of the other. Using multiplication as the composition method could be unstable in the previous work because multiplication"
W15-1520,D09-1045,0,0.0258645,"ut they cannot reflect word orders and grammatical structures. 144 Mitchell and Lapata (2008, 2009) showed that composition by multiplication can be more effective than composition by addition because additive models compose by considering the content altogether whereas multiplicative models focus on the content relevant to the composition by scaling each element of one with the strength of the corresponding element of the other. Using multiplication as the composition method could be unstable in the previous work because multiplication with zero or negative values changes the value abruptly (Mitchell and Lapata, 2009). In our models, however, these instability issues could be alleviated since the training model adapt the constituent word vectors to be proper for the composition by multiplication. Mitchell and Lapata (2010) also showed that the tensor product is effective to represent composition because it allows the interactions between different features in different vectors whereas point-wise multiplication can interact with only the same feature in different vectors. Therefore, we also examine an extension of the CBOW model using tensor product for modeling local context. There are neural network model"
W15-1520,D11-1129,0,0.0624369,"Missing"
W15-1520,W11-2507,0,0.0407099,"Missing"
W15-1520,C12-2054,0,0.06082,"l number of words in a corpus, wt is the tth word, pt is the tth word vector, and c is the half window size. Milajevs et al. (2014) showed that the word vectors generated from the CBOW model are competitive with those from co-occurrence based models for both simple arithmetic compositions and tensorbased compositions for categorical compositional distributional models (Coecke et al., 2010).3 Categorical compositional distributional models represent compositional semantics with algebra of Pregroup by representing each grammatical reduction as a linear map in vector spaces (Coecke et al., 2010; Kartsaklis et al., 2012). For example, cats like milk consists of a subject noun, a transitive verb requiring a subject and an object, and an object noun, respectively. In Pregroup grammar,(the types ) of the r l three words in this example are n, n sn , and n, respectively, where n is a noun, nr can be combined with a n in the left, nl can be combined with a n in the right, and s is a declarative statement. Then, 2 Although sum is used in Mikolov et al. (2013a), the current version of word2vec implementation uses mean. 3 Although Milajevs et al. (2014) described that the skipgram model was used to generate the word"
W15-1520,J00-3003,0,0.0137673,"two word sentences in the training corpus for the training. To deal with these issues, in the last model, we combine the mean with the projection of the tensor product. 4 Experiment results To evaluate the five different CBOW-based models proposed in Section 3, we use the following datasets: similarity of transitive verbs with multi146 ple senses from Grefenstette and Sadrzadeh (2011a), three-word sentence similarity from Kartsaklis and Sadrzadeh (2014), paraphrase detection from Dolan et al. (2013), and dialog act tagging for the Switchboard corpus (Godfrey et al., 1992) from Stolcke et al. (2000). These are all the datasets evaluated in Milajevs et al. (2014)’s work as well. Each phrase in the first two datasets is fixed as a subject, a transitive verb, and an object whereas the length of each phrase in the last two datasets is arbitrary. There are several differences between our word vectors and the ones used in Milajevs et al. (2014). First, we use BNC as the training set while Milajevs et al. (2014) use pretrained word vectors from word2vec that are trained using GoogleNews dataset. To reduce the size of projection matrices, all the words are lower-cased and words occurring 20 time"
W15-1520,D13-1170,0,\N,Missing
W16-1607,P15-1011,0,0.0196327,"formation, inhibiting adjustment of vector spaces to better represent semantic intensity scales. In this work, we adjust word vectors using the semantic intensity information in addition to synonyms and antonyms from WordNet and PPDB, and show improved performance on judging semantic intensity orders of adjective pairs on three different human annotated datasets. 1 Introduction Word embedding models that represent words as real-valued vectors have been directly used in word-level NLP tasks such as word similarity (Mikolov et al., 2013b), antonym detection (Ono et al., 2015; Pham et al., 2015; Chen et al., 2015), knowledge relations (Toutanova et al., 2015; Socher et al., 2013; Bordes et al., 2013), and semantic scale inference (Kim and de Marneffe, 2013). Word embedding models such as Word2Vec (continuous bag-of-words (CBOW) and skip-gram) (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), widely used to generate word vectors, are trained following the distributional hypothesis (Harris, 1954) which assumes that the meaning of words can be represented by their context. However, word embedding models based solely on the distributional hypothesis often place words improperly in vector spaces."
W16-1607,P10-1018,1,0.891855,"Missing"
W16-1607,N15-1184,0,0.200006,"Missing"
W16-1607,N15-1100,0,0.377801,"ract because they can have similar contexts in many cases. For better semantic representations, different approaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors"
W16-1607,P15-2070,0,0.0476149,"Missing"
W16-1607,D14-1162,0,0.0781683,"tensity orders of adjective pairs on three different human annotated datasets. 1 Introduction Word embedding models that represent words as real-valued vectors have been directly used in word-level NLP tasks such as word similarity (Mikolov et al., 2013b), antonym detection (Ono et al., 2015; Pham et al., 2015; Chen et al., 2015), knowledge relations (Toutanova et al., 2015; Socher et al., 2013; Bordes et al., 2013), and semantic scale inference (Kim and de Marneffe, 2013). Word embedding models such as Word2Vec (continuous bag-of-words (CBOW) and skip-gram) (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), widely used to generate word vectors, are trained following the distributional hypothesis (Harris, 1954) which assumes that the meaning of words can be represented by their context. However, word embedding models based solely on the distributional hypothesis often place words improperly in vector spaces. For example, in a vector space, a word and its antonym should be sufficiently far apart, but they can be quite close 62 Proceedings of the 1st Workshop on Representation Learning for NLP, pages 62–69, c Berlin, Germany, August 11th, 2016. 2016 Association for Computational Linguistics word a"
W16-1607,J15-4004,0,0.113272,"ds that are adjusted with semantic intensity orders are small, not all the cases comparing the adjustments using just synonyms and antonyms to the adjustments including semantic intensity orders were significant for p-value &lt; 0.05, as shown in Table 4. However, since many of them are slightly insignificant (like p-value=0.07) and the scores noticeably increased in many cases, using semantic intensity orders for the adjustments seem promising. In addition, to show that the adjustments are not harmful for the representation of the general semantics of the words, we also evaluated on SimLex-999 (Hill et al., 2015), where 999 word For the WordNet synset pair dataset and de Melo and Bansal (2013)’s dataset, where the subtle semantic intensity differences are more critical, using synonyms, antonyms, and semantic intensity orders altogether (“syn&ant,same_ord,diff_ord”) showed significantly higher scores than “syn&ant” in many settings. Here, “diff_ord” corresponds to equation 6. Table 5 shows the adjective pairs whose intensity judgements were changed by including adjustments with semantic intensity orders. The pairs are from the WordNet synset pairs and 67 baseline syn&ant same_ord (kmeans only) same_ord"
W16-1607,P15-2004,0,0.0463164,"an have similar contexts in many cases. For better semantic representations, different approaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors from any kinds of m"
W16-1607,N15-1051,1,0.914173,"Missing"
W16-1607,D13-1169,1,0.925372,"Missing"
W16-1607,levy-andrew-2006-tregex,0,0.00928147,"e of word vectors which are wrongly ordered. To reduce wrong orderings, we formulate a Inferring intensity ordering We follow de Melo and Bansal (2013)’s approach to order the adjectives in each cluster. For every possible pair of adjectives in the cluster, we search for regular expressions like “h∗i but not h∗i” in Google N -gram (Brants and Franz, 2006). These patterns give us the direction of the ordering between the adjectives. For example, if “good but not great” appears frequently in Google N -gram, we infer that “great&quot; is semantically stronger than 1 Shivade et al. (2015) used Tregex (Levy and Andrew, 2006) to extract patterns including more words but it is not necessary when we extract patterns from phrases consisting of less or equal to five words. 64 bad Max # Turkers agreeing great 10 9 8 7 6 5 4 good Figure 1: An example of incoherent word vector positions, where “bad” should be closer to “good” than “great” but the similarity between “bad” and “good” is lower than the similarity between “bad” and “great”. annotator agreement (Fleiss’ kappa) of this dataset is 0.359. Note that Fleiss’ kappa is a very conservative measure given the partial order in the annotation, which is not taken into acc"
W16-1607,D15-1174,0,0.0290489,"r spaces to better represent semantic intensity scales. In this work, we adjust word vectors using the semantic intensity information in addition to synonyms and antonyms from WordNet and PPDB, and show improved performance on judging semantic intensity orders of adjective pairs on three different human annotated datasets. 1 Introduction Word embedding models that represent words as real-valued vectors have been directly used in word-level NLP tasks such as word similarity (Mikolov et al., 2013b), antonym detection (Ono et al., 2015; Pham et al., 2015; Chen et al., 2015), knowledge relations (Toutanova et al., 2015; Socher et al., 2013; Bordes et al., 2013), and semantic scale inference (Kim and de Marneffe, 2013). Word embedding models such as Word2Vec (continuous bag-of-words (CBOW) and skip-gram) (Mikolov et al., 2013a) and GloVe (Pennington et al., 2014), widely used to generate word vectors, are trained following the distributional hypothesis (Harris, 1954) which assumes that the meaning of words can be represented by their context. However, word embedding models based solely on the distributional hypothesis often place words improperly in vector spaces. For example, in a vector space, a word and i"
W16-1607,P15-1145,0,0.124901,"in many cases. For better semantic representations, different approaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors from any kinds of models, which can eventu"
W16-1607,Q15-1025,0,0.129688,"N (i) is the set of the initial neighbors of the i-th word. Word pairs with cosine similarities equal to or higher than 0.8 are regarded as neighbors. The objective function for the word vector adjustment is represented as the sum of the three terms:   C V, V 0 = AF (V ) + SC (V ) + KN V, V 0 (4) This function is minimized with stochastic gradient descent with learning rate 0.1 for 20 iterations. Adjusting word embeddings with semantic lexicons In this study, we start from one of three different off-the-shelf word vector types as a baseline for our studies: GloVe, CBOW, and Paragram-SL999 (Wieting et al., 2015); we adjust each of these sets of vectors with a variety of contrastive methods. Our first contrastive system is a baseline using synonyms and antonyms (“syn&ant”) following Mrkši´c et al. (2016)’s approach, which adjusts word vectors so that the sum of the following three max-margin objective functions are minimized. 3 Adjusting word embeddings with semantic intensity orders In order to better model semantic intensity ordering, we augment the synonym and antonym adjusted model with semantic intensity information to adjust word vectors. We first cluster semantically related words, infer semant"
W16-1607,N13-1090,0,0.0982564,"oaches using semantic lexicons as well as lexical knowledge to adjust word vectors have recently been introduced. Faruqui et al. (2015) adjusted each word vector to be in the middle between the initial position and its synonymous words. Mrkši´c et al. (2016) used max-margin approaches to adjust each word vector with synonyms and antonyms while keeping the relative similarities to the neighbors. While these two approaches are post-processing models that adjust preexisting word vectors, Ono et al. (2015), Pham et al. (2015), and Liu et al. (2015) jointly train models that augment the skip-gram (Mikolov et al., 2013a) objective function to include knowledge from semantic lexicons. The common goal in these approaches is to make semantically close words closer and semantically distant words farther apart while keeping each word vector not to be too far from the original position. Although the joint training models can even indirectly adjust words that are not listed in the semantic lexicons (Pham et al., 2015), the post-processing models are much more efficient and can be applied to word vectors from any kinds of models, which can eventually perform better than the joint training models (Mrkši´c et al., 20"
W16-1607,N16-1018,0,0.109011,"Missing"
W16-1607,Q13-1023,0,\N,Missing
W16-2903,S07-1103,0,0.0335092,". Adjectives have been studied extensively in computational linguistics. WordNet (Fellbaum, 1998) classifies adjectives into two broad categories: descriptive and relational. Descriptive adjectives (e.g., big house, heavy bag) ascribe the value of an attribute to a noun, while relational adjectives (e.g., atomic bomb, dental hygiene) do not. Among the various distinctions between descriptive and relational adjectives, relational adjectives are typically not gradable (Fellbaum, 1998). Although association between adjectives and numerical quantities has been a topic of research in some studies (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010), very few studies have investigated grounding the meaning of adjectives to numerical quantities. de Marneffe et al. (2010) investigated the problem of interpreting implied answers to yes/no questions when the response is not explicit. Specifically, they investigated question-answer pairs in which the question contains an adjective and the answer contains a numerical measure. For example, predicting the correct yes/no answer in (1) involves interpreting a numerical quantity (age) with respect to the gradable adjective little. 1. 8 Conclusio"
W16-2903,C00-1044,0,0.297509,"and Plan”). The beginning of a section is formatted as distinct text with the section name in capital letters followed by a newline characted. We used a simple rule-based system to identify section headers and map the contents of a note to these sections. As with note types, section names also had multiple lexical variations (e.g., “Physical Examination” can be “Physical Exam” or “Physical Assessment” or simply “Exam”). Our corpus had 587 Identification of gradable adjectives First, we want to automatically identifiy gradable adjectives in our corpus. We reimplemented the method described in (Hatzivassiloglou and Wiebe, 2000), a log linear regression model that learns the weights associated with two features: 1) Number of times an adjective is used in comparative and superlative constructs, and 2) Number of times an adjective is modified by terms that intensify or diminish the semantic meaning of adjectives (mostly adverbs such as very, little, somewhat, etc. and a few nouns such as bit, etc.). Hatzivassiloglou and Wiebe (2000) manually created a list of 73 such terms. Their model was generated using the 1987 Wall Street Journal Corpus (Marcus et al., 1993) and tested on a hand curated gold standard dataset of 453"
W16-2903,W03-1314,0,0.107498,"Missing"
W16-2903,P10-1133,0,0.0260639,"studied extensively in computational linguistics. WordNet (Fellbaum, 1998) classifies adjectives into two broad categories: descriptive and relational. Descriptive adjectives (e.g., big house, heavy bag) ascribe the value of an attribute to a noun, while relational adjectives (e.g., atomic bomb, dental hygiene) do not. Among the various distinctions between descriptive and relational adjectives, relational adjectives are typically not gradable (Fellbaum, 1998). Although association between adjectives and numerical quantities has been a topic of research in some studies (Aramaki et al., 2007; Davidov and Rappoport, 2010; Iftene and Moruz, 2010), very few studies have investigated grounding the meaning of adjectives to numerical quantities. de Marneffe et al. (2010) investigated the problem of interpreting implied answers to yes/no questions when the response is not explicit. Specifically, they investigated question-answer pairs in which the question contains an adjective and the answer contains a numerical measure. For example, predicting the correct yes/no answer in (1) involves interpreting a numerical quantity (age) with respect to the gradable adjective little. 1. 8 Conclusion We empirically evaluated us"
W16-2903,de-marneffe-etal-2006-generating,1,0.11017,"Missing"
W16-2903,P10-1018,1,0.899156,"Missing"
W16-2903,Q13-1023,0,0.143111,"Missing"
W16-2903,J93-2004,0,0.0538098,"s. We reimplemented the method described in (Hatzivassiloglou and Wiebe, 2000), a log linear regression model that learns the weights associated with two features: 1) Number of times an adjective is used in comparative and superlative constructs, and 2) Number of times an adjective is modified by terms that intensify or diminish the semantic meaning of adjectives (mostly adverbs such as very, little, somewhat, etc. and a few nouns such as bit, etc.). Hatzivassiloglou and Wiebe (2000) manually created a list of 73 such terms. Their model was generated using the 1987 Wall Street Journal Corpus (Marcus et al., 1993) and tested on a hand curated gold standard dataset of 453 adjectives (235 gradable and 218 non-gradable) created using the Collins Birmingham University International Language Database dictionary, which is annotated for gradable and non-gradable adjectives. We developed a logistic regression model with the two features described above. For the first feature, a morphology analysis component was developed to identify inflections of adjectives from their base form. This consisted of identifying adjectives in their comparative form using simple parts-ofspeech tagging (Toutanova et al., 2003) and"
W16-2903,P13-1038,0,0.330314,"n in males is 11.7 while that for females is 13.2. These variations are small in magnitude. However, this is a problem in cases where the dependency on other variables is much more pronounced. We illustrate this through an example. Bone Marrow Cellularity (BMC) is the volume ratio of hematopoietic cells (blood cells that give rise to other blood cells) and fat. Pathologists perform a bone marrow analysis and use the three adjectives hypocellular, normocellular, and hypercellular to describe the sample. However, BMC 23 little kids”), so that both positive and negative instances can be learned. Narisawa et al. (2013) explore a closely related problem of learning numerical common sense for the task of RTE in Japanese text. They study a broad set of cases that require semantic inference over numerical expressions. They query the web to gather instances of pairs of numerical quantities and corresponding contexts and propose two approaches. The distribution based approach concludes the numerical quantity to be large or small if it appears in the top or bottom five percent of the distribution generated for the numerical quantity and normal if it is in between. The cue-based approach relies on explicit textual"
W16-2903,E14-4023,0,0.0403275,"Missing"
W16-2903,R15-1071,0,0.0217434,"Missing"
W16-2903,N15-1051,1,0.85109,"Missing"
W16-2903,N03-1033,0,0.0116011,"Corpus (Marcus et al., 1993) and tested on a hand curated gold standard dataset of 453 adjectives (235 gradable and 218 non-gradable) created using the Collins Birmingham University International Language Database dictionary, which is annotated for gradable and non-gradable adjectives. We developed a logistic regression model with the two features described above. For the first feature, a morphology analysis component was developed to identify inflections of adjectives from their base form. This consisted of identifying adjectives in their comparative form using simple parts-ofspeech tagging (Toutanova et al., 2003) and regular expression based rules. Although the test set used in (Hatzivassiloglou and Wiebe, 2000) is available, the list of 73 noun phrases and adverbial modifications is not. We therefore compiled this list using ten fold cross validation to capture the second feature. In each fold of training, we found all the adverbs and nouns modifying the gradable adjectives using the Stanford Dependency Parser (version 2.0.4) (de Marneffe et al., 2006). We determined the best subset by choosing an optimal threshold for the (k = 81) most frequent modifiers through cross validation. This gave us the se"
W17-2303,W16-2922,0,0.377467,"s without using a full NLP system. This is due largely to the observations of “linguistic regularities” as linear offFurthermore, we present the BioMedical Analogic Similarity Set (BMASS), a novel dataset for 19 Proceedings of the BioNLP 2017 workshop, pages 19–28, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics 3 analogical reasoning in the biomedical domain. This new resource presents real-world examples of semantic relations of interest for biomedical natural language processing research, and we hope it will support further research into biomedical VSMs (Chiu et al., 2016; Choi et al., 2016).1 2 Analogy completion task 3.1 Standard methodology Given an analogy a:b::c:d, the evaluation task is to guess d out of the vocabulary, given a, b, c as evidence. Recent methods for this involve using the vector difference between embedded representations of the related pairs to rank all terms in the vocabulary by how well they complete the analogy, and choosing the best fit. The vector difference is most commonly used in one of three ways, where cos is cosine similarity: Related work Analogical reasoning has been studied both on its own and as a component of downstream t"
W17-2303,C16-1332,0,0.471358,"ask is to correctly fill in the blank. Recent methods consider the vector difference between related terms as representative of the relationship between them, and use this to find the closest vocabulary term for a target analogy, e.g., England - London + Paris ≈ France. However, recent analyses reveal weaknesses of such offset-based methods, including that the use of cosine similarity often reduces to just reflecting nearest neighbor structure (Linzen, 2016), and that there is significant variance in performance between different kinds of relations (K¨oper et al., 2015; Gladkova et al., 2016; Drozd et al., 2016). We identify three key assumptions encoded in the standard offset-based methodology for analogy completion: that a given analogy has only one correct answer, that all relationships between the example pair and the query-target pair are the same, and that the example pair is sufficiently informative with respect to the query-target pair. We demonstrate that these assumptions are violated in real-world data, including in existing analogy datasets. We then propose several modifications to the standard methodology to relax these assumptions, including allowing for multiple correct answers, making"
W17-2303,N15-1184,0,0.0554811,"Missing"
W17-2303,W97-0813,0,0.320101,"s, where cos is cosine similarity: Related work Analogical reasoning has been studied both on its own and as a component of downstream tasks, using a range of systems. Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships. Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy, antonymy, and some world knowledge (Turney, 2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al.,"
W17-2303,N16-2002,0,0.347372,"and::Paris: , and the task is to correctly fill in the blank. Recent methods consider the vector difference between related terms as representative of the relationship between them, and use this to find the closest vocabulary term for a target analogy, e.g., England - London + Paris ≈ France. However, recent analyses reveal weaknesses of such offset-based methods, including that the use of cosine similarity often reduces to just reflecting nearest neighbor structure (Linzen, 2016), and that there is significant variance in performance between different kinds of relations (K¨oper et al., 2015; Gladkova et al., 2016; Drozd et al., 2016). We identify three key assumptions encoded in the standard offset-based methodology for analogy completion: that a given analogy has only one correct answer, that all relationships between the example pair and the query-target pair are the same, and that the example pair is sufficiently informative with respect to the query-target pair. We demonstrate that these assumptions are violated in real-world data, including in existing analogy datasets. We then propose several modifications to the standard methodology to relax these assumptions, including allowing for multiple co"
W17-2303,W09-0205,0,0.0644098,"Missing"
W17-2303,S12-1047,0,0.0296539,"he properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K¨oper et al. (2015), the semantic relationships tend to be significantly more challenging for embedding-based methods (Drozd et al., 2016). Additionally, Levy et al. (2015b) demonstrate that even for some lexical relations where embeddings appear to perform well, they are actually learning prototypicality as opposed to relatedness. argmaxd∈V cos(d, b − a + c) argmaxd∈V cos(d − c, b − a) cos(d, b)cos(d, c) argmaxd∈V cos(d, a) +    (1) (2) (3) Following the terminology of Levy and Goldberg (2014), we r"
W17-2303,N13-1090,0,0.77452,"and Pirelli, 1997) relationships. Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy, antonymy, and some world knowledge (Turney, 2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; howe"
W17-2303,W15-0105,0,0.037807,"Missing"
W17-2303,D14-1162,0,0.0927899,"nymy, antonymy, and some world knowledge (Turney, 2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K¨oper et al. (2015), the semantic relationships tend to be significantly more challenging for emb"
W17-2303,D15-1036,0,0.0215824,"2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K¨oper et al. (2015), the semantic relationships tend to be significantly more challenging for embedding-based methods (Drozd et al., 2016). Addit"
W17-2303,W09-4618,0,0.0163564,"logical reasoning has been studied both on its own and as a component of downstream tasks, using a range of systems. Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships. Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy, antonymy, and some world knowledge (Turney, 2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations on"
W17-2303,N15-1186,0,0.0185792,"nd as a component of downstream tasks, using a range of systems. Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships. Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy, antonymy, and some world knowledge (Turney, 2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes b"
W17-2303,W14-1618,0,0.162956,"world knowledge (Turney, 2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schnabel et al., 2015; Faruqui et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K¨oper et al. (2015), the semantic relationships tend to be significantly more challenging for embedding-based methods (Dro"
W17-2303,C08-1114,0,0.0222801,"l terms in the vocabulary by how well they complete the analogy, and choosing the best fit. The vector difference is most commonly used in one of three ways, where cos is cosine similarity: Related work Analogical reasoning has been studied both on its own and as a component of downstream tasks, using a range of systems. Early work used rule-based systems for world knowledge (Reitman, 1965) and syntactic (Federici and Pirelli, 1997) relationships. Supervised models were used for SAT (Scholastic Aptitude Test) analogies (Veale, 2004), and later for synonymy, antonymy, and some world knowledge (Turney, 2008; Herda˘gdelen and Baroni, 2009). Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation (Federici et al., 1997) and morphological analysis (Lepage and Goh, 2009; Lavall´ee and Langlais, 2010; Soricut and Och, 2015). Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of Mikolov et al. (2013a), often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models (Pennington et al., 2014; Levy and Goldberg, 2014; Schn"
W17-2303,N15-1098,0,0.150776,"et al., 2015), and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies (Mikolov et al., 2013c), which describe morphological relations only; and BATS (Gladkova et al., 2016), which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 (Jurgens et al., 2012) have also been used to derive analogies; however, as with the lexical Sem-Para dataset of K¨oper et al. (2015), the semantic relationships tend to be significantly more challenging for embedding-based methods (Drozd et al., 2016). Additionally, Levy et al. (2015b) demonstrate that even for some lexical relations where embeddings appear to perform well, they are actually learning prototypicality as opposed to relatedness. argmaxd∈V cos(d, b − a + c) argmaxd∈V cos(d − c, b − a) cos(d, b)cos(d, c) argmaxd∈V cos(d, a) +    (1) (2) (3) Following the terminology of Levy and Goldberg (2014), we refer to Equation 1 as 3C OS A DD, Equation 2 as PAIRWISE D ISTANCE, and Equation 3 (which is equivalent to 3C OS A DD with log cosine similarities) as 3C OS M UL. In order to generate analogy data for this task, recent datasets have followed a similar process (Mi"
W17-2303,W16-2503,0,0.72816,"edding methods. 1 In the analogy completion task, a system is presented with an example term pair and a query, e.g., London:England::Paris: , and the task is to correctly fill in the blank. Recent methods consider the vector difference between related terms as representative of the relationship between them, and use this to find the closest vocabulary term for a target analogy, e.g., England - London + Paris ≈ France. However, recent analyses reveal weaknesses of such offset-based methods, including that the use of cosine similarity often reduces to just reflecting nearest neighbor structure (Linzen, 2016), and that there is significant variance in performance between different kinds of relations (K¨oper et al., 2015; Gladkova et al., 2016; Drozd et al., 2016). We identify three key assumptions encoded in the standard offset-based methodology for analogy completion: that a given analogy has only one correct answer, that all relationships between the example pair and the query-target pair are the same, and that the example pair is sufficiently informative with respect to the query-target pair. We demonstrate that these assumptions are violated in real-world data, including in existing analogy da"
W18-2313,W09-2415,0,0.0581208,"Missing"
W18-2313,P08-1077,0,0.0242991,"ers, 2004; Tuarob et al., 2013) and supervised or active learning approaches (Kholghi et al., 2015a) for concept extraction in information retrieval. in this work) such as “scarlet macaw” and “raccoon” occurring in separate documents d1 and d2 may become related by a novel context such as “exotic pets” that may occur as terms in a query or as a phrase in a document dp which could be related to both d1 and d2 . If by some means, documents d1 and d2 were semantically tagged with the phrase “exotic pets” via dp , those documents would surface in the event of such a query (Hendrickx et al., 2009; Bhagat and Ravichandran, 2008). This could thus help to better close the vocabulary gap between potential user queries and the documents. To our knowledge, ours is the first work that employs word and phrase-level embeddings for local context analysis in a pseudorelevance feedback setting (Xu and Croft, 2000), using a language model-based document ranking framework, to semantically tag documents with appropriate concepts for use in downstream retrieval tasks (Kholghi et al., 2015a; De Vine et al., 2014; Sordoni et al., 2014; Zhang et al., 2016; Zuccon et al., 2015; Tuarob et al., 2013). 2 Background and Motivation The prob"
W18-2313,C02-1144,0,0.318159,"using human expert–assigned concept tags for the queries, on top of a standard Okapi BM25–based document retrieval system. We hypothesize, that similar to human experts who can determine the aboutness of an unseen document by recalling meaningful concepts gleaned from similar past experiences via shared contexts, a completely unsupervised machine learning model could be trained to associate documents within a large collection with meaningful concepts discovered by fully leveraging shared contexts within and between documents, thus surfacing “related” concepts specific to the current context (Lin and Pantel, 2002; Halpin et al., 2007; Xu et al., 2014; Kholghi et al., 2015a; Turney and Pantel, 2010; Pantel et al., 2007; Bhagat and Hovy, 2013; Hendrickx et al., 2009). As a trivial example, ordinarily unrelated concepts (noun phrases, 118 Proceedings of the BioNLP 2018 workshop, pages 118–128 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics ing, e.g. clustering by committee (Lin and Pantel, 2002) or semantic class induction as in (Lin and Pantel, 2001b), LDA-based topic modeling (Blei et al., 2003; Griffiths and Steyvers, 2004; Tuarob et al., 2013) and supervised or a"
W18-2313,N07-1071,0,0.0162013,"trieval system. We hypothesize, that similar to human experts who can determine the aboutness of an unseen document by recalling meaningful concepts gleaned from similar past experiences via shared contexts, a completely unsupervised machine learning model could be trained to associate documents within a large collection with meaningful concepts discovered by fully leveraging shared contexts within and between documents, thus surfacing “related” concepts specific to the current context (Lin and Pantel, 2002; Halpin et al., 2007; Xu et al., 2014; Kholghi et al., 2015a; Turney and Pantel, 2010; Pantel et al., 2007; Bhagat and Hovy, 2013; Hendrickx et al., 2009). As a trivial example, ordinarily unrelated concepts (noun phrases, 118 Proceedings of the BioNLP 2018 workshop, pages 118–128 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics ing, e.g. clustering by committee (Lin and Pantel, 2002) or semantic class induction as in (Lin and Pantel, 2001b), LDA-based topic modeling (Blei et al., 2003; Griffiths and Steyvers, 2004; Tuarob et al., 2013) and supervised or active learning approaches (Kholghi et al., 2015a) for concept extraction in information retrieval. in this"
W18-2313,Q13-1030,0,0.02072,"r complex query reformulation in medical IR Manirupa Das† , Eric Fosler-Lussier† , Simon Lin‡ , Soheil Moosavinasab‡ , David Chen‡ , Steve Rust‡ , Yungui Huang‡ & Rajiv Ramnath† The Ohio State University† & Nationwide Children’s Hospital‡ {das.65, fosler.1, ramnath.6}@osu.edu {Simon.Lin, SeyedSoheil.Moosavinasab, David.Chen3, Steve.Rust, Yungui.Huang}@nationwidechildrens.org 1 Abstract Introduction Existing state-of-the-art information retrieval (IR) systems such as knowledge graphs (Su et al., 2015; Sun et al., 2015), or information extraction techniques centered around entity relationships (Ritter et al., 2013), that often rely on some form of weak supervision from ontological or knowledgebase (KB) sources, tend to perform quite reliably on fact-based information retrieval and factoid question answering tasks. However, such systems may be limited in their ability to address the complex information needs of specific types of queries (Roberts et al., 2016; Diekema et al., 2003) in domains such as clinical decision support (Luo et al., 2008) or guided product search (Teo et al., 2016; McAuley and Yang, 2016), due to: 1) complex and subjective, or lengthy nature of the query containing multiple topics,"
W18-2313,Q14-1034,0,0.0754415,"or the queries, on top of a standard Okapi BM25–based document retrieval system. We hypothesize, that similar to human experts who can determine the aboutness of an unseen document by recalling meaningful concepts gleaned from similar past experiences via shared contexts, a completely unsupervised machine learning model could be trained to associate documents within a large collection with meaningful concepts discovered by fully leveraging shared contexts within and between documents, thus surfacing “related” concepts specific to the current context (Lin and Pantel, 2002; Halpin et al., 2007; Xu et al., 2014; Kholghi et al., 2015a; Turney and Pantel, 2010; Pantel et al., 2007; Bhagat and Hovy, 2013; Hendrickx et al., 2009). As a trivial example, ordinarily unrelated concepts (noun phrases, 118 Proceedings of the BioNLP 2018 workshop, pages 118–128 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics ing, e.g. clustering by committee (Lin and Pantel, 2002) or semantic class induction as in (Lin and Pantel, 2001b), LDA-based topic modeling (Blei et al., 2003; Griffiths and Steyvers, 2004; Tuarob et al., 2013) and supervised or active learning approaches (Kholghi et"
W18-3026,N09-1003,0,0.0598181,"s ability to embed many more entities than prior methods, we also filtered the dataset to the 255 similarity pairs and 260 relatedness pairs that all evaluated entity-level methods could represent;4 Table 3 shows similar gains on this even footing. We follow Rastogi et al. (2015) in calculating significance, and use their statistics to estimate the minimum required difference for significant improvements on our datasets. In UMNSRS, we found that cosine similarity of entities consistently reflected human judgments of similarity better than of relatedness; this reflects previous observations by Agirre et al. (2009) and Muneeb et al. (2015). Interestingly, we see the opposite behavior in WikiSRS, where relatedness is captured better than similarity in all settings. In fact, we see a number of errors of relatedness Similarity and relatedness We evaluate our biomedical embeddings on the UMNSRS datasets (Pakhomov et al., 2010), consisting of pairs of UMLS concepts with judgments of similarity (566 pairs) and relatedness (587 pairs), as assigned by medical experts. For evaluating our Wikipedia entity embeddings, we created WikiSRS, a novel dataset of similarity and relatedness judgments of paired Wikipedia e"
W18-3026,P17-1149,0,0.538879,"decay over 10 iterations through the corpus; minimum frequency for both words and terms of 10, and a subsampling coefficient for frequent words of 1e-5. 3.4 (2) Baselines We compare the words, terms,3 and entities learned in our model against two prior biomedical embedding methods, using pretrained embeddings from each. De Vine et al. (2014) use sequences of automatically identified ambiguous entities for skip-gram training, and Mencia et al. (2016) use texts of documents tagged with MeSH headers to represent the header codes. The most recent comparison method for Wikipedia entities is MPME (Cao et al., 2017), which uses link anchors and graph structure to augment textual contexts. We also include skip-gram vectors as a final baseline; for Pubmed, we use pretrained embeddings with optimized hyperparameters from Chiu et al. (2016a), and we train our own embeddings with word2vec for both Wikipedia and Gigaword. Term and entity updates are only calculated when the final token of one or more terms is reached; word updates are applied at each step. To assign more weight to near contexts, we subsample the window size at each step from [1, k]. 3.2 (3) t∈Te where σ is the logistic function. We use a slidi"
W18-3026,W16-2501,0,0.143439,"n our model against two prior biomedical embedding methods, using pretrained embeddings from each. De Vine et al. (2014) use sequences of automatically identified ambiguous entities for skip-gram training, and Mencia et al. (2016) use texts of documents tagged with MeSH headers to represent the header codes. The most recent comparison method for Wikipedia entities is MPME (Cao et al., 2017), which uses link anchors and graph structure to augment textual contexts. We also include skip-gram vectors as a final baseline; for Pubmed, we use pretrained embeddings with optimized hyperparameters from Chiu et al. (2016a), and we train our own embeddings with word2vec for both Wikipedia and Gigaword. Term and entity updates are only calculated when the final token of one or more terms is reached; word updates are applied at each step. To assign more weight to near contexts, we subsample the window size at each step from [1, k]. 3.2 (3) t∈Te where σ is the logistic function. We use a sliding context window to iterate through our corpus. At each step, the word w at the center of the window Cw is updated using O(w, Cw , Nw ), where Nw are the randomlyselected negative samples. As terms are of variable token len"
W18-3026,W14-1618,0,0.0403871,"st absolute difference in word performance vs entity performance: B3 (geneencodes-product), H1 (refers-to), C6 (associatedwith), L1 (form-of ), and L6 (has-free-acid-orbase-form). The better of word and entity performance is highlighted; all entity vs word differences are significant (McNemar’s test; p  0.01). 4.2 4.1.2 B3 2.9 18.3 20.7 Analogy completion We use analogy completion to further explore the properties of our joint embeddings. Given analogy a : b :: c : d, the task is to guess d given (a, b, c), typically by choosing the word or entity with highest cosine similarity to b − a + c (Levy and Goldberg, 2014). We report accuracy using the top guess (ignoring a, b, and c as candidates, per Linzen, 2016). Comparing entities and words We observe clear differences in the rankings made by entity vs word embeddings. As shown in Table 5, highly related entities tend to have high cosine similarity, while word embeddings are more sensitive to lexical overlap and direct cooccurrence. Combining both sources often gives the most inuitive results, balancing lexical effects with relatedness. For example, while the top three pairs by combination in WikiSRS are likely to co-occur, the top three in UMNSRS are pair"
W18-3026,W16-2503,0,0.109166,", C6 (associatedwith), L1 (form-of ), and L6 (has-free-acid-orbase-form). The better of word and entity performance is highlighted; all entity vs word differences are significant (McNemar’s test; p  0.01). 4.2 4.1.2 B3 2.9 18.3 20.7 Analogy completion We use analogy completion to further explore the properties of our joint embeddings. Given analogy a : b :: c : d, the task is to guess d given (a, b, c), typically by choosing the word or entity with highest cosine similarity to b − a + c (Levy and Goldberg, 2014). We report accuracy using the top guess (ignoring a, b, and c as candidates, per Linzen, 2016). Comparing entities and words We observe clear differences in the rankings made by entity vs word embeddings. As shown in Table 5, highly related entities tend to have high cosine similarity, while word embeddings are more sensitive to lexical overlap and direct cooccurrence. Combining both sources often gives the most inuitive results, balancing lexical effects with relatedness. For example, while the top three pairs by combination in WikiSRS are likely to co-occur, the top three in UMNSRS are pairs of drug choices (antibiotics, ACE inhibitors, and chemotherapy drugs, respectively), only one"
W18-3026,W11-0317,0,0.0699802,"Missing"
W18-3026,K16-1026,0,0.0327481,"Missing"
W18-3026,L16-1733,0,0.0685606,"Missing"
W18-3026,W16-2506,0,0.0213383,"202 6 Discussion cyclopedic similarity and relatedness than prior methods, and approach state-of-the-art performance for unsupervised biomedical word sense disambiguation. Furthermore, entities and words learned jointly with our model capture complementary information, and combining them improves performance in all of our evaluations. We make an implementation of our method available at github.com/OSU-slatelab/JET, along with the source code used for our evaluations and our pretrained entity embeddings. Our novel Wikipedia similarity and relatedness datasets are available at the same source. Faruqui et al. (2016) observe that similarity and relatedness are not clearly distinguished in semantic embedding evaluations, and that it is unclear exactly how vector-space models should capture them. We see more evidence of this, as cosine similarity seems to be capturing a mix of the two properties in our data. This mix is clearly informative, but it empirically favors relatedness judgments, and cosine similarity is insufficient to separate the two properties. Corpus polysemy plays a qualitative role in our embedding model, but less of a quantitative one. It does not correlate with similarity and relatedness j"
W18-3026,P09-1113,0,0.0570265,"d surface forms in each of these terminologies. While iterating through the training corpus, we identify any exact matches of the terms in our terminologies.2 We allow for overlapping terms: thus, “in New York City” will include an occurrence of both the terms “New York” and “New York City.” Each matched term may refer to one or more entities; we do not use a disambiguation model in preprocessing, but rather assign a probability distribution over the possible entities. Methods 3.1 In order to jointly learn entity and text representations from an unannotated corpus, we use distant supervision (Mintz et al., 2009) based on known terms, strings which can represent one or more entities. The mapping between terms and entities is many-to-many; for example, the same infection can be expressed as “cold” or “acute rhinitis”, but “cold” can also describe the temperature or refer to chronic obstructive lung disease. Mappings between terms and entities are defined by a terminology.1 We extracted terminologies from two well-known knowledge bases: Model We extend the skip-gram model of Mikolov et al. (2013), to jointly learn vector representations of words, terms, and entities from shared textual contexts. For a g"
W18-3026,D11-1072,0,0.212275,"Missing"
W18-3026,W15-3820,0,0.027686,"ore entities than prior methods, we also filtered the dataset to the 255 similarity pairs and 260 relatedness pairs that all evaluated entity-level methods could represent;4 Table 3 shows similar gains on this even footing. We follow Rastogi et al. (2015) in calculating significance, and use their statistics to estimate the minimum required difference for significant improvements on our datasets. In UMNSRS, we found that cosine similarity of entities consistently reflected human judgments of similarity better than of relatedness; this reflects previous observations by Agirre et al. (2009) and Muneeb et al. (2015). Interestingly, we see the opposite behavior in WikiSRS, where relatedness is captured better than similarity in all settings. In fact, we see a number of errors of relatedness Similarity and relatedness We evaluate our biomedical embeddings on the UMNSRS datasets (Pakhomov et al., 2010), consisting of pairs of UMLS concepts with judgments of similarity (566 pairs) and relatedness (587 pairs), as assigned by medical experts. For evaluating our Wikipedia entity embeddings, we created WikiSRS, a novel dataset of similarity and relatedness judgments of paired Wikipedia entities (people, places,"
W18-3026,K16-1025,0,0.0551446,"Missing"
W18-3026,W17-2303,1,0.843429,"le word embeddings are more sensitive to lexical overlap and direct cooccurrence. Combining both sources often gives the most inuitive results, balancing lexical effects with relatedness. For example, while the top three pairs by combination in WikiSRS are likely to co-occur, the top three in UMNSRS are pairs of drug choices (antibiotics, ACE inhibitors, and chemotherapy drugs, respectively), only one of which is likely to be prescribed to any given patient at once. 4.2.1 Biomedical analogies To compare between word and entity representations, we use the entity-level biomedical dataset BMASS (Newman-Griffis et al., 2017), which includes both entity and string forms for each analogy. In order to test if words and entities are capturing complementary information, we also include an oracle evaluation, in which an analogy is counted as correct if either words or entities produce a correct response.5 We do not compare against prior biomedical entity embedding methods on this dataset, due to their limited vocabulary. Table 6 contrasts the performance of different jointly-trained representations for five relations with the largest performance differences from this dataset. For gene-encodes-product and refers-to, bot"
W18-3026,N15-1058,0,0.0621305,"Missing"
W18-3026,D15-1174,0,0.0584558,"Missing"
W18-3026,D14-1167,0,0.0248297,"Missing"
W18-3026,N15-1026,0,\N,Missing
W19-2002,W11-2501,0,0.0339267,"Transformations are presented in order of decreasing geometric information about the original vectors, and are applied independent of one another to the original source embedding. and concrete nouns).5 • Word similarity and relatedness, using cosine similarity: WordSim-353 (Finkelstein et al., 2001), SimLex-999 (Hill et al., 2015), RareWords (Luong et al., 2013), RG65 (Rubenstein and Goodenough, 1965), MEN (Bruni et al., 2014), and MTURK (Radinsky et al., 2011).4 • Word categorization, using an oracle combination of agglomerative and k-means clustering: AP (Almuhareb and Poesio, 2005), BLESS (Baroni and Lenci, 2011), Battig (Battig and Montague, 1969), and the ESSLLI 2008 shared task (Baroni et al. (2008), performance averaged across nouns, verbs, Given the well-documented issues with using vector arithmetic-based analogy completion as an intrinsic evaluation (Linzen, 2016; Rogers et al., 2017; Newman-Griffis et al., 2017), we do not include it in our analysis. We follow Rogers et al. (2018) in evaluating on a set of five extrinsic tasks:5 • Relation classification: SemEval-2010 Task 8 (Hendrickx et al., 2010), using a CNN with word and distance embeddings (Zeng et al., 2014). • Sentence-level sentiment"
W19-2002,Q17-1010,0,0.0498331,"ces (Szegedy et al., 2014), as well as the use of recent contextualized representation methods using continuous language models (Peters et al., 2018; Devlin et al., 2018). In this work, we aim to bridge the gap between neighborhood-based semantic analysis and geometric performance analysis. We consider four components of the geometry of word embeddings, and transform pretrained embeddings to expose only subsets of these components to downstream models. We transform three popular sets of embeddings, trained using word2vec (Mikolov et al., 2013),1 GloVe (Pennington et al., 2014),2 and FastText (Bojanowski et al., 2017),3 and use the resulting embeddings in a battery of standard evaluations to measure changes in task performance. We find that intrinsic evaluations, which model linguistic information directly in the vector space, Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing nearest neighbors. However, geometric properties of the continuous feature space contribute directly to the use of embedding features in downstream models, and are largely unexplored. We consider four properties of word embedding geometry, namely: position relative"
W19-2002,D15-1075,0,0.0158711,"model from (Kim, 2014). 4 https://github.com/kudkudak/ word-embeddings-benchmarks using single-word datasets only. For brevity, we omit the Sim/Rel splits of WordSim-353 (Agirre et al., 2009), which showed the same trends as the full dataset. 5 https://github.com/drgriffis/ Extrinsic-Evaluation-tasks 12 • Sentiment classification: IMDB movie reviews (Maas et al., 2011), with a single 100-d LSTM. • Subjectivity/objectivity classification: Rotten Tomato snippets (Pang and Lee, 2004), using a logistic regression over summed word embeddings (Li et al., 2017a). • Natural language inference: SNLI (Bowman et al., 2015), using separate LSTMs for premise and hypothesis, combined with a feed-forward classifier. 5 to the instability of nearest neighborhoods and the importance of locality in embedding learning (Wendlandt et al., 2018), although the effects of the autoencoder component also bear further investigation. By effectively increasing the size of the neighborhood considered, CDE adds additional sources of semantic noise. The similar drops from thresholded-NNE transformations, by the same token, is likely related to observations of the relationship between the frequency ranks of a word and its nearest nei"
W19-2002,D14-1181,0,0.0415359,"is a noisy map, but the intent of node2vec to capture patterns in local graph structure makes it a good fit for our analysis. 3.3 Nearest neighbor encoding (NNE) Finally, as a baseline, we use a random encoding Our nearest neighbor encoding transformation fRand : Rd → Rd 3.4 fNNE : Rd → R|V | Random encoding that discards original vectors entirely. While intrinsic evaluations rely only on input embeddings, and thus lose all source information in this case, extrinsic tasks learn a model to transform input features, making even randomlyinitialized vectors a common baseline (Lample et al., 2016; Kim, 2014). For fair comparison, we generate one set of random baselines for each embedding set and re-use these across all tasks. discards the majority of the global pairwise distance information modeled in CDE, and retains only information about nearest neighborhoods. The output of fNNE (v) is a sparse vector. This transformation relates to the common use of nearest neighborhoods as a proxy for semantic information (Wendlandt et al., 2018; Pierrejean and Tanguy, 2018). We take the previously proposed approach of combining the output of fNNE (v) for each v ∈ V to form a sparse adjacency matrix, which d"
W19-2002,P15-1144,0,0.129836,"ll pretrained word vectors v for a given corpus, embedding algorithm, and parameters. The matrix of embeddings MV associated with this set then has shape |V |× d. For simplicity, we restrict our analysis to transformed embeddings of the same dimensionality d as the original vectors. 9 3.1 Definition 4. For every a ∈ Rd and λ ∈ R  { 0 }, we call the map Ha,λ : Rd → Rd given by Affine transformations Affine transformations have been previously utilized for post-processing of word embeddings. For example, Artetxe et al. (2016) learn a matrix transform to align multilingual embedding spaces, and Faruqui et al. (2015) use a linear sparsification to better capture lexical semantics. In addition, the simplicity of affine functions in machine learning contexts (Hofmann et al., 2008) makes them a good starting point for our analysis. Given a set of embeddings in Rd , referred to as an embedding space, affine transformations Ha,λ (v) = a + λ(v − a) a homothety of center a and ratio λ. A homothety centered at the origin is called a dilation. Parameters used in our analysis for each of these transformations are provided in Appendix A. 3.2 fCDE : Rd → R|V | obfuscates the distribution of features in Rd by represen"
W19-2002,N16-1030,0,0.0240613,"he autoencoder, this is a noisy map, but the intent of node2vec to capture patterns in local graph structure makes it a good fit for our analysis. 3.3 Nearest neighbor encoding (NNE) Finally, as a baseline, we use a random encoding Our nearest neighbor encoding transformation fRand : Rd → Rd 3.4 fNNE : Rd → R|V | Random encoding that discards original vectors entirely. While intrinsic evaluations rely only on input embeddings, and thus lose all source information in this case, extrinsic tasks learn a model to transform input features, making even randomlyinitialized vectors a common baseline (Lample et al., 2016; Kim, 2014). For fair comparison, we generate one set of random baselines for each embedding set and re-use these across all tasks. discards the majority of the global pairwise distance information modeled in CDE, and retains only information about nearest neighborhoods. The output of fNNE (v) is a sparse vector. This transformation relates to the common use of nearest neighborhoods as a proxy for semantic information (Wendlandt et al., 2018; Pierrejean and Tanguy, 2018). We take the previously proposed approach of combining the output of fNNE (v) for each v ∈ V to form a sparse adjacency mat"
W19-2002,D17-1257,0,0.0791321,"+ x Cosine distance encoding (CDE) Our cosine distance encoding transformation faffine : Rd → Rd • • • • (3.5) (3.7) (3.8) Vector h ∈ Rd is then used as the compressed representation of v. (3.4) In our experiments, we use ReLU as our activation function ϕ, and train the autoencoder for 50 ˆ. epochs to minimize L2 distance between v and v and I ∈ Matd,d (R) is the identity matrix. 10 We recognize that low-rank compression using an autoencoder is likely to be noisy, thus potentially inducing additional loss in evaluations. However, precedent for capturing geometric structure with autoencoders (Li et al., 2017b) suggests that this is a viable model for our analysis. use node2vec (Grover and Leskovec, 2016) with default parameters to learn this mapping. Like the autoencoder, this is a noisy map, but the intent of node2vec to capture patterns in local graph structure makes it a good fit for our analysis. 3.3 Nearest neighbor encoding (NNE) Finally, as a baseline, we use a random encoding Our nearest neighbor encoding transformation fRand : Rd → Rd 3.4 fNNE : Rd → R|V | Random encoding that discards original vectors entirely. While intrinsic evaluations rely only on input embeddings, and thus lose all"
W19-2002,N15-1004,0,0.0270389,"et al., 2008) makes them a good starting point for our analysis. Given a set of embeddings in Rd , referred to as an embedding space, affine transformations Ha,λ (v) = a + λ(v − a) a homothety of center a and ratio λ. A homothety centered at the origin is called a dilation. Parameters used in our analysis for each of these transformations are provided in Appendix A. 3.2 fCDE : Rd → R|V | obfuscates the distribution of features in Rd by representing a set of word vectors as a pairwise distance matrix. Such a transformation might be used to avoid the non-interpretability of embedding features (Fyshe et al., 2015) and compare embeddings based on relative organization alone. change positions of points relative to the origin. While prior work has typically focused on linear transformations, which fix the origin, we consider the broader class of affine transformations, which do not. Thus, affine transformations such as translation cannot in general be represented as a square matrix for finite-dimensional spaces. We use the following affine transformations: Definition 5. Let a, b ∈ Rd . Then their cosine distance dcos : Rd × Rd → [0, 2] is given by translations; reflections over a hyperplane; rotations abo"
W19-2002,S17-1017,0,0.0282381,"er to measure the contributions of each geometric aspect described in Section 3 to the utility of word embeddings as input features, we evaluate embeddings transformed using our sequence of operations on a battery of standard intrinsic evaluations, which model linguistic information directly in the vector space; and extrinsic evaluations, which use the embeddings as input to learned models for downstream applications Our intrinsic evaluations include: ψ : R|V |→ Rd to transform the nearest neighbor graph back to d-dimensional vectors for evaluation. Following Newman-Griffis and Fosler-Lussier (2017), we 11 (a) Results of intrinsic evaluations (b) Results of extrinsic evaluations Figure 2: Performance metrics on intrinsic and extrinsic tasks, comparing across different transformations applied to each set of word embeddings. Dotted lines are for visual aid in tracking performance on individual tasks, and do not indicate continuous transformations. Transformations are presented in order of decreasing geometric information about the original vectors, and are applied independent of one another to the original source embedding. and concrete nouns).5 • Word similarity and relatedness, using cos"
W19-2002,W17-2303,1,0.777155,"-999 (Hill et al., 2015), RareWords (Luong et al., 2013), RG65 (Rubenstein and Goodenough, 1965), MEN (Bruni et al., 2014), and MTURK (Radinsky et al., 2011).4 • Word categorization, using an oracle combination of agglomerative and k-means clustering: AP (Almuhareb and Poesio, 2005), BLESS (Baroni and Lenci, 2011), Battig (Battig and Montague, 1969), and the ESSLLI 2008 shared task (Baroni et al. (2008), performance averaged across nouns, verbs, Given the well-documented issues with using vector arithmetic-based analogy completion as an intrinsic evaluation (Linzen, 2016; Rogers et al., 2017; Newman-Griffis et al., 2017), we do not include it in our analysis. We follow Rogers et al. (2018) in evaluating on a set of five extrinsic tasks:5 • Relation classification: SemEval-2010 Task 8 (Hendrickx et al., 2010), using a CNN with word and distance embeddings (Zeng et al., 2014). • Sentence-level sentiment polarity classification: MR movie reviews (Pang and Lee, 2005), with a simplified CNN model from (Kim, 2014). 4 https://github.com/kudkudak/ word-embeddings-benchmarks using single-word datasets only. For brevity, we omit the Sim/Rel splits of WordSim-353 (Agirre et al., 2009), which showed the same trends as th"
W19-2002,C18-1228,0,0.0282661,"Goodenough, 1965), MEN (Bruni et al., 2014), and MTURK (Radinsky et al., 2011).4 • Word categorization, using an oracle combination of agglomerative and k-means clustering: AP (Almuhareb and Poesio, 2005), BLESS (Baroni and Lenci, 2011), Battig (Battig and Montague, 1969), and the ESSLLI 2008 shared task (Baroni et al. (2008), performance averaged across nouns, verbs, Given the well-documented issues with using vector arithmetic-based analogy completion as an intrinsic evaluation (Linzen, 2016; Rogers et al., 2017; Newman-Griffis et al., 2017), we do not include it in our analysis. We follow Rogers et al. (2018) in evaluating on a set of five extrinsic tasks:5 • Relation classification: SemEval-2010 Task 8 (Hendrickx et al., 2010), using a CNN with word and distance embeddings (Zeng et al., 2014). • Sentence-level sentiment polarity classification: MR movie reviews (Pang and Lee, 2005), with a simplified CNN model from (Kim, 2014). 4 https://github.com/kudkudak/ word-embeddings-benchmarks using single-word datasets only. For brevity, we omit the Sim/Rel splits of WordSim-353 (Agirre et al., 2009), which showed the same trends as the full dataset. 5 https://github.com/drgriffis/ Extrinsic-Evaluation-t"
W19-2002,P04-1035,0,0.00939176,"(Zeng et al., 2014). • Sentence-level sentiment polarity classification: MR movie reviews (Pang and Lee, 2005), with a simplified CNN model from (Kim, 2014). 4 https://github.com/kudkudak/ word-embeddings-benchmarks using single-word datasets only. For brevity, we omit the Sim/Rel splits of WordSim-353 (Agirre et al., 2009), which showed the same trends as the full dataset. 5 https://github.com/drgriffis/ Extrinsic-Evaluation-tasks 12 • Sentiment classification: IMDB movie reviews (Maas et al., 2011), with a single 100-d LSTM. • Subjectivity/objectivity classification: Rotten Tomato snippets (Pang and Lee, 2004), using a logistic regression over summed word embeddings (Li et al., 2017a). • Natural language inference: SNLI (Bowman et al., 2015), using separate LSTMs for premise and hypothesis, combined with a feed-forward classifier. 5 to the instability of nearest neighborhoods and the importance of locality in embedding learning (Wendlandt et al., 2018), although the effects of the autoencoder component also bear further investigation. By effectively increasing the size of the neighborhood considered, CDE adds additional sources of semantic noise. The similar drops from thresholded-NNE transformatio"
W19-2002,P05-1015,0,0.222487,"he ESSLLI 2008 shared task (Baroni et al. (2008), performance averaged across nouns, verbs, Given the well-documented issues with using vector arithmetic-based analogy completion as an intrinsic evaluation (Linzen, 2016; Rogers et al., 2017; Newman-Griffis et al., 2017), we do not include it in our analysis. We follow Rogers et al. (2018) in evaluating on a set of five extrinsic tasks:5 • Relation classification: SemEval-2010 Task 8 (Hendrickx et al., 2010), using a CNN with word and distance embeddings (Zeng et al., 2014). • Sentence-level sentiment polarity classification: MR movie reviews (Pang and Lee, 2005), with a simplified CNN model from (Kim, 2014). 4 https://github.com/kudkudak/ word-embeddings-benchmarks using single-word datasets only. For brevity, we omit the Sim/Rel splits of WordSim-353 (Agirre et al., 2009), which showed the same trends as the full dataset. 5 https://github.com/drgriffis/ Extrinsic-Evaluation-tasks 12 • Sentiment classification: IMDB movie reviews (Maas et al., 2011), with a single 100-d LSTM. • Subjectivity/objectivity classification: Rotten Tomato snippets (Pang and Lee, 2004), using a logistic regression over summed word embeddings (Li et al., 2017a). • Natural lan"
W19-2002,D14-1162,0,0.106265,"ural models about continuity in input spaces (Szegedy et al., 2014), as well as the use of recent contextualized representation methods using continuous language models (Peters et al., 2018; Devlin et al., 2018). In this work, we aim to bridge the gap between neighborhood-based semantic analysis and geometric performance analysis. We consider four components of the geometry of word embeddings, and transform pretrained embeddings to expose only subsets of these components to downstream models. We transform three popular sets of embeddings, trained using word2vec (Mikolov et al., 2013),1 GloVe (Pennington et al., 2014),2 and FastText (Bojanowski et al., 2017),3 and use the resulting embeddings in a battery of standard evaluations to measure changes in task performance. We find that intrinsic evaluations, which model linguistic information directly in the vector space, Analysis of word embedding properties to inform their use in downstream NLP tasks has largely been studied by assessing nearest neighbors. However, geometric properties of the continuous feature space contribute directly to the use of embedding features in downstream models, and are largely unexplored. We consider four properties of word embed"
W19-2002,N18-1190,0,0.193263,"nd analogy completion are intuitive and easy to compute, they are limited by both confounding geometric factors (Linzen, 2016) and task-specific factors (Faruqui et al., 2016; Rogers et al., 2017). Chiu et al. (2016) show that these tasks, while correlated with some semantic content, do not always predict downstream performance. Thus, it is necessary to use a more comprehensive set of intrinsic and extrinsic evaluations for embeddings. Nearest neighbors in sets of embeddings are commonly used as a proxy for qualitative semantic information. However, their instability across embedding samples (Wendlandt et al., 2018) is a limiting factor, and they do not necessarily correlate with linguistic analyses (Hellrich and Hahn, 2016). Modeling neighborhoods as a graph structure offers an alternative analysis method (Cuba Gyllensten and Sahlgren, 2015), as does 2-D or 3-D visualization (Heimerl and Gleicher, 2018). However, both of these methods provide qualitative insights only. By systematically analyzing geometric information with a wide variety of evalUsing each of our sets of pretrained word embeddings, we apply a variety of transformations to induce new embeddings that only expose subsets of these attributes"
W19-2002,N18-1202,0,0.266451,"to understand the properties of word embeddings, both in terms of their distribution (Mimno and Thompson, 2017) and correlation with downstream performance (Chandrahas et al., 2018). Through such geometric investigations, neighborhood-based semantic characterizations are augmented with information about the continuous feature space of an embedding. Geometric features offer a more direct connection to the assumptions made by neural models about continuity in input spaces (Szegedy et al., 2014), as well as the use of recent contextualized representation methods using continuous language models (Peters et al., 2018; Devlin et al., 2018). In this work, we aim to bridge the gap between neighborhood-based semantic analysis and geometric performance analysis. We consider four components of the geometry of word embeddings, and transform pretrained embeddings to expose only subsets of these components to downstream models. We transform three popular sets of embeddings, trained using word2vec (Mikolov et al., 2013),1 GloVe (Pennington et al., 2014),2 and FastText (Bojanowski et al., 2017),3 and use the resulting embeddings in a battery of standard evaluations to measure changes in task performance. We find tha"
W19-2002,C14-1220,0,0.0468096,"d Poesio, 2005), BLESS (Baroni and Lenci, 2011), Battig (Battig and Montague, 1969), and the ESSLLI 2008 shared task (Baroni et al. (2008), performance averaged across nouns, verbs, Given the well-documented issues with using vector arithmetic-based analogy completion as an intrinsic evaluation (Linzen, 2016; Rogers et al., 2017; Newman-Griffis et al., 2017), we do not include it in our analysis. We follow Rogers et al. (2018) in evaluating on a set of five extrinsic tasks:5 • Relation classification: SemEval-2010 Task 8 (Hendrickx et al., 2010), using a CNN with word and distance embeddings (Zeng et al., 2014). • Sentence-level sentiment polarity classification: MR movie reviews (Pang and Lee, 2005), with a simplified CNN model from (Kim, 2014). 4 https://github.com/kudkudak/ word-embeddings-benchmarks using single-word datasets only. For brevity, we omit the Sim/Rel splits of WordSim-353 (Agirre et al., 2009), which showed the same trends as the full dataset. 5 https://github.com/drgriffis/ Extrinsic-Evaluation-tasks 12 • Sentiment classification: IMDB movie reviews (Maas et al., 2011), with a single 100-d LSTM. • Subjectivity/objectivity classification: Rotten Tomato snippets (Pang and Lee, 2004)"
W19-2002,N18-4005,0,0.019869,"in this case, extrinsic tasks learn a model to transform input features, making even randomlyinitialized vectors a common baseline (Lample et al., 2016; Kim, 2014). For fair comparison, we generate one set of random baselines for each embedding set and re-use these across all tasks. discards the majority of the global pairwise distance information modeled in CDE, and retains only information about nearest neighborhoods. The output of fNNE (v) is a sparse vector. This transformation relates to the common use of nearest neighborhoods as a proxy for semantic information (Wendlandt et al., 2018; Pierrejean and Tanguy, 2018). We take the previously proposed approach of combining the output of fNNE (v) for each v ∈ V to form a sparse adjacency matrix, which describes a directed nearest neighbor graph (Cuba Gyllensten and Sahlgren, 2015; Newman-Griffis and Fosler-Lussier, 2017), using three versions of fNNE defined below. Thresholded The set of non-zero indices in ˜ such that fNNE (v) correspond to word vectors v ˜ is greater than or the cosine similarity of v and v equal to an arbitrary threshold t. In order to ensure that every word has non-zero out degree in the graph, we also include the k nearest neighbors by"
W19-2002,J15-4004,0,\N,Missing
W19-2002,D15-1292,0,\N,Missing
W19-2002,P11-1015,0,\N,Missing
W19-2002,S10-1006,0,\N,Missing
W19-2002,W13-3512,0,\N,Missing
W19-2002,W16-2501,0,\N,Missing
W19-2002,D16-1250,0,\N,Missing
W19-2002,C16-1262,0,\N,Missing
W19-2002,D17-1308,0,\N,Missing
W19-2002,P18-1012,0,\N,Missing
