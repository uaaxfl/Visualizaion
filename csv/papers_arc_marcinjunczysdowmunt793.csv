2021.naacl-main.92,The Curious Case of Hallucinations in Neural Machine Translation,2021,-1,-1,3,0,3521,vikas raunak,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"In this work, we study hallucinations in Neural Machine Translation (NMT), which lie at an extreme end on the spectrum of NMT pathologies. Firstly, we connect the phenomenon of hallucinations under source perturbation to the Long-Tail theory of Feldman, and present an empirically validated hypothesis that explains hallucinations under source perturbation. Secondly, we consider hallucinations under corpus-level noise (without any source perturbation) and demonstrate that two prominent types of natural hallucinations (detached and oscillatory outputs) could be generated and explained through specific corpus-level noise patterns. Finally, we elucidate the phenomenon of hallucination amplification in popular data-generation processes such as Backtranslation and sequence-level Knowledge Distillation. We have released the datasets and code to replicate our results."
2021.humeval-1.11,On User Interfaces for Large-Scale Document-Level Human Evaluation of Machine Translation Outputs,2021,-1,-1,2,0.6958,6016,roman grundkiewicz,Proceedings of the Workshop on Human Evaluation of NLP Systems (HumEval),0,"Recent studies emphasize the need of document context in human evaluation of machine translations, but little research has been done on the impact of user interfaces on annotator productivity and the reliability of assessments. In this work, we compare human assessment data from the last two WMT evaluation campaigns collected via two different methods for document-level evaluation. Our analysis shows that a document-centric approach to evaluation where the annotator is presented with the entire document context on a screen leads to higher quality segment and document level assessments. It improves the correlation between segment and document scores and increases inter-annotator agreement for document scores but is considerably more time consuming for annotators."
2021.emnlp-main.539,{L}evenshtein Training for Word-level Quality Estimation,2021,-1,-1,2,0,4416,shuoyang ding,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"We propose a novel scheme to use the Levenshtein Transformer to perform the task of word-level quality estimation. A Levenshtein Transformer is a natural fit for this task: trained to perform decoding in an iterative manner, a Levenshtein Transformer can learn to post-edit without explicit supervision. To further minimize the mismatch between the translation task and the word-level QE task, we propose a two-stage transfer learning procedure on both augmented data and human post-editing data. We also propose heuristics to construct reference labels that are compatible with subword-level finetuning and inference. Results on WMT 2020 QE shared task dataset show that our proposed method has superior data efficiency under the data-constrained setting and competitive performance under the unconstrained setting."
W19-5321,{M}icrosoft Translator at {WMT} 2019: Towards Large-Scale Document-Level Neural Machine Translation,2019,11,3,1,1,3523,marcin junczysdowmunt,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"This paper describes the Microsoft Translator submissions to the WMT19 news translation shared task for English-German. Our main focus is document-level neural machine translation with deep transformer models. We start with strong sentence-level baselines, trained on large-scale data created via data-filtering and noisy back-translation and find that back-translation seems to mainly help with translationese input. We explore fine-tuning techniques, deeper models and different ensembling strategies to counter these effects. Using document boundaries present in the authentic and synthetic parallel data, we create sequences of up to 1000 subword segments and train transformer translation models. We experiment with data augmentation techniques for the smaller authentic data with document-boundaries and for larger authentic data without boundaries. We further explore multi-task training for the incorporation of document-level source language monolingual data via the BERT-objective on the encoder and two-pass decoding for combinations of sentence-level and document-level systems. Based on preliminary human evaluation results, evaluators strongly prefer the document-level systems over our comparable sentence-level system. The document-level systems also seem to score higher than the human references in source-based direct assessment."
W19-4427,Neural Grammatical Error Correction Systems with Unsupervised Pre-training on Synthetic Data,2019,0,9,2,0.784061,6016,roman grundkiewicz,Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications,0,"Considerable effort has been made to address the data sparsity problem in neural grammatical error correction. In this work, we propose a simple and surprisingly effective unsupervised synthetic error generation method based on confusion sets extracted from a spellchecker to increase the amount of training data. Synthetic data is used to pre-train a Transformer sequence-to-sequence model, which not only improves over a strong baseline trained on authentic error-annotated data, but also enables the development of a practical GEC system in a scenario where little genuine error-annotated data is available. The developed systems placed first in the BEA19 shared task, achieving 69.47 and 64.24 F$_{0.5}$ in the restricted and low-resource tracks respectively, both on the W{\&}I+LOCNESS test set. On the popular CoNLL 2014 test set, we report state-of-the-art results of 64.16 M{\mbox{$^2$}} for the submitted system, and 61.30 M{\mbox{$^2$}} for the constrained system trained on the NUCLE and Lang-8 data."
D19-5632,From Research to Production and Back: Ludicrously Fast Neural Machine Translation,2019,0,0,2,0,14496,young kim,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"This paper describes the submissions of the {``}Marian{''} team to the WNGT 2019 efficiency shared task. Taking our dominating submissions to the previous edition of the shared task as a starting point, we develop improved teacher-student training via multi-agent dual-learning and noisy backward-forward translation for Transformer-based student models. For efficient CPU-based decoding, we propose pre-packed 8-bit matrix products, improved batched decoding, cache-friendly student architectures with parameter sharing and light-weight RNN-based decoder architectures. GPU-based decoding benefits from the same architecture changes, from pervasive 16-bit inference and concurrent streams. These modifications together with profiler-based C++ code optimization allow us to push the Pareto frontier established during the 2018 edition towards 24x (CPU) and 14x (GPU) faster models at comparable or higher BLEU values. Our fastest CPU model is more than 4x faster than last year{'}s fastest submission at more than 3 points higher BLEU. Our fastest GPU model at 1.5 seconds translation time is slightly faster than last year{'}s fastest RNN-based submissions, but outperforms them by more than 4 BLEU and 10 BLEU points respectively."
D19-5546,Minimally-Augmented Grammatical Error Correction,2019,0,0,2,0.784061,6016,roman grundkiewicz,Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019),0,"There has been an increased interest in low-resource approaches to automatic grammatical error correction. We introduce Minimally-Augmented Grammatical Error Correction (MAGEC) that does not require any error-labelled data. Our unsupervised approach is based on a simple but effective synthetic error generation method based on confusion sets from inverted spell-checkers. In low-resource settings, we outperform the current state-of-the-art results for German and Russian GEC tasks by a large margin without using any real error-annotated training data. When combined with labelled data, our method can serve as an efficient pre-training technique"
W18-6415,{M}icrosoft{'}s Submission to the {WMT}2018 News Translation Task: How {I} Learned to Stop Worrying and Love the Data,2018,10,2,1,1,3523,marcin junczysdowmunt,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,This paper describes the Microsoft submission to the WMT2018 news translation shared task. We participated in one language direction {--} English-German. Our system follows current best-practice and combines state-of-the-art models with new data filtering (dual conditional cross-entropy filtering) and sentence weighting methods. We trained fairly standard Transformer-big models with an updated version of Edinburgh{'}s training scheme for WMT2017 and experimented with different filtering schemes for Paracrawl. According to automatic metrics (BLEU) we reached the highest score for this subtask with a nearly 2 BLEU point margin over the next strongest system. Based on human evaluation we ranked first among constrained systems. We believe this is mostly caused by our data filtering/weighting regime.
W18-6467,{MS}-{UE}din Submission to the {WMT}2018 {APE} Shared Task: Dual-Source Transformer for Automatic Post-Editing,2018,10,0,1,1,3523,marcin junczysdowmunt,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"This paper describes the Microsoft and University of Edinburgh submission to the Automatic Post-editing shared task at WMT2018. Based on training data and systems from the WMT2017 shared task, we re-implement our own models from the last shared task and introduce improvements based on extensive parameter sharing. Next we experiment with our implementation of dual-source transformer models and data selection for the IT domain. Our submissions decisively wins the SMT post-editing sub-task establishing the new state-of-the-art and is a very close second (or equal, 16.46 vs 16.50 TER) in the NMT sub-task. Based on the rather weak results in the NMT sub-task, we hypothesize that neural-on-neural APE might not be actually useful."
W18-6478,Dual Conditional Cross-Entropy Filtering of Noisy Parallel Corpora,2018,10,1,1,1,3523,marcin junczysdowmunt,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"In this work we introduce dual conditional cross-entropy filtering for noisy parallel data. For each sentence pair of the noisy parallel corpus we compute cross-entropy scores according to two inverse translation models trained on clean data. We penalize divergent cross-entropies and weigh the penalty by the cross-entropy average of both models. Sorting or thresholding according to these scores results in better subsets of parallel data. We achieve higher BLEU scores with models trained on parallel data filtered only from Paracrawl than with models trained on clean WMT data. We further evaluate our method in the context of the WMT2018 shared task on parallel corpus filtering and achieve the overall highest ranking scores of the shared task, scoring top in three out of four subtasks."
W18-2716,{M}arian: Cost-effective High-Quality Neural Machine Translation in {C}++,2018,0,8,1,1,3523,marcin junczysdowmunt,Proceedings of the 2nd Workshop on Neural Machine Translation and Generation,0,"This paper describes the submissions of the {``}Marian{''} team to the WNMT 2018 shared task. We investigate combinations of teacher-student training, low-precision matrix products, auto-tuning and other methods to optimize the Transformer model on GPU and CPU. By further integrating these methods with the new averaging attention networks, a recently introduced faster Transformer variant, we create a number of high-quality, high-performance models on the GPU and CPU, dominating the Pareto frontier for this shared task."
W18-2105,Are we experiencing the Golden Age of Automatic Post-Editing?,2018,-1,-1,1,1,3523,marcin junczysdowmunt,Proceedings of the {AMTA} 2018 Workshop on Translation Quality Estimation and Automatic Post-Editing,0,None
P18-4020,{M}arian: Fast Neural Machine Translation in {C}++,2018,8,28,1,1,3523,marcin junczysdowmunt,"Proceedings of {ACL} 2018, System Demonstrations",0,"We present Marian, an efficient and self-contained Neural Machine Translation framework with an integrated automatic differentiation engine based on dynamic computation graphs. Marian is written entirely in C++. We describe the design of the encoder-decoder framework and demonstrate that a research-friendly toolkit can achieve high training and translation speed."
N18-2046,Near Human-Level Performance in Grammatical Error Correction with Hybrid Machine Translation,2018,25,9,2,0.784061,6016,roman grundkiewicz,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"We combine two of the most popular approaches to automated Grammatical Error Correction (GEC): GEC based on Statistical Machine Translation (SMT) and GEC based on Neural Machine Translation (NMT). The hybrid system achieves new state-of-the-art results on the CoNLL-2014 and JFLEG benchmarks. This GEC system preserves the accuracy of SMT output and, at the same time, generates more fluent sentences as it typical for NMT. Our analysis shows that the created systems are closer to reaching human-level performance than any other GEC system reported so far."
N18-1055,Approaching Neural Grammatical Error Correction as a Low-Resource Machine Translation Task,2018,33,17,1,1,3523,marcin junczysdowmunt,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",0,"Previously, neural methods in grammatical error correction (GEC) did not reach state-of-the-art results compared to phrase-based statistical machine translation (SMT) baselines. We demonstrate parallels between neural GEC and low-resource neural MT and successfully adapt several methods from low-resource MT to neural GEC. We further establish guidelines for trustable results in neural GEC and propose a set of model-independent methods for neural GEC that can be easily applied in most GEC settings. Proposed methods include adding source-side noise, domain-adaptation techniques, a GEC-specific training-objective, transfer learning with monolingual data, and ensembling of independently trained GEC models and language models. The combined effects of these methods result in better than state-of-the-art neural GEC models that outperform previously best neural GEC systems by more than 10{\%} M{\mbox{$^2$}} on the CoNLL-2014 benchmark and 5.9{\%} on the JFLEG test set. Non-neural state-of-the-art systems are outperformed by more than 2{\%} on the CoNLL-2014 benchmark and by 4{\%} on JFLEG."
D18-1332,Accelerating Asynchronous Stochastic Gradient Descent for Neural Machine Translation,2018,0,2,4,0.625,5886,nikolay bogoychev,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,In order to extract the best possible performance from asynchronous stochastic gradient descent one must increase the mini-batch size and scale the learning rate accordingly. In order to achieve further speedup we introduce a technique that delays gradient updates effectively increasing the mini-batch size. Unfortunately with the increase of mini-batch size we worsen the stale gradient problem in asynchronous stochastic gradient descent (SGD) which makes the model convergence poor. We introduce local optimizers which mitigate the stale gradient problem and together with fine tuning our momentum we are able to train a shallow machine translation system 27{\%} faster than an optimized baseline with negligible penalty in BLEU.
W17-4707,Predicting Target Language {CCG} Supertags Improves Neural Machine Translation,2017,0,19,5,0,25421,maria nuadejde,Proceedings of the Second Conference on Machine Translation,0,None
W17-4774,The {AMU}-{UE}din Submission to the {WMT} 2017 Shared Task on Automatic Post-Editing,2017,10,4,1,1,3523,marcin junczysdowmunt,Proceedings of the Second Conference on Machine Translation,0,None
Q17-1015,Pushing the Limits of Translation Quality Estimation,2017,29,17,2,0,3896,andre martins,Transactions of the Association for Computational Linguistics,0,"Translation quality estimation is a task of growing importance in NLP, due to its potential to reduce post-editing human effort in disruptive ways. However, this potential is currently limited by the relatively low accuracy of existing systems. In this paper, we achieve remarkable improvements by exploiting synergies between the related tasks of word-level quality estimation and automatic post-editing. First, we stack a new, carefully engineered, neural model into a rich feature-based word-level quality estimation system. Then, we use the output of an automatic post-editing system as an extra feature, obtaining striking results on WMT16: a word-level FMULT1 score of 57.47{\%} (an absolute gain of +7.95{\%} over the current state of the art), and a Pearson correlation score of 65.56{\%} for sentence-level HTER prediction (an absolute gain of +13.36{\%})."
I17-1013,An Exploration of Neural Sequence-to-Sequence Architectures for Automatic Post-Editing,2017,0,2,1,1,3523,marcin junczysdowmunt,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 1: Long Papers),0,"In this work, we explore multiple neural architectures adapted for the task of automatic post-editing of machine translation output. We focus on neural end-to-end models that combine both inputs $mt$ (raw MT output) and $src$ (source language input) in a single neural architecture, modeling $\{mt, src\} \rightarrow pe$ directly. Apart from that, we investigate the influence of hard-attention models which seem to be well-suited for monolingual tasks, as well as combinations of both ideas. We report results on data sets provided during the WMT-2016 shared task on automatic post-editing and can demonstrate that dual-attention models that incorporate all available data in the APE scenario in a single model improve on the best shared task system and on all other published results after the shared task. Dual-attention models that are combined with hard attention remain competitive despite applying fewer changes to the input."
E17-3017,{N}ematus: a Toolkit for Neural Machine Translation,2017,5,124,7,0,2690,rico sennrich,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments."
E17-3029,The {SUMMA} Platform Prototype,2017,8,1,21,0,28433,renars liepins,Proceedings of the Software Demonstrations of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present the first prototype of the SUMMA Platform: an integrated platform for multilingual media monitoring. The platform contains a rich suite of low-level and high-level natural language processing technologies: automatic speech recognition of broadcast media, machine translation, automated tagging and classification of named entities, semantic parsing to detect relationships between entities, and automatic construction / augmentation of factual knowledge bases. Implemented on the Docker platform, it can easily be deployed, customised, and scaled to large volumes of incoming media streams."
W16-2316,The {AMU}-{UEDIN} Submission to the {WMT}16 News Translation Task: Attention-based {NMT} Models as Feature Functions in Phrase-based {SMT},2016,14,24,1,1,3523,marcin junczysdowmunt,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the AMU-UEDIN submissions to the WMT 2016 shared task on news translation. We explore methods of decode-time integration of attention-based neural translation models with phrase-based statistical machine translation. Efficient batch-algorithms for GPU-querying are proposed and implemented. For English-Russian, our system stays behind the state-of-the-art pure neural models in terms of BLEU. Among restricted systems, manual evaluation places it in the first cluster tied with the pure neural model. For the Russian-English task, our submission achieves the top BLEU result, outperforming the best pure neural system by 1.1 BLEU points and our own phrase-based baseline by 1.6 BLEU. After manual evaluation, this system is the best restricted system in its own cluster. In follow-up experiments we improve results by additional 0.8 BLEU."
W16-2378,Log-linear Combinations of Monolingual and Bilingual Neural Machine Translation Models for Automatic Post-Editing,2016,21,24,1,1,3523,marcin junczysdowmunt,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"This paper describes the submission of the AMU (Adam Mickiewicz University) team to the Automatic Post-Editing (APE) task of WMT 2016. We explore the application of neural translation models to the APE problem and achieve good results by treating different models as components in a log-linear model, allowing for multiple inputs (the MT-output and the source) that are decoded to the same target language (post-edited translations). A simple string-matching penalty integrated within the log-linear model is used to control for higher faithfulness with regard to the raw machine translation output. To overcome the problem of too little training data, we generate large amounts of artificial data. Our submission improves over the uncorrected baseline on the unseen test set by -3.2% TER and 5.5% BLEU and outperforms any other system submitted to the shared-task by a large margin."
P16-1161,Target-Side Context for Discriminative Models in Statistical Machine Translation,2016,18,4,4,0,4973,alevs tamchyna,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Discriminative translation models utilizing source context have been shown to help statistical machine translation performance. We propose a novel extension of this work using target context information. Surprisingly, we show that this model can be efficiently integrated directly in the decoding process. Our approach scales to large training data sizes and results in consistent improvements in translation quality on four language pairs. We also provide an analysis comparing the strengths of the baseline source-context model with our extended source-context and target-context model and we show that our extension allows us to better capture morphological coherence. Our work is freely available as part of Moses."
L16-1561,The {U}nited {N}ations Parallel Corpus v1.0,2016,16,96,2,0,35296,michal ziemski,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper describes the creation process and statistics of the official United Nations Parallel Corpus, the first parallel corpus composed from United Nations documents published by the original data creator. The parallel corpus presented consists of manually translated UN documents from the last 25 years (1990 to 2014) for the six official UN languages, Arabic, Chinese, English, French, Russian, and Spanish. The corpus is freely available for download under a liberal license. Apart from the pairwise aligned documents, a fully aligned subcorpus for the six official UN languages is distributed. We provide baseline BLEU scores of our Moses-based SMT systems trained with the full data of language pairs involving English and for all possible translation directions of the six-way subcorpus."
D16-1161,Phrase-based Machine Translation is State-of-the-Art for Automatic Grammatical Error Correction,2016,24,29,1,1,3523,marcin junczysdowmunt,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"In this work, we study parameter tuning towards the M^2 metric, the standard metric for automatic grammar error correction (GEC) tasks. After implementing M^2 as a scorer in the Moses tuning framework, we investigate interactions of dense and sparse features, different optimizers, and tuning strategies for the CoNLL-2014 shared task. We notice erratic behavior when optimizing sparse feature weights with M^2 and offer partial solutions. We find that a bare-bones phrase-based SMT setup with task-specific parameter-tuning outperforms all previously published results for the CoNLL-2014 test set by a large margin (46.37% M^2 over previously 41.75%, by an SMT system with neural features) while being trained on the same, publicly available data. Our newly introduced dense and sparse features widen that gap, and we improve the state-of-the-art to 49.49% M^2."
2016.amta-researchers.4,"Fast, Scalable Phrase-Based {SMT} Decoding",2016,15,1,4,0,22899,hieu hoang,Conferences of the Association for Machine Translation in the Americas: MT Researchers' Track,0,"The utilization of statistical machine translation (SMT) has grown enormously over the last decade, many using open-source software developed by the NLP community. As commercial use has increased, there is need for software that is optimized for commercial requirements, in particular, fast phrase-based decoding and more efficient utilization of modern multicore servers. In this paper we re-examine the major components of phrase-based decoding and decoder implementation with particular emphasis on speed and scalability on multicore machines. The result is a drop-in replacement for the Moses decoder which is up to fifteen times faster and scales monotonically with the number of cores."
W15-4927,{SMT} at the International Maritime Organization: experiences with combining in-house corpora with out-of-domain corpora,2015,11,1,2,0.664334,33358,bruno pouliquen,Proceedings of the 18th Annual Conference of the {E}uropean Association for Machine Translation,0,"This paper presents a machine translation tool xe2x80x93 based on Moses xe2x80x93 developed for the International Maritime Organization (IMO) for the automatic translation of documents from Spanish, French, Russian and Arabic to/from English. The main challenge lies in the insufficient size of inhouse corpora (especially for Russian and Arabic). The United Nations (UN) granted IMO the right to use UN resources and we describe experiments and results we obtained with different translation model combination techniques. While BLEU results remain inconclusive for combinations, we also analyze user preferences for certain models (when choosing betweeen IMO only or combined with UN). The combined models are perceived by translators as being much better for general texts while IMO only models seem better for technical texts."
D15-1052,Human Evaluation of Grammatical Error Correction Systems,2015,21,15,2,0.747375,6016,roman grundkiewicz,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,The paper presents the results of the first large-scale human evaluation of automatic grammatical error correction (GEC) systems. Twelve participating systems and the unchanged input of the CoNLL-2014 shared task have been reassessed in a WMT-inspired human evaluation procedure. Methods introduced for the Workshop of Machine Translation evaluation campaigns have been adapted to GEC and extended where necessary. The produced rankings are used to evaluate standard metrics for grammatical error correction in terms of correlation with human judgment.
2015.eamt-1.28,{SMT} at the International Maritime Organization experiences with combining in-house corpus with more general corpus,2015,-1,-1,2,0.664334,33358,bruno pouliquen,Proceedings of the 18th Annual Conference of the European Association for Machine Translation,0,None
W14-1703,The {AMU} System in the {C}o{NLL}-2014 Shared Task: Grammatical Error Correction by Data-Intensive and Feature-Rich Statistical Machine Translation,2014,25,46,1,1,3523,marcin junczysdowmunt,Proceedings of the Eighteenth Conference on Computational Natural Language Learning: Shared Task,0,"Statistical machine translation toolkits like Moses have not been designed with grammatical error correction in mind. In order to achieve competitive results in this area, it is not enough to simply add more data. Optimization procedures need to be customized, task-specific features should be introduced. Only then can the decoder take advantage of relevant data. We demonstrate the validity of the above claims by combining web-scale language models and large-scale error-corrected texts with parameter tuning according to the task metric and correction-specific features. Our system achieves a result of 35.0% F0.5 on the blind CoNLL-2014 test set, ranking on third place. A similar system, equipped with identical models but without tuned parameters and specialized features, stagnates at 25.4%."
2014.eamt-1.44,{SMT} of {G}erman patents at {WIPO}: decompounding and verb structure pre-reordering,2014,8,0,1,1,3523,marcin junczysdowmunt,Proceedings of the 17th Annual conference of the European Association for Machine Translation,0,None
2013.mtsummit-user.7,Large-scale Multiple Language Translation Accelerator at the {U}nited {N}ations,2013,0,3,3,0.664334,33358,bruno pouliquen,Proceedings of Machine Translation Summit XIV: User track,0,None
2012.eamt-1.58,A Phrase Table without Phrases: Rank Encoding for Better Phrase Table Compression,2012,17,6,1,1,3523,marcin junczysdowmunt,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"This paper describes the first steps towards a minimum-size phrase table implementation to be used for phrase-based statistical machine translation. The focus lies on the size reduction of target language data in a phrase table. Rank Encoding (REnc), a novel method for the compression of word-aligned target language in phrase tables is presented. Combined with Huffman coding a relative size reduction of 56 percent for target phrase words and alignment data is achieved when compared to bare Huffman coding without R-Enc. In the context of the complete phrase table the size reduction is 22 percent."
