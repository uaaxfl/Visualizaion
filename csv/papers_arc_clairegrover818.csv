2020.louhi-1.4,Not a cute stroke: Analysis of Rule- and Neural Network-based Information Extraction Systems for Brain Radiology Reports,2020,-1,-1,3,0,18407,andreas grivas,Proceedings of the 11th International Workshop on Health Text Mining and Information Analysis,0,"We present an in-depth comparison of three clinical information extraction (IE) systems designed to perform entity recognition and negation detection on brain imaging reports: EdIE-R, a bespoke rule-based system, and two neural network models, EdIE-BiLSTM and EdIE-BERT, both multi-task learning models with a BiLSTM and BERT encoder respectively. We compare our models both on an in-sample and an out-of-sample dataset containing mentions of stroke findings and draw on our error analysis to suggest improvements for effective annotation when building clinical NLP models for a new domain. Our analysis finds that our rule-based system outperforms the neural models on both datasets and seems to generalise to the out-of-sample dataset. On the other hand, the neural models do not generalise negation to the out-of-sample dataset, despite metrics on the in-sample dataset suggesting otherwise."
2020.cmlc-1.4,Geoparsing the historical Gazetteers of {S}cotland: accurately computing location in mass digitised texts,2020,-1,-1,2,0,21817,rosa filgueira,Proceedings of the 8th Workshop on Challenges in the Management of Large Corpora,0,"This paper describes work in progress on devising automatic and parallel methods for geoparsing large digital historical textual data by combining the strengths of three natural language processing (NLP) tools, the Edinburgh Geoparser, spaCy and defoe, and employing different tokenisation and named entity recognition (NER) techniques. We apply these tools to a large collection of nineteenth century Scottish geographical dictionaries, and describe preliminary results obtained when processing this data."
L18-1483,Up-cycling Data for Natural Language Generation,2018,0,0,3,0,14826,amy isard,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
P16-3007,Improving Topic Model Clustering of Newspaper Comments for Summarisation,2016,18,4,2,1,827,clare llewellyn,Proceedings of the {ACL} 2016 Student Research Workshop,0,None
L16-1622,Homing in on {T}witter Users: Evaluating an Enhanced Geoparser for User Profile Locations,2016,18,3,3,1,826,beatrice alex,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Twitter-related studies often need to geo-locate Tweets or Twitter users, identifying their real-world geographic locations. As tweet-level geotagging remains rare, most prior work exploited tweet content, timezone and network information to inform geolocation, or else relied on off-the-shelf tools to geolocate users from location information in their user profiles. However, such user location metadata is not consistently structured, causing such tools to fail regularly, especially if a string contains multiple locations, or if locations are very fine-grained. We argue that user profile location (UPL) and tweet location need to be treated as distinct types of information from which differing inferences can be drawn. Here, we apply geoparsing to UPLs, and demonstrate how task performance can be improved by adapting our Edinburgh Geoparser, which was originally developed for processing English text. We present a detailed evaluation method and results, including inter-coder agreement. We demonstrate that the optimised geoparser can effectively extract and geo-reference multiple locations at different levels of granularity with an F1-score of around 0.90. We also illustrate how geoparsed UPLs can be exploited for international information trade studies and country-level sentiment analysis."
W14-4908,A Web-based Geo-resolution Annotation and Evaluation Tool,2014,8,2,3,1,826,beatrice alex,Proceedings of {LAW} {VIII} - The 8th Linguistic Annotation Workshop,0,"In this paper we present the Edinburgh Geo-annotator, a web-based annotation tool for the manual geo-resolution of location mentions in text using a gazetteer. The annotation tool has an interlinked text and map interface which lets annotators pick correct candidates within the gazetteer more easily. The geo-annotator can be used to correct the output of a geoparser or to create gold standard geo-resolution data. We include accompanying scoring software for geo-resolution evaluation."
W14-0617,A Gazetteer and Georeferencing for Historical {E}nglish Documents,2014,7,4,1,1,18408,claire grover,"Proceedings of the 8th Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities ({L}a{T}e{CH})",0,"We report on a newly available gazetteer of historical English place-names and describe how it was created from a recent digitisation of the Survey of English Place-Names, published by the English Place-Name Society (EPNS). The gazetteer resource is accessible via a number of routes, not currently as linked data but in formats that do provide connections between a number of different datasets. In particular, connections between the historical gazetteer and the Unlock 1 and GeoNames 2 gazetteer services have been established along with links to the Key to English Place-Names database 3 . The gazetteer is available via the Unlock API and in the final part of the paper we describe how the Edinburgh Geoparser, which forms the basis of Unlock Text, has been adapted to allow users to georeference historical texts."
llewellyn-etal-2014-using,Re-using an Argument Corpus to Aid in the Curation of Social Media Collections,2014,18,19,2,1,827,clare llewellyn,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This work investigates how automated methods can be used to classify social media text into argumentation types. In particular it is shown how supervised machine learning was used to annotate a Twitter dataset (London Riots) with argumentation classes. An investigation of issues arising from a natural inconsistency within social media data found that machine learning algorithms tend to over fit to the data because Twitter contains a lot of repetition in the form of retweets. It is also noted that when learning argumentation classes we must be aware that the classes will most likely be of very different sizes and this must be kept in mind when analysing the results. Encouraging results were found in adapting a model from one domain of Twitter data (London Riots) to another (OR2012). When adapting a model to another dataset the most useful feature was punctuation. It is probable that the nature of punctuation in Twitter language, the very specific use in links, indicates argumentation class."
W10-4125,Space characters in {C}hinese semi-structured texts,2010,9,0,2,0,45160,rongzhou shen,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-1804,Agile Corpus Annotation in Practice: An Overview of Manual and Automatic Annotation of {CV}s,2010,10,6,2,0,45372,bea alex,Proceedings of the Fourth Linguistic Annotation Workshop,0,"This paper describes work testing agile data annotation by moving away from the traditional, linear phases of corpus creation towards iterative ones and by recognizing the potential for sources of error occurring throughout the annotation process."
W10-0514,Labelling and Spatio-Temporal Grounding of News Events,2010,3,3,2,0,45372,bea alex,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Linguistics in a World of Social Media,0,This paper describes work in progress on labelling and spatio-temporal grounding of news events as part of a news analysis system that is under development.
S10-1074,{E}dinburgh-{LTG}: {T}emp{E}val-2 System Description,2010,9,30,1,1,18408,claire grover,Proceedings of the 5th International Workshop on Semantic Evaluation,0,We describe the Edinburgh information extraction system which we are currently adapting for analysis of newspaper text as part of the SYNC3 project. Our most recent focus is geospatial and temporal grounding of entities and it has been useful to participate in TempEval-2 to measure the performance of our system and to guide further development. We took part in Tasks A and B for English.
wang-grover-2008-learning,Learning the Species of Biomedical Named Entities from Annotated Corpora,2008,11,11,2,0,46478,xinglong wang,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In biomedical articles, terms with the same surface forms are often used to refer to different entities across a number of model organisms, in which case determining the species becomes crucial to term identification systems that ground terms to specific database identifiers. This paper describes a rule-based system that extracts Âspecies indicating wordsÂ, such as human or murine, which can be used to decide the species of the nearby entity terms, and a machine-learning species disambiguation system that was developed on manually species-annotated corpora. Performance of both systems were evaluated on gold-standard datasets, where the machine-learning system yielded better overall results."
grover-etal-2008-named,Named Entity Recognition for Digitised Historical Texts,2008,8,24,1,1,18408,claire grover,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,We describe and evaluate a prototype system for recognising person and place names in digitised records of British parliamentary proceedings from the late 17th and early 19th centuries. The output of an OCR engine is the input for our system and we describe certain issues and errors in this data and discuss the methods we have used to overcome the problems. We describe our rule-based named entity recognition system for person and place names which is implemented using the LT-XML2 and LT-TTT2 text processing tools. We discuss the annotation of a development and testing corpus and provide results of an evaluation of our system on the test corpus.
W07-1009,Recognising Nested Named Entities in Biomedical Text,2007,21,63,3,1,826,beatrice alex,"Biological, translational, and clinical language processing",0,"Although recent named entity (NE) annotation efforts involve the markup of nested entities, there has been limited focus on recognising such nested structures. This paper introduces and compares three techniques for modelling and recognising nested entities by means of a conventional sequence tagger. The methods are tested and evaluated on two biomedical data sets that contain entity nesting. All methods yield an improvement over the baseline tagger that is only trained on flat annotation."
W06-2703,Tools to Address the Interdependence between Tokenisation and Standoff Annotation,2006,17,15,1,1,18408,claire grover,Proceedings of the 5th Workshop on {NLP} and {XML} ({NLPXML}-2006): Multi-Dimensional Markup in Natural Language Processing,0,"In this paper we discuss technical issues arising from the interdependence between tokenisation and XML-based annotation tools, in particular those which use standoff annotation in the form of pointers to word tokens. It is common practice for an XML-based annotation tool to use word tokens as the target units for annotating such things as named entities because it provides appropriate units for stand-off annotation. Furthermore, these units can be easily selected, swept out or snapped to by the annotators and certain classes of annotation mistakes can be prevented by building a tool that does not permit selection of a substring which does not entirely span one or more XML elements. There is a downside to this method of annotation, however, in that it assumes that for any given data set, in whatever domain, the optimal tokenisation is known before any annotation is performed. If mistakes are made in the initial tokenisation and the word boundaries conflict with the annotators' desired actions, then either the annotation is inaccurate or expensive retokenisation and reannotation will be required. Here we describe the methods we have developed to address this problem. We also describe experiments which explore the effects of different granularities of tokenisation on NER tagger performance."
alex-etal-2006-impact,The Impact of Annotation on the Performance of Protein Tagging in Biomedical Text,2006,11,9,3,1,826,beatrice alex,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,In this paper we discuss five different corpora annotated forprotein names. We present several within- and cross-dataset proteintagging experiments showing that different annotation schemes severelyaffect the portability of statistical protein taggers. By means of adetailed error analysis we identify crucial annotation issues thatfuture annotation projects should take into careful consideration.
grover-tobin-2006-rule,Rule-Based Chunking and Reusability,2006,15,39,1,1,18408,claire grover,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,In this paper we discuss a rule-based approach to chunking implemented using the LT-XML2 and LT-TTT2 tools. We describe the tools and the pipeline and grammars that have been developed for the task of chunking. We show that our rule-based approach is easy to adapt to different chunking styles and that the mark-up of further linguistic information such as nominal and verbal heads can be added to the rules at little extra cost. We evaluate our chunker against the CoNLL 2000 data and discuss discrepancies between our output and the CoNLL mark-up as well as discrepancies within the CoNLL data itself. We contrast our results with the higher scores obtained using machine learning and argue that the portability and flexibility of our approach still make it a more practical solution.
W04-1907,The {HOLJ} Corpus. Supporting Summarisation of Legal Texts,2004,18,17,1,1,18408,claire grover,Proceedings of the 5th International Workshop on Linguistically Interpreted Corpora,0,"We describe an XML-encoded corpus of texts in the legal domain which was gathered for an automatic summarisation project. We describe two distinct layers of annotation: manual annotation of the rhetorical status of sentences and an entirely automatic annotation process incorporating a host of individual linguistic processors. The manual rhetorical status annotation has been developed as training and testing material for a summarisation system based on the work of Teufel and Moens, while the automatic layer of annotation encodes linguistic information as features for a machine learning approach to rhetorical status classification."
W04-1007,A Rhetorical Status Classifier for Legal Text Summarisation,2004,0,0,2,0,19587,ben hachey,Text Summarization Branches Out,0,None
W03-2406,Automatic Multi-Layer Corpus Annotation for Evaluation Question Answering Methods: {CBC}4{K}ids,2003,16,3,5,0,29355,jochen leidner,Proceedings of 4th International Workshop on Linguistically Interpreted Corpora ({LINC}-03) at {EACL} 2003,0,None
W03-0505,Summarising Legal Texts: Sentential Tense and Argumentative Roles,2003,16,25,1,1,18408,claire grover,Proceedings of the {HLT}-{NAACL} 03 Text Summarization Workshop,0,We report on the SUM project which applies automatic summarisation techniques to the legal domain. We pursue a methodology based on Teufel and Moens (2002) where sentences are classified according to their argumentative role. We describe some experiments with judgments of the House of Lords where we have performed automatic linguistic annotation of a small sample set in order to explore correlations between linguistic features and argumentative roles. We use state-of-the-art NLP techniques to perform the linguistic annotation using XML-based tools and a combination of rule-based and statistical methods. We focus here on the predictive capacity of tense and aspect features for a classifier.
N03-4007,Demonstration of the {CROSSMARC} System,2003,3,1,4,0,23295,vangelis karkaletsis,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"The EC-funded R&D project, CROSSMARC, is developing technology for extracting information from domain-specific web pages, employing language technology methods as well as machine learning methods in order to facilitate technology porting to new domains. CROSSMARC also employs localisation methodologies and user modelling techniques in order to provide the results of extraction in accordance with the user's personal preferences and constraints. The system's implementation is based on a multi-agent architecture, which ensures a clear separation of responsibilities and provides the system with clear interfaces and robust and intelligent information processing capabilities."
W02-1706,{XML}-based {NLP} Tools for Analysing and Annotating Medical Language,2002,17,13,1,1,18408,claire grover,COLING-02: The 2nd Workshop on NLP and XML (NLPXML-2002),0,"We describe the use of a suite of highly flexible XML-based NLP tools in a project for processing and interpreting text in the medical domain. The main aim of the paper is to demonstrate the central role that XML mark-up and XML NLP tools have played in the analysis process and to describe the resultant annotated corpus of MEDLINE abstracts. In addition to the XML tools, we have succeeded in integrating a variety of non-XML 'off the shelf' NLP tools into our pipelines, so that their output is added into the mark-up. We demonstrate the utility of the annotations that result in two ways. First, we investigate how they can be used to improve parse coverage of a hand-crafted grammar that generates logical forms. And second, we investigate how they contribute to automatic lexical semantic acquisition processes."
grover-etal-2002-multilingual,Multilingual {XML}-Based Named Entity Recognition for {E}-Retail Domains,2002,17,14,1,1,18408,claire grover,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"We describe the multilingual Named Entity Recognition and Classification (NERC) subpart of an e-retail product comparison system which is currently under development as part of the EU-funded project CROSSMARC. The system must be rapidly extensible, both to new languages and new domains. To achieve this aim we use XML as our common exchange format and the monolingual NERC components use a combination of rule-based and machine-learning techniques. It has been challenging to process web pages which contain heavily structured data where text is intermingled with HTML and other code. Our preliminary evaluation results demonstrate the viability of our approach."
P01-1034,{XML}-Based Data Preparation for Robust Deep Parsing,2001,16,20,1,1,18408,claire grover,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"We describe the use of XML tokenisation, tagging and mark-up tools to prepare a corpus for parsing. Our techniques are generally applicable but here we focus on parsing Medline abstracts with the ANLT wide-coverage grammar. Hand-crafted grammars inevitably lack coverage but many coverage failures are due to inadequacies of their lexicons. We describe a method of gaining a degree of robustness by interfacing POS tag information with the existing lexicon. We also show that XML tools provide a sophisticated approach to pre-processing, helping to ameliorate the 'messiness' in real language data and improve parse performance."
grover-etal-2000-lt,{LT} {TTT} - A Flexible Tokenisation Tool,2000,12,93,1,1,18408,claire grover,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"We describe LT TTT, a recently developed software system which provides tools to perform text tokenisation and mark-up. The system includes ready-made components to segment text into paragraphs, sentences, words and other kinds of token but, crucially, it also allows users to tailor rule-sets to produce mark-up appropriate for particular applications. We present three case studies of our use of LT TTT: named-entity recognition (MUC-7), citation recognition and mark-up and the preparation of a corpus in the medical domain. We conclude with a discussion of the use of browsers to visualise marked-up text."
E99-1001,Named Entity Recognition without Gazetteers,1999,9,352,3,0.666667,53577,andrei mikheev,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"It is often claimed that Named Entity recognition systems need extensive gazetteers---lists of names of people, organisations, locations, and other named entities. Indeed, the compilation of such gazetteers is sometimes mentioned as a bottleneck in the design of Named Entity recognition systems.We report on a Named Entity recognition system which combines rule-based grammars with statistical (maximum entropy) models. We report on the system's performance with gazetteers of different types and different sizes, using test material from the MUC-7 competition. We show that, for the text type and task of this competition, it is sufficient to use relatively small gazetteers of well-known names, rather than large gazetteers of low-frequency names. We conclude with observations about the domain independence of the competition and of our experiments."
M98-1021,Description of the {LTG} System Used for {MUC}-7,1998,5,151,2,0.666667,53577,andrei mikheev,"Seventh Message Understanding Conference ({MUC}-7): Proceedings of a Conference Held in Fairfax, Virginia, {A}pril 29 - May 1, 1998",0,"The basic building blocks in our muc system are reusable text handling tools which we have been developing and using for a number of years at the Language Technology Group. They are modular tools with stream input/output; each tool does a very speci c job, but can be combined with other tools in a unix pipeline. Di erent combinations of the same tools can thus be used in a pipeline for completing di erent tasks."
E95-1035,Algorithms for Analysing the Temporal Structure of Discourse,1995,11,56,3,0,47991,janet hitzeman,Seventh Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We describe a method for analysing the temporal structure of a discourse which takes into account the effects of tense, aspect, temporal adverbials and rhetorical structure and which minimises unnecessary ambiguity in the temporal structure. It is part of a discourse grammar implemented in Carpenter's ALE formalism. The method for building up the temporal structure of the discourse combines constraints and prefernces: we use constraints to reduce the number of possible structures, exploiting the HPSG type hierarchy and unification for this purpose; and we apply preferences to choose between the remaining options using a temporal centering mechanism. We end by recommending that an underspecified representation of the structure using these techniques be used to avoid generating the temporal/rhetorical structure until higher-level information can be used to disambiguate."
P94-1003,Priority Union and Generalization in Discourse Grammars,1994,11,25,1,1,18408,claire grover,32nd Annual Meeting of the Association for Computational Linguistics,1,"We describe an implementation in Carpenter's typed feature formalism, ALE, of a discourse grammar of the kind proposed by Scha, Polanyi, et al. We examine their method for resolving parallelism-dependent anaphora and show that there is a coherent feature-structural rendition of this type of grammar which uses the operations of priority union and generalization. We describe an augmentation of the ALE system to encompass these operations and we show that an appropriate choice of definition for priority union gives the desired multiple output for examples of VP-ellipsis which exhibit a strict/sloppy ambiguity."
E89-1035,The Syntactic Regularity of {E}nglish Noun Phrases,1989,6,13,2,0,57811,lita taylor,Fourth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Approximately, 10,000 naturally occurring noun phrases taken from the LOB corpus were used firstly, to evaluate the NP component of the Alvey ANLT grammar (Grover et al., 1987, 1989) and secondly, to retest Sampson's (1987a) claim that this data provide evidence for the lack of a clear-cut distinction between grammatical and 'deviant' examples. The examples were sorted and classified on the basis of the lexical and syntactic analysis undertaken as part of the LOB corpus project (Sampson, 1987b). Tokens of each resulting type were parsed using the ANLT grammar and the results analysed to determine the success rate of the parses and the generality of the rules employed."
C88-1012,Software Support for Practical Grammar Development,1988,20,19,4,0,57635,bran boguraev,{C}oling {B}udapest 1988 Volume 1: {I}nternational {C}onference on {C}omputational {L}inguistics,0,"Even though progress in theoretical linguistics does not necessarily rely on the construction of working programs, a large proportion of current research in syntactic theory is facilitated by suitable computational tools. However, when natural language processing applications seek to draw on the results from new developments in theories of grammar, not only the nature of the tools has to change, but they face the challenge of reconciling the seemingly contradictory requirements of notational perspicuity and efficiency of performance. In this paper, we present a comparison and an evaluation of a number of software systems for grammar development, and argue that they are inadequate as practical tools for building wide-coverage grammars. We discuss a number of factors characteristic of this task, demonstrate how they influence the design of a suitable software environment, and describe the implementation of a system which has supported efficient development of a large computational grammar of English."
P87-1027,The Derivation of a Grammatically Indexed Lexicon from the Longman Dictionary of Contemporary {E}nglish,1987,8,64,5,0,57635,bran boguraev,25th Annual Meeting of the Association for Computational Linguistics,1,"We describe a methodology and associated software system for the construction of a large lexicon from an existing machine-readable (published) dictionary. The lexicon serves as a component of an English morphological and syntactic analyser and contains entries with grammatical definitions compatible with the word and sentence grammar employed by the analyser. We describe a software system with two integrated components. One of these is capable of extracting syntactically rich, theory-neutral lexical templates from a suitable machine-readable source. The second supports interactive and semi-automatic generation and testing of target lexical entries in order to derive a sizeable, accurate and consistent lexicon from the source dictionary which contains partial (and occasionally in-accurate) information. Finally, we evaluate the utility of the Longman Dictionary of Contemporary English as a suitable source dictionary for the target lexicon."
