2006.eamt-1.2,W02-1502,0,0.141929,"Semantics representations (MRS; Copestake, Flickinger, Sag, & Pollard, 2003) are used as interface structures. However, unlike the LOGON architecture, which uses different grammars based on different formalisms and linguistic theories, we see it as an advantage to use the same grammatical framework for both source and target languages, as this means that the same parser and generator can be used throughout. We use HPSG-like Typed Feature Structure Grammars as our framework because of the availability of the LKB workbench (Copestake, 2001) and the store of type definitions known as the Matrix (Bender, Flickinger, & Oepen, 2002), made available by the delph-in collaboration (Bond, Oepen, Siegel, Copestake, & Flickinger, 2005). In addition, we see it as desirable that grammars are designed on similar principles, so that solutions to translation problems can be coordinated between languages and implemented using a shared inventory of types. This makes the system more homogenous and facilitates the addition of new languages. We also believe that the development of practical applications benefits from the existence of a core system that provides a library of solutions to the translation problems that are likely to be en"
2006.eamt-1.2,1995.tmi-1.2,0,0.644555,"d as the core of a small MT system. Copestake, Flickinger, Malouf, Riehemann, & Sag (1995) describe how MRS can be used for translation. They suggest a design that is based on semantic transfer using MRS. The transfer component works on MRS to produce output that the target grammar can accept. It is possible that the transfer component can output more than one form, some of which may be unacceptable by the generator. When several forms are output they will be ordered by a control mechanism that is distinct from both the transfer component and the generator. The transfer component suggested by Copestake et al. (1995) is based on setting up symmetric and bidirectional transfer equivalences between each pair of languages. Their suggestion also allows interlingual predicates that are common for all languages such as negation. A large-scale project where semantic transfer with MRS is used is LOGON, which focus on translation between Norwegian and English (Oepen et al., 2004). The main architecture is: analysis of Norwegian to MRS using the Norwegian LFG grammar NorGram (Dyvik, 1999), MRS transfer as described above, and generation to English using the ERG. The LOGON system is unidirectional. It only translate"
2006.eamt-1.2,J94-4004,0,0.333457,"h the delph-in resources. Section 4 describes BiTSE, the bilingual grammar that is the core of our MT system. In section 5 we explain our treatment of several types of divergences. Section 6 contains a discussion on the merits and limits of our approach and section 7 contains the conclusion. 2 Verb Frame Divergences As part of this study we have investigated verb frame divergences (VFDs). A verb frame consists of a verb and its arguments. A verb frame divergence is when two verb frames with the same meaning have different structures. Some examples of this, based on the categories suggested by Dorr (1994), will be presented here. All examples in this article are taken from the Europarl corpus (Koehn, 2005). Some of the examples shown contain more than one type of divergence. (1) In that case, the matter turns out to be a national problem after all. Till sist kommer a¨ndock a¨rendet att visa sig vara ett nationellt problem. (2) This appears to be the case with the events which Mr Lomas reports in his question. Det tycks n¨amligen vara fallet med de fakta som herr Lomas f¨or p˚ a tal i sin fr˚ aga. (3) But that is precisely why we first need a clear strategy. Just d¨arf¨or a¨r till att b¨orja me"
2006.eamt-1.2,2005.mtsummit-papers.11,0,0.00562172,"system. In section 5 we explain our treatment of several types of divergences. Section 6 contains a discussion on the merits and limits of our approach and section 7 contains the conclusion. 2 Verb Frame Divergences As part of this study we have investigated verb frame divergences (VFDs). A verb frame consists of a verb and its arguments. A verb frame divergence is when two verb frames with the same meaning have different structures. Some examples of this, based on the categories suggested by Dorr (1994), will be presented here. All examples in this article are taken from the Europarl corpus (Koehn, 2005). Some of the examples shown contain more than one type of divergence. (1) In that case, the matter turns out to be a national problem after all. Till sist kommer a¨ndock a¨rendet att visa sig vara ett nationellt problem. (2) This appears to be the case with the events which Mr Lomas reports in his question. Det tycks n¨amligen vara fallet med de fakta som herr Lomas f¨or p˚ a tal i sin fr˚ aga. (3) But that is precisely why we first need a clear strategy. Just d¨arf¨or a¨r till att b¨orja med en klar strategi n¨odv¨andig h¨ar. In (1) “turns out to be” corresponds to “visa sig vara” (“show its"
2006.eamt-1.2,1991.mtsummit-papers.9,0,0.0584048,"interlinguas and the merits and drawbacks of interlingual approaches as compared to transfer approaches (e.g. Boitet, 1988; Nirenburg & Goldman, 1990). Basically a MRS relation is a place-holder for a concept with known argument structure which is associated with one or more linguistic expressions in a lexicon. Semantic relations such as hyponomy and antonomy, and even some semantic decomposition, could be added, but is not part of the current setup, though hyponymy could be dealt with within the type system. Domain knowledge, as used in knowledgebased interlingual MT such as the KANT system (Mitamura, Nyberg, & Carbonell, 1991), is also not handled. Similarities between two (or more) languages can be encoded in formal grammars in different ways. The Rosetta project (Rosetta, 1994) explored the idea of isomorphic grammars. Our framework does not require grammars to be isomorphic; the important thing is that they produce a common MRS for sentences that are translations of one another. In addition, the grammars are actually implemented as one bilingual (or multilingual) grammar, allowing types to be shared between languages. 3.1 MT using sources DELPH-IN reThere have been previous suggestions for MT using the delph-in"
2006.eamt-1.2,2004.tmi-1.2,0,0.0387312,"Missing"
2006.eamt-1.2,2005.mtsummit-osmtw.3,0,\N,Missing
2008.eamt-1.25,E03-1076,0,0.777473,"other suggestions of how to split compounds, but they are often only evaluated against a gold standard, not on a translation task. Alfonseca et al. [6] suggested a language independent supervised learning method, which needs a corpus of annotated compounds. They also showed that the training corpus can be of another language than the corpus to be split. For Swedish, Sj¨ obergh and Kann [7] suggested several ways of choosing the correct splitting points of compounds that were split using a method based on word lists. The corpus-based language independent compound splitting method suggested in [1] was shown to be useful for PBSMT from and into German. In this work we investigate if a similar empirical method is useful for translation from and into Swedish. In addition we investigate the effects of marking compound parts [3], compared to the more commonly used strategy where no marking of compound parts is used. We also present a novel POS- and corpus-based merging algorithm for compounds. 2 Swedish Compounds Compounds in Swedish are normally formed by joining words, without any spaces or other word boundaries. Compound parts can have special compound forms, created by addition of lette"
2008.eamt-1.25,W08-0317,1,0.919525,"glish. Koehn and Knight [1] suggested an empirical algorithm for splitting compounds, which was successfully applied to German-to-English translation. The same method was used by Popovi´c et al. [2] for translation in both directions between English and German. In addition, they merged compounds in a postprocessing step for translation into German. Stymne [3] tried a number of variations of the algorithm for translation in both translation directions. In both studies translation quality was improved. Compound parts are usually treated as ordinary words in the training data, e.g. in [1, 2]. In [3, 4], however, compound parts were marked with a symbol, to separate them from normal words, resulting in improved translation quality compared to an unsplit baseline. Virpioja et al.[5] used an unsupervised algorithm for morphological splitting and merging, where both compounds and other words were split into stems and affixes, for translation between Swedish and other Scandinavian languages. 182 12th EAMT conference, 22-23 September 2008, Hamburg, Gemany Table 1. Compound forms in Swedish Type None Suffixes Additions -s -t Truncations -e -a Combinations -a/-s -a/-e -a/-u -a/-o -e/-a -e/-s -el/-l"
2008.eamt-1.25,2007.mtsummit-papers.65,0,0.195303,"´c et al. [2] for translation in both directions between English and German. In addition, they merged compounds in a postprocessing step for translation into German. Stymne [3] tried a number of variations of the algorithm for translation in both translation directions. In both studies translation quality was improved. Compound parts are usually treated as ordinary words in the training data, e.g. in [1, 2]. In [3, 4], however, compound parts were marked with a symbol, to separate them from normal words, resulting in improved translation quality compared to an unsplit baseline. Virpioja et al.[5] used an unsupervised algorithm for morphological splitting and merging, where both compounds and other words were split into stems and affixes, for translation between Swedish and other Scandinavian languages. 182 12th EAMT conference, 22-23 September 2008, Hamburg, Gemany Table 1. Compound forms in Swedish Type None Suffixes Additions -s -t Truncations -e -a Combinations -a/-s -a/-e -a/-u -a/-o -e/-a -e/-s -el/-la -la/-el -ra/-er Example riskkapital (risk + kapital) risk capital frihetsl¨ angtan (frihet + l¨ angtan) longing for peace pojkv¨ an (pojke + v¨ an) boyfriend arbetsgrupp (arbete +"
2008.eamt-1.25,P08-2064,0,0.350859,"l -ra/-er Example riskkapital (risk + kapital) risk capital frihetsl¨ angtan (frihet + l¨ angtan) longing for peace pojkv¨ an (pojke + v¨ an) boyfriend arbetsgrupp (arbete + grupp) working group All but the last part of a word were marked with a symbol, which was used in the merging step. They did not get improved translations when measured automatically, but saw other advantages such as a reduction of out-of-vocabulary words. There have been many other suggestions of how to split compounds, but they are often only evaluated against a gold standard, not on a translation task. Alfonseca et al. [6] suggested a language independent supervised learning method, which needs a corpus of annotated compounds. They also showed that the training corpus can be of another language than the corpus to be split. For Swedish, Sj¨ obergh and Kann [7] suggested several ways of choosing the correct splitting points of compounds that were split using a method based on word lists. The corpus-based language independent compound splitting method suggested in [1] was shown to be useful for PBSMT from and into German. In this work we investigate if a similar empirical method is useful for translation from and"
2008.eamt-1.25,sjobergh-kann-2004-finding,0,0.717156,"th a symbol, which was used in the merging step. They did not get improved translations when measured automatically, but saw other advantages such as a reduction of out-of-vocabulary words. There have been many other suggestions of how to split compounds, but they are often only evaluated against a gold standard, not on a translation task. Alfonseca et al. [6] suggested a language independent supervised learning method, which needs a corpus of annotated compounds. They also showed that the training corpus can be of another language than the corpus to be split. For Swedish, Sj¨ obergh and Kann [7] suggested several ways of choosing the correct splitting points of compounds that were split using a method based on word lists. The corpus-based language independent compound splitting method suggested in [1] was shown to be useful for PBSMT from and into German. In this work we investigate if a similar empirical method is useful for translation from and into Swedish. In addition we investigate the effects of marking compound parts [3], compared to the more commonly used strategy where no marking of compound parts is used. We also present a novel POS- and corpus-based merging algorithm for c"
2008.eamt-1.25,P07-2045,0,0.0196992,"Missing"
2008.eamt-1.25,P03-1021,0,0.0330809,"an easily be understood by a human, even if the form is wrong. The most common error is a left out addition of -s. 3 System Description The translation system we use is a factored phrase-based SMT system, with part-of-speech as an additional output factor. We use TreeTagger [11] to tag the English texts and Granska tagger [10] to tag the Swedish texts. We use two sequence models, produced by SRILM [12], a 5-gram language model on surface form and a 7-gram model on part-of-speech. For training and decoding we use the Moses toolkit [13]. We tune feature weights using minimum error rate training [14], that optimizes the Neva metric [15]. A pre-processing step is performed on the Swedish side where compounds are split. Thus we train the system on English and modified Swedish. Compounds are merged after translation into Swedish and during tuning. 185 12th EAMT conference, 22-23 September 2008, Hamburg, Gemany Table 2. Number of tokens and types for the training corpus System Tokens Types Baseline 13603062 182000 Swedish Unmarked 14401784 100492 Marked 14401784 107047 English 15043321 67044 We train three systems for this study, a baseline system without splitting, and two systems with split"
2008.eamt-1.25,2005.mtsummit-papers.11,0,0.0336769,"Missing"
2008.eamt-1.25,J96-2004,0,0.0413805,"has 500 sentences. 4 Evaluation of Compound Splitting We use two manually created gold standards to evaluate compound splitting. The gold standard corpus consists of the first 5395 words (245 sentences) from the test set. 4.1 Gold Standards For the first gold standard all compounds2 in the gold standard corpus were annotated. This standard was prepared by two human judges who are native speakers of Swedish. To investigate the difficulty of the task we calculated agreement as suggested in [6], as the percentage of agreement in classification as compounds or noncompounds (CCA), the Kappa score [17] obtained from CCA and the percentage of words for which the suggested decomposition was identical (DA). Since we evaluate on running text, which has a very large percentage of non-compound words, the agreement could be expected to be high. Therefore we also measured agreement on only those words that are 12 characters or longer, to have a more 2 A word is considered to be a compound if it has several parts which all are semantically meaningful with respect to the full compound and can be used as stand alone words in some form. 186 12th EAMT conference, 22-23 September 2008, Hamburg, Gemany Ta"
2008.eamt-1.25,P02-1040,0,0.074504,"easures and by human error analysis, which focus on out-of-vocabulary words and translation of compounds. 187 12th EAMT conference, 22-23 September 2008, Hamburg, Gemany Table 5. Results for translation from Swedish to English System Meteor Baseline 55.47 Unmarked 55.82 Marked 55.78 5.1 Bleu Neva 29.97 34.08 29.89 34.08 29.85 34.05 Table 6. Results for translation from English to Swedish NIST 7.3127 7.3470 7.2933 System Meteor Baseline 57.86 Unmarked 58.43 Marked 58.31 Bleu 21.63 22.12 21.92 Neva NIST 26.53 6.1085 26.99 6.1430 26.81 6.2025 Automatic Metrics We use four automatic metrics, Bleu [18], NIST [19], Neva [15] and Meteor3 [20]. Case sensitive versions of the metrics are used. The result for translation from Swedish can be seen in Table 5. The unmarked system is slightly better than the baseline on Meteor and NIST, but worse on Bleu. The marked system is worse than the unmarked on all metrics, and only better than the baseline on Meteor. Table 6 shows the results for translation into Swedish. In this direction the differences between the scores are bigger and both split systems beat the baseline on all metrics. The unmarked system is better than the marked system on all metrics"
2008.eamt-1.25,W07-0734,0,0.0314317,"ich focus on out-of-vocabulary words and translation of compounds. 187 12th EAMT conference, 22-23 September 2008, Hamburg, Gemany Table 5. Results for translation from Swedish to English System Meteor Baseline 55.47 Unmarked 55.82 Marked 55.78 5.1 Bleu Neva 29.97 34.08 29.89 34.08 29.85 34.05 Table 6. Results for translation from English to Swedish NIST 7.3127 7.3470 7.2933 System Meteor Baseline 57.86 Unmarked 58.43 Marked 58.31 Bleu 21.63 22.12 21.92 Neva NIST 26.53 6.1085 26.99 6.1430 26.81 6.2025 Automatic Metrics We use four automatic metrics, Bleu [18], NIST [19], Neva [15] and Meteor3 [20]. Case sensitive versions of the metrics are used. The result for translation from Swedish can be seen in Table 5. The unmarked system is slightly better than the baseline on Meteor and NIST, but worse on Bleu. The marked system is worse than the unmarked on all metrics, and only better than the baseline on Meteor. Table 6 shows the results for translation into Swedish. In this direction the differences between the scores are bigger and both split systems beat the baseline on all metrics. The unmarked system is better than the marked system on all metrics except NIST. 5.2 Out-of-Vocabulary Wor"
2011.eamt-1.39,P05-1066,0,0.298466,"Missing"
2011.eamt-1.39,2010.iwslt-papers.6,0,0.0257368,"Missing"
2011.eamt-1.39,W09-0420,0,0.0319434,"Missing"
2011.eamt-1.39,H05-1085,0,0.0685687,"Missing"
2011.eamt-1.39,2005.mtsummit-papers.11,0,0.0141863,"s in the source, target, or both sides can be separated by gaps that have to be filled by other phrases at translation time. In the experiments we allowed up to four gaps per phrase pair. Moses, and most other phrase-based decoders can only use contiguous phrases. We use a 3-gram language model in Matrax, and a 5-gram model in Moses. In some experiments, we also used an additional sequence model based on part-of-speech. The sequence models were trained using the SRILM toolkit (Stolcke, 2002). To train the decoder we used two different corpora, Europarl, proceedings of the European Parliament (Koehn, 2005), and an automotive corpus, collected from translation memory data. To reduce training times we did not use all data from Europarl. For the Italian-Danish experiment we randomly selected the sentences to use, and for English-Swedish we used version 2 of Europarl. The types and sizes of corpora used in the experiments are shown in Table 5. For English-Danish, the first experiment is repeated from Stymne (2009b), and in the second experiment a POS sequence model and compound processing as described in Stymne and Holmqvist (2008) were added. In all cases the number of words is higher in the sourc"
2011.eamt-1.39,C00-2162,0,0.661231,"Missing"
2011.eamt-1.39,P02-1040,0,0.0814813,"d 1000 sentences for translation into Danish, 500 sentences with Moses, and 2000 sentences otherwise. We trained systems with source side processing, which will be called DEF-proc for translation into Danish, and DEF-proc1, and DEF-proc2, for the two different strategies for translation into Swedish and Norwegian. For English–Swedish Europarl we also trained a system with target side processing, which is called Target-proc. We compare all results to baseline systems that do not use any transformations. 5.1 Results Table 6 shows the results of the experiments, on the two standard metrics Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) with one reference translation. Significance was tested using approximate randomization (Riezler and Maxwell, 2005), with α < 0.05. Overall the results are much higher on the automotive corpus, than on Europarl, which is expected since that corpus is more homogenous, and has shorter sentences. For English-Danish translation we see a large improvement of 5.44 Bleu points in the first experiment. In the second experiment, where we added a POS-sequence model and compound processing, the baseline is significantly better than the baseline of the first experiment. Again,"
2011.eamt-1.39,W05-0908,0,0.0192523,"ocessing, which will be called DEF-proc for translation into Danish, and DEF-proc1, and DEF-proc2, for the two different strategies for translation into Swedish and Norwegian. For English–Swedish Europarl we also trained a system with target side processing, which is called Target-proc. We compare all results to baseline systems that do not use any transformations. 5.1 Results Table 6 shows the results of the experiments, on the two standard metrics Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) with one reference translation. Significance was tested using approximate randomization (Riezler and Maxwell, 2005), with α < 0.05. Overall the results are much higher on the automotive corpus, than on Europarl, which is expected since that corpus is more homogenous, and has shorter sentences. For English-Danish translation we see a large improvement of 5.44 Bleu points in the first experiment. In the second experiment, where we added a POS-sequence model and compound processing, the baseline is significantly better than the baseline of the first experiment. Again, definite processing gives an improvement, of 2.08 Bleu points, but it is smaller than in the first case, and the scores with definite processin"
2011.eamt-1.39,H05-1095,0,0.0238701,"ng step to remove them. (7) grundvalen a¨ r det sv˚ara beslutet 293 DEF grundvalen a¨ r det sv˚ara beslutet (the) basis-DEF is the hard decision-DEF The added definite articles are separate tokens, DEF, which differ in surface form from the normal Swedish definite article, since we need to be able to identify them, in order to remove them in the postprocessing step. The tokens are added in NPs without pre-modifiers, where the head noun is in definite form. After translation, the DEF tokens are removed in the translation output. 5 Experiments We used two standard phrase-based decoders, Matrax (Simard et al., 2005) and Moses (Koehn et al., 2007). Matrax allows noncontiguous bi-phrases, such as jeopardize – bringe . . . i fare (bring . . . into danger) for English-Danish, where words in the source, target, or both sides can be separated by gaps that have to be filled by other phrases at translation time. In the experiments we allowed up to four gaps per phrase pair. Moses, and most other phrase-based decoders can only use contiguous phrases. We use a 3-gram language model in Matrax, and a 5-gram model in Moses. In some experiments, we also used an additional sequence model based on part-of-speech. The se"
2011.eamt-1.39,2008.eamt-1.25,1,0.81462,"r we used two different corpora, Europarl, proceedings of the European Parliament (Koehn, 2005), and an automotive corpus, collected from translation memory data. To reduce training times we did not use all data from Europarl. For the Italian-Danish experiment we randomly selected the sentences to use, and for English-Swedish we used version 2 of Europarl. The types and sizes of corpora used in the experiments are shown in Table 5. For English-Danish, the first experiment is repeated from Stymne (2009b), and in the second experiment a POS sequence model and compound processing as described in Stymne and Holmqvist (2008) were added. In all cases the number of words is higher in the source language than in the target language, but, as shown in the second last column of Table 5, the number of words is reduced and somewhat closer to the number of target words, after the source side definiteness processing. For test we used 1000 sentences, and for parameter optimization we used 1000 sentences for translation into Danish, 500 sentences with Moses, and 2000 sentences otherwise. We trained systems with source side processing, which will be called DEF-proc for translation into Danish, and DEF-proc1, and DEF-proc2, fo"
2011.eamt-1.39,E09-3008,1,0.93433,"nguage. One such issue is the unusual realization of definite noun phrases in Scandinavian languages. Definiteness can be expressed in two ways in Scandinavian languages, either by a definite article or by a suffix on the head noun. This is problematic for translation from languages that only use definite articles, such as English or Italian, leading to problems such as wrong noun forms and spurious definite articles in the translation output. It has previously been shown that definite noun phrases can successfully be handled by a preprocessing step for translation between English and Danish (Stymne, 2009b). In this study, source language noun phrases were transformed to be similar in structure to target language NPs. In this paper we show that preprocessing of definiteness c 2011 European Association for Machine Translation. also can be successful for translation from English into Swedish and Norwegian, and from Italian to Danish, using the same basic strategy as in Stymne (2009b). However, some small careful modifications to the original English-Danish preprocessing strategy were necessary. 2 Definiteness In the Scandinavian languages there are two mechanisms for expressing definiteness, by"
2011.eamt-1.39,C04-1073,0,0.272727,"Missing"
2011.eamt-1.39,2007.iwslt-1.3,0,0.308016,"lections for translation into German, and recreated it in a postprocessing step, however, with negative results. In these three studies, some information is removed before the translation process. It seems, however, that care has to be taken not to remove too much information. All these approaches work on different levels of linguistic representations, and require different 291 linguistic tools. The lowest possible level of representation is surface form, which does not require any linguistic processing, and is used in Samuelsson (2006). Methods based on part-ofspeech (Stymne, 2009b), chunks (Zhang et al., 2007), or parse trees (Collins et al., 2005) are more commonly used. Some approaches also use morphological analyzers (Goldwater and McClosky, 2005). While there is more information on the higher level of linguistic representations, tools tend to make more errors, the more complex they are. There is thus a trade-off between the expressivity and generality of the representation used, and its correctness using automatic tools. 4 Preprocessing Strategies Our main strategy used to improve the translation with respect to definiteness is to transform definite NPs in the source language, to make them simi"
2011.eamt-1.39,P07-2045,0,\N,Missing
2020.calcs-1.4,2014.lilt-9.1,0,0.029642,". 139). This phenomenon is clearly observable in social media not only used by Indonesians but also across languages (C´ardenas-Claros and Isharyanti, 2009; Shafie and Nayan, 2013). Code-mixed text is a challenge for the computational linguistic community, where work based on social media text (Chakma and Das, 2016; Barman et al., 2014) is common. This poses a challenge because most models such as word-embedding models assume the training data is monolingual. In this paper, we focus on code-mixed Indonesian–English text. Code-mixing has also been referred to as intrasentential code-switching (Hoffmann, 2014), i.e. the two or more languages are mixed within sentences, not only between sentences. In code-mixed sentences, depending on the language, the word in L1 (e.g. Indonesian) is usually not only replaced with its translation in L2 (e.g. English), but can also be merged with affixes of the L1 (e.g. Indonesian). For instance ”Kita perlu revise documentnya” (ID: Kita perlu memperbaiki dokumennya; EN: We need to revise the document). In the example, the English word ”revise” is used instead of the Indonesian word ”memperbaiki” and the English word ”document” is merged with the Indonesian suffix ”-n"
2020.calcs-1.4,P15-1162,0,0.0758821,"Missing"
2020.calcs-1.4,P07-2045,0,0.00539725,"pt which based on the pycontractions library2 . Several Indonesian sentences in the corpus were formed in an informal manner. Since this study only concerns formal Indonesian, formalization of the informal sentences was done during the pre-processing. To formalize the informal sentence, we are using the Colloquial Indonesian Lexicon3 as well as a lexical normalization method from the study by Barik et al. (2019). The lexicon and the normalization method are sufficient for mapping the informal words and affixes back to the standard form. We then lower-cased all data. For English we used Moses (Koehn et al., 2007) to tokenize the data, and for Indonesian, the InaNLP toolkit (Purwarianti et al., 2016). To make sure that the character replacement in Indonesian is consistent with the English Corpus (e.g. apostrophe (’) → &apos; ), the tokenized corpus was re-tokenized using Moses with the English language as an option. The sentences were not lemmatized because affixes are part of the Indonesian code-mixed pattern. For synthesizing our code-mixed corpus, and for some of the methods for creating cross-lingual embeddings, as well as for the cleaning described above, we used the bilingual Indonesian–English l"
2020.calcs-1.4,P18-1073,0,0.0184562,"r sentiment classifier, and the results of the evaluation. We first train monolingual embeddings for each language based on each monolingual side of the parallel corpus using FastText5 (Joulin et al., 2016) with default configuration. The model chosen to create word embeddings is the skipgram model (Mikolov et al., 2013).6 The code-mixed word embeddings were created in the same way as the monolingual embeddings, but from the synthetic code-mixed corpus using the same tool (fastText) and configuration as for the monolingual embeddings.7 To create cross-lingual word embeddings, we used Vecmap8 (Artetxe et al., 2018) to combine the Indonesian and English monolingual embeddings into a shared space. We used four different modes provided by Vecmap: supervised, semi-supervised, identical, and unsupervised, with the default configuration. The supervised mode, the semisupervised mode, and the identical mode are trying to learn how to map two monolingual embeddings to a shared space using seed dictionaries. The difference is on how each mode creates seed dictionaries. The supervised mode uses the full bilingual lexicon of 96,518 word pairs to create a seed dictionary. The semi-supervised mode is intended to be u"
2020.calcs-1.4,D19-5554,0,0.200438,"corpus, there have been multiple attempts such as Turkish–German Code-Switching Corpus (C¸etino˘glu, 2016), Arabic– Moroccan Darija Code-Switched Corpus (Samih and Maier, 2016), Hindi–English Code-Mixed Corpus (Vijay et al., 2018), and English–Spanish Code-Switching Twitter Corpus (Vilares et al., 2016). Yet, English–Indonesian Code-Mixed Corpus does not exist. Most of the codemixed corpora are created by fetching social media information such as Tweets. There are several studies that precede our study in codemixed text synthesis. Some of the recent approach use neural networks. Winata et al. (2019) create a sequence-tosequence model with a copy mechanism which learn how to combine sentences from parallel corpora to generate codemixed text. Chang et al. (2019) utilize a generative adversarial network to generate code-mixed sentences. One early study which synthesizes code-mixed sentences is the work by Wick et al. (2016). They present a method for creating artificially code-switched text across many languages, which they apply to five languages. They use an algorithm where they randomly sample words to be replaced. For each sampled word, they pick one of the concepts, or word senses, pos"
2020.calcs-1.4,W14-3902,0,0.0331822,"erring, 2003; Kramarae, 1999; Poppi, 2014). To prepare Indonesians to go global, the English language has been taught to Indonesian students from elementary school. This exposure to the English language instigates frequent Indonesian–English code-mixing in Indonesia (Brown, 2000, p. 139). This phenomenon is clearly observable in social media not only used by Indonesians but also across languages (C´ardenas-Claros and Isharyanti, 2009; Shafie and Nayan, 2013). Code-mixed text is a challenge for the computational linguistic community, where work based on social media text (Chakma and Das, 2016; Barman et al., 2014) is common. This poses a challenge because most models such as word-embedding models assume the training data is monolingual. In this paper, we focus on code-mixed Indonesian–English text. Code-mixing has also been referred to as intrasentential code-switching (Hoffmann, 2014), i.e. the two or more languages are mixed within sentences, not only between sentences. In code-mixed sentences, depending on the language, the word in L1 (e.g. Indonesian) is usually not only replaced with its translation in L2 (e.g. English), but can also be merged with affixes of the L1 (e.g. Indonesian). For instance"
2020.calcs-1.4,L16-1667,0,0.0391903,"Missing"
2020.calcs-1.4,2012.eamt-1.60,0,0.0170108,"e concerned with the formal variant, since the formal variant has smaller word variation than the informal variant. Formalizing the corpus helps reduce the Out-of-Vocabulary (OOV) when we do a downstream task such as sentiment analysis. Furthermore, the sentiment analysis corpus (Saputri et al., 2018) we use mostly contains formal Indonesian. However, all methods used would be equally applicable to informal Indonesian. 4. Data and Pre-Processing We use multiple English–Indonesian sentence-aligned corpora from several genres: The Open Subtitle 2018 Corpus (Lison et al., 2018), TED 2018 Corpus (Cettolo et al., 2012), and GlobalVoice Corpus (Tiedemann, 2012), all taken from the OPUS collection (Tiedemann, 2012). These corpora are cleaned and pre-processed. Table 1 gives an overview of the size of the corpus before and after preprocessing. Open Subtitle 2018 dominates the corpus. The open subtitle corpus contains a lot of non-letter characters (e.g. ¶∗#) and formatting (e.g. {cHFFFFFF}). To clean this, we adjusted the PrepCorpus script1 to accommodate the Indonesian translation. Moreover, the subtitles also contain song lyrics that are not translated into Indonesian. Therefore, the non-translated sentenc"
2020.calcs-1.4,P18-1143,0,0.33116,"ched words since cross-lingual embedding tries to merge word representations from two sets of monolingual texts. Mixed words will therefore not be represented in cross-lingual word-embeddings. To address this issue, we suggest that a cross-lingual word embedding model based on code-mixed sentences might be needed. We will call these embeddings code-mixed word embeddings. These word embeddings still cover more than one language like cross-lingual embeddings, but they do so in a setting where the languages are mixed, rather than separate. This has previously been proposed for English–Spanish by Pratapa et al. (2018b). People from around the world are able to connect and exchange information instantly, through the internet and social media. This exchange of information is dominated by the English language (Danet and Herring, 2003; Kramarae, 1999; Poppi, 2014). To prepare Indonesians to go global, the English language has been taught to Indonesian students from elementary school. This exposure to the English language instigates frequent Indonesian–English code-mixing in Indonesia (Brown, 2000, p. 139). This phenomenon is clearly observable in social media not only used by Indonesians but also across langu"
2020.calcs-1.4,D18-1344,0,0.445041,"ched words since cross-lingual embedding tries to merge word representations from two sets of monolingual texts. Mixed words will therefore not be represented in cross-lingual word-embeddings. To address this issue, we suggest that a cross-lingual word embedding model based on code-mixed sentences might be needed. We will call these embeddings code-mixed word embeddings. These word embeddings still cover more than one language like cross-lingual embeddings, but they do so in a setting where the languages are mixed, rather than separate. This has previously been proposed for English–Spanish by Pratapa et al. (2018b). People from around the world are able to connect and exchange information instantly, through the internet and social media. This exchange of information is dominated by the English language (Danet and Herring, 2003; Kramarae, 1999; Poppi, 2014). To prepare Indonesians to go global, the English language has been taught to Indonesian students from elementary school. This exposure to the English language instigates frequent Indonesian–English code-mixing in Indonesia (Brown, 2000, p. 139). This phenomenon is clearly observable in social media not only used by Indonesians but also across langu"
2020.calcs-1.4,L16-1658,0,0.0257311,"ick et al. (2016) train code-mixed embeddings on synthetic data where five languages are mixed. They evaluate these embeddings on a bilingual analogy tasks and on cross-lingual sentiment analysis. The goal of this work is different from us, since their purpose was to find embeddings that are useful in a cross-lingual learning setting, rather than in a setting where code-mixed data is processed. As for non-synthetically creation of code-mixed corpus, there have been multiple attempts such as Turkish–German Code-Switching Corpus (C¸etino˘glu, 2016), Arabic– Moroccan Darija Code-Switched Corpus (Samih and Maier, 2016), Hindi–English Code-Mixed Corpus (Vijay et al., 2018), and English–Spanish Code-Switching Twitter Corpus (Vilares et al., 2016). Yet, English–Indonesian Code-Mixed Corpus does not exist. Most of the codemixed corpora are created by fetching social media information such as Tweets. There are several studies that precede our study in codemixed text synthesis. Some of the recent approach use neural networks. Winata et al. (2019) create a sequence-tosequence model with a copy mechanism which learn how to combine sentences from parallel corpora to generate codemixed text. Chang et al. (2019) utili"
2020.calcs-1.4,tiedemann-2012-parallel,0,0.0162481,"formal variant has smaller word variation than the informal variant. Formalizing the corpus helps reduce the Out-of-Vocabulary (OOV) when we do a downstream task such as sentiment analysis. Furthermore, the sentiment analysis corpus (Saputri et al., 2018) we use mostly contains formal Indonesian. However, all methods used would be equally applicable to informal Indonesian. 4. Data and Pre-Processing We use multiple English–Indonesian sentence-aligned corpora from several genres: The Open Subtitle 2018 Corpus (Lison et al., 2018), TED 2018 Corpus (Cettolo et al., 2012), and GlobalVoice Corpus (Tiedemann, 2012), all taken from the OPUS collection (Tiedemann, 2012). These corpora are cleaned and pre-processed. Table 1 gives an overview of the size of the corpus before and after preprocessing. Open Subtitle 2018 dominates the corpus. The open subtitle corpus contains a lot of non-letter characters (e.g. ¶∗#) and formatting (e.g. {cHFFFFFF}). To clean this, we adjusted the PrepCorpus script1 to accommodate the Indonesian translation. Moreover, the subtitles also contain song lyrics that are not translated into Indonesian. Therefore, the non-translated sentence pairs were removed from the corpus. Thes"
2020.calcs-1.4,N18-4018,0,0.0120168,"c data where five languages are mixed. They evaluate these embeddings on a bilingual analogy tasks and on cross-lingual sentiment analysis. The goal of this work is different from us, since their purpose was to find embeddings that are useful in a cross-lingual learning setting, rather than in a setting where code-mixed data is processed. As for non-synthetically creation of code-mixed corpus, there have been multiple attempts such as Turkish–German Code-Switching Corpus (C¸etino˘glu, 2016), Arabic– Moroccan Darija Code-Switched Corpus (Samih and Maier, 2016), Hindi–English Code-Mixed Corpus (Vijay et al., 2018), and English–Spanish Code-Switching Twitter Corpus (Vilares et al., 2016). Yet, English–Indonesian Code-Mixed Corpus does not exist. Most of the codemixed corpora are created by fetching social media information such as Tweets. There are several studies that precede our study in codemixed text synthesis. Some of the recent approach use neural networks. Winata et al. (2019) create a sequence-tosequence model with a copy mechanism which learn how to combine sentences from parallel corpora to generate codemixed text. Chang et al. (2019) utilize a generative adversarial network to generate code-m"
2020.calcs-1.4,L16-1655,0,0.0469847,"Missing"
2020.calcs-1.4,K19-1026,0,0.102904,"ot guarantee that we get balanced corpora of diverse patterns of code-mixed sentences, nor that we get a large enough set of code-mixed sentences. To avoid getting a skewed corpus, we need to be able to control class (code-mixed pattern) distribution in our corpus. One possible method is to synthesize the training corpus By synthesizing a corpus, we will be able to control the class distribution in our data set, and we can easily create a large corpus. A number of studies has previously proposed methods for synthesizing code-mixed text, using a variety of approaches, based on neural networks (Winata et al., 2019; Chang et al., 2019), linguistic theories (Lee et al., 2019; Pratapa et al., 2018a), or heuristics (Wick et al., 2016). None of these studies have incorporated mixed morphology, which is important in the Indonesian–English setting we are interested in. Our study is using a heuristic approach similar to the work by Wick et al. (2016), which is, however, focused on artificial code-switching involving several languages with the end goal of improving cross-lingual NLP, rather then on mimicking naturally occurring codemixed data with the end goal of processing code-mixed data. The main purpose of"
2020.cl-4.3,P18-2003,0,0.0404026,"Missing"
2020.cl-4.3,P15-1033,0,0.456325,"re are more labels than these two and labels do not encode information about whether they are dependency or transfer relations. 765 Computational Linguistics Volume 46, Number 4 However, this notion has not been made explicit when training parsers. In pre-neural transition-based parsers as in Nivre, Hall, and Nilsson (2006), when a dependent gets attached to its head, features of the head are still used for further parsing but features of the dependent are usually discarded.2 In neural parsers, it is less clear what information is used by parsers. Current stateof-the-art models use (Bi)LSTMs (Dyer et al. 2015; Kiperwasser and Goldberg 2016; Dozat and Manning 2017), and LSTMs make it possible to encode information about the surrounding context of words in an unbounded window (which is usually limited to a sentence in practice). In this article, we take a step in finding out what neural parsers learn by testing if they capture the notion of dissociated nuclei. We do this by looking in detail at what a BiLSTM-based parser learns about a specific type of dissociated nucleus: auxiliary verb constructions. We focus on AVCs as they are a typical example of dissociated nucleus and are well attested typolo"
2020.cl-4.3,N18-1108,0,0.0769137,"icle, we use this method to investigate whether or not a specific aspect of linguistic theory is learned, the notion of dissociated nucleus from dependency grammar (explained in §2.1), as a byproduct of learning the task of dependency parsing for several languages. For this, we focus on auxiliary verb constructions (AVCs). A prominent question in neural modeling of syntax is whether or not it is necessary to model hierarchical structure. Sequential models (long short-term memory networks [LSTMs]) have shown surprising capability for learning syntactic tasks (Linzen, Dupoux, and Goldberg 2016; Gulordava et al. 2018), and models of dependency parsing using sequential models are very accurate (Kiperwasser and Goldberg 2016). Although recursive neural networks surpass the abilities of sequential models for learning syntactic tasks (Kuncoro et al. 2018), their use in dependency parsing seems superfluous compared to using sequential models when looking at parsing accuracy (de Lhoneux, Ballesteros, and Nivre 2019). However, there may be benefits to using recursive neural networks in parsing that are not reflected in parsing accuracy. In particular, for reasons outlined in §2.2, they may be useful when it comes"
2020.cl-4.3,Q16-1023,0,0.575166,"rned, the notion of dissociated nucleus from dependency grammar (explained in §2.1), as a byproduct of learning the task of dependency parsing for several languages. For this, we focus on auxiliary verb constructions (AVCs). A prominent question in neural modeling of syntax is whether or not it is necessary to model hierarchical structure. Sequential models (long short-term memory networks [LSTMs]) have shown surprising capability for learning syntactic tasks (Linzen, Dupoux, and Goldberg 2016; Gulordava et al. 2018), and models of dependency parsing using sequential models are very accurate (Kiperwasser and Goldberg 2016). Although recursive neural networks surpass the abilities of sequential models for learning syntactic tasks (Kuncoro et al. 2018), their use in dependency parsing seems superfluous compared to using sequential models when looking at parsing accuracy (de Lhoneux, Ballesteros, and Nivre 2019). However, there may be benefits to using recursive neural networks in parsing that are not reflected in parsing accuracy. In particular, for reasons outlined in §2.2, they may be useful when it comes to learning the notion of dissociated nucleus. In this article, we evaluate the usefulness of recursive neu"
2020.cl-4.3,P11-1068,0,0.51359,"Missing"
2020.cl-4.3,P18-1132,0,0.110018,"ng for several languages. For this, we focus on auxiliary verb constructions (AVCs). A prominent question in neural modeling of syntax is whether or not it is necessary to model hierarchical structure. Sequential models (long short-term memory networks [LSTMs]) have shown surprising capability for learning syntactic tasks (Linzen, Dupoux, and Goldberg 2016; Gulordava et al. 2018), and models of dependency parsing using sequential models are very accurate (Kiperwasser and Goldberg 2016). Although recursive neural networks surpass the abilities of sequential models for learning syntactic tasks (Kuncoro et al. 2018), their use in dependency parsing seems superfluous compared to using sequential models when looking at parsing accuracy (de Lhoneux, Ballesteros, and Nivre 2019). However, there may be benefits to using recursive neural networks in parsing that are not reflected in parsing accuracy. In particular, for reasons outlined in §2.2, they may be useful when it comes to learning the notion of dissociated nucleus. In this article, we evaluate the usefulness of recursive neural networks when it comes to learning the notion of dissociated nucleus. The goals of this article are thus threefold. First, we"
2020.cl-4.3,Q16-1037,0,0.06206,"Missing"
2020.cl-4.3,J08-4003,1,0.594581,"ry in case of AVCs) and concatenate the features Person and Number. The possible values are therefore all possible combinations of 1st, 2nd, and 3rd person with plural and singular. There are cases where this information is not available, in which case the agreement task is undefined for the AVC. The code to reproduce our experiments is available at https://github.com /mdelhoneux/avc analyser, including the modifications we made to the parser to freeze the vector representations at the different layers in the network. 3.2 BiLSTM-based Parsing We use UUParser, a greedy transition-based parser (Nivre 2008) based on the framework of Kiperwasser and Goldberg (2016) where BiLSTMs (Hochreiter and Schmidhuber 7 We experimented with a harder task: predicting the number of objects. In the example in Figure 3, that number would be 1. In case of intransitive use of verbs, it would be 0, and with a ditransitive use of a verb, it would be 2. We observed the same trends and therefore do not report these results. 770 de Lhoneux et al. What Should/Do/Can LSTMs Learn When Parsing Auxiliary Verb Constructions? 1997; Graves 2008) learn representations of tokens in context, and are trained together with a multil"
2020.cl-4.3,L16-1262,1,0.850608,"Missing"
2020.cl-4.3,N18-1202,0,0.0470304,"ed by the parser, it is useful for the task of parsing, since the representations are obtained from a model that is trained end-to-end without explicit supervision about the prediction tasks. Recent research has cast some doubt on this: As discussed by Belinkov (2018), it is not impossible that the information is in the network activations but is not used by the network. In any case, we are not interested in improving the parser here but in finding out what it learns. 9 Note that for this kind of experiment, training language models that learn contextual representations of words such as ELMo (Peters et al. 2018) or BERT (Devlin et al. 2019) and compare these representations to our token vectors would be more appropriate. However, these models are typically trained on very large data sets and it is unclear how well they perform when trained on just treebank data. We leave doing this to future work. 773 Computational Linguistics Volume 46, Number 4 3.4 Vectors We train parsers for 30 epochs for all these treebanks and pick the model of the best epoch based on LAS score on the development set. We report parsing results in Table 2. We train the parser with the same hyperparameters as in Smith et al. (201"
2020.cl-4.3,N19-1000,0,0.0979021,"Missing"
2020.cl-4.3,K18-2011,1,0.868896,"Missing"
2020.cl-4.3,Q18-1019,0,0.0530749,"Missing"
2020.lrec-1.103,J96-2004,0,0.692745,"ollapsed all categories except speech and speech tags into the other group. The only such cases were A1 having annotated 9 instances of embedded speech, and 2 thoughts, that A2 had not identified. Overall, though, there are no mismatches between the different categories. The only errors are cases where one annotator has identified the annotation, and the other annotator has not. In the overwhelming majority of cases, the annotators have marked the same spans, there are only a few cases of overlapping spans. We also calculated the Kappa statistic of overlap between the two pairs of annotators (Carletta, 1996), for the identification of main category. The values were 0.72 for A1 vs A2 and 0.83 for A1 vs A3, which is normally interpreted as substantial and near perfect agreement. Studying the annotations made only by one annotator, it seems that they are mostly due to mistakes. There are, however, also a few difficult borderline cases. One example is (6) from S¨oderberg (p. 191), where it is hard to say if the speech is direct or indirect, and if it should be excluded, since we are not concerned with indirect speech. In most cases the difference between direct speech (DS) and indirect speech (IS) ha"
2020.lrec-1.103,L18-1131,0,0.604141,"Missing"
2020.lrec-1.103,P10-1015,0,0.238822,"re the lack of reliable tools for identifying narrative, in contrast to speech, and the speaker of an utterance, as well as the speech tag, was problematic, is Stymne et al. (2018), where we tried to analyse style breaks in the novel Kallocain by Karin Boye. Due to the lack of such tools, we had to focus most of the study on the main narrative, which could be identified relatively reliable based on paragraph breaks, but which excluded more than necessary of the text. Dialogue in English literature has previously been analysed on a large scale, especially the attribution of speakers to speech. Elson et al. (2010) studied the automatic identification of social networks in text, through extracting speakers engaged in dialogue. They could identify speech easily, since quotation marks were used as speech markers, even though, as they note, quotes are also used for other purposes, so the extraction was noisy. They also annotated the speakers of over 3,000 utterances, using Mechanical Turk, and used this to perform automatic quote attribution. He et al. (2013) and Muzny et al. (2017) both focus on quote attribution, and create a corpus for this task. QouteLi3 (Muzny et al., 2017) is a corpus of over 6,000 l"
2020.lrec-1.103,P13-1129,0,0.69734,"Missing"
2020.lrec-1.103,E17-1044,0,0.367931,"te, that this means that we also get a reliable annotation of narrative, since we annotate everything that is not part of the narrative. At the highest level, we mark all words in a text segment containing cited material, or being a speech tag. The annotation is then given one of the following labels: • Speech • Speech tag • Embedded Speech • Thought • Sign • Letter • Quotation • Other Note that quotation is used for references to text outside of the novel, not synonymously with utterances from characters in the novel, which often are referred to as quotes in computational linguistics papers (Muzny et al., 2017, among others), which we call speech (segments) in this paper. The remaining types include text from signs of different types, letters, and thoughts. Embedded speech is speech that is quoted within a normal speech segment. The Other tag is used when the annotator did not find any of the other tags suitable. They were instructed to describe the type in free text, if they found any such instances. This tag was rarely used, though. Speech, and to some extent thoughts, are often accompanied by a speech tag. The speech tag could occur after the speech (2), in the middle of a speech segment (3) or"
2020.lrec-1.103,P13-4001,0,0.0379755,"Missing"
2020.lrec-1.103,P14-5016,0,0.0269419,"Missing"
2020.mwe-1.14,calzolari-etal-2002-towards,0,0.0556036,"ary discovery methods. We released annotated and raw corpora in 14 languages, and this semi-supervised challenge attracted 7 teams who submitted 9 system results. This paper describes the effort of corpus creation, the task design, and the results obtained by the participating systems, especially their performance on unseen expressions. 1 Introduction Multiword expressions (MWEs) such as to throw someone under the bus ‘to cause one’s suffering to gain personal advantage’ are idiosyncratic word combinations which need to be identiﬁed prior to further semantic processing (Baldwin and Kim, 2010; Calzolari et al., 2002). The task of MWE identiﬁcation, that is, automatically locating instances of MWEs in running text (Constant et al., 2017) has received growing attention in the last 4 years. Progress on this task was especially motivated by shared tasks such as DiMSUM (Schneider et al., 2016), and two editions of the PARSEME shared tasks, edition 1.0 in 2017 (Savary et al., 2017), and edition 1.1 in 2018 (Ramisch et al., 2018). Previous editions of the PARSEME shared task focused on the identiﬁcation of verbal MWEs (VMWEs), because of their challenging traits: complex structure, discontinuities, variability,"
2020.mwe-1.14,J17-4005,1,0.888334,"Missing"
2020.mwe-1.14,W17-4418,0,0.065602,"Missing"
2020.nlpbt-1.5,N19-1202,0,0.0273428,"Missing"
2020.nlpbt-1.5,N18-1008,0,0.0201856,"Missing"
2020.nlpbt-1.5,W19-6603,0,0.0227566,"Missing"
2020.nlpbt-1.5,2020.lrec-1.520,0,0.0225426,"ritize speaker variety: it contains only a few audio segments per chapter, and a few chapters per book. Augmented LibriSpeech (Kocabiyikoglu et al., 2018) is an augmentation of LibriSpeech with French textual translations. Kocabiyikoglu et al. (2018) used part of the LibriSpeech metadata (around 1500 English e-book titles) to retrieve their 2.4 CoVoST and CoVoST2 Facebook AI4 recently released CoVoST(Wang et al., 2020a) and CoVoST2 (Wang et al., 2020b). Each CoVoST corpus is an augmentation of CoVo 2 https://github.com/lowerquality/ gentle 3 https://www.ted.com/ 4 https://ai.facebook.com/ 42 (Ardila et al., 2020), a multilingual speech recognition corpus. CoVo already provides pairs of aligned audio and transcription. Wang et al. (2020a) selected 11 languages from Common Voice. Then, they paid professional translators to translate 11 Common Voice data-sets (one for each selected language) into English. In order to ensure the quality of the translations, Wang et al. (2020a) applied different sanity checks to find weak translations and send them back to the professional translators. Interestingly, one of those sanity checks was to compute similarity scores between the sentence embeddings of the source l"
2020.nlpbt-1.5,L18-1531,0,0.0141086,"omain French e-books. The collected English and French e-books were aligned with hunalign (Varga et al., 2007), resulting in a textual parallel corpus. Finally, the English side of the parallel corpus was aligned with the LibriSpeech English audio recordings with Gentle2 . There have been some previous work on creating resources for end-to-end speech-to-text machine translation (Kocabiyikoglu et al., 2018; Di Gangi et al., 2019a; Iranzo-S´anchez et al., 2020; Wang et al., 2020a,b). There are also other related efforts, including computational language documentation for low-resource languages (Godard et al., 2018), multilingual speech corpora creation (Black, 2019), and multi-modal corpora creation (Sanabria et al., 2018). Godard et al. (2018) created a speechto-text corpus of 5 thousands triplets of Mboshi speech, Mboshi transcription, and French textual translations. Speech elicitation from text was done manually by three qualified speakers. Black (2019) created a large corpus of aligned text, speech, and pronounciation for 700 languages, with texts from the bible. The average duration for each language is 2 hours. Sanabria et al. (2018) created a multimodal corpus by aligning at a word-level 2000 ho"
2020.nlpbt-1.5,2020.lrec-1.441,0,0.0170566,"age duration for each language is 2 hours. Sanabria et al. (2018) created a multimodal corpus by aligning at a word-level 2000 hours of English instructional YouTube videos with their subtitles. Portuguese textual translations were added by paying bilingual English-Portuguese speakers. Augmented LibriSpeech (Kocabiyikoglu et al., 2018) seem to be the first corpus designed for training end-to-end English-to-French speech-totext machine translation systems. It was created by collecting public domain audiobooks and e-books from the web and automatically align them. A similar approach was used by Beilharz et al. (2020) to create LibriVoxDeEn, a corpus for German-toEnglish speech translation. Most recent works have been focused on multilingual corpora creation for speech-to-text machine translation: MuST-C (Di Gangi et al., 2019a), Europarl-ST, CoVoST (Wang et al., 2020a) and CoVoST2 (Wang et al., 2020b) 2.1 2.2 MuST-C Data were collected from the English TED website3 . Di Gangi et al. (2019a) selected those talks that include both a transcription and a textual translation in German, Spanish, French, Italian, Dutch, Portuguese, Romanian or Russian. MuST-C is split in different data-sets for each language dir"
2020.nlpbt-1.5,L18-1001,0,0.26068,"ww.aclweb.org/anthology/W23-20%2d 2 Related Work corresponding French e-book titles by querying Dbpedia (Auer et al., 2007). They then compared the retrieved French e-book titles against a web index containing public domain French e-books. The collected English and French e-books were aligned with hunalign (Varga et al., 2007), resulting in a textual parallel corpus. Finally, the English side of the parallel corpus was aligned with the LibriSpeech English audio recordings with Gentle2 . There have been some previous work on creating resources for end-to-end speech-to-text machine translation (Kocabiyikoglu et al., 2018; Di Gangi et al., 2019a; Iranzo-S´anchez et al., 2020; Wang et al., 2020a,b). There are also other related efforts, including computational language documentation for low-resource languages (Godard et al., 2018), multilingual speech corpora creation (Black, 2019), and multi-modal corpora creation (Sanabria et al., 2018). Godard et al. (2018) created a speechto-text corpus of 5 thousands triplets of Mboshi speech, Mboshi transcription, and French textual translations. Speech elicitation from text was done manually by three qualified speakers. Black (2019) created a large corpus of aligned text"
2020.nlpbt-1.5,2005.mtsummit-papers.11,0,0.134149,"ebook LASER (Schwenk and Douze, 2017; Schwenk and Li, 2018; Schwenk, 2018) to map into vectors all possible English and Italian consecutive sentences in a window of size 10 for each pair of documents to be aligned. Since the size of the LFAligner Italian-English dictionary was rather small (around 14500 terms) and we did not find other accurate and manually annotated freely available English-Italian lexicons, we investigated if a large automatically created lexicon could be useful. We compiled a large EnglishItalian corpus (containing 3131200 parallel sentences) by concatenating the Europarl (Koehn, 2005), the Wikipedia (Wołk and Marasek, 2014), the GlobalVoices12 , and the books13 corpora from OPUS (Tiedemann, 2012). We used Giza++ (Och and Ney, 2003) to align the corpus, followed by using Moses SMT (Koehn et al., 2007) to symmetrize the directional alignments, and extract a lexical translation table. The bidirectional tables contain a great amount of extremely low-probability translation terms hypotheses. We inferred a bilingual dictionary containing 692437 bilingual terms by filtering out the terms scoring less than 0.10. This inferred lexicon was used with Hunalign. 5.4 Evaluation Methodol"
2020.nlpbt-1.5,D19-1136,0,0.0198579,"s, prioritizing the comparison and evaluation of different alignment methods instead of using all retrieved Italian e-books (39), which would have resulted in around 500 hours of audio material to be aligned. Bilingual Sentence Alignment Kocabiyikoglu et al. (2018) only use one method for the bilingual sentence alignment task. Instead, we tested and evaluated three different methods: hunalign in conjunction with a small size hand-crafted dictionary, hunalign in conjunction with a larger bilingual dictionary automatically inferred using statistical machine translation techniques, and Vecalign (Thompson and Koehn, 2019). Vecalign is a recently proposed method to perform bilingual text alignment computing cosine similarity of embeddings of consecutive sentences. We did not experiment with Gargantua, the alignment tool used by Di Gangi et al. (2019a), mainly because the tool seems to be more effective on large documents (Abdul-Rauf et al., 2010), while we want to align each pair of English and Italian parallel chapters individually, to maintain parallelism with the English LibriVox chapter audio recordings. Hunalign is not designed for aligning documents with more than 20000 sentences, but is effective on rela"
2020.nlpbt-1.5,J03-1002,0,0.0204899,"ences in a window of size 10 for each pair of documents to be aligned. Since the size of the LFAligner Italian-English dictionary was rather small (around 14500 terms) and we did not find other accurate and manually annotated freely available English-Italian lexicons, we investigated if a large automatically created lexicon could be useful. We compiled a large EnglishItalian corpus (containing 3131200 parallel sentences) by concatenating the Europarl (Koehn, 2005), the Wikipedia (Wołk and Marasek, 2014), the GlobalVoices12 , and the books13 corpora from OPUS (Tiedemann, 2012). We used Giza++ (Och and Ney, 2003) to align the corpus, followed by using Moses SMT (Koehn et al., 2007) to symmetrize the directional alignments, and extract a lexical translation table. The bidirectional tables contain a great amount of extremely low-probability translation terms hypotheses. We inferred a bilingual dictionary containing 692437 bilingual terms by filtering out the terms scoring less than 0.10. This inferred lexicon was used with Hunalign. 5.4 Evaluation Methodology There are two main challenges in evaluating the sentence aligners. First, the lack of a gold standard file for computing the F1 score. Second, the"
2020.nlpbt-1.5,tiedemann-2012-parallel,0,0.0516939,"glish and Italian consecutive sentences in a window of size 10 for each pair of documents to be aligned. Since the size of the LFAligner Italian-English dictionary was rather small (around 14500 terms) and we did not find other accurate and manually annotated freely available English-Italian lexicons, we investigated if a large automatically created lexicon could be useful. We compiled a large EnglishItalian corpus (containing 3131200 parallel sentences) by concatenating the Europarl (Koehn, 2005), the Wikipedia (Wołk and Marasek, 2014), the GlobalVoices12 , and the books13 corpora from OPUS (Tiedemann, 2012). We used Giza++ (Och and Ney, 2003) to align the corpus, followed by using Moses SMT (Koehn et al., 2007) to symmetrize the directional alignments, and extract a lexical translation table. The bidirectional tables contain a great amount of extremely low-probability translation terms hypotheses. We inferred a bilingual dictionary containing 692437 bilingual terms by filtering out the terms scoring less than 0.10. This inferred lexicon was used with Hunalign. 5.4 Evaluation Methodology There are two main challenges in evaluating the sentence aligners. First, the lack of a gold standard file for"
2020.nlpbt-1.5,2020.lrec-1.517,0,0.166995,"s by querying Dbpedia (Auer et al., 2007). They then compared the retrieved French e-book titles against a web index containing public domain French e-books. The collected English and French e-books were aligned with hunalign (Varga et al., 2007), resulting in a textual parallel corpus. Finally, the English side of the parallel corpus was aligned with the LibriSpeech English audio recordings with Gentle2 . There have been some previous work on creating resources for end-to-end speech-to-text machine translation (Kocabiyikoglu et al., 2018; Di Gangi et al., 2019a; Iranzo-S´anchez et al., 2020; Wang et al., 2020a,b). There are also other related efforts, including computational language documentation for low-resource languages (Godard et al., 2018), multilingual speech corpora creation (Black, 2019), and multi-modal corpora creation (Sanabria et al., 2018). Godard et al. (2018) created a speechto-text corpus of 5 thousands triplets of Mboshi speech, Mboshi transcription, and French textual translations. Speech elicitation from text was done manually by three qualified speakers. Black (2019) created a large corpus of aligned text, speech, and pronounciation for 700 languages, with texts from the bible"
2020.nlpbt-1.5,1983.tc-1.13,0,0.471969,"Missing"
2020.nlpbt-1.5,P18-2037,0,0.0147772,"ent is used to heuristically increase the size of the bilingual dictionary by looking at the co-occurrences found in the bi-sentences (the output of the first alignment). Finally, a second alignment is performed with the enriched bilingual dictionary (resulting from the first alignment). 9 https://github.com/JonathanReeve/ chapterize 10 https://spacy.io/ 11 https://sourceforge.net/projects/ aligner/ 45 5.2 Hunalign with a Bilingual Dictionary Inferred with Moses and Giza++ Jack” - English: ”Hi.”, ”I am Jack”) or viceversa. We used Facebook LASER (Schwenk and Douze, 2017; Schwenk and Li, 2018; Schwenk, 2018) to map into vectors all possible English and Italian consecutive sentences in a window of size 10 for each pair of documents to be aligned. Since the size of the LFAligner Italian-English dictionary was rather small (around 14500 terms) and we did not find other accurate and manually annotated freely available English-Italian lexicons, we investigated if a large automatically created lexicon could be useful. We compiled a large EnglishItalian corpus (containing 3131200 parallel sentences) by concatenating the Europarl (Koehn, 2005), the Wikipedia (Wołk and Marasek, 2014), the GlobalVoices12 ,"
2020.nlpbt-1.5,W17-2619,0,0.0185745,"the GaleChurch algorithm. The resulting alignment is used to heuristically increase the size of the bilingual dictionary by looking at the co-occurrences found in the bi-sentences (the output of the first alignment). Finally, a second alignment is performed with the enriched bilingual dictionary (resulting from the first alignment). 9 https://github.com/JonathanReeve/ chapterize 10 https://spacy.io/ 11 https://sourceforge.net/projects/ aligner/ 45 5.2 Hunalign with a Bilingual Dictionary Inferred with Moses and Giza++ Jack” - English: ”Hi.”, ”I am Jack”) or viceversa. We used Facebook LASER (Schwenk and Douze, 2017; Schwenk and Li, 2018; Schwenk, 2018) to map into vectors all possible English and Italian consecutive sentences in a window of size 10 for each pair of documents to be aligned. Since the size of the LFAligner Italian-English dictionary was rather small (around 14500 terms) and we did not find other accurate and manually annotated freely available English-Italian lexicons, we investigated if a large automatically created lexicon could be useful. We compiled a large EnglishItalian corpus (containing 3131200 parallel sentences) by concatenating the Europarl (Koehn, 2005), the Wikipedia (Wołk an"
2020.nlpbt-1.5,L18-1560,0,0.011403,". The resulting alignment is used to heuristically increase the size of the bilingual dictionary by looking at the co-occurrences found in the bi-sentences (the output of the first alignment). Finally, a second alignment is performed with the enriched bilingual dictionary (resulting from the first alignment). 9 https://github.com/JonathanReeve/ chapterize 10 https://spacy.io/ 11 https://sourceforge.net/projects/ aligner/ 45 5.2 Hunalign with a Bilingual Dictionary Inferred with Moses and Giza++ Jack” - English: ”Hi.”, ”I am Jack”) or viceversa. We used Facebook LASER (Schwenk and Douze, 2017; Schwenk and Li, 2018; Schwenk, 2018) to map into vectors all possible English and Italian consecutive sentences in a window of size 10 for each pair of documents to be aligned. Since the size of the LFAligner Italian-English dictionary was rather small (around 14500 terms) and we did not find other accurate and manually annotated freely available English-Italian lexicons, we investigated if a large automatically created lexicon could be useful. We compiled a large EnglishItalian corpus (containing 3131200 parallel sentences) by concatenating the Europarl (Koehn, 2005), the Wikipedia (Wołk and Marasek, 2014), the"
2020.tlt-1.6,N19-1253,0,0.0262184,"the term domain to cover both these types of text.1 The main novelty in this work is that we combine domain adaptation with cross-lingual learning for dependency parsing. We note that treebanks for a specific domain (IND: in-domain) often exist for some languages, and we show that we can take advantage of such data for parsing this domain in other languages. Our main focus is on the case where we want to parse data for a language that has some resources, but none for the domain in question (OOD: out-of-domain). While there is plenty of work both on cross-lingual parsing (Ammar et al., 2016a; Ahmad et al., 2019; Kondratyuk and Straka, 2019) and domain adaptation for parsing (Kim et al., 2016; Sato et al., 2017; Xiuming et al., 2019), there is to the best of our knowledge no attempts to combine these approaches in a uniform framework for dependency parsing. We adapt the parsing framework of Smith et al. (2018a) which incorporates treebank embeddings to represent treebanks, similarly to how language embeddings has been used to represent the languages (Ammar et al., 2016b; de Lhoneux et al., 2017a). In this framework each parsing model is trained on a concatenation of different treebanks, and the repre"
2020.tlt-1.6,C18-1111,0,0.0258837,"parsing. We adapt the parsing framework of Smith et al. (2018a) which incorporates treebank embeddings to represent treebanks, similarly to how language embeddings has been used to represent the languages (Ammar et al., 2016b; de Lhoneux et al., 2017a). In this framework each parsing model is trained on a concatenation of different treebanks, and the representation of each input token includes an embedding 1 The term domain has often been used as a catch-all term in NLP, to cover many different types of text type differences, often without being clearly defined, see e.g. (Weiss et al., 2016; Chu and Wang, 2018), even though there has been some attempts to investigate different aspects of domains, e.g. (van der Wees et al., 2015; Ruder et al., 2016). representing the treebank from which the token comes from. Depending on the mix of treebanks, the treebank embedding can encode aspects such as differences between languages, domains, and annotation style. Parsing with treebank embeddings has previously been applied monolingually (Stymne et al., 2018; Wagner et al., 2020) and cross-lingually for related languages, but without taking domain into account (Smith et al., 2018a; Lim et al., 2018),2 In this pa"
2020.tlt-1.6,K17-3022,1,0.896567,"Missing"
2020.tlt-1.6,W17-6314,1,0.879825,"Missing"
2020.tlt-1.6,N19-1423,0,0.175625,"fect addressing the task of cross-lingual domain adaptation. It is a simple and efficient method, which does not require expensive pre-processing, pre-training, translation, or similar tasks required by many other cross-lingual approaches, while giving competitive results across many settings. In this work we explore how such a resource lean method can be applied to cross-domain parsing on its own. We leave to future work an investigation of how the proposed technique interacts with other techniques for domain adaptation, for instance based on pre-training contextualized embeddings like BERT (Devlin et al., 2019). At test time, there is a need to determine which treebank embedding to use, which is straightforward for test data from a treebank used during training. However, when the input sentence is from a treebank not used during training there is a need to determine the treebank embedding. One option is to use a proxy treebank (Stymne et al., 2018), i.e. to choose the embedding of one of the treebanks used during training, which can be determined based on development data. Wagner et al. (2020) show that it is often advantageous to interpolate the embeddings of the treebanks used for training instead"
2020.tlt-1.6,2020.conll-1.6,0,0.01647,"acle score, where we pick the best proxy treebank based on test performance. We use the default hyperparameters of uuparser, as specified in Smith et al. (2018a). Note that no POStags are used, since POS-tagging in these difficult domains would lead to the same issues as for parsing. In addition, character embeddings compensate for the lack of POS-tags to a large extent across several typologically different languages (Smith et al., 2018b), and in order for universal POS-tags, the most feasible choice cross-lingually, to be useful for parsing, the tagging quality has to be prohibitively high (Gómez-Rodríguez, 2020). The parser is trained end-to-end on treebank data, without any pre-training. All embeddings are initialized randomly at training time. Each model is trained for 30 epochs, and the best epoch is chosen based on average development scores among treebanks used at training time. Evaluation Metrics We use unlabelled and labelled attachment score, UAS and LAS, as evaluation metrics. Our system was optimized based on development UAS scores, since we believe that it is a good fit to the case of inconsistent labeling in the treebanks for each target domain. Overall, the test results reflect the trend"
2020.tlt-1.6,C16-1038,0,0.0283866,"s that we combine domain adaptation with cross-lingual learning for dependency parsing. We note that treebanks for a specific domain (IND: in-domain) often exist for some languages, and we show that we can take advantage of such data for parsing this domain in other languages. Our main focus is on the case where we want to parse data for a language that has some resources, but none for the domain in question (OOD: out-of-domain). While there is plenty of work both on cross-lingual parsing (Ammar et al., 2016a; Ahmad et al., 2019; Kondratyuk and Straka, 2019) and domain adaptation for parsing (Kim et al., 2016; Sato et al., 2017; Xiuming et al., 2019), there is to the best of our knowledge no attempts to combine these approaches in a uniform framework for dependency parsing. We adapt the parsing framework of Smith et al. (2018a) which incorporates treebank embeddings to represent treebanks, similarly to how language embeddings has been used to represent the languages (Ammar et al., 2016b; de Lhoneux et al., 2017a). In this framework each parsing model is trained on a concatenation of different treebanks, and the representation of each input token includes an embedding 1 The term domain has often be"
2020.tlt-1.6,Q16-1023,0,0.0236038,"g hardly any resources like Komi Zyrian, to large resources, like English, and cover some interesting special cases, such as code switching, a Creole language, Naija, and a language with two written varieties, Norwegian. Parser We use uuparser4 (de Lhoneux et al., 2017b) which is a transition-based dependency parser using the arc-hybrid transition system with the addition of a swap transition and a static-dynamic oracle, to be able to handled non-projectivity. The parser uses a two-layer BiLSTM as a feature extractor followed by a multi-layer perceptron predicting transitions, in the style of Kiperwasser and Goldberg (2016). Each word, wi , is represented by the concatenation of a word embedding, ew (wi ), a character-level embedding, obtained by running a BiLSTM over the characters ch j (1 ≤ j ≤ m) of wi , where m is the word length in characters, and a treebank embedding, etb (t ∗ ): ei = [ew (wi ); B I LSTM(ch1:m ); etb (t ∗ )] (1) The treebank embedding represents a treebank, t ∗ , which is chosen among the set of k treebanks used when training the model. During training, t ∗ is chosen as the treebank to which the current word/sentence belongs. When applying the model, the treebank of the sentence can be use"
2020.tlt-1.6,D19-1279,0,0.0164994,"cover both these types of text.1 The main novelty in this work is that we combine domain adaptation with cross-lingual learning for dependency parsing. We note that treebanks for a specific domain (IND: in-domain) often exist for some languages, and we show that we can take advantage of such data for parsing this domain in other languages. Our main focus is on the case where we want to parse data for a language that has some resources, but none for the domain in question (OOD: out-of-domain). While there is plenty of work both on cross-lingual parsing (Ammar et al., 2016a; Ahmad et al., 2019; Kondratyuk and Straka, 2019) and domain adaptation for parsing (Kim et al., 2016; Sato et al., 2017; Xiuming et al., 2019), there is to the best of our knowledge no attempts to combine these approaches in a uniform framework for dependency parsing. We adapt the parsing framework of Smith et al. (2018a) which incorporates treebank embeddings to represent treebanks, similarly to how language embeddings has been used to represent the languages (Ammar et al., 2016b; de Lhoneux et al., 2017a). In this framework each parsing model is trained on a concatenation of different treebanks, and the representation of each input token"
2020.tlt-1.6,K18-2014,0,0.0154406,"al., 2016; Chu and Wang, 2018), even though there has been some attempts to investigate different aspects of domains, e.g. (van der Wees et al., 2015; Ruder et al., 2016). representing the treebank from which the token comes from. Depending on the mix of treebanks, the treebank embedding can encode aspects such as differences between languages, domains, and annotation style. Parsing with treebank embeddings has previously been applied monolingually (Stymne et al., 2018; Wagner et al., 2020) and cross-lingually for related languages, but without taking domain into account (Smith et al., 2018a; Lim et al., 2018),2 In this paper, we show that joint training with treebank embeddings can be applied simultaneously across both across languages and domains, in effect addressing the task of cross-lingual domain adaptation. It is a simple and efficient method, which does not require expensive pre-processing, pre-training, translation, or similar tasks required by many other cross-lingual approaches, while giving competitive results across many settings. In this work we explore how such a resource lean method can be applied to cross-domain parsing on its own. We leave to future work an investigation of how th"
2020.tlt-1.6,N18-1088,0,0.0183122,", LinES (50.1K) ParTUT (43.5K), Hindi_HDTB (281K) ISDT (294K), ParTUT (52.4K), VIT (241K) Table 1: Treebanks and number of tokens in train, dev, and test data sets for the target treebanks. Top of table is spoken data, and bottom is for Twitter data. Additional data lists treebanks used for each target treebank, which is in-language unless otherwise noted, and the number of tokens in the training set for each treebank. Treebanks in italics are used in the contrastive data sets. between the treebanks used, especially for the rather unusual features of spoken data and Twitter. For instance, see Liu et al. (2018) for a discussion of differences between English and Italian Twitter treebanks, or the Naija-NSC documentation for known deviations from UD standards.3 To be able to compare the effect of adding in-domain data, we create a contrastive treebank for each IND language of the same size, counted in the number of tokens. We use data from the treebank(s) marked with italics in Table 1. We think the language sample is interesting and covers many aspects. Even though the majority of languages are Indoeuropean, they mostly have different genera. They range from having hardly any resources like Komi Zyri"
2020.tlt-1.6,W19-7713,0,0.0143215,"tead add an equivalent amount of out-of-language OOD data, we see minor average gains and a performance that is considerably worse than for IND data. Norwegian is an outlier here as well, with good results for OOD data. We leave an investigation of why to future work. These results confirm that our treebank combination strategy is useful. The two top lines of Table 2 simulates results when no in-language data is available. As expected these scores are considerably lower than when using in-language OOD data, being so poor that these parsers are hardly useful, confirming previous research, e.g. Meechan-Maddon and Nivre (2019) and Vania et al. (2019). In this case there is no clear difference between IND and OOD data. The scores for English and Hindi–English with IND data are closer to in-language OOD scores, which can be explained by the partial language match between these two treebanks. As a point of comparison, the bottom part of Table 2 shows the results when data matching both language and domain is available. As expected, it leads to large gains. For all languages, the model trained 6 Using a subset of these treebanks mostly gave lower scores but showed the same trends. French Norwegian Slovenian Slovenian+S"
2020.tlt-1.6,2020.lrec-1.497,0,0.059882,"Missing"
2020.tlt-1.6,W16-6012,0,0.02791,"how language embeddings has been used to represent the languages (Ammar et al., 2016b; de Lhoneux et al., 2017a). In this framework each parsing model is trained on a concatenation of different treebanks, and the representation of each input token includes an embedding 1 The term domain has often been used as a catch-all term in NLP, to cover many different types of text type differences, often without being clearly defined, see e.g. (Weiss et al., 2016; Chu and Wang, 2018), even though there has been some attempts to investigate different aspects of domains, e.g. (van der Wees et al., 2015; Ruder et al., 2016). representing the treebank from which the token comes from. Depending on the mix of treebanks, the treebank embedding can encode aspects such as differences between languages, domains, and annotation style. Parsing with treebank embeddings has previously been applied monolingually (Stymne et al., 2018; Wagner et al., 2020) and cross-lingually for related languages, but without taking domain into account (Smith et al., 2018a; Lim et al., 2018),2 In this paper, we show that joint training with treebank embeddings can be applied simultaneously across both across languages and domains, in effect"
2020.tlt-1.6,N06-2033,0,0.0435498,"ed during training. In other cases some other method has to be used. In this work we explore the following methods: • Proxy treebank: when dev data is available, we can try all possible proxy treebanks i.e. all treebanks used during training the model, and choose the treebank, t ∗ , which performs best on dev data. • Interpolation: We interpolate the embeddings from all treebanks used during training by averaging k 1 them with equal weights: (t ∗ = ∑t=1 k etb (t)) • Ensemble: We run the model with each possible proxy treebank, obtaining k output trees. Then we apply the reparsing technique by Sagae and Lavie (2006) which applies the Chu-Liu-Edmonds (Edmonds, 1967) algorithm with each arc being weighted by the number of trees for which that arc was predicted.5 Note that in all cases we only apply these techniques at test time. The interpolation method only requires a single test run. Proxy treebank requires k dev test runs, followed by a single test run. Ensembling 3 https://github.com/UniversalDependencies/UD_Naija-NSC/blob/master/README.md 4 https://github.com/UppsalaNLP/uuparser 5 Weighting the arcs by development UAS or LAS instead had little impact on the results, but requires development data. 1 2"
2020.tlt-1.6,K17-3007,0,0.0205544,"domain adaptation with cross-lingual learning for dependency parsing. We note that treebanks for a specific domain (IND: in-domain) often exist for some languages, and we show that we can take advantage of such data for parsing this domain in other languages. Our main focus is on the case where we want to parse data for a language that has some resources, but none for the domain in question (OOD: out-of-domain). While there is plenty of work both on cross-lingual parsing (Ammar et al., 2016a; Ahmad et al., 2019; Kondratyuk and Straka, 2019) and domain adaptation for parsing (Kim et al., 2016; Sato et al., 2017; Xiuming et al., 2019), there is to the best of our knowledge no attempts to combine these approaches in a uniform framework for dependency parsing. We adapt the parsing framework of Smith et al. (2018a) which incorporates treebank embeddings to represent treebanks, similarly to how language embeddings has been used to represent the languages (Ammar et al., 2016b; de Lhoneux et al., 2017a). In this framework each parsing model is trained on a concatenation of different treebanks, and the representation of each input token includes an embedding 1 The term domain has often been used as a catch-"
2020.tlt-1.6,K18-2011,1,0.887408,"Missing"
2020.tlt-1.6,D18-1291,1,0.894841,"Missing"
2020.tlt-1.6,P18-2098,1,0.901319,"Missing"
2020.tlt-1.6,P15-2092,0,0.0417226,"Missing"
2020.tlt-1.6,D19-1102,0,0.0137016,"-of-language OOD data, we see minor average gains and a performance that is considerably worse than for IND data. Norwegian is an outlier here as well, with good results for OOD data. We leave an investigation of why to future work. These results confirm that our treebank combination strategy is useful. The two top lines of Table 2 simulates results when no in-language data is available. As expected these scores are considerably lower than when using in-language OOD data, being so poor that these parsers are hardly useful, confirming previous research, e.g. Meechan-Maddon and Nivre (2019) and Vania et al. (2019). In this case there is no clear difference between IND and OOD data. The scores for English and Hindi–English with IND data are closer to in-language OOD scores, which can be explained by the partial language match between these two treebanks. As a point of comparison, the bottom part of Table 2 shows the results when data matching both language and domain is available. As expected, it leads to large gains. For all languages, the model trained 6 Using a subset of these treebanks mostly gave lower scores but showed the same trends. French Norwegian Slovenian Slovenian+Slavic Italian English Hi"
2020.tlt-1.6,2020.acl-main.778,0,0.110563,"term in NLP, to cover many different types of text type differences, often without being clearly defined, see e.g. (Weiss et al., 2016; Chu and Wang, 2018), even though there has been some attempts to investigate different aspects of domains, e.g. (van der Wees et al., 2015; Ruder et al., 2016). representing the treebank from which the token comes from. Depending on the mix of treebanks, the treebank embedding can encode aspects such as differences between languages, domains, and annotation style. Parsing with treebank embeddings has previously been applied monolingually (Stymne et al., 2018; Wagner et al., 2020) and cross-lingually for related languages, but without taking domain into account (Smith et al., 2018a; Lim et al., 2018),2 In this paper, we show that joint training with treebank embeddings can be applied simultaneously across both across languages and domains, in effect addressing the task of cross-lingual domain adaptation. It is a simple and efficient method, which does not require expensive pre-processing, pre-training, translation, or similar tasks required by many other cross-lingual approaches, while giving competitive results across many settings. In this work we explore how such a"
2021.nodalida-main.32,W17-0401,0,0.0513461,"Missing"
2021.nodalida-main.32,J19-2006,0,0.123077,"an and Yang, 2010). Determining the optimal transfer language for any target language is a crucial step usually leading to the selection of a language that belongs to the same language family as the target language (Dong et al., 2015; Guo et al., 2016; Dehouck and Denis, 2019). However, language proximity is not always the best criterion, since there are other properties that 1 often mentioned as cross-lingual dependency Parsing could lead to better results such as the content of the syntactic, geographical, or phonological distances, which is confirmed by studies both in Machine Translation (Bjerva et al., 2019) and Syntactic Parsing (Lin et al., 2019). Smith et al. (2018) noted that for Latin, it was useful to group it with other ancient languages such as ancient Greek and Gothic, but they did not provide a comparison with other potential transfer languages. We perform an investigation of parsing Latin in a low-resource setting, with the goal of investigating if Greek languages are better as transfer languages than Italic languages. We also explore the role of factors such as treebank size, treebank content and linguistic distance measures. We find that ancient Greek, and also modern Greek, are inde"
2021.nodalida-main.32,N19-1017,0,0.119001,"of multilingual dependency parsing1 (Ammar et al., 2016; Ponti et al., 2018) and especially the use of transfer learning in low-resource scenarios (Guo et al., 2015; Ponti et al., 2018). Transfer learning in the context of parsing lowresource languages uses knowledge from a transfer language in order to parse the low-resource target language (Pan and Yang, 2010). Determining the optimal transfer language for any target language is a crucial step usually leading to the selection of a language that belongs to the same language family as the target language (Dong et al., 2015; Guo et al., 2016; Dehouck and Denis, 2019). However, language proximity is not always the best criterion, since there are other properties that 1 often mentioned as cross-lingual dependency Parsing could lead to better results such as the content of the syntactic, geographical, or phonological distances, which is confirmed by studies both in Machine Translation (Bjerva et al., 2019) and Syntactic Parsing (Lin et al., 2019). Smith et al. (2018) noted that for Latin, it was useful to group it with other ancient languages such as ancient Greek and Gothic, but they did not provide a comparison with other potential transfer languages. We p"
2021.nodalida-main.32,P15-1166,0,0.0149826,"iple projects exploiting the benefits of multilingual dependency parsing1 (Ammar et al., 2016; Ponti et al., 2018) and especially the use of transfer learning in low-resource scenarios (Guo et al., 2015; Ponti et al., 2018). Transfer learning in the context of parsing lowresource languages uses knowledge from a transfer language in order to parse the low-resource target language (Pan and Yang, 2010). Determining the optimal transfer language for any target language is a crucial step usually leading to the selection of a language that belongs to the same language family as the target language (Dong et al., 2015; Guo et al., 2016; Dehouck and Denis, 2019). However, language proximity is not always the best criterion, since there are other properties that 1 often mentioned as cross-lingual dependency Parsing could lead to better results such as the content of the syntactic, geographical, or phonological distances, which is confirmed by studies both in Machine Translation (Bjerva et al., 2019) and Syntactic Parsing (Lin et al., 2019). Smith et al. (2018) noted that for Latin, it was useful to group it with other ancient languages such as ancient Greek and Gothic, but they did not provide a comparison w"
2021.nodalida-main.32,P15-1119,0,0.0311909,"ed Italic languages, and show that this is indeed the case. We further investigate the influence of other factors including training set size and content as well as linguistic distances. We find that one explanatory factor seems to be the syntactic similarity between Latin and Ancient Greek. The influence of genres or shared annotation projects seems to have a smaller impact. 1 Introduction There have been multiple projects exploiting the benefits of multilingual dependency parsing1 (Ammar et al., 2016; Ponti et al., 2018) and especially the use of transfer learning in low-resource scenarios (Guo et al., 2015; Ponti et al., 2018). Transfer learning in the context of parsing lowresource languages uses knowledge from a transfer language in order to parse the low-resource target language (Pan and Yang, 2010). Determining the optimal transfer language for any target language is a crucial step usually leading to the selection of a language that belongs to the same language family as the target language (Dong et al., 2015; Guo et al., 2016; Dehouck and Denis, 2019). However, language proximity is not always the best criterion, since there are other properties that 1 often mentioned as cross-lingual depe"
2021.nodalida-main.32,W17-6314,1,0.849742,"Missing"
2021.nodalida-main.32,2020.coling-main.345,0,0.0278572,"Missing"
2021.nodalida-main.32,E17-2002,0,0.159103,"ource scenario, we use 250 and 500 sentences, respectively, while we use 2.5K and 10K sentences for the transfer languages. In the latter scenario we focus on Italian and ancient Greek, due to the small size of the modern Greek treebank and the poor performance with French as a target language. Table 1 contains information about the treebanks. All development and test sets include 250 sentences. 3.3 Linguistic Distances Linguistic distance defines how distant a set of languages is based on genealogical, geographical, or typological features created with linguistic analysis (Lin et al., 2019). Littell et al. (2017) provide various vector information on linguistic features in URIEL Typological database which can be used to calculate how distant are the languages.3 In this work using the URIEL database we use the following linguistic distances:4 • Geographic distance (dgeo ): The spherical distance among languages on Earth’s surface, divided by the diametrically opposite Earth’s distance. The language points are abstractions, and not precise facts, derived from 3 https://github.com/antonisa/lang2vec 4 Inventory distance was not used in this study, since it is similar to phonological distance, but the phon"
2021.nodalida-main.32,2020.lrec-1.497,0,0.0609997,"Missing"
2021.nodalida-main.32,P18-1142,0,0.048006,"Missing"
2021.nodalida-main.32,P15-2040,0,0.0287719,"Missing"
2021.nodalida-main.32,K18-2011,1,0.885115,"Missing"
2021.nodalida-main.32,C12-2115,0,0.0751412,"notation projects. These results also hold for different training data sizes. Finally we note that ancient Greek is syntactically more similar to Latin than Italian, which can be an explanatory factor. 2 Related Work Multilingual parsing has been an active topic of research over the last decade, but there is a limited number of studies that focus on transfer language selection. There are works that include language selection techniques for dependency parsing such as using a typological database to choose transfer languages based on their typological weight similarities to the target language (Søgaard and Wulff, 2012). Similarly, Agi´c (2017) use a partof-speech sequence similarity method between the source and target language. A more detailed investigation on transfer language selection is performed by Lin et al. (2019). They attempt to build models that rank languages based on linguistic distance measures in order to predict the optimal transfer languages. Another option is to choose the most suitable single-source parser among a set of parsers, either at the level of language (Rosa ˇ and Zabokrtsk´ y, 2015) or for individual sentences (Litschko et al., 2020), often based on part-ofspeech patterns. 3 Exp"
2021.nodalida-main.32,P18-2098,1,0.872407,"Missing"
2021.semeval-1.15,W19-5804,0,0.0347544,"Missing"
2021.semeval-1.15,2020.acl-main.747,0,0.0515982,"Missing"
2021.semeval-1.15,N19-1423,0,0.0087621,"both English and multilingual word sense disambiguation (WSD) tasks, and showed that SensEmBERT is able to achieve state-of-the-art result on both English and multilingual WSD datasets. 3 XLMR XLMR (XLM-RoBERTa) is a scaled cross-lingual sentence encoder (Conneau et al., 2020), which is trained on 2.5T of data obtained from Common Crawl that covers more than 100 languages. XLMR has achieved state-of-the-art results on various cross-lingual NLP tasks. 3.2 mBERT mBERT (multilingual BERT) is pre-trained on the largest Wikipedias (Libovick´y et al., 2019). It is a multilingual extension of BERT (Devlin et al., 2019) that provides word and sentence representations for 104 languages, which has been shown to be capable of clustering polysemic words into distinct sense regions in the embedding space (Wiedemann et al., 2019). Related Work In WiC at SemDeep-5, many participating systems capitalized on contextualized word representations. The LMMS (Language Modelling Makes Sense) system by Loureiro and Jorge (2019) used word embeddings from BERT, together with sense embeddings from WordNet 3.0 (Marciniak, 2020). Ansell et al. (2019) used the contextualized representations from ELMo (Peters et al., 2018) and tra"
2021.semeval-1.15,D18-2012,0,0.0339986,"Missing"
2021.semeval-1.15,P19-1493,0,0.0394467,"Missing"
2021.semeval-1.15,P19-1569,0,0.0237384,"Missing"
2021.semeval-1.15,2020.mmw-1.5,0,0.0326221,"Missing"
2021.semeval-1.15,2021.semeval-1.3,0,0.0266269,"case we also experiment with using dependency-based information. We find that fine-tuning is better than feature extraction. XLMR performs better than mBERT in the cross-lingual setting both with fine-tuning and feature extraction, whereas these two models give a similar performance in the multilingual setting. mDistilBERT performs poorly with fine-tuning but gives similar results to the other models when used as a feature extractor. We submitted our two best systems, fine-tuned with XLMR and mBERT. 1 SemEval-2021 Task 2: Multilingual and Crosslingual Word-in-Context Disambiguation (MCLWiC) (Martelli et al., 2021) is an extension from WiC (Pilehvar and Camacho-Collados, 2019), a shared task at the IJCAI-19 SemDeep workshop (SemDeep-5). WiC was proposed as a benchmark to evaluate context-sensitive word representations. The WiC dataset1 consists of a list of English sentence-pairs. Each sentence-pair has a target word, and the task is to determine whether the target word is used in the same meaning or different meanings in the two sentences, thus as a binary classification task. MCL-WiC extends WiC to multilingual and cross-lingual datasets,2 and covers 5 https://pilehvar.github.io/wic/. https://github.c"
2021.semeval-1.15,N18-1202,0,0.116484,"Missing"
2021.semeval-1.15,N19-1128,0,0.0316456,"Missing"
2021.semeval-1.15,W19-5802,0,0.0261491,"Missing"
2021.unimplicit-1.7,W18-0528,0,0.0146796,"the training data. The development and test data was verified by human annotators (see Roth and Anthonio, 2021, for details). There are two subsets: • Sentences extracted from the revision history, which later received edits which made the sentence more precise. These are labelled REQ REVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of sentences 39187 3264 3458 Table 2: Statistics of the dataset. Table 1: Examples with generic pronouns that require revision. 2 Req Revision 19599 1632 • Sentences that remained unchanged over multiple revisions of the article. The"
2021.unimplicit-1.7,2020.lrec-1.702,0,0.0355272,"EVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of sentences 39187 3264 3458 Table 2: Statistics of the dataset. Table 1: Examples with generic pronouns that require revision. 2 Req Revision 19599 1632 • Sentences that remained unchanged over multiple revisions of the article. These are labelled KEEP UNREVIS. The dataset includes training, development and test sets. However, the type of edit in case of a revision and the revised version of the target sentence, are available only for the training set. We therefore used k-fold cross-validation to randomly partiti"
2021.unimplicit-1.7,2020.emnlp-main.675,0,0.0610568,"Missing"
2021.unimplicit-1.7,N19-1423,0,0.0177259,"beddings. Therefore, we concatenated the BERT embeddings and neuralcoref vectors. the dimensions of the concatenated output vector are 1418. Mention Extraction 5 Mention BERT Embeddings 5 https://github.com/huggingface/neuralcoref. 60 https://github.com/hanxiao/bert-as-service Model M MB M+MB training data, where we use cross-validation, we report the average scores across the five folds. 6.2 Recall 0.2000 0.6783 0.7273 F1 -score 0.0510 0.3799 0.3797 Acc 0.7123 0.6772 0.6523 Sentence-Level System Table 4: Results of our models on mention-level. For the sentence-level system, we use BERT-Base (Devlin et al., 2019), uncased model (12 transformer blocks, 768 hidden size, 12 attention heads and 110M parameters) fine-tuned with an additional output layer on top of BERT’s final representation. We use the Huggingface Transformers library with TensorFlow and load a pre-trained BERT from the Transformers library. We train this model for 2 epochs with a learning rate of 3 · 10−5 and batch size 32. The mention-level system does not have extracted mentions for all sentences, and therefore does not provide predictions for all sentences. In our combined system we use the predictions from the mention-level system as"
2021.unimplicit-1.7,2021.unimplicit-1.4,0,0.0796605,"un phrase. As a result, our proposed classification model for the task of Revision Requirements Detection is based on extracting mention embeddings for each sentence The Revision Requirements task aims to recognize whether or not a sentence requires revision. Revision Requirements prediction not only acts as a standalone tool for grammar correction but also has potential applications in natural language processing (NLP) such as ambiguity detection, machine translation refinement, sentence understanding, knowledge base construction, etc. The shared task on implicit and underspecified language (Roth and Anthonio, 2021)1 aims to provide a binary classification for revision requirements to make a prediction of whether sentences in instructional texts require revision to improve understandability. Since instructional texts must be clear enough so that readers and machines can actually achieve the goal described by the instructions, this task focuses on modeling implicit elements that make the sentence more precise and clear. The dataset used in this shared task consists of instances from wikiHowToImprove, a collection of instructional texts, which has recently been introduced 1 https://unimplicit.github.io 58"
2021.unimplicit-1.7,P14-2066,0,0.0142564,"ow.com articles. These how-to articles cover many fields such as Arts and Entertainment, Computers and Electronics, Health, along with their revision history. The revisions and classes were extracted automatically from the training data. The development and test data was verified by human annotators (see Roth and Anthonio, 2021, for details). There are two subsets: • Sentences extracted from the revision history, which later received edits which made the sentence more precise. These are labelled REQ REVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of senten"
2021.unimplicit-1.7,P17-1144,0,0.0224886,"ubsets: • Sentences extracted from the revision history, which later received edits which made the sentence more precise. These are labelled REQ REVISION. Related Work There has been a lot of work on revisions to improve understandability, Tan and Lee (2014) conducted research on revisions in academic writing, using a qualitative approach to distinguish between strong and weak sentences, by analyzing the differences in the original and revised sentences. Afrin and Litman (2018) introduced a classification model based on Random Forest (RF) for revisions in argumentative essays from ArgRewrite (Zhang et al., 2017) to examine whether we can predict improvement for non-expert and predict if the revised sentence is better than the original. Anthonio et al. (2020) worked with edits in instructional texts and applied a supervised learning 2 N. of sentences 39187 3264 3458 Table 2: Statistics of the dataset. Table 1: Examples with generic pronouns that require revision. 2 Req Revision 19599 1632 • Sentences that remained unchanged over multiple revisions of the article. These are labelled KEEP UNREVIS. The dataset includes training, development and test sets. However, the type of edit in case of a revision a"
2021.vardial-1.5,P15-2044,0,0.0662714,"Missing"
2021.vardial-1.5,C18-1139,0,0.0203893,"Missing"
2021.vardial-1.5,W13-3520,0,0.260854,"cent years, Scottish identity has experienced a revival, and with it interest in the Scots language. Scots is reemerging as an active language, especially amongst the young rural population of Scotland and amongst Scottish nationalists (Lemeshchenko-Lagoda, 2019). Although this has generated academic interest in the fields of phonology (Lawson et al., 2019) and socio-linguistics (Shoemark et al., 2017a,b), no NLP-tools are available specifically for Scots. The only Scots NLP efforts are two sets of word embeddings, namely Fasttext embeddings (Joulin et al., 2016) and Polyglot Word Embeddings (Al-Rfou et al., 2013). There are several complications relating to NLP for Scots. Despite attempts at standardization by the Report & Recommends o the Scots Spellin Comatee (Allan et al., 1998), Scots does not have a de jure standard. As there is no official orthography, major variations in spelling exist, mostly depending on the written dialect of Scots. Additionally, almost no parallel English-Scots texts are publicly available, which excludes approaches relying on such data. Scots grammar and morphology, while similar to and not necessarily more or less complex, differs from English grammar and morphology. Some"
2021.vardial-1.5,P16-1231,0,0.0191048,"Missing"
2021.vardial-1.5,W13-2302,0,0.0746829,"Missing"
2021.vardial-1.5,W18-3401,0,0.0250843,"Missing"
2021.vardial-1.5,C18-1111,0,0.0403752,"Missing"
2021.vardial-1.5,D19-1146,0,0.0394088,"Missing"
2021.vardial-1.5,D17-1302,0,0.016054,"EWT data set containing web data, performed better than the GUM Treebank with mostly edited data in a zero-shot setting, the GUM Treebank, trained on a mix of genres performed better in the cross-lingual setting. While greatly improving on the zero-shot model and slightly improving on the Scots-only model, the cross-lingual models for both data sets scored lower than other studies that examine low-resource settings for PoS tagging. Longer training, higher dimensional embeddings and modifications to the model architecture such as private LSTMs and the use of adversarial training could be made. Kim et al. (2017) achieve higher accuracies for all languages using 320 tokens per language, showing the efficacy of the combination of these factors. An improvement on the model architecture can be made as well by, rather than weighting English and Scots data equally in the loss function as was done in these experiments, weighting the Scots data more heavily in the loss function of the Bi-LSTM in order to prevent underfitting on the Scots data. Another possible cause of these results is the previously mentioned questionable quality of the word embeddings, and we think it would be interesting to train Scots wo"
2021.vardial-1.5,P11-3010,0,0.0878903,"Missing"
2021.vardial-1.5,P17-2093,0,0.066847,"Missing"
2021.vardial-1.5,P13-1057,0,0.0602026,"Missing"
2021.vardial-1.5,D17-1256,0,0.0356717,"Missing"
2021.vardial-1.5,D07-1041,0,0.148099,"Missing"
2021.vardial-1.5,P16-2067,0,0.0909396,". In: The ploy wasnae faur awa fae wirkin — ”The ploy nearly worked”, the tokens faur ”far” and awa ”away” were initially tagged as ADJ, as wasnae — ”was not” is a frequently used copula verb that is often followed by an adjective phrase. Awa only has ADV entries in the Dictionary of the Scots Language,4 however, and faur modifies awa making faur another ADV. Faur awa was therefore reinterpreted as an adverbial phrase and both words were tagged ADV. An example of a tagged Scots sentence can be found in Figure 2. 4.3 settings, bar the addition of the polyglot embeddings (Al-Rfou et al., 2013). Plank et al. (2016)’s tagger uses a Bi-LSTM for character embeddings, the output of which is concatenated with the word embeddings. These embeddings are the input for the BiLSTM which has the softmax for the PoS tags as an output layer. The network used 100-dimensional character embeddings, 100 dimensional polyglot embeddings, 100-dimensional LSTM hidden states and had one hidden-layer for the encoder and the decoder. During training, stochastic gradient descent was applied with a dropout rate of 0.25 as well as a sigma of 0.2 Gaussian noise. The network used a softmax activation function in the decoder to gener"
2021.vardial-1.5,N16-1130,0,0.0670173,"Missing"
2021.vardial-1.5,W17-4908,0,0.0377442,"Missing"
2021.vardial-1.5,E17-1116,0,0.0350893,"Missing"
2021.vardial-1.5,silveira-etal-2014-gold,0,0.0523811,"Missing"
2021.vardial-1.5,H01-1035,0,0.372892,"Missing"
D18-1291,D15-1041,0,0.276216,"rsity Abstract 2017). When task-specific training data is scarce or the morphological complexity of a language leads to sparsity at the word-type level, word embeddings often need to be augmented with sub-word or partof-speech (POS) tag information in order to release their full power (Kim et al., 2016; Sennrich et al., 2016; Chen and Manning, 2014). Initialising vectors with embeddings trained for a different task, typically language modelling, on huge unlabelled corpora has also been shown to improve results significantly (Dhingra et al., 2017a). In dependency parsing, the use of character (Ballesteros et al., 2015) and POS (Dyer et al., 2015) models is widespread, and the majority of parsers make use of pre-trained word embeddings (Zeman et al., 2017). We provide a comprehensive analysis of the interactions between pre-trained word embeddings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces large improvements over a baseline system using randomly initialised word embe"
D18-1291,D14-1082,0,0.136168,"s • For all techniques, improvements are largest for low-frequency and open-class words and for morphologically rich languages. • These improvements are largely redundant when the techniques are used together. • Character-based models are the most effective technique for low-frequency words. • Part-of-speech tags are potentially very effective for high-frequency function words, but current state-of-the-art taggers are not accurate enough to take full advantage of this. • Large character embeddings are helpful for morphologically rich languages, regardless of character set size. 2 Related Work Chen and Manning (2014) introduced POS tag embeddings: a learned dense representation of each tag designed to exploit semantic similarities between tags. In their greedy transition-based parser, the inclusion of these POS tag embeddings improved labelled attachment score (LAS) by 1.7 on the English Penn Treebank (ETB) and almost 10 on the Chinese Penn Treebank (CTB). They also tested the use of pre-trained word embeddings for initialisation of word vectors, finding gains of 0.7 for PTB and 1.7 for CTB. Dyer et al. (2015) in their Stack Long ShortTerm Memory (LSTM) dependency parser, show that POS tag embeddings in t"
D18-1291,P17-1168,0,0.121491,"e Joakim Nivre Department of Linguistics and Philology, Uppsala University Abstract 2017). When task-specific training data is scarce or the morphological complexity of a language leads to sparsity at the word-type level, word embeddings often need to be augmented with sub-word or partof-speech (POS) tag information in order to release their full power (Kim et al., 2016; Sennrich et al., 2016; Chen and Manning, 2014). Initialising vectors with embeddings trained for a different task, typically language modelling, on huge unlabelled corpora has also been shown to improve results significantly (Dhingra et al., 2017a). In dependency parsing, the use of character (Ballesteros et al., 2015) and POS (Dyer et al., 2015) models is widespread, and the majority of parsers make use of pre-trained word embeddings (Zeman et al., 2017). We provide a comprehensive analysis of the interactions between pre-trained word embeddings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces larg"
D18-1291,K17-3002,0,0.295654,"ng to reading comprehension to machine translation, a unique dense vector is learned for each word type in the training data. These word embeddings have been shown to capture essential semantic and morphological relationships between words (Mikolov et al., 2013), and have precipitated the enormous success of neural network-based architectures across a wide variety of NLP tasks (Plank et al., 2016; Dhingra et al., 2017b; Vaswani et al., While previous research has examined in detail the benefits of character and POS models in dependency parsing and their interactions (Ballesteros et al., 2015; Dozat et al., 2017), there has been no systematic investigation into the way these techniques combine with the use of pretrained embeddings. Our results suggest a large amount of redundancy between all three techniques: in isolation, each gives large improvements over a simple baseline model, but these improvements are not additive. In fact combining any two of the three methods gives similar results, close to the performance of the fully combined system. We set out to systematically investigate the ways in which pre-trained embeddings, character and POS models contribute to improving parser quality. We break do"
D18-1291,P15-1033,0,0.397559,"cific training data is scarce or the morphological complexity of a language leads to sparsity at the word-type level, word embeddings often need to be augmented with sub-word or partof-speech (POS) tag information in order to release their full power (Kim et al., 2016; Sennrich et al., 2016; Chen and Manning, 2014). Initialising vectors with embeddings trained for a different task, typically language modelling, on huge unlabelled corpora has also been shown to improve results significantly (Dhingra et al., 2017a). In dependency parsing, the use of character (Ballesteros et al., 2015) and POS (Dyer et al., 2015) models is widespread, and the majority of parsers make use of pre-trained word embeddings (Zeman et al., 2017). We provide a comprehensive analysis of the interactions between pre-trained word embeddings, character models and POS tags in a transition-based dependency parser. While previous studies have shown POS information to be less important in the presence of character models, we show that in fact there are complex interactions between all three techniques. In isolation each produces large improvements over a baseline system using randomly initialised word embeddings only, but combining t"
D18-1291,Q16-1023,0,0.397497,"vel as a model using a combination of word embeddings and POS tags. Combining character and POS models produced even better results, but they conclude that POS tags are less important for character-based parsers. They also showed that character models are particularly effective for morphologically rich languages, but that performance remains good in languages with little morphology, and that character models help substantially with out-of-vocabulary (OOV) words, but that this does not fully explain the improvements they bring. The use of pretrained embeddings was not considered in their work. Kiperwasser and Goldberg (2016), in the transition-based version of their parser based on BiLSTM feature extractors, found that POS tags improved performance by 0.3 LAS for English and 4.4 LAS for Chinese. Like Dyer et al. (2015), they concatenate a randomly-initialised word embeddings to a pre-trained word vector; however in this case the pre-trained vector is also updated during training. They find that this helps LAS by 0.5–0.7 for English and 0.9–1.2 for Chinese, depending on the specific architecture of their system. Dozat et al. (2017), building on the graph-based version of Kiperwasser and Goldberg (2016), confirmed"
D18-1291,K17-3022,1,0.633554,"Missing"
D18-1291,W17-6314,1,0.693983,"Missing"
D18-1291,K18-2011,1,0.828401,"Missing"
D18-1291,L16-1680,0,0.0506962,"Missing"
D18-1291,K17-3009,0,0.0291312,"Missing"
D18-1291,J08-4003,1,0.758759,"both transition- and graph-based dependency parsing, and has quickly become a de facto standard in the field (Zeman et al., 2017). In a K&G parser, BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) are employed to learn useful representations of tokens in context. A multi-layer perceptron (MLP) is trained to predict transitions and possible arc labels, taking as input the BiLSTM vectors of a few tokens at a time. Crucially, the BiLSTMs and MLP are trained together, enabling the parser to learn very effective token representations for parsing. For further details we refer the reader to Nivre (2008) and Kiperwasser and Goldberg (2016), for transition-based parsing and BiLSTM feature extractors, respectively. Our version of the K&G parser is extended with a S WAP transition to facilitate the construction 1 https://github.com/UppsalaNLP/ uuparser 2712 of non-projective dependency trees (Nivre, 2009). We use a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time, as described in de Lhoneux et al. (2017b). In this paper we experiment with a total of eight variations of the parser, where the d"
D18-1291,P09-1040,1,0.827167,"is trained to predict transitions and possible arc labels, taking as input the BiLSTM vectors of a few tokens at a time. Crucially, the BiLSTMs and MLP are trained together, enabling the parser to learn very effective token representations for parsing. For further details we refer the reader to Nivre (2008) and Kiperwasser and Goldberg (2016), for transition-based parsing and BiLSTM feature extractors, respectively. Our version of the K&G parser is extended with a S WAP transition to facilitate the construction 1 https://github.com/UppsalaNLP/ uuparser 2712 of non-projective dependency trees (Nivre, 2009). We use a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time, as described in de Lhoneux et al. (2017b). In this paper we experiment with a total of eight variations of the parser, where the difference between each version resides in the vector representations xi of word types wi before they are passed to the BiLSTM feature extractors (see Section 3 of Kiperwasser and Goldberg (2016)). In the simplest case, we set xi equal to the word embedding er (wi ): xi = er (wi ) The superscript r refer"
D18-1291,K17-3001,1,0.867824,"Missing"
D18-1291,P16-2067,0,0.023463,"y rich languages. 1 Introduction The last few years of research in natural language processing (NLP) have witnessed an explosion in the application of neural networks and word embeddings. In tasks ranging from POS tagging to reading comprehension to machine translation, a unique dense vector is learned for each word type in the training data. These word embeddings have been shown to capture essential semantic and morphological relationships between words (Mikolov et al., 2013), and have precipitated the enormous success of neural network-based architectures across a wide variety of NLP tasks (Plank et al., 2016; Dhingra et al., 2017b; Vaswani et al., While previous research has examined in detail the benefits of character and POS models in dependency parsing and their interactions (Ballesteros et al., 2015; Dozat et al., 2017), there has been no systematic investigation into the way these techniques combine with the use of pretrained embeddings. Our results suggest a large amount of redundancy between all three techniques: in isolation, each gives large improvements over a simple baseline model, but these improvements are not additive. In fact combining any two of the three methods gives similar res"
D18-1291,D17-1035,0,0.0266976,"factors. In the frequency and POS tag cases, we want to examine the overall contribution to LAS of words from each category. We expect changing the representation of a token to affect how likely it is to be assigned the correct head in the dependency tree, but also how likely it is to be assigned correctly as the head of other words. We thus introduce a new metric for this part of the analysis: the head and dependents labelled attachment score, which we refer to as HD3 Changing the random seed has been shown to produce results that appear statistically significant different in neural systems (Reimers and Gurevych, 2017). 4 Available at https://web.stanford.edu/˜tdozat/. LAS. When calculating HDLAS, the dependency analysis for a given token is only considered correct if the token has the correct labelled head and the complete set of correctly labelled dependents. This is a harsher metric than LAS, which only considers whether a token has the correct labelled head. Note that when calculating HDLAS for all tokens in a sentence, each dependency relation is counted twice, once for the head word and once for the dependent. It only makes sense to use this metric when analysing individual tokens in a sentence, or wh"
D18-1291,P16-1162,0,0.060115,"Missing"
E09-3008,2005.mtsummit-papers.11,0,0.0350387,"Missing"
E09-3008,P08-2064,0,0.00755441,"Missing"
E09-3008,J03-1002,0,0.00412055,"two automatic metrics. In addition the performance of the merging algorithms are analysed in some more detail. In both cases the effect of the POS sequence model is also discussed. Even when the POS sequence model is not used, part-of-speech is carried through the translation process, so that it can be used in the merging step. Table 3: Type and token counts for the 701157 sentence training corpus ing, both for uppercasing German nouns and as a knowledge source for compound merging. The tools used are the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. A 5-gram model is used for surface form, and a 7-gram model is used for part-of-speech. To tune feature weights minimum error rate training is used (Och, 2003), optimized against the Neva metric (Forsbom, 2003). Compound splitting is performed on the training corpus, prior to training. Merging is performed after translation, both for test, and incorporated into the tuning step. 4.1 5.1 Evaluation of Translation Translations are evaluated on two automatic metrics: Bleu (Papineni et al., 2002) and PER, position independent error-rate (Tillmann et"
E09-3008,P03-1021,0,0.0299854,"not used, part-of-speech is carried through the translation process, so that it can be used in the merging step. Table 3: Type and token counts for the 701157 sentence training corpus ing, both for uppercasing German nouns and as a knowledge source for compound merging. The tools used are the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. A 5-gram model is used for surface form, and a 7-gram model is used for part-of-speech. To tune feature weights minimum error rate training is used (Och, 2003), optimized against the Neva metric (Forsbom, 2003). Compound splitting is performed on the training corpus, prior to training. Merging is performed after translation, both for test, and incorporated into the tuning step. 4.1 5.1 Evaluation of Translation Translations are evaluated on two automatic metrics: Bleu (Papineni et al., 2002) and PER, position independent error-rate (Tillmann et al., 1997). Case-sensitive versions of the metrics are used. PER does not consider word order, it evaluates the translation as a bag-of-word, and thus the systems without part-of-speech sequence models can be"
E09-3008,E06-1032,0,0.00641502,"Missing"
E09-3008,P02-1040,0,0.116262,"erman. Popovi´c et al. (2006) suggested using compound splitting to improve alignment, or to merge English compounds prior to training. Some work has discussed merging of not only compounds, but of all morphs. Virpioja et al. (2007) merged translation output that was split into morphs for Finnish, Swedish and Danish. They marked split parts with a symbol, and merged every word in the output which had this symbol with the next word. If morphs were misplaced in the translation output, they were merged anyway, possibly creating non-existent words. This system was worse than the baseline on Bleu (Papineni et al., 2002), but an error analysis showed some improvements. El-Kahlout and Oflazer (2006), discuss merging of morphs in Turkish. They also mark morphs with a symbol, and in addition normalize affixes to standard form. In the merging Related Work Splitting German compounds into their parts prior to translation has been suggested by many researchers. Koehn and Knight (2003) presented an empirical splitting algorithm that is used to improve translation from German to English. They split all words in all possible places, and considered a splitting option valid if all the parts are existing words in a monoli"
E09-3008,D08-1064,0,0.0103697,"Missing"
E09-3008,W06-3102,0,0.214628,"Missing"
E09-3008,D07-1091,0,0.0126866,"e 1. For all algorithms, compounds can have an arbitrary number of parts. If there is a marked compound part that cannot be combined with the next word, in any of the algorithms, the markup is removed, and the part is left as a single word. For the sepmarked system, coordinated compounds are handled as part of the symbol algorithms, by using the special markup symbol that indicates them. 3.2 4 System Description The translation system used is a factored phrasebased translation system. In a factored translation model other factors than surface form can be used, such as lemma or part-of-speech (Koehn and Hoang, 2007). In the current system part-ofspeech is used only as an output factor in the target language. Besides the standard language model a sequence model on part-of-speech is used, which can be expected to lead to better word order in the translation output. There are no input factors, so no tagging has to be performed prior to translation, only the training corpus needs to be tagged. In addition, the computational overhead is small. One possible benefit gained by using part-of-speech as an output factor is that ordering, both in general, and of compound parts, can be improved. This hypothesis is te"
E09-3008,2008.eamt-1.25,1,0.855613,"s, or part-of-speech tags. Choices made at split-time influence which internal knowledge sources are available at mergetime. I will explore and compare three markup schemes for compound parts, and eight merging algorithms that use different combinations of knowledge sources. 2 with the next word if it resulted in a known compound. They only discussed merging of binary compounds. The drawback of this method is that novel compounds cannot be merged. Nevertheless, this strategy led to improved translation measured by three automatic metrics. In a study of translation between English and Swedish, Stymne and Holmqvist (2008) suggested a merging algorithm based on part-of-speech, which can be used in a factored translation system with part-of-speech as an output factor. Compound parts had special part-of-speech tags based on the head of the compound, and merging was performed if that part-of-speech tag matched that of the following word. When compound forms had been normalized the correct compound form was found by using frequency lists of parts and words compiled at split-time. This method can merge unseen compounds, and the tendency to merge too much is reduced by the restriction that POS-tags need to match. In"
E09-3008,E03-1076,0,0.393735,"erged every word in the output which had this symbol with the next word. If morphs were misplaced in the translation output, they were merged anyway, possibly creating non-existent words. This system was worse than the baseline on Bleu (Papineni et al., 2002), but an error analysis showed some improvements. El-Kahlout and Oflazer (2006), discuss merging of morphs in Turkish. They also mark morphs with a symbol, and in addition normalize affixes to standard form. In the merging Related Work Splitting German compounds into their parts prior to translation has been suggested by many researchers. Koehn and Knight (2003) presented an empirical splitting algorithm that is used to improve translation from German to English. They split all words in all possible places, and considered a splitting option valid if all the parts are existing words in a monolingual corpus. They allowed the addition of -s or -es at all splitting points. If there were several valid splitting options they chose one based on the number of splits, the geometric mean of part frequencies or based on alignment data. Stymne (2008) extended this algorithm in a number of ways, for instance by allowing more compound forms. She found that for tra"
E09-3008,P07-2045,0,0.0225445,"merging algorithms on the overall translation quality is evaluated, using two automatic metrics. In addition the performance of the merging algorithms are analysed in some more detail. In both cases the effect of the POS sequence model is also discussed. Even when the POS sequence model is not used, part-of-speech is carried through the translation process, so that it can be used in the merging step. Table 3: Type and token counts for the 701157 sentence training corpus ing, both for uppercasing German nouns and as a knowledge source for compound merging. The tools used are the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. A 5-gram model is used for surface form, and a 7-gram model is used for part-of-speech. To tune feature weights minimum error rate training is used (Och, 2003), optimized against the Neva metric (Forsbom, 2003). Compound splitting is performed on the training corpus, prior to training. Merging is performed after translation, both for test, and incorporated into the tuning step. 4.1 5.1 Evaluation of Translation Translations are evaluated on two automatic metrics: Bleu (Papi"
E09-3008,2007.mtsummit-papers.65,0,0.119425,"Missing"
E09-3008,W08-0318,0,0.109455,"ecial part-of-speech tags based on the head of the compound, and merging was performed if that part-of-speech tag matched that of the following word. When compound forms had been normalized the correct compound form was found by using frequency lists of parts and words compiled at split-time. This method can merge unseen compounds, and the tendency to merge too much is reduced by the restriction that POS-tags need to match. In addition coordinated compounds were handled by the algorithm. This strategy resulted in improved scores on automatic metrics, which were confirmed by an error analysis. Koehn et al. (2008) discussed treatment of hyphened compounds in translation into German by splitting at hyphens and treat the hyphen as a separate token, marked by a symbol. The impact on translation results was small. There are also other ways of using compound processing to improve SMT into German. Popovi´c et al. (2006) suggested using compound splitting to improve alignment, or to merge English compounds prior to training. Some work has discussed merging of not only compounds, but of all morphs. Virpioja et al. (2007) merged translation output that was split into morphs for Finnish, Swedish and Danish. They"
holmqvist-etal-2012-alignment,W09-0421,1,\N,Missing
holmqvist-etal-2012-alignment,J93-2003,0,\N,Missing
holmqvist-etal-2012-alignment,C04-1073,0,\N,Missing
holmqvist-etal-2012-alignment,W08-0406,0,\N,Missing
holmqvist-etal-2012-alignment,C08-1027,0,\N,Missing
holmqvist-etal-2012-alignment,P02-1040,0,\N,Missing
holmqvist-etal-2012-alignment,W08-0312,0,\N,Missing
holmqvist-etal-2012-alignment,W09-0401,0,\N,Missing
holmqvist-etal-2012-alignment,P07-2045,0,\N,Missing
holmqvist-etal-2012-alignment,P10-2033,0,\N,Missing
holmqvist-etal-2012-alignment,J03-1002,0,\N,Missing
holmqvist-etal-2012-alignment,P11-4010,1,\N,Missing
holmqvist-etal-2012-alignment,P05-1066,0,\N,Missing
holmqvist-etal-2012-alignment,W05-0908,0,\N,Missing
holmqvist-etal-2012-alignment,W11-1007,0,\N,Missing
holmqvist-etal-2012-alignment,2005.iwslt-1.8,0,\N,Missing
J13-4009,W05-0909,0,0.177447,"Missing"
J13-4009,D08-1078,0,0.0530341,"Missing"
J13-4009,C12-1022,0,0.0361736,"Missing"
J13-4009,2002.tmi-papers.3,0,0.0382296,"Missing"
J13-4009,P08-1009,0,0.0597835,"Missing"
J13-4009,P11-2031,0,0.0550678,"Missing"
J13-4009,W02-1001,0,0.0443174,"Missing"
J13-4009,J05-1003,0,0.0266713,"Missing"
J13-4009,A92-1018,0,0.48823,"Missing"
J13-4009,N09-1046,0,0.0911061,"Missing"
J13-4009,W06-3102,0,0.0630632,"Missing"
J13-4009,W09-0420,0,0.0373189,"Missing"
J13-4009,W07-2432,0,0.0591911,"Missing"
J13-4009,W10-1734,0,0.510195,"Missing"
J13-4009,W07-0723,1,0.89382,"Missing"
J13-4009,2005.mtsummit-papers.11,0,0.116679,"Missing"
J13-4009,W08-0318,0,0.0527943,"Missing"
J13-4009,D07-1091,0,0.146081,"Missing"
J13-4009,E03-1076,0,0.173848,"the desired compounds are missing in the training data or that they have not been aligned correctly. When a compound is the idiomatic word choice in the translation, systems often produce separate words, genitive or other alternative constructions, or translate only one part of the compound. For an SMT system to cope with the productivity of the phenomenon, any effective strategy should be able to correctly process compounds that have never been seen in the training data as such, although possibly their components have, either in isolation or within a different compound. Previous work (e.g., Koehn and Knight 2003) has shown that compound splitting improves translation from compounding languages into English. In this article we explore several aspects of the less-researched area of compound treatment for translation into such languages, using three Germanic languages (German, Swedish, and Danish) as examples.1 The assumption is that splitting compounds will also improve translation for this translation direction and lead to more natural translations. The strategy we adopt is to split compounds in the training data, and to merge them in the translation output. Our overall goal is to improve translation q"
J13-4009,W07-0734,0,0.0772725,"Missing"
J13-4009,P11-1140,0,0.0852879,"Missing"
J13-4009,C00-2162,0,0.320895,"Missing"
J13-4009,J04-2003,0,0.102576,"Missing"
J13-4009,P03-1021,0,0.153712,"Missing"
J13-4009,J03-1002,0,0.00859661,"Missing"
J13-4009,P02-1040,0,0.0885145,"Missing"
J13-4009,W05-0908,0,0.0723536,"Missing"
J13-4009,C08-1098,0,0.0270212,"Missing"
J13-4009,H05-1095,1,0.710001,"Missing"
J13-4009,sjobergh-kann-2004-finding,0,0.0641585,"Missing"
J13-4009,E09-3008,1,0.867101,"Missing"
J13-4009,W11-2129,1,0.753498,"Missing"
J13-4009,2008.eamt-1.25,1,0.869509,"Missing"
J13-4009,W08-0317,1,0.889497,"Missing"
J13-4009,A97-1011,0,0.155725,"Missing"
J13-4009,2007.mtsummit-papers.65,0,0.0650218,"Missing"
J13-4009,P07-2045,0,\N,Missing
J13-4009,P08-2039,0,\N,Missing
K17-3022,Q16-1031,0,0.032839,"ce of length n with words w1 , . . . , wn , we create a sequence of vectors x1:n , where the vector xi representing wi is the concatenation of a word embedding, a pretrained embedding, and a character vector. We construct a character vector che (wi ) for each wi by running a BiLSTM over the characters chj (1 ≤ j ≤ m) of wi : 100 50 12 100 100 2 200 / 200 0.25 0.33 0.1 Table 2: Hyper-parameter values for parsing. With the aim of training a multilingual parser, we additionally created a variant of the parser which adds a language embedding to input vectors in a spirit similar to what is done in Ammar et al. (2016). In this setting, the vector for each word xi is the concatenation of a word embedding, a pretrained word embedding, a character vector, and a language embedding li with the language corresponding to the word. As was mentioned in the introduction, our experiments with this model was limited to the languages with little data. Those experiments are described in Section 5. che (wi ) = B I L STM(ch1:m ) As in the original parser, we also concatenate these vectors with pretrained word embeddings pe(wi ). The input vectors xi are therefore: xi = e(wi ) ◦ pe(wi ) ◦ che (wi ) Our pretrained word embe"
K17-3022,D15-1041,0,0.0715977,"holz and Marsi, 2006; Nivre et al., 2007). Even models that perform joint inference, like those of Hatori et al. (2012) and Bohnet et al. (2013), depend heavily on part-of-speech tags, so we were unlikely to reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al., 2015). The Uppsala system is a very simple pipeline consisting of two main components. The first is a model for joint sentence and word segmentation, which uses the BiRNN-CRF framework of Shao et al. (2017) to predict sentence and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. Th"
K17-3022,W17-0203,1,0.918606,"der-decoder (Bahdanau et al., 2014) equipped with shared long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) as the basic recurrent cell. At test time, multi-word tokens are first queried in the dictionary. If not found, the segmented words are generated via the encoder-decoder as a sequence-to-sequence transOur original plans included training a single universal model on data from all languages, with cross-lingual word embeddings, but in the limited time available we could only start exploring two simple enhancements. First, we constructed word embeddings based on the RSV model (Basirat and Nivre, 2017), using universal part-of-speech tags as contexts (Section 4). Secondly, we used multilingual training data for languages with little or no training data (Section 5). Our system was trained only on the training sets provided by the organizers (Nivre et al., 2017a). We did not make any use of large unlabeled data sets, parallel data sets, or word embeddings derived from such data. After evaluation on the official test sets (Nivre et al., 2017b), run on the TIRA server (Potthast et al., 2014), the Uppsala system ranked 23 of 33 systems with respect to the main evaluation metric, with a macro-ave"
K17-3022,Q13-1034,1,0.885612,"Missing"
K17-3022,W06-2920,0,0.223232,"ahu Kiperwasser† Sara Stymne∗ Yoav Goldberg† Joakim Nivre∗ ∗ Department of Linguistics and Philology Uppsala University Uppsala, Sweden Abstract Computer Science Department Bar-Ilan University Ramat-Gan, Israel lemmas, despite the fact that these annotations are available in the training and development data. In this way, we go against a strong tradition in dependency parsing, which has generally favored pipeline systems with part-of-speech tagging as a crucial component, a tendency that has probably been reinforced by the widespread use of data sets with gold tags from the early CoNLL tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). Even models that perform joint inference, like those of Hatori et al. (2012) and Bohnet et al. (2013), depend heavily on part-of-speech tags, so we were unlikely to reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al.,"
K17-3022,W14-4012,0,0.0702859,"Missing"
K17-3022,L16-1262,1,0.853436,"Missing"
K17-3022,Q13-1033,1,0.85172,"ure over the words of each sentence. In particular, the system makes no use of part-of-speech tags, morphological features, or 207 Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 207–217, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. 2 informative features of words in context and a feed-forward network for predicting the next parsing transition. The parser uses the arc-hybrid transition system (Kuhlmann et al., 2011) with greedy inference and a dynamic oracle for exploration during training (Goldberg and Nivre, 2013). For the shared task, the parser has been modified to use character-based representations instead of part-ofspeech tags and to use pseudo-projective parsing to capture non-projective dependencies (Nivre and Nilsson, 2005). The parsing component is further described in Section 3. Sentence and Word Segmentation We model joint sentence and word segmentation as a character-level sequence labeling problem in a Bi-RNN-CRF model (Huang et al., 2015; Ma and Hovy, 2016). We simultaneously predict sentence boundaries and word boundaries and identify multi-word tokens that require further transduction."
K17-3022,P12-1110,0,0.0235023,"psala University Uppsala, Sweden Abstract Computer Science Department Bar-Ilan University Ramat-Gan, Israel lemmas, despite the fact that these annotations are available in the training and development data. In this way, we go against a strong tradition in dependency parsing, which has generally favored pipeline systems with part-of-speech tagging as a crucial component, a tendency that has probably been reinforced by the widespread use of data sets with gold tags from the early CoNLL tasks (Buchholz and Marsi, 2006; Nivre et al., 2007). Even models that perform joint inference, like those of Hatori et al. (2012) and Bohnet et al. (2013), depend heavily on part-of-speech tags, so we were unlikely to reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al., 2015). The Uppsala system is a very simple pipeline consisting of two main components. The first is"
K17-3022,P81-1022,0,0.685375,"Missing"
K17-3022,P05-1013,1,0.822588,"ependencies, pages 207–217, c 2017 Association for Computational Linguistics Vancouver, Canada, August 3-4, 2017. 2 informative features of words in context and a feed-forward network for predicting the next parsing transition. The parser uses the arc-hybrid transition system (Kuhlmann et al., 2011) with greedy inference and a dynamic oracle for exploration during training (Goldberg and Nivre, 2013). For the shared task, the parser has been modified to use character-based representations instead of part-ofspeech tags and to use pseudo-projective parsing to capture non-projective dependencies (Nivre and Nilsson, 2005). The parsing component is further described in Section 3. Sentence and Word Segmentation We model joint sentence and word segmentation as a character-level sequence labeling problem in a Bi-RNN-CRF model (Huang et al., 2015; Ma and Hovy, 2016). We simultaneously predict sentence boundaries and word boundaries and identify multi-word tokens that require further transduction. In the BiRNN-CRF architecture, characters – regardless of writing system – are represented as dense vectors and fed into the bidirectional recurrent layers. We employ the gated recurrent unit (GRU) (Cho et al., 2014) as th"
K17-3022,Q16-1032,1,0.762352,"e and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. The second main component of our system is a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence. The starting point for this model is the transitionbased parser described in Kiperwasser and Goldberg (2016b), which relies on a BiLSTM to learn We present the Uppsala submission to the CoNLL 2017 shared task on parsing from raw text to universal dependencies. Our system is a simple pipeline consisting of two components. The first performs joint word and sentence segmentation on raw text; the second predicts dependency trees from raw words. The parser bypasses the need for part-of-speech tagging, but uses word embeddings based on universal tag distributions. We achieved a macroaveraged LAS F1 of 65.11 in the official test run and obtained the 2nd best result for sentence segmentation with a score o"
K17-3022,Q16-1023,1,0.778627,"e and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. The second main component of our system is a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence. The starting point for this model is the transitionbased parser described in Kiperwasser and Goldberg (2016b), which relies on a BiLSTM to learn We present the Uppsala submission to the CoNLL 2017 shared task on parsing from raw text to universal dependencies. Our system is a simple pipeline consisting of two components. The first performs joint word and sentence segmentation on raw text; the second predicts dependency trees from raw words. The parser bypasses the need for part-of-speech tagging, but uses word embeddings based on universal tag distributions. We achieved a macroaveraged LAS F1 of 65.11 in the official test run and obtained the 2nd best result for sentence segmentation with a score o"
K17-3022,P11-1068,0,0.532177,"Missing"
K17-3022,I17-1018,1,0.927202,"reach top scores in the shared task without them. However, from a scientific perspective, we thought it would be interesting to explore how far we can get with a bare-bones system that does not predict redundant linguistic categories. In addition, we take inspiration from recent work showing that character-based representations can at least partly obviate the need for part-of-speech tags (Ballesteros et al., 2015). The Uppsala system is a very simple pipeline consisting of two main components. The first is a model for joint sentence and word segmentation, which uses the BiRNN-CRF framework of Shao et al. (2017) to predict sentence and word boundaries in the raw input and simultaneously marks multiword tokens that need non-segmental analysis. The latter are handled by a simple dictionary lookup or by an encoder-decoder network. We use a single universal model regardless of writing system, but train separate models for each language. The segmentation component is described in more detail in Section 2. The second main component of our system is a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence. The starting point for this model is the transitionbased p"
K17-3022,P16-1101,0,0.031215,"c-hybrid transition system (Kuhlmann et al., 2011) with greedy inference and a dynamic oracle for exploration during training (Goldberg and Nivre, 2013). For the shared task, the parser has been modified to use character-based representations instead of part-ofspeech tags and to use pseudo-projective parsing to capture non-projective dependencies (Nivre and Nilsson, 2005). The parsing component is further described in Section 3. Sentence and Word Segmentation We model joint sentence and word segmentation as a character-level sequence labeling problem in a Bi-RNN-CRF model (Huang et al., 2015; Ma and Hovy, 2016). We simultaneously predict sentence boundaries and word boundaries and identify multi-word tokens that require further transduction. In the BiRNN-CRF architecture, characters – regardless of writing system – are represented as dense vectors and fed into the bidirectional recurrent layers. We employ the gated recurrent unit (GRU) (Cho et al., 2014) as the basic recurrent cell. Dropout (Srivastava et al., 2014) is applied to the output of the recurrent layers, which are concatenated and passed further to the first order chain CRF layer. The CRF layer models conditional scores over all possible"
K17-3022,L16-1680,0,0.0727605,"Missing"
K18-2011,C16-1012,0,0.0132499,"OS tagging and morphological features (UFEATS). Treebanks sharing a parsing model grouped together; substitute and proxy treebanks for segmentation, tagging, parsing far right (SPECIAL models detailed in the text). Confidence intervals for coloring: |< µ−σ < |< µ− SE < µ < µ+ SE < |< µ+σ < |. 119 able method for pooling training data both within and across languages. It is also worth noting that this method is easy to use and does not require extra external resources used in most work on multilingual parsing, like multilingual word embeddings (Ammar et al., 2016) or linguistic re-write rules (Aufrant et al., 2016) to achieve good results. coding. Scores that are significantly higher/lower than the mean score of the 21 systems that successfully parsed all test sets are marked with two shades of green/red. The lighter shade marks differences that are outside the interval defined by √the standard error of the mean (µ ± SE, SE = σ/ N ) but within one standard deviation (std dev) from the mean. The darker shade marks differences that are more than one std dev above/below the mean (µ ± σ). Finally, scores that are no longer valid because of the Thai UPOS tagger are crossed out in yellow cells, and corrected"
K18-2011,P82-1020,0,0.805881,"Missing"
K18-2011,P18-1246,1,0.843945,"(2017a). We use the default parameter settings introduced by Shao et al. (2018) and train a segmentation model for all treebanks with at least 50 sentences of training data. For treebanks with less or no training data (except Thai discussed below), we substitute a model for another treebank/language: • For Japanese Modern, Czech PUD, English PUD and Swedish PUD, we use the model trained on the largest treebank from the same language (Japanese GSD, Czech PDT, English EWT and Swedish Talbanken). 4 Tagging and Morphological Analysis We use two separate instantiations of the tagger6 described in Bohnet et al. (2018) to predict UPOS tags and morphological features, respectively. The tagger uses a Meta-BiLSTM over the output of a sentence-based character model and a word model. There are two features that mainly distinguishes the tagger from previous work. The character BiLSTMs use the full context of the sentence in contrast to most other taggers which use words only as context for the character model. This character model is combined with the word model in the Meta-BiLSTM relatively late, after two layers of BiLSTMs. For both the word and character models, we use two layers of BiLSTMs with 300 LSTM cells"
K18-2011,Q17-1010,0,0.119675,"Missing"
K18-2011,W13-4902,1,0.850186,"ogical Features Having a strong morphological analyzer, we were interested in finding out whether or not we can improve parsing accuracy using predicted morphological information. We conducted several experiments on the development sets for a subset of treebanks. However, no experiment gave us any improvement in terms of LAS and we decided not to use this technique for the shared task. What we tried was to create an embedding representing either the full set of morphological features or a subset of potentially useful features, for example case (which has been shown to be useful for parsing by Kapociute-Dzikiene et al. (2013) and Eryigit et al. (2008)), verb form and a few others. That embedding was concatenated to the word embedding at the input of the BiLSTM. We varied the embedding size (10, 20, 30, 40), tried different subsets of morphological features, and tried with and without using dropout on that embedding. We also tried creating an embedding of a concatenation of the universal POS tag and the Case feature and replace the POS embedding with this one. We are currently unsure why none of these experiments were successful and plan to investigate this 9 https://radimrehurek.com/gensim/ An alternative strategy"
K18-2011,D14-1082,0,0.0860011,"pped to universal POS tags and a few morphological features like person, number and gender. For Thai, we annotated about 33,000 sentences from Wikipedia using PyThaiNLP8 and mapped only to UPOS tags (no features). Unfortunately, we realized only after the test phase that PyThaiNLP was not a permitted resource, which invalidates our UPOS tagging scores for Thai, as well as the LAS and MLAS scores which depend on the tagger. Note, however, that the score for morphological features xi = e(wi ) ◦ e(pi ) ◦ BiLSTM(ch1:m ). Here, e(wi ) represents the word embedding and e(pi ) the POS tag embedding (Chen and Manning, 2014); these are concatenated to a character-based vector, obtained by running a BiLSTM over the characters ch1:m of wi . With the aim of training multi-treebank models, we additionally created a variant of the parser which adds a treebank embedding e(tbi ) to input vectors in a spirit similar to the language embeddings of Ammar et al. (2016) and de Lhoneux et al. (2017a): xi = e(wi ) ◦ e(pi ) ◦ BiLSTM(ch1:m ) ◦ e(tbi ). We have previously shown that treebank embeddings provide an effective way to combine multiple monolingual heterogeneous treebanks (Stymne et al., 2018) and applied them to lowreso"
K18-2011,Q16-1032,0,0.158942,"or the PUD treebanks (except Thai), Japanese Modern and Naija NSC, we use the same model substitutions as for segmentation (see Table 2). is not affected, as we did not predict features at all for Thai. The same goes for sentence and word segmentation, which do not depend on the tagger. Lemmas Due to time constraints we chose not to focus on the BLEX metric in this shared task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneux et al., 2017b). In our parser, the vector representatio"
K18-2011,K17-3022,1,0.72705,"Missing"
K18-2011,Q16-1023,0,0.434449,"or the PUD treebanks (except Thai), Japanese Modern and Naija NSC, we use the same model substitutions as for segmentation (see Table 2). is not affected, as we did not predict features at all for Thai. The same goes for sentence and word segmentation, which do not depend on the tagger. Lemmas Due to time constraints we chose not to focus on the BLEX metric in this shared task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneux et al., 2017b). In our parser, the vector representatio"
K18-2011,W17-6314,1,0.712346,"Missing"
K18-2011,K17-3002,0,0.0838483,"ainian IU. All in all, the 2018 edition of the Uppsala parser can be characterized as a system that is strong on segmentation (especially word segmentation) and prediction of UPOS tags and morphological features, and where the dependency parsing component performs well in low-resource scenarios thanks to the use of multi-treebank models, both within and across languages. For what it is worth, we also seem to have the highest ranking singleparser transition-based system in a task that is otherwise dominated by graph-based models, in particular variants of the winning Stanford system from 2017 (Dozat et al., 2017). 8 9 Conclusion We have described the Uppsala submission to the CoNLL 2018 shared task, consisting of a segmenter that jointly extracts words and sentences from a raw text, a tagger that provides UPOS tags and morphological features, and a parser that builds a dependency tree given the words and tags of each sentence. For the parser we applied multi-treebank models both monolingually and multilingually, resulting in only 34 models for 82 treebanks as well as significant improvements in parsing accuracy especially for low-resource languages. We ranked 7th for the official LAS and MLAS scores,"
K18-2011,K18-2002,0,0.0159289,"icial LAS and MLAS scores, and first for the unofficial scores on word segmentation, UPOS tagging and morphological features. Acknowledgments We are grateful to the shared task organizers and to Dan Zeman and Martin Potthast in particular, and we acknowledge the computational resources provided by CSC in Helsinki and Sigma2 in Oslo through NeIC-NLPL (www.nlpl.eu). Aaron Smith was supported by the Swedish Research Council. Extrinsic Parser Evaluation In addition to the official shared task evaluation, we also participated in the 2018 edition of the Extrinsic Parser Evaluation Initiative (EPE) (Fares et al., 2018), where parsers developed for the CoNLL 2018 shared task were evaluated with respect to their contribution to three downstream systems: biological event extraction, fine-grained opinion analysis, and negation resolution. The downstream systems are available for English only, and we participated with our English model trained on English EWT, English LinES and English GUM, using English EWT as the proxy. In the extrinsic evaluation, the Uppsala system ranked second for event extraction, first for opinion analysis, and 16th (out of 16 systems) for negation resolution. Our results for the first tw"
K18-2011,J08-4003,1,0.671677,"e adopt three different strategies: • For the PUD treebanks (except Thai), Japanese Modern and Naija NSC, we use the same model substitutions as for segmentation (see Table 2). is not affected, as we did not predict features at all for Thai. The same goes for sentence and word segmentation, which do not depend on the tagger. Lemmas Due to time constraints we chose not to focus on the BLEX metric in this shared task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneu"
K18-2011,P09-1040,1,0.839321,"task. In order to avoid zero scores, however, we simply copied a lowercased version of the raw token into the lemma column. 5 Dependency Parsing We use a greedy transition-based parser (Nivre, 2008) based on the framework of Kiperwasser and Goldberg (2016b) where BiLSTMs (Hochreiter and Schmidhuber, 1997; Graves, 2008) learn representations of tokens in context, and are trained together with a multi-layer perceptron that predicts transitions and arc labels based on a few BiLSTM vectors. Our parser is extended with a S WAP transition to allow the construction of nonprojective dependency trees (Nivre, 2009). We also introduce a static-dynamic oracle to allow the parser to learn from non-optimal configurations at training time in order to recover better from mistakes at test time (de Lhoneux et al., 2017b). In our parser, the vector representation xi of a word type wi before it is passed to the BiLSTM feature extractors is given by: • For Faroese we used the model for Norwegian Nynorsk, as we believe this to be the most closely related language. • For treebanks with small training sets we use only the provided training sets for training. Since these treebanks do not have development sets, we use"
K18-2011,J93-1004,0,0.0296153,"Missing"
K18-2011,Q18-1030,1,0.830131,"arginally) higher score with the mono-treebank baseline model: Estonian EDT, Russian SynTagRus, Slovenian SSJ, and Turkish IMST. Looking at the aggregate sets, we see that, as expected, the pooling of resources helps most for LOW- RESOURCE (25.33 vs. 17.72) and SMALL (63.60 vs. 60.06), but even for BIG there is some improvement (80.21 vs. 79.61). We find these results very encouraging, as they indicate that our treebank embedding method is a reliFor word segmentation, we obtained the best results overall, strongly outperforming the mean for all groups except SMALL. We know from previous work (Shao et al., 2018) that our word segmenter performs well on more challenging languages like Arabic, Hebrew, Japanese, and Chinese (although we were beaten by the Stanford team for the former two and by the HIT-SCIR team for the latter two). By contrast, it sometimes falls below the mean for the easier languages, but typically only by a very small fraction (for example 99.99 vs. 100.00 for 3 treebanks). Finally, it is worth noting that the maximum-matching segmenter developed specifically for Thai achieved a score of 69.93, which was more than 5 points better than any other system. Our results for UPOS tagging i"
K18-2011,I17-1018,1,0.925902,"ext, without any linguistic annotation, and output full labelled dependency trees for 82 test treebanks covering 46 different languages. Besides the labeled attachment score (LAS) used to evaluate systems in the 2017 edition of the Shared Task (Zeman et al., 2017), this year’s task introduces two new metrics: morphology-aware labeled attachment score (MLAS) and bi-lexical dependency score (BLEX). The Uppsala system focuses exclusively on LAS and MLAS, and consists of a three-step pipeline. The first step is a model for joint sentence and word segmentation which uses the BiRNN-CRF framework of Shao et al. (2017, 2018) to predict sentence and word boundaries in the raw input and Corrigendum: After the test phase was over, we discovered that we had used a non-permitted resource when developing the UPOS tagger for Thai PUD (see Section 4). Setting our LAS, MLAS and UPOS scores to 0.00 for Thai PUD gives the corrected scores: LAS 72.31, MLAS 59.17, UPOS 90.50. This does not affect the ranking for any of the three scores, as confirmed by the shared task organizers. 2 Resources All three components of our system were trained principally on the training sets of Universal Dependencies v2.2 released to coinc"
K18-2011,D18-1291,1,0.828983,"Missing"
K18-2011,P18-2098,1,0.816536,"Missing"
K18-2011,tiedemann-2012-parallel,0,0.122801,"Missing"
K18-2011,C96-1035,0,0.0335101,"Missing"
K18-2011,K18-2001,1,0.844683,"Missing"
K18-2011,P07-2045,0,\N,Missing
K18-2011,J08-4010,1,\N,Missing
K18-2011,J08-3003,1,\N,Missing
P11-3003,D08-1078,0,0.0184811,"m reasonable. The main focus for SMT to date has been on translation into English, for which the models work relatively well, especially for source languages that are structurally similar to English. There has been less research on translation out of English, or between other language pairs. Methods that are useful for translation into English have problems in many cases, for instance for translation into morphologically rich languages. Word order differences and morphological complexity of a language have been shown to be explanatory variables for the performance of phrase-based SMT systems (Birch et al., 2008). German and the Scandinavian languages are a good sample of languages, I believe, since they are both more morphologically complex than English to a varying degree, and the word order differ to some extent, with mostly local differences between English and Scandinavian, and also long distance differences with German, especially for verbs. Some problems with SMT into German and Swedish are exemplified in Table 1. In the German example, the translation of the verb welcome is missing in the SMT output. Missing and misplaced verbs are common error types, since the German verb should appear last i"
P11-3003,J07-2003,0,0.0326544,"d part-of-speech, rather than only as surface words (Koehn and Hoang, 2007). I mostly utilize factors to translate into both words and (morphological) partof-speech, and can then use an additional sequence model based on part-of-speech, which potentially can improve word order and agreement. I take advantage of available tools, such as the Moses toolkit (Koehn et al., 2007) for factored phrase-based translation. I have chosen to focus on PBSMT, which is a very successful MT approach, and have received much research focus. Other SMT approaches, such as hi13 erarchical and syntactical SMT (e.g. Chiang (2007), Zhang et al. (2007a)) can potentially overcome some language differences that are problematic for PBSMT, such as long-distance word order differences. Many of these models have had good results, but they have the drawback of being more complex than PBSMT, and some methods do not scale well to large corpora. While these models at least in principle address some of the drawbacks of the flat structure in PBSMT, Wang et al. (2010) showed that a syntactic SMT system can still gain from preprocessing such as parse-tree modification. 3 Evaluation and Error Analysis Machine translation systems are o"
P11-3003,P05-1066,0,0.0914776,"Missing"
P11-3003,P08-1115,0,0.0590135,"Missing"
P11-3003,2006.eamt-1.27,0,0.0837301,"Missing"
P11-3003,2010.eamt-1.12,0,0.030986,"Missing"
P11-3003,D07-1091,0,0.0608026,"and postprocessing on four areas: compounding, definite noun phrases, word order, and error correction. In addition I am making an effort into error analysis, to identify and classify errors in the MT output, both in order to focus my research effort, and to evaluate and compare systems. My work is based on the phrase-based approach to statistical machine translation (PBSMT, Koehn et al. (2003)). I further use the framework of factored machine translation, where each word is represented as a vector of factors, such as surface word, lemma and part-of-speech, rather than only as surface words (Koehn and Hoang, 2007). I mostly utilize factors to translate into both words and (morphological) partof-speech, and can then use an additional sequence model based on part-of-speech, which potentially can improve word order and agreement. I take advantage of available tools, such as the Moses toolkit (Koehn et al., 2007) for factored phrase-based translation. I have chosen to focus on PBSMT, which is a very successful MT approach, and have received much research focus. Other SMT approaches, such as hi13 erarchical and syntactical SMT (e.g. Chiang (2007), Zhang et al. (2007a)) can potentially overcome some language"
P11-3003,E03-1076,0,0.0373871,"describe the four main problem areas I will focus on in my thesis project. I summarize briefly previous work in each area, and outline my own current and planned contributions. Sample results from the different studies are shown in Table 2. 4.1 Compounding In most Germanic languages, compounds are written without spaces or other word boundaries, which makes them problematic for SMT, mainly due to sparse data problems. The standard method for treating compounds for translation from Germanic languages is to split them in both the training data and translation input (e.g. (Nießen and Ney, 2000; Koehn and Knight, 2003; Popovi´c et al., 2006)). Koehn and Knight (2003) also suggested a corpus14 based compound splitting method that has been much used for SMT, where compounds are split based on corpus frequencies of its parts. If compounds are split for translation into Germanic languages, the SMT system produces output with split compounds, which need to be postprocessed into full compounds. There has been very little research into this problem. For this process to be successful, it is important that the SMT system produces the split compound parts in a correct word order. To encourage this I have used a fact"
P11-3003,N03-1017,0,0.010107,"age has been preprocessed, in order to restore it to the normal target language. Postprocessing can also be used on standard MT output, in order to correct some of the errors from the MT system. I focus my work about pre- and postprocessing on four areas: compounding, definite noun phrases, word order, and error correction. In addition I am making an effort into error analysis, to identify and classify errors in the MT output, both in order to focus my research effort, and to evaluate and compare systems. My work is based on the phrase-based approach to statistical machine translation (PBSMT, Koehn et al. (2003)). I further use the framework of factored machine translation, where each word is represented as a vector of factors, such as surface word, lemma and part-of-speech, rather than only as surface words (Koehn and Hoang, 2007). I mostly utilize factors to translate into both words and (morphological) partof-speech, and can then use an additional sequence model based on part-of-speech, which potentially can improve word order and agreement. I take advantage of available tools, such as the Moses toolkit (Koehn et al., 2007) for factored phrase-based translation. I have chosen to focus on PBSMT, wh"
P11-3003,P07-2045,0,0.00579856,"ed on the phrase-based approach to statistical machine translation (PBSMT, Koehn et al. (2003)). I further use the framework of factored machine translation, where each word is represented as a vector of factors, such as surface word, lemma and part-of-speech, rather than only as surface words (Koehn and Hoang, 2007). I mostly utilize factors to translate into both words and (morphological) partof-speech, and can then use an additional sequence model based on part-of-speech, which potentially can improve word order and agreement. I take advantage of available tools, such as the Moses toolkit (Koehn et al., 2007) for factored phrase-based translation. I have chosen to focus on PBSMT, which is a very successful MT approach, and have received much research focus. Other SMT approaches, such as hi13 erarchical and syntactical SMT (e.g. Chiang (2007), Zhang et al. (2007a)) can potentially overcome some language differences that are problematic for PBSMT, such as long-distance word order differences. Many of these models have had good results, but they have the drawback of being more complex than PBSMT, and some methods do not scale well to large corpora. While these models at least in principle address som"
P11-3003,W09-0433,0,0.0139011,"not successful (Stymne, 2011b). A small modification to the basic strategy, so that superfluous English articles were removed, but no suffixes were added, was successful for translation from English into Swedish and Norwegian. A planned extension is to integrate the transformations into a lattice that is fed to the decoder, in the spirit of (Dyer et al., 2008). 4.3 Word Order There has been a lot of research on how to handle word order differences between languages. Prepro15 cessing approaches can use either hand-written rules targeting known language differences (e.g. Collins et al. (2005), Li et al. (2009)), or automatically learnt rules (e.g. Xia and McCord (2004), Zhang et al. (2007b)), which are basically language independent. I have performed an initial study on a language independent word order strategy where reordering rule learning and word alignment are performed iteratively, since they both depend on the other process (Stymne, 2011c). There were no overall improvements as measured by Bleu, but an investigation of the reordering rules showed that the rules learned in the different iterations are different with regard to the linguistic phenomena they handle, indicating that it is possibl"
P11-3003,C00-2162,0,0.0518018,"lems In this section I describe the four main problem areas I will focus on in my thesis project. I summarize briefly previous work in each area, and outline my own current and planned contributions. Sample results from the different studies are shown in Table 2. 4.1 Compounding In most Germanic languages, compounds are written without spaces or other word boundaries, which makes them problematic for SMT, mainly due to sparse data problems. The standard method for treating compounds for translation from Germanic languages is to split them in both the training data and translation input (e.g. (Nießen and Ney, 2000; Koehn and Knight, 2003; Popovi´c et al., 2006)). Koehn and Knight (2003) also suggested a corpus14 based compound splitting method that has been much used for SMT, where compounds are split based on corpus frequencies of its parts. If compounds are split for translation into Germanic languages, the SMT system produces output with split compounds, which need to be postprocessed into full compounds. There has been very little research into this problem. For this process to be successful, it is important that the SMT system produces the split compound parts in a correct word order. To encourage"
P11-3003,P02-1040,0,0.0835286,"at are problematic for PBSMT, such as long-distance word order differences. Many of these models have had good results, but they have the drawback of being more complex than PBSMT, and some methods do not scale well to large corpora. While these models at least in principle address some of the drawbacks of the flat structure in PBSMT, Wang et al. (2010) showed that a syntactic SMT system can still gain from preprocessing such as parse-tree modification. 3 Evaluation and Error Analysis Machine translation systems are often only evaluated quantitatively by using automatic metrics, such as Bleu (Papineni et al., 2002), which compares the system output to one or more human reference translations. While this type of evaluation has its advantages, mainly that it is fast and cheap, its correlation with human judgments is often low, especially for translation out of English (Callison-Burch et al., 2009). In order to overcome these problems to some extent I use several metrics in my studies, instead of only Bleu. Despite this, metrics only give a single score per sentence batch and system, which even using several metrics gives us little information on the particular problems with a system, or about what the pos"
P11-3003,stymne-ahrenberg-2010-using,1,0.889116,"Missing"
P11-3003,2008.eamt-1.25,1,0.858481,"these part-of-speech tags to merge compounds only when the next part-of-speech tag matches. This merging method outperforms reimplementations and variations of previous merging suggestions (Popovi´c et al., 2006), and methods adapted from morphology merging (Virpioja et al., 2007) for translation into German (Stymne, 2009a). It also has the advantage over previous merging methods that it can produce novel compounds, while at the same time reducing the risk of merging parts into non-words. I have also shown that these compound processing methods work equally well for translation into Swedish (Stymne and Holmqvist, 2008). Currently I am working on methods for further improving compound merging, with promising initial results. 4.2 Definite Noun Phrases In Scandinavian languages there are two ways to express definiteness in noun phrases, either by a definite article, or by a suffix on the noun. This leads to problems when translating into these languages, such as superfluous definite articles and wrong forms of nouns. I am not aware of any published research in this area, but an unpublished Language pair Corpus Corpus size Testset size In article En-De Europarl 439,513 2,000 Stymne (2008) En-Se Europarl 701,157"
P11-3003,E09-3008,1,0.882724,"ord order. To encourage this I have used a factored translation system that outputs parts-of-speech and uses a sequence model on parts-of-speech. I extended the part-of-speech tagset to use special part-of-speech tags for split compound parts, which depend on the head part-of-speech of the compound. For instance, the Swedish noun p¨arontr¨ad (pear tree) would be tagged as p¨aron|N-part tr¨ad|N when split. Using this model the number of compound parts that were produced in the wrong order was reduced drastically compared to not using a part-of-speech sequence model for translation into German (Stymne, 2009a). I also designed an algorithm for the merging task that uses these part-of-speech tags to merge compounds only when the next part-of-speech tag matches. This merging method outperforms reimplementations and variations of previous merging suggestions (Popovi´c et al., 2006), and methods adapted from morphology merging (Virpioja et al., 2007) for translation into German (Stymne, 2009a). It also has the advantage over previous merging methods that it can produce novel compounds, while at the same time reducing the risk of merging parts into non-words. I have also shown that these compound proc"
P11-3003,P11-4010,1,0.826851,"missing, incorrect, unknown, word order, and punctuation, which have also been used by other researchers, mainly for evaluation. However, this typology is relatively shallow and mixes classification of errors with causes of errors. Farr´us et al. (2010) suggested a typology based on linguistic categories, such as orthography and semantics, but their descriptions of these categories and their subcategories are not detailed. Thus, as part of my research, I am in the progress of designing a fine-grained typology and guidelines for EA. I have also created a tool for performing MT error analysis (Stymne, 2011a). Initial annotations have helped to focus my research efforts, and will be discussed below. I also plan to use EA as one means of evaluating my work on pre- and postprocessing. 4 Main Research Problems In this section I describe the four main problem areas I will focus on in my thesis project. I summarize briefly previous work in each area, and outline my own current and planned contributions. Sample results from the different studies are shown in Table 2. 4.1 Compounding In most Germanic languages, compounds are written without spaces or other word boundaries, which makes them problematic"
P11-3003,2011.eamt-1.39,1,0.793383,"missing, incorrect, unknown, word order, and punctuation, which have also been used by other researchers, mainly for evaluation. However, this typology is relatively shallow and mixes classification of errors with causes of errors. Farr´us et al. (2010) suggested a typology based on linguistic categories, such as orthography and semantics, but their descriptions of these categories and their subcategories are not detailed. Thus, as part of my research, I am in the progress of designing a fine-grained typology and guidelines for EA. I have also created a tool for performing MT error analysis (Stymne, 2011a). Initial annotations have helped to focus my research efforts, and will be discussed below. I also plan to use EA as one means of evaluating my work on pre- and postprocessing. 4 Main Research Problems In this section I describe the four main problem areas I will focus on in my thesis project. I summarize briefly previous work in each area, and outline my own current and planned contributions. Sample results from the different studies are shown in Table 2. 4.1 Compounding In most Germanic languages, compounds are written without spaces or other word boundaries, which makes them problematic"
P11-3003,W11-4648,1,0.840179,"missing, incorrect, unknown, word order, and punctuation, which have also been used by other researchers, mainly for evaluation. However, this typology is relatively shallow and mixes classification of errors with causes of errors. Farr´us et al. (2010) suggested a typology based on linguistic categories, such as orthography and semantics, but their descriptions of these categories and their subcategories are not detailed. Thus, as part of my research, I am in the progress of designing a fine-grained typology and guidelines for EA. I have also created a tool for performing MT error analysis (Stymne, 2011a). Initial annotations have helped to focus my research efforts, and will be discussed below. I also plan to use EA as one means of evaluating my work on pre- and postprocessing. 4 Main Research Problems In this section I describe the four main problem areas I will focus on in my thesis project. I summarize briefly previous work in each area, and outline my own current and planned contributions. Sample results from the different studies are shown in Table 2. 4.1 Compounding In most Germanic languages, compounds are written without spaces or other word boundaries, which makes them problematic"
P11-3003,vilar-etal-2006-error,0,0.0652781,"Missing"
P11-3003,2007.mtsummit-papers.65,0,0.165334,"Missing"
P11-3003,J10-2004,0,0.0132438,"en to focus on PBSMT, which is a very successful MT approach, and have received much research focus. Other SMT approaches, such as hi13 erarchical and syntactical SMT (e.g. Chiang (2007), Zhang et al. (2007a)) can potentially overcome some language differences that are problematic for PBSMT, such as long-distance word order differences. Many of these models have had good results, but they have the drawback of being more complex than PBSMT, and some methods do not scale well to large corpora. While these models at least in principle address some of the drawbacks of the flat structure in PBSMT, Wang et al. (2010) showed that a syntactic SMT system can still gain from preprocessing such as parse-tree modification. 3 Evaluation and Error Analysis Machine translation systems are often only evaluated quantitatively by using automatic metrics, such as Bleu (Papineni et al., 2002), which compares the system output to one or more human reference translations. While this type of evaluation has its advantages, mainly that it is fast and cheap, its correlation with human judgments is often low, especially for translation out of English (Callison-Burch et al., 2009). In order to overcome these problems to some e"
P11-3003,C04-1073,0,0.0358394,"o the basic strategy, so that superfluous English articles were removed, but no suffixes were added, was successful for translation from English into Swedish and Norwegian. A planned extension is to integrate the transformations into a lattice that is fed to the decoder, in the spirit of (Dyer et al., 2008). 4.3 Word Order There has been a lot of research on how to handle word order differences between languages. Prepro15 cessing approaches can use either hand-written rules targeting known language differences (e.g. Collins et al. (2005), Li et al. (2009)), or automatically learnt rules (e.g. Xia and McCord (2004), Zhang et al. (2007b)), which are basically language independent. I have performed an initial study on a language independent word order strategy where reordering rule learning and word alignment are performed iteratively, since they both depend on the other process (Stymne, 2011c). There were no overall improvements as measured by Bleu, but an investigation of the reordering rules showed that the rules learned in the different iterations are different with regard to the linguistic phenomena they handle, indicating that it is possible to learn new information from iterating rule learning and"
P11-3003,2007.mtsummit-papers.71,0,0.310702,"h, rather than only as surface words (Koehn and Hoang, 2007). I mostly utilize factors to translate into both words and (morphological) partof-speech, and can then use an additional sequence model based on part-of-speech, which potentially can improve word order and agreement. I take advantage of available tools, such as the Moses toolkit (Koehn et al., 2007) for factored phrase-based translation. I have chosen to focus on PBSMT, which is a very successful MT approach, and have received much research focus. Other SMT approaches, such as hi13 erarchical and syntactical SMT (e.g. Chiang (2007), Zhang et al. (2007a)) can potentially overcome some language differences that are problematic for PBSMT, such as long-distance word order differences. Many of these models have had good results, but they have the drawback of being more complex than PBSMT, and some methods do not scale well to large corpora. While these models at least in principle address some of the drawbacks of the flat structure in PBSMT, Wang et al. (2010) showed that a syntactic SMT system can still gain from preprocessing such as parse-tree modification. 3 Evaluation and Error Analysis Machine translation systems are often only evaluated"
P11-3003,2007.iwslt-1.3,0,0.13349,"h, rather than only as surface words (Koehn and Hoang, 2007). I mostly utilize factors to translate into both words and (morphological) partof-speech, and can then use an additional sequence model based on part-of-speech, which potentially can improve word order and agreement. I take advantage of available tools, such as the Moses toolkit (Koehn et al., 2007) for factored phrase-based translation. I have chosen to focus on PBSMT, which is a very successful MT approach, and have received much research focus. Other SMT approaches, such as hi13 erarchical and syntactical SMT (e.g. Chiang (2007), Zhang et al. (2007a)) can potentially overcome some language differences that are problematic for PBSMT, such as long-distance word order differences. Many of these models have had good results, but they have the drawback of being more complex than PBSMT, and some methods do not scale well to large corpora. While these models at least in principle address some of the drawbacks of the flat structure in PBSMT, Wang et al. (2010) showed that a syntactic SMT system can still gain from preprocessing such as parse-tree modification. 3 Evaluation and Error Analysis Machine translation systems are often only evaluated"
P11-3003,W09-0401,0,\N,Missing
P11-4010,E03-1086,0,0.0115609,"r analysis. This tool was highly connected to their translation system, and it required users to post-edit sentences by modifying word alignments, something that many users found difficult. Glenn et al. (2008) described a postediting tool used for HTER calculation, which has been used in large evaluation campaigns. The tool is a pure post-editing tool and the edits are not classified. Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and system comparison (Callison-Burch et al., 2007), and word alignment (Ahrenberg et al., 2003). 2 Though it does, at least in principle, seem possible to mine HTER annotations for more information We created B LAST with the goal that it should be flexible, and allow maximum freedom for the user, 57 3 System Overview B LAST is a tool for human annotations of bilingual material. Its main purpose is error analysis for machine translation. B LAST is designed for use in any MT evaluation project. It is not tied to the information provided by specific MT systems, or to specific languages, and it can be used with any hierarchical error typology. It has a preprocessing module for automatically"
P11-4010,P05-1074,0,0.00532072,"ology, for instance, to search for all word order errors, regardless of subclassification. Search is active between all segments, not only for the currently shown segment. Search is useful for controlling the consistency of annotations, and for finding instances of specific errors. 4.4 Support annotations module for exact matching with the original casing kept. The exact and lower-cased matching works for most languages, and stemming for 15 languages. The synonym module uses WordNet, and is only available for English. The paraphrase module is based on an automatic paraphrase induction method (Bannard and Callison-Burch, 2005), it is currently trained for five languages, but the Meteor-NEXT code for training it for additional languages is included. Support annotations are normally only created automatically, but B LAST allows the user to edit them. The mechanism for adding, removing or changing support annotations is separate from error annotations, and can be used regardless of mode. 4.5 The statistics module prints statistics about the currently loaded annotation project. The statistics are printed to a file, in a human-readable format. It contains information about the number of sentences and errors in the proje"
P11-4010,W07-0718,0,0.0427285,"(Denkowski and Lavie, 2010), where the MT output is compared to one or more human reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system. Comparing scores from different metrics can give a very rough indication of some major problems, especially in combination with a part-ofspeech analysis (Popovi´c et al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al. (2007)). A combination of human and automatic metrics is 56 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 56–61, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics human-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed (Snover et al., 2006). While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualit"
P11-4010,W10-1751,0,0.110872,". B LAST has a graphical user interface, and is designed to be easy 1 The BiLingual Annotation/Annotator/Analysis Support Tool, available for download at http://www.ida.liu. se/∼sarst/blast/ MT Evaluation and Error Analysis Hovy et al. (2002) discussed the complexity of MT evaluation, and stressed the importance of adjusting evaluation to the purpose and context of the translation. However, MT is very often only evaluated quantitatively using a single metric, especially in research papers. Quantitative evaluations can be automatic, using metrics such as Bleu (Papineni et al., 2002) or Meteor (Denkowski and Lavie, 2010), where the MT output is compared to one or more human reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system. Comparing scores from different metrics can give a very rough indication of some major problems, especially in combination with a part-ofspeech analysis (Popovi´c et al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al. (2007)"
P11-4010,2010.eamt-1.12,0,0.524979,"Missing"
P11-4010,1994.amta-1.9,0,0.688032,"-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed (Snover et al., 2006). While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al., 2006; Farr´us et al., 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. It is also possible to focus error analysis at a specific problem, such as verb form errors (Murata et al., 2005). We have not been able to find any other freely available tool for error analysis of MT. Vilar et al. (2006) mentioned in a footnote that “a tool for highlighting the differences [between the MT system and a correct translation] also proved to be quite useful” for error analysis. They do not describe this tool an"
P11-4010,font-llitjos-carbonell-2004-translation,0,0.37247,"Missing"
P11-4010,glenn-etal-2008-management,0,0.0238881,"annotations themselves. Some tools for post-editing of MT output, a related activity to error analysis, have been described in the literature. Font Llitj´os and Carbonell (2004) presented an online tool for eliciting information from the user when post-editing sentences, in order to improve a rule-based translation system. The post-edit operations were labeled with error categories, making it a type of error analysis. This tool was highly connected to their translation system, and it required users to post-edit sentences by modifying word alignments, something that many users found difficult. Glenn et al. (2008) described a postediting tool used for HTER calculation, which has been used in large evaluation campaigns. The tool is a pure post-editing tool and the edits are not classified. Graphical tools have also successfully been used to aid humans in other MT-related tasks, such as human MT evaluation of adequacy, fluency and system comparison (Callison-Burch et al., 2007), and word alignment (Ahrenberg et al., 2003). 2 Though it does, at least in principle, seem possible to mine HTER annotations for more information We created B LAST with the goal that it should be flexible, and allow maximum freed"
P11-4010,Y05-1014,0,0.10404,"ming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al., 2006; Farr´us et al., 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. It is also possible to focus error analysis at a specific problem, such as verb form errors (Murata et al., 2005). We have not been able to find any other freely available tool for error analysis of MT. Vilar et al. (2006) mentioned in a footnote that “a tool for highlighting the differences [between the MT system and a correct translation] also proved to be quite useful” for error analysis. They do not describe this tool any further, and do not discuss if it was also used to mark and store the error annotations themselves. Some tools for post-editing of MT output, a related activity to error analysis, have been described in the literature. Font Llitj´os and Carbonell (2004) presented an online tool for"
P11-4010,P02-1040,0,0.099185,"T system and for any language pair. B LAST has a graphical user interface, and is designed to be easy 1 The BiLingual Annotation/Annotator/Analysis Support Tool, available for download at http://www.ida.liu. se/∼sarst/blast/ MT Evaluation and Error Analysis Hovy et al. (2002) discussed the complexity of MT evaluation, and stressed the importance of adjusting evaluation to the purpose and context of the translation. However, MT is very often only evaluated quantitatively using a single metric, especially in research papers. Quantitative evaluations can be automatic, using metrics such as Bleu (Papineni et al., 2002) or Meteor (Denkowski and Lavie, 2010), where the MT output is compared to one or more human reference translations. Metrics, however, only give a single quantitative score, and do not give any information about the strengths and weaknesses of the system. Comparing scores from different metrics can give a very rough indication of some major problems, especially in combination with a part-ofspeech analysis (Popovi´c et al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different syst"
P11-4010,W06-3101,0,0.201099,"Missing"
P11-4010,2006.amta-papers.25,0,0.0452619,"al., 2006). Human evaluation is also often quantitative, for instance in the form of estimates of values such as adequacy and fluency, or by ranking sentences from different systems (e.g. Callison-Burch et al. (2007)). A combination of human and automatic metrics is 56 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 56–61, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics human-targeted metrics such as HTER, where a human corrects the output of a system to the closest correct translation, on which standard metrics such as TER is then computed (Snover et al., 2006). While these types of evaluation are certainly useful, they are expensive and time-consuming, and still do not tell us anything about the particular errors of a system.2 Thus, we think that qualitative evaluation is an important complement, and that error analysis, the identification and classification of MT errors, is an important task. There have been several suggestions for general MT error typologies (Flanagan, 1994; Vilar et al., 2006; Farr´us et al., 2010), targeted at different user groups and purposes, focused on either evaluation of single systems, or comparison between systems. It i"
P11-4010,stymne-ahrenberg-2010-using,1,0.859863,"is an important complement to the tool itself. We intend to define a new finegrained general error typology, with extensions to a number of target languages. The modularity of B LAST also makes it possible to add new modules, for instance for preprocessing and to support other file formats. One example would be to support error annotation of only specific phenomena, such as verb errors, by adding a preprocessing module for highlighting verbs with support annotations, and a suitable verb-focused error typology. We are also working on a preprocessing module based on grammar checker techniques (Stymne and Ahrenberg, 2010), that highlights parts of the MT output that it suspects are non-grammatical. Even though the main purpose of B LAST is for error annotation of machine translation output, the freedom in the use of error typologies and support annotations also makes it suitable for other tasks where bilingual material is used, such as for annotations of named entities in bilingual texts, or for analyzing human translations, e.g. giving feedback to second language learners, with only the addition of a suitable typology, and possibly a preprocessing module. 6 Conclusion We presented B LAST; a flexible tool for"
P11-4010,vilar-etal-2006-error,0,0.515269,"Missing"
P13-4033,W09-2404,0,0.256094,"på (particular attentive on) + Simplified expression Table 2: Example translation snippets with comments Feature Baseline TTR OVIX QW QP All BLEU 0.243 0.243 0.243 0.242 0.243 0.235 OVIX 56.88 55.25 54.65 57.16 57.07 47.80 LIX 51.17 51.04 51.00 51.16 51.06 49.29 Table 1: Results for adding single lexical consistency features to Docent To evaluate our system we used the BLEU score (Papineni et al., 2002) together with a set of readability metrics, since readability is what we hoped to improve by adding consistency features. Here we used OVIX to confirm a direct impact on consistency, and LIX (Björnsson, 1968), which is a common readability measure for Swedish. Unfortunately we do not have access to simplified translated text, so we calculate the MT metrics against a standard reference, which means that simple texts will likely have worse scores than complicated texts closer to the reference translation. We tuned the standard features using Moses and MERT, and then added each lexical consistency feature with a small weight, using a grid search approach to find values with a small impact. The results are shown in Table 1. As can be seen, for individual features the translation quality was maintained"
P13-4033,D12-1026,0,0.0267005,"se and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2"
P13-4033,E12-3001,0,0.0603375,"the GNU General Public License and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based S"
P13-4033,2010.iwslt-papers.10,1,0.855274,"s. The code is released under the GNU General Public License and published on Github1 to make it easy for other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-leve"
P13-4033,D12-1108,1,0.812834,"(Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is performed by splitting the input sentence into a number of contiguous word sequences, called phrases, which are translated into the target language through a phrase dictionary lookup and optionally reordered. The choice between different translations of an ambiguous"
P13-4033,W11-2123,0,0.0578591,"e models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level models provided by Docent include the phrase table, n-gram language models implemented with the KenLM toolkit (Heafield, 2011), an unlexicalised distortion cost model with geometric decay (Koehn et al., 2003) and a word penalty cost. All of these features are designed to be compatible with the corresponding features in Moses. From among the typical set of baseline features in Moses, we have not implemented the lexicalised distortion model, but this model could easily be added if required. Docent uses the same binary file format for phrase tables as Moses, so the same training apparatus can be used. DP-based SMT decoders have a parameter called distortion limit that limits the difference in word order between the inpu"
P13-4033,D11-1125,0,0.0200806,"iscourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using different optimisation algorithms such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), and selective tuning of a subset of features. Since document features only give meaningful scores on the document level and not on the sentence level, we naturally perform optimisation on document level, which typically means that we need more data than for the optimisation of sentence-based decoding. The results we obtain are relatively stable and competitive with sentence-level optimisation of the same models (Stymne et al., 2013a). 4 Implementing Feature Models Efficiently While translating a document, the local search decoder attempts to make a great number of moves. For each move, a sco"
P13-4033,N03-1017,0,0.011021,"ystematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is"
P13-4033,P07-2045,0,0.0251314,"ed SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local search instead of dynamic programming and permits the integration of Document-Level Decoding with Local Search Our decoder is based on the phrase-based SMT model described by Koehn et al. (2003) and implemented, for example, in the popular Moses decoder (Koehn et al., 2007). Translation is performed by splitting the input sentence into a number of contiguous word sequences, called phrases, which are translated into the target language through a phrase dictionary lookup and optionally reordered. The choice between different translations of an ambiguous source phrase and the ordering of the target phrases are guided by a scoring function that combines a set of scores taken from the phrase table with scores from other models such as an n-gram language model. The actual translation process is realised as a search for the highest-scoring translation in the space of a"
P13-4033,2007.tmi-papers.13,0,0.0462402,"ocument at every 1 https://github.com/chardmeier/docent/wiki 193 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 193–198, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics stage of the search progress. Search proceeds by making small changes to the current search state in order to transform it gradually into a better translation. This differs from the DP algorithm used in other decoders, which starts with an empty translation and expands it bit by bit. It is similar to previous work on phrase-based SMT decoding by Langlais et al. (2007), but enables the creation of document-level models, which was not addressed by earlier approaches. Docent currently implements two search algorithms that are different generalisations of the hill climbing local search algorithm by Hardmeier et al. (2012). The original hill climbing algorithm starts with an initial state and generates possible successor states by randomly applying simple elementary operations to the state. After each operation, the new state is scored and accepted if its score is better than that of the previous state, else rejected. Search terminates when the decoder cannot f"
P13-4033,W10-1737,0,0.254585,"Missing"
P13-4033,2012.amta-papers.20,0,0.0288297,"r other researchers to use it in their own experiments. Motivation 2 Most of the research on statistical machine translation (SMT) that was conducted during the last 20 years treated every text as a “bag of sentences” and disregarded all relations between elements in different sentences. Systematic research into explicitly discourse-related problems has only begun very recently in the SMT community (Hardmeier, 2012) with work on topics such as pronominal anaphora (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Guillou, 2012), verb tense (Gong et al., 2012) and discourse connectives (Meyer et al., 2012). One of the problems that hamper the development of cross-sentence models for SMT is the fact that the assumption of sentence independence is at the heart of the dynamic programming (DP) beam search algorithm most commonly used for decoding in phrase-based SMT systems (Koehn et al., 2003). For integrating cross-sentence features into the decoding process, researchers had to adopt strategies like two-pass decoding (Le Nagard and Koehn, 2010). We have previously proposed an algorithm for document-level phrase-based SMT decoding (Hardmeier et al., 2012). Our decoding algorithm is based on local"
P13-4033,W03-2117,0,0.0178285,"as originally proposed by Hardmeier et al. (2012) to improve lexical cohesion in a document. It is a cross-sentence model over sequences of content words that are scored based on their similarity in a word vector space. The readability models serve to improve the readability of the translation by encouraging the selection of easier and more consistent target words. They are described and demonstrated in more detail in section 5. Docent can read input files both in the NISTXML format commonly used to encode documents in MT shared tasks such as NIST or WMT and in the more elaborate MMAX format (Müller and Strube, 2003). The MMAX format makes it possible to include a wide range of discourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using dif"
P13-4033,P03-1021,0,0.00654474,"e a wide range of discourselevel corpus annotations such as coreference links. 195 These annotations can then be accessed by the feature models. To allow for additional targetlanguage information such as morphological features of target words, Docent can handle simple word-level annotations that are encoded in the phrase table in the same way as target language factors in Moses. In order to optimise feature weights we have adapted the Moses tuning infrastructure to Docent. In this way we can take advantage of all its features, for instance using different optimisation algorithms such as MERT (Och, 2003) or PRO (Hopkins and May, 2011), and selective tuning of a subset of features. Since document features only give meaningful scores on the document level and not on the sentence level, we naturally perform optimisation on document level, which typically means that we need more data than for the optimisation of sentence-based decoding. The results we obtain are relatively stable and competitive with sentence-level optimisation of the same models (Stymne et al., 2013a). 4 Implementing Feature Models Efficiently While translating a document, the local search decoder attempts to make a great number"
P13-4033,P02-1040,0,0.105205,"ound to genitive construction − Changing long compound to English-based abbreviation − Removal of important word − Bad grammar because of changed part of speech and missing verb planen (the plan) särskilt uppmärksam på (particular attentive on) + Simplified expression Table 2: Example translation snippets with comments Feature Baseline TTR OVIX QW QP All BLEU 0.243 0.243 0.243 0.242 0.243 0.235 OVIX 56.88 55.25 54.65 57.16 57.07 47.80 LIX 51.17 51.04 51.00 51.16 51.06 49.29 Table 1: Results for adding single lexical consistency features to Docent To evaluate our system we used the BLEU score (Papineni et al., 2002) together with a set of readability metrics, since readability is what we hoped to improve by adding consistency features. Here we used OVIX to confirm a direct impact on consistency, and LIX (Björnsson, 1968), which is a common readability measure for Swedish. Unfortunately we do not have access to simplified translated text, so we calculate the MT metrics against a standard reference, which means that simple texts will likely have worse scores than complicated texts closer to the reference translation. We tuned the standard features using Moses and MERT, and then added each lexical consisten"
P13-4033,W13-3308,1,0.807594,"oes not require that another phrase be moved in the opposite direction at the same time. A pair of operations called permute-phrases and linearisephrases can reorder a sequence of phrases into random order and back into the order corresponding to the source language. Since the search algorithm in Docent is stochastic, repeated runs of the decoder will generally produce different output. However, the variance of the output is usually small, especially when initialising with a DP search pass, and it tends to be lower than the variance introduced by feature weight tuning (Hardmeier et al., 2012; Stymne et al., 2013a). 3 Available Feature Models In its current version, Docent implements a selection of sentence-local feature models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level mo"
P13-4033,W13-5634,1,0.909717,"oes not require that another phrase be moved in the opposite direction at the same time. A pair of operations called permute-phrases and linearisephrases can reorder a sequence of phrases into random order and back into the order corresponding to the source language. Since the search algorithm in Docent is stochastic, repeated runs of the decoder will generally produce different output. However, the variance of the output is usually small, especially when initialising with a DP search pass, and it tends to be lower than the variance introduced by feature weight tuning (Hardmeier et al., 2012; Stymne et al., 2013a). 3 Available Feature Models In its current version, Docent implements a selection of sentence-local feature models that makes it possible to build a baseline system with a configuration comparable to that of a typical Moses baseline system. The published source code also includes prototype implementations of a few document-level models. These models should be considered work in progress and serve as a demonstration of the cross-sentence modelling capabilities of the decoder. They have not yet reached a state of maturity that would make them suitable for production use. The sentence-level mo"
P13-4033,W10-2602,1,0.794735,"the readability of texts by encouraging simple and consistent terminology (Stymne et al., 2013b). This work is a first step towards achieving joint SMT and text simplification, with the final goal of adapting MT to user groups such as people with reading disabilities. Lexical consistency modelling for SMT has been attempted before. The suggested approaches have been limited by the use of sentence-level decoders, however, and had to resort to procedures like post processing (Carpuat, 2009), multiple decoding runs with frozen counts from previous runs (Ture et al., 2012), or cache-based models (Tiedemann, 2010). In Docent, however, we always have access to a full document translation, which makes it straightforward to include features directly into the decoder. We implemented four features on the document level. The first two features are type token ratio (TTR) and a reformulation of it, OVIX, which is less sensitive to text length. These ratios have been related to the “idea density” of a text (Mühlenbock and Kokkinakis, 2009). We also wanted to encourage consistent translations of words, for which we used the Q-value (Deléger et al., 2006), which has been proposed to measure term quality. We appli"
P13-4033,N12-1046,0,0.0590301,"can be used in Docent in order to improve the readability of texts by encouraging simple and consistent terminology (Stymne et al., 2013b). This work is a first step towards achieving joint SMT and text simplification, with the final goal of adapting MT to user groups such as people with reading disabilities. Lexical consistency modelling for SMT has been attempted before. The suggested approaches have been limited by the use of sentence-level decoders, however, and had to resort to procedures like post processing (Carpuat, 2009), multiple decoding runs with frozen counts from previous runs (Ture et al., 2012), or cache-based models (Tiedemann, 2010). In Docent, however, we always have access to a full document translation, which makes it straightforward to include features directly into the decoder. We implemented four features on the document level. The first two features are type token ratio (TTR) and a reformulation of it, OVIX, which is less sensitive to text length. These ratios have been related to the “idea density” of a text (Mühlenbock and Kokkinakis, 2009). We also wanted to encourage consistent translations of words, for which we used the Q-value (Deléger et al., 2006), which has been p"
P18-2098,Q16-1031,0,0.0226612,"Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics the parser (see section 3.1). To the best of our knowledge this approach is novel when applied to the monolingual case as treebank embeddings. The most similar approach we have found in the literature is Lim and Poibeau (2017), who used one-hot treebank representations to combine data for improving monolingual parsing for three tiny treebanks, with improvements of 0.6–1.9 LAS. It is also related to work on domain embeddings for machine translation (Kobus et al., 2017), and language embeddings for parsing (Ammar et al., 2016). We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-write rules (Aufrant et al., 2016), or machine translation"
P18-2098,W17-6314,1,0.74758,"Missing"
P18-2098,C16-1012,0,0.069536,"ge embeddings for parsing (Ammar et al., 2016). We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-write rules (Aufrant et al., 2016), or machine translation (Tiedemann, 2015). Unlike much work on cross-lingual parsing, we do not focus on a low-resource scenario. dev data for most languages, results were mixed on the actual PUD test sets. For the two Norwegian language variants, concatenation has been proposed (Velldal et al., 2017), but it hurts results unless combined with machine translation. Training on concatenated treebanks can be improved by a subsequent fine-tuning step. In this set-up, after training the model on concatenated data, it is refined for each treebank by training only on its own training set for a few a"
P18-2098,K17-3006,0,0.0400169,"al. (2017) and Das et al. (2017) used this strategy to parse the PUD test sets in the 2017 CoNLL Shared Task. Little details are given on the results, but while it was successful on 619 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 619–625 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics the parser (see section 3.1). To the best of our knowledge this approach is novel when applied to the monolingual case as treebank embeddings. The most similar approach we have found in the literature is Lim and Poibeau (2017), who used one-hot treebank representations to combine data for improving monolingual parsing for three tiny treebanks, with improvements of 0.6–1.9 LAS. It is also related to work on domain embeddings for machine translation (Kobus et al., 2017), and language embeddings for parsing (Ammar et al., 2016). We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedem"
P18-2098,W18-0207,0,0.0121505,"representations to combine data for improving monolingual parsing for three tiny treebanks, with improvements of 0.6–1.9 LAS. It is also related to work on domain embeddings for machine translation (Kobus et al., 2017), and language embeddings for parsing (Ammar et al., 2016). We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-write rules (Aufrant et al., 2016), or machine translation (Tiedemann, 2015). Unlike much work on cross-lingual parsing, we do not focus on a low-resource scenario. dev data for most languages, results were mixed on the actual PUD test sets. For the two Norwegian language variants, concatenation has been proposed (Velldal et al., 2017), but it hurts results unless combined with machine translation. Training"
P18-2098,P09-1040,1,0.826754,"aster/udapi/block/ eval/conll17.py 620 3.1 The Parser based on LAS score on the dev set, using average dev scores when training on more than one treebank, and apply the model from this epoch to the test data. UUParser2 We use (de Lhoneux et al., 2017a), which is based on the transition-based parser of Kiperwasser and Goldberg (2016), and adapted to UD. It uses the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). This model allows the construction of non-projective dependency trees (Nivre, 2009). A configuration c is represented by a feature function φ(·) over a subset of its elements and, for each configuration, transitions are scored by a classifier. In this case, the classifier is a multilayer perceptron (MLP) and φ(·) is a concatenation of the BiLSTM vectors vi of words on top of the stack and at the beginning of the buffer. The MLP scores transitions together with the arc labels for transitions that involve adding an arc. For an input sentence of length n with words w1 , . . . , wn , the parser creates a sequence of vectors x1:n , where the vector xi representing wi is the conca"
P18-2098,K17-3019,0,0.021669,"human annotators, whereas others are based entirely on automatic conversions. All this means that it is often far from trivial to combine multiple treebanks for the same language. 2 Training with Multiple Treebanks The most obvious way to combine treebanks for a particular language, provided that they use the same annotation scheme, is simply to concatenate the training sets. This has the advantage that it does not require any modifications to the parser itself, and it produces a single model that can be directly used for any input from the language in question. Bj¨orkelund et al. (2017) and Das et al. (2017) used this strategy to parse the PUD test sets in the 2017 CoNLL Shared Task. Little details are given on the results, but while it was successful on 619 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 619–625 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics the parser (see section 3.1). To the best of our knowledge this approach is novel when applied to the monolingual case as treebank embeddings. The most similar approach we have found in the literature is Lim and Poibeau (2017), who used"
P18-2098,K17-3001,1,0.721773,"checked by human annotators, whereas others are based entirely on automatic conversions. All this means that it is often far from trivial to combine multiple treebanks for the same language. 2 Training with Multiple Treebanks The most obvious way to combine treebanks for a particular language, provided that they use the same annotation scheme, is simply to concatenate the training sets. This has the advantage that it does not require any modifications to the parser itself, and it produces a single model that can be directly used for any input from the language in question. Bj¨orkelund et al. (2017) and Das et al. (2017) used this strategy to parse the PUD test sets in the 2017 CoNLL Shared Task. Little details are given on the results, but while it was successful on 619 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 619–625 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics the parser (see section 3.1). To the best of our knowledge this approach is novel when applied to the monolingual case as treebank embeddings. The most similar approach we have found in the literature is Lim and Poi"
P18-2098,Q16-1023,0,0.0626433,"efit of using treebank embeddings is that we can train a single model for each language using all available data while remaining sensitive to the differences between treebanks. The addition of treebank embeddings requires only minor modifications to 1 https://github.com/udapi/ udapi-python/blob/master/udapi/block/ eval/conll17.py 620 3.1 The Parser based on LAS score on the dev set, using average dev scores when training on more than one treebank, and apply the model from this epoch to the test data. UUParser2 We use (de Lhoneux et al., 2017a), which is based on the transition-based parser of Kiperwasser and Goldberg (2016), and adapted to UD. It uses the arc-hybrid transition system from Kuhlmann et al. (2011) extended with a S WAP transition and a static-dynamic oracle, as described in de Lhoneux et al. (2017b). This model allows the construction of non-projective dependency trees (Nivre, 2009). A configuration c is represented by a feature function φ(·) over a subset of its elements and, for each configuration, transitions are scored by a classifier. In this case, the classifier is a multilayer perceptron (MLP) and φ(·) is a concatenation of the BiLSTM vectors vi of words on top of the stack and at the beginn"
P18-2098,kobus-etal-2017-domain,0,0.0152307,"Computational Linguistics (Short Papers), pages 619–625 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics the parser (see section 3.1). To the best of our knowledge this approach is novel when applied to the monolingual case as treebank embeddings. The most similar approach we have found in the literature is Lim and Poibeau (2017), who used one-hot treebank representations to combine data for improving monolingual parsing for three tiny treebanks, with improvements of 0.6–1.9 LAS. It is also related to work on domain embeddings for machine translation (Kobus et al., 2017), and language embeddings for parsing (Ammar et al., 2016). We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-w"
P18-2098,P11-1068,0,0.16891,"Missing"
P18-2098,N16-1161,0,0.0187953,"the literature is Lim and Poibeau (2017), who used one-hot treebank representations to combine data for improving monolingual parsing for three tiny treebanks, with improvements of 0.6–1.9 LAS. It is also related to work on domain embeddings for machine translation (Kobus et al., 2017), and language embeddings for parsing (Ammar et al., 2016). We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-write rules (Aufrant et al., 2016), or machine translation (Tiedemann, 2015). Unlike much work on cross-lingual parsing, we do not focus on a low-resource scenario. dev data for most languages, results were mixed on the actual PUD test sets. For the two Norwegian language variants, concatenation has been proposed (Velldal et al., 201"
P18-2098,W17-0201,0,0.0232521,"vetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-write rules (Aufrant et al., 2016), or machine translation (Tiedemann, 2015). Unlike much work on cross-lingual parsing, we do not focus on a low-resource scenario. dev data for most languages, results were mixed on the actual PUD test sets. For the two Norwegian language variants, concatenation has been proposed (Velldal et al., 2017), but it hurts results unless combined with machine translation. Training on concatenated treebanks can be improved by a subsequent fine-tuning step. In this set-up, after training the model on concatenated data, it is refined for each treebank by training only on its own training set for a few additional epochs. This enables the models to learn differences between treebanks, but it requires more training, and results in separate models for each treebank. When the parser is applied to new data, there is thus a choice of which fine-tuned version to use. This approach was used by Che et al. (201"
P18-2098,E17-2102,0,0.0172275,"and Poibeau (2017), who used one-hot treebank representations to combine data for improving monolingual parsing for three tiny treebanks, with improvements of 0.6–1.9 LAS. It is also related to work on domain embeddings for machine translation (Kobus et al., 2017), and language embeddings for parsing (Ammar et al., 2016). We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-write rules (Aufrant et al., 2016), or machine translation (Tiedemann, 2015). Unlike much work on cross-lingual parsing, we do not focus on a low-resource scenario. dev data for most languages, results were mixed on the actual PUD test sets. For the two Norwegian language variants, concatenation has been proposed (Velldal et al., 2017), but it hurts results unles"
P18-2098,K17-3007,0,0.326154,"s thus a choice of which fine-tuned version to use. This approach was used by Che et al. (2017) and Shi et al. (2017) for languages with multiple treebanks in the CoNLL 2017 Shared Task. Che et al. (2017) apply fine-tuning to all but the largest treebank for each language, and show average gains of 1.8 LAS for a subset of nine treebanks. Shi et al. (2017) show that the choice of treebank for parsing the PUD test set is important, but do not have any specific evaluation of the effect of fine-tuning. Another approach, not explored in this paper, is shared gated adversarial networks, proposed by Sato et al. (2017) for the CoNLL 2017 Shared Task. They use treebank prediction as an adversarial task. In this model, treebank-specific BiLSTMs are constructed for all treebanks in addition to a shared BiLSTM which is used both for parsing and for the adversarial task. This method requires knowing at test time which treebank the input belongs to. Sato et al. (2017) show that this strategy can give substantial improvements, especially for small treebanks. For large treebanks, however, there are mostly no or only minor improvements. 3 Experimental Setup We perform experiments for 24 treebanks from 9 languages, u"
P18-2098,K17-3003,0,0.0583157,"t hurts results unless combined with machine translation. Training on concatenated treebanks can be improved by a subsequent fine-tuning step. In this set-up, after training the model on concatenated data, it is refined for each treebank by training only on its own training set for a few additional epochs. This enables the models to learn differences between treebanks, but it requires more training, and results in separate models for each treebank. When the parser is applied to new data, there is thus a choice of which fine-tuned version to use. This approach was used by Che et al. (2017) and Shi et al. (2017) for languages with multiple treebanks in the CoNLL 2017 Shared Task. Che et al. (2017) apply fine-tuning to all but the largest treebank for each language, and show average gains of 1.8 LAS for a subset of nine treebanks. Shi et al. (2017) show that the choice of treebank for parsing the PUD test set is important, but do not have any specific evaluation of the effect of fine-tuning. Another approach, not explored in this paper, is shared gated adversarial networks, proposed by Sato et al. (2017) for the CoNLL 2017 Shared Task. They use treebank prediction as an adversarial task. In this model"
P18-2098,W15-2137,0,0.0788772,"We previously used a similar architecture for combining languages with very small training sets with additional languages (de Lhoneux et al., 2017a). Language embeddings have also been explored for other cross-lingual tasks such as lan¨ guage modeling (Tsvetkov et al., 2016; Ostling and Tiedemann, 2017) and POS-tagging (Bjerva and Augenstein, 2018). Cross-lingual parsing, however, often requires substantially more complex models. They typically include features such as multilingual word embeddings (Ammar et al., 2016), linguistic re-write rules (Aufrant et al., 2016), or machine translation (Tiedemann, 2015). Unlike much work on cross-lingual parsing, we do not focus on a low-resource scenario. dev data for most languages, results were mixed on the actual PUD test sets. For the two Norwegian language variants, concatenation has been proposed (Velldal et al., 2017), but it hurts results unless combined with machine translation. Training on concatenated treebanks can be improved by a subsequent fine-tuning step. In this set-up, after training the model on concatenated data, it is refined for each treebank by training only on its own training set for a few additional epochs. This enables the models"
P18-2098,K17-3005,0,\N,Missing
P18-2098,K17-3004,0,\N,Missing
stymne-ahrenberg-2010-using,1997.tmi-1.12,0,\N,Missing
stymne-ahrenberg-2010-using,W09-0402,0,\N,Missing
stymne-ahrenberg-2010-using,W07-0411,0,\N,Missing
stymne-ahrenberg-2010-using,W09-0441,0,\N,Missing
stymne-ahrenberg-2010-using,W07-2209,0,\N,Missing
stymne-ahrenberg-2010-using,W08-0332,0,\N,Missing
stymne-ahrenberg-2010-using,E09-3008,1,\N,Missing
stymne-ahrenberg-2010-using,1999.mtsummit-1.8,0,\N,Missing
stymne-ahrenberg-2010-using,P02-1040,0,\N,Missing
stymne-ahrenberg-2010-using,D07-1091,0,\N,Missing
stymne-ahrenberg-2010-using,W09-0401,0,\N,Missing
stymne-ahrenberg-2010-using,P07-2045,0,\N,Missing
stymne-ahrenberg-2010-using,W08-0317,1,\N,Missing
stymne-ahrenberg-2010-using,W07-0734,0,\N,Missing
stymne-ahrenberg-2010-using,2005.mtsummit-papers.11,0,\N,Missing
stymne-ahrenberg-2010-using,W99-1005,0,\N,Missing
stymne-ahrenberg-2010-using,P03-1021,0,\N,Missing
stymne-ahrenberg-2012-practice,elliott-etal-2004-fluency,0,\N,Missing
stymne-ahrenberg-2012-practice,P02-1040,0,\N,Missing
stymne-ahrenberg-2012-practice,P07-2045,0,\N,Missing
stymne-ahrenberg-2012-practice,P08-1087,0,\N,Missing
stymne-ahrenberg-2012-practice,W07-0718,0,\N,Missing
stymne-ahrenberg-2012-practice,J11-4004,0,\N,Missing
stymne-ahrenberg-2012-practice,P11-4010,1,\N,Missing
stymne-ahrenberg-2012-practice,W05-0908,0,\N,Missing
stymne-ahrenberg-2012-practice,2005.mtsummit-papers.11,0,\N,Missing
stymne-ahrenberg-2012-practice,vilar-etal-2006-error,0,\N,Missing
stymne-ahrenberg-2012-practice,2008.eamt-1.25,1,\N,Missing
stymne-ahrenberg-2012-practice,2010.eamt-1.12,0,\N,Missing
stymne-ahrenberg-2012-practice,2011.eamt-1.36,0,\N,Missing
stymne-etal-2012-eye,E06-1032,0,\N,Missing
stymne-etal-2012-eye,W10-1751,0,\N,Missing
stymne-etal-2012-eye,P02-1040,0,\N,Missing
stymne-etal-2012-eye,1999.mtsummit-1.42,0,\N,Missing
stymne-etal-2012-eye,P07-2045,0,\N,Missing
stymne-etal-2012-eye,W07-0734,0,\N,Missing
stymne-etal-2012-eye,D08-1064,0,\N,Missing
stymne-etal-2012-eye,W07-0718,0,\N,Missing
stymne-etal-2012-eye,W05-0908,0,\N,Missing
stymne-etal-2012-eye,2005.mtsummit-papers.11,0,\N,Missing
stymne-etal-2012-eye,2008.eamt-1.25,1,\N,Missing
stymne-etal-2012-eye,2010.eamt-1.12,0,\N,Missing
stymne-etal-2012-eye,W06-3101,0,\N,Missing
W07-0723,E03-1076,0,0.382824,"riments were performed on the Europarl data. With factored statistical machine translation, different levels of linguistic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1 . It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic information: lemma, partof-speech and morphology. The compound boundary identification was used to split nou"
W07-0723,J04-2003,0,0.0270794,"und boundary identification was used to split noun com1 Connexor Oy, http://www.connexor.com. 181 Proceedings of the Second Workshop on Statistical Machine Translation, pages 181–184, c Prague, June 2007. 2007 Association for Computational Linguistics pounds to make the German input more similar to English text. 1 Mit mit pm&gt;2 @PREMARK PREP 2 Blick blick advl&gt;10 @NH N MSC SG DAT 3 auf auf pm&gt;5 @PREMARK PREP Figure 1. Example of parser output We used the parser’s tokenization as given. Some common multiword units, such as ‘at all’ and ‘von heute’, are treated as single words by the parser (cf. Niessen and Ney, 2004). The German parser also splits contracted prepositions and determiners like ‘zum’ – ‘zu dem’ (“to the”). 3 System description For our experiments with Moses we basically followed the shared task baseline system setup to train our factored translation models. After training a statistical model, minimum error-rate tuning was performed to tune the model parameters. All experiments were performed on an AMD 64 Athlon 4000+ processor with 4 Gb of RAM and 32 bit Linux (Ubuntu). Since time as well as computer resources were limited we designed a model that we hoped would make the best use of all avai"
W07-0723,C00-2162,0,0.0395781,"-wrong Erbonkel Uncle dna Ref: Sugar daddy U-untranslated Schlussentwurf Schlussentwurf Ref: Final draft Table 3. Classification scheme with examples for compound translations Tot C 44 V 18 P 5 W 3 U 0 F 1 4 5 Tot 38 10 8 2 12 5 75 Table 4. Classification of 75 compounds from our second system and the baseline system Second system C 36 1 Baseline system V P W U F 1 3 3 1 9 2 1 5 3 2 1 2 184 Decompounding of nouns reduced the number of untranslated words, but there were still some left. Among these were cases that can be handled such as separable prefix verbs like ‘aufzeigten’ (“pointed out”) (Niessen and Ney, 2000) or adjective compounds such as ‘multidimensionale’ (“multi dimensional”). There were also some noun compounds left which indicates that we might need a better decompounding strategy than the one used by the parser (see e.g. Koehn and Knight, 2003). 4.2 Experiences and future plans With the computer equipment at our disposal, training of the models and tuning of the parameters turned out to be a very time-consuming task. For this reason, the number of system setups we could test was small, and much fewer than we had hoped for. Thus it is too early to draw any conclusions as regards our hypothe"
W07-0723,C04-1045,0,0.0230352,"er than our knowledge of German, making it easier to judge the quality of translation output. Experiments were performed on the Europarl data. With factored statistical machine translation, different levels of linguistic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1 . It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic informati"
W07-0723,W06-3108,0,0.0273706,"tic information can be taken into account during training of a statistical translation system and decoding. In our experiments we combined syntactic and morphological factors from an off-the-shelf parser with the factored translation framework in Moses (Moses, 2007). We wanted to test the following hypotheses: • Translation models based on lemmas will improve translation quality (Popovič and Ney, 2004) • Decompounding German nominal compounds will improve translation quality (Koehn and Knight, 2003) • Re-ordering models based on word forms and parts-of-speech will improve translation quality (Zens and Ney, 2006). 2 The parser The parser, Machinese Syntax, is a commercially available dependency parser from Connexor Oy 1 . It provides each word with lemma, part-of-speech, morphological features and dependency relations (see Figure 1). In addition, the lemmas of compounds are marked by a ‘#’ separating the two parts of the compound. For the shared task we only used shallow linguistic information: lemma, partof-speech and morphology. The compound boundary identification was used to split noun com1 Connexor Oy, http://www.connexor.com. 181 Proceedings of the Second Workshop on Statistical Machine Translat"
W08-0317,P05-1066,0,0.218385,"Missing"
W08-0317,W07-0723,1,0.860427,"amework using factored models (Koehn et al., 2007). Furthermore, by parsing input sentences and restructuring based on the result to narrow the structural difference between source and target language, the current phrase-based models can be used more effectively (Collins et al., 2005). German differs structurally from English in several respects (see e.g. Collins et al., 2005). In this work we wanted to look at one particular aspect of restructuring, namely splitting of German compounds, and evaluate its effect in both translation directions, thus extending the initial experiments reported in Holmqvist et al. (2007). In addition, since For both English and German we used the part-ofspeech tagger TreeTagger (Schmid, 1994) to obtain POS-tags. The German POS-tags from TreeTagger were refined by adding morphological information from a commercial dependency parser, including case, number, gender, definiteness, and person for nouns, pronouns, verbs, adjectives and determiners in the cases where both tools agreed on the POS-tag. If they did not agree, the POS-tag from TreeTagger was chosen. This tag set seemed more suitable for SMT, with tags for proper names and foreign words which the commercial parser does n"
W08-0317,E03-1076,0,0.706555,"und (Langer, 1998). We have chosen to allow simple additions of letter(s) (-s, -n, -en, -nen, -es, -er, -ien) and simple truncations (-e, 135 Proceedings of the Third Workshop on Statistical Machine Translation, pages 135–138, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics -en, -n). Example of compounds with additions and truncations can be seen in (1). (1) a. Staatsfeind (Staat + Feind) public enemy b. Kirchhof (Kirche + Hof) graveyard 3.1 Splitting compounds Noun and adjective compounds are split by a modified version of the corpus-based method presented by Koehn and Knight (2003). First the German language model data is POS-tagged and used to calculate frequencies of all nouns, verbs, adjectives, adverbs and the negative particle. Then, for each noun and adjective all splits into these known words from the corpus, allowing filler additions and truncations, are considered, choosing the splitting option with the highest arithmetic mean1 of the frequencies of its parts. A length limit of each part was set to 4 characters. For adjectives we restrict the number of parts to maximum two, since they do not tend to have multiple parts as often as nouns. In addition we added a"
W08-0317,P07-2045,0,0.0229417,"ce model for German using morphologically rich parts-of-speech. It is shown that these additions lead to improved translations. 2 Part-of-speech and Morphology 1 Introduction Research in statistical machine translation (SMT) increasingly makes use of linguistic analysis in order to improve performance. By including abstract categories, such as lemmas and parts-of-speech (POS), in the models, it is argued that systems can become better at handling sentences for which training data at the word level is sparse. Such categories can be integrated in the statistical framework using factored models (Koehn et al., 2007). Furthermore, by parsing input sentences and restructuring based on the result to narrow the structural difference between source and target language, the current phrase-based models can be used more effectively (Collins et al., 2005). German differs structurally from English in several respects (see e.g. Collins et al., 2005). In this work we wanted to look at one particular aspect of restructuring, namely splitting of German compounds, and evaluate its effect in both translation directions, thus extending the initial experiments reported in Holmqvist et al. (2007). In addition, since For bo"
W08-0317,P02-1040,0,0.0797884,"comparison, we constructed a baseline according to the shared-task description, but with smaller tuning corpus, and the same sentence filtering for the translation model as in the submitted system, using only sentences of length 2-40. In addition we constructed a factored baseline system, with POS as an output factor and a sequence model for POS. Here we only used the original POS-tags from TreeTagger, no additional morphology was added for German. 137 De-En 19.54 20.16 20.61 En-De 14.31 14.37 14.77 Table 2: Bleu scores for News Commentary (nc-test2007) 5 Results Case-sensitive Bleu scores4 (Papineni et al., 2002) for the Europarl devtest set (test2007) are shown in table 1. We can see that the submitted system performs best, and that the factored baseline is better than the pure baseline, especially for translation into English. Bleu scores for News Commentary5 (nc-test2007) are shown in Table 2. Here we can also see that the submitted system is the best. As expected, Bleu is much lower on out-of-domain news text than on the Europarl development test set. 5.1 Compounds The quality of compound translations were analysed manually. The first 100 compounds that could be found by the splitting algorithm we"
W09-0421,W08-0312,0,0.11068,"st, Sara Stymne, Jody Foo and Lars Ahrenberg Department of Computer and Information Science Linköping University, Sweden {marho,sarst,jodfo,lah}@ida.liu.se Abstract the first method in addition to the processing of compounds and additional sequence models used by Stymne et al. (2008). Heuristics were used to generate true-cased versions of the translations that were submitted, as reported in section 6. In this paper we report case-insensitive Bleu scores (Papineni et al., 2002), unless otherwise stated, calculated with the NIST tool, and caseinsensitive Meteor-ranking scores, without WordNet (Agarwal and Lavie, 2008). We describe the LIU systems for EnglishGerman and German-English translation in the WMT09 shared task. We focus on two methods to improve the word alignment: (i) by applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) by adding lexical data obtained as highprecision alignments from a different word aligner. These methods were studied in the context of a system that uses compound processing, a morphological sequence model for German, and a partof-speech sequence model for English. Both methods gave some im"
W09-0421,P04-1023,0,0.0501952,"Missing"
W09-0421,P02-1040,0,0.0760101,"Missing"
W09-0421,P06-1097,0,0.0596532,"Missing"
W09-0421,J07-3002,0,0.0219777,"ystem of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by extending the alignment algorithm. For this year’s shared task we therefore studied the effects of improving word alignment in the context of our system for the WMT09 shared task. Two methods were tried: (i) applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) adding lexical data obtained as high-precision alignments f"
W09-0421,P08-1112,0,0.0267503,"l., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by extending the alignment algorithm. For this year’s shared task we therefore studied the effects of improving word alignment in the context of our system for the WMT09 shared task. Two methods were tried: (i) applying Giza++ in a second phase to a reordered training corpus, where reordering is based on the alignments from the first phase, and (ii) adding lexical data obtained as high-precision alignments from a different word aligner. The submitted system includes 2.1 Sequence model based on part-of-speech and morphology The translat"
W09-0421,W08-0317,1,0.862537,"sion alignments from a different word aligner. These methods were studied in the context of a system that uses compound processing, a morphological sequence model for German, and a partof-speech sequence model for English. Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whethe"
W09-0421,E03-1076,0,0.0342188,"he Fourth Workshop on Statistical Machine Translation , pages 120–124, c Athens, Greece, 30 March – 31 March 2009. 2009 Association for Computational Linguistics 120 dependency parser2 . We used the extra factor in an additional sequence model which can improve agreement between words, and word order. For German this factor was also used for compound merging. 2.2 Corpus news-commentary09 Europarl news-train08 Table 1: Number of sentences in the corpora (after filtering) Compound processing Prior to training and translation, compound processing was performed using an empirical method based on (Koehn and Knight, 2003; Stymne, 2008). Words were split if they could be split into parts that occur in a monolingual corpus. We chose the split with the highest arithmetic mean of the corpus frequencies of compound parts. We split nouns, adjectives and verbs into parts that were content words or particles. A part had to be at least 3 characters in length and a stop list was used to avoid parts that often lead to errors, such as arische (Aryan) in konsularische (consular). Compound parts sometimes have special compound suffixes, which could be additions or truncations of letters, or combinations of these. We used t"
W09-0421,W07-0733,0,0.040885,"improved the results drastically, as shown in Table 2. The other experiments reported in this paper are based on the mixed+ system. Domain adaptation This year three training corpora were available, a small bilingual news commentary corpus, a reasonably large Europarl corpus, and a very large monolingual news corpus, see Table 1 for details. The bilingual data was filtered to remove sentences longer than 60 words. Because the German news training corpus contained a number of English sentences, this corpus was cleaned by removing sentences containing a number of common English words. Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data 4 Improved alignment by reordering Word alignment with Giza++ has been shown to improve from making the source and target language more similar, e.g., in terms of segmentation (Ma et al., 2007) or word order. We used the following simple procedure to improve alignment of the training corpus by reordering the words in one of the texts according to the 2 Machinese syntax, from Connexor Oy http://www. connexor.eu 121 Corpus Mixed+ Re-Src Re-Trg En⇒De Bleu Meteor 14.62 49.48 14.63 49.80 14.51 48.62 D"
W09-0421,E09-3008,1,0.826008,"ten lead to errors, such as arische (Aryan) in konsularische (consular). Compound parts sometimes have special compound suffixes, which could be additions or truncations of letters, or combinations of these. We used the top 10 suffixes from a corpus study of Langer (1998), and we also treated hyphens as suffixes of compound parts. Compound parts were given a special part-of-speech tag that matched the head word. For translation into German, compound parts were merged to form compounds, both during test and tuning. The merging is based on the special part-of-speech tag used for compound parts (Stymne, 2009). A token with this POS-tag is merged with the next token, either if the POS-tags match, or if it results in a known word. 3 German English 81,141 1,331,262 9,619,406 21,215,311 Corpus News com. Europarl Mixed Mixed+ En⇒De Bleu Meteor 12.13 47.01 12.92 47.27 12.91 47.96 14.62 49.48 De⇒En Bleu Meteor 17.21 36.08 18.53 37.65 18.76 37.69 19.92 38.18 Table 2: Results of domain adaptation from the news domain. We used the possibility to include several translation models in the Moses decoder by using multiple alternative decoding paths. We first trained systems on either bilingual news data or Euro"
W09-0421,P07-2045,0,0.0133413,"e model for English. Both methods gave some improvements to translation quality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev e"
W09-0421,P07-1039,0,0.0319336,"gual news corpus, see Table 1 for details. The bilingual data was filtered to remove sentences longer than 60 words. Because the German news training corpus contained a number of English sentences, this corpus was cleaned by removing sentences containing a number of common English words. Based on Koehn and Schroeder (2007) we adapted our system from last year, which was focused on Europarl, to perform well on test data 4 Improved alignment by reordering Word alignment with Giza++ has been shown to improve from making the source and target language more similar, e.g., in terms of segmentation (Ma et al., 2007) or word order. We used the following simple procedure to improve alignment of the training corpus by reordering the words in one of the texts according to the 2 Machinese syntax, from Connexor Oy http://www. connexor.eu 121 Corpus Mixed+ Re-Src Re-Trg En⇒De Bleu Meteor 14.62 49.48 14.63 49.80 14.51 48.62 De⇒En Bleu Meteor 19.92 38.18 20.54 38.86 20.48 38.73 5 Augmenting the corpus with an extracted dictionary Previous research (Callison-Burch et al., 2004; Fraser and Marcu, 2006) has shown that including word aligned data during training can improve translation results. In our case we include"
W09-0421,W07-2456,1,0.870679,"s according to the 2 Machinese syntax, from Connexor Oy http://www. connexor.eu 121 Corpus Mixed+ Re-Src Re-Trg En⇒De Bleu Meteor 14.62 49.48 14.63 49.80 14.51 48.62 De⇒En Bleu Meteor 19.92 38.18 20.54 38.86 20.48 38.73 5 Augmenting the corpus with an extracted dictionary Previous research (Callison-Burch et al., 2004; Fraser and Marcu, 2006) has shown that including word aligned data during training can improve translation results. In our case we included a dictionary extracted from the news-commentary corpus during the word alignment. Using a method originally developed for term extraction (Merkel and Foo, 2007), the newscommentary09 corpus was grammatically annotated and aligned using a heuristic word aligner. Candidate dictionary entries were extracted from the alignments. In order to optimize the quality of the dictionary, dictionary entry candidates were ranked according to their Q-value, a metric specifically designed for aligned data (Merkel and Foo, 2007). The Q-value is based on the following statistics: Table 3: Results of reordering experiments word order in the other language: 1. Word align the corpus with Giza++. 2. Reorder the German words according to the order of the English words they"
W09-0421,J03-1002,0,0.0173665,"ality as measured by Bleu and Meteor scores, though not consistently. All systems used both out-ofdomain and in-domain data as the mixed corpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by"
W09-0421,P03-1021,0,0.00739997,"rpus had better scores in the baseline configuration. 1 2 Baseline system Our baseline system uses compound splitting, compound merging and part-ofspeech/morphological sequence models (Stymne et al., 2008). Except for these additions it is similar to the baseline system of the workshop1 . The translation system is a factored phrasebased translation system that uses the Moses toolkit (Koehn et al., 2007) for decoding and training, GIZA++ for word alignment (Och and Ney, 2003), and SRILM (Stolcke, 2002) for language models. Minimum error rate training was used to tune the model feature weights (Och, 2003). Tuning was performed on the news-dev2009a set with 1025 sentences. All development testing was performed on the news-dev2009b set with 1026 sentences. Introduction It is an open question whether improved word alignment actually improves statistical MT. Fraser and Marcu (2007) found that improved alignments as measured by AER will not necessarily improve translation quality, whereas Ganchev et al. (2008) did improve translation quality on several language pairs by extending the alignment algorithm. For this year’s shared task we therefore studied the effects of improving word alignment in the"
W10-1727,W08-0312,0,0.0748074,"Missing"
W10-1727,J03-1002,0,0.0213145,"sis of the alignments from Giza++, and (ii) reorder by moving all verbs to the end of segments. In translation, out-ofvocabulary words were preprocessed in a knowledge-lite fashion to identify a likely equivalent. All three strategies were implemented for our English↔German system submitted to the WMT10 shared task. Combining them lead to improvements in both language directions. 1 2 Baseline System The 2010 Liu system is based on the PBSMT baseline system for the WMT shared translation task1 . We use the Moses toolkit (Koehn et al., 2007) for decoding and to train translation models, Giza++ (Och and Ney, 2003) for word alignment, and the SRILM toolkit (Stolcke, 2002) to train language models. The main difference to the WMT baseline is that the Liu system is trained on truecased data, as in Koehn et al. (2008), instead of lowercased data. This means that there is no need for a full recasing step after translation, instead we only need to uppercase the first word in each sentence. Introduction We present the Liu translation system for the constrained condition of the WMT10 shared translation task, between German and English in both directions. The system is based on the 2009 Liu submission (Holmqvist"
W10-1727,P02-1040,0,0.0778775,"Missing"
W10-1727,W05-0908,0,0.0138745,"u.se Abstract step, based on casing, stemming, and splitting of hyphenated compounds. In addition, we perform general compound splitting for German both before training and translation, which also reduces the OOV rate. All results in this article are for the development test set newstest2009, on truecased output. We report Bleu scores (Papineni et al., 2002) and Meteor ranking (without WordNet) scores (Agarwal and Lavie, 2008), using percent notation. We also used other metrics, but as they gave similar results they are not reported. For significance testing we used approximate randomization (Riezler and Maxwell, 2005), with p &lt; 0.05. In this paper we report on experiments with three preprocessing strategies for improving translation output in a statistical MT system. In training, two reordering strategies were studied: (i) reorder on the basis of the alignments from Giza++, and (ii) reorder by moving all verbs to the end of segments. In translation, out-ofvocabulary words were preprocessed in a knowledge-lite fashion to identify a likely equivalent. All three strategies were implemented for our English↔German system submitted to the WMT10 shared task. Combining them lead to improvements in both language di"
W10-1727,W09-0421,1,0.932571,"ey, 2003) for word alignment, and the SRILM toolkit (Stolcke, 2002) to train language models. The main difference to the WMT baseline is that the Liu system is trained on truecased data, as in Koehn et al. (2008), instead of lowercased data. This means that there is no need for a full recasing step after translation, instead we only need to uppercase the first word in each sentence. Introduction We present the Liu translation system for the constrained condition of the WMT10 shared translation task, between German and English in both directions. The system is based on the 2009 Liu submission (Holmqvist et al., 2009), that used compound processing, morphological sequence models, and improved alignment by reordering. This year we have focused on two issues: translation of verbs, which is problematic for translation between English and German since the verb placement is different with German verbs often being placed at the end of sentences; and OOVs, outof-vocabulary words, which are problematic for machine translation in general. Verb translation is targeted by trying to improve alignment, which we believe is a crucial step for verb translation since verbs that are far apart are often not aligned at all. W"
W10-1727,C08-1098,0,0.0647831,"Missing"
W10-1727,E03-1076,0,0.0501394,"ms instead of 7-grams for the morphological models. It was still impossible to train on the monolingual English news corpus, with nearly 50 million sentences, so we split that corpus into three equal size parts, and trained three models, that were interpolated with equal weights. 3 Bleu 13.42 13.85 14.24 Bleu 18.34 18.39 18.50 Meteor 38.13 37.86 38.47 Table 2: Results for morphological processing, German→English and agreement between words. For German the factor was also used for compound merging. Prior to training and translation, compound processing was performed, using an empirical method (Koehn and Knight, 2003; Stymne, 2008) that splits words if they can be split into parts that occur in a monolingual corpus, choosing the splitting option with the highest arithmetic mean of its part frequencies in the corpus. We split nouns, adjectives and verbs, into parts that are content words or particles. We imposed a length limit on parts of 3 characters for translation from German and of 6 characters for translation from English, and we had a stop list of parts that often led to errors, such as arische (Aryan) in konsularische (consular). We allowed 10 common letter changes (Langer, 1998) and hyphens at spli"
W10-1727,W07-0733,0,0.0247001,"he translation and reordering models were trained using the bilingual Europarl and news commentary corpora, which we concatenated. We used two sets of language models, one where we first trained two models on Europarl and news commentary, which we then interpolated 1 http://www.statmt.org/wmt10/baseline. html 183 Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 183–188, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics System Baseline + morph + comp with more weight given to the news commentary, using weights from Koehn and Schroeder (2007). The second set of language models were trained on monolingual news data. For tuning we used every second sentence, in total 1025 sentences, of news-test2008. 2.2 Meteor 48.83 49.69 49.41 Table 1: Results for morphological processing, English→German Training with Limited Computational Resources System Baseline + morph + comp One challenge for us was to train the translation sytem with limited computational resources. We trained all systems on one Intel Core 2 CPU, 3.0Ghz, 16 Gb of RAM, 64 bit Linux (RedHat) machine. This constrained the possibilities of using the data provided by the workshop"
W10-1727,2008.eamt-1.25,1,0.927718,"orpus. We split nouns, adjectives and verbs, into parts that are content words or particles. We imposed a length limit on parts of 3 characters for translation from German and of 6 characters for translation from English, and we had a stop list of parts that often led to errors, such as arische (Aryan) in konsularische (consular). We allowed 10 common letter changes (Langer, 1998) and hyphens at split points. Compound parts were given a special part-of-speech tag that matches the head word. For translation into German, compound parts were merged into full compounds using a method described in Stymne and Holmqvist (2008), which is based on matching of the special part-of-speech tag for compound parts. A word with a compound POS-tag were merged with the next word, if their POS-tags were matching. Tables 1 and 2 show the results of the additional morphological processing. Adding the sequence models on morphologically enriched partof-speech tags gave a significant improvement for translation into German, but similar or worse results as the baseline for translation into English. This is not surprising, since German morphology is more complex than English morphology. The addition of compound processing significant"
W10-1727,W08-0318,0,0.0524119,"equivalent. All three strategies were implemented for our English↔German system submitted to the WMT10 shared task. Combining them lead to improvements in both language directions. 1 2 Baseline System The 2010 Liu system is based on the PBSMT baseline system for the WMT shared translation task1 . We use the Moses toolkit (Koehn et al., 2007) for decoding and to train translation models, Giza++ (Och and Ney, 2003) for word alignment, and the SRILM toolkit (Stolcke, 2002) to train language models. The main difference to the WMT baseline is that the Liu system is trained on truecased data, as in Koehn et al. (2008), instead of lowercased data. This means that there is no need for a full recasing step after translation, instead we only need to uppercase the first word in each sentence. Introduction We present the Liu translation system for the constrained condition of the WMT10 shared translation task, between German and English in both directions. The system is based on the 2009 Liu submission (Holmqvist et al., 2009), that used compound processing, morphological sequence models, and improved alignment by reordering. This year we have focused on two issues: translation of verbs, which is problematic for"
W10-1727,N04-1022,0,0.0215819,"reference The best and most technically well-equipped telephones, with a 3.5 mm jack for ordinary headphones, cost up to fifteen thousand crowns. Figure 1: Example of the effects of OOV processing for German→English System base + OOV base+verb + OOV + MBR Bleu 14.24 14.26 14.38 14.42 14.41 Meteor 49.41 49.43 49.72 49.75 49.77 6 For the final Liu shared task submission we used the base+verb+reorder+OOV system for German→English and the base+verb+OOV system for English→German, which had the best overall scores considering all metrics. To these systems we added minimum Bayes risk (MBR) decoding (Kumar and Byrne, 2004). In standard decoding, the top suggestion of the translation system is chosen as the system output. In MBR decoding the risk is spread by choosing the translation that is most similar to the N highest scoring translation suggestions from the system, with N = 100, as suggested in Koehn et al. (2008). MBR decoding gave hardly any changes in automatic scores, as shown in Tables 6 and 7. The final system was significantly better than the baseline in all cases, and significantly better than base on Meteor in both translation directions, and on Bleu for translation into English. Table 6: Results fo"
W10-1727,D07-1077,0,0.0206414,"logical sequence models. Overall, we believe that both compound splitting and morphology are useful; thus all experiments reported in the sequel are based on the baseline system with morphology models and compound splitting, which we will call base. 4 Meteor 49.41 49.58 49.22 49.72 49.39 Table 3: Results for improved alignment, English→German System base reorder verb base+verb base+verb+reorder Improved Alignment by Reordering Previous work has shown that translation quality can be improved by making the source language more similar to the target language, for instance in terms of word order (Wang et al., 2007; Xia and McCord, 2004). In order to harmonize the word order of the source and target sentence, they applied hand-crafted or automatically induced reordering rules to the source sentences of the training corpus. At decoding time, reordering rules were again applied to input sentences before translation. The positive effects of such methods seem to come from a combination of improved alignment and improved reordering during translation. In contrast, we focus on improving the word alignment by reordering the training corpus. The training corpus is reordered prior to word alignment with Giza++ ("
W10-1727,C04-1073,0,0.0651508,"dels. Overall, we believe that both compound splitting and morphology are useful; thus all experiments reported in the sequel are based on the baseline system with morphology models and compound splitting, which we will call base. 4 Meteor 49.41 49.58 49.22 49.72 49.39 Table 3: Results for improved alignment, English→German System base reorder verb base+verb base+verb+reorder Improved Alignment by Reordering Previous work has shown that translation quality can be improved by making the source language more similar to the target language, for instance in terms of word order (Wang et al., 2007; Xia and McCord, 2004). In order to harmonize the word order of the source and target sentence, they applied hand-crafted or automatically induced reordering rules to the source sentences of the training corpus. At decoding time, reordering rules were again applied to input sentences before translation. The positive effects of such methods seem to come from a combination of improved alignment and improved reordering during translation. In contrast, we focus on improving the word alignment by reordering the training corpus. The training corpus is reordered prior to word alignment with Giza++ (Och and Ney, 2003) and"
W10-1727,P09-1089,0,0.0283,"Missing"
W10-1727,P07-2045,0,\N,Missing
W11-2129,W02-1001,0,0.054442,"Missing"
W11-2129,W06-3102,0,0.0742761,"suggested a merging method based on partof-speech matching, in a factored translation system, where compound parts had a special part-of-speech tag, and compound parts are only merged with the next word if the part-of-speech tags match. This resulted in improved translation quality from English to German, and from English to Swedish (Stymne and Holmqvist, 2008). Another method, based on several decoding runs, was investigated by Fraser (2009). Stymne (2009a) investigated and compared merging methods inspired by Popovi´c et al. (2006), Stymne (2008) and a method inspired by morphology merging (El-Kahlout and Oflazer, 2006; Virpioja et al., 2007), where compound parts were annotated with symbols, and parts with symbols in the translation output were merged with the next word. 3 Promoting coalescence of compounds If compounds are split in the training set, then there is no guarantee that translations of components will end up in contiguous positions and in the correct order. This is primarily a language model problem, and we will model it as such by applying POS language models on specially designed part-of-speech sets, and by applying language model inspired count features. The approach proposed in Stymne (2008"
W11-2129,W09-0420,0,0.147896,"parts, and merged with the next word if it results in a known compound. This method led to improved overall translation results from English to German. Stymne (2008) suggested a merging method based on partof-speech matching, in a factored translation system, where compound parts had a special part-of-speech tag, and compound parts are only merged with the next word if the part-of-speech tags match. This resulted in improved translation quality from English to German, and from English to Swedish (Stymne and Holmqvist, 2008). Another method, based on several decoding runs, was investigated by Fraser (2009). Stymne (2009a) investigated and compared merging methods inspired by Popovi´c et al. (2006), Stymne (2008) and a method inspired by morphology merging (El-Kahlout and Oflazer, 2006; Virpioja et al., 2007), where compound parts were annotated with symbols, and parts with symbols in the translation output were merged with the next word. 3 Promoting coalescence of compounds If compounds are split in the training set, then there is no guarantee that translations of components will end up in contiguous positions and in the correct order. This is primarily a language model problem, and we will mod"
W11-2129,W07-2432,0,0.211255,"word after. We aimed to include features representing the knowledge available to the list and POS heuristics, by including part-of-speech tags and frequencies for compounds and bigrams, as well as a comparison between them. Features were also inspired by previous work on compound splitting, based on the intuition that features that are useful for splitting compounds, could also be useful for merging. Character n-grams has successfully been used for splitting Swedish compounds, as the only knowledge source by Brodda (1979), and as one of several knowledge sources by Sj¨obergh and Kann (2004). Friberg (2007) tried to normalize letters, beside using the original letters. While she was not successful, we still believe in the potential of this feature. Larson et al. (2000), used frequencies of prefixes and suffixes from a corpus, as a basis of their method for splitting German compounds. 254 Training data for the sequence labeler Since features are strongly lexicalized, a suitably large training dataset is required to prevent overfitting, ruling out the possibility of manual labelling. We created our training data automatically, using the two heuristics described earlier, plus a third one enabled by"
W11-2129,D07-1091,0,0.0420983,"translated the whole source, then labeled decision points according to the heuristics. The translations we obtain are thus biased, of higher quality than those we should expect to obtain on unseen data. Nevertheless they are substantially more similar to what will be observed in operations than the reference translations. 5 Experiments We performed experiments on translation from English into Swedish and Danish on two different corpora, an automotive corpus collected from a proprietary translation memory, and on Europarl (Koehn, 2005) for the merging experiments. We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the Corpus Languages Compounds split POS tag-sets Decoder Training sentences SMT Training words SMT (target) Training sentences CRF Extra training sentences CRF EU-Sv Europarl English→Swedish N, V, Adj POS Moses 1,520,549 34,282,247 248,808 3,000 Auto-Sv Automotive English→Swedish N, V, Adj POS,RPOS in-house 329,090 3,061,282 317,398 3,000 Auto-Da Automotive English→Danish N RPOS in-house 168,047 1,553,382 164,702 163,201 Table 2: Overview of the experimental settings target side, with a sequence model on part-ofspeech. We used two decoders,"
W11-2129,E03-1076,0,0.303651,"317,398 3,000 Auto-Da Automotive English→Danish N RPOS in-house 168,047 1,553,382 164,702 163,201 Table 2: Overview of the experimental settings target side, with a sequence model on part-ofspeech. We used two decoders, Matrax (Simard et al., 2005) and Moses (Koehn et al., 2007), both standard statistical phrase based decoders. For parameter optimization we used minimum error rate training (Och, 2003) with Moses and gradient ascent on smoothed NIST for the in-house decoder. In the merging experiments we used the CRF++ toolkit.2 Compounds were split before training using a corpus-based method (Koehn and Knight, 2003; Stymne, 2008). For each word we explored all possible segmentations into parts that had at least 3 characters, and choose the segmentation which had the highest arithmetic mean of frequencies for each part in the training corpus. We constrained the splitting based on part-of-speech by only allowing splitting options where the compound head had the same tag as the full word. The split compound parts kept their form, which can be special to compounds, and no symbols or other markup were added. The experiment setup is summarized in Table 2. The extra training sentences for CRF are sentences tha"
W11-2129,2005.mtsummit-papers.11,0,0.034689,"for a compromise: we trained the SMT system on the whole training data, translated the whole source, then labeled decision points according to the heuristics. The translations we obtain are thus biased, of higher quality than those we should expect to obtain on unseen data. Nevertheless they are substantially more similar to what will be observed in operations than the reference translations. 5 Experiments We performed experiments on translation from English into Swedish and Danish on two different corpora, an automotive corpus collected from a proprietary translation memory, and on Europarl (Koehn, 2005) for the merging experiments. We used factored translation (Koehn and Hoang, 2007), with both surface words and part-of-speech tags on the Corpus Languages Compounds split POS tag-sets Decoder Training sentences SMT Training words SMT (target) Training sentences CRF Extra training sentences CRF EU-Sv Europarl English→Swedish N, V, Adj POS Moses 1,520,549 34,282,247 248,808 3,000 Auto-Sv Automotive English→Swedish N, V, Adj POS,RPOS in-house 329,090 3,061,282 317,398 3,000 Auto-Da Automotive English→Danish N RPOS in-house 168,047 1,553,382 164,702 163,201 Table 2: Overview of the experimental s"
W11-2129,P03-1021,0,0.0510912,"ences CRF Extra training sentences CRF EU-Sv Europarl English→Swedish N, V, Adj POS Moses 1,520,549 34,282,247 248,808 3,000 Auto-Sv Automotive English→Swedish N, V, Adj POS,RPOS in-house 329,090 3,061,282 317,398 3,000 Auto-Da Automotive English→Danish N RPOS in-house 168,047 1,553,382 164,702 163,201 Table 2: Overview of the experimental settings target side, with a sequence model on part-ofspeech. We used two decoders, Matrax (Simard et al., 2005) and Moses (Koehn et al., 2007), both standard statistical phrase based decoders. For parameter optimization we used minimum error rate training (Och, 2003) with Moses and gradient ascent on smoothed NIST for the in-house decoder. In the merging experiments we used the CRF++ toolkit.2 Compounds were split before training using a corpus-based method (Koehn and Knight, 2003; Stymne, 2008). For each word we explored all possible segmentations into parts that had at least 3 characters, and choose the segmentation which had the highest arithmetic mean of frequencies for each part in the training corpus. We constrained the splitting based on part-of-speech by only allowing splitting options where the compound head had the same tag as the full word. The"
W11-2129,P02-1040,0,0.0812911,"000 iterations, and α &lt; 0.05. 5.1 Experiments: Promoting compound coalescence We performed experiments with factored translation models with the restricted part-of-speech set on the Danish and Swedish automotive corpus. In these experiments we compared the restricted part-of-speech set we suggest in this work to several baseline systems without any compound processing and with factored models using the extended part-of-speech set suggested by Stymne (2008). Compound parts were merged using the POS-based heuristic. Results are reported on two standard metrics, NIST (Doddington, 2002) and Bleu (Papineni et al., 2002), on lower-cased data. For all sequence models we use 3-grams. Results on the two Automotive corpora are summarized in Table 3. The scores are very high, which is due to the fact that it is an easy domain with many repetitive sentence types. On the Danish dataset, we observe significant improvements in BLEU and NIST over the baseline for all methods where compounds were split before translation and merged afterwards. Some of the gain is already obtained using a language model on the extended part-of-speech set. Additional gains can however be obtained using instead a language model on a reduce"
W11-2129,W05-0908,0,0.0181803,"here we used 2,000 sentences for tuning. In the Swedish experiments we split nouns, adjectives and verbs, and used the full POS-set, except in the coalescence experiments where we compared the full and restricted POS-sets. For Danish we only split nouns, and used the restricted POS-set. For frequency calculations of compounds and compound parts that were needed for compound splitting and some of the com2 Available at http://crfpp.sourceforge.net/ 255 pound merging strategies, we used the respective training data in all cases. Significance testing was performed using approximate randomization (Riezler and Maxwell, 2005), with 10,000 iterations, and α &lt; 0.05. 5.1 Experiments: Promoting compound coalescence We performed experiments with factored translation models with the restricted part-of-speech set on the Danish and Swedish automotive corpus. In these experiments we compared the restricted part-of-speech set we suggest in this work to several baseline systems without any compound processing and with factored models using the extended part-of-speech set suggested by Stymne (2008). Compound parts were merged using the POS-based heuristic. Results are reported on two standard metrics, NIST (Doddington, 2002)"
W11-2129,H05-1095,1,0.803103,"th surface words and part-of-speech tags on the Corpus Languages Compounds split POS tag-sets Decoder Training sentences SMT Training words SMT (target) Training sentences CRF Extra training sentences CRF EU-Sv Europarl English→Swedish N, V, Adj POS Moses 1,520,549 34,282,247 248,808 3,000 Auto-Sv Automotive English→Swedish N, V, Adj POS,RPOS in-house 329,090 3,061,282 317,398 3,000 Auto-Da Automotive English→Danish N RPOS in-house 168,047 1,553,382 164,702 163,201 Table 2: Overview of the experimental settings target side, with a sequence model on part-ofspeech. We used two decoders, Matrax (Simard et al., 2005) and Moses (Koehn et al., 2007), both standard statistical phrase based decoders. For parameter optimization we used minimum error rate training (Och, 2003) with Moses and gradient ascent on smoothed NIST for the in-house decoder. In the merging experiments we used the CRF++ toolkit.2 Compounds were split before training using a corpus-based method (Koehn and Knight, 2003; Stymne, 2008). For each word we explored all possible segmentations into parts that had at least 3 characters, and choose the segmentation which had the highest arithmetic mean of frequencies for each part in the training co"
W11-2129,sjobergh-kann-2004-finding,0,0.288208,"Missing"
W11-2129,2008.eamt-1.25,1,0.824481,"by Popovi´c et al. (2006). Each word in the translation output is looked up in a list of compound parts, and merged with the next word if it results in a known compound. This method led to improved overall translation results from English to German. Stymne (2008) suggested a merging method based on partof-speech matching, in a factored translation system, where compound parts had a special part-of-speech tag, and compound parts are only merged with the next word if the part-of-speech tags match. This resulted in improved translation quality from English to German, and from English to Swedish (Stymne and Holmqvist, 2008). Another method, based on several decoding runs, was investigated by Fraser (2009). Stymne (2009a) investigated and compared merging methods inspired by Popovi´c et al. (2006), Stymne (2008) and a method inspired by morphology merging (El-Kahlout and Oflazer, 2006; Virpioja et al., 2007), where compound parts were annotated with symbols, and parts with symbols in the translation output were merged with the next word. 3 Promoting coalescence of compounds If compounds are split in the training set, then there is no guarantee that translations of components will end up in contiguous positions an"
W11-2129,E09-3008,1,0.87514,"ged with the next word if it results in a known compound. This method led to improved overall translation results from English to German. Stymne (2008) suggested a merging method based on partof-speech matching, in a factored translation system, where compound parts had a special part-of-speech tag, and compound parts are only merged with the next word if the part-of-speech tags match. This resulted in improved translation quality from English to German, and from English to Swedish (Stymne and Holmqvist, 2008). Another method, based on several decoding runs, was investigated by Fraser (2009). Stymne (2009a) investigated and compared merging methods inspired by Popovi´c et al. (2006), Stymne (2008) and a method inspired by morphology merging (El-Kahlout and Oflazer, 2006; Virpioja et al., 2007), where compound parts were annotated with symbols, and parts with symbols in the translation output were merged with the next word. 3 Promoting coalescence of compounds If compounds are split in the training set, then there is no guarantee that translations of components will end up in contiguous positions and in the correct order. This is primarily a language model problem, and we will model it as such"
W11-2129,2007.mtsummit-papers.65,0,0.345095,"Missing"
W11-2129,P07-2045,0,\N,Missing
W11-2147,W08-0312,0,0.0472087,"Missing"
W11-2147,P05-1066,0,0.22974,"Missing"
W11-2147,2010.iwslt-papers.6,0,0.0176481,"as measured by NIST and Meteor. For translation in the other direction, the differences on automatic metrics were very small. Still, we decided to use the hierarchical model in all our systems. 3 German–English For translation from German into English we focused on making the German source text more similar to English by removing redundant morphology and changing word order before training translation models. 3.1 Normalization We performed normalization of German words to remove distinctions that do not exist in English, such as case distinctions on nouns. This strategy is similar to that of El-Kahlout and Yvon (2010), but we used a slightly different set of transformations, that we thought better mirrored the English structure. For morphological tags we used RFTagger and for lemmas we used TreeTagger. The morphological transformations we performed were the following: • Nouns: – Replace with lemma+s if plural number – Replace with lemma otherwise • Verbs: – Replace with lemma if present tense, not third person singular – Replace with lemma+p if past tense Baseline +hier reo +normalization +source reordering + OOV proc. BLEU 21.01 20.94 20.85 21.06 21.22 NIST 6.2742 6.2800 6.2370 6.3082 6.3692 Meteor 41.32"
W11-2147,D08-1089,0,0.0171223,"mpound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-speech tags, was merged with the next word if that word had a matching part-of-speech tag. If the compound part was followed by the conjunction und (and), we added a hyphen to the part, to account for coordinated compounds. 2.3 Hierarchical reordering In our baseline system we experimented with two lexicalized reordering models. The standard model in Moses (Koehn et al., 2005), and the hierarchical model of Galley and Manning (2008). In both models the placement of a phrase is compared to that of the previous and/or next phrase. In the standard model up to three reorderings are distinguished, monotone, swap, and discontinuous. In the hierarchical model the discontinuous class can be further subdivided into two classes, left and right discontinuous. The hierarchical model further differs from the standard model in that it compares the order of the phrase with the next or previous block of phrases, not only with the next or previous single phrase. We investigated one configuration of each model. For the standard model we u"
W11-2147,W11-2123,0,0.0163588,"slation we also added morphological normalization, source side reordering, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reordering models were trained using the bilingual Europarl and News Commentary corpora that were concatenated before training. We created two language models. The first model is a 5-gram model that we created by interpolating two language 393 P"
W11-2147,holmqvist-2010-heuristic,1,0.849719,"ee that while the normalization did not seem to have a positive effect on any metric, both source reordering and OOV processing led to small increases on all scores. 4 English–German For translation from English into German we attempted to improve the quality of the phrase table by adding new word alignments to the standard Giza++ alignments. 4.1 Phrase-based word alignment We experimented with different ways of combining word alignments from Giza++ with alignments created using phrase-based word alignment (PAL) which previously has been shown to improve alignment quality for English–Swedish (Holmqvist, 2010). The idea of phrase-based word alignment is to use word and part-of-speech sequence patterns from manual word alignments to align new texts. First, parallel phrases containing a source segment, a target segment and links between source and target words are extracted from word aligned texts (Figure 1). In the second step, these phrases are matched against new parallel text and if a matching phrase is found, word links from the phrase are added to the corresponding words in the new text. In order to increase the number of matching phrases and improve word alignment recall, words in the parallel"
W11-2147,E03-1076,0,0.0364454,"s were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs. We trained two sequence models for each system over this output factor and added them as features in our baseline system. The first sequence model is a 7-gram model interpolated from models of bilingual Europarl and News Commentary. The second model is a 6-gram model trained on monolingual News only. 2.2 Compound processing In both translation directions we split compounds, using a modified version of the corpus-based splitting method of Koehn and Knight (2003). We split nouns, verb, and adjective compounds into known parts that were content words or cardinal numbers, based on the arithmetic mean of the frequency of the parts in the training corpus. We allowed 10 common letter changes (Langer, 1998) and hyphens at split points. Compound parts were kept in their surface form and compound modifiers received a partof-speech tag based on that of the tag of the full compound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-"
W11-2147,2005.iwslt-1.8,0,0.024884,"-speech tag based on that of the tag of the full compound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-speech tags, was merged with the next word if that word had a matching part-of-speech tag. If the compound part was followed by the conjunction und (and), we added a hyphen to the part, to account for coordinated compounds. 2.3 Hierarchical reordering In our baseline system we experimented with two lexicalized reordering models. The standard model in Moses (Koehn et al., 2005), and the hierarchical model of Galley and Manning (2008). In both models the placement of a phrase is compared to that of the previous and/or next phrase. In the standard model up to three reorderings are distinguished, monotone, swap, and discontinuous. In the hierarchical model the discontinuous class can be further subdivided into two classes, left and right discontinuous. The hierarchical model further differs from the standard model in that it compares the order of the phrase with the next or previous block of phrases, not only with the next or previous single phrase. We investigated one"
W11-2147,J03-1002,0,0.00546384,"ce models, and a hierarchical reordering model. For German–English translation we also added morphological normalization, source side reordering, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reordering models were trained using the bilingual Europarl and News Commentary corpora that were concatenated before training. We created two language models. The first model i"
W11-2147,P03-1021,0,0.0310854,"g, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reordering models were trained using the bilingual Europarl and News Commentary corpora that were concatenated before training. We created two language models. The first model is a 5-gram model that we created by interpolating two language 393 Proceedings of the 6th Workshop on Statistical Machine Translation,"
W11-2147,P06-1146,0,0.0179087,"ks: 0-0 1-1 2-2 En: DT JJ NN De: ART ADJA N Links: 0-0 1-1 2-2 Figure 1: Examples of parallel phrases used in word alignment. Baseline +hier reo +pal-gdfa +pal-dual +pal-inter BLEU 16.16 16.06 16.14 15.71 15.92 NIST 6.2742 6.2800 5.6527 5.5735 5.6230 Meteor 50.89 51.25 51.10 50.43 50.73 Table 2: English–German translation results, results are cumulative except for the three alternative PALconfigurations. segments were replaced by POS/morphological tags from RFTagger. Alignment patterns were extracted from 1000 sentences in the manually word aligned sample of English–German Europarl texts from Pado and Lapata (2006). All parallel phrases were extracted from the word aligned texts, as when extracting a translation model. Parallel phrases that contain at least 3 words were generalized with POS tags to form word/POS patterns for alignment. A subset of these patterns, with high alignment precision (&gt; 0.80) on the 1000 sentences, were used to align the entire training corpus. We combined the new word alignments with the Giza++ alignments in two ways. In the first method, we used a symmetrization heuristic similar to grow-diag-final-and to combine three word alignments into one, the phrase-based alignment and"
W11-2147,P02-1040,0,0.0796483,"Missing"
W11-2147,C08-1098,0,0.0130567,"newstest2009 and all results reported in Tables 1 and 2 are based on a devtest set of 1244 sentences from newstest2010. 2.1 Sequence models with part-of-speech and morphology To improve target word order and agreement in the translation output, we added an extra output factor in our translation models consisting of tags with POS and morphological features. For English we used tags that were obtained by enriching POS tags from TreeTagger (Schmid, 1994) with additional morphological features such as number for determiners. For German, the POS and morphological tags were obtained from RFTagger (Schmid and Laws, 2008) which provides morphological information such as case, number and gender for nouns and tense for verbs. We trained two sequence models for each system over this output factor and added them as features in our baseline system. The first sequence model is a 7-gram model interpolated from models of bilingual Europarl and News Commentary. The second model is a 6-gram model trained on monolingual News only. 2.2 Compound processing In both translation directions we split compounds, using a modified version of the corpus-based splitting method of Koehn and Knight (2003). We split nouns, verb, and ad"
W11-2147,W10-1727,1,0.79791,"ty of German compared to English, as well as dealing with previously unseen words. In both translation directions our systems include compound processing, morphological sequence models, and a hierarchical reordering model. For German–English translation we also added morphological normalization, source side reordering, and processing of out-of-vocabulary words (OOVs). For English–German translation, we extracted word alignments with a supervised method and combined these alignments with Giza++ alignments in various Baseline System This years improvements were added to the LIU baseline system (Stymne et al., 2010). Our baseline is a factored phrase based SMT system that uses the Moses toolkit (Koehn et al., 2007) for translation model training and decoding, GIZA++ (Och and Ney, 2003) for word alignment, SRILM (Stolcke, 2002) an KenLM (Heafield, 2011) for language modelling and minimum error rate training (Och, 2003) to tune model feature weights. In addition, the LIU baseline contains: • Compound processing, including compound splitting and for translation into German also compound merging • Part-of-speech and morphological sequence models All models were trained on truecased data. Translation and reor"
W11-2147,E09-3008,1,0.833722,"ds, using a modified version of the corpus-based splitting method of Koehn and Knight (2003). We split nouns, verb, and adjective compounds into known parts that were content words or cardinal numbers, based on the arithmetic mean of the frequency of the parts in the training corpus. We allowed 10 common letter changes (Langer, 1998) and hyphens at split points. Compound parts were kept in their surface form and compound modifiers received a partof-speech tag based on that of the tag of the full compound. For translation into German, compounds were 394 merged using the POS-merging strategy of Stymne (2009). A compound part in the translation output, identified by the special part-of-speech tags, was merged with the next word if that word had a matching part-of-speech tag. If the compound part was followed by the conjunction und (and), we added a hyphen to the part, to account for coordinated compounds. 2.3 Hierarchical reordering In our baseline system we experimented with two lexicalized reordering models. The standard model in Moses (Koehn et al., 2005), and the hierarchical model of Galley and Manning (2008). In both models the placement of a phrase is compared to that of the previous and/or"
W11-2147,P07-2045,0,\N,Missing
W11-2159,P06-2005,0,0.11664,"Missing"
W11-2159,N10-1064,0,0.0192777,"pelling variant in the training corpus. We thus decided to treat OOVs using a method inspired by spell-checking techniques, and applied an approximate string matching technique to OOVs in the translation input in order to change them into known spelling variants. OOV replacement has been proposed by several researchers, replacing OOVs e.g. by morphological variants (Arora et al., 2008) or synonyms (Mirkin et al., 2009). Habash (2008) used several techniques for expanding OOVs in order to extend the phrasetable. Yang and Kirchhoff (2006) trained a morphologically based back-off model for OOVs. Bertoldi et al. (2010) created confusion networks as input of translation input with artificially created misspelled words, not specifically targetting OOVs, however. The work most similar to ours is DeNeefe et al. (2008), who also created lattices with spelling alternatives for OOVs, which did not improve translation results, however. Contrary to us, they only considered one edit per word, and did not weigh edits or lattice arcs. Many standard spell checkers are based on the noisy channel model, which use an error (channel) model and a source model, which is normally modeled by a language model. The error model no"
W11-2159,P00-1037,0,0.665215,"r model normally use some type of approximate string matching, such as Levenshtein distance (Levenshtein, 1966), which measures the distance between two strings as the number of insertions, deletions, and substitutions of characters. It is often normalized based on the length of the strings (Yujian and Bo, 2007), and the distance calculation has also been improved by associating different costs to individual error operations. Church and Gale (1991) used a large training corpus to assign probabilities to each unique error operation, and also conditioned operations on one consecutive character. Brill and Moore (2000) introduced a model that worked on character sequences, not only on character level, and was conditioned on where in the word the sequences occurred. They trained weights on a corpus of misspelled words with corrections. Treating OOVs in the SMS corpus as a spell checking problem differs from a standard spell checking scenario in that the goal is not necessarily to change an incorrectly spelled word into a correct word, but rather to change a word that is not in our corpus into a spelling variant that we have seen in the corpus, but which might not necessarily be correctly spelled. It is also"
W11-2159,2008.amta-papers.7,0,0.0364607,"ion input in order to change them into known spelling variants. OOV replacement has been proposed by several researchers, replacing OOVs e.g. by morphological variants (Arora et al., 2008) or synonyms (Mirkin et al., 2009). Habash (2008) used several techniques for expanding OOVs in order to extend the phrasetable. Yang and Kirchhoff (2006) trained a morphologically based back-off model for OOVs. Bertoldi et al. (2010) created confusion networks as input of translation input with artificially created misspelled words, not specifically targetting OOVs, however. The work most similar to ours is DeNeefe et al. (2008), who also created lattices with spelling alternatives for OOVs, which did not improve translation results, however. Contrary to us, they only considered one edit per word, and did not weigh edits or lattice arcs. Many standard spell checkers are based on the noisy channel model, which use an error (channel) model and a source model, which is normally modeled by a language model. The error model normally use some type of approximate string matching, such as Levenshtein distance (Levenshtein, 1966), which measures the distance between two strings as the number of insertions, deletions, and subs"
W11-2159,P08-2015,0,0.0144523,"op`ot, and aewopo, and in the devtest in a seventh variant ay´eoport. The non-standardized spelling means that many unknown words (out-of-vocabulary words, OOVs) have a known spelling variant in the training corpus. We thus decided to treat OOVs using a method inspired by spell-checking techniques, and applied an approximate string matching technique to OOVs in the translation input in order to change them into known spelling variants. OOV replacement has been proposed by several researchers, replacing OOVs e.g. by morphological variants (Arora et al., 2008) or synonyms (Mirkin et al., 2009). Habash (2008) used several techniques for expanding OOVs in order to extend the phrasetable. Yang and Kirchhoff (2006) trained a morphologically based back-off model for OOVs. Bertoldi et al. (2010) created confusion networks as input of translation input with artificially created misspelled words, not specifically targetting OOVs, however. The work most similar to ours is DeNeefe et al. (2008), who also created lattices with spelling alternatives for OOVs, which did not improve translation results, however. Contrary to us, they only considered one edit per word, and did not weigh edits or lattice arcs. Ma"
W11-2159,W11-2123,0,0.0399596,"Missing"
W11-2159,W07-0733,0,0.0202186,"ge The corpora available for the task was a small bilingual in-domain corpus of SMT data, a limited amount of bilingual out-of-domain corpora, such as dictionaries and the Bible. This is different to the common situation of domain adaptation, as in the standard WMT shared tasks, where there is a small bilingual in-domain corpus, a larger in-domain monolingual corpus, and possibly several out-ofdomain corpora that can be both monolingual and bilingual. In such a scenario it is often useful to use all available training data for both translation and language models, possibly in separate models (Koehn and Schroeder, 2007). Table 1 summarizes how we used the available corpora, in our different models. For translation and language models we separated the bilingual data into three parts, the SMS data, the Bible, and everything else. For our lexicalized reordering model we only used SMS data, since we believe word order there is likely to differ from the other corpora. For the English true-casing model we concatenated the English side of all bilingual corpora that were not lower-cased. Table 2 shows the results of the different model combinations on the clean devtest data. When we used only the SMS data in the tra"
W11-2159,2005.iwslt-1.8,0,0.0604121,"Missing"
W11-2159,W07-0734,0,0.0137772,"85k 90k 25k 3k 3k 850k TM SMS other other other other other other other bible LM SMS other other other other other other other bible Reo yes – – – – – – – – TC yes – yes – yes – yes yes yes Table 1: Corpora used for training translation models (TM), language models (LM), lexicalized reordering model (Reo), and true-casing model (TC). All corpora are bilingual English–Haitian Creole. All translation results are reported for the devtest corpus, on truecased data. We report results on three metrics, Bleu (Papineni et al., 2002), NIST (Doddington, 2002), and Meteor optimized on fluency/adequacy (Lavie and Agarwal, 2007). 3 Corpus Usage The corpora available for the task was a small bilingual in-domain corpus of SMT data, a limited amount of bilingual out-of-domain corpora, such as dictionaries and the Bible. This is different to the common situation of domain adaptation, as in the standard WMT shared tasks, where there is a small bilingual in-domain corpus, a larger in-domain monolingual corpus, and possibly several out-ofdomain corpora that can be both monolingual and bilingual. In such a scenario it is often useful to use all available training data for both translation and language models, possibly in sep"
W11-2159,P09-1089,0,0.0142982,"ayeop`o, aeroport, aeyop`ot, and aewopo, and in the devtest in a seventh variant ay´eoport. The non-standardized spelling means that many unknown words (out-of-vocabulary words, OOVs) have a known spelling variant in the training corpus. We thus decided to treat OOVs using a method inspired by spell-checking techniques, and applied an approximate string matching technique to OOVs in the translation input in order to change them into known spelling variants. OOV replacement has been proposed by several researchers, replacing OOVs e.g. by morphological variants (Arora et al., 2008) or synonyms (Mirkin et al., 2009). Habash (2008) used several techniques for expanding OOVs in order to extend the phrasetable. Yang and Kirchhoff (2006) trained a morphologically based back-off model for OOVs. Bertoldi et al. (2010) created confusion networks as input of translation input with artificially created misspelled words, not specifically targetting OOVs, however. The work most similar to ours is DeNeefe et al. (2008), who also created lattices with spelling alternatives for OOVs, which did not improve translation results, however. Contrary to us, they only considered one edit per word, and did not weigh edits or l"
W11-2159,J03-1002,0,0.00881959,"Missing"
W11-2159,P03-1021,0,0.0131739,"Missing"
W11-2159,P02-1040,0,0.0819727,"Bible Sentences 17,192 1,619 13,517 35,728 8,476 10,499 1,687 658 30,715 Words 35k 10k 30k 85k 90k 25k 3k 3k 850k TM SMS other other other other other other other bible LM SMS other other other other other other other bible Reo yes – – – – – – – – TC yes – yes – yes – yes yes yes Table 1: Corpora used for training translation models (TM), language models (LM), lexicalized reordering model (Reo), and true-casing model (TC). All corpora are bilingual English–Haitian Creole. All translation results are reported for the devtest corpus, on truecased data. We report results on three metrics, Bleu (Papineni et al., 2002), NIST (Doddington, 2002), and Meteor optimized on fluency/adequacy (Lavie and Agarwal, 2007). 3 Corpus Usage The corpora available for the task was a small bilingual in-domain corpus of SMT data, a limited amount of bilingual out-of-domain corpora, such as dictionaries and the Bible. This is different to the common situation of domain adaptation, as in the standard WMT shared tasks, where there is a small bilingual in-domain corpus, a larger in-domain monolingual corpus, and possibly several out-ofdomain corpora that can be both monolingual and bilingual. In such a scenario it is often useful"
W11-2159,E06-1006,0,0.027139,"elling means that many unknown words (out-of-vocabulary words, OOVs) have a known spelling variant in the training corpus. We thus decided to treat OOVs using a method inspired by spell-checking techniques, and applied an approximate string matching technique to OOVs in the translation input in order to change them into known spelling variants. OOV replacement has been proposed by several researchers, replacing OOVs e.g. by morphological variants (Arora et al., 2008) or synonyms (Mirkin et al., 2009). Habash (2008) used several techniques for expanding OOVs in order to extend the phrasetable. Yang and Kirchhoff (2006) trained a morphologically based back-off model for OOVs. Bertoldi et al. (2010) created confusion networks as input of translation input with artificially created misspelled words, not specifically targetting OOVs, however. The work most similar to ours is DeNeefe et al. (2008), who also created lattices with spelling alternatives for OOVs, which did not improve translation results, however. Contrary to us, they only considered one edit per word, and did not weigh edits or lattice arcs. Many standard spell checkers are based on the noisy channel model, which use an error (channel) model and a"
W11-2159,W09-0429,0,\N,Missing
W11-2159,P07-2045,0,\N,Missing
W11-4648,P05-1066,0,0.223246,"Missing"
W11-4648,W09-0421,1,0.850354,"anslation reordering is usually performed by applying rules, that can either be handwritten rules targeting known syntactic differences (Collins et al., 2005), or be learnt automatically (Xia and McCord, 2004). In these studies the reordering decision was taken deterministically on the source side. This decision can also be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Rottmann and Vogel, 2007). There have also been attempts to integrate reordering rules into a PBSMT decoder (Elming, 2008). A different way to use reordering, was investigated in Holmqvist et al. (2009), who used the reordering only to improve the word alignment, and moved the words back into original order after the alignment phase. For most of the automatically learnt rules, some rule-extraction method is used, that only takes the word alignments into account. This work is inspired by the approach of Holmqvist et al. (2009), but further develops it both by iterating word alignment and reordering, and by creating rules that can be used on monolingual test data. The machine learning used in this study is similar to that of Elming (2008), who also uses Ripper. A different feature set is used,"
W11-4648,2005.mtsummit-papers.11,0,0.0268147,"d using the Moses toolkit (Koehn et al., 2007), with a 5-gram language model. The SMT system used a distance-based reordering penalty (distortion penalty), which adds a factor δ n for movements over n words, where phrase movement is also limited to a distance of six words. In addition we applied a monotone model, which prohibits any phrase reorderings, and thus is unlikely to work well for the baseline system, but could work well for the systems where the source language has been reordered to mimic the target language. The translation systems were trained and tested using the Europarl corpus (Koehn, 2005). The training part contained 439513 sentences, where sentences longer than 40 words was filtered out. The test set has 2000 sentences and the development set had 500 sentences. We performed two iterations of the iterative reordering rules learning and word alignment algorithm. After each iteration we trained a PBSMT system, which will be called Reo1 and Reo2, and which will be compared to baseline without any reorderings. Reo1 is similar to many previous approaches to reordering, since it is based on only one iteration of alignment and reordering. 5.1 Reordering Results At each iteration, eac"
W11-4648,W07-0734,0,0.0227005,"examples of rules are those that handle adverb placement (c), but there are also some rules that are hard to explain linguistically, such as (e), which moves a noun to the end of a sentence. All linguitstic levels are used in the rules, and are often mixed. Out of the totally 91 rules, 28 are lexicalized (b,c). It is encouraging that new types of rules are learnt in iteration 2, but at the same time many of the useful rule types from iteration 1 unfortunately are missing. 5.2 Translation Results Translation results are reported on the standard MT metrics Bleu (Papineni et al., 2002), Meteor (Lavie and Agarwal, 2007), and PER, position independent word error rate. PER does not take word order into account, which the other two metrics do. The results with distortion penalty are presented in Table 5, and for monotone decoding in Table 6. As expected the results are overall higher with the distance-based reordering model in the decoder. On the systems with a distortion penalty there 2 317 letters refer to rule ID in Table 3 Sara Stymne System Base Reo1 Reo2 Bleu 20.15 19.76 20.13 Meteor 26.87 26.49 26.99 1-PER 0.712 0.731 0.736 Table 5: Results with distortion penalty System Base Reo1 Reo2 Bleu 19.32 19.40 1"
W11-4648,J03-1002,0,0.00738382,"Skadin¸a (Eds.) NODALIDA 2011 Conference Proceedings, pp. 315–318 Sara Stymne Type Word POS Dep Func Syntax LC able A comp V PCOMPL-S NH LS to refuse INFMARK> V pm V mod A INFMARK> -FMAINV VG RS new tasks AN attr N obj V A> OBJ NP RC if CS pm V CS CS Table 1: Example of a positive training example Any automatic method can be used for word alignment. Learning reordering rules should be based on the word alignments as a knowledge source, for the iterations to be useful. In this particular implementation of the main strategy we use the standard IBM models up to model 4, as implemented in GIZA++ (Och and Ney, 2003) for word alignment, and a rule induction learner for the reordering rule learning. 4 Reordering We used rule-induction learning, as implemented in Ripper (Cohen, 1995). A rule-induction learner produces rules for the positive class(es), where each rule only contains a subset of all features. Some example rules can be seen in Table 3. The rules are human readable, allowing us to analyse them in a useful manner, and to apply them to unseen source text in a simple way. The reordering rules were learnt for the English source side of the corpus, which was parsed using a commercial functional depen"
W11-4648,P02-1040,0,0.0857015,"end of the sentence (a,b). Other examples of rules are those that handle adverb placement (c), but there are also some rules that are hard to explain linguistically, such as (e), which moves a noun to the end of a sentence. All linguitstic levels are used in the rules, and are often mixed. Out of the totally 91 rules, 28 are lexicalized (b,c). It is encouraging that new types of rules are learnt in iteration 2, but at the same time many of the useful rule types from iteration 1 unfortunately are missing. 5.2 Translation Results Translation results are reported on the standard MT metrics Bleu (Papineni et al., 2002), Meteor (Lavie and Agarwal, 2007), and PER, position independent word error rate. PER does not take word order into account, which the other two metrics do. The results with distortion penalty are presented in Table 5, and for monotone decoding in Table 6. As expected the results are overall higher with the distance-based reordering model in the decoder. On the systems with a distortion penalty there 2 317 letters refer to rule ID in Table 3 Sara Stymne System Base Reo1 Reo2 Bleu 20.15 19.76 20.13 Meteor 26.87 26.49 26.99 1-PER 0.712 0.731 0.736 Table 5: Results with distortion penalty System"
W11-4648,2007.tmi-papers.21,0,0.0424112,"imilar. This leads us to the hypothesis that we should be able to improve both reordering rules and word alignments by performing the two tasks iteratively. 2 Previous work Pre-translation reordering is usually performed by applying rules, that can either be handwritten rules targeting known syntactic differences (Collins et al., 2005), or be learnt automatically (Xia and McCord, 2004). In these studies the reordering decision was taken deterministically on the source side. This decision can also be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Rottmann and Vogel, 2007). There have also been attempts to integrate reordering rules into a PBSMT decoder (Elming, 2008). A different way to use reordering, was investigated in Holmqvist et al. (2009), who used the reordering only to improve the word alignment, and moved the words back into original order after the alignment phase. For most of the automatically learnt rules, some rule-extraction method is used, that only takes the word alignments into account. This work is inspired by the approach of Holmqvist et al. (2009), but further develops it both by iterating word alignment and reordering, and by creating rul"
W11-4648,C04-1073,0,0.144778,"tegies proposed uses word alignments between texts as their main knowledge source for learning reorderings. Word alignments are also created automatically with methods that perform better when the word order in the two languages is similar. This leads us to the hypothesis that we should be able to improve both reordering rules and word alignments by performing the two tasks iteratively. 2 Previous work Pre-translation reordering is usually performed by applying rules, that can either be handwritten rules targeting known syntactic differences (Collins et al., 2005), or be learnt automatically (Xia and McCord, 2004). In these studies the reordering decision was taken deterministically on the source side. This decision can also be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Rottmann and Vogel, 2007). There have also been attempts to integrate reordering rules into a PBSMT decoder (Elming, 2008). A different way to use reordering, was investigated in Holmqvist et al. (2009), who used the reordering only to improve the word alignment, and moved the words back into original order after the alignment phase. For most of the automatically learnt rules, some rul"
W11-4648,W08-0406,0,\N,Missing
W11-4648,C08-1027,0,\N,Missing
W11-4648,W09-0435,0,\N,Missing
W11-4648,W05-0909,0,\N,Missing
W11-4648,P07-2045,0,\N,Missing
W11-4648,2007.iwslt-1.3,0,\N,Missing
W11-4648,P03-1021,0,\N,Missing
W12-0704,P06-3002,0,0.0294394,"ave been several suggestions of clustering methods for obtaining word classes that are completely unsupervised, and induce classes from raw 28 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28–34, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics text. Brown et al. (1992) described a hierarchical word clustering method which maximizes the mutual information of bigrams. Sch¨utze (1995) described a distributional clustering algorithm that uses global context vectors as a basis for clustering. Biemann (2006) described a graph-based clustering methods for word classes. Goldwater and Griffiths (2007) used Bayesian reasoning for word class induction. Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. He also described a monolingual word clustering method, which is based on a maximum likelihood approach, using the frequencies of unigrams and bigrams in the training corpus. The above methods are fully unsupervised, and produce unlabelled classes. There has also been work"
W12-0704,J92-4003,0,0.583704,"ages, the presented approach can also be used as an extrinsic evaluation method for unsupervised POS-tagging methods. This is especially useful for the task of word class clustering which is hard to evaluate. 2 Unsupervised POS-tagging There have been several suggestions of clustering methods for obtaining word classes that are completely unsupervised, and induce classes from raw 28 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28–34, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics text. Brown et al. (1992) described a hierarchical word clustering method which maximizes the mutual information of bigrams. Sch¨utze (1995) described a distributional clustering algorithm that uses global context vectors as a basis for clustering. Biemann (2006) described a graph-based clustering methods for word classes. Goldwater and Griffiths (2007) used Bayesian reasoning for word class induction. Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. He also described a monolingual wor"
W12-0704,J93-2003,0,0.0402222,"velopment set has 500 sentences. For the Haitian Creole–English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al., 2011). The corpus contains 17192 sentences and 352326 words. The test and development data both contain 900 sentences each. Since we know of no POS-tagger for Haitian Creole, we only compare the clustered result to a baseline system. Reordering rules were extracted from the same corpora that were used for training the SMT system. The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al., 1977). 31 5 Results Table 2 shows the number of rules, and the average number of paths for each sentence in the test data lattice, using each tagset. For the standard tagsets the number of rules is relatively constant, despite the fact that the number of tags in the tagsets are quite different. For the clustered word classes, there are slightly fewer rules with 50 classes than for the standard tags, and the number of rules decreases with a higher number of classes. For the average number"
W12-0704,W11-2103,0,0.0208034,"for modelling reordering. For the Haitian Creole–English experiments we also added a lexicalized reordering model (Koehn et al., 2005), both to the baseline and to the reordered systems. For the English–German experiments, the translation system was trained and tested using a part of the Europarl corpus (Koehn, 2005). The training part contained 439513 sentences and 9.4 million words. Sentences longer than 40 words were filtered out. The test set has 2000 sentences and the development set has 500 sentences. For the Haitian Creole–English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al., 2011). The corpus contains 17192 sentences and 352326 words. The test and development data both contain 900 sentences each. Since we know of no POS-tagger for Haitian Creole, we only compare the clustered result to a baseline system. Reordering rules were extracted from the same corpora that were used for training the SMT system. The word alignments needed for reordering were created using GIZA++ (Och and Ney, 2003), an implementation of the IBM models (Brown et al., 1993) of alignment, which is trained in a fully unsupervised manner based on the EM algorithm (Dempster et al., 1977). 31 5 Results T"
W12-0704,J07-2003,0,0.0896055,"red word classes for semi-supervised dependency parsing and showed that using word class features together with POS-based features led to improvements, but using word class features instead of POS-based features only degraded results somewhat. 3 Reordering for SMT There is a large amount of work on reordering for statistical machine translation. One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al., 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). Preordering is another common strategy for handling reordering. Here the source side of the corpus is transformed in a preprocessing step to become more similar to the target side. There have been many suggestions of preordering strategies. Transformation rules can be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. Th"
W12-0704,P05-1066,0,0.19316,"Missing"
W12-0704,W06-1609,0,0.0612935,"Missing"
W12-0704,W07-0720,0,0.047571,"Missing"
W12-0704,W11-2165,0,0.0142292,"ly unsupervised, and produce unlabelled classes. There has also been work on what Goldwater and Griffiths (2007) call POS disambiguation, where the learning of classes is constrained by a dictionary of the allowable tags for each word. Such work has for instance been based on hidden Markov models (Merialdo, 1994), log-linear models (Smith and Eisner, 2005), and Bayesian reasoning (Goldwater and Griffiths, 2007). Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-juss`a et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). To the best of our knowledge this is the first study of applying clustered word classes for creating pre-translation reordering rules. The most similar work we are aware of is Costa-juss`a and Fonollosa (2006) who used clustered word classes in a strategy they call statistical machine reordering, where the corpus is translated into a reordered language using standard SMT techniques in a pre-processing step. The addition of word classes led to improvements over just using surface form, but no comparison to using POS-tags were shown. Clustered word classes have also been used in a discriminate"
W12-0704,P07-1094,0,0.0199465,"s that are completely unsupervised, and induce classes from raw 28 Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28–34, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics text. Brown et al. (1992) described a hierarchical word clustering method which maximizes the mutual information of bigrams. Sch¨utze (1995) described a distributional clustering algorithm that uses global context vectors as a basis for clustering. Biemann (2006) described a graph-based clustering methods for word classes. Goldwater and Griffiths (2007) used Bayesian reasoning for word class induction. Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. He also described a monolingual word clustering method, which is based on a maximum likelihood approach, using the frequencies of unigrams and bigrams in the training corpus. The above methods are fully unsupervised, and produce unlabelled classes. There has also been work on what Goldwater and Griffiths (2007) call POS disambiguation, where the learning of class"
W12-0704,2007.mtsummit-papers.29,0,0.0342574,"(Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). Preordering is another common strategy for handling reordering. Here the source side of the corpus is transformed in a preprocessing step to become more similar to the target side. There have been many suggestions of preordering strategies. Transformation rules can be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. This decision can be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Zhang et al., 2007; Niehues and Kolss, 2009) or as an n-best list (Li et al., 2007). Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al., 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al., 2007) or parse trees"
W12-0704,P04-1061,0,0.0126226,"s Costa-juss`a and Fonollosa (2006) who used clustered word classes in a strategy they call statistical machine reordering, where the corpus is translated into a reordered language using standard SMT techniques in a pre-processing step. The addition of word classes led to improvements over just using surface form, but no comparison to using POS-tags were shown. Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. Klein and Manning (2004) used POS-tags as the basis of a fully unsupervised parsing method, both for dependency and constituency parsing. They showed that clustered word classes can be used instead of conventional POS-tags, with some result degradation, but that it is better than several baseline systems. Koo et al. (2008) used features based on clustered word classes for semi-supervised dependency parsing and showed that using word class features together with POS-based features led to improvements, but using word class features instead of POS-based features only degraded results somewhat. 3 Reordering for SMT There"
W12-0704,P07-2045,0,0.00491556,"bers of clusters, 50, 125, and 625. The clustering was performed on the same corpus as the SMT training. The translation system used is a standard phrase-based SMT system. The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al., 2005). From this many-to-many alignment, consistent phrases of up to length 7 were extracted. A 5-gram language model was used, produced by SRILM (Stolcke, 2002). For training and decoding we used the Moses toolkit (Koehn et al., 2007) and the feature weights were optimized using minimum error rate training (Och, 2003). 1 http://www.connexor.eu/technology/ machinese/machinesesyntax/ 2 http://www-i6.informatik. rwth-aachen.de/web/Software/mkcls.html Tagset pos dep func syntax class50 class125 class625 Classes 23 523 49 20 50 125 625 Rules 319147 328415 325091 315407 303292 271348 211606 Paths 2.1e09 2.8e09 1.5e10 4.5e11 6.2e09 1.3e07 31654 Table 2: Number of tags for each tagset in the English training corpus, number of rules extracted for each tagset, and average numbers of paths per sentence in the testset lattice using ea"
W12-0704,2005.mtsummit-papers.11,0,0.0317738,"each tagset in the English training corpus, number of rules extracted for each tagset, and average numbers of paths per sentence in the testset lattice using each tagset to create rules The baseline systems were trained using no additional preordering, only a distance-based reordering penalty for modelling reordering. For the Haitian Creole–English experiments we also added a lexicalized reordering model (Koehn et al., 2005), both to the baseline and to the reordered systems. For the English–German experiments, the translation system was trained and tested using a part of the Europarl corpus (Koehn, 2005). The training part contained 439513 sentences and 9.4 million words. Sentences longer than 40 words were filtered out. The test set has 2000 sentences and the development set has 500 sentences. For the Haitian Creole–English experiments we used the SMS corpus released for WMT11 (Callison-Burch et al., 2011). The corpus contains 17192 sentences and 352326 words. The test and development data both contain 900 sentences each. Since we know of no POS-tagger for Haitian Creole, we only compare the clustered result to a baseline system. Reordering rules were extracted from the same corpora that wer"
W12-0704,P08-1068,0,0.134715,"ce form, but no comparison to using POS-tags were shown. Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. Klein and Manning (2004) used POS-tags as the basis of a fully unsupervised parsing method, both for dependency and constituency parsing. They showed that clustered word classes can be used instead of conventional POS-tags, with some result degradation, but that it is better than several baseline systems. Koo et al. (2008) used features based on clustered word classes for semi-supervised dependency parsing and showed that using word class features together with POS-based features led to improvements, but using word class features instead of POS-based features only degraded results somewhat. 3 Reordering for SMT There is a large amount of work on reordering for statistical machine translation. One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al., 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modellin"
W12-0704,P07-1091,0,0.0481756,"preprocessing step to become more similar to the target side. There have been many suggestions of preordering strategies. Transformation rules can be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. This decision can be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Zhang et al., 2007; Niehues and Kolss, 2009) or as an n-best list (Li et al., 2007). Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al., 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). Common for all these levels is that a tool like a tagger or parser is needed for them to work. In all the above studies, the reordering rules are applied to the translation input, but they are only applied to the training data in a few cases, for instance in Popovi´"
W12-0704,J94-2001,0,0.222965,"ve the extraction of alignment templates through alignments between classes, not only between words. He also described a monolingual word clustering method, which is based on a maximum likelihood approach, using the frequencies of unigrams and bigrams in the training corpus. The above methods are fully unsupervised, and produce unlabelled classes. There has also been work on what Goldwater and Griffiths (2007) call POS disambiguation, where the learning of classes is constrained by a dictionary of the allowable tags for each word. Such work has for instance been based on hidden Markov models (Merialdo, 1994), log-linear models (Smith and Eisner, 2005), and Bayesian reasoning (Goldwater and Griffiths, 2007). Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-juss`a et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). To the best of our knowledge this is the first study of applying clustered word classes for creating pre-translation reordering rules. The most similar work we are aware of is Costa-juss`a and Fonollosa (2006) who used clustered word classes in a strategy they call statistical machine reo"
W12-0704,2009.mtsummit-posters.13,0,0.0201164,"n be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. This decision can be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Zhang et al., 2007; Niehues and Kolss, 2009) or as an n-best list (Li et al., 2007). Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al., 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). Common for all these levels is that a tool like a tagger or parser is needed for them to work. In all the above studies, the reordering rules are applied to the translation input, but they are only applied to the training data in a few cases, for instance in Popovi´c and Ney (2006). Rottmann and Vogel (2007) compared two strategies for reorder29 ing the training corpus, by using alignments, and by applying t"
W12-0704,W09-0435,0,0.445073,"source side of the corpus is transformed in a preprocessing step to become more similar to the target side. There have been many suggestions of preordering strategies. Transformation rules can be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. This decision can be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Zhang et al., 2007; Niehues and Kolss, 2009) or as an n-best list (Li et al., 2007). Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al., 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). Common for all these levels is that a tool like a tagger or parser is needed for them to work. In all the above studies, the reordering rules are applied to the translation input, but they are only applied to the training data"
W12-0704,W09-0413,0,0.0194594,"ly applied to the training data in a few cases, for instance in Popovi´c and Ney (2006). Rottmann and Vogel (2007) compared two strategies for reorder29 ing the training corpus, by using alignments, and by applying the reordering rules to create a lattice from which they extracted the 1-best reordering. They found that it was better to use the latter option, to reorder the training data based on the rules, than to use the original order in the training data. Using alignment-based reordering was not successful, however. Another option for using reorderings in the training data was presented by Niehues et al. (2009), who directly extracted phrase pairs from reordering lattices, and showed a small gain over non-reordered training data. 3.1 POS-based Preordering Our work is based on the POS-based reordering model described by Niehues and Kolss (2009), in which POS-based rules are extracted from a word aligned corpus, where the source side is part-of-speech tagged. There are two types of rules. Short-range rules (Rottmann and Vogel, 2007) contain a pattern of POS-tags, and a possible reordering to resemble the target language, such as VVIMP VMFIN PPER → PPER VMFIN VVIMP, which moves a personal pronoun to a"
W12-0704,J03-1002,0,0.0174877,"TOP mod N attr N pcomp PREP. The other tag types are directly exemplified in Table 1. The tagsets have different sizes, as shown in Table 2. For the unsupervised tags, we used clustered word classes obtained using the mkcls software,2 which implements the approach of Och (1999). We explored three different numbers of clusters, 50, 125, and 625. The clustering was performed on the same corpus as the SMT training. The translation system used is a standard phrase-based SMT system. The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al., 2005). From this many-to-many alignment, consistent phrases of up to length 7 were extracted. A 5-gram language model was used, produced by SRILM (Stolcke, 2002). For training and decoding we used the Moses toolkit (Koehn et al., 2007) and the feature weights were optimized using minimum error rate training (Och, 2003). 1 http://www.connexor.eu/technology/ machinese/machinesesyntax/ 2 http://www-i6.informatik. rwth-aachen.de/web/Software/mkcls.html Tagset pos dep func syntax class50 class125 class625 Classes 23 523 4"
W12-0704,E99-1010,0,0.198757,"the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 28–34, c Avignon, France, April 23 - 27 2012. 2012 Association for Computational Linguistics text. Brown et al. (1992) described a hierarchical word clustering method which maximizes the mutual information of bigrams. Sch¨utze (1995) described a distributional clustering algorithm that uses global context vectors as a basis for clustering. Biemann (2006) described a graph-based clustering methods for word classes. Goldwater and Griffiths (2007) used Bayesian reasoning for word class induction. Och (1999) described a method for determining bilingual word classes, used to improve the extraction of alignment templates through alignments between classes, not only between words. He also described a monolingual word clustering method, which is based on a maximum likelihood approach, using the frequencies of unigrams and bigrams in the training corpus. The above methods are fully unsupervised, and produce unlabelled classes. There has also been work on what Goldwater and Griffiths (2007) call POS disambiguation, where the learning of classes is constrained by a dictionary of the allowable tags for e"
W12-0704,P03-1021,0,0.0350806,"training. The translation system used is a standard phrase-based SMT system. The translation model was trained by first creating unidirectional word alignments in both directions using GIZA++ (Och and Ney, 2003), which are then symmetrized by the grow-diag-final-and method (Koehn et al., 2005). From this many-to-many alignment, consistent phrases of up to length 7 were extracted. A 5-gram language model was used, produced by SRILM (Stolcke, 2002). For training and decoding we used the Moses toolkit (Koehn et al., 2007) and the feature weights were optimized using minimum error rate training (Och, 2003). 1 http://www.connexor.eu/technology/ machinese/machinesesyntax/ 2 http://www-i6.informatik. rwth-aachen.de/web/Software/mkcls.html Tagset pos dep func syntax class50 class125 class625 Classes 23 523 49 20 50 125 625 Rules 319147 328415 325091 315407 303292 271348 211606 Paths 2.1e09 2.8e09 1.5e10 4.5e11 6.2e09 1.3e07 31654 Table 2: Number of tags for each tagset in the English training corpus, number of rules extracted for each tagset, and average numbers of paths per sentence in the testset lattice using each tagset to create rules The baseline systems were trained using no additional preor"
W12-0704,P02-1040,0,0.0849811,"ord classes, there are slightly fewer rules with 50 classes than for the standard tags, and the number of rules decreases with a higher number of classes. For the average number of lattice paths per sentence, there are some differences for the standard tags, but it is not related to tagset size. Again, the clustering with 50 classes has a similar number as the standard classes, but here there is a sharp decrease of lattice paths with a higher number of classes. The translation results for the English–German experiments are shown in Table 3. We report translation results for two metrics, Bleu (Papineni et al., 2002) and NIST (Doddington, 2002), and significance testing is performed using approximate randomization (Riezler and Maxwell, 2005), with 10,000 iterations. All the systems with reordering have higher scores than the baseline on both metrics. This difference is always significant for NIST, and significant for Bleu in all cases except for two systems, one with standard tags and one with clustered tags. Between most of the systems with reordering the differences are small and most of them are not significant. Overall the systems with standard word classes perform slightly better than the clustered s"
W12-0704,popovic-ney-2006-pos,0,0.050127,"Missing"
W12-0704,W05-0908,0,0.0189119,"ith a higher number of classes. For the average number of lattice paths per sentence, there are some differences for the standard tags, but it is not related to tagset size. Again, the clustering with 50 classes has a similar number as the standard classes, but here there is a sharp decrease of lattice paths with a higher number of classes. The translation results for the English–German experiments are shown in Table 3. We report translation results for two metrics, Bleu (Papineni et al., 2002) and NIST (Doddington, 2002), and significance testing is performed using approximate randomization (Riezler and Maxwell, 2005), with 10,000 iterations. All the systems with reordering have higher scores than the baseline on both metrics. This difference is always significant for NIST, and significant for Bleu in all cases except for two systems, one with standard tags and one with clustered tags. Between most of the systems with reordering the differences are small and most of them are not significant. Overall the systems with standard word classes perform slightly better than the clustered systems, especially the func tagset gives consistently high results, and is significantly better than four of the clustered syst"
W12-0704,2007.tmi-papers.21,0,0.526292,"ring rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al., 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). Common for all these levels is that a tool like a tagger or parser is needed for them to work. In all the above studies, the reordering rules are applied to the translation input, but they are only applied to the training data in a few cases, for instance in Popovi´c and Ney (2006). Rottmann and Vogel (2007) compared two strategies for reorder29 ing the training corpus, by using alignments, and by applying the reordering rules to create a lattice from which they extracted the 1-best reordering. They found that it was better to use the latter option, to reorder the training data based on the rules, than to use the original order in the training data. Using alignment-based reordering was not successful, however. Another option for using reorderings in the training data was presented by Niehues et al. (2009), who directly extracted phrase pairs from reordering lattices, and showed a small gain over"
W12-0704,E95-1020,0,0.371853,"Missing"
W12-0704,P05-1044,0,0.0174262,"ates through alignments between classes, not only between words. He also described a monolingual word clustering method, which is based on a maximum likelihood approach, using the frequencies of unigrams and bigrams in the training corpus. The above methods are fully unsupervised, and produce unlabelled classes. There has also been work on what Goldwater and Griffiths (2007) call POS disambiguation, where the learning of classes is constrained by a dictionary of the allowable tags for each word. Such work has for instance been based on hidden Markov models (Merialdo, 1994), log-linear models (Smith and Eisner, 2005), and Bayesian reasoning (Goldwater and Griffiths, 2007). Word clusters have previously been used for SMT for improving word alignment (Och, 1999), in a class-based language model (Costa-juss`a et al., 2007) or for extracting gappy patterns (Gimpel and Smith, 2011). To the best of our knowledge this is the first study of applying clustered word classes for creating pre-translation reordering rules. The most similar work we are aware of is Costa-juss`a and Fonollosa (2006) who used clustered word classes in a strategy they call statistical machine reordering, where the corpus is translated into"
W12-0704,C04-1073,0,0.0700818,"005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). Preordering is another common strategy for handling reordering. Here the source side of the corpus is transformed in a preprocessing step to become more similar to the target side. There have been many suggestions of preordering strategies. Transformation rules can be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. This decision can be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Zhang et al., 2007; Niehues and Kolss, 2009) or as an n-best list (Li et al., 2007). Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al., 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al., 2007)"
W12-0704,P02-1039,0,0.132814,"arsing and showed that using word class features together with POS-based features led to improvements, but using word class features instead of POS-based features only degraded results somewhat. 3 Reordering for SMT There is a large amount of work on reordering for statistical machine translation. One way to approach reordering is by extending the translation model, either by adding extra models, such as lexicalized (Koehn et al., 2005) or discriminative (Zens and Ney, 2006) reordering models or by directly modelling reordering in hierarchical (Chiang, 2007) or syntactical translation models (Yamada and Knight, 2002). Preordering is another common strategy for handling reordering. Here the source side of the corpus is transformed in a preprocessing step to become more similar to the target side. There have been many suggestions of preordering strategies. Transformation rules can be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. This decision can be delayed to decoding time by presenting se"
W12-0704,W06-3108,0,0.019986,"r knowledge this is the first study of applying clustered word classes for creating pre-translation reordering rules. The most similar work we are aware of is Costa-juss`a and Fonollosa (2006) who used clustered word classes in a strategy they call statistical machine reordering, where the corpus is translated into a reordered language using standard SMT techniques in a pre-processing step. The addition of word classes led to improvements over just using surface form, but no comparison to using POS-tags were shown. Clustered word classes have also been used in a discriminate reordering model (Zens and Ney, 2006), and were shown to reduce the classification error rate. Word clusters have also been used for unsupervised and semi-supervised parsing. Klein and Manning (2004) used POS-tags as the basis of a fully unsupervised parsing method, both for dependency and constituency parsing. They showed that clustered word classes can be used instead of conventional POS-tags, with some result degradation, but that it is better than several baseline systems. Koo et al. (2008) used features based on clustered word classes for semi-supervised dependency parsing and showed that using word class features together w"
W12-0704,2007.iwslt-1.3,0,0.0682476,"reordering. Here the source side of the corpus is transformed in a preprocessing step to become more similar to the target side. There have been many suggestions of preordering strategies. Transformation rules can be handwritten rules targeting known syntactic differences (Collins et al., 2005; Popovi´c and Ney, 2006), or they can be learnt automatically (Xia and McCord, 2004; Habash, 2007). In these studies the reordering decision was taken deterministically on the source side. This decision can be delayed to decoding time by presenting several reordering options to the decoder as a lattice (Zhang et al., 2007; Niehues and Kolss, 2009) or as an n-best list (Li et al., 2007). Generally reordering rules are applied to the source language, but there have been attempts at target side reordering as well (Na et al., 2009). Reordering rules can be based on different levels of linguistic annotation, such as POS-tags (Niehues and Kolss, 2009), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). Common for all these levels is that a tool like a tagger or parser is needed for them to work. In all the above studies, the reordering rules are applied to the translation input, but they are only app"
W12-0704,2005.iwslt-1.8,0,\N,Missing
W13-2229,J03-1002,0,0.0232763,"Missing"
W13-2229,D12-1108,1,0.797773,"n of the search space. While this decoding approach delivers excellent search performance at a very reasonable speed, it limits the information available to the feature models to an n-gram window similar to a language model history. In stack decoding, it is difficult to implement models with sentence-internal long-range dependencies and cross-sentence dependencies, where the model score of a given sentence depends on the translations generated for another sentence. In contrast to this very popular stack decoding approach, our decoder Docent implements a search procedure based on local search (Hardmeier et al., 2012). At any stage of the search process, its search state consists of a complete document translation, making it easy for feature models to access the complete document with its current translation at any point in time. The search algorithm is a stochastic variant of standard hill climbing. At each step, it generates a successor of the current search state by randomly applying We describe the Uppsala University system for WMT13, for English-to-German translation. We use the Docent decoder, a local search decoder that translates at the document level. We add tunable distortion limits, that is, sof"
W13-2229,P03-1021,0,0.00789271,"he translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimization was performed using MERT (Och, 2003) at the document-level (Stymne et al., 2013a). In this setup we calculate both model and metric scores on the document-level instead of on the sentence-level. We produce kbest lists by sampling from the decoder. In each optimization run we run 40,000 hill-climbing iterations of the decoder, and sample translations with interval 100, from iteration 10,000. This procedure has been shown to give competitive results to standard tuning with Moses (Koehn et al., 2007) with relatively stable results (Stymne et al., 2013a). For tuning data we concatenated the tuning sets news-test 2008–2010 and newssy"
W13-2229,P02-1040,0,0.0863828,"y the WMT13 workshop. We always concatenated the two bilingual corpora Europarl and News Commentary, which we will call EP-NC. We pre-processed all corpora by using the tools provided for tokenization and we also lower-cased all corpora. For the bilingual corpora we also filtered sentence pairs with a length ratio larger than three, or where either sentence was longer than 60 tokens. Recasing was performed as a post-processing step, trained using the resources To evaluate our system we use newstest2012, which has 99 documents and 3003 sentences. In this article we give lower-case Bleu scores (Papineni et al., 2002), except in Section 6 where we investigate the effect of different recasing models. 226 Cleaning None Basic Langid Alignment-based Sentences 2,399,123 2,271,912 2,072,294 1,512,401 Reduction sentences into four categories, cases where both languages were correctly identified, but under the confidence threshold of 0.999, cases where both languages were incorrectly identified, and cases where one language was incorrectly identified. Overall the language identification was accurate on 54 of the 93 removed sentences. In 18 of the cases where it was wrong, the sentences were not translation corresp"
W13-2229,P13-4033,1,0.906674,"at the document level. We add tunable distortion limits, that is, soft constraints on the maximum distortion allowed, to Docent. We also investigate cleaning of the noisy Common Crawl corpus. We show that we can use alignment-based filtering for cleaning with good results. Finally we investigate effects of corpus selection for recasing. 1 The Docent Decoder Introduction In this paper we present the Uppsala University submission to WMT 2013. We have submitted one system, for translation from English to German. In our submission we use the document-level decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013). In the current setup, we take advantage of Docent in that we introduce tunable distortion limits, that is, modeling distortion limits as soft constraints instead of as hard constraints. In addition we perform experiments on corpus cleaning. We investigate how the noisy Common Crawl corpus can be cleaned, and suggest an alignmentbased cleaning method, which works well. We also investigate corpus selection for recasing. In Section 2 we introduce our decoder, Docent, followed by a general system description in Section 3. In Section 4 we describe our experiments with corpus cleaning, and in Sect"
W13-2229,E12-1055,0,0.0123678,"ined two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were trained using the SRILM toolkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimizati"
W13-2229,W11-2123,0,0.0206337,"-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were trained using the SRILM toolkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Doce"
W13-2229,D07-1103,0,0.0298921,"lkit (Stolcke, 2002) and during decoding we used the KenLM toolkit (Heafield, 2011). For the translation model we also trained two models, one with EP-NC, and one with Common Crawl. These two models were interpolated and used as a single model at decoding time, based on perplexity minimization interpolation (Sennrich, 2012), see details in Section 4. The translation models were trained using the Moses toolkit (Koehn et al., 2007), with standard settings with 5 features, phrase probabilities and lexical weighting in both directions and a phrase penalty. We applied significance-based filtering (Johnson et al., 2007) to the resulting phrase tables. For decoding we used the Docent decoder with random initialization and standard parameter settings (Hardmeier et al., 2012; Hardmeier et al., 2013), which beside translation and language model features include a word penalty and a distortion penalty. Parameter optimization was performed using MERT (Och, 2003) at the document-level (Stymne et al., 2013a). In this setup we calculate both model and metric scores on the document-level instead of on the sentence-level. We produce kbest lists by sampling from the decoder. In each optimization run we run 40,000 hill-c"
W13-2229,N03-1017,0,0.0408954,"Missing"
W13-2229,W13-3308,1,0.864943,"of the complete document. On the downside, there is an increased risk of search errors because the document-level hill-climbing decoder cannot make as strong assumptions about the problem structure as the stack decoder does. In practice, this drawback can be mitigated by initializing the hill-climber with the output of a stack decoding pass using the baseline set of models without document-level features (Hardmeier et al., 2012). Since its inception, Docent has been used to experiment with document-level semantic language models (Hardmeier et al., 2012) and models to enhance text readability (Stymne et al., 2013b). Work on other discourse phenomena is ongoing. In the present paper, we focus on sentence-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were tra"
W13-2229,2005.iwslt-1.8,0,0.114614,"Missing"
W13-2229,W13-5634,1,0.871264,"of the complete document. On the downside, there is an increased risk of search errors because the document-level hill-climbing decoder cannot make as strong assumptions about the problem structure as the stack decoder does. In practice, this drawback can be mitigated by initializing the hill-climber with the output of a stack decoding pass using the baseline set of models without document-level features (Hardmeier et al., 2012). Since its inception, Docent has been used to experiment with document-level semantic language models (Hardmeier et al., 2012) and models to enhance text readability (Stymne et al., 2013b). Work on other discourse phenomena is ongoing. In the present paper, we focus on sentence-internal reordering by exploiting the fact that Docent implements distortion limits as soft constraints rather than strictly enforced limitations. We do not include any of our document-level feature functions. 3 For the language model we trained two separate models, one on the German side of EP-NC, and one on the monolingual News corpus. In both cases we trained 5-gram models. For the large News corpus we used entropy-based pruning, with 10−8 as a threshold (Stolcke, 1998). The language models were tra"
W13-2229,W08-0318,0,0.0188269,"s EP-NC-News EP-NC-CC-News EP-NC 13.8 13.9 13.9 13.9 13.9 EP-NC-CC 14.4 14.5 14.5 14.5 14.5 Language model News EP-NC-News 14.8 14.8 14.9 14.8 14.9 14.9 14.9 14.9 14.9 14.9 EP-NC-CC-News 14.8 14.8 14.9 14.9 15.0 Table 7: Case-sensitive Bleu scores with different corpus combinations for the language model and translation model (TM) for recasing Test system Docent (random) Docent (stack) Moses Docent (random) Docent (stack) Moses ing. It is common to train the system on truecased data instead of lower-cased data, which has been shown to lead to small gains for the English– German language pair (Koehn et al., 2008). In this framework there is still a need to find the correct case for the first word of each sentence, for which a similar corpus study might be useful. 7 Tuning system Docent Docent Docent Moses Moses Moses Bleu 15.7 15.9 15.9 15.9 16.8 16.8 Table 8: Bleu scores for Docent initialized randomly or with stack decoding compared to Moses. Tuning is performed with either Moses or Docent. For the top line we used tunable distortion limits 6,10 with Docent, in the other cases a standard hard distortion limit of 6, since Moses does not allow soft distortion limits. Comparison to Moses So far we have"
W13-2229,P12-3005,0,0.118617,"Missing"
W13-2229,P07-2045,0,\N,Missing
W13-3308,D11-1084,0,0.0941275,"in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing"
W13-3308,W12-3102,0,0.0373536,"Missing"
W13-3308,D07-1007,0,0.0195191,"onnectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-be"
W13-3308,D12-1026,0,0.234011,"ion (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2"
W13-3308,W09-2404,0,0.411772,"ng has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Tr"
W13-3308,E12-3001,0,0.09251,"ic programming for exploring a large search space (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translated sentences (Hardmeier and Federico, 2010), unfortunately without any convincing results. To address the translation of discourse connectives, source-side pre-processing has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good tra"
W13-3308,P07-1005,0,0.016844,"ed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-best list that have a high score on some MT metric. The translation step"
W13-3308,2010.iwslt-papers.10,1,0.900759,"he dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translated sentences (Hardmeier and Federico, 2010), unfortunately without any convincing results. To address the translation of discourse connectives, source-side pre-processing has been used to annotate surface forms either in the corpus or in the Discourse has largely been ignored in traditional machine translation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues f"
W13-3308,N12-1047,0,0.127102,"best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works well for small sets of features. In this paper we ar"
W13-3308,D12-1108,1,0.932337,"slation (MT). Typically each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimiz"
W13-3308,D08-1024,0,0.0690584,"Missing"
W13-3308,P13-4033,1,0.695247,"each sentence has been translated in isolation, essentially yielding translations that are bags of sentences. It is well known from translation studies, however, that discourse is important in order to achieve good translations of documents (Hatim and Mason, 1990). Most attempts to address discourse-level issues for statistical machine translation (SMT) have had to resort to solutions such as postprocessing to address lexical cohesion (Carpuat, 2009) or two-step translation to address pronoun anaphora (Le Nagard and Koehn, 2010). Recently, however, we presented Docent (Hardmeier et al., 2012; Hardmeier et al., 2013), a decoder based on local search that translates full documents. So far this decoder has not included a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to i"
W13-3308,P11-2031,0,0.0276307,"ment-level feature weight optimization for SMT. We first describe the experimental setup, followed by baseline results using sentencelevel optimization. We then present validation experiments with standard sentence-level features, 1 http://www.statmt.org/wmt13/ translation-task.html 63 System Moses Docent-M Docent-R Moses Docent-M Docent-R million iterations. We show results on the Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) metrics. For German–English we show the average result and standard deviation of three optimization runs, to control for optimizer instability as proposed by Clark et al. (2011). For English–Swedish we report results on single optimization runs, due to time constraints. 5.2 Bleu 17.7 17.7 15.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to show the effectiveness of the document-level tuning procedure described above. In order to do this, we created a baseline using sentence-level optimization with a tuning set of 2525 sentences and the News2009"
W13-3308,D11-1125,0,0.344821,"til some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works well for small sets of features. In this paper we are not concerned with the actu"
W13-3308,W12-3139,0,0.205173,"5.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to show the effectiveness of the document-level tuning procedure described above. In order to do this, we created a baseline using sentence-level optimization with a tuning set of 2525 sentences and the News2009 corpus for evaluation. Increasing the tuning set is known to give only modest improvements (Turchi et al., 2012; Koehn and Haddow, 2012). The feature weights optimized with the standard Moses decoder can then directly be used in our document-level decoder as we only include sentence-level features in our baseline model. As expected, these optimized weights also lead to a better performance in document-level decoding compared to an untuned model as shown in Table 2. Note, that Docent can be initialized in two ways, by Moses and randomly. Not surprisingly, the result for the runs initialized with Moses are identical with the pure sentence-level decoder. Initializing randomly gives a slightly lower Bleu score but with a larger va"
W13-3308,N12-1023,0,0.0116367,"repeated using the new weights for decoding, and optimization is continued on a new k-best list, or on a combination of all k-best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are ba"
W13-3308,N03-1017,0,0.0371705,"rity of scoring and the process of extracting k-best lists. For document-level features we do not have meaningful scores on the sentence level which are required in standard optimization frameworks. Furthermore, the extraction of k-best lists is not as Here we instead choose to work with the recent document-level SMT decoder Docent (Hardmeier et al., 2012). Unlike in traditional decoding were documents are generated sentence by sentence, feature models in Docent always have access to the complete discourse context, even before decoding is finished. It implements the phrase-based SMT approach (Koehn et al., 2003) and is based on local search, where a state consists of a full translation of a document, which is improved by applying a series of operations to improve the translation. A hill-climbing strategy is used to find a (local) maximum. The operations allow changing the translation of a phrase, changing the word order by swapping the positions of two phrases, and resegmenting phrases. The initial state can either be initialized randomly in monotonic order, or be based on an initial run from a standard sentence-based decoder. The number of iterations in the decoder is controlled by two parameters, t"
W13-3308,P12-1048,0,0.0480289,"t al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase tables (Zhao and Xing, 2010; Su et al., 2012). For a more thorough overview of discourse in SMT, see Hardmeier (2012). 3 Sentence-Level Tuning Traditionally, feature weight optimization, or tuning, for SMT is performed by an iterative process where a development set is translated to produce a k-best list. The parameters are then optimized using some procedure, generally to favor translations in the k-best list that have a high score on some MT metric. The translation step is then repeated using the new weights for decoding, and optimization is continued on a new k-best list, or on a combination of all k-best lists. This is repeated until"
W13-3308,W10-2602,1,0.888675,"shop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to integrate topic modeling into phrase t"
W13-3308,2005.mtsummit-papers.11,0,0.014426,"is already somewhat unstable this is a potential issue that needs to be explored further, which we do in Section 5. Implementation-wise we adapted Docent to output k-lists and adapted the infrastructure available for tuning in the Moses decoder (Koehn et al., 2007) to work with document-level scores. This setup allows us to use the variety of optimization procedures implemented there. 5 5.1 Experimental Setup Most of our experiments are for German-toEnglish news translation using data from the WMT13 workshop.1 We also show results with document-level features for English-to-Swedish Europarl (Koehn, 2005). The size of the training, tuning, and test sets are shown in Table 1. First of all, we need to extract documents for tuning and testing with Docent. Fortunately, the news data already contain document markup, corresponding to individual news articles. For Europarl we define a document as a consecutive sequence of utterances from a single speaker. To investigate the effect of the size of the tuning set, we used different subsets of the available tuning data. All our document-level experiments are carried out with Docent but we also contrast with the Moses decoder (Koehn et al., 2007). For the"
W13-3308,W10-1737,0,0.202315,"Missing"
W13-3308,N12-1046,0,0.199813,"el features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type of approach is to int"
W13-3308,W12-0117,0,0.0131317,"ed a feature weight optimization framework. However, feature weight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has"
W13-3308,2011.mtsummit-papers.13,0,0.118068,"ts for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities for each sentence (Carpuat and Wu, 2007), by introducing extra features in the phrasetable (Chan et al., 2007), or as a k-best re-ranking task (Specia et al., 2008). Another type o"
W13-3308,2012.amta-papers.20,0,0.335871,"eight optimization, or tuning, is important for any modern SMT decoder to achieve a good translation performance. In previous research with Docent, we used grid search to find weights for document-level features 60 Proceedings of the Workshop on Discourse in Machine Translation (DiscoMT), pages 60–69, c Sofia, Bulgaria, August 9, 2013. 2013 Association for Computational Linguistics level feature weight optimization is not limited to it. It can be used with any decoder that outputs feature values at the document level. phrase-table (Meyer and Popescu-Belis, 2012) or by using factored decoding (Meyer et al., 2012) to disambiguate connectives, with small improvements. Lexical consistency has been addressed by the use of post-processing (Carpuat, 2009), multi-pass decoding (Xiao et al., 2011; Ture et al., 2012), and cache models (Tiedemann, 2010; Gong et al., 2011). Gong et al. (2012) addressed the issue of tense selection for translation from Chinese, by the use of inter-sentential tense n-grams, exploiting information from previously translated sentences. Another way to use a larger context is by integrating word sense disambiguation and SMT. This has been done by re-initializing phrase probabilities f"
W13-3308,W01-1408,0,0.0344757,"res to model discourse phenomena such as anaphora, discourse connectives, and lexical consistency. In this paper, we therefore propose an approach that supports discourse-wide features in documentlevel decoding by adapting existing frameworks for sentence-level optimization. Furthermore, we include a thorough empirical investigation of this approach. Introduction 2 Discourse-Level SMT Traditional SMT systems translate texts sentence by sentence, assuming independence between sentences. This assumption allows efficient algorithms based on dynamic programming for exploring a large search space (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level features into a traditional SMT decoder. Nevertheless, there have been several attempts to integrate intersentential and long distance models for discourselevel phenomena into standard decoders, usually as ad-hoc additions to standard models, addressing a single phenomenon. Several studies have tried to improve pronoun anaphora by adding information about the antecedent, either by using two-step decoding (Le Nagard and Koehn, 2010; Guillou, 2012) or by extracting information from previously translat"
W13-3308,P03-1021,0,0.0673075,"est list, or on a combination of all k-best lists. This is repeated until some end condition is satisfied, for instance for a set number of iterations, until there is only very small changes in parameter weights, or until there are no new translations in the k-best lists. SMT tuning is a hard problem in general, partly because the correct output is unreachable and also because the translation process includes latent variables, which means that many efficient standard optimization procedures cannot be used (Gimpel and Smith, 2012). Nevertheless, there are a number of techniques including MERT (Och, 2003), MIRA (Chiang et al., 2008; Cherry and Foster, 2012), PRO (Hopkins and May, 2011), and Rampion (Gimpel and Smith, 2012). All of these optimization methods can be plugged into the standard optimization loop. All of the methods work relatively well in practice, even though there are limitations, for instance that many methods are non-deterministic meaning that their results are somewhat unstable. However, there are some important differences. MERT is based on scores for the full test set, whereas the other methods are based on sentence-level scores. MERT also has the drawback that it only works"
W13-3308,P02-1040,0,0.0886974,"riments, the decoder always stopped when reaching the rejection limit, usually between 1–5 Experiments In this section we report experimental results where we investigate several issues in connection with document-level feature weight optimization for SMT. We first describe the experimental setup, followed by baseline results using sentencelevel optimization. We then present validation experiments with standard sentence-level features, 1 http://www.statmt.org/wmt13/ translation-task.html 63 System Moses Docent-M Docent-R Moses Docent-M Docent-R million iterations. We show results on the Bleu (Papineni et al., 2002) and NIST (Doddington, 2002) metrics. For German–English we show the average result and standard deviation of three optimization runs, to control for optimizer instability as proposed by Clark et al. (2011). For English–Swedish we report results on single optimization runs, due to time constraints. 5.2 Bleu 17.7 17.7 15.2 (0.05) 18.3 (0.04) 18.3 (0.04) 18.1 (0.13) NIST 6.25 6.25 5.88 (0.00) 6.22 (0.01) 6.22 (0.01) 6.23 (0.01) Table 2: Baseline results, where Docent-M is initialized with Moses and Docent-R randomly Docs 111 345 100 200 100 200 300 Baselines Most importantly, we would like to sh"
W13-3308,W13-5634,1,0.917086,"an restarting the decoder from the previous best state. 62 Training Tuning Test German–English Type Sentences Europarl 1.9M News Commentary 178K News2009 2525 News2008-2010 7567 News2012 3003 Documents – – 111 345 99 English–Swedish Type Sentences Europarl 1.5M – – Europarl (Moses) 2000 Europarl (Docent) 1338 Europarl 690 Documents – – – 100 20 Table 1: Domain and number of sentences and documents for the corpora which can be compared to standard optimization. Finally, we report results with a set of documentlevel features that have been proposed for joint translation and text simplification (Stymne et al., 2013). As seen in Figure 1, there are some additional parameters in our procedure: the sample start iteration and the sample interval. We also need to set the number of decoder iterations to run. In Section 5 we empirically investigate the effect of these parameters. Compared to sentence-level optimization, we also have a smaller number of units to get scores from, since we use documents as units, and not sentences. The importance of this depends on the optimization algorithm. MERT calculates metric scores over the full tuning set, not for individual sentences, and should not be affected too much b"
W13-3308,P07-2045,0,\N,Missing
W13-5634,2012.eamt-1.33,0,0.0610232,"Missing"
W13-5634,W09-2404,0,0.0675313,"summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NO"
W13-5634,W12-3156,0,0.0177139,"easured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electr"
W13-5634,D07-1007,0,0.0351635,"relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document leve"
W13-5634,P07-1005,0,0.0350039,"bility and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document level, which would be an"
W13-5634,daelemans-etal-2004-automatic,0,0.0211848,"a har sagt det - EU:s möte i Lissabon lagt särskild vikt vid vår för att genomföra risk i så att den plan att bli klar under 2003. SL (high) Som ledamöterna vet vissa talare har nämnt - Europeiska rådet i Lissabon särskilt uppmärksammat främja våra ansträngningar att genomföra riskkapital så att handlingsplanen avslutas 2003. Table 5: Examples of translation output from a sample of systems sentence compression (e.g., Knight and Marcu, 2000; Specia, 2010). Furthermore, there is a wide range of publications using other methods for monolingual sentence compression and text simplification, (e.g., Daelemans et al., 2004; Cohn and Lapata, 2009). Readability has also been investigated as an effect of text summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexi"
W13-5634,D11-1108,0,0.0132082,"et al. (2012) investigate the task of translating subtitles where time and space constraints are important, which leads to the task of sentence compression, which is related to our work on simplifying translated texts. They introduce dynamic length penalties which they integrate in a standard SMT decoder. Their model successfully compresses subtitles on three data sets. However, they also show that a similar compression can be achieved with appropriate tuning data that meets the length constraints. There are also a number of studies that use SMT techniques for monolingual paraphrasing (e.g., Ganitkevitch et al., 2011) and Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 381 of 474] Source As the honourable Members know - some speakers have mentioned it - the European Council at Lisbon paid particular attention to promoting our efforts to implement risk capital in such a way that the action plan will be finished in 2003. Baseline Som de ärade ledamöterna vet - vissa talare har nämnt det - som Europeiska rådet i Lissabon ägnat särskild uppmärksamhet åt att främja våra ansträngningar att genomföra riskkapital på ett s"
W13-5634,D10-1016,0,0.0191157,"on metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 382 of 474] poetic form. They use features in the decoder such as rhyme and meter. They also introduce constraints over the target language output in order to adapt to the task-specific properties. However, they do not work on the document level, which would be an interesting direction for future work. 6 Conclusion a"
W13-5634,D12-1108,1,0.934115,"rt words as indicators. Our goal is to incorporate these features in machine translation in order to combine text simplification and adequate translation in one system. To the best of our knowledge, this has not been attempted before and represents a novel and challenging idea in the field of MT research. Global features such as the ones mentioned above require new approaches to the general problem of decoding in SMT. Fortunately, we have recently presented a new document-level decoder, which, contrary to standard SMT decoders, translates documents as a unit instead of sentences in isolation (Hardmeier et al., 2012). This allows us to define document-wide features in the target language to test our ideas. Our application is also a good test case for the capabilities of the decoder and we would like to use our findings in future developments of general user-targeted machine translation. The contributions of this paper are thus two-fold: (1) We show that document-wide decoding can effectively use global features and (2) we demonstrate that readability features can be used in SMT to produce simplified text translations. The remainder of the paper is organized as follows: First, we introduce important backgr"
W13-5634,2005.mtsummit-papers.11,0,0.0391844,"alue feature both on word level and on phrase level. On the phrase level we consider the phrases used by the SMT decoder, and on the word level we consider individual source words, and their alignment to 0 − N target words. TTR = Q-value = 4 C(tokens) C(types) f (st) n(s) + n(t) (7) (8) Experiments In the following, we show results for our experiments with the Docent decoder that include readability features and compare them to runs without them. The systems are evaluated using both MT and readability metrics. 4.1 Experimental Setup We evaluate our models on parliamentary texts from Europarl (Koehn, 2005), which contain both complex sentences and a lot of domain-specific terminology. All tests are performed for English–Swedish translation. Our system is trained on 1,488,322 sentences. For evaluation, we extracted 20 documents with a total of 690 sentences from a separate part of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We init"
W13-5634,P07-2045,0,0.0103962,"both MT and readability metrics. 4.1 Experimental Setup We evaluate our models on parliamentary texts from Europarl (Koehn, 2005), which contain both complex sentences and a lot of domain-specific terminology. All tests are performed for English–Swedish translation. Our system is trained on 1,488,322 sentences. For evaluation, we extracted 20 documents with a total of 690 sentences from a separate part of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We initialized our experiments with a Moses model that uses standard features of a phrase-based system: a 5-gram language model, five translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weig"
W13-5634,N03-1017,0,0.0152721,"endence between the sentences in a text. This independence assumption is exploited in the most popular SMT decoding algorithms, which efficiently explore a very large search space by using dynamic programming (Och et al., 2001). Integrating discourse-wide information into traditional SMT decoders is difficult because of these dynamic programming assumptions. We therefore implement our document-level readability models in the recently published document-level SMT decoder Docent (Hardmeier et al., 2012), which does not have these limitations. The model implemented by Docent is phrase-based SMT (Koehn et al., 2003). The decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by applying a series of operations using a hill climbing strategy to find a (local) maximum of the score function. The three operations used are to change the translation of phrases, to swap the position of two phrases , and to resegment phrases. This setup is not limited by dynamic programming constraints, so we can define Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference"
W13-5634,P03-1021,0,0.0433779,"of Europarl. A document is defined as a complete contiguous sequence of utterances of one speaker. We excluded documents that are shorter than 20 sentences and longer than 79 sentences. Moses (Koehn et al., 2007) was used for training the translation model and SRILM (Stolcke, 2002) for training the language model. We initialized our experiments with a Moses model that uses standard features of a phrase-based system: a 5-gram language model, five translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weights for the readability-based features with low, medium, and high impact relative to the standard features. We performed automatic evaluations using a set of common metrics for MT quality and readability. For MT quality we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Con"
W13-5634,W01-1408,0,0.0740805,"paper is organized as follows: First, we introduce important background on document-level decoding and readability. Thereafter, we present our experiments using a set of global features. Finally, we add some information about related work, summarize our findings and give ideas about future work. 2 Document-wide SMT Most current SMT systems translate sentences individually, assuming independence between the sentences in a text. This independence assumption is exploited in the most popular SMT decoding algorithms, which efficiently explore a very large search space by using dynamic programming (Och et al., 2001). Integrating discourse-wide information into traditional SMT decoders is difficult because of these dynamic programming assumptions. We therefore implement our document-level readability models in the recently published document-level SMT decoder Docent (Hardmeier et al., 2012), which does not have these limitations. The model implemented by Docent is phrase-based SMT (Koehn et al., 2003). The decoder uses a local search approach whose state consists of a complete translation of an entire document at any time. The initial state is improved by applying a series of operations using a hill climb"
W13-5634,P02-1040,0,0.0861851,"ive translation model features, a distance-based reordering penalty, and a word counter. These features were optimized using minimum error-rate training (Och, 2003) and the same weights were then used in Docent. Currently, we are developing the optimization procedure in Docent and could not use it in this work. We thus used a grid search approach for choosing weights for the readability-based features with low, medium, and high impact relative to the standard features. We performed automatic evaluations using a set of common metrics for MT quality and readability. For MT quality we used BLEU (Papineni et al., 2002) and NIST (Doddington, 2002), Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Proceedings #85 [page 378 of 474] Feature Reference Baseline OVIX TTR Qw Qp nLW nXLW SL Weight – – low medium high low medium high low medium high low medium high low medium high low medium high low medium high BLEU↑ – 0.243 0.243 0.228 0.144 0.243 0.225 0.150 0.242 0.231 0.165 0.243 0.229 0.097 0.244 0.225 0.106 0.241 0.225 0.224 0.242 0.211 0.150 NIST↑ – 6.12 6.11 5.83 4.41 6.12 5.75 4.48 6.10 5.90 4.93 6.12 5.99 3.90 6.14 5.96 4.11 6.10 5.85 5"
W13-5634,W11-4627,0,0.0286822,"det i Lissabon särskilt uppmärksammat främja våra ansträngningar att genomföra riskkapital så att handlingsplanen avslutas 2003. Table 5: Examples of translation output from a sample of systems sentence compression (e.g., Knight and Marcu, 2000; Specia, 2010). Furthermore, there is a wide range of publications using other methods for monolingual sentence compression and text simplification, (e.g., Daelemans et al., 2004; Cohn and Lapata, 2009). Readability has also been investigated as an effect of text summarization, as measured by user studies (Margarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translatio"
W13-5634,P11-4010,1,0.864145,"Missing"
W13-5634,W10-2602,1,0.879769,"rgarido et al., 2008) and automatic metrics (Smith and Jönsson, 2011). In these studies the readability was generally better in the summarized texts than in the original texts. Stymne and Smith (2012) showed that SMT is affected by summarization, but found no relation between readability and SMT quality measured by standard evaluation metrics. There is also related work concerned with the integration of wide contextual features in machine translation, such as lexical consistency. The effect of lexical consistency in translation has been studied by Carpuat (2009) and Carpuat and Simard (2012). Tiedemann (2010) proposed cached models to push consistent translation with some success in the case of domain adaptation. The use of word sense disambiguation in SMT is another example where wide contextual information can be incorporated on the source side (Carpuat and Wu, 2007; Chan et al., 2007) Another study related to ours is Genzel et al. (2010), who study poetry translation and perform joint translation and poetry creation of news text as well as translation of poems that keep the Proceedings of the 19th Nordic Conference of Computational Linguistics (NODALIDA 2013); Linköping Electronic Conference Pr"
W14-3312,J92-4003,0,0.0276371,"nd 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special placeholders. At decoding time, if a placeholder is encountered in a target language phrase, the applicable pronouns are generated with equal translation model proba"
W14-3312,P10-2033,0,0.0198937,"e case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The final system is a two-step model in which we apply translation and language models trained on preordered target language data to perform the first step, which also includes a reordered POS language model. The second step is also treated as a translation problem as in Sudoh et al. (2011), and in our newstest2013 19.3 19.4 18.6 19.5 19.5 19.7 newstest2014"
W14-3312,W13-2210,0,0.0299303,"Missing"
W14-3312,P05-1066,0,0.162059,"Missing"
W14-3312,N13-1073,0,0.0402683,"US The fall of Saddam ushers in the right circumstances. Der Sturz von Saddam leitet solche richtigen Umst¨ande ein. Der Sturz von Saddam ein leitet solche richtigen Umst¨ande. Table 1: Two examples of pre-ordering outputs. The first two lines are the original English and German sentences and the third line shows the reordered sentence. We use three systems based on Moses to compare the effect of reordering on alignment and translation. All systems are case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation"
W14-3312,C10-1043,0,0.0141995,"l training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering be"
W14-3312,E12-1074,0,0.0128122,"tables. Another reason is the possible distance of finite and infinitival verbs in German verb phrases that can lead to the same problems described above with verb-particle constructions. The auxiliary or modal verb is placed at the second position but the main verb appears at the end of the associated verb phrase. The distances can be arbitrarily long and long-range dependencies are quite frequent. Similarly, negation particles and adverbials move away from the inflected verb forms in certain constructions. For more details on specific phenomena in German, we refer to (Collins et al., 2005; Gojun and Fraser, 2012). Pre-ordering, i.e. moving English words into German word order does not seem to be a good option as we loose the connection between related items when moving particles and main verbs away from their associated elements. Hence, we are interested in reordering the target language German into English word order which can be beneficial in two ways: (i) Reordering the German part of the parallel training data makes it possible to improve word alignment (which tends to prefer monotonic mappings) and subsequent phrase extraction which leads to better translation models. (ii) We can explore a two-st"
W14-3312,D13-1037,1,0.676415,"n Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dyn"
W14-3312,W11-2123,0,0.0336108,"em submitted by Cho et al. (2013) to the WMT 2013 shared task. Our phrase table is trained on data taken from the News commentary, Europarl, UN, Common crawl and 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special pl"
W14-3312,D07-1103,0,0.0113749,"Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and o"
W14-3312,2005.iwslt-1.8,0,0.039688,"tymne et al., 2013) that it was useful to relax the hard distortion limit by either using a soft constraint, which could be tuned, or removing the limit completely. In that work we still used the standard parametrization of distortion, based on the positions of the first and last words in phrases. Our Docent decoder, however, always provides us with a full target translation that is step-wise improved, which means that we can apply distortion measures on the phrase-level without resorting to heuristics, which, for instance, are needed in the case of the lexicalized reordering models in Moses (Koehn et al., 2005). Because of this it is possible to use phrase-based distortion, where we calculate distortion based on the order of phrases, not on the order of some words. It is possible to parametrize phrase-distortion in different ways. In this work we use the phrase-distortion distance and a soft limit on the distortion distance, to mimic the word-based distortion. In our experiments we always set the soft limit to a distance of four phrases. In addition we use a measure based on how many crossings a phrase order gives rise to. We thus have three phrase-distortion features. As captured by lexicalized reo"
W14-3312,2009.mtsummit-posters.13,0,0.0172318,"e shortest distance between any pair of words in the aligned sets. The network is a binary classifier trained to discriminate positive examples extracted from human-made reference 123 amod nn auxpass by SMT systems with limited reordering capabilities such as phrase-based models. Preordering is often done on the entire training data as well to optimize translation models for the pre-ordered input. Less common is the idea of post-ordering, which refers to a separate step after translating source language input to an intermediate target language with corrupted (source-language like) word order (Na et al., 2009; Sudoh et al., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using th"
W14-3312,D12-1108,1,0.882504,"mne J¨org Tiedemann Aaron Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2"
W14-3312,W09-0435,0,0.11068,"trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering becomes attractive for several reasons: One reason is"
W14-3312,P13-4033,1,0.682907,"n Smith Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract We describe the Uppsala University systems for WMT14. We look at the integration of a model for translating pronominal anaphora and a syntactic dependency projection model for English–French. Furthermore, we investigate post-ordering and tunable POS distortion models for English– German. 1 Introduction In this paper we describe the Uppsala University systems for WMT14. We present three different systems. Two of them are based on the documentlevel decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dyn"
W14-3312,W11-2124,0,0.0221714,"ared task. Our phrase table is trained on data taken from the News commentary, Europarl, UN, Common crawl and 109 corpora. The first three of these corpora were included integrally into the training set after filtering out sentences of more than 80 words. The Common crawl and 109 data sets were run through an additional filtering step with an SVM classifier, closely following Mediani et al. (2011). The system includes three language models, a regular 6-gram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) trained with KenLM (Heafield, 2011), a 4-gram bilingual language model (Niehues et al., 2011) with Kneser-Ney smoothing trained with KenLM and a 9-gram model over Brown clusters (Brown et al., 1992) with Witten-Bell smoothing (Witten and Bell, 1991) trained with SRILM (Stolcke, 2002). 122 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 122–129, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics place all pronouns that should be handled by the classifier, i.e. instances of il and elle aligned to it and instances of ils and elles aligned to they, with special placeholders. At decoding time, if a placeholder is encounte"
W14-3312,W01-1408,0,0.0359546,"et al., 2012; Hardmeier et al., 2013a). In our English–French system we extend Docent to handle pronoun anaphora, and in our English–German system we add partof-speech phrase-distortion models to Docent. For German–English we also have a system based on Moses (Koehn et al., 2007). Again the focus is on word order, this time by using pre- and postreordering. 2 Document-Level Decoding Traditional SMT decoders translate texts as bags of sentences, assuming independence between sentences. This assumption allows efficient algorithms for exploring a large search space based on dynamic programming (Och et al., 2001). Because of the dynamic programming assumptions it is hard to directly include discourse-level and long-distance features into a traditional SMT decoder. In contrast to this very popular stack decoding approach, our decoder Docent (Hardmeier et al., 2012; Hardmeier et al., 2013a) implements a search procedure based on local search. At any stage of the search process, its search state consists of a complete document translation, making it easy for feature models to access the complete document Joakim Nivre with its current translation at any point in time. The search algorithm is a stochastic"
W14-3312,P03-1021,0,0.00912326,"our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain lang"
W14-3312,popovic-ney-2006-pos,0,0.0838413,"Missing"
W14-3312,2007.tmi-papers.21,0,0.0459927,"a and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Post-ordering becomes attractive for sever"
W14-3312,W10-1727,1,0.857022,"e-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The final system is a two-step model in which we apply translation and language models trained on preordered target language data to perform the first step, which also includes a reordered POS language model. The second step is also treated as a translation problem as in Sudoh et al. (2011), and in our newstest2013 19.3 19.4 18.6 19.5 19.5 19.7 newstest2014 19.1 19.3 18.7 19.3 1"
W14-3312,W13-2229,1,0.877852,"l., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et al. (2013). We use factored models with POS tags as a second output factor for German. The possibility to use language models for different factors has been added to our Docent decoder. Language models include an in-domain news language model, an out-of-domain model trained on the target side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are give"
W14-3312,2011.mtsummit-papers.36,0,0.175414,"ce between any pair of words in the aligned sets. The network is a binary classifier trained to discriminate positive examples extracted from human-made reference 123 amod nn auxpass by SMT systems with limited reordering capabilities such as phrase-based models. Preordering is often done on the entire training data as well to optimize translation models for the pre-ordered input. Less common is the idea of post-ordering, which refers to a separate step after translating source language input to an intermediate target language with corrupted (source-language like) word order (Na et al., 2009; Sudoh et al., 2011). punct nsubjpass prep pobj Domestic meat production is dominated by chicken . La production int´erieure de viande est domin´ee par le poulet . Figure 2: Dependency projection model translations from negative examples extracted from n-best lists generated by a baseline SMT system. 4 English–German For English–German we have two systems, one based on Moses, and one based on Docent. In both cases we have focused on word order, particularly for verbs and particles. Both our systems are trained on the same data made available by WMT. The Common crawl data was filtered using the method of Stymne et"
W14-3312,tiedemann-2012-parallel,1,0.687825,"st¨ande ein. Der Sturz von Saddam ein leitet solche richtigen Umst¨ande. Table 1: Two examples of pre-ordering outputs. The first two lines are the original English and German sentences and the third line shows the reordered sentence. We use three systems based on Moses to compare the effect of reordering on alignment and translation. All systems are case-sensitive phrase-based systems with lexicalized reordering trained on data provided by WMT. Word alignment is performed using fast align (Dyer et al., 2013). For tuning we use newstest2011. Additionally, we also test parallel data from OPUS (Tiedemann, 2012) filtered by a method adopted from Mediani et al. (2011). To contrast our baseline system, we trained a phrase-based model on parallel data that has been aligned on data pre-ordered using the reordering rules for German, which has been restored to the original word order after word alignment and before phrase extraction (similar to (Carpuat et al., 2010; Stymne et al., 2010)). We expect that the word alignment is improved by reducing crossings and long-distance links. However, the translation model as such has the same limitations as the baseline system in terms of long-range distortions. The"
W14-3312,C04-1073,0,0.0399491,"et side of the parallel training data and a POS language model trained on tagged news data. The LMs are trained in the same way as for English–French. All systems are tuned using MERT (Och, 2003). Phrase-tables are filtered using entropy-based pruning (Johnson et al., 2007) as implemented in Moses. All BLEU scores are given for uncased data. 4.1 Pre-Ordered Alignment and Post-Ordered Translation The use of syntactic reordering as a separate preprocessing step has already a long tradition in statistical MT. Handcrafted rules (Collins et al., 2005; Popovi´c and Ney, 2006) or data-driven models (Xia and McCord, 2004; Genzel, 2010; Rottmann and Vogel, 2007; Niehues and Kolss, 2009) for preordering training data and system input have been explored in numerous publications. For certain language pairs, such as German and English, this method can be very effective and often improves the quality of standard SMT systems significantly. Typically, the source language is reordered to better match the syntax of the target language when translating between languages that exhibit consistent word order differences, which are difficult to handle In our experiments, we focus on the translation from English to German. Po"
W14-3312,P07-2045,0,\N,Missing
W14-3312,W12-3144,0,\N,Missing
W14-3312,2011.iwslt-evaluation.9,0,\N,Missing
W14-3334,C10-1043,0,0.40539,"tion units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. TUER(A, G) = 1 − 2|AU ∩ GU | |AU |+ |GU | Ahrenberg (2010) also proposed to measure reorderings. He does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a higher number of pairwise crossing links. Birch and Osborne (2011) suggest using squared Kendall τ distance (SKTD), see Eq. 8, where n is the number of links, as a basis of LR-score, an MT metric that takes reordering into account. They found that squaring τ better explained reordering, than using only τ . In this study we will use both, crossing score and SKT"
W14-3334,ahrenberg-etal-2000-evaluation,1,0.732969,"Missing"
W14-3334,P04-1064,0,0.0336358,"ents we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much smaller trainin"
W14-3334,ahrenberg-2010-alignment,0,0.150189,"nd Szał, 2012; Dyer et al., 2013). The relation between the two types of evaluation is often quite weak. Sev1 TUER is similar to CPER (Ayan and Dorr, 2006), which measures the error rate of extracted phrases. Due to how phrase extraction handle null links, there are differences, however. 277 is also possible to define Precision, Recall and Fmeasure over translation units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. TUER(A, G) = 1 − 2|AU ∩ GU | |AU |+ |GU | Ahrenberg (2010) also proposed to measure reorderings. He does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a hi"
W14-3334,W09-0421,1,0.925521,"al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these tasks we are mainly interested in the full translation task, for which we report Bleu scores. In"
W14-3334,P06-1002,0,0.0214734,"nd Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it (4) Ayan and Dorr (2006) on the other hand found some evidence for the importance of precision over recall. However, they used much smaller training data than Fraser and Marcu (2007). They also suggested using a measure called consistent phrase error-rate (CPER), but found that it was hard to assess the impact of alignment on MT, both with AER and CPER. Lambert et al. (2012) performed a study where they investigated the effect of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important"
W14-3334,holmqvist-etal-2012-alignment,1,0.91956,"ordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these tasks we are mainly interested in the full translation task, for which we report Bleu scores. In addition we also show fuzzy reordering score (FRS), which focuses 281 SMT, Bleu POSReo, FRS POSReo, Bleu AlignReo, FRS AlignReo, Bleu Total .33 −.80 −.64 −.77 −.81 SMT, Bleu POS"
W14-3334,P11-1103,0,0.0961479,"e does this by calculating the percentage of links with crossings of different lengths. To define this he only considers adjacent links in the source using the distance between corresponding target words, which means that his metric becomes a directional measure. Reorderings of alignments was also used by Genzel (2010), who used crossing score, the number of crossing links, to rank reordering rules. This is non-directional and simpler to calculate than Ahrenberg (2010)’s metrics, and implicitly covers length since a long distance reordering leads to a higher number of pairwise crossing links. Birch and Osborne (2011) suggest using squared Kendall τ distance (SKTD), see Eq. 8, where n is the number of links, as a basis of LR-score, an MT metric that takes reordering into account. They found that squaring τ better explained reordering, than using only τ . In this study we will use both, crossing score and SKTD. Figure 1 shows these scores for an example sentence. These two measures only tell us how much reordering there is. To quantify this relative to the gold standard we also report the absolute difference between the number of gold standard crossings and system crossings, which we call Crossdiff. To acco"
W14-3334,J93-2003,0,0.0624236,"A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Word Alignment and SMT Word alignment is the task of relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is general"
W14-3334,N03-1017,0,0.014538,"igned Table 1: Symmetrization strategies for word alignments AT S and AST in two directions these models require external tools (for creating linguistic features) and manually aligned training data, which we do not have for our data sets (besides the data we need for evaluation). Investigating these types of models are outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A"
W14-3334,P10-2033,0,0.0155304,"on, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization h"
W14-3334,2005.iwslt-1.8,0,0.177029,"e been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8"
W14-3334,J07-2003,0,0.467625,"s of models are outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard do"
W14-3334,P05-1066,0,0.119492,"Missing"
W14-3334,P07-2045,0,0.0152871,"68 .370 .455 .514 .486 .392 .444 .500 .545 .524 .452 .445 .507 .548 .495 .450 .503 .537 .584 .525 .515 Link crossings P R F – – – 41060 31660 27245 12101 8122 29429 20671 17053 6718 4146 6718 4547 3044 12764 9779 8340 8277 8197 15829 11585 20099 17481 18934 22086 20439 Crossdiff 0 Table 2: Values for alignment quality indicators for the different alignments, where 2–4, HMM, and fa are alignment models, and symmetrization strategies refer to Table 1 Total 22629 Section 3.2) of 2M sentences during alignment. For symmetrization we used all methods in Table 1, as implemented in the Moses toolkit (Koehn et al., 2007) and in fast align (Dyer et al., 2013). Link level ↓ P R F Based on the automatically aligned gold standard, we calculated all alignment indicators for all settings. The complete results can be found in Table 2, where we have ordered the symmetrization methods with the most sparse, intersection, on top. Overall we can see that while several of the alignment methods create a much higher number of alignment links than the gold standard, they do not produce many more translation units. This is very interesting and indicates why link level statistics may not be accurate enough to predict the perfo"
W14-3334,N13-1073,0,0.516213,"words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide r"
W14-3334,W08-0306,0,0.0189954,"l., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral o"
W14-3334,D13-1049,0,0.0133298,"seline. These results confirm results from previous studies that link level measures, especially recall and weighted F-measure show some correlation with SMT quality whereas precision does not. 4 In the preordering studies cited above it is often not even stated which alignment model was used. A few authors mention the alignment tool that has been applied but no comparison between different alignment models is performed in any of the papers we are aware of. Li et al. (2007), for example, simply state that they used GIZA++ and gdf symmetrization and that they removed less probable multi links. Lerner and Petrov (2013) use the intersection of HMM alignments and claims that model 4 did not add much value. Genzel (2010) did mention that using a standard model 4 was not successful for his rule learning approach. Instead he used filtered model-1-alignments, which he claims was more successful. However, there are no further analyses or comparisons between the alignments reported in any of these papers. Reordering Tasks for SMT Reordering is an important part of any SMT system. One way to address it is to add reordering models to standard PBSMT systems, for instance lexicalized reordering models (Koehn et al., 20"
W14-3334,J07-3002,0,0.550421,"set of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. 1 Introduction Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment 275 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275–286, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Symmetrization int: intersection"
W14-3334,P07-1091,0,0.154673,"is a standard PBSMT system trained on WMT13 280 inter gd gdfa gdf union m2 18.1 20.4 20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are partial matches. There are no significant correlations with link degree or link crossings, except a negative correlation with Crossdiff, which means that it is good to have a similar number of crossings as the baseline. These resul"
W14-3334,N04-1035,0,0.0518221,"r current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Word"
W14-3334,P08-1112,0,0.0371481,"Missing"
W14-3334,N06-1014,0,0.0456501,"study where they investigated the effect of word alignment on MT using a large number of word alignment indicators. They found that there was a difference between large and small datasets in that alignment precision was more important with small data sets, and recall more important with large data sets. Overall they did not find any indicator that was significant over two language pairs and different corpus sizes. There were more significant indicators for large datasets, however. Most researchers who propose new alignment models perform both a gold standard evaluation and an SMT evaluation (Liang et al., 2006; Ganchev et al., 2008; Junczys-Dowmunt and Szał, 2012; Dyer et al., 2013). The relation between the two types of evaluation is often quite weak. Sev1 TUER is similar to CPER (Ayan and Dorr, 2006), which measures the error rate of extracted phrases. Due to how phrase extraction handle null links, there are differences, however. 277 is also possible to define Precision, Recall and Fmeasure over translation units in the same way as for alignment links. We will use these three measures to get a broader picture of TUs in alignment evaluation. Also in this case, 1−TUER is equivalent to F-measure. T"
W14-3334,W09-3805,0,0.0194029,"s the classic group where metrics are calculated on the alignment link level, which has been used in several studies. In our experiments we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use TUER, it (4) Ayan and Dor"
W14-3334,P05-1057,0,0.0280805,"inks) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality an"
W14-3334,W07-2456,0,0.0307144,"T Reordering Tasks Sara Stymne J¨org Tiedemann Joakim Nivre Uppsala University Department of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacit"
W14-3334,W10-1727,1,0.858503,"eech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word alignments on non-reordered texts (Holmqvist et al., 2009). For the latter approach a small study of the effect of gd and gdfa symmetrizations was presented, which only showed small variations in quality scores (Holmqvist et al., 2012). Below we present the two tasks that we study in this paper: part-of-speech-based reordering for creating input lattices for SMT and alignmentbased reordering for improving phrase-tables. We evaluate the performance of these tasks in relation to the use of different alignment models and symmetrization heuristics. For these t"
W14-3334,H05-1011,0,0.0350232,"versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality and AER or weig"
W14-3334,W09-0435,0,0.31843,"part of the evaluation of MT systems (Birch and Osborne, 2011). We can distinguish two main types of approaches to preordering in SMT, either by using hand-written rules, which often operate on syntactic trees (Collins et al., 2005), or by reordering rules that are learnt automatically based on a word aligned corpus (Xia and McCord, 2004). The latter approach is of interest to us, since it is based on word alignments. There has been much work on automatic learning of reordering rules, which can be based on different levels of annotation, such as part-of-speech tags (Rottmann and Vogel, 2007; Niehues and Kolss, 2009; Genzel, 2010), chunks (Zhang et al., 2007) or parse trees (Xia and McCord, 2004). In general, all these approaches lead to improvements of translation quality. The reordering is Another type of approach to reordering is to only reorder the data in order to improve word alignments, and to restore the original word order before training the SMT system. This type of approach has the advantage that no modifications are needed for the translation input. This approach has also been used both with hand-written rules (Carpuat et al., 2010; Stymne et al., 2010) and with rules based on initial word al"
W14-3334,W12-0704,1,0.823036,"somewhat, the correlations with alignment indicators were stable. SMT Experiments For reference, we first study the impact of alignment on SMT performance. Our SMT system is a standard PBSMT system trained on WMT13 280 inter gd gdfa gdf union m2 18.1 20.4 20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are partial matches. There are no significant correlations w"
W14-3334,C00-2163,0,0.19044,"stimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction between S and P. Word Alignment and SMT Word alignment is the task of relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora usi"
W14-3334,N12-1052,0,0.053597,"Missing"
W14-3334,J03-1002,0,0.198899,"its, and on the subset of crossing links, like intersected HMM models, are preferred. Unlike SMT performance the desired alignment characteristics are similar for small and large training data for the pre-reordering tasks. Moreover, we confirm previous research showing that the fuzzy reordering score is a useful and cheap proxy for performance on SMT reordering tasks. 1 Introduction Word alignment is a key component in all state-ofthe-art statistical machine translation (SMT) systems, and there has been some work exploring the connection between word alignment quality and translation quality (Och and Ney, 2003; Fraser and Marcu, 2007; Lambert et al., 2012). The standard way to evaluate word alignments in this context is by using metrics like alignment error rate (AER) and F-measure on the link level, and the general conclusion appears to be that translation quality benefits from alignments with high recall (rather than precision), at least for large training data. Although many other ways of measuring alignment 275 Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 275–286, c Baltimore, Maryland USA, June 26–27, 2014. 2014 Association for Computational Linguistics Symmetriz"
W14-3334,W11-2102,0,0.112784,"−.57 .83 −.79 Total .65 −.23 .05 −.11 −.24 Total −.05 −.81 −.71 −.80 −.93 Translation units P R F −.20 .16 −.02 .90 .81 .89 .80 .80 .86 .90 .88 .92 .75 .64 .72 SKTD −.14 −.70 −.60 −.73 −.91 Link crossings P R −.09 .25 .90 .21 .79 .42 .94 .27 .86 −.07 P −.29 .82 .67 .81 .71 F .07 .86 .89 .92 .69 MWU R .59 −.45 −.23 −.37 −.53 F .44 .22 .35 .31 .04 Crossdiff −.63 −.41 −.49 −.38 −.52 Table 5: Pearson correlations between different alignment characteristics and scores for the translation and reordering tasks. Significant correlations are marked with bold (< 0.01). only on the reordering component (Talbot et al., 2011). It compares a system reordering to a reference reordering, by measuring how many chunks that have to be moved to get an identical word order, see Eq. 9, where C is the number of contiguously aligned chunks, and M the number of words. To find the reference ordering we apply the method of Holmqvist et al. (2009), described in Section 4.2, to the gold standard alignment. FRS = 1 − 4.1 C −1 M −1 inter gd gdfa gdf union m2 .577 .555 .540 .439 .442 m3 .575 .559 .540 .499 .492 m4 .581 .570 .559 .542 .544 HMM .596 .589 .579 .560 .563 fa .567 .546 .539 .495 .486 Table 6: Fuzzy reordering scores for p"
W14-3334,W99-0604,0,0.161786,"shown some relation between translation quality and AER or weighted F-measure, it has rarely been investigated thoroughly in its own right, and, as far as we are aware, not for other tasks than SMT. Furthermore, most of these studies considers nothing else but link level agreement. In this paper we take a broader view on alignment quality and explore the effect of other types of quality indicators as well. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM and HMM models on the alignment template approach (Och et al., 1999) in terms of AER. They found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the P set is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. F(A, G, α) =  α 1−α + Pr"
W14-3334,H05-1010,0,0.0390747,"generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et al., 2008). There is also a wide range of alternative approaches to word alignment. For example, various discriminative models have been proposed in the literature (Liu et al., 2005; Moore, 2005; Taskar et al., 2005). Their advantage is that they may integrate a wide range of features that may lead to improved alignment quality. However, most of |G ∩ A| |A| |G ∩ A| Recall(A, G) = |G| |P ∩ A |+ |S ∩ A| AER = 1 − |S |+ |A| Precision(A, G) = 276 (1) (2) (3) Crossing p =8 SKDT = 8/66 ≈ 0.65 6 1–1 links 3 multi links 0 null links Figure 1: An example alignment illustrating n–1, 1–m and crossing links. eral of these studies only show AER on their gold standard, despite its well-known shortcomings. Even though many studies have shown some relation between translation quality and AER or weighted F-measure, it has"
W14-3334,P03-1021,0,0.0838948,"on translation unit level. Significant correlations are marked with bold (< 0.01). data.2 We trained a German–English system on 2M sentences from Europarl and News Commentary. We used the target side of the parallel corpus and the SRILM toolkit (Stolcke, 2002) to train a 5gram language model. For training the translation model and for decoding we used the Moses toolkit (Koehn et al., 2007). We applied a standard feature set consisting of a language model feature, four translation model features, word penalty, phrase penalty, and distortion cost. For tuning we used minimum error-rate training (Och, 2003). In order to minimize the risk of tuning influencing the results, we used a fixed set of weights for each experiment, tuned on a model 4+gdfa alignment.3 For tuning we used newstest2009 with 2525 sentences, and for testing we used newstest2013 with 3000 sentences. Evaluation was performed using the Bleu metric (Papineni et al., 2002). The same system setup was used for the SMT systems with reordering. Table 4 shows the results on the SMT task. Model 3 and 4 with gd/gdfa symmetrization yield the highest scores. There is a larger difference between systems with different symmetrization than bet"
W14-3334,H05-1108,0,0.469573,"e.lastname@lingfil.uu.se Abstract quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks. In this paper we set out to explore the"
W14-3334,C96-2141,0,0.612207,"f relating words in one language to words in the translation in another language, see an example in Figure 1. Word alignment models can be learnt automatically from large corpora of sentence aligned data. Brown et al. (1993) proposed the so-called IBM models, which are still widely used. These five models estimate alignments from corpora using the expectation-maximization algorithm, and each model adds some complexity. Model 4 is commonly used in SMT systems. There have been many later suggestions of alternatives to these models. These are often alternatives to model 2, such as the HMM model (Vogel et al., 1996) and fast align (Dyer et al., 2013). All these generative models produce directional alignments where one word in the source can be linked to many target words (1–m links) but not vice versa. It is generally desirable to also allow n–1 and n–m links, and to achieve this it is common practice to perform word alignment in both directions and to symmetrize them using some heuristic. A number of common symmetrization strategies are described in Table 1 (Koehn et al., 2005). There are also other alternatives, such as the refined method (Och and Ney, 2003), or link deletion from the union (Fossum et"
W14-3334,P02-1040,0,0.0905907,"studies considers nothing else but link level agreement. In this paper we take a broader view on alignment quality and explore the effect of other types of quality indicators as well. The relation between word alignment quality and PBSMT has been studied by some researchers. Och and Ney (2000) looked at the impact of IBM and HMM models on the alignment template approach (Och et al., 1999) in terms of AER. They found that AER correlates with human evaluation of sentence level quality, but not with word error rate. Fraser and Marcu (2007) found that there is no correlation between AER and Bleu (Papineni et al., 2002), especially not when the P set is large. They found that a balanced F-measure is a better indicator of Bleu, but that a weighted F-measure is even better (see Eq. 4) mostly with a higher weight for recall than for precision. This weight, however, needs to be optimized for each data set, language pair, and gold standard alignment separately. F(A, G, α) =  α 1−α + Precision(A,G) Recall(A,G) −1 3 Word Alignment Quality Indicators We investigate four groups of quality indicators. The first group is the classic group where metrics are calculated on the alignment link level, which has been used i"
W14-3334,C04-1073,0,0.545815,"Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks. In this paper we set out to explore the impact of alignment quality on two pre-reordering tasks for SMT. In doing so, we employ a wider range of quality indicators than is customary, and for reference these indicators are used also to assess overall translation quality. To allow an in-depth exploration of the connections between several aspects of word alignment and reordering, we limi"
W14-3334,postolache-etal-2006-transferring,0,0.0614037,"Missing"
W14-3334,P02-1039,0,0.321492,"e outside the scope of our current work. Word alignments are used as an important knowledge source for training SMT systems. In word-based SMT, the parameters of the generative word alignment models are essentially the translation model of the system. In phrase-based SMT (PBSMT) (Koehn et al., 2003), which is among the state-of-the-art systems today, word alignments are used as a basis for extracting phrases and estimating phrase alignment probabilities. Similarly, word alignments are also used for estimating rule probabilities in various kinds of hierarchical and syntactic SMT (Chiang, 2007; Yamada and Knight, 2002; Galley et al., 2004). Intrinsic evaluation of word alignment is generally based on a comparison to a gold standard of human alignments. Based on the gold standard, metrics like precision, recall and F-measure can be calculated for each alignment link, see Eqs. 1– 2, where A are hypothesized alignment links and G are gold standard links. Another common metric is alignment error rate (AER) (Och and Ney, 2000), which is based on a distinction between sure, S, and possible, P , links in the gold standard. 1−AER is identical to balanced F-measure when the gold standard does not make a distinction"
W14-3334,2007.tmi-papers.21,0,0.619227,"eparate tunings for each alignment. While the absolute results varied somewhat, the correlations with alignment indicators were stable. SMT Experiments For reference, we first study the impact of alignment on SMT performance. Our SMT system is a standard PBSMT system trained on WMT13 280 inter gd gdfa gdf union m2 18.1 20.4 20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are"
W14-3334,H01-1035,0,0.0602033,"nt of Linguistics and Philology firstname.lastname@lingfil.uu.se Abstract quality have been proposed, such as working on translation units (Ahrenberg et al., 2000; Ayan and Dorr, 2006; Søgaard and Kuhn, 2009) or using link degree and related measures (Ahrenberg, 2010), these methods have not been used to study the relation between alignment and translation quality, with the exception of Lambert et al. (2012). Word alignment is also used for many other tasks besides translation, including term bank creation (Merkel and Foo, 2007), cross-lingual annotation projection for part-of-speech tagging (Yarowsky et al., 2001), semantic roles (Pado and Lapata, 2005), pronoun anaphora (Postolache et al., 2006), and cross-lingual clustering (T¨ackstr¨om et al., 2012). Even within SMT itself, there are tasks such as reordering that often make crucial use of word alignments. For instance, source language reordering commonly relies on rules learnt automatically from word-aligned data (e.g., Xia and McCord (2004)). As far as we know, no one has studied the impact of alignment quality on these additional tasks, and it seems to be tacitly assumed that alignments that are good for translation are also good for other tasks."
W14-3334,W09-2303,0,0.023373,"ndicators. The first group is the classic group where metrics are calculated on the alignment link level, which has been used in several studies. In our experiments we use a gold standard that does not make use of distinctions between sure and possible links, as suggested by Fraser and Marcu (2007). With this, we can calculate the standard metrics P(recision) R(ecall) and F(-measure). We will mainly use balanced F-measure, but occasionally also report weighted F-measure. As noted before, 1−AER is equivalent to balanced F when only sure links are used, and will thus not be reported separately. Søgaard and Kuhn (2009) and Søgaard and Wu (2009) suggested working on the translation unit (TU) level, instead of the link level. A translation unit, or cept (Goutte et al., 2004), is defined as a maximally connected subgraph of an alignment. In Figure 1, the twelve links form nine translation units. Søgaard and Wu (2009) suggest the metric TUER, translation unit error rate, shown in Eq. 5, where AU are hypothesized translation units, and GU are gold standard translation units.1 They use TUER to establish lower bounds for the coverage of alignments from different formalisms, not to evaluate SMT. While they only use"
W14-3334,2007.iwslt-1.3,0,0.141624,"20.4 19.4 19.2 m3 19.1 20.9 20.7 19.7 19.6 m4 19.3 20.9 20.8 20.1 19.8 HMM 18.8 20.5 20.5 19.9 19.7 fa 18.9 20.6 20.5 20.0 20.0 always applied on the translation input. It can also be applied on the source side of the training corpora, which sometimes improves the results (Rottmann and Vogel, 2007), but sometimes does not make a difference (Stymne, 2012). When preordering is performed on the translation input, it can be presented to the decoder as a 1-best reordering (Xia and McCord, 2004), as an n-best list (Li et al., 2007), or as a lattice of possible reorderings (Rottmann and Vogel, 2007; Zhang et al., 2007). Table 4: Baseline Bleu scores for different symmetrization heuristics suggesting that they measure similar things. Intuitively it seems important for SMT to match full translation units, but it might be the case that the phrase extraction strategy is robust as long as there are partial matches. There are no significant correlations with link degree or link crossings, except a negative correlation with Crossdiff, which means that it is good to have a similar number of crossings as the baseline. These results confirm results from previous studies that link level measures, especially recall and"
W15-2501,W15-2508,1,0.700494,"all submissions, both primary and secondary, in terms of macro-averaged F-score, several systems performed better in terms of accuracy. The other systems did not use explicit anaphora resolution, but attempted to gather relevant information about possible antecedents by considering a certain number of preceding, or preceding and following, noun phrases. They differed in the type of classifier and in the information sources used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED U"
W15-2501,E06-1032,0,0.027114,"itself achieves the best scores, but considering the inadequacy of BLEU for pronoun evaluation, we do not see this as a major concern in itself. The other submissions fall behind in terms of automatic MT metrics. The UU - HARDMEIER system is similar to the other SMT systems, but uses different language and translation models, which evidently do not yield the same level of raw MT performance as the baseline system. ITS 2 is a rule-based system. Since it is well known that n-gram-based evaluation metrics do not always do full justice to rule-based MT approaches not using n-gram language models (Callison-Burch et al., 2006), it is difficult to draw definite conclusions from this system’s lower scores. Finally, the extremely low scores for the A 3-108 system indicate serious problems with translation quality, an impression that we easily confirmed by examining the system output. 8 5 The low scores for the ITS 2 system were partly due to a design decision. The anaphora prediction component of ITS 2 only generated the personal pronouns il, elle, ils and elles; this led to zero recall for ce and ça/cela and, as a consequence, to a large number of misses that would have been comparatively easy to predict with an n-gr"
W15-2501,P14-1065,1,0.749966,"Missing"
W15-2501,2012.eamt-1.60,1,0.298652,"est data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare"
W15-2501,2010.iwslt-papers.10,1,0.800853,"g such a setup makes it possible to explore a variety of approaches for solving the problem at hand since the participating groups independently come up with various ways to address it. All of this is highly beneficial for continued research as it creates a well-defined benchmark with a low entry barrier, a set of results to compare to, and a collection of properly evaluated ideas to start from. We decided to base this shared task on the problem of pronoun translation. Historically, this was one of the first discourse problems to be considered in the context of SMT (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010); yet, it is still far from being solved. For an overview of the existing work on pronoun translation, we refer the reader to Hardmeier (2014, Section 2.3.1). The typical case is an anaphoric pronoun – one that refers to an entity mentioned earlier in the discourse, its antecedent. Many languages have agreement constraints between pronouns and their antecedents. In translation, these constraints must be satisfied in the target language. Note that source language information is not enough for this task. To see why, consider the following example for English– French:1 We describe the design, the"
W15-2501,P13-4033,1,0.934301,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,chrupala-etal-2008-learning,0,0.0467164,"Missing"
W15-2501,W11-2107,0,0.0222412,"Missing"
W15-2501,D13-1037,1,0.793439,"n software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), t"
W15-2501,N13-1073,0,0.0272151,"inting characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoot"
W15-2501,W15-2510,1,0.750725,"hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphora resolution (Hardmeier et al., 2013b), but using the Stanford coreference resolution software at test time. 4 While discourse-aware MT evaluation metrics were proposed recently (Guzmán et al., 2014b; Joty et al., 2014; Guzmán et al., 2014a), they do not specifically focus on pronoun translation. 5 Machine Translation Evaluation http://stp.lingfil.uu.se/~ch/DiscoMT2015.maneval/index.php Machine Translation Eva"
W15-2501,J07-3002,0,0.00746111,"n order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all three models, the results on pronoun links are better than those on all links. More"
W15-2501,P13-2121,0,0.00705082,"Missing"
W15-2501,D07-1103,0,0.0077006,"00 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million words in English and 70.0 million words in French. We word-aligned the data using fast_align (Dyer et al., 2013) and we symmetrized the word alignments using the grow-diag-final-and heuristics. The phrase tables were extracted from the word-aligned bitext using Moses with standard settings. We also filtered the resulting phrase table using significance testing (Johnson et al., 2007) with the recommended filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200"
W15-2501,guillou-etal-2014-parcor,1,0.687529,"g requirements: 1. The talks have been transcribed (in English) and translated into French. 2. They were not included in the training, development, and test datasets of any IWSLT evaluation campaign, so DiscoMT.tst2015 can be used as held-out data with respect to those. 3. They contain a sufficient number of tokens of the English pronouns it and they translated into the French pronouns listed in Table 1. 4. They amount to a total number of words suitable for evaluation purposes (e.g., tens of thousands). 2 http://www.ted.com 3 The following overview of text characteristics is based on work by Guillou et al. (2014). 3 To meet requirement 3, we selected talks for which the combined count of the rarer classes ça, cela, elle, elles and on was high. The resulting distribution of pronoun classes, according to the extraction procedure described in Section 5.1, can be found in Table 8 further below. We aimed to have at least one pair of talks given by the same speaker and at least one pair translated by the same translator. These two features are not required by the DiscoMT shared task, but could be useful for further linguistic analysis, such as the influence of speakers and translators on the use of pronouns"
W15-2501,W15-2509,0,0.0482925,"gy of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T sy"
W15-2501,W14-3352,1,0.899484,"Missing"
W15-2501,D14-1027,1,0.902972,"Missing"
W15-2501,P07-2045,0,0.00942963,"2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment using the cleaning script provided by Moses, with 100 tokens as the maximum sentence length. The indexes of the retained lines were saved to make it possible to map sentences back to the annotated corpora. The final parallel corpus contained 2.4 million sentence pairs with 63.6 million w"
W15-2501,2005.mtsummit-papers.11,0,0.0282203,"e resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version 7 (Koehn, 2005), News Commentary version 9 and the shuffled news data from WMT 2007–2013 (Bojar et al., 2014). test set IWSLT 2010 IWSLT 2012 BLEU 33.86 40.06 (BP=0.982) (BP=0.959) Table 4: Baseline models for English-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For compl"
W15-2501,W15-2514,0,0.067491,"Missing"
W15-2501,W10-1737,0,0.740286,"Missing"
W15-2501,W11-1902,0,0.185739,"sions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with"
W15-2501,W15-2512,0,0.121441,"glish-French machine translation: case-insensitive BLEU scores. 4 We experimented with additional datasets and other settings (GIZA++ instead of fast_align, unfiltered phrase tables), but could not improve. All datasets, models and parameters were made available on the shared task website to make it easy to get started with new developments and to compare results with the provided baseline. For completeness, we also provided a recasing model that was trained on the same dataset to render it straightforward to produce case-sensitive output, which we required as the final submission. 4.2 ITS 2 (Loáiciga and Wehrli, 2015) was a rulebased machine translation system using syntaxbased transfer. For the shared task, it was extended with an anaphora resolution component influenced by Binding Theory (Chomsky, 1981). For the sixth submission, A 3-108, no system description paper was submitted. Its output seemed to have been affected by problems at the basic MT level, yielding very bad translation quality. 4.3 Evaluation Methods Evaluating machine translations for pronoun correctness automatically is difficult because standard assumptions fail. In particular, it is incorrect to assume that a pronoun is translated corr"
W15-2501,2006.amta-papers.25,0,0.144804,"Missing"
W15-2501,W15-2511,0,0.0430542,"s used. UU - TIEDEMANN (Tiedemann, 2015) used a linear support vector machine with local features and simple surface features derived from preceding noun phrases. WHATELLES (Callin et al., 2015) used a neural network classifier based on work by Hardmeier et al. (2013b), but replacing all (explicit or latent) anaphora resolution with information extracted from preceding noun phrases. The IDIAP system (Luong et al., 2015) used a Naïve Bayes classifier and extracted features from both preceding and following noun phrases to account for the possibility of cataphoric references. The GENEVA system (Loáiciga, 2015) used maximum entropy classification; unlike the other submissions, it included features derived from syntactic parse trees. 12 2: secondary submission BASELINE - NP 0 UU - TIED UEDIN MALTA 2 MALTA WHATELLES UEDIN 2 UU - TIED 2 GENEVA GENEVA 2 IDIAP IDIAP 2 A 3-108 ( WITHDRAWN ) Macro-F Accuracy ce cela elle elles F-score il 0.584 0.579 0.571 0.565 0.561 0.553 0.550 0.539 0.437 0.421 0.206 0.164 0.129 0.122 0.663 0.742 0.723 0.740 0.732 0.721 0.714 0.734 0.592 0.579 0.307 0.407 0.240 0.325 0.817 0.862 0.823 0.875 0.853 0.862 0.823 0.849 0.647 0.611 0.282 0.152 0.225 0.220 0.346 0.235 0.213 0.1"
W15-2501,W14-3334,1,0.85254,"ed the same bitext as for the MT baseline in the first task (Section 4.1); we pre-processed it like before, except for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignme"
W15-2501,W15-2513,0,0.169961,"ompared to the perhaps more obvious methodology of having the annotators judge examples as good or bad, treating evaluation as a gap-filling task has the advantage of avoiding a bias in favour of solutions generated by the evaluated systems. Submitted Systems We received six submissions to the pronounfocused translation task, and there are system descriptions for five of them. Four submissions were phrase-based SMT systems, three of which were based on the baseline described in Section 4.1. One was a rule-based MT system using a completely different approach to machine translation. The IDIAP (Luong et al., 2015) and the AUTO POST EDI T (Guillou, 2015) submissions were phrase-based, built using the same training and tuning resources and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline"
W15-2501,tiedemann-2012-parallel,1,0.765503,"ic or human processing. Table 3 shows some statistics and metadata about the TED talks that are part of the DiscoMT.tst2015 set. talk id segs 205 1756 1819 1825 1894 1935 1938 1950 1953 1979 2043 2053 189 186 147 120 237 139 107 243 246 160 175 144 4,188 4,320 2,976 2,754 5,827 3,135 2,565 5,989 4,520 2,836 3,413 2,828 4,109 4,636 3,383 3,078 6,229 3,438 2,802 6,416 4,738 2,702 3,568 3,023 J.J. Abrams A. Solomon S. Shah B. Barber A. Solomon S. Chandran P. Evans E. Snowden L. Page M. Laberge N. Negroponte H. Knabe total 2,093 45,351 48,122 – en tokens fr The parallel data were taken from OPUS (Tiedemann, 2012), which provides sentencealigned corpora with annotation. The latter is useful for finding document boundaries, which can be important when working with discourseaware translation models. All training data were pre-processed with standard tools from the Moses toolkit (Koehn et al., 2007), and the final datasets were lower-cased and normalized (punctuation was unified, and non-printing characters were removed). The pre-processing pipeline was made available on the workshop website in order to ensure compatibility between the submitted systems. The parallel data were prepared for word alignment"
W15-2501,W15-2515,1,0.722711,"and methods as the official baseline. Both adopted a two-pass approach involving an automatic post-editing step to correct the pronoun translations output by the baseline system, and both of them relied on the Stanford anaphora resolution software (Lee et al., 2011). They differed in the way the correct pronoun was assigned: the IDIAP submission used a classifier with features that included properties of the hypothesized antecedent together with the output of the baseline system, whereas the AUTO - POST EDI T system followed a simpler rule-based decision procedure. The UU - TIEDEMANN system (Tiedemann, 2015) was another phrase-based SMT system extending the official baseline. In contrast to the other submissions, it made no attempt to resolve pronominal anaphora explicitly. Instead, it used the Docent document-level decoder (Hardmeier et al., 2013a) with a cross-sentence n-gram model over determiners and pronouns to bias the SMT model towards selecting correct pronouns. The UU - HARDMEIER system (Hardmeier, 2015) was yet another phrase-based SMT using Docent, but built on a different baseline configuration. It included a neural network classifier for pronoun prediction trained with latent anaphor"
W15-2501,C96-2141,0,0.0650665,"for lowercasing. Then, we generated the following two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . Fo"
W15-2501,P00-1056,0,0.0244007,"reated automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and symmetrization performed best on the pronoun metric, followed by grow-diag and intersection, which also performed best for general alignments. Table 7 shows the results for different models with grow-diag-final-and symmetrization. We can see that, for all t"
W15-2501,W15-2516,0,0.0553089,"air, and (ii) the sums for each row/column; • accuracy; All six groups with system description papers used some form of machine learning. The main difference was whether or not they explicitly attempted to resolve pronominal coreference. Two systems relied on explicit anaphora resolution: UEDIN and MALTA. They both applied the Stanford coreference resolver (Lee et al., 2011) on the source language text, then projected the antecedents to the target language through the word alignments, and finally obtained morphological tags with the Morfette software (Chrupała et al., 2008). The UEDIN system (Wetzel et al., 2015) was built around a maximum entropy classifier. In addition to local context and antecedent information, it used the NADA tool (Bergsma and Yarowsky, 2011) to identify nonreferring pronouns and included predictions by a standard n-gram language model as a feature. The MALTA system (Pham and van der Plas, 2015) was based on a feed-forward neural network combined with word2vec continuous-space word embeddings (Mikolov et al., 2013). It used local context and antecedent information. • precision (P), recall (R), and F-score for each label; • micro-averaged P, R, F-score (note that in our setup, mi"
W15-2501,J03-1002,0,0.00752171,"ing two resources: (i) a bitext with target pronouns identified and their translations removed, and (ii) word alignments between the source and the target sentences in the bitext. Since the word alignments in the training and in the testing datasets were created automatically, without manual inspection, we performed a small study in order to investigate which alignment method performed best for pronouns. We followed the methodology in Stymne et al. (2014), by aligning English–French data using all IBM models (Brown et al., 1993) and the HMM model (Vogel et al., 1996) as implemented in GIZA++ (Och and Ney, 2003), as well as fast_align (Dyer et al., 2013), with a number of different symmetrization methods. IBM models 1, 2 and 3 yielded subpar results, so we will not discuss them. To evaluate the alignments, we used 484 goldaligned sentences from Och and Ney (2000).5 We used the F-score of correct sure and possible links (Fraser and Marcu, 2007) for a general evaluation, which we will call Fall .6 In order to specifically evaluate pronoun alignment, we used the F-score of the subset of links that align the two sets of pronouns we are interested in, Fpro . For all alignment models, grow-diag-final-and s"
W15-2501,P03-1021,0,0.00636797,"filter values and parameters. The phrase table was provided in raw and binary formats to make it easy to integrate it in other systems. For the language model, we used all monolingual datasets and the French parts of the parallel datasets and trained a 5-gram language model with modified Kneser-Ney smoothing using KenLM (Heafield et al., 2013). We provided the language model in ARPA format and in binary format using a trie data structure with quantization and pointer compression. The SMT model was tuned on the IWSLT 2010 development data and IWSLT 2011 test data using 200-best lists and MERT (Och, 2003). The resulting baseline system achieved reasonably good scores on the IWSLT 2010 and 2012 test datasets (Table 4). speaker Table 3: Statistics about the talks that were included in DiscoMT.tst2015. 4 4.1 Pronoun-Focused Translation Baseline System For comparison purposes and to lower the entry barrier for the participants, we provided a baseline system based on a phrase-based SMT model. The baseline system was trained on all parallel and monolingual datasets provided for the DiscoMT shared task, namely aligned TED talks from the WIT3 project (Cettolo et al., 2012), as well as Europarl version"
W15-2501,P02-1040,0,\N,Missing
W15-2501,W14-3302,0,\N,Missing
W16-2326,E14-1061,1,0.903294,"s. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alignment quality. Place-holder Prepositions: In contrast t"
W16-2326,W15-2501,1,0.829554,"ents (system (c) in Table 2). In combination with the surfaceoriented translation model this also leads to a slight improvement over the non-factored model (without back-translated news), which is also evi394 3.4 ilar to Tiedemann et al. (2015b), we introduce place-holder prepositions at the beginning of noun phrases bearing the corresponding case-marker in order to support word alignment. Gappy Language Models Tiedemann (2015) introduces the use of language models over selected words in the framework of document-level SMT using Docent applied to the pronoun-aware translation task of DiscoMT (Hardmeier et al., 2015). We extended this idea by developing a general framework for what we call gappy language models that refer to monolingual or bilingual n-gram language models over selected words and their alignments. We can use different factors attached to the source and target language tokens to filter for word sequences that we would like to consider. Given word alignments are used to establish the link between source and target tokens. Gappy language models may cross sentence-boundaries but may also stop at those borders. Regular expressions can be used to make the selection more flexible. Multi-word alig"
W16-2326,N12-1047,0,0.0925095,"ta sets. We use the English models for sentence boundary detection and tokenisation provided by OpenNLP,2 which is compatible with the Penn Treebank style of tokenisation. This is important for the subsequent tagging and parsing steps, which we trained on the Universal Dependencies treebank for English using MarMoT and mate-tools. MT Tools: Most of our systems are based on Moses (Koehn et al., 2007) and common components for training and tuning models. We apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner us"
W16-2326,N13-1138,0,0.0239033,"a to their heads and train the models on this representation. As we are using the words and lemmas as features for the CRFs, the reduction of compounds to their heads reduces data sparsity and allows the model to better generalise over all occurrences. For the translation output we remove all compound modifiers before case prediction. Morphological Generation The predicted casemarkers are then fed into the morphological generation automaton (Pirinen, 2015) in order to get fully inflected forms. In cases where this generation failed, we used a supervised machine learning approach as a backoff (Durrett and DeNero, 2013). Compound Processing In a final step, we merge compounds using a POS-matching strategy (Stymne et al., 2008). We merge the marked compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel r"
W16-2326,P13-2121,0,0.0411259,"Missing"
W16-2326,N13-1073,0,0.0338579,"e apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner using Gibbs sampling with a Bayesian extension of the IBM alignment models. It is both fast and accurate and works as a straightforward plug-in replacement for standard tools in the SMT training pipeline. The aligner is faster than fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do both fast align and GIZA++) is that"
W16-2326,W11-2123,0,0.0837537,"d compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel resources and all of the Finnish data available from WMT to train five-gram language models with SRILM (Stolcke, 2002) and KENLM (Heafield, 2011). No particular cleaning or preprocessing of the data has happened. This makes the re-inflection systems differ from all other systems in this paper. Otherwise, we trained a conventional phrase-based Moses system with default settings, tuned weights using batch-MIRA with ”safe-hope” (Cherry and Foster, 2012) and used an underspecified representation of the tuning reference set to derive BLEU scores. The final result of our system is listed in Table 4. • nouns and their alignments (sentence-internal only and even document-wide) • verbs and their alignments (sentence-internal only and even docum"
W16-2326,E12-1068,1,0.904335,"with the other systems. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alignment quality. Place-holder Preposit"
W16-2326,P13-4033,1,0.860325,"an fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do both fast align and GIZA++) is that inference remains quadratic with respect to sentence length even when word order and fertility models are added, which enables the efficient use of higher-order models. This is the first time that the performance of this tool is reported in the setting of statistical machine translation. Besides Moses, we also apply another phrasebased machine translation decoder, Docent (Hardmeier et al., 2013), which implements a stochastic local search decoder that is able to incorporate features with long-distance dependencies even across sentence boundaries. Docent emphasises document-level decoding but includes standard local features that make the decoder comparable with standard phrase-based SMT. The decoding algorithm applies randomly selected statechange operations to complete translation hypotheses (covering the whole document) that may be accepted by a strict hill-climbing procedure or a simulated annealing schedule. The main motivation for using Docent in our setup is to introduce non-lo"
W16-2326,W15-3021,1,0.892607,"Helsinki Fabienne Cap Uppsala University Jenna Kanerva and Filip Ginter University of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 mi"
W16-2326,L16-1147,1,0.802036,"y of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 million English tokens, but, nevertheless, it contributes to the overall collection esp"
W16-2326,tiedemann-2012-parallel,1,0.855367,"Missing"
W16-2326,W15-2124,1,0.739948,"a, Better Models and Alternative Alignment and Translation Tools J¨org Tiedemann University of Helsinki Fabienne Cap Uppsala University Jenna Kanerva and Filip Ginter University of Turku Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution"
W16-2326,W15-2515,1,0.854248,". Using this type of lexicalisation helps to find construction-like mappings between the two languages which seems to be beneficial for the system according to the scores in our experiments (system (c) in Table 2). In combination with the surfaceoriented translation model this also leads to a slight improvement over the non-factored model (without back-translated news), which is also evi394 3.4 ilar to Tiedemann et al. (2015b), we introduce place-holder prepositions at the beginning of noun phrases bearing the corresponding case-marker in order to support word alignment. Gappy Language Models Tiedemann (2015) introduces the use of language models over selected words in the framework of document-level SMT using Docent applied to the pronoun-aware translation task of DiscoMT (Hardmeier et al., 2015). We extended this idea by developing a general framework for what we call gappy language models that refer to monolingual or bilingual n-gram language models over selected words and their alignments. We can use different factors attached to the source and target language tokens to filter for word sequences that we would like to consider. Given word alignments are used to establish the link between source"
W16-2326,D13-1032,0,0.0777915,"Missing"
W16-2326,L16-1559,1,0.839226,"Sara Stymne Uppsala University ¨ Robert Ostling University of Helsinki Marion Di Marco University of Stuttgart Abstract strained systems apply all the data provided by WMT and also the English Giga-Word corpus that is distributed by the LDC. Our best systems include additional parallel data sets coming from OPUS (Tiedemann, 2012) and syntactically analysed monolingual data from the Finnish Internet Parsebank (Luotolahti et al., 2015). Additional to the parallel data we used in our submission last year (Tiedemann et al., 2015a), we include the new version of the OpenSubtitle corpus (Lison and Tiedemann, 2016) with its 18.6 million aligned translation units in English and Finnish. Furthermore, we make use of alternative subtitle translations that have been aligned monolingually in the same collection (Tiedemann, 2016). Expanding the parallel corpus with alternative translations extends the subtitle corpus by roughly 350,000 translation units with about 6.8 million tokens (counting both languages together). The contribution is quite small compared to the original corpus with its 107 million Finnish tokens and 167 million English tokens, but, nevertheless, it contributes to the overall collection esp"
W16-2326,J03-1002,0,0.0224522,"s for training and tuning models. We apply KenLM (Heafield et al., 2013) and SRILM (Stolcke, 2002) for estimating language model parameters and MERT (Och, 2003) and batch-MIRA (Cherry and Foster, 2012) for parameter tuning. Most of our models are based on lowercased training data. All language models use order five with modified Kneser-Ney smoothing if not stated otherwise. All MT systems apply the phrase-based paradigm, some of them with factored representations and generation models if necessary. For word alignment we experiment with different tools. We apply standard tools like GIZA++ 1 2 (Och and Ney, 2003) and fast align (Dyer et al., 2013) but also the recently proposed Bayesian ¨ word aligner efmaral (Ostling, 2015). Efmaral is an efficient implementation of a Markov-Chain aligner using Gibbs sampling with a Bayesian extension of the IBM alignment models. It is both fast and accurate and works as a straightforward plug-in replacement for standard tools in the SMT training pipeline. The aligner is faster than fast align but more accurate in terms of alignment error rate in various benchmark tests. The advantage of using Gibbs sampling rather than the Expectation-Maximisation algorithm (as do b"
W16-2326,P03-1021,0,0.0559308,"Missing"
W16-2326,P08-1059,0,0.0341143,"are, therefore, not directly comparable with the other systems. The underlying idea of what we call re-inflection models in our submission is that we reduce all Finnish training data to an underspecified representation, where words are reduced to their lemmas and noun and adjective compounds are split into their component parts. Then, we train models and translate from English into this underspecified representation of Finnish and in a post-processing step we then merge compounds and predict morphological features for Finnish. This approach has been successfully applied to Russian and Arabic (Toutanova et al., 2008) and to German (Fraser et al. (2012), Cap et al. (2014)). Note however, that for example Fraser et al. (2012) relied on German prepositions to predict case-markers on underspecified German SMT output. In contrast to many other languages, Finnish only has a limited number of stand-alone pre- and postpositions. Instead, the prepositional meaning is encoded by case-marking. We thus adapt an approach by Tiedemann et al. (2015b) and introduce place-holder prepositions in the Finnish training data, which are likely to correspond to the prepositions used on the English side and thus improve word alig"
W16-2326,W15-1844,0,0.0376448,"older Prepositions: In contrast to Tiedemann et al. (2015b), we do not apply factored models (with both, lemmatised and surface forms) here but strip the case-markers from those words and only keep the underspecified representation. Moreover, we apply the approach in the opposite translation direction, which requires a generation component. The place-holder prepositions will not only lead to improved word alignments, but we will also use them to predict case-markers after translation. Overall, we follow the processing pipeline of (Cap et al., 2014): we use a rule-based morphological analyser (Pirinen, 2015) to split compounds (using the Finnish parsing pipeline to disambiguate multiple analyses) and lemmatise all Finnish training data. Compound modifiers are reduced to their lemmas and marked with a symbol that distinguishes them from other words. SimBLEU 14.10 5.45 10.89 14.17 14.70 Table 2: Lower-cased BLEU scores for factored SMT models on development test data (newstest 2015). System (a) is the same as the constrained model in Table 1. System (b) uses a factored model that translates surface words to target lemmas and morphosyntactic features separately. System (c) keeps closed-class words i"
W16-2326,W15-1821,1,0.867757,"Missing"
W16-2326,W08-0317,1,0.79278,"the CRFs, the reduction of compounds to their heads reduces data sparsity and allows the model to better generalise over all occurrences. For the translation output we remove all compound modifiers before case prediction. Morphological Generation The predicted casemarkers are then fed into the morphological generation automaton (Pirinen, 2015) in order to get fully inflected forms. In cases where this generation failed, we used a supervised machine learning approach as a backoff (Durrett and DeNero, 2013). Compound Processing In a final step, we merge compounds using a POS-matching strategy (Stymne et al., 2008). We merge the marked compound modifiers with the following word if it is a noun or adjective, and add hyphens for modifiers in coordinated compounds. Compounding forms of modifiers are restored based on corpus frequencies. Like Stymne et al. (2008) and Cap et al. (2014), we also merge compounds in every iteration of the tuning process before the translations are scored against the reference. All re-inflection systems are constrained systems. We used Europarl and Wikipedia as parallel resources and all of the Finnish data available from WMT to train five-gram language models with SRILM (Stolck"
W16-2326,P07-2045,0,\N,Missing
W16-2345,D12-1133,0,0.253709,"set of the provided training data that has well-defined document boundaries in order to allow for meaningful extraction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddi"
W16-2345,W16-2348,0,0.0249438,"urce word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER class. The difference between the primary and contrastive systems is small. In the primary system, the feature val"
W16-2345,P06-1005,0,0.150956,"res based on the target-language model estimates provided by the baseline system, linguistic features concerning the source word aligned to the target pronoun, and approximations of the coreference and dependency relations in the target language. Following the submission of the CUNI systems for English–German, an error was discovered in the merging of the classifier output into the test data file for submission. Fixing it yielded an improvement, with the contrastive system achieving recall of 51.74, and 54.37 for the primary system. Except for the English wordlist with gender distributions by Bergsma and Lin (2006), only the shared task data was used in the CUNI systems. 5.2 IDIAP 5.3 LIMSI The LIMSI systems (Bawden, 2016) for the English–French task are linguistically-driven statistical classification systems. The systems use random forests, with few, high-level features, relying on explicit coreference resolution and external linguistic resources and syntactic dependencies. The systems include several types of contextual features, including a single feature using context templates to target particularly discriminative contexts for the prediction of certain pronoun classes, in particular the OTHER clas"
W16-2345,2012.eamt-1.60,1,0.85967,"predicting all of the other pronouns, the system relied solely on the scores coming from the proposed PLM model. This target-side PLM model uses a large target-language training dataset to learn a probabilistic relation between each target pronoun and the distribution of the gender-number of its preceding nouns and pronouns. For prediction, given each source pronoun “it” or “they”, the system uses the PLM to score all possible candidates and to select the one with the highest score. In addition to the PoS-tagged lemmatised data that was provided for the shared task, the WIT3 parallel corpus (Cettolo et al., 2012), provided as part of the training data at the DiscoMT 2015 workshop, was used to train the PLM model. Furthermore, a French PoS-tagger, Morfette (Chrupala et al., 2008), was employed for gendernumber extraction. Before extracting the examples as feature vectors, the data is linguistically preprocessed usˇ ing the Treex framework (Popel and Zabokrtsk´ y, 2010). The source-language texts undergo a thorough analysis and are enriched with PoS tags, dependency syntax, as well as semantic roles and coreference for English. On the other hand, only grammatical genders are assigned to nouns in the tar"
W16-2345,chrupala-etal-2008-learning,0,0.0898214,"Missing"
W16-2345,W16-2350,1,0.838182,"on. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trai"
W16-2345,W11-2123,0,0.0192239,"e classifier is trained on a combination of semantic, based on lexical resources such as VerbNet (Schuler, 2005) and WordNet (Miller, 1995), and frequencies computed over the annotated Gigaword corpus (Napoles et al., 2012), syntactic, from the dependency parser in the Mate tools (Bohnet et al., 2013), and contextual features. The event classification results are modest, reaching only 54.2 F-score for the event class. The translation model, into which the classifier is integrated, is a 6-gram language model computed over target lemmata using modified KneserNey smoothing and the KenLM toolkit (Heafield, 2011). In addition to the pure target lemma context, it also has access to the identity of the sourcelanguage pronoun, used as a concatenated label to each REPLACE item. This provides information about the number marking of the pronouns in the source, and also allows for the incorporation of the output of the ‘it’-label classifier. To predict classes for an unseen test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels p"
W16-2345,W16-2349,0,0.0373816,"sing the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun translation decisions. The model performs reasonably well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features"
W16-2345,W10-1737,0,0.434398,"Missing"
W16-2345,guillou-etal-2014-parcor,1,0.910739,"the fact that all talks are originally given in English, which means that French–English translation is in reality a back-translation. • she: feminine singular subject pronoun; 3 1 We explain below in Section 3.3.3 how non-subject pronouns are filtered out from the data. 528 TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. As shown in analysis presented by Guillou et al. (2014), TED talks differ from other text types with respect to pronoun usage. TED speakers frequently use first- and second-person pronouns (singular and plural): first-person to refer to themselves and their colleagues or to themselves and the audience, second-person to refer to the audience, the larger set of viewers, or people in general. TED speakers often use the pronoun “they” without a specific textual antecedent, in sentences such as “This is what they think.” They also use deictic and third-person pronouns to refer to things in the spatio-temporal context shared by the speaker and the audie"
W16-2345,W16-2351,1,0.900928,"Missing"
W16-2345,E12-3001,1,0.880326,"it is required by syntax to fill the subject position. An event reference pronoun may refer to a verb phrase (VP), a clause, an entire sentence, or a longer passage of text. Examples of each of these pronoun functions are provided in Figure 1. It is clear that instances of the English pronoun “it” belonging to each of these functions would have different translation requirements in French and German. Introduction Pronoun translation poses a problem for current state-of-the-art Statistical Machine Translation (SMT) systems (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). 525 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 525–542, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 2 The problem of pronouns in machine translation has long been studied. In particular, for SMT systems, the recent previous studies cited above have focused on the translation of anaphoric pronouns. In this case, a well-known constraint of languages with grammatical gender is that agreement must hold between an anaphoric pronoun and the NP with which it corefers, called its antecede"
W16-2345,W16-2352,1,0.881771,"Missing"
W16-2345,W16-2353,0,0.0435664,"s useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed sequences of these embeddings within a certain window to the left and to the right of the target pronoun. The window size used by the system is 50 tokens or until the end of the sentence boundary. All of these inputs are read"
W16-2345,2010.iwslt-papers.10,1,0.888921,"Missing"
W16-2345,D13-1037,1,0.883273,"3 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the preprocessing part of a coreference resolution system. Anaphora resolution is treated as a latent variable by the model. This system is combined by linear interpolation with a specially trained 6gram language model identical to the contrastive system of the UUPPSALA submission described above. The"
W16-2345,S16-1001,1,0.795211,"d 69.76 in macro-averaged recall. This is very much above the performance of baseline0 and baseline-1.5, which are in the low-mid 40s. It is also well above the majority/random baseline (not shown) at 11.11, which is outperformed by far by all systems. Note that the top-3 systems in terms of macro-averaged recall are also the top-3 in terms of accuracy, but in different order. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we adopted macro-averaged recall, which was also recently adopted by some other competitions, e.g., by SemEval-2016 Task 4 (Nakov et al., 2016). Moreover, as in 2015, we also report accuracy as a secondary evaluation measure. Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. 8 If the test data did not have any instances of some of the classes, we excluded these classes from the macro-a"
W16-2345,W15-2501,1,0.657407,"ould replace a placeholder value (represented by the token REPLACE) in the target-language text. It requires no specific Machine Translation (MT) expertise and is interesting as a machine learning task in its own right. Within the context of SMT, one could think of the task of cross-lingual pronoun prediction as a component of an SMT system. This component may take the form of a decoder feature or it may be used to provide “corrected” pronoun translations in a post-editing scenario. The design of the WMT 2016 shared task has been influenced by the design and the results of a 2015 shared task (Hardmeier et al., 2015) organised at the EMNLP workshop on Discourse in MT (DiscoMT). The first intuition about evaluating pronoun translation is to require participants to submit MT systems — possibly with specific strategies for pronoun translation — and to estimate the correctness of the pronouns they output. This estimation, however, cannot be performed with full reliability only by comparing pronouns across candidate and reference translations because this would miss the legitimate variation of certain pronouns, as well as variations in gender or number of the antecedent itself. Human judges are thus required f"
W16-2345,W16-2354,0,0.0469939,"Missing"
W16-2345,H05-1108,0,0.0601982,"Missing"
W16-2345,W14-3334,1,0.800608,"he OTHER class. For the DiscoMT 2015 shared task, we explored this issue for English–French and found that GIZA++ model 4 and HMM with grow-diag-final-and symmetrisation gave the best results. For pronoun– pronoun links, we had an F-score of 0.96, with perfect recall and precision of 0.93 (Hardmeier et al., 2015). This was slightly higher than for other links, which had an F-score of 0.92. For German–English, we explored this issue this year since it is a new language pair. We used an aligned gold standard of 987 sentences from (Pad´o and Lapata, 2005), which has been extensively evaluated by Stymne et al. (2014). We used the same methodology as in 2015, and performed an evaluation on the subset of links between the pronouns we are interested in. We report precision and recall of links both for the pronoun subset and for all links, shown in Table 4. The alignment quality is considerably worse than for French–English both for all links and for pronouns, but again the results for pronouns is better than for all links in both precision and recall. 6 https://github.com/slavpetrov/ universal-pos-tags 530 Alignment Symmetrisation Model 4 fast-align gdfa HMM gd gdf ∪ ∩ All links P R Pronouns P R .75 .69 .80"
W16-2345,W16-2355,1,0.832701,"the test dataset is imbalanced. Thus, one cannot interpret the absolute value of accuracy (e.g., is 0.7 a good or a bad value?) without comparing it to a baseline that must be computed for each specific test dataset. In contrast, for macro-averaged recall, it is clear that a value of, e.g., 0.7, is well above the majority-class and the random baselines, which are both always 1/C (e.g., 0.5 with two classes, 0.33 with three classes, etc.). Standard F1 and macro-averaged F1 are also sensitive to class imbalance for the same reason; see Sebastiani (2015) for more detail. The UU-S TYMNE systems (Stymne, 2016) use linear SVM classifiers for all language pairs. A number of different features were explored, but anaphora is not explicitly modelled. The features used can be grouped in the following way: source pronouns, local context words/lemmata, preceding nouns, target PoS n-grams with two different PoS tag-sets, dependency heads of pronouns, target LM scores, alignments, and pronoun position. A joint tagger and dependency parser on the source text is used for some of the features. The primary system is a 2-step classifier where a binary classifier is first used to distinguish between the OTHER clas"
W16-2345,petrov-etal-2012-universal,0,0.0937891,"Missing"
W16-2345,W16-2356,1,0.48149,"networks, except for the embedding for the aligned pronoun. All outputs of the recurrent layers are concatenated to a single vector along with the embedding of the aligned pronoun. This vector is then used to make the pronoun prediction by a dense neural network layer. The primary systems are trained to optimise macro-averaged recall and the contrastive systems are optimised without preference towards rare classes. The system is trained only on the shared task data and all parts of the data, in-domain and out-of-domain, are used for training the system. 5.5 5.6 UHELSINKI The UHELSINKI system (Tiedemann, 2016) implements a simple linear classifier based on LibSVM with its L2-loss SVC dual solver. The system applies local source-language and target-language context using the given tokens and PoS labels as features. Coreference resolution is not used, but additional selected items in the prior context are extracted to enrich the model. In particular, a small number of the nearest determiners, nouns and proper nouns are taken as possible antecedent candidates. The contribution of these features is limited even with the lemmatised target-language context that makes it harder to disambiguate pronoun tra"
W16-2345,W16-2357,0,0.0259257,"well especially for the prediction of pronoun translations into English. 5.7 UEDIN UKYOTO The UKYOTO system (Dabre et al., 2016) is a simple Recurrent Neural Network system with an attention mechanism which encodes both the source sentence and the context of the pronoun to be predicted and then predicts the pronoun. The interesting thing about the approach is that it uses a simple language-independent Neural Network (NN) mechanism that performs well in almost all cases. Another interesting aspect is that good performance is achieved, even though only the IWSLT data is used. The UEDIN systems (Wetzel, 2016) for English– French and English–German are Maximum Entropy (MaxEnt) classifiers with the following set of features: tokens and their PoS tags are extracted from a context window around source- and targetside pronouns. N -gram combinations of these features are included by concatenating adjacent tokens or PoS tags. Furthermore, the pleonastic use of a pronoun is detected with NADA (Bergsma and Yarowsky, 2011) on the source side. 534 This CRF approach has been applied only to German, but there are plans to extend it to other languages. This indicates that the NN mechanism is quite effective. Th"
W16-2345,sagot-2010-lefff,0,0.0184156,"traction of coreference chains. The MaxEnt classifiers consistently outperform the CRF models. Feature ablation shows that the antecedent feature is useful for English–German, and predicting NULL-translations is useful for English–French. It also reveals that the LM feature hurts performance. A number of tools and resources are used in the LIMSI system. Stanford CoreNLP is used for PoS tagging, syntactic dependencies, and coreference resolution over the English text. The Mate Parser (Bohnet and Nivre, 2012), retrained on SPMRL 2014 data (Seddah et al., 2014) (dependency trees), and the Lefff (Sagot, 2010), a morphological and syntactic lexicon (used for information on noun gender and impersonal adjectives and verbs), are both used for French. 5.4 TurkuNLP The architecture for the T URKU NLP system (Luotolahti et al., 2016) is based on token-level sequence classification around the target pronoun using stacked recurrent neural networks. The system learns token-level embeddings for the source-language lemmata, target-language tokens, PoS tags, combination of words and PoS tags and separate embeddings for the sourcelanguage pronouns that are aligned with the target pronoun. The network is fed seq"
W16-2345,schmid-etal-2004-smor,0,0.0349386,"test set, a uniform unannotated RE PLACE tag is used for all classes. The ‘disambig’ tool of the SRILM toolkit (Stolcke, 2002) is then used to recover the tag annotated with the correct solution. The combined system with the ‘it’-labels performed slightly worse than the system without it (57.03 vs. 59.84 macro-averaged recall). The same underlying translation model forms the contrastive system for English–French, and the primary system for all other subtasks. 5.9 The CRF model was trained on the IWSLT15 corpus and used the TED talks for development. The rule-based morphological Analyser SMOR (Schmid et al., 2004) as well as its English spinoff EMOR (not published) were used to derive the gender and number of the German and English words. 5.10 UU-Hardmeier The UU-H ARDMEIER system (Hardmeier, 2016) is a system combination of two different models. One of them, based on earlier work (Hardmeier et al., 2013), is a feed-forward neural network that takes as input the source pronoun and the source context words, target lemmata and target PoS tags in a window of 3 words to the left and to the right of the pronoun. In addition, the network receives a list of potential antecedent candidates identified by the pr"
W16-2345,W12-3018,0,\N,Missing
W16-2345,2015.iwslt-evaluation.1,1,\N,Missing
W16-2345,W14-6111,0,\N,Missing
W16-2355,W15-2514,0,0.0361284,"Missing"
W16-2355,D12-1133,0,0.106464,"f It, which in this case should be er since it refers to the masculine word Saal (room) in the previous sentence. Had the antecedent instead been the neuter Zimmer, the correct pronoun would have been es. The target words are lemmatized with coarse POS-tags, to better mimic the SMT task, in contrast to previous versions of this task where full forms were used. For full details of the task and training data, see the task overview paper (Guillou et al., 2016). For some of our features we needed dependency trees and POS-tags for the source. We used Mate Tools to jointly tag and dependency parse (Bohnet and Nivre, 2012) the source text for sentences that contained pronoun examples. For all languages the output is a dependency parse tree and POStags, and for German and French it also gives morphological descriptions. For the target side we used the given POS-tags. (1) It ’s smaller than this . REPLACE 0 sein|VERB klein|ADJ als|CONJ dies|PRON hier|ADV .|. 609 Proceedings of the First Conference on Machine Translation, Volume 2: Shared Task Papers, pages 609–615, c Berlin, Germany, August 11-12, 2016. 2016 Association for Computational Linguistics 3 2-step classification tried both to use bag-of-words models fo"
W16-2355,W15-2515,0,0.275939,"large number of features for cross-lingual pronoun prediction for translation between English and German/French. We find that features related to German/French are more informative than features related to English, regardless of the translation direction. Our most useful features are local context, dependency head features, and source pronouns. We also find that it is sometimes more successful to employ a 2-step procedure that first makes a binary choice between pronouns and other, then classifies pronouns. For the pronoun/other distinction POS ngrams were very useful. 1 2 System We followed Tiedemann (2015) by using linear SVMs implemented in LIBLINEAR (Fan et al., 2008). In all experiments we use L2-loss support vector classification with dual solvers and the 1vs-rest strategy for multi-class classification. The regularization parameter C was optimized using grid search and cross-validation as implemented in LIBLINEAR. The results were quite stable for reasonable values of C, however, and in all cases we used values between 2−2 and 2−5 . Introduction In most of our experiments we only used IWSLT training data, with 66K–92K pronoun examples, to train our classifier, since it contains TED talks l"
W16-2355,W15-2516,0,0.237788,"e nouns, so we only included up to four previous nouns within the sentence, which meant that we often had 0 or just a few nouns on the source side. Since the source contains full forms, we also included some morphological information for these nouns, we added a feature for each POS-tag extended with morphology for number, and gender for proper names. Finally we added a feature indicating how many previous nouns there were in the sentence. Preceding nouns or NPs have previously been used for this task with differing results. Callin et al. (2015) used up to four preceding nouns and determiners. Wetzel et al. (2015) also used preceding noun tokens, however they were identified by co-reference resolution. A difference from 2015 is that this year there are no determiners or other words on the target side that carries information such as gender, since it is lemmatized. Features We explored a high number of features of different types, which will be described in this section. We did not explicitly attempt to model anaphora in any way, but tried to identify other types of features that could give indications of which pronoun translation to use. The main reasons why we decided not to use any anaphora software"
W16-2355,W15-2508,0,0.0835723,"Because of this, we did not include cross sentence instances of source nouns, so we only included up to four previous nouns within the sentence, which meant that we often had 0 or just a few nouns on the source side. Since the source contains full forms, we also included some morphological information for these nouns, we added a feature for each POS-tag extended with morphology for number, and gender for proper names. Finally we added a feature indicating how many previous nouns there were in the sentence. Preceding nouns or NPs have previously been used for this task with differing results. Callin et al. (2015) used up to four preceding nouns and determiners. Wetzel et al. (2015) also used preceding noun tokens, however they were identified by co-reference resolution. A difference from 2015 is that this year there are no determiners or other words on the target side that carries information such as gender, since it is lemmatized. Features We explored a high number of features of different types, which will be described in this section. We did not explicitly attempt to model anaphora in any way, but tried to identify other types of features that could give indications of which pronoun translation to"
W16-2355,D13-1037,0,0.119089,"b for words before the pronoun and a for words after, we included the following n-gram windows: 3b, 1b, 2b+2a, 1b+1a, 1a, 3a. POS-tags were used in several 2015 systems (Callin et al., 2015; Lo´aiciga, 2015; Wetzel et al., 2015), with either positive results or no separate results shown in the paper. They all used single tags, though, not POS n-grams. Source pronoun (SP) The source pronoun to be translated was added as a feature. We believe that this is an important feature since it restricts the possible translations. Source pronouns has been used before for cross-lingual pronoun prediction (Hardmeier et al., 2013; Wetzel et al., 2015). Local context (LCS, LCT) For these features we considered the source words surrounding the source pronoun and the lemmas+POS-tags surrounding the target pronouns. We included up to 3 words before and 3 words after the pronouns. We Target extended POS n-grams (EPOS) The tag sets in the data are coarse-grained, with the 12 universal POS-tags (Petrov et al., 2012) for En610 Null penalty 0 −2 glish and German, and a set of 15 POS-tags for French. To compensate somewhat for this, we also included n-grams using an extended tag set where we use the identity of the 100 most com"
W16-2355,W15-2501,1,0.781305,"ccessfully used fine-grained morphological target tags last year, for instance Pham and van der Plas (2015), which was not possible this year, given the lemma+POS representation. de-en .361 .389 fr-en .337 .388 en-de .344 .358 en-fr .406 .411 Table 1: Macro-R for workshop baseline. word, NONE. The language model we used was also provided for the shared task, a large 5-gram model trained using KenLM (Heafield, 2011) on the workshop data and monolingual News data (Guillou et al., 2016). There is a penalty for the NONE case, which we set to −2, which was the best value from the 2015 shared task (Hardmeier et al., 2015), and that we found to give good results for all language pairs, as shown in Table 1. Note that this LM used lemmatized data, which gave a much worse performance than the full form LM from 2015, which had .584 MACRO-F (Hardmeier et al., 2015), compared to .342 on lemmas. The baseline system can output marginal probabilities for each pronoun or alternative word and NONE, giving all options larger than 0.001. We used these probabilities as feature values for each word. In addition we had features giving the highest scoring word, always and if it had a probability over 0.85; the highest probabili"
W16-2355,W11-2123,0,0.0177336,"ions for VERB-VERB-DET. We use the same n-gram windows as for standard POS n-grams. As far as we know, no one has used this particular extension of POS-tags for this task. However, several teams successfully used fine-grained morphological target tags last year, for instance Pham and van der Plas (2015), which was not possible this year, given the lemma+POS representation. de-en .361 .389 fr-en .337 .388 en-de .344 .358 en-fr .406 .411 Table 1: Macro-R for workshop baseline. word, NONE. The language model we used was also provided for the shared task, a large 5-gram model trained using KenLM (Heafield, 2011) on the workshop data and monolingual News data (Guillou et al., 2016). There is a penalty for the NONE case, which we set to −2, which was the best value from the 2015 shared task (Hardmeier et al., 2015), and that we found to give good results for all language pairs, as shown in Table 1. Note that this LM used lemmatized data, which gave a much worse performance than the full form LM from 2015, which had .584 MACRO-F (Hardmeier et al., 2015), compared to .342 on lemmas. The baseline system can output marginal probabilities for each pronoun or alternative word and NONE, giving all options lar"
W16-2355,W15-2511,0,0.215264,"Missing"
W16-2355,petrov-etal-2012-universal,0,0.0283908,"oun to be translated was added as a feature. We believe that this is an important feature since it restricts the possible translations. Source pronouns has been used before for cross-lingual pronoun prediction (Hardmeier et al., 2013; Wetzel et al., 2015). Local context (LCS, LCT) For these features we considered the source words surrounding the source pronoun and the lemmas+POS-tags surrounding the target pronouns. We included up to 3 words before and 3 words after the pronouns. We Target extended POS n-grams (EPOS) The tag sets in the data are coarse-grained, with the 12 universal POS-tags (Petrov et al., 2012) for En610 Null penalty 0 −2 glish and German, and a set of 15 POS-tags for French. To compensate somewhat for this, we also included n-grams using an extended tag set where we use the identity of the 100 most common lemmas in the training data in addition to the POS tags. As an example, be-VERB-all and can-makethe are two EPOS options for VERB-VERB-DET. We use the same n-gram windows as for standard POS n-grams. As far as we know, no one has used this particular extension of POS-tags for this task. However, several teams successfully used fine-grained morphological target tags last year, for"
W16-2355,W16-2345,1,\N,Missing
W16-2390,P11-1103,0,0.0252865,"roblematic. It is common for English • Crossing score: the number of crossings in alignments between source and target 1 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 • Kendall Tau distance between alignments in source and target 826 Structural integrity We also investigated features measuring well-formedness as conveyed by syntactic parse trees in line with Avramidis (2012) as well as POS language models • Squared Kendall Tau distance between alignments in source and target Crossing score was suggested by Genzel (2010) for SMT reordering and Tau was suggested by Birch and Osborne (2011) for use in a standard metric with a reference translation. To our knowledge we are the first to use these measures for quality estimation. The features are computed over the crossing link pairs in a word alignment file, where the number of crossing links considers crossings of all lengths and the Squared Kendall Tau Distance (SKTD) is defined as shown in Eq. 1. SKTD = 1 − s |crossing link pairs| |link pairs| • Source PCFG average confidence of all possible parses in the parser n-best list • Target PCFG average confidence of all possible parses in the parser n-best list • Source PCFG log proba"
W16-2390,C04-1046,0,0.10799,"es. We participated in task 1, which focuses on sentence-level QE. Most modern approaches set the task as a regression problem - attempting to accurately predict a continuous quality label through representing translations with feature vectors. The performance of such approaches rely on determining and extracting features that correlate strongly with the proposed quality label and the impact of a wide variety of features. Different types of systems, including system-dependent (glass-box) or system-independent (black-box), linguistically or statistically motivated features, have been explored (Blatz et al., 2004; Quirk, 2004; Specia et al., 2009). The quality label proposed for the sentence-level task is Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), which Features and resources In this section we will describe the dataset we used and the baseline system. We also give a detailed description of our suggested features. 2.1 Dataset The dataset for task 1 spans a total of 15,000 English-German translations from the IT domain. Each entry consists of a source segment, its machine translation, a post-edition of the translation and an edit distance score (HTER) derived from the post-edite"
W16-2390,N13-1073,0,0.0291788,"/der. English: Any noun or proper noun preceeded by a noun and the possesive clitic ’s or the possessive preposition of. Note that these patterns could also match other constructions since “of” can have other uses and “der” is also used for masculine nominative and feminine dative. Proposed features In addition to the provided resources, further tools were used to extract the features: A modified version of the QuEst++ framework, (Specia et al., 2015) with processors and features added and modified where needed, used to extract the baseline features and a majority of our features. Fast align (Dyer et al., 2013) was used to generate word alignment files. We used Berkeley Parser (Petrov et al., 2006), trained with the included grammars for English and German, to extract phrase structure-based features. We also used SRILM (Stolcke, 2002) to train a Part-Of-Speech (POS) Language Model over the training dataset as well as to compute all LM-based segment probabilities and perplexities. Lastly, we used TreeTagger (Schmid, 1994) trained with the included models for English and German to obtain all POSrelated features. We aimed to obtain consistent features capturing sources of and results of difficulties fo"
W16-2390,W12-3110,0,0.0185933,"rage confidence of all possible parses in the parser n-best list • Source PCFG log probability • Target PCFG log probability • LM log perplexity of POS of the target • LM log probability of POS of the target (1) We experimented with different sizes of n-best lists and found that small sizes (1-3) were preferred due to difficulties in coming up with more parse trees for several of the input sentences. Grammatical correspondence We explored several features quantifying grammatical discrepancy, mainly measured in terms of occurences of syntactic phrases or POS tags in accordance with the work of Felice and Specia (2012). 2.4 Learning As per the baseline system methodology, we use SVM regression (Chang and Lin, 2011) with a Radial Basis Function (RBF) kernel and a grid search algorithm for parameter optimisation, implemented in QuEst++. • Ratio of percentage of verb phrases between source and target 3 • Ratio of percentage of noun phrases between source and target Experiments Initial experiments consisted of concatenating features with the baseline set, in order to sort out the features that had a positive impact on performance and disregard the ones that had a negative impact. As per the QuEst++ framework, p"
W16-2390,C10-1043,0,0.0294397,"we have noted that the translation of noun compounds is problematic. It is common for English • Crossing score: the number of crossings in alignments between source and target 1 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 • Kendall Tau distance between alignments in source and target 826 Structural integrity We also investigated features measuring well-formedness as conveyed by syntactic parse trees in line with Avramidis (2012) as well as POS language models • Squared Kendall Tau distance between alignments in source and target Crossing score was suggested by Genzel (2010) for SMT reordering and Tau was suggested by Birch and Osborne (2011) for use in a standard metric with a reference translation. To our knowledge we are the first to use these measures for quality estimation. The features are computed over the crossing link pairs in a word alignment file, where the number of crossing links considers crossings of all lengths and the Squared Kendall Tau Distance (SKTD) is defined as shown in Eq. 1. SKTD = 1 − s |crossing link pairs| |link pairs| • Source PCFG average confidence of all possible parses in the parser n-best list • Target PCFG average confidence of"
W16-2390,P06-1055,0,0.0213051,"or the possessive preposition of. Note that these patterns could also match other constructions since “of” can have other uses and “der” is also used for masculine nominative and feminine dative. Proposed features In addition to the provided resources, further tools were used to extract the features: A modified version of the QuEst++ framework, (Specia et al., 2015) with processors and features added and modified where needed, used to extract the baseline features and a majority of our features. Fast align (Dyer et al., 2013) was used to generate word alignment files. We used Berkeley Parser (Petrov et al., 2006), trained with the included grammars for English and German, to extract phrase structure-based features. We also used SRILM (Stolcke, 2002) to train a Part-Of-Speech (POS) Language Model over the training dataset as well as to compute all LM-based segment probabilities and perplexities. Lastly, we used TreeTagger (Schmid, 1994) trained with the included models for English and German to obtain all POSrelated features. We aimed to obtain consistent features capturing sources of and results of difficulties for SMT systems by quantifying noun translation errors, reordering measures, grammatical co"
W16-2390,quirk-2004-training,0,0.0601508,"in task 1, which focuses on sentence-level QE. Most modern approaches set the task as a regression problem - attempting to accurately predict a continuous quality label through representing translations with feature vectors. The performance of such approaches rely on determining and extracting features that correlate strongly with the proposed quality label and the impact of a wide variety of features. Different types of systems, including system-dependent (glass-box) or system-independent (black-box), linguistically or statistically motivated features, have been explored (Blatz et al., 2004; Quirk, 2004; Specia et al., 2009). The quality label proposed for the sentence-level task is Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), which Features and resources In this section we will describe the dataset we used and the baseline system. We also give a detailed description of our suggested features. 2.1 Dataset The dataset for task 1 spans a total of 15,000 English-German translations from the IT domain. Each entry consists of a source segment, its machine translation, a post-edition of the translation and an edit distance score (HTER) derived from the post-edited version. Th"
W16-2390,2006.amta-papers.25,0,0.126726,"ict a continuous quality label through representing translations with feature vectors. The performance of such approaches rely on determining and extracting features that correlate strongly with the proposed quality label and the impact of a wide variety of features. Different types of systems, including system-dependent (glass-box) or system-independent (black-box), linguistically or statistically motivated features, have been explored (Blatz et al., 2004; Quirk, 2004; Specia et al., 2009). The quality label proposed for the sentence-level task is Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), which Features and resources In this section we will describe the dataset we used and the baseline system. We also give a detailed description of our suggested features. 2.1 Dataset The dataset for task 1 spans a total of 15,000 English-German translations from the IT domain. Each entry consists of a source segment, its machine translation, a post-edition of the translation and an edit distance score (HTER) derived from the post-edited version. The dataset was split into 12,000 segments as training data, 1,000 for development and 2,000 for testing. The translations were produced by a single"
W16-2390,2009.eamt-1.5,0,0.0178316,"ich focuses on sentence-level QE. Most modern approaches set the task as a regression problem - attempting to accurately predict a continuous quality label through representing translations with feature vectors. The performance of such approaches rely on determining and extracting features that correlate strongly with the proposed quality label and the impact of a wide variety of features. Different types of systems, including system-dependent (glass-box) or system-independent (black-box), linguistically or statistically motivated features, have been explored (Blatz et al., 2004; Quirk, 2004; Specia et al., 2009). The quality label proposed for the sentence-level task is Human-targeted Translation Edit Rate (HTER) (Snover et al., 2006), which Features and resources In this section we will describe the dataset we used and the baseline system. We also give a detailed description of our suggested features. 2.1 Dataset The dataset for task 1 spans a total of 15,000 English-German translations from the IT domain. Each entry consists of a source segment, its machine translation, a post-edition of the translation and an edit distance score (HTER) derived from the post-edited version. The dataset was split in"
W16-2390,P15-4020,0,0.0840421,"target, and is computed as the ratio of genitive constructions, defined as follows: German: Any noun or proper noun preceeded by a noun and the genitive article des/der. English: Any noun or proper noun preceeded by a noun and the possesive clitic ’s or the possessive preposition of. Note that these patterns could also match other constructions since “of” can have other uses and “der” is also used for masculine nominative and feminine dative. Proposed features In addition to the provided resources, further tools were used to extract the features: A modified version of the QuEst++ framework, (Specia et al., 2015) with processors and features added and modified where needed, used to extract the baseline features and a majority of our features. Fast align (Dyer et al., 2013) was used to generate word alignment files. We used Berkeley Parser (Petrov et al., 2006), trained with the included grammars for English and German, to extract phrase structure-based features. We also used SRILM (Stolcke, 2002) to train a Part-Of-Speech (POS) Language Model over the training dataset as well as to compute all LM-based segment probabilities and perplexities. Lastly, we used TreeTagger (Schmid, 1994) trained with the i"
W16-2390,J13-4009,1,0.839156,"correspondence and structural integrity. The following features were considered and tested for inclusion in the feature set for the submission: Reordering measures Reordering is problematic for MT in general, and for English–German especially for the placement of verbs, which differ between these languages. We explored three metrics that measure the amount of reordering done by the MT system, to investigate a correlation between SMT reordering and edit operations. All metrics are based on alignments between individual words. Noun Translation Errors In our previous work on English–German SMT (Stymne et al., 2013), we have noted that the translation of noun compounds is problematic. It is common for English • Crossing score: the number of crossings in alignments between source and target 1 http://www.quest.dcs.shef.ac.uk/ quest_files/features_blackbox_baseline_ 17 • Kendall Tau distance between alignments in source and target 826 Structural integrity We also investigated features measuring well-formedness as conveyed by syntactic parse trees in line with Avramidis (2012) as well as POS language models • Squared Kendall Tau distance between alignments in source and target Crossing score was suggested by"
W16-2390,W15-3001,0,\N,Missing
W17-0230,2009.mtsummit-papers.9,0,0.875846,"Missing"
W17-0230,D11-1034,0,0.756985,"Missing"
W17-0230,P11-2031,0,0.0319194,"LM (Stolcke, 2002) was used to train a 7-gram POS LM. Tagging was performed using Tree Tagger (Schmid, 1994). For training we used Europarl and News commentary, provided by WMT, with a total of over 2M segments for German and French and .77M for Czech. For English→German we used additional data: bilingual Common Crawl (1.5M) and monolingual News (83M). For tuning we used MERT (Och, 2003) as implemented in Moses, optimized towards the Bleu metric (Papineni et al., 2002). For each tuning condition we ran tuning three times and show the mean result, in order to account for optimizer instability (Clark et al., 2011). For the manual analysis we use the system with the median Bleu score. 2.3 Evaluation In much of the work on translationese, with the exception of Lembersky (2013), only Bleu (Papineni et al., 2002) has been used for evaluation. Bleu has its limitations though, and to give a somewhat more thorough evaluation we also show results on Meteor (Denkowski and Lavie, 2010) and TER (Snover et al., 2006). These metrics capture somewhat different aspects of MT quality. Bleu is mainly based on the precision of n-grams up to length 4, and thus rewards local fluency highly. Meteor is based on a weighted F"
W17-0230,E12-1026,0,0.126184,"Missing"
W17-0230,W10-1751,0,0.0339849,"ing we used MERT (Och, 2003) as implemented in Moses, optimized towards the Bleu metric (Papineni et al., 2002). For each tuning condition we ran tuning three times and show the mean result, in order to account for optimizer instability (Clark et al., 2011). For the manual analysis we use the system with the median Bleu score. 2.3 Evaluation In much of the work on translationese, with the exception of Lembersky (2013), only Bleu (Papineni et al., 2002) has been used for evaluation. Bleu has its limitations though, and to give a somewhat more thorough evaluation we also show results on Meteor (Denkowski and Lavie, 2010) and TER (Snover et al., 2006). These metrics capture somewhat different aspects of MT quality. Bleu is mainly based on the precision of n-grams up to length 4, and thus rewards local fluency highly. Meteor is based on a weighted F-score on unigrams, with a matching step that consider word forms, stems, synonyms (for English), and paraphrases with different weights for content and function words, and a fragmentation score. It is thus less sensitive than Bleu to allowable linguistic variation. Meteor is also tuned for different target languages, to increase correlation with human 1 Until 2013 t"
W17-0230,P03-1021,0,0.029484,"systems. For German↔English we use word and POS-tag factors (Koehn and Hoang, 2007) and have LMs for both; for the other language pairs we only use words. KenLM (Heafield, 2011) was used to train a 5-gram word LM and SRILM (Stolcke, 2002) was used to train a 7-gram POS LM. Tagging was performed using Tree Tagger (Schmid, 1994). For training we used Europarl and News commentary, provided by WMT, with a total of over 2M segments for German and French and .77M for Czech. For English→German we used additional data: bilingual Common Crawl (1.5M) and monolingual News (83M). For tuning we used MERT (Och, 2003) as implemented in Moses, optimized towards the Bleu metric (Papineni et al., 2002). For each tuning condition we ran tuning three times and show the mean result, in order to account for optimizer instability (Clark et al., 2011). For the manual analysis we use the system with the median Bleu score. 2.3 Evaluation In much of the work on translationese, with the exception of Lembersky (2013), only Bleu (Papineni et al., 2002) has been used for evaluation. Bleu has its limitations though, and to give a somewhat more thorough evaluation we also show results on Meteor (Denkowski and Lavie, 2010) a"
W17-0230,P02-1040,0,0.101809,"Hoang, 2007) and have LMs for both; for the other language pairs we only use words. KenLM (Heafield, 2011) was used to train a 5-gram word LM and SRILM (Stolcke, 2002) was used to train a 7-gram POS LM. Tagging was performed using Tree Tagger (Schmid, 1994). For training we used Europarl and News commentary, provided by WMT, with a total of over 2M segments for German and French and .77M for Czech. For English→German we used additional data: bilingual Common Crawl (1.5M) and monolingual News (83M). For tuning we used MERT (Och, 2003) as implemented in Moses, optimized towards the Bleu metric (Papineni et al., 2002). For each tuning condition we ran tuning three times and show the mean result, in order to account for optimizer instability (Clark et al., 2011). For the manual analysis we use the system with the median Bleu score. 2.3 Evaluation In much of the work on translationese, with the exception of Lembersky (2013), only Bleu (Papineni et al., 2002) has been used for evaluation. Bleu has its limitations though, and to give a somewhat more thorough evaluation we also show results on Meteor (Denkowski and Lavie, 2010) and TER (Snover et al., 2006). These metrics capture somewhat different aspects of M"
W17-0230,W11-2123,0,0.0288215,"h on translationese. The ratios for the test and tuning corpus are similar in all cases except for Czech→English. Tuning Test Original Foreign English Foreign English Mixed German 0.88 1.03 0.90 1.03 0.98 French 1.07 1.16 1.09 1.17 1.14 Czech 0.79 0.92 0.85 0.95 0.88 Table 1: Ratio of foreign to English words for sets with different original language. 2.2 SMT system We use Moses (Koehn et al., 2007) to train standard phrase-based SMT systems. For German↔English we use word and POS-tag factors (Koehn and Hoang, 2007) and have LMs for both; for the other language pairs we only use words. KenLM (Heafield, 2011) was used to train a 5-gram word LM and SRILM (Stolcke, 2002) was used to train a 7-gram POS LM. Tagging was performed using Tree Tagger (Schmid, 1994). For training we used Europarl and News commentary, provided by WMT, with a total of over 2M segments for German and French and .77M for Czech. For English→German we used additional data: bilingual Common Crawl (1.5M) and monolingual News (83M). For tuning we used MERT (Och, 2003) as implemented in Moses, optimized towards the Bleu metric (Papineni et al., 2002). For each tuning condition we ran tuning three times and show the mean result, in o"
W17-0230,Q15-1030,0,0.0446109,"lated tends to have shorter sentences and a lower type/token ratio than original texts, and explicitate information, for instance by using more cohesive markers than in original texts (Lembersky, 2013). Several studies have shown that it is possible to use text classification techniques to distinguish between original and translated texts with high accuracy (Baroni and Bernardini, 2006; Volansky et al., 2015), further supporting that there is a clear difference between original and translated texts. However, the domain of the text interacts to a high degree with translationese identification (Rabinovich and Wintner, 2015). Translationese has been shown to have an effect in relation to the training of statistical machine translation (SMT) systems, where the best results are seen when the texts used for training the SMT system have been translated in the same direction as that of the SMT system. This has been shown 2 Experimental setup To facilitate presentation we will use the abbreviations O for original texts and T for translated texts, and the term foreign to represent either of the languages German, French, and Czech. 2.1 Data We use data from the WMT shared tasks of News translation between 2008–2013 (Boja"
W17-0230,W09-0421,1,0.9173,"Statistical Machine Translation Sara Stymne Department of Linguistics and Philology Uppsala University sara.stymne@lingfil.uu.se Abstract both for the translation model (TM) (Kurokawa et al., 2009; Lembersky et al., 2012; Joelsson, 2016) and for the language model (LM) for which it is better to use translated than original texts (Lembersky et al., 2011). It works nearly as well to use predicted translationese as known translationese, both for the LM and TM (Twitto et al., 2015). It has also been noted that the original language of the test sentences influences the Bleu score of translations (Holmqvist et al., 2009). Besides the data used for the LM and TM, another important text for SMT training is the data used for tuning. The tuning set is used for tuning, or optimizing, the log-linear feature weights of the models, such as TM, LM, and reordering models. It is small compared to the other training data, and usually contains a couple of thousands of sentences, as opposed to millions of sentences for the LM and TM. It is supposed to be representative of the test set. To the best of our knowledge the effect of translationese has not previously been studied with respect to the tuning set. We investigate th"
W17-0230,2006.amta-papers.25,0,0.0709219,"emented in Moses, optimized towards the Bleu metric (Papineni et al., 2002). For each tuning condition we ran tuning three times and show the mean result, in order to account for optimizer instability (Clark et al., 2011). For the manual analysis we use the system with the median Bleu score. 2.3 Evaluation In much of the work on translationese, with the exception of Lembersky (2013), only Bleu (Papineni et al., 2002) has been used for evaluation. Bleu has its limitations though, and to give a somewhat more thorough evaluation we also show results on Meteor (Denkowski and Lavie, 2010) and TER (Snover et al., 2006). These metrics capture somewhat different aspects of MT quality. Bleu is mainly based on the precision of n-grams up to length 4, and thus rewards local fluency highly. Meteor is based on a weighted F-score on unigrams, with a matching step that consider word forms, stems, synonyms (for English), and paraphrases with different weights for content and function words, and a fragmentation score. It is thus less sensitive than Bleu to allowable linguistic variation. Meteor is also tuned for different target languages, to increase correlation with human 1 Until 2013 the WMT test and tuning sets we"
W17-0230,D07-1091,0,0.0160836,"er than originally authored texts, which is not a tendency that has been stressed in previous research on translationese. The ratios for the test and tuning corpus are similar in all cases except for Czech→English. Tuning Test Original Foreign English Foreign English Mixed German 0.88 1.03 0.90 1.03 0.98 French 1.07 1.16 1.09 1.17 1.14 Czech 0.79 0.92 0.85 0.95 0.88 Table 1: Ratio of foreign to English words for sets with different original language. 2.2 SMT system We use Moses (Koehn et al., 2007) to train standard phrase-based SMT systems. For German↔English we use word and POS-tag factors (Koehn and Hoang, 2007) and have LMs for both; for the other language pairs we only use words. KenLM (Heafield, 2011) was used to train a 5-gram word LM and SRILM (Stolcke, 2002) was used to train a 7-gram POS LM. Tagging was performed using Tree Tagger (Schmid, 1994). For training we used Europarl and News commentary, provided by WMT, with a total of over 2M segments for German and French and .77M for Czech. For English→German we used additional data: bilingual Common Crawl (1.5M) and monolingual News (83M). For tuning we used MERT (Och, 2003) as implemented in Moses, optimized towards the Bleu metric (Papineni et"
W17-0230,stymne-ahrenberg-2012-practice,1,0.855004,"h the addition of a shift operation to account for movement. Like Bleu, TER only considers exact word form matches. We also give the length ratio (LR), counted as the number of words, of the translation hypothesis relative to the reference text. In addition we perform a small human evaluation on a sample of segments for German→English translation. For each setting, we randomly picked 100 segments of length 10–15 words. One annotator compared the output from two systems for overall quality. Using only short segments can introduce a bias, since they might not be representative for all segments (Stymne and Ahrenberg, 2012), but it has the trade-off of being much faster and more consistent. 3 Table 3 also includes a custom system, where the tuning direction was chosen separately for each sentence based on its original language. We would expect this system to give the best results on this test set, since it is optimized for each language direction, but again the results are conflicting. It overall gives a good length ratio, though, and has the best or (near)-equal Bleu score to the O→T tuning. The TER score is always between that of T→O and O→T tuning. The Meteor score, however, is always lower for the custom sys"
W17-0230,W15-3002,0,0.0316012,"Missing"
W17-0230,P07-2045,0,\N,Missing
W17-0301,P16-1070,0,0.226552,"uthor’s first language is derived by analyzing texts written in his or her second language, is often treated as a text classification problem. NLI has proven useful in various applications, including in language-learning settings. As it is wellestablished that a speaker’s first language informs mistakes made in a second language, a system that can identify a learner’s first language is better equipped to provide learner-specific feedback and identify likely problem areas. The Treebank of Learner English (TLE) is the first publicly available syntactic treebank for English as a Second Language (Berzak et al., 2016). One particularly interesting feature of the TLE is 2 Related Work 2.1 L1 Identification in L2 Texts As mentioned in the previous section, the task of native language identification (NLI) involves determining a writer’s first language (L1) by analyzing texts produced in their second language (L2). Language learner data is used to train clasThis work is licenced under a Creative Commons Attribution 4.0 International Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/ Allison Adams and Sara Stymne 2017. Learning with learner corpora: Using the TLE for native language identifi"
W17-0301,P07-2053,0,0.0509736,"Missing"
W17-0301,W14-3625,0,0.0532186,"Missing"
W17-0301,D11-1148,0,0.069895,"Missing"
W17-0301,P11-1019,0,0.0367894,"be used to facilitate cross-lingual parsing research. All of the treebanks have been annotated according to the UD annotation scheme, in order to ensure consistency in annotation across treebanks. These guidelines have been developed with the goal of maximizing parallelism between languages (Nivre et al., 2016). The Treebank of Learner English (TLE) is a part of the UD project and is a manually annotated syntactic treebank for English as a Second Language (Berzak et al., 2016). It includes PoS tags and UD trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011). The treebank is split randomly in to a training set of 4,124 sentences, a development set of 500 sentences and a test set of 500 sentences. Ten different language backgrounds are represented in this corpus: Chinese, French, German, Italian, Japanese, Korean, Portuguese,Spanish, Russian and Turkish. For each language background, the TLE contains 500 randomly sampled sentences from the FCE data set, in order to ensure even representation. All sentences included in the TLE were selected so that they contain grammatical errors of some kind. The creators of the treebank exploit a pre-existing err"
W17-0301,silveira-etal-2014-gold,0,0.0605914,"Missing"
W17-0301,P12-2038,0,0.0607369,"Missing"
W17-0301,W13-1719,0,0.0483191,"Missing"
W17-0301,C12-1158,0,0.12008,"ccuracy, despite having LAS and UAS scores nearly five points above the other TLE two models, the corrected model had the lowest classification performance of the three. The full EWT model with a much lower parsing accuracy also Using dependency arcs as features Similar to most other NLI systems, in this paper, the task of native language identification is approached as a text classification problem. In order to solve this classification problem, dependency relations were extracted from each document to be used as frequency-based features. To do this, a system similar to the one presented in (Tetreault et al., 2012) was used, with the main difference being that MaltParser, rather than the Stanford Dependency parser was used to obtain them. This system, represented below in Table 4, can be described as follows: each basic dependency relation, consisting of the dependency label, the parent node, and the child node is extracted from the sentence. To mitigate sparsity, each dependency in the document was represented in several different ways. In the first representation, the lemmas for the root and child node were used to form the dependency relation. Secondly, part-of-speech tags were considered instead of"
W17-0306,P16-1070,0,0.0220219,"ernational Licence. Licence details: http: //creativecommons.org/licenses/by/4.0/ Sara Stymne, Eva Pettersson, Beáta Megyesi and Anne Palmér 2017. Annotating errors in student texts: First experiences and experiments. Proceedings of the Joint 6th Workshop on NLP for Computer Assisted Language Learning and 2nd Workshop on NLP for Research on Language Acquisition at NoDaLiDa 2017. Linköping Electronic Conference Proceedings 134: 47–60. 47 sion. A part of this corpus has been manually annotated with POS-tags and dependency structures, and was recently released as the Treebank of Learner English (Berzak et al., 2016). In the spell checking and grammar checking literature, e.g. Brill and Moore (2000) or Carlberger et al. (2005), corpora with annotated errors are often used for evaluation. However, little is usually written about these annotations. possibly also adapting the models to different age groups, levels, and for students of Swedish as L1 or L2. Being able to correct errors is also important in order to achieve good performance on downstream tasks like tagging and parsing. From a writing development perspective, the normalized corpus can allow analysis of writing skills development during school ye"
W17-0306,P00-1037,0,0.26635,"Sara Stymne, Eva Pettersson, Beáta Megyesi and Anne Palmér 2017. Annotating errors in student texts: First experiences and experiments. Proceedings of the Joint 6th Workshop on NLP for Computer Assisted Language Learning and 2nd Workshop on NLP for Research on Language Acquisition at NoDaLiDa 2017. Linköping Electronic Conference Proceedings 134: 47–60. 47 sion. A part of this corpus has been manually annotated with POS-tags and dependency structures, and was recently released as the Treebank of Learner English (Berzak et al., 2016). In the spell checking and grammar checking literature, e.g. Brill and Moore (2000) or Carlberger et al. (2005), corpora with annotated errors are often used for evaluation. However, little is usually written about these annotations. possibly also adapting the models to different age groups, levels, and for students of Swedish as L1 or L2. Being able to correct errors is also important in order to achieve good performance on downstream tasks like tagging and parsing. From a writing development perspective, the normalized corpus can allow analysis of writing skills development during school years in Swedish as L1 or L2. The error identification accomplished in this corpus is"
W17-0306,J96-2004,0,0.141011,"he two versions of the text, both with the original tokens, and with the corrected tokens. 4.4 Inter-Annotator Agreement In this section we present results on interannotator agreement between the three annotators that took part in the final annotation process. In order to do this analysis, a sample of 2–3 texts each from level C-3, C-5, C-6, C-9 and US-1 were chosen, with a mix of Swedish and Swedish as a second language. In total there were 11 texts with 2923 tokens. The three annotators annotated this text independently with access to the guidelines. First we calculated agreement and kappa (Carletta, 1996) for each pair of annotators in the final phase, for the 6-way classification of each word into one of the error categories, or correct. Table 2 shows the results of this analysis. Since the majority of words are correct, the scores are very high in all cases, but even if we exclude the cases where both annotators agreed on that a word is correct, the agreement scores are reasonably high, with a kappa value over 0.6, which is considered substantial agreement (Landis and Koch, 1977). In most cases the disagreement is between an error Proceedings of the Joint 6th Workshop on NLP for Computer Ass"
W17-0306,L16-1509,1,0.853954,"Missing"
W17-0306,P02-1019,0,0.0809302,"ecting (i.e., has a higher normalization accuracy for) the texts written by older students. One reason for this is that since the older students often write less frequent and longer words, there are typically only one word in the dictionary with an edit distance of one to the original word form. For texts written by younger students on the other hand, shorter words are often used, where there are several entries to choose from as normalization candidates in the dictionary. To improve accuracy for these cases, it could be helpful to add knowledge about phonetics to the normalization algorithm (Toutanova and Moore, 2002), so that the system becomes aware that it is more likely that for example cyckeln should be normalized into cykeln (’the bike’), rather than nyckeln (’the key’), even if the two candidates both are within one edit distance from the original word form. Another reason that the texts written by the younger students are harder to correct is that the Quality of Tagging and Parsing Proceedings of the Joint 6th Workshop on NLP for Computer Assisted Language Learning and 2nd Workshop on NLP for Research on Language Acquisition at NoDaLiDa 2017 57 Correct Spelling Grammar Split Merged Casing POS 447 ("
W17-0306,P81-1022,0,0.527338,"Missing"
W17-0306,L16-1031,0,0.162344,"Missing"
W17-0306,W17-0216,1,0.875493,"Missing"
W17-0306,W13-5617,1,0.885616,"ended to be a monitor corpus, extended with new, analyzed tests. The texts in UCSW are annotated automatically in a pipeline using SweGram (N¨asman et al., 2017), an online tool for automatic analysis of Swedish texts. The tool includes tokenization, normalization to correct spelling errors and split compounds, part-of-speech tagging, and dependency parsing. First tokenization is performed to separate sentences and tokens, using the Svannotate tool (Nivre et al., 2008). Then spelling errors are corrected by using a simple unweighted Levenshtein distance, with threshold 1 on all unknown words (Pettersson et al., 2013). Split compounds are addressed by using a set of a few rules ¨ (Ohrman, 1998). Part-of-speech tagging and morphological analysis are carried out using efselab ¨ (Ostling, 2016) and dependency parsing is performed using MaltParser (Nivre et al., 2006). The analysis tools achieve state-of-the-art accuracy on standard texts with the exception of the normalizer. The corrections of spelling errors and split compounds are very noisy and far from human quality, thus necessitating work on these issues. USCW uses an extension of the CoNLL-U format, a format which is used in the universal 2 Related Wor"
W17-0306,nivre-etal-2006-maltparser,0,\N,Missing
W17-0306,L16-1262,0,\N,Missing
W17-0306,tenfjord-etal-2006-ask,0,\N,Missing
W17-4801,D12-1133,0,0.165034,"16 (Guillou et al., 2016), but the differences in the resulting evaluation scores are actually minor. As we have explained above, the shared task focused primarily on subject pronouns. However, in English and German, some pronouns are ambiguous between subject and object position, e.g., the English it and the German es and sie. In order to address this issue, in 2016 we introduced filtering of object pronouns based on dependency parsing. This filtering removed all pronoun instances that did not have a subject dependency label.6 For joint dependency parsing and POS-tagging, we used Mate Tools (Bohnet and Nivre, 2012), with default models. Since in 2016 we found that this filtering was very accurate, this year we performed only automatic filtering for the training and the development, and also for the test datasets. Note that since only subject pronouns can be realized as prodropped pronouns in Spanish, subject filtering was not necessary. 4 Baseline Systems The baseline system is based on an n-gram language model (LM). The architecture is the same as that used for the WMT 2016 cross-lingual pronoun prediction task.7 In 2016, most systems outperformed this baseline, and for the sake of comparison, we thoug"
W17-4801,W16-2350,1,0.857196,"Missing"
W17-4801,W17-4807,1,0.863641,"Missing"
W17-4801,2010.iwslt-papers.10,1,0.907647,"is is hard as selecting the correct pronoun may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-au"
W17-4801,W15-2501,1,0.855394,"gradually raised interest in the research community for a shared task that would allow to compare various competing proposals and to quantify the extent to which they improve the translation of different pronouns for different language pairs and different translation directions. However, evaluating pronoun translation comes with its own challenges, as reference-based evaluation, which is standard for machine translation in general, cannot easily take into account legitimate variations of translated pronouns or their placement in the sentence. Thus, building upon experience from DiscoMT 2015 (Hardmeier et al., 2015) and WMT 2016 (Guillou et al., 2016), this year’s cross-lingual pronoun prediction shared task has been designed to test the capacity of the participating systems for translating pronouns correctly, in a framework that allows for objective evaluation, as we will explain below. 2 ce OTHER ce|PRON qui|PRON It ’s an idiotic debate . It has to stop . REPLACE 0 eˆ tre|VER un|DET d´ebat|NOM idiot|ADJ REPLACE 6 devoir|VER stopper|VER .|. 0-0 1-1 2-2 3-4 4-3 6-5 7-6 8-6 9-7 10-8 Figure 2: English→French example from the development dataset. First come the gold class labels, followed by the pronouns (t"
W17-4801,N13-1073,0,0.0460377,"raka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED talks, one for each subtask. However, we chose the current setup as using a smaller set of documents r"
W17-4801,W17-4806,0,0.0618776,"ial anaphora) or in different sentences (inter-sentential anaphora). Most MT systems translate sentences in isolation, and thus inter-sentential anaphoric pronouns will be translated without knowledge of their antecedent, and thus pronoun-antecedent agreement cannot be guaranteed. NMT yields generally higher-quality translation, but is harder to analyze, and thus little is known about how well it handles pronoun translation. Yet, it is clear that it has access to larger context compared to phrase-based SMT models, potentially spanning multiple sentences, which can improve pronoun translation (Jean et al., 2017a). Motivated by these challenges, the DiscoMT 2017 workshop on Discourse in Machine Translation offered a shared task on cross-lingual pronoun prediction. This was a classification task, asking the participants to make predictions about which pronoun should replace a placeholder in the target-language text. The task required no MT expertise and was designed to be interesting as a machine learning task on its own right, e.g., for researchers working on co-reference resolution. Source Target POS tags Reference The above constraints start playing a role in pronoun translation in situations where"
W17-4801,E12-3001,0,0.0230275,"may need discourse analysis as well as linguistic and world knowledge. Null subjects in pro-drop languages pose additional challenges as they express person and number within the verb’s morphology, rendering a subject pronoun or noun phrase redundant. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the sou"
W17-4801,2005.mtsummit-papers.11,0,0.0869283,"teresting research challenges from the perspective of both speech recognition and machine translation. Therefore, both research communities are making increased use of them in building benchmarks. TED talks address topics of general interest and are delivered to a live public audience whose responses are also audible on the recordings. The talks generally aim to be persuasive and to change the viewers’ behaviour or beliefs. The genre of the TED talks is transcribed planned speech. 3.1.2 Europarl and News For training purposes, in addition to TED talks, we further made available the Europarl3 (Koehn, 2005) and News Commentary4 corpora for all language pairs but Spanish-English, for which only TED talks and Europarl were available. We used the alignments provided by OPUS, including the document boundaries from the original sources. For Europarl, we used ver. 7 of the data release, and for News Commentary we used ver. 9. 3.2 Test Set Selection We selected the test data from talks added recently to the TED repository such that: 1. The talks have been transcribed (in English) and translated into both German and French. 2. They were not used in the IWSLT evaluation campaigns, nor in the DiscoMT 2015"
W17-4801,2005.iwslt-1.8,0,0.121419,"OS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English dir"
W17-4801,W10-1737,0,0.13529,"Missing"
W17-4801,J03-1002,0,0.0103978,"rted the TreeTagger’s POS tags to the target coarse POS tags using pre-defined mappings.5 For French, we clipped the morphosyntactic information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction,"
W17-4801,P14-2050,0,0.026726,"that is aligned to the pronoun to be predicted. All input sequences are fed in an embedding layer followed by two layers of GRUs. The values in the last layer form a vector, which is further concatenated to the pronoun alignment embeddings, to form a larger vector, which is then used to make the final prediction using a dense neural network. The pretraining is a modification of the skip-gram model of WORD 2 VEC (Mikolov et al., 2013), in which along with the skip-gram token context, all target sentence pronouns are predicted as well. The process of pretraining is performed using WORD 2 VECF (Levy and Goldberg, 2014). 5.2 NYU The NYU system (Jean et al., 2017b) uses an attention-based neural machine translation model and three variants that incorporate information from the preceding source sentence. The sentence is added as an auxiliary input using additional encoder and attention models. The systems are not specifically designed for pronoun prediction and may be used to generate complete sentence translations. They are trained exclusively on the data provided for the task, using the text only and ignoring the provided POS tags and alignments. 5.4 UU-Hardmeier The UU- HARDMEIER system (Hardmeier, 2017) is"
W17-4801,petrov-etal-2012-universal,0,0.0455296,"Missing"
W17-4801,W16-2351,1,0.891541,"Missing"
W17-4801,S17-2088,1,0.872336,"Missing"
W17-4801,W16-2202,0,0.0276957,"Missing"
W17-4801,D15-1166,0,0.0502002,"nt. Thus, translating from such languages requires generating a pronoun in the target language for which there is no pronoun in the source. Pronoun translation is known to be challenging not only for MT in general, but also for Statistical Machine Translation (SMT) in particular (Le Nagard and Koehn, 2010; Hardmeier and Federico, 2010; Nov´ak, 2011; Guillou, 2012; Hardmeier, 2014). Phrase-based SMT (Koehn et al., 2013) was state of the art until recently, but it is gradually being replaced by Neural Machine Translation, or NMT, (Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015). We describe the design, the setup, and the evaluation results of the DiscoMT 2017 shared task on cross-lingual pronoun prediction. The task asked participants to predict a target-language pronoun given a source-language pronoun in the context of a sentence. We further provided a lemmatized target-language human-authored translation of the source sentence, and automatic word alignments between the source sentence words and the targetlanguage lemmata. The aim of the task was to predict, for each target-language pronoun placeholder, the word that should replace it from a small, closed set of cl"
W17-4801,W16-2353,0,0.0406223,"/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a different tag: EP. Still, we decided to use the same filtering this year, to keep the task sta"
W17-4801,L16-1680,0,0.054481,"Missing"
W17-4801,W17-4808,0,0.0205662,"mata constructed from news texts, parliament debates, and the TED talks of the training/development portions of the datasets. The additional monolingual news data comprises the shuffled news texts from WMT, including the 2014 editions for German and English, and the 2007– 2013 editions for French. 5 Submitted Systems A total of five teams participated in the shared task, submitting primary systems for all subtasks. Most teams also submitted contrastive systems, which have unofficial status for the purpose of ranking, but are included in the tables of results. 5.1 TurkuNLP The TurkuNLP system (Luotolahti et al., 2017) is an improvement of the last year’s system by the same team (Luotolahti et al., 2016). The improvement mainly consists of a pre-training scheme for vocabulary embeddings based on the task. The system is based on a recurrent neural network based on stacked Gated Recurrent Units (GRUs). The pretraining scheme involves a modification of WORD 2 VEC to use all target sequence pronouns along with typical skip-gram contexts in order to induce embeddings suitable for the task. 6 In 2016, we found that this filtering was too aggressive for German, since it also removed expletives, which had a differe"
W17-4801,W16-2355,1,0.796565,"the data is used in each epoch. For the primary system, all classes are sampled equally, as long as there are enough instances for each class. Although this sampling method biases the system towards macro-averaged recall, on the test data the system performed very well in terms of both macro-averaged recall and accuracy. The secondary system uses a sampling method in which the samples are proportional to the class distribution in the development dataset. 5.5 UU-Stymne16 The UU-S TYMNE 16 system uses linear SVM classifiers, and it is the same system that was submitted for the 2016 shared task (Stymne, 2016). It is based mainly on local features, and anaphora is not explicitly modeled. The features used include source pronouns, local context words/lemmata, target POS n-grams with two different POS tagsets, dependency heads of pronouns, alignments, and position of the pronoun. A joint tagger and dependency parser (Bohnet and Nivre, 2012) is used on the source text in order to produce some of the features. Overall, the source pronouns, the local context and the dependency features performed best across all language pairs. 8 7 Stymne (2016) describes several variations of the method, including both"
W17-4801,W17-4805,1,0.882705,"Missing"
W17-4801,S16-1001,1,0.80372,"erforming system here is T URKU NLP with a macro-averaged recall of 58.82. However, it is nearly tied with U PPSALA, and both are somewhat close to NYU. Noteworthy, though, is that the highest-scoring system on macro-average recall is the contrastive system of NYU; NYU also has the second-best accuracy, outperformed only by U PPSALA. Evaluation While in 2015 we used macro-averaged F1 as an official evaluation measure, this year we followed the setup of 2016, where we switched to macroaveraged recall, which was also recently adopted by some other competitions, e.g., by SemEval2016/2017 Task 4 (Nakov et al., 2016; Rosenthal et al., 2017). Moreover, as in 2015 and 2016, we also report accuracy as a secondary evaluation measure (but we abandon F1 altogether). Macro-averaged recall ranges in [0, 1], where a value of 1 is achieved by the perfect classifier,8 and a value of 0 is achieved by the classifier that misclassifies all examples. The value of 1/C, where C is the number of classes, is achieved by a trivial classifier that assigns the same class to all examples (regardless of which class is chosen), and is also the expected value of a random classifier. The advantage of macro-averaged recall over acc"
W17-4801,C96-2141,0,0.453266,"c information and we reduced the number of verb form tags to just one. For Spanish, we used UDPipe (Straka et al., 2016), which includes universal POS tags and a lemmatizer. In previous years, the automatic alignments used for the task were optimized to improve the precision and recall of pronoun alignments. For the repeated language pairs, we reused the best performing alignment strategies from 2015 and 2016. For English→French and Spanish→English we used GIZA++ (Och and Ney, 2003) model 4 with grow-diag-final-and (Koehn et al., 2005) as symmetrization. For English↔German we used GIZA++ HMM (Vogel et al., 1996) alignment with intersection for symmetrization. In all cases, we used fast align (Dyer et al., 2013) as backoff for sentences that are longer than the 100-word limit of GIZA++. Tokens source target 11,716 12,624 13,139 12,623 Data Preparation 13,360 11,859 13,439 13,242 Table 2: Statistics about the 2017 test datasets. In total, we selected 16 TED talks for testing, which we split into two groups as follows: 8 TED talks for the English to French/German direction, and 8 TED talks for the Spanish/German to English direction. Another option would have been to create four separate groups of TED t"
W17-4805,P15-1162,0,0.051767,"Missing"
W17-4805,D12-1133,0,0.0710208,"endency heads for pronoun classification, especially for the source languages German and French. We followed these findings and included head dependency information into our current system. 3 languages. We first train models on all available data, then continue training these models for additional epochs using only in-domain IWSLT data. While the source side sentences are regular inflected words, the target side sentences are given as lemmas with POS-tags. In order to utilize richer representations for the source side we tag and parse the source data. For English and German we use Mate Tools (Bohnet and Nivre, 2012) and for Spanish we use UD-Pipe (Straka et al., 2016). To achieve a flat representation, we represent each source word by its word form, POStag and the dependency label for its head (e.g. woman|NOUN|SBJ, false|JJ|NMOD). After parsing, all input words and lemmas are lowercased, and all numerals are replaced by a single token. 3.1 Sampling One of the inherent difficulties of the task is the imbalance in the distribution of the classes. Every language pair is different, but in general the OTHER class is large in comparison to all other classes, and masculine pronouns are more frequent than femini"
W17-4805,Q16-1023,0,0.170877,"to create features, potentially providing the means to understand the different aspects involved in pronoun translation. First formalized by Hardmeier (2014), the approach was introduced as a shared task at the DiscoMT 2015 Workshop (Hardmeier et al., 2015). In 2016, the shared task included more language pairs and lemmatized target data (Guillou et al., 2016). This year’s edition (Lo´aiciga et al., 2017) 2 Related Work Our system architecture draws inspiration from several sources, most prominently from the pronoun prediction system by Luotolahti et al. (2016) and the parser architecture by Kiperwasser and Goldberg (2016). Luotolahti et al. (2016) built the winning system for the 2016 edition of this shared task. The system is based on two stack levels of GRU units and it relies almost uniquely on context. Other 47 Proceedings of the Third Workshop on Discourse in Machine Translation, pages 47–53, c Copenhagen, Denmark, September 8, 2017. 2017 Association for Computational Linguistics. than representations of the source pronouns, its input contains up to 50 tokens of context, reading away from the pronoun to be predicted, to the left and the right, both for the source and the target language. It uses a weighte"
W17-4805,W15-2511,1,0.908215,"Missing"
W17-4805,W16-2349,0,0.02063,"for all language pairs. 1 me ayudan a ser escuchada “me help3.Pers.Pl to be heard” REPLACE help me to be heard PRON VERB PRON PART AUX VERB They help me to be heard Figure 1: Spanish-English example. also features lemmatized target data and it includes the Spanish-English language pair, which introduces pro-drops or null subjects to the task. These refer to omitted subject pronouns whose interpretation is recovered through the verb’s morphology, as shown in Figure 1. Given the success of neural networks for crosslingual pronoun classification (Hardmeier et al., 2013; Luotolahti et al., 2016; Dabre et al., 2016), we wanted to explore this type of system architecture. Our system is based on BiLSTMs enhanced with information about the source pronoun, the pronoun’s syntactic head dependency and character-level representations of the source words. Our system ranked first for English– German, with 10 percentage points of macro recall ahead of the second best team. For the other three language pairs, the system obtained the second best macro recall. In addition, our system reached the highest accuracy for three out of the four language pairs. Introduction Cross-lingual pronoun prediction is a classificatio"
W17-4805,W17-4801,1,0.883153,"Missing"
W17-4805,K17-3022,1,0.89204,"Missing"
W17-4805,W16-2353,0,0.237968,"in first or second place for all language pairs. 1 me ayudan a ser escuchada “me help3.Pers.Pl to be heard” REPLACE help me to be heard PRON VERB PRON PART AUX VERB They help me to be heard Figure 1: Spanish-English example. also features lemmatized target data and it includes the Spanish-English language pair, which introduces pro-drops or null subjects to the task. These refer to omitted subject pronouns whose interpretation is recovered through the verb’s morphology, as shown in Figure 1. Given the success of neural networks for crosslingual pronoun classification (Hardmeier et al., 2013; Luotolahti et al., 2016; Dabre et al., 2016), we wanted to explore this type of system architecture. Our system is based on BiLSTMs enhanced with information about the source pronoun, the pronoun’s syntactic head dependency and character-level representations of the source words. Our system ranked first for English– German, with 10 percentage points of macro recall ahead of the second best team. For the other three language pairs, the system obtained the second best macro recall. In addition, our system reached the highest accuracy for three out of the four language pairs. Introduction Cross-lingual pronoun predicti"
W17-4805,W15-2501,1,0.853595,"pronoun. The task is restricted to pronouns at subject positions only and it is defined as a “fill-in-the-gap-task”: given an input text and a translation with placeholders, replace the placeholders with pronouns. Word alignment links of the placeholders to the source sentence are also given. This setting allows to analyze both the source and the target languages to create features, potentially providing the means to understand the different aspects involved in pronoun translation. First formalized by Hardmeier (2014), the approach was introduced as a shared task at the DiscoMT 2015 Workshop (Hardmeier et al., 2015). In 2016, the shared task included more language pairs and lemmatized target data (Guillou et al., 2016). This year’s edition (Lo´aiciga et al., 2017) 2 Related Work Our system architecture draws inspiration from several sources, most prominently from the pronoun prediction system by Luotolahti et al. (2016) and the parser architecture by Kiperwasser and Goldberg (2016). Luotolahti et al. (2016) built the winning system for the 2016 edition of this shared task. The system is based on two stack levels of GRU units and it relies almost uniquely on context. Other 47 Proceedings of the Third Work"
W17-4805,L16-1680,0,0.0805793,"Missing"
W17-4805,D13-1037,0,0.0285692,"em is competitive and is in first or second place for all language pairs. 1 me ayudan a ser escuchada “me help3.Pers.Pl to be heard” REPLACE help me to be heard PRON VERB PRON PART AUX VERB They help me to be heard Figure 1: Spanish-English example. also features lemmatized target data and it includes the Spanish-English language pair, which introduces pro-drops or null subjects to the task. These refer to omitted subject pronouns whose interpretation is recovered through the verb’s morphology, as shown in Figure 1. Given the success of neural networks for crosslingual pronoun classification (Hardmeier et al., 2013; Luotolahti et al., 2016; Dabre et al., 2016), we wanted to explore this type of system architecture. Our system is based on BiLSTMs enhanced with information about the source pronoun, the pronoun’s syntactic head dependency and character-level representations of the source words. Our system ranked first for English– German, with 10 percentage points of macro recall ahead of the second best team. For the other three language pairs, the system obtained the second best macro recall. In addition, our system reached the highest accuracy for three out of the four language pairs. Introduction Cross"
W17-4805,W16-2355,1,0.748326,"nsition-based parser. We use the same underlying BiLSTM layer for word representations, but in our case, we feed the representation of selected words to a pronoun classifier. de Lhoneux et al. (2017) describe several additions to this parser, including character embeddings as part of the word representation. Given their value to capture morphological information, we include character embeddings for the source language in our system. Lo´aiciga (2015) reports that pronoun prediction benefits from syntactic features when using a Maximum Entropy classifier. Similarly, but using an SVM classifier, Stymne (2016) provides evidence in favor of including information about dependency heads for pronoun classification, especially for the source languages German and French. We followed these findings and included head dependency information into our current system. 3 languages. We first train models on all available data, then continue training these models for additional epochs using only in-domain IWSLT data. While the source side sentences are regular inflected words, the target side sentences are given as lemmas with POS-tags. In order to utilize richer representations for the source side we tag and par"
W17-4805,D16-1163,0,0.0327728,"the same distribution of classes as the development data and also the same size. Because the sample size is small, this Data and Evaluation We use only the training data provided by the shared task (Lo´aiciga et al., 2017).1 For development data, we concatenate all available development data for each language pair. Test data is the official shared task test data. For training data we either concatenate all available training data, or use only the in-domain IWSLT data, which contains TED talks. In addition, we perform experiments with a very simple domain adaptation technique in the spirit of Zoph et al. (2016), but applying it to different domains instead of to different 2 Using all pronoun instances of a sentence improves training efficiency, but at the cost of making the sample proportions less precise. 1 See also https://www.idiap.ch/workshop/ DiscoMT/shared-task. 48 word pos dep char Parameter Word embedding dimensions Lemma embedding dimensions POS-tag embedding dimensions Dep label embedding dimensions Character embedding dimensions Character BiLSTM dimensions BiLSTM Layers BiLSTM hidden dimensions BiLSTM output dimensions Hidden units in MLP α (for word dropout) LSTM dropout HEAD source lang"
W17-6314,W06-2922,0,0.0299327,"uracy and is significantly better than a system trained with a purely static oracle. 1 Introduction Non-projective sentences are a notorious problem in dependency parsing. Traditional algorithms like those developed by Nivre (2003, 2004) for transition-based parsing only allow the construction of projective trees. These algorithms make use of a stack, a buffer and a set of arcs, and parsing consists of performing a sequence of transitions on these structures. Traditional algorithms have been extended in different ways to allow the construction of non-projective trees (Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2007; G´omez-Rodr´ıguez and Nivre, 2010). One method proposed by Nivre (2009) is based on the idea of word reordering. This is achieved by adding a transition that swaps two items in the data structures used, enabling the construction of arbitrary non-projective trees while still only adding arcs between adjacent words (after possible reordering). This technique was previously used in the arc-standard transition system (Nivre, 2004). The first contribution of this paper is to show that it can also be combined with the arc-hybrid system 99 Proceedings of the 15th International Conferen"
W17-6314,P09-1040,1,0.921113,"transition systems that satisfy the property of arc-decomposability, meaning that a tree is reachable from a configuration if and only if every arc in the tree is reachable in itself. Based on this result, they defined dynamic oracles for the arc-eager (Nivre, 2003), arc-hybrid (Kuhlmann et al., 2011) and easy-first (Goldberg and Elhadad, 2010) systems. Transition systems that allow non-projective trees are in general not arc-decomposable and therefore require different methods for constructing dynamic oracles (G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). The online reordering system of Nivre (2009) is furthermore based on the arc-standard system, which is not even arc-decomposable in itself (Goldberg and Nivre, 2013). The second contribution of this paper is to show that we can take advantage of the arcdecomposability of the arc-hybrid transition system and extend the existing dynamic oracle to deal with the added swap transition. The resulting orWe extend the arc-hybrid transition system for dependency parsing with a S WAP transition that enables reordering of the words and construction of non-projective trees. Although this extension potentially breaks the arc-decomposability of the t"
W17-6314,K17-3022,1,0.665096,"Missing"
W17-6314,N10-1115,0,0.0457228,"cle, we need to be able to compute the cost of any transition in any configuration, where cost is usually defined as minimum Hamming loss with respect to the best tree reachable from that configuration. Goldberg and Nivre (2013) showed that this computation is straightforward for transition systems that satisfy the property of arc-decomposability, meaning that a tree is reachable from a configuration if and only if every arc in the tree is reachable in itself. Based on this result, they defined dynamic oracles for the arc-eager (Nivre, 2003), arc-hybrid (Kuhlmann et al., 2011) and easy-first (Goldberg and Elhadad, 2010) systems. Transition systems that allow non-projective trees are in general not arc-decomposable and therefore require different methods for constructing dynamic oracles (G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). The online reordering system of Nivre (2009) is furthermore based on the arc-standard system, which is not even arc-decomposable in itself (Goldberg and Nivre, 2013). The second contribution of this paper is to show that we can take advantage of the arcdecomposability of the arc-hybrid transition system and extend the existing dynamic oracle to deal with the added swap transi"
W17-6314,W09-3811,1,0.667477,"nor any of its reachable dependents. In the old projective case, the loss was limited to a head and dependents in b|β, but because s0 can potentially be swapped back to the buffer, we again define reachability explicitly through RDEPS(s0 ) (for dependents) Dynamic Oracle Since we use a static oracle for S WAP transitions, these will always have zero cost. The dynamic oracle thus only needs to define costs for the remaining three transitions. To construct the oracle, we start from the old dynamic oracle for the projective 5 This is equivalent to an eager static oracle for S WAP in the sense of Nivre et al. (2009). 101 1 s1 2 s0 3 b [ 1 2 ]Σ 4 [ 3 4 ]B 1 R IGHT ⇒ 2 [ 1 ]Σ 3 4 [ 3 4 ]B S HIFT ⇓ 1 2 [ 1 2 3 ]Σ 3 4 1 s1 [ 4 ]B 2 s0 [ 1 2 ]Σ 4 b 3 [ 4 3 ]B Figure 2: Top left: Configuration with all nodes in projective order and gold tree displayed above the nodes. Top right: Gold arc lost (the red dotted arc) when applying a R IGHT transition from the top left configuration. The arc added by the transition is in blue, it is not in the gold tree. Bottom left: Gold arcs lost (the red dotted arcs) when applying a S HIFT transition from the top left configuration. Bottom right: Configuration where b is higher"
W17-6314,P05-1013,1,0.857982,"tem gives competitive accuracy and is significantly better than a system trained with a purely static oracle. 1 Introduction Non-projective sentences are a notorious problem in dependency parsing. Traditional algorithms like those developed by Nivre (2003, 2004) for transition-based parsing only allow the construction of projective trees. These algorithms make use of a stack, a buffer and a set of arcs, and parsing consists of performing a sequence of transitions on these structures. Traditional algorithms have been extended in different ways to allow the construction of non-projective trees (Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2007; G´omez-Rodr´ıguez and Nivre, 2010). One method proposed by Nivre (2009) is based on the idea of word reordering. This is achieved by adding a transition that swaps two items in the data structures used, enabling the construction of arbitrary non-projective trees while still only adding arcs between adjacent words (after possible reordering). This technique was previously used in the arc-standard transition system (Nivre, 2004). The first contribution of this paper is to show that it can also be combined with the arc-hybrid system 99 Proceedings of the 15th Interna"
W17-6314,Q13-1033,1,0.961767,"were trained in a static way and were only exposed to configurations resulting from optimal transitions during training. Dynamic oracles define optimal transition sequences for any configuration in which the parser may be. The use of dynamic oracles enables training with exploration of errors, which mitigates the problem of error propagation at prediction time. In order to define a dynamic oracle, we need to be able to compute the cost of any transition in any configuration, where cost is usually defined as minimum Hamming loss with respect to the best tree reachable from that configuration. Goldberg and Nivre (2013) showed that this computation is straightforward for transition systems that satisfy the property of arc-decomposability, meaning that a tree is reachable from a configuration if and only if every arc in the tree is reachable in itself. Based on this result, they defined dynamic oracles for the arc-eager (Nivre, 2003), arc-hybrid (Kuhlmann et al., 2011) and easy-first (Goldberg and Elhadad, 2010) systems. Transition systems that allow non-projective trees are in general not arc-decomposable and therefore require different methods for constructing dynamic oracles (G´omez-Rodr´ıguez and Fern´and"
W17-6314,P15-2042,0,0.0648949,"Missing"
W17-6314,P10-1151,1,0.895825,"Missing"
W17-6314,Q16-1023,0,0.169531,"e attached to its head (3) and therefore makes us lose the arc 3 → 2, as shown in the top right corner. If we instead apply a S HIFT transition, we lose the arc between b (3) and its head (1) as well as the arc 3 → 2, as shown in the bottom left corner. By contrast, a L EFT transition has zero cost, because no arcs are lost so the best tree reachable in the orig4 Experiments We extend the parser we used in de Lhoneux et al. (2017), a greedy transition-based parser that predicts the dependency tree given the raw words of a sentence. That parser is itself an extension of the parser developed by Kiperwasser and Goldberg (2016). It relies on a BiLSTM to learn informative features of words in context and a feed-forward network for predicting the next parsing transition. It learns vector representations of the words as well as characters. Contrary to parsing tradition, it makes no use of part-of-speech tags. We released our system as UUparser 2.0, available at https: //github.com/UppsalaNLP/uuparser. 102 Language A.Greek Arabic Basque English Portuguese We first compare our system, which uses our static-dynamic oracle, with the same system using a static oracle. This is to find out if we can benefit from error explora"
W17-6314,P11-1068,0,0.42275,"Missing"
W17-6314,W03-3017,1,0.84856,"propagation at prediction time. In order to define a dynamic oracle, we need to be able to compute the cost of any transition in any configuration, where cost is usually defined as minimum Hamming loss with respect to the best tree reachable from that configuration. Goldberg and Nivre (2013) showed that this computation is straightforward for transition systems that satisfy the property of arc-decomposability, meaning that a tree is reachable from a configuration if and only if every arc in the tree is reachable in itself. Based on this result, they defined dynamic oracles for the arc-eager (Nivre, 2003), arc-hybrid (Kuhlmann et al., 2011) and easy-first (Goldberg and Elhadad, 2010) systems. Transition systems that allow non-projective trees are in general not arc-decomposable and therefore require different methods for constructing dynamic oracles (G´omez-Rodr´ıguez and Fern´andez-Gonz´alez, 2015). The online reordering system of Nivre (2009) is furthermore based on the arc-standard system, which is not even arc-decomposable in itself (Goldberg and Nivre, 2013). The second contribution of this paper is to show that we can take advantage of the arcdecomposability of the arc-hybrid transition"
W17-6314,W04-0308,1,0.505296,"these structures. Traditional algorithms have been extended in different ways to allow the construction of non-projective trees (Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2007; G´omez-Rodr´ıguez and Nivre, 2010). One method proposed by Nivre (2009) is based on the idea of word reordering. This is achieved by adding a transition that swaps two items in the data structures used, enabling the construction of arbitrary non-projective trees while still only adding arcs between adjacent words (after possible reordering). This technique was previously used in the arc-standard transition system (Nivre, 2004). The first contribution of this paper is to show that it can also be combined with the arc-hybrid system 99 Proceedings of the 15th International Conference on Parsing Technologies, pages 99–104, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics acle is static with respect to the new transition but remains dynamic for all other transitions. We show experimentally that this static-dynamic oracle gives a significant advantage over the alternative static oracle and results in competitive results for non-projective parsing. 2 • S WAP[(σ|s0 , b|β, A)] = (σ, b|s0"
W17-6314,N07-1050,1,0.664396,"gnificantly better than a system trained with a purely static oracle. 1 Introduction Non-projective sentences are a notorious problem in dependency parsing. Traditional algorithms like those developed by Nivre (2003, 2004) for transition-based parsing only allow the construction of projective trees. These algorithms make use of a stack, a buffer and a set of arcs, and parsing consists of performing a sequence of transitions on these structures. Traditional algorithms have been extended in different ways to allow the construction of non-projective trees (Nivre and Nilsson, 2005; Attardi, 2006; Nivre, 2007; G´omez-Rodr´ıguez and Nivre, 2010). One method proposed by Nivre (2009) is based on the idea of word reordering. This is achieved by adding a transition that swaps two items in the data structures used, enabling the construction of arbitrary non-projective trees while still only adding arcs between adjacent words (after possible reordering). This technique was previously used in the arc-standard transition system (Nivre, 2004). The first contribution of this paper is to show that it can also be combined with the arc-hybrid system 99 Proceedings of the 15th International Conference on Parsing"
W18-6305,L16-1100,1,0.826254,"rough human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work has specifically targeted the differences in performance between NMT and SMT (Burlot and Yvon, 2017; Sennrich, 2017). There are also other types of error analysis targeting this difference, e.g. bas"
W18-6305,N18-1118,0,0.0110707,"tems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific err"
W18-6305,guillou-etal-2014-parcor,1,0.901195,"Missing"
W18-6305,D16-1025,0,0.0160536,"comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work has specifically targeted the differences in performance between NMT and SMT (Burlot and Yvon, 2017; Sennrich, 2017). There are also other types of error analysis targeting this difference, e.g. based on post-edits (Bentivogli et al., 2016). For Croatian in particular, Klubiˇcka et al. (2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along with the observations based on the manual data analysis. 4.1 Parallel Corpora As the use of coreference phenomena varies across different registers and text types, we decided to perform the analysis on corpora from three different domains: • DGT-TM (Steinberger et al., 2012): EU legal texts, 950K sentences • SETIMES2 (Tiedemann, 2009):"
W18-6305,2010.iwslt-papers.10,1,0.808379,"ulated through word order, which makes pleonastic pronouns largely redundant in Croatian. Finally, it does not easily create participial constructions, preferring to elaborate the concise English participial expressions into full, finite relative clauses using the relative pronoun koji. 37 ysis of SMT and NMT systems, finding that the translation of function words in general is considerably improved in NMT. However, they do not present separate results for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomen"
W18-6305,W15-2501,1,0.851562,"sis of SMT and NMT systems, finding that the translation of function words in general is considerably improved in NMT. However, they do not present separate results for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall sc"
W18-6305,P17-4012,0,0.0119387,"one of the authors, who is a native speaker of Croatian. To reflect the scalar nature of error severity, we assign a penalty to each error category. This also enables us to produce a provisional score for relative comparison and evaluation of the systems. Some clarification might be needed for categories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is largely focused on intra-sentential phenomen"
W18-6305,W17-4705,0,0.0120542,"mains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work ha"
W18-6305,P07-2045,0,0.00690842,"in Table 3. The evaluation was performed by one of the authors, who is a native speaker of Croatian. To reflect the scalar nature of error severity, we assign a penalty to each error category. This also enables us to produce a provisional score for relative comparison and evaluation of the systems. Some clarification might be needed for categories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is"
W18-6305,N03-1017,0,0.054564,"d The three datasets cover an interesting range from very formal, strictly standardized and highly repetitive texts (DGT) to fairly loose and informal translation of speeches (TedTalks). For the purposes of the analyses, English is taken as the source and Croatian as the target language. The corpora were tokenized, tagged for parts of speech and parsed using the pre-trained models for the respective languages developed for the annotation pipeline UDPipe (2017). The parallel data were then aligned at word-level with efmaral (Östling and Tiedemann, 2016), using the grow-diag-finaland heuristic (Koehn et al., 2003). Relying on the approach of LapshinovaKoltunski and Hardmeier (2017), we used POStags and dependency information to extract a highrecall list of pronouns and determiners in both languages, in order to identify potentially interesting coreference patterns. The main criterion for their extraction was the pron or det tag, as the original research has found this approach to permit reliable identification of phenomena, even with multi-word units. Similarly to LapshinovaKoltunski and Hardmeier (2017), we couple the 38 POS-tags with syntactic information to create linguistic patterns in the format l"
W18-6305,E12-3001,0,0.021885,"akes pleonastic pronouns largely redundant in Croatian. Finally, it does not easily create participial constructions, preferring to elaborate the concise English participial expressions into full, finite relative clauses using the relative pronoun koji. 37 ysis of SMT and NMT systems, finding that the translation of function words in general is considerably improved in NMT. However, they do not present separate results for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-awa"
W18-6305,E17-2060,0,0.0163927,"y do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct and an incorrect translation of an input sentence, whereas Sennrich (2017) describes a ranking approach for evaluating NMT systems on grammaticality. Some of the above work has specifically targeted the differences in performance between NMT and SMT (Burlot and Yvon, 2017; Sennrich, 2017). There are also other types of error analysis targeting this difference, e.g. based on post-edits (Bentivogli et al., 2016). For Croatian in particular, Klubiˇcka et al. (2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along"
W18-6305,W17-4810,1,0.859017,"ivalent counterpart in the other language. We use the same procedure to automatically extract phenomena, but extend the methodology to include cases where the phenomenon does have an equivalent construction in the other language, despite the alignment data suggesting that it is more frequently left unaligned. In this research, we perform an in-depth study of the way in which diverse discourse phenomena are handled in translation from English to Croatian. We investigate both human translation and the output of different types of MT systems. In the first step, we use the extended methodology of Lapshinova-Koltunski and Hardmeier (2017) to extract interesting diverging discourse patterns that commonly occur in the parallel data. While reflections on the relevant linguistic intuitions are given as a reference, the selection of the phenomena chosen for further examination is primarily based on the data obtained from corpora. This makes our approach strongly usage-based and provides ample space for making observations unconstrained by a particular theoretical framework. In the second step, we construct a dataset with sentences containing challenging discourse phenomena identified in the analysis of human translations. The const"
W18-6305,P16-1162,0,0.0117969,"ategories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is largely focused on intra-sentential phenomena. Although the segmented nature of the artificially constructed test set might be considered a constraint, it is difficult to find an alternative way of testing such a variety of phenomena, while retaining as much data as possible for training. 4 http://hdl.handle.net/11234/1-2855 5 Error Analysis"
W18-6305,W10-1737,0,0.0804279,"Missing"
W18-6305,steinberger-etal-2012-dgt,0,0.0246511,"r analysis targeting this difference, e.g. based on post-edits (Bentivogli et al., 2016). For Croatian in particular, Klubiˇcka et al. (2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along with the observations based on the manual data analysis. 4.1 Parallel Corpora As the use of coreference phenomena varies across different registers and text types, we decided to perform the analysis on corpora from three different domains: • DGT-TM (Steinberger et al., 2012): EU legal texts, 950K sentences • SETIMES2 (Tiedemann, 2009): newspaper articles, 200K sentences • TedTalks (Tiedemann, 2012): speeches, 86K sentences prepared The three datasets cover an interesting range from very formal, strictly standardized and highly repetitive texts (DGT) to fairly loose and informal translation of speeches (TedTalks). For the purposes of the analyses, English is taken as the source and Croatian as the target language. The corpora were tokenized, tagged for parts of speech and parsed using the pre-trained models for the respective languages developed for the annotation"
W18-6305,D15-1166,0,0.0122494,"o produce a provisional score for relative comparison and evaluation of the systems. Some clarification might be needed for categories 4 to 6. Agreement error means that the phenomenon does not agree with the grammati5.2 MT Systems For the experiment we trained a baseline SMT system and several baseline NMT systems. We used open-source toolkits, the phrase-based SMT package Moses (Koehn et al., 2007) and the OpenNMT toolkit (Klein et al., 2017) respectively, and followed the standard training procedures. The NMT systems were based on a sequence-to-sequence architecture with general attention (Luong et al., 2015) and were trained for 13 epochs. We also experimented with sub-word segmentation with byte pair encoding (Sennrich et al., 2016), trained both individually and jointly, for which 10,000 operations were performed. However, only the two models with the highest BLEU scores were retained for the manual analysis. An overview of 3 Due to the nature of the extraction process, the study is largely focused on intra-sentential phenomena. Although the segmented nature of the artificially constructed test set might be considered a constraint, it is difficult to find an alternative way of testing such a va"
W18-6305,K17-3009,0,0.0331541,"Missing"
W18-6305,W13-3303,0,0.0214506,"urse phenomena in translation. Given the immense variety of linguistic phenomena that fall within the scope of the term, research on discourse phenomena in translation has often focused on a limited group of phenomena (e.g. Furkó, 2014; Zinsmeister et al., 2012; Bührig and House, 2004), which frequently have to be studied in reference to particular registers (Kunz and LapshinovaKoltunski, 2015). Moreover, the pronouncedly language-specific character of their form has led to examinations of explicitation and implicitation of these phenomena in translation (Blum-Kulka, 1986). On a similar note, Meyer and Webber (2013) compare implicitation tendencies in human and machine translation and find that the latter displays more cases where the phenomena are kept in translation. Scarton and Specia (2015) assess the impact of discourse structures on MT quality through quantitative analysis, while Lapshinova-Koltunski (2017) compares human and machine translations to identify and describe variation in the distribution of different cohesive devices. On the other hand, a variety of approaches have also been proposed to incorporate discursive inforMotivation As a South Slavic language, Croatian is a morphologically ric"
W18-6305,stymne-ahrenberg-2012-practice,1,0.840746,"ntext-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, developed by Guillou and Hardmeier (2016), enables relative comparisons between MT systems in terms of pronoun translation. Bawden et al. (2018) construct a contrastive test set to evaluate anaphoric pronouns, cohesion and coherence by having NMT systems rank a correct a"
W18-6305,W17-4811,0,0.0109415,". mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error an"
W18-6305,tiedemann-2012-parallel,0,0.0657745,"(2017) conducted an error anal4 Human Translation Analysis In this section we give an overview of the used datasets and their preprocessing. We also describe the extraction process and the selected phenomena, along with the observations based on the manual data analysis. 4.1 Parallel Corpora As the use of coreference phenomena varies across different registers and text types, we decided to perform the analysis on corpora from three different domains: • DGT-TM (Steinberger et al., 2012): EU legal texts, 950K sentences • SETIMES2 (Tiedemann, 2009): newspaper articles, 200K sentences • TedTalks (Tiedemann, 2012): speeches, 86K sentences prepared The three datasets cover an interesting range from very formal, strictly standardized and highly repetitive texts (DGT) to fairly loose and informal translation of speeches (TedTalks). For the purposes of the analyses, English is taken as the source and Croatian as the target language. The corpora were tokenized, tagged for parts of speech and parsed using the pre-trained models for the respective languages developed for the annotation pipeline UDPipe (2017). The parallel data were then aligned at word-level with efmaral (Östling and Tiedemann, 2016), using t"
W18-6305,P02-1040,0,0.101538,"n techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There have been attempts at error analysis where specific errors are identified and classified into typologies (Vilar et al., 2006; Stymne and Ahrenberg, 2012; Comelles et al., 2016), but these classifications usually do not target discourserelated phenomena. Taking a more specific approach to MT evaluation, Burlot and Yvon (2017) describe how test suites can be created and used automatically for the evaluation of MT systems on morphological phenomena, while the test suite PROTEST, develo"
W18-6305,vilar-etal-2006-error,0,0.0949915,"Missing"
W18-6305,P18-1117,0,0.0104849,"lts for pronouns or other elements with coreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT"
W18-6305,D17-1301,0,0.0113975,"oreference functions. mation in the workflow of MT systems. The approaches of Le Nagard and Koehn (2010), Hardmeier and Federico (2010) and Guillou (2012) are based on the projection of the source side annotation of coreferring pronouns. A number of discourse-oriented pronoun prediction systems, statistical and rule-based, have also been developed for the submission for the DiscoMT shared task (Hardmeier et al., 2015). The systems experimented with different coreference resolution techniques to improve the translation of pronouns. In recent approaches, Voita et al. (2018), Jean et al. (2017), Wang et al. (2017), Tiedemann and Scherrer (2017) and Bawden et al. (2018) all attempt to improve the translation of discourse phenomena using context-aware NMT systems. Although the degree of their success varies, all papers notably report improvement over the baseline systems. However, the evaluation of these systems remains problematic, as MT evaluation research has typically been focused on providing an overall score for documents, either through automatic metrics like BLEU (Papineni et al., 2002), or through human evaluation, such as the ranking of systems in the WMT evaluations (Bojar et al., 2017). There"
