2020.acl-main.270,D19-1223,0,0.0301916,"Missing"
2020.acl-main.270,P19-1285,0,0.417687,"sublayers. When k = 0, we get the original transformer model, and when k = n − 1 (its maximal value) we get the previously mentioned s n f n model. We refer to k as the transformer’s sandwich coefficient. We train sandwich transformers for n = 16 (to remain within the same parameter budget as our baseline language model) and all values of k ∈ {0, . . . , 15}. Figure 5 shows the transformer’s performance as a function of the sandwich coefficient k. With the exception of k = 14, 15, all sandwich transformers achieve lower perplexities 2999 Test Baseline (Baevski and Auli, 2019) Transformer XL (Dai et al., 2019) kNN-LM (Khandelwal et al., 2019) 18.70 18.30 15.79 Baseline (5 Runs) Sandwich16 6 18.63 ± 0.26 17.96 19.00 18.75 Perplexity Model Table 3: Performance on the WikiText-103 test set. We compare the best sandwich transformer to the unmodified, interleaved transformer baseline (Baevski and Auli, 2019) trained over 5 random seeds and to other previously reported results. 5 One Reordering to Rule Them All? The sandwich transformer is a manually-crafted pattern motivated by the performance of random 18.25 18.00 17.75 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 Sandwich Coefficient Figure 5: The transformer’"
2020.acl-main.270,N19-1423,0,0.232145,"ded in order to unlock additional gains.1 (a) Interleaved Transformer s ss s s s s f sf s f s f sf s f s f s ff f f f f f (b) Sandwich Transformer Figure 1: A transformer model (a) is composed of interleaved self-attention (green) and feedforward (purple) sublayers. Our sandwich transformer (b), a reordering of the transformer sublayers, performs better on language modeling. Input flows from left to right. Introduction The transformer layer (Vaswani et al., 2017) is currently the primary modeling component in natural language processing, playing a lead role in recent innovations such as BERT (Devlin et al., 2019) and GPT-2 (Radford et al., 2019). Each transformer layer consists of a self-attention sublayer ( s ) followed by a feedforward sublayer ( f ), creating an interleaving pattern of self-attention and feedforward sublayers ( s f s f s f · · · ) throughout a multilayer transformer model. To the best of our knowledge, there is no reason to expect this particular pattern to be optimal. We conduct a series of explorations to obtain insights about the nature of transformer orderings that work well, and based on this, we 1 Our code is available at https://github.com/ ofirpress/sandwich_transformer des"
2020.acl-main.270,W18-6301,0,0.146902,"self-attention ( s ) and cross-attention ( c ) sublayers, and treat them as a single unit for reordering purposes ( s c ). For example, a three layer decoder ( s c f sc f s c f ) with a sandwiching coefficient of k = 1 would be: s c sc f s c f f . We apply the sandwich pattern to either the encoder or decoder separately, while keeping the other stack in its original interleaved pattern. Experiment Setting As a baseline, we use the large transformer model (6 encoder/decoder layers, embedding size of 1024, feedforward inner dimension of 4096, and 16 attention heads) with the hyperparameters of Ott et al. (2018). We also follow their setup for training and evaluation: we train on the WMT 2014 En-De dataset which contains 4.5M sentence pairs; we validate on newstest13 and test on newstest14. We use a vocabulary of 32K symbols based on a joint source and target byte pair encoding (Sennrich et al., 2016). For inference we use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. (2017) and Ott et al. (2018). As before, we do not modify our model’s hyperparameters or training procedure. Results Table 6 shows that reordering of either the encoder or decoder does not have a"
2020.acl-main.270,E17-2025,1,0.883093,"i and Auli (2019). Specifically, we use the Toronto Books Corpus (Zhu et al., 2015), which has previously been used to train GPT (Radford et al., 2018) and also BERT (Devlin et al., 2019) (combined with Wikipedia). The corpus contains roughly 700M tokens. We use the same train/validation/test split as Khandelwal et al. (2019), as well as their tokenization, which uses BERT’s vocabulary of 29K byte-pair encodings. Since the vocabulary is much smaller than WikiText-103’s, we replace the adaptive word embedding and softmax of Baevski and Auli (2019) with a tied word embedding and softmax matrix (Press and Wolf, 2017; Inan et al., 2017). Finally, we tune the sandwich coefficient on the development set for k ∈ {4, . . . , 8}, i.e., a neighborhood of 2 around the best value we found for WikiText-103 (k = 6). Table 4 shows that the sandwich transformer transfers well to the books domain, improving performance by 1.06 perplexity, achieving similar performance to the datastore-augmented kNN-LM (Khandelwal et al., 2019), which is the state of the art on WikiText-103 (see Section 4). 5.2 Character-level Language Modeling Modeling text as a stream of characters, rather than word or subword tokens, presents a diff"
2020.acl-main.270,P16-1162,0,0.0648078,"he encoder or decoder separately, while keeping the other stack in its original interleaved pattern. Experiment Setting As a baseline, we use the large transformer model (6 encoder/decoder layers, embedding size of 1024, feedforward inner dimension of 4096, and 16 attention heads) with the hyperparameters of Ott et al. (2018). We also follow their setup for training and evaluation: we train on the WMT 2014 En-De dataset which contains 4.5M sentence pairs; we validate on newstest13 and test on newstest14. We use a vocabulary of 32K symbols based on a joint source and target byte pair encoding (Sennrich et al., 2016). For inference we use beam search with a beam width of 4 and length penalty of 0.6, following Vaswani et al. (2017) and Ott et al. (2018). As before, we do not modify our model’s hyperparameters or training procedure. Results Table 6 shows that reordering of either the encoder or decoder does not have a significant impact on performance, across the board. We also find that using the most extreme sandwich decoder ( s c )6 f 6 performs almost exactly the same as the average baseline; this result is consistent with our observation from Section 4, where we show that the extreme sandwich language"
2020.acl-main.270,P19-1032,0,0.278645,"= 6). Table 4 shows that the sandwich transformer transfers well to the books domain, improving performance by 1.06 perplexity, achieving similar performance to the datastore-augmented kNN-LM (Khandelwal et al., 2019), which is the state of the art on WikiText-103 (see Section 4). 5.2 Character-level Language Modeling Modeling text as a stream of characters, rather than word or subword tokens, presents a different modeling challenge: long-range dependencies become critical, and the vocabulary takes on a more uniform distribution. We apply our sandwich reordering to the adaptive span model of Sukhbaatar et al. (2019), which is state of the art on the popular English-language benchmark text8 and is currently a close second on enwik8.3 The adaptive span 3 Both datasets are taken from http://mattmahoney. net/dc/textdata.html model learns to control each attention head’s maximal attention span, freeing up memory in the bottom layers (which typically need very short attention spans) and applying it to the top layers, allowing the top-level attention heads to reach significantly longer distances. The adaptive span model’s efficient use of attention also results in a significant speed boost. We tune the sandwich"
2020.acl-main.270,D19-1083,0,0.0315578,"Missing"
2020.acl-main.703,I05-5002,0,0.0666519,"Missing"
2020.acl-main.703,N19-1409,0,0.0820284,"Missing"
2020.acl-main.703,P19-1346,0,0.0551333,"Missing"
2020.acl-main.703,2020.tacl-1.5,1,0.898817,"Missing"
2020.acl-main.703,P17-1099,0,0.560122,"Missing"
2020.acl-main.703,W16-2323,0,0.0872041,"Missing"
2020.acl-main.703,D13-1170,0,0.0103825,"Missing"
2020.acl-main.703,D18-1206,0,0.21281,"Missing"
2020.acl-main.703,W18-5446,1,0.889474,"Missing"
2020.acl-main.703,N18-1202,1,0.621418,"Missing"
2020.acl-main.703,N18-1101,0,0.138739,"Missing"
2020.acl-main.703,D16-1264,0,0.368154,"Missing"
2020.findings-emnlp.232,D19-1223,0,0.0157011,"ccessful application to masked language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), several approaches have been proposed to simplify the model and its training process. We summarize these attempts as follows: Attention layer simplification There are currently two lines of research trying to simplify the multi-head attention layers. The first one focuses on attention matrix sparsification. Notable examples include Star Transformer (Guo et al., 2019), Sparse Transformer (Child et al., 2019), Adaptive Sparse Transformer (Correia et al., 2019; Sukhbaatar et al., 2019), Log-Sparse Transformer (Li et al., 2019) , Reformer (Kitaev et al., 2020) and Longformer (Beltagy et al., 2020). However, due to the insufficient support for sparse tensors from the current deep learning platforms, some 2561 SearchQA EM F1 TriviaQA EM F1 NewsQA EM F1 NaturalQA EM F1 HotpotQA EM F1 N Model 512 Google BERT RoBERTa-2seq RoBERTa-1seq SparseB ERT BlockB ERT n=2 BlockB ERT n=3 74.94 76.12 77.09 73.36 76.68 75.54 80.37 81.74 82.62 79.01 82.33 81.07 70.18 71.92 73.65 68.71 72.36 72.05 75.35 76.79 78.22 73.15 77.53 76.74 51.27 52.45 56.13 51.18 54.66 53.82 6"
2020.findings-emnlp.232,N19-1423,0,0.522195,"et al., 2017), the building block of BERT. As the attention operation is quadratic to the sequence length, this fundamentally limits the maximum length of the input sequence, and thus restricts the model capacity in terms of capturing long-distance dependencies. As a result, downstream tasks have to either truncate their sequences to leading tokens (Nogueira and Cho, 2019) or split their sequences with a sliding window (Joshi et al., 2019a,b). Ad-hoc handling of long sequences is also required in the pre-training stage, such as updating the model using only short sequences in the early stage (Devlin et al., 2019). Common strategies for reducing memory consumption, unfortunately, do not work. For instance, 1 github.com/google-research/bert 2555 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555–2565 c November 16 - 20, 2020. 2020 Association for Computational Linguistics shrinking the model by lowering the number of layers L, attention heads A, or hidden units H leads to significant performance degradation (Vaswani et al., 2017; Devlin et al., 2019) and does not address the long sequence issue. Alternatively, general low-memory training techniques, such as microbatching ("
2020.findings-emnlp.232,P84-1044,0,0.238639,"Missing"
2020.findings-emnlp.232,D18-1325,0,0.0732224,"Missing"
2020.findings-emnlp.232,P17-1147,0,0.071275,"Missing"
2020.findings-emnlp.232,D19-1588,1,0.912101,"9; Brown et al., 2020), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019), has drastically reshaped the landscape of the natural language processing research. These methods first pre-train a deep model with language model objectives using a large corpus and then fine-tune the model using in-domain supervised data for target applications. Despite its conceptual simplicity, this paradigm has re-established the new state-of-theart baselines across various tasks, such as question answering (Devlin et al., 2019), coreference resolution (Joshi et al., 2019b), relation extraction (Soares et al., 2019) and text retrieval (Lee et al., 2019; Nogueira and Cho, 2019), to name a few. ⇤ This work was partially done when the first author was an intern at Facebook AI. Code is available at https:// github.com/xptree/BlockBERT Building such models in practice, however, is an extremely resource-intensive process. For instance, the training of BERT-family models is notoriously expensive. Devlin et al. (2019) report that it takes four days to pre-train BERT-Base/BERTLarge on 4/16 Cloud TPUs. In order to reduce the pre-training time of RoBERTa to 1 day, Liu et"
2020.findings-emnlp.232,Q19-1026,0,0.0321512,"Missing"
2020.findings-emnlp.232,N19-4009,0,0.0267825,"an see that for BERT-Base, the O(N 2 ) component accounts for 3.66 GB, and the O(N ) component accounts for 4.83 GB. When the sequence length N increases to 1024 (i.e., b = 4), the O(N 2 ) component increases to 7.32 GB, while the O(N ) part is unchanged. 2.3 Techniques for Reducing Traing Memory Observing that activation memory is the training bottleneck, we discuss common memory reduction techniques below. Low Precision (Micikevicius et al., 2017) Low precision is to use half-precision/mixed-precision for training neural networks. This technique has been widely used in Transformer training (Ott et al., 2019; Liu et al., 2019). In this work, we already Microbatching (Huang et al., 2018) Microbatching is to split a batch into small microbatches (which can be fit into memory), and then run forward and backward passes on them separately with gradients for each micro-batch accumulated. Because it runs forward/backward pass multiple times for a single batch, it trades off time for memory. Gradient Checkpointing (Chen et al., 2016) Gradient checkpointing saves memory by only caching activations of a subset of layers. The un-cached activations will be recomputed during backpropagation from the latest ch"
2020.findings-emnlp.232,N18-1202,0,0.120894,"Missing"
2020.findings-emnlp.232,P18-2124,0,0.0588447,"Missing"
2020.findings-emnlp.232,P19-1612,0,0.0297355,"Missing"
2020.findings-emnlp.232,2021.ccl-1.108,0,0.0678144,"Missing"
2020.findings-emnlp.232,A94-1016,0,0.159416,"Missing"
2020.findings-emnlp.232,P19-1279,0,0.0235209,", 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019), has drastically reshaped the landscape of the natural language processing research. These methods first pre-train a deep model with language model objectives using a large corpus and then fine-tune the model using in-domain supervised data for target applications. Despite its conceptual simplicity, this paradigm has re-established the new state-of-theart baselines across various tasks, such as question answering (Devlin et al., 2019), coreference resolution (Joshi et al., 2019b), relation extraction (Soares et al., 2019) and text retrieval (Lee et al., 2019; Nogueira and Cho, 2019), to name a few. ⇤ This work was partially done when the first author was an intern at Facebook AI. Code is available at https:// github.com/xptree/BlockBERT Building such models in practice, however, is an extremely resource-intensive process. For instance, the training of BERT-family models is notoriously expensive. Devlin et al. (2019) report that it takes four days to pre-train BERT-Base/BERTLarge on 4/16 Cloud TPUs. In order to reduce the pre-training time of RoBERTa to 1 day, Liu et al. (2019) use 1,024 V100 GPUs. One crucial"
2020.findings-emnlp.232,P19-1032,0,0.0408141,"o masked language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), several approaches have been proposed to simplify the model and its training process. We summarize these attempts as follows: Attention layer simplification There are currently two lines of research trying to simplify the multi-head attention layers. The first one focuses on attention matrix sparsification. Notable examples include Star Transformer (Guo et al., 2019), Sparse Transformer (Child et al., 2019), Adaptive Sparse Transformer (Correia et al., 2019; Sukhbaatar et al., 2019), Log-Sparse Transformer (Li et al., 2019) , Reformer (Kitaev et al., 2020) and Longformer (Beltagy et al., 2020). However, due to the insufficient support for sparse tensors from the current deep learning platforms, some 2561 SearchQA EM F1 TriviaQA EM F1 NewsQA EM F1 NaturalQA EM F1 HotpotQA EM F1 N Model 512 Google BERT RoBERTa-2seq RoBERTa-1seq SparseB ERT BlockB ERT n=2 BlockB ERT n=3 74.94 76.12 77.09 73.36 76.68 75.54 80.37 81.74 82.62 79.01 82.33 81.07 70.18 71.92 73.65 68.71 72.36 72.05 75.35 76.79 78.22 73.15 77.53 76.74 51.27 52.45 56.13 51.18 54.66 53.82 66.25 66.73 70.64 65.47 69."
2020.findings-emnlp.232,W17-2623,0,0.0306333,"ce Prediction) task, and an input sequence is up to N tokens until it reaches a document boundary. A summary of the pre-training performance comparison between BlockB ERT and RoBERTa-1seq is shown in Table 2. Besides memory saving, we also achieve a significant speedup. For example, when N = 1024, BlockB ERT (n = 2) reduces the training time from RoBERTa’s 9.7 days to 7.5 days. 4.2 Fine-tuning Tasks We evaluate BlockB ERT on several question answering tasks, including SQuAD 1.1/2.0 (Rajpurkar et al., 2018) and five other tasks from the MrQA shared task7 — HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019). Since MrQA does not have an official test set, we follow Joshi et al. (2019a) to split the devel6 We adopt Sparse Transformer implemented by Fairseq, which first computes the N ⇥ N attention matrix, and then masks it to be a sparse one. This implementation cannot avoid the O(N 2 ) attention computation, and thus has a similar training time/memory cost to RoBERTa. 7 mrqa.github.io 2559 N Model Training Time (day) Memory (per GPU, GB) Heads Config. Valid. ppl 512 RoBERTa-1seq BlockB ERT n=2 Bl"
2020.findings-emnlp.232,P19-1580,0,0.0579792,"Missing"
2020.findings-emnlp.232,D18-1259,0,0.0252582,"we drop the NSP (Next Sentence Prediction) task, and an input sequence is up to N tokens until it reaches a document boundary. A summary of the pre-training performance comparison between BlockB ERT and RoBERTa-1seq is shown in Table 2. Besides memory saving, we also achieve a significant speedup. For example, when N = 1024, BlockB ERT (n = 2) reduces the training time from RoBERTa’s 9.7 days to 7.5 days. 4.2 Fine-tuning Tasks We evaluate BlockB ERT on several question answering tasks, including SQuAD 1.1/2.0 (Rajpurkar et al., 2018) and five other tasks from the MrQA shared task7 — HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019). Since MrQA does not have an official test set, we follow Joshi et al. (2019a) to split the devel6 We adopt Sparse Transformer implemented by Fairseq, which first computes the N ⇥ N attention matrix, and then masks it to be a sparse one. This implementation cannot avoid the O(N 2 ) attention computation, and thus has a similar training time/memory cost to RoBERTa. 7 mrqa.github.io 2559 N Model Training Time (day) Memory (per GPU, GB) Heads Config. Valid. ppl 5"
2020.tacl-1.5,N19-1423,0,0.486582,"n boundary and the regular masked language model objectives for each token xi in the masked span (xs , . . . , xe ), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across diff"
2020.tacl-1.5,D19-5801,1,0.908682,"Missing"
2020.tacl-1.5,S17-2001,0,0.0656926,"Missing"
2020.tacl-1.5,W07-1401,0,0.151879,"ks including MRPC (Dolan and Brockett, 2005), a binary paraphrasing task sentence pairs from news sources, STS-B (Cer et al., 2017), a graded similarity task for news headlines, and QQP,6 a binary paraphrasing tasking between Quora question pairs. es(x,y) s(x,y  ) y  ∈Y e P (y ) =  The span pair scoring function s(x, y ) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y : • Four natural language inference tasks including MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), and WNLI (Levesque et al., 2011). s(x, y ) = sm (x) + sm (y ) + sc (x, y ) sm (x) = FFNN m (gx ) sc (x, y ) = FFNN c (gx , gy , φ(x, y )) Unlike question answering, coreference resolution, and relation extraction, these sentence-level tasks do not require explicit modeling of spanlevel semantics. However, they might still benefit from implicit span-based reasoning (e.g., the Prime Minister is the head of the government). Following previous work (Devlin et al., 2019; Radford et al., 2018),7 we exclude WNLI from the results to enable a fair comparison. Although recent work Liu et al. (2019a) h"
2020.tacl-1.5,P18-2058,1,0.849186,"19), but deviates from its bi-text classification framework in three ways. First, we use a different random process to mask spans of tokens, rather than individual ones. We also introduce a novel auxiliary objective—the SBO—which tries to predict the entire masked span using only the representations of the tokens at the span’s boundary. Finally, SpanBERT samples a single contiguous segment of text for each training example (instead of two), and thus does not use BERT’s next sentence prediction objective, which we omit. 3.2 Span Boundary Objective Span selection models (Lee et al., 2016, 2017; He et al., 2018) typically create a fixed-length representation of a span using its boundary tokens (start and end). To support such models, we would ideally like the representations for the end of the span to summarize as much of the internal span content as possible. We do so by introducing a span boundary objective that involves predicting each token of a masked span using only the representations of the observed tokens at the boundaries (Figure 1). Formally, we denote the output of the transformer encoder for each token in the sequence by x1 , . . . , xn . Given a masked span of tokens (xs , . . . , xe )"
2020.tacl-1.5,P19-1285,0,0.030626,"ping (Song et al., 2019; Chan et al., 2019) multiple words from the input—particularly as pretraining for lan72 (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering. substantially better performance on span selection tasks in particular. Concurrent with our work, RoBERTa (Liu et al., 2019b) presents a replication study of BERT pre-training that measures the impact of many key hyperparameters and training data size. Also concurrent, XLNet (Yang et al., 2019) combines an autoregressive loss and the Transformer-XL (Dai et al., 2019) architecture with a more than an eight-fold increase in data to achieve current stateof-the-art results on multiple benchmarks. XLNet also masks spans (of 1–5 tokens) during pretraining, but predicts them autoregressively. Our model focuses on incorporating span-based pretraining, and as a side effect, we present a stronger BERT baseline while controlling for the corpus, architecture, and the number of parameters. Appendices A Pre-training Procedure We describe our pre-training procedure as follows: 1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre"
2020.tacl-1.5,P18-1031,0,0.0617737,"Missing"
2020.tacl-1.5,N19-1362,1,0.921184,"at, e.g., all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept. 67 answer span to be the special token [CLS] for both training and testing. was born in [OBJ-LOC] , Michigan, . . . ’’, and finally add a linear classifier on top of the [CLS] token to predict the relation type. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coreference model (Lee et al., 2018). The document is divided into non-overlapping segments of a pre-defined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of 9 sentence-level classification tasks: • Two sentence-level classification tasks including CoLA (Warstadt et al., 2018)"
2020.tacl-1.5,P17-1147,1,0.858746,"MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1 , p2 , . . . , pl ) and question Q = (q1 ,"
2020.tacl-1.5,D19-1588,1,0.819519,"at, e.g., all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept. 67 answer span to be the special token [CLS] for both training and testing. was born in [OBJ-LOC] , Michigan, . . . ’’, and finally add a linear classifier on top of the [CLS] token to predict the relation type. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coreference model (Lee et al., 2018). The document is divided into non-overlapping segments of a pre-defined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of 9 sentence-level classification tasks: • Two sentence-level classification tasks including CoLA (Warstadt et al., 2018)"
2020.tacl-1.5,P19-1441,0,0.137858,"iccolo et al., 2007), and WNLI (Levesque et al., 2011). s(x, y ) = sm (x) + sm (y ) + sc (x, y ) sm (x) = FFNN m (gx ) sc (x, y ) = FFNN c (gx , gy , φ(x, y )) Unlike question answering, coreference resolution, and relation extraction, these sentence-level tasks do not require explicit modeling of spanlevel semantics. However, they might still benefit from implicit span-based reasoning (e.g., the Prime Minister is the head of the government). Following previous work (Devlin et al., 2019; Radford et al., 2018),7 we exclude WNLI from the results to enable a fair comparison. Although recent work Liu et al. (2019a) has applied several task-specific strategies to increase performance on the individual GLUE tasks, we follow BERT’s single-task setting and only add a linear classifier on top of the [CLS] token for these classification tasks. Here gx and gy denote the span representations, which are a concatenation of the two transformer output states of the span endpoints and an attention vector computed over the output representations of the token in the span. FFNNm and FFNNc represent two feedforward neural networks with one hidden layer, and φ(x, y ) represents the handengineered features (e.g., speake"
2020.tacl-1.5,Q19-1026,0,0.0481222,"and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1 , p2 , . . . , pl ) and question Q = (q1 , q2 , . . . , ql ) into a single sequence X = [CLS]p1 p2 . . . pl [SEP] q1 q2 ."
2020.tacl-1.5,K16-1006,0,0.0787294,"Missing"
2020.tacl-1.5,D17-1018,1,0.83938,"Missing"
2020.tacl-1.5,N19-4009,0,0.0302846,"re information). A more detailed description of the model can be found in Joshi et al. (2019b). Relation Extraction TACRED (Zhang et al., 2017) is a challenging relation extraction dataset. Given one sentence and two spans within it— subject and object—the task is to predict the relation between the spans from 42 pre-defined relation types, including no relation. We follow the entity masking schema from Zhang et al. (2017) and replace the subject and object entities by their NER tags such as ‘‘[CLS] [SUBJ-PER] 4.2 Implementation We reimplemented BERT’s model and pretraining method in fairseq (Ott et al., 2019). 6 https://data.quora.com/First-QuoraDataset-Release-Question-Pairs. 7 Previous work has excluded WNLI on account of construction issues outlined on the GLUE website – https:// gluebenchmark.com/faq. The length was chosen from {128, 256, 384, 512}. See more details in Appendix B. 5 68 We used the model configuration of BERTlarge as in Devlin et al. (2019) and also pre-trained all our models on the same corpus: BooksCorpus and English Wikipedia using cased Wordpiece tokens. Compared with the original BERT implementation, the main differences in our implementation include: (a) We use different"
2020.tacl-1.5,N18-1202,1,0.849243,"Missing"
2020.tacl-1.5,W12-4501,0,0.659074,"Missing"
2020.tacl-1.5,E17-2025,0,0.0297017,"t al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. h0 = [xs−1 ; xe+1 ; pi−s+1 ] h1 = LayerNorm (GeLU(W1 h0 )) yi = LayerNorm (GeLU(W2 h1 )) We then use the vector representation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs , . . . , xe ), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA ("
2020.tacl-1.5,W17-2623,0,0.0605729,"ut embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert t"
2020.tacl-1.5,P18-2124,0,0.0900822,"Missing"
2020.tacl-1.5,D16-1264,0,0.546497,"presentation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs , . . . , xe ), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO: Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain"
2020.tacl-1.5,P19-1279,0,0.0469656,"Missing"
2020.tacl-1.5,N18-1101,0,0.0381691,"lity and SST-2 (Socher et al., 2013) for sentiment classification. • Three sentence-pair similarity tasks including MRPC (Dolan and Brockett, 2005), a binary paraphrasing task sentence pairs from news sources, STS-B (Cer et al., 2017), a graded similarity task for news headlines, and QQP,6 a binary paraphrasing tasking between Quora question pairs. es(x,y) s(x,y  ) y  ∈Y e P (y ) =  The span pair scoring function s(x, y ) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y : • Four natural language inference tasks including MNLI (Williams et al., 2018), QNLI (Rajpurkar et al., 2016), RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007), and WNLI (Levesque et al., 2011). s(x, y ) = sm (x) + sm (y ) + sc (x, y ) sm (x) = FFNN m (gx ) sc (x, y ) = FFNN c (gx , gy , φ(x, y )) Unlike question answering, coreference resolution, and relation extraction, these sentence-level tasks do not require explicit modeling of spanlevel semantics. However, they might still benefit from implicit span-based reasoning (e.g., the Prime Minister is the head of the government). Following previous work (Devlin et al., 2019; Radford et al., 2018)"
2020.tacl-1.5,D13-1170,0,0.00994137,"odel (Lee et al., 2018). The document is divided into non-overlapping segments of a pre-defined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) consists of 9 sentence-level classification tasks: • Two sentence-level classification tasks including CoLA (Warstadt et al., 2018) for evaluating linguistic acceptability and SST-2 (Socher et al., 2013) for sentiment classification. • Three sentence-pair similarity tasks including MRPC (Dolan and Brockett, 2005), a binary paraphrasing task sentence pairs from news sources, STS-B (Cer et al., 2017), a graded similarity task for news headlines, and QQP,6 a binary paraphrasing tasking between Quora question pairs. es(x,y) s(x,y  ) y  ∈Y e P (y ) =  The span pair scoring function s(x, y ) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y : • Four natural language inference tasks including MNLI (Williams et al., 2018), QNLI (Rajpur"
2020.tacl-1.5,2020.emnlp-demos.6,0,0.0633789,"Missing"
2020.tacl-1.5,D18-1259,0,0.0368485,"on Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4 : NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good test bed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1 , p2 , . . . , pl ) and question Q = (q1 , q2 , . . . , ql ) into a sin"
2020.tacl-1.5,P19-1139,0,0.0641703,"Missing"
2021.acl-long.239,2020.tacl-1.30,0,0.0256716,"uate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation is then used to sele"
2021.acl-long.239,D19-1606,0,0.0380564,"Missing"
2021.acl-long.239,N19-1423,0,0.456229,"n SQuAD with only 128 training examples), while maintaining competitive performance in the high-resource setting.1 1 80.0 F1 60.0 20.0 0.0 16 64 128 256 Figure 1: Performance of SpanBERT (red) and RoBERTa (yellow) base-size models on SQuAD, given different amounts of training examples. Our model (Splinter, green) dramatically improves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrea"
2021.acl-long.239,N19-1246,0,0.0414153,"Missing"
2021.acl-long.239,D19-5801,0,0.521048,"tation to select the answer span. The QASS layer seamlessly integrates with fine-tuning on real question-answer pairs. We simply append the [QUESTION] token to the input question, and use the QASS layer to select the answer span (Figure 3). This is unlike existing models for span selection, which do not include an explicit question representation. The compatibility between pretraining and fine-tuning makes Splinter an effective few-shot learner. Splinter exhibits surprisingly high performance given only a few training examples throughout a variety of benchmarks from the MRQA 2019 shared task (Fisch et al., 2019). For example, Splinter-base achieves 72.7 F1 on SQuAD with only 128 examples, outperforming all baselines by a very wide margin. An ablation study shows that the pretraining method and the QASS layer itself (even without pretraining) both contribute to improved performance. Analysis indicates that Splinter’s representations change significantly less during fine-tuning compared to the baselines, suggesting that our pretraining is more adequate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an approp"
2021.acl-long.239,2021.acl-long.295,0,0.066632,"), while the other models’ representations seem to change drastically. This suggests that fine-tuning with even 128 questionanswering examples makes significant modifications to the functionality of pretrained masked language models. Splinter’s pretraining, on the other hand, is much more similar to the fine-tuning task, resulting in much more modest changes to the produced vector representations. 7 Related Work The remarkable results of GPT-3 (Brown et al., 2020) have inspired a renewed interest in few-shot learning. While some work focuses on classification tasks (Schick and Sch¨utze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering. One approach to this problem is to create synthetic text-question-answer examples. Both Lewis et al. (2019) and Glass et al. (2020) use the traditional NLP pipeline to select noun phrases and named entities in Wikipedia paragraphs as potential answers, which are then masked from the context to create pseudo-questions. Lewis et al. (2019) use methods from unsupervised machine translation to translate the pseudo-questions into real ones, while Glass et al. (2020) keep the pseudo-questions but use informati"
2021.acl-long.239,2020.acl-main.247,0,0.127054,"guage models. Splinter’s pretraining, on the other hand, is much more similar to the fine-tuning task, resulting in much more modest changes to the produced vector representations. 7 Related Work The remarkable results of GPT-3 (Brown et al., 2020) have inspired a renewed interest in few-shot learning. While some work focuses on classification tasks (Schick and Sch¨utze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering. One approach to this problem is to create synthetic text-question-answer examples. Both Lewis et al. (2019) and Glass et al. (2020) use the traditional NLP pipeline to select noun phrases and named entities in Wikipedia paragraphs as potential answers, which are then masked from the context to create pseudo-questions. Lewis et al. (2019) use methods from unsupervised machine translation to translate the pseudo-questions into real ones, while Glass et al. (2020) keep the pseudo-questions but use information retrieval to find new text passages that can answer them. Both works assume access to language- and domain-specific NLP tools such as part-of-speech taggers, syntactic parsers, and named-entity recognizers, which might"
2021.acl-long.239,2020.tacl-1.5,1,0.93661,"training examples), while maintaining competitive performance in the high-resource setting.1 1 80.0 F1 60.0 20.0 0.0 16 64 128 256 Figure 1: Performance of SpanBERT (red) and RoBERTa (yellow) base-size models on SQuAD, given different amounts of training examples. Our model (Splinter, green) dramatically improves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrealistic as we venture"
2021.acl-long.239,P17-1147,0,0.0559974,"ntext. 4 A Few-Shot QA Benchmark To evaluate how pretrained models work when only a small amount of labeled data is available for finetuning, we simulate various low-data scenarios by sampling subsets of training examples from larger datasets. We use a subset of the MRQA 2019 shared task (Fisch et al., 2019), which contains extractive question answering datasets in a unified format, where the answer is a single span in the given text passage. Split I of the MRQA shared task contains 6 large question answering datasets: SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017), TriviaQA (Joshi et al., 2017), SearchQA (Dunn et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019). For each dataset, we sample smaller training datasets from the original training set with sizes changing on a 3069 logarithmic scale, from 16 to 1,024 examples. To reduce variance, for each training set size, we sample 5 training sets using different random seeds and report average performance across training sets. We also experiment with fine-tuning the models on the full training sets. Since Split I of the MRQA shared task does not contain test sets, we evaluate using the official"
2021.acl-long.239,D19-1439,0,0.130029,"producing contextualized to2 In practice, only some sets of recurring spans are processed; see Cluster Selection below. 3068 ken representations. The model is then tasked with predicting the start and end positions of the answer given each [QUESTION] token representation. In Figure 2b, we observe four instances of this prediction task: two for the “Roosevelt” cluster, one for the “Allied countries” cluster, and one for “Declaration by United Nations”. Taking advantage of recurring words in a passage (restricted to nouns or named entities) was proposed in past work as a signal for coreference (Kocijan et al., 2019; Ye et al., 2020). We further discuss this connection in Section 7. Span Filtering To focus pretraining on semantically meaningful spans, we use the following definition for “spans”, which filters out recurring spans that are likely to be uninformative: (1) spans must begin and end at word boundaries, (2) we consider only maximal recurring spans, (3) spans containing only stop words are ignored, (4) spans are limited to a maximum of 10 tokens. These simple heuristic filters do not require a model, as opposed to masking schemes in related work (Glass et al., 2020; Ye et al., 2020; Guu et al.,"
2021.acl-long.239,Q19-1026,0,0.26222,"roves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrealistic as we venture outside the lab conditions of English Wikipedia, and attempt to crowdsource question-answer pairs in other languages or domains of expertise (Tsatsaronis et al., 2015; Kembhavi et al., 2017). How do question answering models fare in the more practical case, where an in-house annotation effort can only produce a cou"
2021.acl-long.239,K17-1034,1,0.842402,"esigning objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation is then used to select the answer span. such as BERT (Devlin et al., 2019), and adds two paramete"
2021.acl-long.239,P19-1484,0,0.0615571,"of pretrained masked language models. Splinter’s pretraining, on the other hand, is much more similar to the fine-tuning task, resulting in much more modest changes to the produced vector representations. 7 Related Work The remarkable results of GPT-3 (Brown et al., 2020) have inspired a renewed interest in few-shot learning. While some work focuses on classification tasks (Schick and Sch¨utze, 2020; Gao et al., 2021), our work investigates few-shot learning in the context of extractive question answering. One approach to this problem is to create synthetic text-question-answer examples. Both Lewis et al. (2019) and Glass et al. (2020) use the traditional NLP pipeline to select noun phrases and named entities in Wikipedia paragraphs as potential answers, which are then masked from the context to create pseudo-questions. Lewis et al. (2019) use methods from unsupervised machine translation to translate the pseudo-questions into real ones, while Glass et al. (2020) keep the pseudo-questions but use information retrieval to find new text passages that can answer them. Both works assume access to language- and domain-specific NLP tools such as part-of-speech taggers, syntactic parsers, and named-entity r"
2021.acl-long.239,2020.acl-main.653,0,0.0220653,"raining is more adequate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation"
2021.acl-long.239,P18-2124,0,0.0297517,"and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concatenated to the context. The [QUESTION] token’s representation is then used to select the answer span. such as BERT (Devlin et al., 2019), and adds two parameter vectors s, e to the pre"
2021.acl-long.239,D16-1264,0,0.810559,"xamples. Our model (Splinter, green) dramatically improves performance. SpanBERT-base trained on the full training data of SQuAD (blue, dashed) is shown for reference. The standard approach to question answering is to pretrain a masked language model on raw text, and then fine-tune it with a span selection layer on top (Devlin et al., 2019; Joshi et al., 2020; Liu et al., 2019). While this approach is effective, and sometimes exceeds human performance, its success is based on the assumption that large quantities of annotated question answering examples are available. For instance, both SQuAD (Rajpurkar et al., 2016, 2018) and Natural Questions (Kwiatkowski et al., 2019) contain an order of 100,000 question and ∗ 32 # Examples Introduction Equal contribution. Our code, models, and datasets are publicly available: https://github.com/oriram/splinter. 1 40.0 answer pairs in their training data. This assumption quickly becomes unrealistic as we venture outside the lab conditions of English Wikipedia, and attempt to crowdsource question-answer pairs in other languages or domains of expertise (Tsatsaronis et al., 2015; Kembhavi et al., 2017). How do question answering models fare in the more practical case, wh"
2021.acl-long.239,W17-2623,0,0.450597,"ine-tuning compared to the baselines, suggesting that our pretraining is more adequate for question answering. Overall, our results highlight the importance of designing objectives and architectures in the few-shot setting, where an appropriate inductive bias can lead to dramatic performance improvements. 2 Background Extractive question answering is a common task in NLP, where the goal is to select a contiguous span a from a given text T that answers a question Q. This format was popularized by SQuAD (Rajpurkar et al., 2016), and has since been adopted by several datasets in various domains (Trischler et al., 2017; Kembhavi et al., 2017) and languages (Lewis et al., 2020; Clark et al., 2020), with some extensions allowing for unanswerable questions (Levy et al., 2017; Rajpurkar et al., 2018) or multiple answer spans (Dua et al., 2019; Dasigi et al., 2019). In this work, we follow the assumptions in the recent MRQA 2019 shared task (Fisch et al., 2019) and focus on questions whose answer is a single span. The standard approach uses a pretrained encoder, 3067 Figure 3: An example of our fine-tuning setup, taken from the development set of SQuAD. The question, followed by the [QUESTION] token, is concaten"
2021.acl-short.3,D19-1588,1,0.950166,"t. 1 Introduction Until recently, the standard methodology in NLP was to design task-specific models, such as BiDAF for question answering (Seo et al., 2017) and ESIM for natural language inference (Chen et al., 2017). With the introduction of pretraining, many of these models were replaced with simple output layers, effectively fine-tuning the transformer layers below to perform the traditional model’s function (Radford et al., 2018). A notable exception to this trend is coreference resolution, where a multi-layer taskspecific model (Lee et al., 2017, 2018) is appended to a pretrained model (Joshi et al., 2019, 2020). This model uses intricate span and span-pair representations, a representation refinement mechanism, handcrafted features, pruning heuristics, and more. While the model is highly effective, it comes at a great cost in memory consumption, limiting the amount of examples that can be loaded on a large GPU to a single document, which often needs to ∗ 2 Background: Coreference Resolution Coreference resolution is the task of clustering multiple mentions of the same entity within a given text. It is typically modeled by identifying entity mentions (contiguous spans of text), and predicting"
2021.acl-short.3,P19-1066,0,0.0135102,"hi et al., 2019). Recent work on efficient transformers (Beltagy et al., 2020) has been able to shift towards processing complete documents, albeit with a smaller model (base) and only one training example per batch. where Wm and vm are learned parameters. Then, span representations are enhanced with more global information through a refinement process that interpolates each span representation with a weighted average of its candidate antecedents. More recently, Xu and Choi (2020) demonstrated that this span refinement technique, as well as other modifications to it (e.g. entity equalization (Kantor and Globerson, 2019)) do not improve performance. 3 Scoring Antecedents The antecedent score fa (c, q) is derived from a vector representation of the span pair v(c,q) . This, in turn, is a function of the individual span representations vc and vq , as well as a vector of handcrafted features φ(c, q) Model We present start-to-end (s2e) coreference resolution, a simpler and more efficient model with respect to c2f (Section 2). Our model utilizes the endpoints of a span (rather than all span tokens) to compute the mention and antecedent scores fm (·) 15 Figure 1: The antecedent score fa (c, q) of a query mention q ="
2021.acl-short.3,D17-1018,0,0.344676,"rrent standard model, while being simpler and more efficient. 1 Introduction Until recently, the standard methodology in NLP was to design task-specific models, such as BiDAF for question answering (Seo et al., 2017) and ESIM for natural language inference (Chen et al., 2017). With the introduction of pretraining, many of these models were replaced with simple output layers, effectively fine-tuning the transformer layers below to perform the traditional model’s function (Radford et al., 2018). A notable exception to this trend is coreference resolution, where a multi-layer taskspecific model (Lee et al., 2017, 2018) is appended to a pretrained model (Joshi et al., 2019, 2020). This model uses intricate span and span-pair representations, a representation refinement mechanism, handcrafted features, pruning heuristics, and more. While the model is highly effective, it comes at a great cost in memory consumption, limiting the amount of examples that can be loaded on a large GPU to a single document, which often needs to ∗ 2 Background: Coreference Resolution Coreference resolution is the task of clustering multiple mentions of the same entity within a given text. It is typically modeled by identifyin"
2021.acl-short.3,N18-2108,0,0.110315,"Missing"
2021.acl-short.3,W12-4501,0,0.560497,"tion and antecedent scores through a series of bilinear functions over their contextualized representations. Our model has a significantly lighter memory footprint, allowing us to process multiple documents in a single batch, with no truncation or sliding windows. We do not use any handcrafted features, priors, or pruning heuristics. Experiments show that our minimalist approach performs on par with the standard model, despite removing a significant amount of complexity, parameters, and heuristics. Without any hyperparameter tuning, our model achieves 80.3 F1 on the English OntoNotes dataset (Pradhan et al., 2012), with the best comparable baseline reaching 80.2 F1 (Joshi et al., 2020), while consuming less than a third of the memory. These results suggest that transformers can learn even difficult structured prediction tasks such as coreference resolution without investing in complex task-specific architectures.1 The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effectiv"
2021.acl-short.3,P17-1152,0,0.0348369,"n and span-pair representations – which hinders the processing of complete documents and the ability to train on multiple instances in a single batch. We introduce a lightweight end-to-end coreference model that removes the dependency on span representations, handcrafted features, and heuristics. Our model performs competitively with the current standard model, while being simpler and more efficient. 1 Introduction Until recently, the standard methodology in NLP was to design task-specific models, such as BiDAF for question answering (Seo et al., 2017) and ESIM for natural language inference (Chen et al., 2017). With the introduction of pretraining, many of these models were replaced with simple output layers, effectively fine-tuning the transformer layers below to perform the traditional model’s function (Radford et al., 2018). A notable exception to this trend is coreference resolution, where a multi-layer taskspecific model (Lee et al., 2017, 2018) is appended to a pretrained model (Joshi et al., 2019, 2020). This model uses intricate span and span-pair representations, a representation refinement mechanism, handcrafted features, pruning heuristics, and more. While the model is highly effective,"
2021.acl-short.3,2020.emnlp-main.685,0,0.401237,"tion is at least three times more memory efficient than the baseline. This improvement results from a combination of three factors: (1) the fact that our model is lighter on memory and does not need to construct span or span-pair representations, (2) our simplified framework, which does not use sliding windows, and (3) our implementation, which was written “from scratch”, and might thus be more (or less) efficient than the original. 5 Related Work Recent work on memory-efficient coreference resolution sacrifices speed and parallelism for guarantees on memory consumption. Xia et al. (2020) and Toshniwal et al. (2020) present variants of the c2f model (Lee et al., 2017, 2018) that use an iterative process to maintain a fixed number of span representations at all times. Specifically, spans are processed sequentially, either joining existing clusters or forming new ones, and an eviction mechanism ensures the use of a constant number of clusters. While these approach constrains the space complexity, their sequential nature slows down the computation, and slightly deteriorates the performance. Our approach is able to alleviate the large Performance Table 1 and Table 2 show that, despite our model’s simplicity,"
2021.acl-short.3,2020.tacl-1.5,1,0.913859,"r contextualized representations. Our model has a significantly lighter memory footprint, allowing us to process multiple documents in a single batch, with no truncation or sliding windows. We do not use any handcrafted features, priors, or pruning heuristics. Experiments show that our minimalist approach performs on par with the standard model, despite removing a significant amount of complexity, parameters, and heuristics. Without any hyperparameter tuning, our model achieves 80.3 F1 on the English OntoNotes dataset (Pradhan et al., 2012), with the best comparable baseline reaching 80.2 F1 (Joshi et al., 2020), while consuming less than a third of the memory. These results suggest that transformers can learn even difficult structured prediction tasks such as coreference resolution without investing in complex task-specific architectures.1 The introduction of pretrained language models has reduced many complex task-specific NLP models to simple lightweight layers. An exception to this trend is coreference resolution, where a sophisticated task-specific model is appended to a pretrained transformer encoder. While highly effective, the model has a very large memory footprint – primarily due to dynamic"
2021.acl-short.3,Q18-1042,0,0.0189301,"rs vs , ve and the matrix Bm are the trainable parameters of our mention scoring function fm . We efficiently compute mention scores for all possible spans while masking spans that exceed a certain length `.2 We then retain only the top-scoring λn mention candidates to avoid O(n4 ) complexity when computing antecedents. Similarly, we extract start and end token representations for the antecedent scoring function fa : s a = GeLU(Was x) e a = 4 Dataset We train and evaluate on two datasets: the document-level English OntoNotes 5.0 dataset (Pradhan et al., 2012), and the GAP coreference dataset (Webster et al., 2018). The OntoNotes dataset contains speaker metadata, which the baselines use through a hand-crafted feature that indicates whether two spans were uttered by the same speaker. Instead, we insert the speaker’s name to the text every time the speaker changes, making the metadata available to any model. Pretrained Model We use Longformer-Large (Beltagy et al., 2020) as our underlying pretrained model, since it is able to process long documents without resorting to sliding windows or truncation. GeLU(Wae x) Then, we sum over four bilinear functions: s s se e fa (c, q) = ascs · Bss a · aqs + acs · Ba"
2021.acl-short.3,2020.acl-main.622,0,0.537467,"Missing"
2021.acl-short.3,2020.emnlp-main.695,0,0.4033,"Missing"
2021.acl-short.3,2020.emnlp-main.686,0,0.909092,"he limited sequence length of most pretrained transformers, documents are often split into segments of a few hundred tokens each (Joshi et al., 2019). Recent work on efficient transformers (Beltagy et al., 2020) has been able to shift towards processing complete documents, albeit with a smaller model (base) and only one training example per batch. where Wm and vm are learned parameters. Then, span representations are enhanced with more global information through a refinement process that interpolates each span representation with a weighted average of its candidate antecedents. More recently, Xu and Choi (2020) demonstrated that this span refinement technique, as well as other modifications to it (e.g. entity equalization (Kantor and Globerson, 2019)) do not improve performance. 3 Scoring Antecedents The antecedent score fa (c, q) is derived from a vector representation of the span pair v(c,q) . This, in turn, is a function of the individual span representations vc and vq , as well as a vector of handcrafted features φ(c, q) Model We present start-to-end (s2e) coreference resolution, a simpler and more efficient model with respect to c2f (Section 2). Our model utilizes the endpoints of a span (rathe"
2021.emnlp-main.344,N18-2017,1,0.890885,"Missing"
2021.emnlp-main.344,D18-2012,0,0.0521205,"Missing"
2021.emnlp-main.344,P15-1070,0,0.0205667,"g, and the Cryptonite dataset could prove useful in this direction as well. Language disambiguation In addition to works and datasets specifically targeting disambiguation on the word level (Levesque et al., 2011; Raganato et al., 2017; Sakaguchi et al., 2020), there are 6 Cryptonite’s metadata contains additional information other domains strongly related to language disamthat could help a solver, such as orientation (whether the clue biguation. Among them are pun disambiguation is across or down in the grid). Knowing the orientation can help in finding the clue’s wordplay-definition split. (Miller and Gurevych, 2015; Miller et al., 2017), 4189 and sarcasm detection (Joshi et al., 2017; Oprea and Magdy, 2020). However, to the best of our knowledge Cryptonite is the first dataset both large in scale (unlike pun disambiguation), and containing a variety of wordplays (unlike sarcasm detection). Non-cryptic crosswords As described in Section 2, non-cryptic (“regular”) crosswords are the common crosswords found in most newspapers. There are works introducing regular crossword datasets, some even containing a small percentage of more “tricky” clues7 (Littman et al., 2002). However, identifying this small portio"
2021.emnlp-main.344,S17-2005,0,0.0142312,"et could prove useful in this direction as well. Language disambiguation In addition to works and datasets specifically targeting disambiguation on the word level (Levesque et al., 2011; Raganato et al., 2017; Sakaguchi et al., 2020), there are 6 Cryptonite’s metadata contains additional information other domains strongly related to language disamthat could help a solver, such as orientation (whether the clue biguation. Among them are pun disambiguation is across or down in the grid). Knowing the orientation can help in finding the clue’s wordplay-definition split. (Miller and Gurevych, 2015; Miller et al., 2017), 4189 and sarcasm detection (Joshi et al., 2017; Oprea and Magdy, 2020). However, to the best of our knowledge Cryptonite is the first dataset both large in scale (unlike pun disambiguation), and containing a variety of wordplays (unlike sarcasm detection). Non-cryptic crosswords As described in Section 2, non-cryptic (“regular”) crosswords are the common crosswords found in most newspapers. There are works introducing regular crossword datasets, some even containing a small percentage of more “tricky” clues7 (Littman et al., 2002). However, identifying this small portion of clues requires hu"
2021.emnlp-main.344,2020.acl-main.118,0,0.0385679,"on In addition to works and datasets specifically targeting disambiguation on the word level (Levesque et al., 2011; Raganato et al., 2017; Sakaguchi et al., 2020), there are 6 Cryptonite’s metadata contains additional information other domains strongly related to language disamthat could help a solver, such as orientation (whether the clue biguation. Among them are pun disambiguation is across or down in the grid). Knowing the orientation can help in finding the clue’s wordplay-definition split. (Miller and Gurevych, 2015; Miller et al., 2017), 4189 and sarcasm detection (Joshi et al., 2017; Oprea and Magdy, 2020). However, to the best of our knowledge Cryptonite is the first dataset both large in scale (unlike pun disambiguation), and containing a variety of wordplays (unlike sarcasm detection). Non-cryptic crosswords As described in Section 2, non-cryptic (“regular”) crosswords are the common crosswords found in most newspapers. There are works introducing regular crossword datasets, some even containing a small percentage of more “tricky” clues7 (Littman et al., 2002). However, identifying this small portion of clues requires human effort, whereas Cryptonite is already guaranteed to consist entirely"
2021.emnlp-main.344,E17-1010,0,0.183254,"re out the type of wordplay, which is often hinted by an indicator (purple). In our case, “shifting” hints that the answer is an anagram of some part of the wordplay. As the enumeration (gray) states the answer is a five-letter word, “earth” is a promising candidate for anagraming. Finally, given that “hater” is both an anagram of “earth” and a synonym of the definition, we conclude it to be the correct answer. The ambiguity of natural language is one of the most fundamental challenges in NLP research. While there are works and datasets specifically targeting ambiguity (Levesque et al., 2011; Raganato et al., 2017; Sakaguchi et al., 2020), these can be solved by a native speaker with relative ease. Can we design a dataset with ambiguities that pose a challenge even to competent native speakers? We present Cryptonite, a large-scale dataset based on cryptic crosswords, which is both linguistically complex and naturally sourced. Cryptonite’s 523K examples are taken from professionallyauthored cryptic crosswords, making them less prone to artifacts and biases than examples created by crowdsourcing (Gururangan et al., 2018; Geva et al., 2019). Each example in Cryptonite is a cryptic clue, a short phrase or"
2021.emnlp-main.344,P15-2033,0,0.1696,"train and test examples are mutually exclusive is critical for a candid estimation of T5’s ability to solve cryptic clues in general. 4186 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4186–4192 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Cryptic Crosswords A cryptic crossword, just like a regular (noncryptic) crossword, is a puzzle designed to entertain humans, comprised of clues whose answers are to be filled in a letter grid. Unlike regular crosswords, where answers are typically synonyms or hyponyms of the clues (Severyn et al., 2015), cryptic clues have a misleading surface reading, and solving them requires disambiguating wordplays. A cryptic clue has only one possible answer, even when taken outside the context of the letter grid.1 Generally, a cryptic clue (henceforth, “clue”) consists of two parts: wordplay and definition. The wordplay-definition split is not given to the solver, and parsing it is usually the first step in solving a clue. Both the wordplay and the definition lead to the answer, but each in a different manner. While the definition is directly related to the answer (e.g. a synonym or a hypernym), the wo"
2021.emnlp-main.446,W19-4828,1,0.924686,"es. The feed-forward layer’s output is thus the weighted sum of its values. Transformer-based language models (Vaswani et al., 2017) are at the core of state-of-the-art natuparameter matrix in the layer corresponds to keys, ral language processing (Devlin et al., 2019; Brown et al., 2020), largely due to the success of self- and the second parameter matrix to values. Figure 1 shows how the keys (first parameter matrix) interattention. While much literature has been devoted act with the input to produce coefficients, which to analyzing the function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Be- are then used to compute a weighted sum of the vallinkov, 2019), they account for only a third of a typ- ues (second parameter matrix) as the output. While the theoretical similarity between feed-forward layical transformer’s parameters (4d2 per layer, where ers and key-value memories has previously been d is the model’s hidden dimension). Most of the parameter budget is spent on position-wise feed- suggested by Sukhbaatar et al. (2019), we take forward layers (8d2 per layer), yet their role re- this observation one step further, and analyze the mains under-explored. What, if so,"
2021.emnlp-main.446,N19-1423,0,0.0246823,"4 x5 self-attention layer Transformer layers Stay with you for a Figure 1: An illustration of how a feed-forward layer emulates a key-value memory. Input vectors (here, x5 ) are multiplied by keys to produce memory coefficients (e.g., the memory coefficient for v1 is 0.2), which then weigh distributions over the output vocabulary, stored in the values. The feed-forward layer’s output is thus the weighted sum of its values. Transformer-based language models (Vaswani et al., 2017) are at the core of state-of-the-art natuparameter matrix in the layer corresponds to keys, ral language processing (Devlin et al., 2019; Brown et al., 2020), largely due to the success of self- and the second parameter matrix to values. Figure 1 shows how the keys (first parameter matrix) interattention. While much literature has been devoted act with the input to produce coefficients, which to analyzing the function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Be- are then used to compute a weighted sum of the vallinkov, 2019), they account for only a third of a typ- ues (second parameter matrix) as the output. While the theoretical similarity between feed-forward layical transformer’s parameters"
2021.emnlp-main.446,2020.emnlp-main.395,0,0.0327808,"t in the final layer. We find that in most cases (66 examples), the output changes to a semantically distant word (e.g., “people” → “same”) and in the rest of the cases (34 examples), the feed-forward layer’s output shifts the residual prediction to a related word (e.g. “later” → “earlier” and “gastric” → “stomach”). This suggests that feed-forward layers tune the residual predictions at varying granularity, even in the last layer of the model. 6 Related Work extensive line of work targeted neuron functionality in general, extracting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importanc"
2021.emnlp-main.446,2020.acl-main.492,0,0.0145535,"xtract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations has been used previously in NLP (Rethmeier et al., 2020) and vision (Erhan et al., 2009). 7 Discussion and Conclusion Understanding how and why transformers work is crucial to many aspects of modern NLP, including model interpretability, data security, and development of better models. Feed-forward layers account for most of a t"
2021.emnlp-main.446,W18-5408,0,0.0230668,"examples), the feed-forward layer’s output shifts the residual prediction to a related word (e.g. “later” → “earlier” and “gastric” → “stomach”). This suggests that feed-forward layers tune the residual predictions at varying granularity, even in the last layer of the model. 6 Related Work extensive line of work targeted neuron functionality in general, extracting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explo"
2021.emnlp-main.446,N19-1112,0,0.0230888,"ixes associated with shallow patterns and semantic patterns (Figure 2), the lower layers (layers 1-9) are dominated by shallow patterns, often with prefixes that share the last word (e.g. k1449 in Table 1). In contrast, the upper layers (layers 10-16) are characterized by more semantic patterns, with prefixes from similar contexts but without clear surface-form similarities (e.g. k16 1935 in Table 1). This observation corroborates recent findings that lower (upper) layers in deep contextualized models encode shallow (semantic) features of the inputs (Peters et al., 2018; Jawahar et al., 2019; Liu et al., 2019). To further test this hypothesis, we sample 1600 random keys (100 keys per layer) and apply local modifications to the top-50 trigger examples of every key. Specifically, we remove either the first, last, or a random token from the input, and measure how this mutation affects the memory coefficient. Figure 3 shows that the model considers the end of an example as more salient than the beginning for predicting the next token. In upper layers, removing the last token has less impact, supporting our conclusion that upper-layer keys are less correlated with shallow patterns. 4 Values Represent Di"
2021.emnlp-main.446,N18-1202,0,0.0194337,"allow patterns Comparing the amount of prefixes associated with shallow patterns and semantic patterns (Figure 2), the lower layers (layers 1-9) are dominated by shallow patterns, often with prefixes that share the last word (e.g. k1449 in Table 1). In contrast, the upper layers (layers 10-16) are characterized by more semantic patterns, with prefixes from similar contexts but without clear surface-form similarities (e.g. k16 1935 in Table 1). This observation corroborates recent findings that lower (upper) layers in deep contextualized models encode shallow (semantic) features of the inputs (Peters et al., 2018; Jawahar et al., 2019; Liu et al., 2019). To further test this hypothesis, we sample 1600 random keys (100 keys per layer) and apply local modifications to the top-50 trigger examples of every key. Specifically, we remove either the first, last, or a random token from the input, and measure how this mutation affects the memory coefficient. Figure 3 shows that the model considers the end of an example as more salient than the beginning for predicting the next token. In upper layers, removing the last token has less impact, supporting our conclusion that upper-layer keys are less correlated wit"
2021.emnlp-main.446,2020.acl-main.270,1,0.701989,"20; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations has been used previously in NLP (Rethmeier et al., 2020) and vision (Erhan et al., 2009). 7 Discussion and Conclusion Understandi"
2021.emnlp-main.446,P19-1452,0,0.0160944,"racting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations"
2021.emnlp-main.446,W19-4808,0,0.0160153,"l. 6 Related Work extensive line of work targeted neuron functionality in general, extracting the properties that neurons and subsets of neurons capture (Durrani et al., 2020; Dalvi et al., 2019; Rethmeier et al., 2020; Mu and Andreas, 2020; Vig et al., 2020), regardless of the model architecture or neurons’ position in it. Jacovi et al. (2018) analyzed CNN architectures in text classification and showed that they extract key n-grams from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the"
2021.emnlp-main.446,P19-1580,0,0.154987,", stored in the values. The feed-forward layer’s output is thus the weighted sum of its values. Transformer-based language models (Vaswani et al., 2017) are at the core of state-of-the-art natuparameter matrix in the layer corresponds to keys, ral language processing (Devlin et al., 2019; Brown et al., 2020), largely due to the success of self- and the second parameter matrix to values. Figure 1 shows how the keys (first parameter matrix) interattention. While much literature has been devoted act with the input to produce coefficients, which to analyzing the function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Be- are then used to compute a weighted sum of the vallinkov, 2019), they account for only a third of a typ- ues (second parameter matrix) as the output. While the theoretical similarity between feed-forward layical transformer’s parameters (4d2 per layer, where ers and key-value memories has previously been d is the model’s hidden dimension). Most of the parameter budget is spent on position-wise feed- suggested by Sukhbaatar et al. (2019), we take forward layers (8d2 per layer), yet their role re- this observation one step further, and analyze the mains under-ex"
2021.emnlp-main.446,D19-1002,0,0.0262223,"from the inputs. The study of the transformer architecture has focused on the role and function of self-attention layers (Voita et al., 2019; Clark et al., 2019; Vig and Belinkov, 2019) and on inter-layer differences (i.e. lower vs. upper layers) (Tenney et al., 2019; Jawahar et al., 2019). Previous work also highlighted the importance of feed-forward layers in transformers (Press et al., 2020; Pulugundla et al., 2021; Xu et al., 2020). Still, to date, the role of feed-forward layers remains under-explored. Also related are interpretability methods that explain predictions (Han et al., 2020; Wiegreffe and Pinter, 2019), however, our focus is entirely different: we do not interpret individual predictions, but aim to understand the mechanism of transformers. Characterizing the functionality of memory cells based on examples that trigger maximal activations has been used previously in NLP (Rethmeier et al., 2020) and vision (Erhan et al., 2009). 7 Discussion and Conclusion Understanding how and why transformers work is crucial to many aspects of modern NLP, including model interpretability, data security, and development of better models. Feed-forward layers account for most of a transformer’s parameters, yet"
2021.emnlp-main.831,2020.emnlp-main.20,0,0.098458,"Missing"
2021.emnlp-main.831,W18-6301,0,0.0170741,"framework to the official implementation of Devlin et al. (2019).3 Table 1 shows that using the official code to train BERTBASE could take almost 6 days under our hardware assumptions (Section 2), and a large model might require close to a month of non-stop computation. In contrast, our recipe significantly speeds up training, allowing one to train BERTLARGE with the original number of steps (1M) in a third of the time (8 days), or converge in 2-3 days by enlarging the batch size. While larger batch sizes do not guarantee convergence to models of equal quality, they are generally recommended (Ott et al., 2018; Liu et al., 2019), and present a more realistic starting point for our next phase (hyperparameter tuning) given our 24-hour constraint. We also conduct an ablation study of engineering improvements in our model. Table 2 shows that efficient implementation gains an additional 1.75 hours (out of 24) for training operations, which would have otherwise been wasted. Software We base our implementation on the DeepSpeed software package (Rasley et al., 2020), which includes optimizations for training language models, such as data parallelization, and mixedprecision training. We further improve the"
2021.emnlp-main.831,D16-1264,0,0.0988516,"Missing"
2021.emnlp-main.831,2021.eacl-main.20,0,0.0899641,"Missing"
2021.emnlp-main.831,D13-1170,0,0.0259255,"Missing"
2021.emnlp-main.831,W18-5446,1,0.888803,"Missing"
2021.emnlp-main.831,Q19-1040,0,0.0508723,"Missing"
2021.emnlp-main.831,N18-1101,0,0.042198,"Missing"
2021.mrqa-1.11,N19-1423,0,0.0775195,"Missing"
2021.mrqa-1.11,N18-2017,1,0.89089,"Missing"
2021.mrqa-1.11,E17-5001,0,0.0205342,"baseline results using recently-released BERT-style models for Hebrew, showing that there is significant room for improvement on this task. 1 Introduction Natural language processing has seen a surge in the pretraining paradigm in recent years with the appearance of pretrained models in a plethora of languages, including Hebrew (Chriqui and Yahav, 2021; Seker et al., 2021). While such models have shown to perform remarkably well on a variety of tasks, most of the evaluation of the Hebrew models, however, has been focused on morphology and syntax tasks in the spirit of universal dependencies (Nivre et al., 2017), while end-user-focused evaluation has been limited to sentiment analysis (Chriqui and Yahav, 2021) and named entity recognition (Bareket and Tsarfaty, 2020). In this paper, we try to remedy the scarcity of semantic datasets by presenting PARA S HOOT,1 the first question answering dataset in Hebrew, in the style of SQuAD (Rajpurkar et al., 2016). We follow similar work in constructing non-English question answering datasets (d’Hoffschmidt et al., 2020; Mozannar et al., 2019; Lim et al., 2019, 1 A portmanteau of paragraph and !( שו!&quot;תshoot), the Hebrew abbreviation of Q&A. inter alia), and t"
2021.mrqa-1.11,2020.acl-main.156,0,0.0860543,"Missing"
2021.mrqa-1.11,D16-1264,0,0.169087,"Yahav, 2021; Seker et al., 2021). While such models have shown to perform remarkably well on a variety of tasks, most of the evaluation of the Hebrew models, however, has been focused on morphology and syntax tasks in the spirit of universal dependencies (Nivre et al., 2017), while end-user-focused evaluation has been limited to sentiment analysis (Chriqui and Yahav, 2021) and named entity recognition (Bareket and Tsarfaty, 2020). In this paper, we try to remedy the scarcity of semantic datasets by presenting PARA S HOOT,1 the first question answering dataset in Hebrew, in the style of SQuAD (Rajpurkar et al., 2016). We follow similar work in constructing non-English question answering datasets (d’Hoffschmidt et al., 2020; Mozannar et al., 2019; Lim et al., 2019, 1 A portmanteau of paragraph and !( שו!&quot;תshoot), the Hebrew abbreviation of Q&A. inter alia), and turn to Hebrew-speaking crowdsource workers, asking them to write questions given paragraphs sampled at random from Hebrew Wikipedia. Through this process, we collect approximately 3000 annotated (paragraph, question, answer) triplets, in a setting that may be suitable for few-shot learning, simulating the amount of data a startup or academic grou"
2021.mrqa-1.11,W10-1401,0,0.0138738,"he data, for example: ? נקראת האופרה האחרונה שכתבו גילברטKאי ! יחדיוN( וסאליבWhat is the last opera written jointly by Gilbert and Sullivan called?) There are even questions with only 2 words; due to Hebrew’s rich morphology, these questions are usually translated to 3-4 words in English, e.g. ?!M( מהו המניכאיזWhat is Manichaeism?) Answer lengths, however, can vary greatly, depending on whether the annotators wrote minimal spans (typically 1-4 words) or included supporting information in the answer spans (see Section 3.1). 3.4 Linguistic Phenomena As a morphologically-rich language (Tsarfaty et al., 2010; Seddah et al., 2013), modern Hebrew exhibits a variety of non-trivial phenomena that are uncommon in English and could be challenging for NLP models (Tsarfaty et al., 2020). We can identify some of these phenomena in our dataset. Consider for example the following question-answer pair from the validation set: To measure the dataset’s diversity, we cluster questions by their question word (typically the first word in the question). Table 3 shows that what (! )מהand which (! )איזהquestions account for a third 108 Q: A: ?!Mמה היה שטחו של כפר שמריהו כשהוק ma haya shitkho shel kfar shmaryah"
2021.mrqa-1.11,2021.acl-long.239,1,0.761926,"ding in Hebrew.2 2 Dataset We present PARA S HOOT, a question answering dataset in Hebrew, in a format that closely follows that of SQuAD (Rajpurkar et al., 2016). Each example in the dataset is a triplet consisting of a paragraph, a question, and a span from the paragraph text constituting the answer to the question. We scrape paragraphs from random Hebrew Wikipedia articles, and crowdsource questions and answers for each one, resulting in 3038 annotated examples. While larger datasets may facilitate betterperforming models, recent work has advocated for research on smaller labeled datasets (Ram et al., 2021), which more accurately reflect the amount of data a startup or academic lab can collect in a short amount of time and resources. 2 The dataset is publicly available at https://github. com/omrikeren/ParaShoot 106 Proceedings of the 3rd Workshop on Machine Reading for Question Answering , pages 106–112 Nov 10th, 2021 ©2021 Association for Computational Linguistics #Articles #Paragraphs #Questions Train Validation Test 295 33 165 565 63 319 1792 221 1025 Total 493 947 3038 Table 1: The number of unique articles, paragraphs, and questions in each split of PARA S HOOT. The dataset is partitioned b"
2021.mrqa-1.11,2020.acl-main.660,0,0.0123981,"uestions with only 2 words; due to Hebrew’s rich morphology, these questions are usually translated to 3-4 words in English, e.g. ?!M( מהו המניכאיזWhat is Manichaeism?) Answer lengths, however, can vary greatly, depending on whether the annotators wrote minimal spans (typically 1-4 words) or included supporting information in the answer spans (see Section 3.1). 3.4 Linguistic Phenomena As a morphologically-rich language (Tsarfaty et al., 2010; Seddah et al., 2013), modern Hebrew exhibits a variety of non-trivial phenomena that are uncommon in English and could be challenging for NLP models (Tsarfaty et al., 2020). We can identify some of these phenomena in our dataset. Consider for example the following question-answer pair from the validation set: To measure the dataset’s diversity, we cluster questions by their question word (typically the first word in the question). Table 3 shows that what (! )מהand which (! )איזהquestions account for a third 108 Q: A: ?!Mמה היה שטחו של כפר שמריהו כשהוק ma haya shitkho shel kfar shmaryahu what was area-of-it of Kfar Shmaryahu kshe-hukam when-was.established ‘What was Kfar Shmaryahu’s area when it was established?’ ... ! על שטח שלMהישוב הוק ha-yeshuv huka"
2021.naacl-main.17,P06-1084,0,0.0562961,"mbedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subwordlevel models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.1 1 Omer Levy♦♠ Introduction Neural NLP models often operate on the subword level, which requires language-specific tokenizers (Koehn et al., 2007; Adler and Elhadad, 2006) and subword induction algorithms, such as BPE (Sennrich et al., 2016; Kudo, 2018). Instead, working at the byte level by representing each character as a variable number of Unicode (UTF-8) bytes, does not require any form of preprocessing, allowing the model to read and predict every computerized text using a single vocabulary of 256 types. While previous work found that byte-level models tend to underperform models based on subword tokens (Wang et al., 2019), byte-based models exhibit an interesting property: their vocabulary is smaller than the number of latent dimensions (256 &lt; d). In this"
2021.naacl-main.17,K16-1002,0,0.0135459,"er models. We observe this result consistently throughout a wide variety of target languages and writing systems. The fact that removing parameters improves performance is counter-intuitive, especially given recent trends in machine learning that advocate for increasingly larger networks. We further investigate why embeddingless models yield better results and find implicit token dropout (commonly referred to as “word dropout”) as the main source of that boost. While prior work shows that randomly masking tokens from the decoder input can improve the performance of language generation models (Bowman et al., 2016), we find that this effect is amplified when operating at the byte level. Overall, our results suggest that, even without additional parameters, byte-based models can compete and potentially outperform subword models, but that they may require alternative optimization techniques to achieve that goal. 2 Byte Tokenization Modern software typically represents text using Unicode strings (UTF-8), which allows one to encode virtually any writing system using a variable number of bytes per token; English characters are typically represented by a single byte, with other writing systems taking two (e.g"
2021.naacl-main.17,2014.iwslt-evaluation.1,0,0.0203522,"Missing"
2021.naacl-main.17,D18-1461,0,0.0182869,"languages with non-concatenative morphology such as Arabic and Hebrew. Character and byte-based language models (Lee et al., 2017; Al-Rfou et al., 2019) treat the raw text as a sequence of tokens (characters or bytes) and do not require any form of preprocessing or word tokenization, and Choe et al. (2019) even demonstrate that byte-based language models can perform comparably to word-based language models on the billion-word benchmark (Chelba et al., 2013). Although earlier results on LSTM-based machine translation models show that character tokenization can outperform subword tokenization (Cherry et al., 2018), recent literature shows that 6 https://github.com/google/ sentencepiece/blob/master/doc/ experiments.md Conclusions This work challenges two key assumptions in neural machine translation models: the necessity of embedding layers, and the superiority of subword tokenization. Experiments on 10 different languages show that, despite their ubiquitous usage, competitive models can be trained without any embeddings by treating text as a sequence of bytes. Our investigation suggests that different tokenization methods may require revisiting the standard optimization techniques used with transformer"
2021.naacl-main.17,2020.acl-main.145,0,0.0674386,"embeddings, models without embeddings consistently achieve higher BLEU scores in 19 of 20 cases (and an equal score for ru-en), with a boost of about 0.5 BLEU on average. When compared to models based on character embeddings, the embeddingless byte-to-byte approach yields higher BLEU scores in 17 out of 20 cases, though the average difference is quite small in practice (0.3 BLEU). Is subword tokenization superior to bytes or characters? Previous work in machine translation shows that subword models consistently outperform character or byte-based models (Gupta et al., 2019; Wang et al., 2019; Gao et al., 2020). However, our results indicate that this is not necessarily the case. When translating from English to a foreign language, the original direction of the IWSLT dataset, embeddingless byte-to-byte models achieve performance that is equal or better than subword embedding models’ in 8 out of 10 cases. We observe a different trend when translating into English, where subword models surpass other models for every source language; the fact that Moses is a particularly good tokenizer for English – and less so for other languages – is perhaps related to this phenomenon. Whereas prior work proposed clo"
2021.naacl-main.17,Q17-1026,0,0.0148625,"ichardson, 2018) tokenizes raw Unicode strings into subwords using BPE (Sennrich et al., 2016) or unigram LM (Kudo, 2018). Byte BPE (Wang et al., 2019) extends SentencePiece to operate at the byte level. While this approach is indeed more language-agnostic than heuristic tokenizers, it does suffer from performance degradation when no pre-tokenization (e.g. splitting by whitespace) is applied.6 Moreover, the assumption that subword units must be contiguous segments does not hold for languages with non-concatenative morphology such as Arabic and Hebrew. Character and byte-based language models (Lee et al., 2017; Al-Rfou et al., 2019) treat the raw text as a sequence of tokens (characters or bytes) and do not require any form of preprocessing or word tokenization, and Choe et al. (2019) even demonstrate that byte-based language models can perform comparably to word-based language models on the billion-word benchmark (Chelba et al., 2013). Although earlier results on LSTM-based machine translation models show that character tokenization can outperform subword tokenization (Cherry et al., 2018), recent literature shows that 6 https://github.com/google/ sentencepiece/blob/master/doc/ experiments.md Conc"
2021.naacl-main.17,N19-4009,0,0.0382696,"Missing"
2021.naacl-main.17,E17-1074,0,0.022458,"nt optimization techniques to do so. Why does removing the embedding matrix improve the performance of byte-based models? As mentioned in Section 3, the embeddingless models do not use dropout on the encoder input and decoder output, but do apply dropout on the decoder input 7 Related Work while training. Since the embeddingless decoder’s inputs are fixed one-hot vectors, using dropout im- There is prior work on replacing language-specific plicitly drops out complete tokens. In prior work, tokenizers with more universal tokenization aptoken dropout (“word dropout”) has been shown to proaches. Schütze (2017) shows how character have a consistently positive effect (Bowman et al., n-gram embeddings can be effectively trained by 2016). We, therefore, rerun our experiments while segmenting text using a stochastic process. Sen184 Benchmark Src Tgt Embedding-based Models Subword Char Byte the same does not hold for transformers (Gupta et al., 2019; Wang et al., 2019; Gao et al., 2020). To narrow the gap, recent work suggests using deeper models (Gupta et al., 2019) or specialized architectures (Gao et al., 2020). Our work deviates from this trend by removing layers to improve the model. This observatio"
2021.naacl-main.17,P16-1162,0,0.376506,"erformance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subwordlevel models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.1 1 Omer Levy♦♠ Introduction Neural NLP models often operate on the subword level, which requires language-specific tokenizers (Koehn et al., 2007; Adler and Elhadad, 2006) and subword induction algorithms, such as BPE (Sennrich et al., 2016; Kudo, 2018). Instead, working at the byte level by representing each character as a variable number of Unicode (UTF-8) bytes, does not require any form of preprocessing, allowing the model to read and predict every computerized text using a single vocabulary of 256 types. While previous work found that byte-level models tend to underperform models based on subword tokens (Wang et al., 2019), byte-based models exhibit an interesting property: their vocabulary is smaller than the number of latent dimensions (256 &lt; d). In this work, we demonstrate that this property allows us to remove the inpu"
2021.naacl-main.17,D19-5214,0,0.0374382,"troduction Neural NLP models often operate on the subword level, which requires language-specific tokenizers (Koehn et al., 2007; Adler and Elhadad, 2006) and subword induction algorithms, such as BPE (Sennrich et al., 2016; Kudo, 2018). Instead, working at the byte level by representing each character as a variable number of Unicode (UTF-8) bytes, does not require any form of preprocessing, allowing the model to read and predict every computerized text using a single vocabulary of 256 types. While previous work found that byte-level models tend to underperform models based on subword tokens (Wang et al., 2019), byte-based models exhibit an interesting property: their vocabulary is smaller than the number of latent dimensions (256 &lt; d). In this work, we demonstrate that this property allows us to remove the input and output embedding layers from byte-to-byte translation models, and in doing so, improve the models’ performance consistently. We replace the dense trainable embedding matrix with a fixed one-hot encoding of the vocabulary as the first and last layers of a standard transformer model. Machine translation experiments on 10 language pairs show that byte-to-byte models without an embedding la"
2021.naacl-main.17,P07-2045,0,0.00856295,"ing the ubiquitous embedding layer with one-hot representations of each byte does not hurt performance; experiments on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subwordlevel models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.1 1 Omer Levy♦♠ Introduction Neural NLP models often operate on the subword level, which requires language-specific tokenizers (Koehn et al., 2007; Adler and Elhadad, 2006) and subword induction algorithms, such as BPE (Sennrich et al., 2016; Kudo, 2018). Instead, working at the byte level by representing each character as a variable number of Unicode (UTF-8) bytes, does not require any form of preprocessing, allowing the model to read and predict every computerized text using a single vocabulary of 256 types. While previous work found that byte-level models tend to underperform models based on subword tokens (Wang et al., 2019), byte-based models exhibit an interesting property: their vocabulary is smaller than the number of latent dim"
2021.naacl-main.17,P18-1007,0,0.134433,"on byte-to-byte machine translation from English to 10 different languages show a consistent improvement in BLEU, rivaling character-level and even standard subwordlevel models. A deeper investigation reveals that the combination of embeddingless models with decoder-input dropout amounts to token dropout, which benefits byte-to-byte models in particular.1 1 Omer Levy♦♠ Introduction Neural NLP models often operate on the subword level, which requires language-specific tokenizers (Koehn et al., 2007; Adler and Elhadad, 2006) and subword induction algorithms, such as BPE (Sennrich et al., 2016; Kudo, 2018). Instead, working at the byte level by representing each character as a variable number of Unicode (UTF-8) bytes, does not require any form of preprocessing, allowing the model to read and predict every computerized text using a single vocabulary of 256 types. While previous work found that byte-level models tend to underperform models based on subword tokens (Wang et al., 2019), byte-based models exhibit an interesting property: their vocabulary is smaller than the number of latent dimensions (256 &lt; d). In this work, we demonstrate that this property allows us to remove the input and output"
2021.naacl-main.17,D18-2012,0,0.0198922,".8 21.2 36.8 13.1 18.2 29.3 13.1 14.3 29.1 12.2 27.1 20.8 36.8 12.7 17.7 29.2 12.5 14.4 28.9 12.1 27.1 21.0 36.8 12.9 18.2 29.1 13.1 14.1 28.7 12.1 26.7 zh es ar ru de ja tr vi fa he en en en en en en en en en en 17.3 40.0 32.0 22.9 35.6 13.5 24.3 27.4 24.5 38.2 17.2 39.1 31.1 22.4 34.9 12.8 23.3 25.9 23.2 37.8 16.3 39.1 31.2 22.5 35.0 12.3 23.7 25.9 23.3 37.4 16.1 38.8 30.8 22.0 34.5 11.2 23.3 25.3 22.6 37.4 8 Table 4: Test BLEU scores of the baseline and embeddingless models on the IWSLT dataset, when decoderside token dropout is considered as a potential hyperparameter setting. tencePiece (Kudo and Richardson, 2018) tokenizes raw Unicode strings into subwords using BPE (Sennrich et al., 2016) or unigram LM (Kudo, 2018). Byte BPE (Wang et al., 2019) extends SentencePiece to operate at the byte level. While this approach is indeed more language-agnostic than heuristic tokenizers, it does suffer from performance degradation when no pre-tokenization (e.g. splitting by whitespace) is applied.6 Moreover, the assumption that subword units must be contiguous segments does not hold for languages with non-concatenative morphology such as Arabic and Hebrew. Character and byte-based language models (Lee et al., 2017"
2021.naacl-main.209,D18-1336,0,0.266329,"dels. Can they improve autoregressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing. 1 Introduction Latent alignment objectives, such as CTC (Graves et al., 2006) and AXE (Ghazvininejad et al., 2020a), have been recently proposed for training nonautoregressive models for machine translation (Libovický and Helcl, 2018; Saharia et al., 2020). These objectives use a dynamic program to comb the space of monotonic alignments between the “gold” target sequence and the token probabilities the model predicts, thus reducing the loss from positional misalignments and focusing on the original prediction error instead. For example, consider the target sequence “there is a tiny difference between pink and magenta”; if the model’s distribution favors the paraphrase “there is a very small difference between pink and magenta”, substituting one token (“tiny”) with two (“very small”) will cause a misalignment, and result i"
2021.naacl-main.209,N19-4009,0,0.0545436,"Missing"
2021.naacl-main.209,P02-1040,0,0.114153,"h AXE, and add the empty token ε to the vocabulary. We remove the ε tokens after decoding. Figure 2: Training and validation loss when using the AXE objective on IWSLT’14 DE-EN with an autoregressive model. (Sennrich et al., 2016) using the scripts provided by fairseq. We also use the implementation’s default hyperparameters: 6 layers of encoder/decoder, 512 model dimensions, 1024 hidden dimensions, 4 attention heads. We optimize with Adam (Kingma and Ba, 2015) for 50k steps with early stopping using 4096 tokens per batch. We decode with beam search (b = 5) and evaluate performance with BLEU (Papineni et al., 2002). Results We observe two seemingly contradictory behaviors. On the one hand, the model approaches a near-zero training loss within a single epoch, and observes similar results when computing AXE loss on unseen examples in the validation set (Figure 2). Meanwhile, at inference time, the model consistently produces the empty sequence (after removing all instances of ε), scoring 0 BLEU on the test set. This indicates that the model has learned to “game” the AXE objective without actually learning anything useful about machine translation. What shortcut did the model learn? 5 Analysis To understan"
2021.naacl-main.209,2020.emnlp-main.83,0,0.206348,"regressive models as well? We explore the possibility of training autoregressive machine translation models with latent alignment objectives, and observe that, in practice, this approach results in degenerate models. We provide a theoretical explanation for these empirical results, and prove that latent alignment objectives are incompatible with teacher forcing. 1 Introduction Latent alignment objectives, such as CTC (Graves et al., 2006) and AXE (Ghazvininejad et al., 2020a), have been recently proposed for training nonautoregressive models for machine translation (Libovický and Helcl, 2018; Saharia et al., 2020). These objectives use a dynamic program to comb the space of monotonic alignments between the “gold” target sequence and the token probabilities the model predicts, thus reducing the loss from positional misalignments and focusing on the original prediction error instead. For example, consider the target sequence “there is a tiny difference between pink and magenta”; if the model’s distribution favors the paraphrase “there is a very small difference between pink and magenta”, substituting one token (“tiny”) with two (“very small”) will cause a misalignment, and result in a disproportionately"
2021.naacl-main.209,P16-1162,0,0.0466512,"even when m = n, because it uses both the delimiter operator (which inflates Y ) as well as the clone prediction operator (which inflates P ). 4 Applying AXE to Autoregressive NMT To apply AXE to autoregressive machine translation, we use a standard sequence-to-sequence transformer model (Vaswani et al., 2017) trained with teacher forcing, replace the simple cross entropy loss function with AXE, and add the empty token ε to the vocabulary. We remove the ε tokens after decoding. Figure 2: Training and validation loss when using the AXE objective on IWSLT’14 DE-EN with an autoregressive model. (Sennrich et al., 2016) using the scripts provided by fairseq. We also use the implementation’s default hyperparameters: 6 layers of encoder/decoder, 512 model dimensions, 1024 hidden dimensions, 4 attention heads. We optimize with Adam (Kingma and Ba, 2015) for 50k steps with early stopping using 4096 tokens per batch. We decode with beam search (b = 5) and evaluate performance with BLEU (Papineni et al., 2002). Results We observe two seemingly contradictory behaviors. On the one hand, the model approaches a near-zero training loss within a single epoch, and observes similar results when computing AXE loss on unsee"
C16-1272,S12-1051,0,0.158401,"rase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which"
C16-1272,J05-3002,0,0.134834,"Missing"
C16-1272,P99-1071,0,0.315577,"Missing"
C16-1272,W04-1016,0,0.0794161,"Missing"
C16-1272,C04-1051,0,0.749669,"e it is difficult for non-experts to annotate. We analyze 200 pairs of similar sentences and identify several underlying properties of sentence intersection. We leverage these insights to design an algorithm that decomposes the sentence intersection task into several simpler annotation tasks, facilitating the construction of a high quality dataset via crowdsourcing. We implement this approach and provide an annotated dataset of 1,764 sentence intersections. 1 Introduction Various paradigms exist for comparing the meanings of two texts and modeling their semantic overlap. Paraphrase detection (Dolan et al., 2004), for example, tries to identify whether two texts express the same information. It cannot, however, capture cases where there is only partial information overlap. One paradigm that addresses this issue is textual entailment (Dagan et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for th"
C16-1272,D08-1019,0,0.068328,"Missing"
C16-1272,P08-2049,0,0.0504803,"Missing"
C16-1272,E14-1057,0,0.0558176,"Missing"
C16-1272,P16-2041,1,0.833208,"sist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t,"
C16-1272,P13-2080,1,0.857695,"d in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in semantic text similarity, sentence intersection captures what this shared information is. Although sentence intersection has existed for over a decade, it has received little attention due to a lack of annotated data. Previous annotation attempts have either used experts, which did not scale, or crowdsourcing, which yielded unreliable annotations (McKeown et al., 2010). We also observe that annotating sentence intersection is difficult for non-experts. We hypothesize that this difficulty stems from the task’s require"
C16-1272,W04-1013,0,0.0536885,"ria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i.e. s1 ∩ s2 ∩ s3 = (s1"
C16-1272,W05-1612,0,0.420831,"et al., 2006), which asks whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than q"
C16-1272,S07-1009,0,0.0447691,"in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree within that sentence t, and another subtree t0 , which is not necessarily part of s. It creates a new sentence s0 by"
C16-1272,N10-1044,0,0.285035,"s whether one text can be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount"
C16-1272,P13-1131,1,0.858605,"context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sentence tree s, a subtree"
C16-1272,N04-1019,0,0.0377941,"Missing"
C16-1272,P02-1040,0,0.0954815,"scope. Combining both these criteria – output set and extractiveness – we define extractive sentence intersection as the set of all sentences that each contains all the information common to the input sentences, while being composed only of words that appeared in the input sentences and placeholders.2 The entire set of extractive intersections allows for more accurate automatic evaluation, since the evaluation mechanism does not need to overcome issues in lexical variability; instead, it can simply select the most similar expert-crafted sentence from the set using simple metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004). Having multiple possible solutions is not a foreign concept to NLP, and is widely used in translation and summarization. 2 While this paper discusses intersections between two input sentences, one can theoretically extend this setting to multiple input sentences by consecutively applying the intersection operation. For example, if we have three sentences s1 , s2 , s3 , we could first find the intersection between s1 and s2 , and then for each sentence s0 in s1 ∩ s2 , intersect S that with s3 . We would essentially take the union of the latter set of intersection sets, i."
C16-1272,P15-2070,0,0.0224273,"Missing"
C16-1272,P13-1051,0,0.0304221,"Missing"
C16-1272,W11-1606,0,0.0173189,"be inferred from another. While textual entailment models when the information of one text is subsumed by another, it falls short when neither text strictly subsumes the other, yet a significant amount of information is common to both. A more recent paradigm, semantic text similarity (Agirre et al., 2012), allows for these cases by measuring “how much” similar two sentences are. In this paper, we consider an additional paradigm for modeling the semantic overlap between two texts. The task of sentence intersection, as defined in several variants (Marsi and Krahmer, 2005; McKeown et al., 2010; Thadani and McKeown, 2011), is to construct a sentence containing all the information shared by the two input sentences (Figure 1). While sentence intersection was originally proposed for abstractive summarization, we argue that it is an interesting generic paradigm for modeling information overlap, which generalizes over several previous approaches. In particular, it is expressive enough to capture partial semantic overlaps that are not modeled by textual entailment or even partial textual entailment (Nielsen et al., 2009; Levy et al., 2013). Furthermore, rather than quantifying the amount of shared information as in"
C16-1272,I13-1198,0,0.0305508,"Missing"
C16-1272,P12-2031,1,0.836148,"subtree entailment in context, which will assist in defining the underlying structure of extractive intersection. Subtree entailment in context models inference between rich lexical-syntactic patterns (subtrees of syntactic dependency trees), while considering internal context (instantiated arguments) and external context (substitution in a complete sentence). As such, it generalizes over previous ideas presented separately in prior art; subtrees were used in TEASE (Szpektor et al., 2015) and PPDB (Pavlick et al., 2015), internal context was considered in contextsensitive relation inference (Zeichner et al., 2012; Melamud et al., 2013; Levy and Dagan, 2016), and external context is studied in the lexical substitution task (McCarthy and Navigli, 2007; Biemann, 2013; Kremer et al., 2014). Subtree entailment in context is the first notion that combines all these traits. In addition to these advantages, we also introduce a new mechanism for handling changes in arity, in case one subtree contains more/less arguments than another. The Substitution Function To define subtree entailment in context, we must first define an auxiliary operation – subtree substitution. The substitution function σ is given a sente"
C16-1272,N07-1051,0,\N,Missing
D19-1588,P15-1136,0,0.547985,"Missing"
D19-1588,D16-1245,0,0.283549,"xample, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT (Joshi et al., 2019) focuses on pretraining span representations achieving current state of the art results on OntoNotes with the independ"
D19-1588,D08-1069,0,0.0429317,"errors suggest that models are still unable to resolve cases requiring mention paraphrasing. For example, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT (Joshi et al., 2019) focuses on pretrainin"
D19-1588,N19-1423,0,0.641498,"1) and GAP (+11.5 F1) benchmarks. A qualitative analysis of model predictions indicates that, compared to ELMo and BERT-base, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available1 . 1 Introduction Recent BERT-based models have reported dramatic gains on multiple semantic benchmarks including question-answering, natural language inference, and named entity recognition (Devlin et al., 2019). Apart from better bidirectional reasoning, one of BERT’s major improvements over previous methods (Peters et al., 2018; McCann et al., 2017) is passage-level training,2 which allows it to better model longer sequences. We fine-tune BERT to coreference resolution, achieving strong improvements on the GAP (Webster et al., 2018) and OntoNotes (Pradhan et al., 2012) benchmarks. We present two ways of extending the c2f-coref model in Lee et al. (2018). The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT. The overlap variant splits the docum"
D19-1588,D13-1203,0,0.0958185,"lve cases requiring mention paraphrasing. For example, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies. SpanBERT (Joshi et al., 2019) focuses on pretraining span representations achieving current state of"
D19-1588,P19-1064,0,0.467621,"Missing"
D19-1588,W12-4502,0,0.0428419,"Missing"
D19-1588,P19-1066,0,0.511259,"Missing"
D19-1588,D17-1018,1,0.948252,"s from using longer context windows (384 word pieces) while BERT-base performs better with shorter contexts (128 word pieces). Yet, both variants perform much worse with longer context windows (512 tokens) in spite of being trained on 512-size contexts. Moreover, the overlap variant, which artificially extends the context window beyond 512 tokens provides no improvement. This indicates that using larger context windows for pretraining might not translate into effective long-range features for a downstream task. Larger models also exacerbate the memory-intensive nature of span representations (Lee et al., 2017), which have driven recent improvements in coreference resolution. Together, these observations suggest that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. 2 Method For our experiments, we use the higher-order coreference model in Lee et al. (2018) which is the current state of the art for the English OntoNotes dataset (Pradhan et al., 2012). We refer to this as c2f-coref in the paper. 2.1 Overview of c2f-coref For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : 5803 Proceedings of t"
D19-1588,N18-2108,1,0.806336,"Missing"
D19-1588,P15-1137,0,0.743197,"ning. Likewise, we use a batch size of 1 document following (Lee et al., 2018). While training the large model requires 32GB GPUs, all models can be tested on 16GB GPUs. We use the cased English variants in all our experiments. Baselines We compare the c2f-coref + BERT system with two main baselines: (1) the original ELMo-based c2f-coref system (Lee et al., 2018), and (2) its predecessor, e2e-coref (Lee et al., 3 http://github.com/kentonl/e2e-coref/ https://github.com/google-research/ bert 5804 4 P MUC R F1 B3 R P F1 P CEAFφ4 R F1 Avg. F1 Martschat and Strube (2015) (Clark and Manning, 2015) (Wiseman et al., 2015) Wiseman et al. (2016) Clark and Manning (2016) e2e-coref (Lee et al., 2017) c2f-coref (Lee et al., 2018) Fei et al. (2019) EE (Kantor and Globerson, 2019) 76.7 76.1 76.2 77.5 79.2 78.4 81.4 85.4 82.6 68.1 69.4 69.3 69.8 70.4 73.4 79.5 77.9 84.1 72.2 72.6 72.6 73.4 74.6 75.8 80.4 81.4 83.4 66.1 65.6 66.2 66.8 69.9 68.6 72.2 77.9 73.3 54.2 56.0 55.8 57.0 58.0 61.8 69.5 66.4 76.2 59.6 60.4 60.5 61.5 63.4 65.0 70.8 71.7 74.7 59.5 59.4 59.4 62.1 63.5 62.7 68.2 70.6 72.4 52.3 53.0 54.9 53.9 55.5 59.0 67.1 66.3 71.1 55.7 56.0 57.1 57.7 59.2 60.8 67.6 68.4 71.8 62.5 63.0 63.4 64.2 65.7 67.2 73.0 73.8"
D19-1588,N16-1114,0,0.609599,"Missing"
D19-1588,P19-1593,1,0.896497,"Missing"
D19-1588,Q15-1029,0,0.11685,"Missing"
D19-1588,C02-1139,0,0.193019,"he document. Lastly, a considerable number of errors suggest that models are still unable to resolve cases requiring mention paraphrasing. For example, bridging the Royals with Prince Charles and his wife Camilla likely requires pretraining models to encode relations between entities, especially considering that such learning signal is rather sparse in the training set. 5 Related Work Scoring span or mention pairs has perhaps been one of the most dominant paradigms in coreference resolution. The base coreference model used in this paper from Lee et al. (2018) belongs to this family of models (Ng and Cardie, 2002; Bengtson and Roth, 2008; Denis and Baldridge, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016; Lee et al., 2017). More recently, advances in coreference resolution and other NLP tasks have been driven by unsupervised contextualized representations (Peters et al., 2018; Devlin et al., 2019; McCann et al., 2017; Joshi et al., 2019). Of these, BERT (Devlin et al., 2019) notably uses pretraining on passage-level sequences (in conjunction with a bidirectional masked language modeling objective) to more effectively model long-range dependencies."
D19-1588,N18-1202,1,0.900076,"ase, BERT-large is particularly better at distinguishing between related but distinct entities (e.g., President and CEO). However, there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. Our code and models are publicly available1 . 1 Introduction Recent BERT-based models have reported dramatic gains on multiple semantic benchmarks including question-answering, natural language inference, and named entity recognition (Devlin et al., 2019). Apart from better bidirectional reasoning, one of BERT’s major improvements over previous methods (Peters et al., 2018; McCann et al., 2017) is passage-level training,2 which allows it to better model longer sequences. We fine-tune BERT to coreference resolution, achieving strong improvements on the GAP (Webster et al., 2018) and OntoNotes (Pradhan et al., 2012) benchmarks. We present two ways of extending the c2f-coref model in Lee et al. (2018). The independent variant uses non-overlapping segments each of which acts as an independent instance for BERT. The overlap variant splits the document into overlapping segments so as to provide the model with context beyond 512 tokens. BERT-large improves over ELMo-b"
D19-1588,W12-4501,0,0.470215,"r context windows for pretraining might not translate into effective long-range features for a downstream task. Larger models also exacerbate the memory-intensive nature of span representations (Lee et al., 2017), which have driven recent improvements in coreference resolution. Together, these observations suggest that there is still room for improvement in modeling document-level context, conversations, and mention paraphrasing. 2 Method For our experiments, we use the higher-order coreference model in Lee et al. (2018) which is the current state of the art for the English OntoNotes dataset (Pradhan et al., 2012). We refer to this as c2f-coref in the paper. 2.1 Overview of c2f-coref For each mention span x, the model learns a distribution P (·) over possible antecedent spans Y : 5803 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 5803–5808, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics P (y) = P es(x,y) y 0 ∈Y es(x,y0 ) (1) The scoring function s(x, y) between spans x and y uses fixed-length span representations, gx and gy to represent its in"
D19-1588,D08-1031,0,\N,Missing
D19-1633,Q19-1042,0,0.330019,"sked words in the target translation. We use transformer CMLMs, where the decoder’s self attention (Vaswani et al., 2017) can attend to the ∗ Equal contribution, sorted alphabetically. Our code is publicly available at: https://github.com/facebookresearch/Mask-Predict 1 Luke Zettlemoyer entire sequence (left and right context) to predict each masked word. We train with a simple masking scheme where the number of masked target tokens is distributed uniformly, presenting the model with both easy (single mask) and difficult (completely masked) examples. Unlike recently proposed insertion models (Gu et al., 2019; Stern et al., 2019), which treat each token as a separate training instance, CMLMs can train from the entire sequence in parallel, resulting in much faster training. We also introduce a new decoding algorithm, mask-predict, which uses the order-agnostic nature of CMLMs to support highly parallel decoding. Mask-predict repeatedly masks out and repredicts the subset of words in the current translation that the model is least confident about, in contrast to recent parallel decoding translation approaches that repeatedly predict the entire sequence (Lee et al., 2018). Decoding starts with a comp"
D19-1633,D18-1149,0,0.161842,"tly proposed insertion models (Gu et al., 2019; Stern et al., 2019), which treat each token as a separate training instance, CMLMs can train from the entire sequence in parallel, resulting in much faster training. We also introduce a new decoding algorithm, mask-predict, which uses the order-agnostic nature of CMLMs to support highly parallel decoding. Mask-predict repeatedly masks out and repredicts the subset of words in the current translation that the model is least confident about, in contrast to recent parallel decoding translation approaches that repeatedly predict the entire sequence (Lee et al., 2018). Decoding starts with a completely masked target text, to predict all of the words in parallel, and ends after a constant number of mask-predict cycles. This overall strategy allows the model to repeatedly reconsider word choices within a rich bi-directional context and, as we will show, produce high-quality translations in just a few cycles. Experiments on benchmark machine translation datasets show the strengths of mask-predict decoding for transformer CMLMs. With just 4 iterations, BLEU scores already surpass the performance of the best non-autoregressive and parallel decoding models.2 Wit"
D19-1633,D18-1336,0,0.199333,"Missing"
D19-1633,N19-4007,0,0.0181455,"into a uni-modal distribution. The decrease in repetitions also correlates with the steep rise in translation quality (BLEU), supporting the conjecture of Gu et al. (2018) that multi-modality is a major roadblock for purely non-autoregressive machine translation. 5.2 Do Longer Sequences Need More Iterations? A potential concern with using a constant amount of decoding iterations is that it may be effective for short sequences (where the number of iterations T is closer to the output’s length N ), but insufficient for longer sequences. To determine whether this is the case, we use compare-mt (Neubig et al., 2019) to bucket the evaluation data by target sentence length and compute the performance with different values of T . Table 4 shows that increasing the number of decoding iterations (T ) appears to mainly improve the performance on longer sequences. Having said that, the performance differences across length buckets are not very large, and it seems that even 4 mask-predict iterations are enough to produce decent translations for long sequences (40 ≤ N ). 5.3 Do More Length Candidates Help? Traditional autoregressive models can dynamically decide the length of the target sequence by generating a sp"
D19-1633,P02-1040,0,0.104444,"of standard autoregressive models (Section 4.2), while decoding significantly faster (Section 4.3). 4.1 Experimental Setup Translation Benchmarks We evaluate on three standard datasets, WMT’14 EN-DE (4.5M sentence pairs), WMT’16 EN-RO (610k pairs) and WMT’17 EN-ZH (20M pairs) in both directions. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We use the same preprocessed data as Vaswani et al. (2017) and Wu et al. (2019) for WMT’14 EN-DE and WMT’17 EN-ZH respectively, and use the data from Lee et al. (2018) for WMT’16 EN-RO. We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except from EN to ZH, where we use SacreBLEU (Post, 2018).3 Hyperparameters We follow most of the standard hyperparameters for transformers in the base configuration (Vaswani et al., 2017): 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We also experiment with 512 hidden dimensions, for comparison with previous parallel decoding models (Gu et al., 2018; Lee et al., 2018). We follow the weight initialization scheme from BERT (Devlin et al., 2018), which samples weights from N (0, 0.02), initializes biases to zero, and sets"
D19-1633,W18-6319,0,0.0196285,"n 4.3). 4.1 Experimental Setup Translation Benchmarks We evaluate on three standard datasets, WMT’14 EN-DE (4.5M sentence pairs), WMT’16 EN-RO (610k pairs) and WMT’17 EN-ZH (20M pairs) in both directions. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We use the same preprocessed data as Vaswani et al. (2017) and Wu et al. (2019) for WMT’14 EN-DE and WMT’17 EN-ZH respectively, and use the data from Lee et al. (2018) for WMT’16 EN-RO. We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except from EN to ZH, where we use SacreBLEU (Post, 2018).3 Hyperparameters We follow most of the standard hyperparameters for transformers in the base configuration (Vaswani et al., 2017): 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We also experiment with 512 hidden dimensions, for comparison with previous parallel decoding models (Gu et al., 2018; Lee et al., 2018). We follow the weight initialization scheme from BERT (Devlin et al., 2018), which samples weights from N (0, 0.02), initializes biases to zero, and sets layer normalization parameters to β = 0, γ = 1. For regularization, we use 0.3 dr"
D19-1633,P16-1162,0,0.267068,"). 4 Experiments We evaluate CMLMs with mask-predict decoding on standard machine translation benchmarks. We find that our approach significantly outperforms prior parallel decoding machine translation methods and even approaches the performance of standard autoregressive models (Section 4.2), while decoding significantly faster (Section 4.3). 4.1 Experimental Setup Translation Benchmarks We evaluate on three standard datasets, WMT’14 EN-DE (4.5M sentence pairs), WMT’16 EN-RO (610k pairs) and WMT’17 EN-ZH (20M pairs) in both directions. The datasets are tokenized into subword units using BPE (Sennrich et al., 2016). We use the same preprocessed data as Vaswani et al. (2017) and Wu et al. (2019) for WMT’14 EN-DE and WMT’17 EN-ZH respectively, and use the data from Lee et al. (2018) for WMT’16 EN-RO. We evaluate performance with BLEU (Papineni et al., 2002) for all language pairs, except from EN to ZH, where we use SacreBLEU (Post, 2018).3 Hyperparameters We follow most of the standard hyperparameters for transformers in the base configuration (Vaswani et al., 2017): 6 layers per stack, 8 attention heads per layer, 512 model dimensions, 2048 hidden dimensions. We also experiment with 512 hidden dimensions"
D19-1633,W19-2304,0,0.0577819,"encoder and decoder parameters (as in our model) and pre-training them jointly in an autoregressive version of masked language modeling, although with monolingual data. While this work demonstrates that pretraining CMLMs can improve autoregressive machine translation, it does not try to leverage the parallel and bi-directional nature of CMLMs to generate text in a non-left-to-right manner. Generating from Masked Language Models One such approach for generating text from a masked language model casts BERT (Devlin et al., 2018), a non-conditional masked language model, as a Markov random field (Wang and Cho, 2019). By masking a sequence of length N and then iteratively sampling a single token at each time from the model (either sequentially or in arbitrary order), one can produce grammatical examples. While this sampling process has a theoretical justification, it also requires N forward passes of the model; mask-predict decoding, on the other hand, can produce text in a constant number of iterations. Parallel Decoding for Machine Translation There have been several advances in parallel decoding machine translation by training nonautoregressive models. Gu et al. (2018) introduce a transformer-based app"
D19-5506,P18-1163,0,0.0999062,"of characters, while Sakaguchi et al. (2017) use character-level recurrent neural networks combined with special representations for the first and last characters of each token. These models are particularly suited for specific types of swapping and scrambling noises, but are not robust to natural noise. We conducted preliminary experiments with noise-invariant encoders, but obtained better results by adding noise at training time. A related idea is to optimize an adversarial objective, in which a discriminator tries to distinguish noised and clean examples from their encoded representations (Cheng et al., 2018). This improves performance on clean data, but it makes optimization unstable, which is a well-known defect of adversarial learning (Arjovsky et al., 2017). Cheng et al. (2018) do not evaluate on natural noise. Table 5: The performance of a machine translation model on the MTNT task. non-standard spellings inherent to the dataset. As shown in Table 5, noised training has minimal impact on performance. We did not exhaustively explore the space of possible noising strategies, and so these negative results should be taken only as a preliminary finding. Nonetheless, there are reasons to believe th"
D19-5506,P18-2006,0,0.0531483,"generally trained on clean data, without spelling errors. Yet many translation scenarios require robustness to such errors: for example, social media text in which there is little emphasis on standard spelling (Michel and Neubig, 2018), and interactive settings in which users must enter text on a mobile device. Systems trained on clean data generally perform poorly when faced with such errors at test time (Heigold et al., 2017; Belinkov and Bisk, 2018). One potential solution is to introduce noise at training time, similar in spirit to the use of adversarial examples (Goodfellow et al., 2014; Ebrahimi et al., 2018). So far, using synthetic noise at training time has been found to improve performance only on test data with exactly the same kind of synthetic noise, while at the same time impairing performance on clean test data (Heigold et al., 2017; Belinkov and Bisk, 2018). We desire methods that perform well on both clean text and naturally-occurring noise, but this is beyond the current state of the art. ∗ 2 Noise Models We focus on orthographical noise; character-level noise that affects the spelling of individual terms. Orthographical noise is problematic for machine translation systems that operate"
D19-5506,N19-1190,0,0.0570227,"r, we find that deleting and inserting random characters play a key role in preparing the model for test-time typos. While our method works well on misspellings, it does not appear to generalize to non-standard text in social media. We conjecture that spelling mistakes constitute a small part of the deviations from standard text, and that the main challenges in this 1 Contemporaneous work shows that MTNT performance can be improved by a domain-specific noising distribution that includes character insertions and deletions, as well as the random insertion of emoticons, stopwords, and profanity (Vaibhav et al., 2019). The specific impact of spelling noise is not evaluated, nor is the impact on clean text. 45 domain stem from other linguistic phenomena. Lasse Holmstrom and Petri Koistinen. 1992. Using additive noise in back-propagation training. IEEE Transactions on Neural Networks, 3(1):24–38. Acknowledgments Thanks to the anonymous reviewers for their feedback. We also thank Luke Zettlemoyer and our colleagues at FAIR for valuable feedback. Specifically, we thank Abdelrahman Mohamed for sharing his expertise on nonautoregressive models. Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016."
D19-5506,N13-1037,1,0.800142,"cter encoders to spelling errors across multiple input languages (German, French, and Czech). Of the different noise types we use at training, we find that random character deletions are particularly useful, followed by character insertions. However, noisy training does not improve translations of social media text, as indicated by performance on the MTNT dataset of Reddit posts (Michel and Neubig, 2018). This finding aligns with previous work arguing that the distinctive feature of social media text is not noise or orthographical errors, but rather, variation in writing style and vocabulary (Eisenstein, 2013). Contemporary machine translation systems achieve greater coverage by applying subword models such as BPE and character-level CNNs, but these methods are highly sensitive to orthographical variations such as spelling mistakes. We show how training on a mild amount of random synthetic noise can dramatically improve robustness to these variations, without diminishing performance on clean text. We focus on translation performance on natural typos, and show that robustness to such noise can be achieved using a balanced diet of simple synthetic noises at training time, without access to the natura"
D19-5506,E12-1054,0,0.0614443,"Missing"
D19-5506,max-wisniewski-2010-mining,0,\N,Missing
E17-1072,Q16-1031,0,0.026374,".3869 .4119 .1364 .2408 .1280 .1877 .1403 .1791 .2299 .2759 .2207 .2598 0.2830 0 Multilingual SID-SGNS .4433 .4632 .4893 .5015 .4047 .4151 .4091 .4302 .2989 .3049 .2514 .2753 .2737 .3195 .2404 .2945 .3304 .3893 .3509 .3868 .4058 .4376 .1605 .3082 .1591 .2584 .1448 .2403 .2482 .3372 .2437 .3080 0.3289 22 Table 3: The performance of SID-SGNS compared to state-of-the-art cross-lingual embedding methods and traditional alignment methods. advantage, SID-SGNS performs significantly better than the other methods combined.6 This result is similar in vein to recent findings in the parsing literature (Ammar et al., 2016; Guo et al., 2016), where multi-lingual transfer was shown to improve upon bilingual transfer. In absolute terms, Multilingual SID-SGNS’s performance is still very low. However, this experiment demonstrates that one way of making significant improvement in cross-lingual embeddings is by considering additional sources of information, such as the multi-lingual signal demonstrated here. We hypothesize that, regardless of the algorithmic approach, relying solely on sentence IDs from bilingual parallel corpora will probably not be able to improve much beyond IBM Model-1. 6 Data Paradigms In §2, we"
E17-1072,J93-2003,0,0.156922,"oss-lingual similarity. Another important delineation of this work is that we focus on algorithms that rely on sentence-aligned data; in part, because these algorithms are particularly interesting for low-resource languages, but also to make our analysis and comparison with alignment algorithms more focused. We observe that the top performing embedding algorithms share the same underlying feature space – sentence IDs – while their different algorithmic approaches seem to have a negligible impact on performance. We also notice that several statistical alignment algorithms, such as IBM Model-1 (Brown et al., 1993), operate under the same data assumptions. Specifically, we find that using the translation probabilities learnt by Model-1 as the cross-lingual similarity function (in place of the commonly-used cosine similarity between word embeddings) performs on-par with state-of-the-art cross-lingual embeddings on word alignment and bilingual dictionary induction tasks. In other words, as long as the similarity function is based on the sentence ID feature space and the embedding/alignment algorithm itself is not too na¨ıve, the actual difference in performance between different approaches is marginal. Th"
E17-1072,E14-1049,0,0.0924706,"nce. This paper draws both empirical and theoretical parallels between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account. 1 Introduction Cross-lingual word embedding algorithms try to represent the vocabularies of two or more languages in one common continuous vector space. These vectors can be used to improve monolingual word similarity (Faruqui and Dyer, 2014) or support cross-lingual transfer (Gouws and Søgaard, 2015). In this work, we focus on the second (cross-lingual) aspect of these embeddings, and try to determine what makes some embedding approaches better than others on a set of ∗ These authors contributed equally to this work. 765 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 765–774, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics generalize it, resulting in an embedding model that is as effective as Model-1 and o"
E17-1072,N15-1157,1,0.39673,"els between the embedding and alignment literature, and suggests that adding additional sources of information, which go beyond the traditional signal of bilingual sentence-aligned corpora, may substantially improve cross-lingual word embeddings, and that future baselines should at least take such features into account. 1 Introduction Cross-lingual word embedding algorithms try to represent the vocabularies of two or more languages in one common continuous vector space. These vectors can be used to improve monolingual word similarity (Faruqui and Dyer, 2014) or support cross-lingual transfer (Gouws and Søgaard, 2015). In this work, we focus on the second (cross-lingual) aspect of these embeddings, and try to determine what makes some embedding approaches better than others on a set of ∗ These authors contributed equally to this work. 765 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 765–774, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics generalize it, resulting in an embedding model that is as effective as Model-1 and other sophisticated state-of-the-art embedding methods, but t"
E17-1072,graca-etal-2008-building,0,0.0204464,"nt-aligned data, and can be built in a similar manner: create a pseudo-bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a sour"
E17-1072,J03-1002,0,0.116258,"similarity function (in place of the commonly-used cosine similarity between word embeddings) performs on-par with state-of-the-art cross-lingual embeddings on word alignment and bilingual dictionary induction tasks. In other words, as long as the similarity function is based on the sentence ID feature space and the embedding/alignment algorithm itself is not too na¨ıve, the actual difference in performance between different approaches is marginal. This leads us to revisit another statistical alignment algorithm from the literature that uses the same sentence-based signal – the Dice aligner (Och and Ney, 2003). We first observe that the vanilla Dice aligner is significantly outperformed by the Model-1 aligner. We then recast Dice as the dot-product between two word vectors (based on the sentence ID feature space), which allows us to While cross-lingual word embeddings have been studied extensively in recent years, the qualitative differences between the different algorithms remain vague. We observe that whether or not an algorithm uses a particular feature set (sentence IDs) accounts for a significant performance gap among these algorithms. This feature set is also used by traditional alignment alg"
E17-1072,P15-1165,1,0.0916604,"Missing"
E17-1072,W11-4615,0,0.0169403,"bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not aligned with anything, whereas ta"
E17-1072,C12-1089,0,0.0182685,"show that a generalization of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representatio"
E17-1072,2005.mtsummit-papers.11,0,0.0150526,"Missing"
E17-1072,J10-4005,0,0.075475,"Missing"
E17-1072,P16-1157,0,0.0326699,"-based model over the same features. 1 bitbucket.org/omerlevy/xling_ embeddings We study which factors determine the success of cross-lingual word embedding algorithms 2 Background: Cross-lingual Embeddings 766 that use sentence-aligned data, and evaluate them against baselines from the statistical machine translation literature that incorporate the same data assumptions. We go on to generalize one of these, the Dice aligner, showing that one variant is a much stronger baseline for cross-lingual word embedding algorithms than standard baselines. Finally, we would like to point out the work of Upadhyay et al. (2016), who studied how different data assumptions affect embedding quality in both monolingual and cross-lingual tasks. Our work focuses on one specific data assumption (sentencelevel alignments) and only on cross-lingual usage. This more restricted setting allows us to: (a) compare embeddings to alignment algorithms, (b) decouple the feature space from the algorithm, and make a more specific observation about the contribution of each component to the end result. In that sense, our findings complement those of Upadhyay et al. (2016). from the word in question, and defines a slightly different inter"
E17-1072,W14-1613,0,0.0112876,"alignment algorithms that also rely on sentence ID signals. We show that a generalization of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep"
E17-1072,Q15-1016,1,0.0780471,"Missing"
E17-1072,W15-1521,0,0.0257223,"oth source and target language sentences as the same intermediate sentence vector. Recently, a simpler model, BilBOWA (Gouws et al., 2015), showed similar performance without using a hidden sentencerepresentation layer, giving it a dramatic speed advantage over its predecessors. BilBOWA is essentially an extension of skip-grams with negative sampling (SGNS) (Mikolov et al., 2013b), which simultaneously optimizes each word’s similarity to its inter-lingual context (words that appeared in the aligned target language sentence) and its intra-lingual context (as in the original monolingual model). Luong et al. (2015) proposed a similar SGNS-based model over the same features. 1 bitbucket.org/omerlevy/xling_ embeddings We study which factors determine the success of cross-lingual word embedding algorithms 2 Background: Cross-lingual Embeddings 766 that use sentence-aligned data, and evaluate them against baselines from the statistical machine translation literature that incorporate the same data assumptions. We go on to generalize one of these, the Dice aligner, showing that one variant is a much stronger baseline for cross-lingual word embedding algorithms than standard baselines. Finally, we would like t"
E17-1072,D13-1141,0,0.065972,"on of one of these, the Dice aligner, is a very strong baseline for crosslingual word embedding algorithms, performing better than several state-of-the-art algorithms, especially when exploiting a multi-lingual signal. Our code and data are publicly available.1 Previous approaches to cross-lingual word embeddings can be divided into three categories, according to assumptions on the training data. The first category assumes word-level alignments, in the form of bilingual dictionaries (Mikolov et al., 2013a; Xiao and Guo, 2014) or automatically produced word alignments (Klementiev et al., 2012; Zou et al., 2013; Faruqui and Dyer, 2014). Sizable bilingual dictionaries are not available for many language pairs, and the quality of automatic word alignment greatly affects the quality of the embeddings. It is also unclear whether the embedding process provides significant added value beyond the initial word alignments (Zou et al., 2013). We Algorithms that rely on sentence-aligned data typically create intermediate sentence representations from each sentence’s constituent words. Hermann and Blunsom (2014) proposed a deep neural model, BiCVM, which compared the two sentence representations at the final la"
E17-1072,W03-0301,0,0.0358193,"ilar manner: create a pseudo-bilingual sentence from each aligned sentence, and for each word in question, consider all the other words in this sentence as its features. BilBOWA (Gouws et al., 2015) also uses a similar set of features, but restricts the source language words to those that appeared within a certain distance Benchmarks We measure the quality of each embedding using both manually annotated word alignment datasets and bilingual dictionaries. We use 16 manually annotated word alignment datasets – Hansards3 and data from four other sources (Graca et al., 2008; Lambert et al., 2005; Mihalcea and Pedersen, 2003; Holmqvist and Ahrenberg, 2011; Cakmak et al., 2012) – as well as 16 bilingual dictionaries from Wiktionary. 2 homepages.inf.ed.ac.uk/s0787820/ bible/ 3 www.isi.edu/natural-language/ download/hansard/ 767 tuning hyperparameters for truly low-resource languages. In the word alignment benchmark, each word in a given source language sentence is aligned with the most similar target language word from the target language sentence – this is exactly the same greedy decoding algorithm that is implemented in IBM Model-1 (Brown et al., 1993). If a source language word is out of vocabulary, it is not al"
E17-1072,cakmak-etal-2012-word,0,\N,Missing
K15-1018,P14-1113,0,0.203499,"Missing"
K15-1018,C92-2082,0,0.65747,"Missing"
K15-1018,H05-1087,0,0.0916619,"Missing"
K15-1018,W14-1610,1,0.879591,"Missing"
K15-1018,W11-2501,0,0.175332,"Missing"
K15-1018,N15-1098,1,0.918016,"baselines, tuning the regularization parameter on the validation set. 5.2 6.1 A New Proper-Name Dataset Performance on WordNet We examine whether our algorithm can replicate the common use of WordNet (§2.1), by manually constructing 4 whitelists based on the literature An important linguistic component that is missing from these lexical-inference datasets is propernames. We conjecture that much of the added value in utilizing structured resources is the ability to cover terms such as celebrities (Lady Gaga) 7 Since our methods do not use lexical features, we did not use lexical splits as in (Levy et al., 2015). 179 Figure 4: Recall-precision curve for proper2015. Name basic +holo +mero +hypo Edge Types {synonym, hypernym, instance hypernym} basic ∪ {holonym} basic ∪ {meronym} basic ∪ {hyponym} Table 4: The manual whitelists commonly used in WordNet. Figure 3 compares our algorithm to WordNet’s baselines, showing that our binary model always replicates the best-performing manuallyconstructed whitelists, for certain values of β 2 . Synonyms and hypernyms are often selected, and additional edges are added to match the semantic flavor of each particular dataset. In turney2014, for example, where merony"
K15-1018,E12-1004,0,0.215574,"d Resources for Lexical Inference Vered Shwartz† Omer Levy† Ido Dagan† Jacob Goldberger§ † Computer Science Department, Bar-Ilan University § Faculty of Engineering, Bar-Ilan University vered1986@gmail.com {omerlevy,dagan}@cs.biu.ac.il jacob.goldberger@biu.ac.il Abstract Corpus-based methods are often employed to recognize lexical inferences, based on either cooccurrence patterns (Hearst, 1992; Turney, 2006) or distributional representations (Weeds and Weir, 2003; Kotlerman et al., 2010). While earlier methods were mostly unsupervised, recent trends introduced supervised methods for the task (Baroni et al., 2012; Turney and Mohammad, 2015; Roller et al., 2014). In these settings, a targeted lexical inference relation is implicitly defined by a training set of term-pairs, which are annotated as positive or negative examples of this relation. Several such datasets have been created, each representing a somewhat different flavor of lexical inference. While corpus-based methods usually enjoy high recall, their precision is often limited, hindering their applicability. An alternative common practice is to mine high-precision lexical inferences from structured resources, particularly WordNet (Fellbaum, 199"
K15-1018,D13-1160,0,0.0411595,"roaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet"
K15-1018,N04-3012,0,0.0762085,"and provides lexical inferences on entities that are absent from WordNet, particularly proper-names. Background Common Use of WordNet for Inference WordNet (Fellbaum, 1998) is widely used for identifying lexical inference. It is usually used in an unsupervised setting where the relations relevant for each specific inference task are manually selected a priori. One approach looks for chains of these predefined relations (Harabagiu and Moldovan, 1998), e.g. dog → mammal using a chain of hypernyms: dog → canine → carnivore → placental mammal → mammal. Another approach is via WordNet Similarity (Pedersen et al., 2004), which takes two synsets and returns a numeric value that represents their similarity based on WordNet’s hierarchical hypernymy structure. While there is a broad consensus that synonyms entail each other (elevator ↔ lif t) and hyponyms entail their hypernyms (cat → animal), other relations, such as meronymy, are not agreed 2 We also considered Freebase, but it required significantly larger computational resources to work in our framework, which, at the time of writing, exceeded our capacity. §4.1 discusses complexity. 176 “Beyonc´e” term to concept Beyonc´e Knowles occupation musician subclas"
K15-1018,C14-1212,0,0.213181,"Missing"
K15-1018,P11-1082,0,0.0144223,"rpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic know"
K15-1018,P10-1013,0,0.0123057,"stateof-the-art corpus-based methods. Combining the two approaches can particularly benefit real-world tasks in which proper-names are prominent. 2 #Entities 4,500,000 6,000,000 10,000,000 150,000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 200"
K15-1018,C14-1097,0,0.336102,"Missing"
K15-1018,P09-1051,1,0.808142,",000 2.2 Structured Knowledge Resources While WordNet is quite extensive, it is handcrafted by expert lexicographers, and thus cannot compete in terms of scale with community-built knowledge bases such as Wikidata (Vrandeˇci´c, 2012), which connect millions of entities through a rich variety of structured relations (properties). Using these resources for various NLP tasks has become exceedingly popular (Wu and Weld, 2010; Rahman and Ng, 2011; Unger et al., 2012; Berant et al., 2013). Little attention, however, was given to leveraging them for identifying lexical inference; the exception being Shnarch et al. (2009), who used structured data from Wikipedia for this purpose. In this paper, we experimented with such resources, in addition to WordNet. DBPedia (Auer et al., 2007) contains structured information from Wikipedia: info boxes, redirections, disambiguation links, etc. Wikidata (Vrandeˇci´c, 2012) contains facts edited by humans to support Wikipedia and other Wikimedia projects. Yago (Suchanek et al., 2007) is a semantic knowledge base derived from Wikipedia, WordNet, and GeoNames.2 Table 2 compares the scale of the resources we used. The massive scale of the more recent resources and their rich sc"
K15-1018,J06-3003,0,0.349541,"Missing"
K15-1018,W03-1011,0,0.684426,"Missing"
K15-1018,J14-1003,0,\N,Missing
K15-1018,P06-1013,0,\N,Missing
K17-1008,D14-1179,0,0.0191444,"Missing"
K17-1008,N16-1150,0,0.438819,"noisy text is available, as often occurs in social media, questions and answers, and other short web documents. For example, Huang et al. (2014) collected many tweets from the same author in order to apply a global disambiguation method. Since this work focuses on disambiguating entities within short fragments of text, our algorithmic approach tries to extract as much information from the local context, without resorting to external signals. for entity and context, allowing it to extract signals from noisy and unexpected context patterns. While convolutional neural networks (Sun et al., 2015; Francis-Landau et al., 2016) and probabilistic attention (Lazic et al., 2015) have been applied to the task, this is the first model to use RNNs and a neural attention model for NED. RNNs account for the sequential nature of textual context while the attention model is applied to reduce the impact of noise in the text. Our experiments show that our model significantly outperforms existing state-of-the-art NED algorithms on WikilinksNED, suggesting that RNNs with attention are able to model short and noisy context better than current approaches. In addition, we evaluate our algorithm on CoNLLYAGO (Hoffart et al., 2011), a"
K17-1008,fromreide-etal-2014-crowdsourcing,0,0.024334,"le publicly available. Our WikilinksNED dataset is substantially different from currently available datasets since they are all based on high-quality content from either news articles or Wikipedia, while WikilinksNED is a benchmark for noisier, less coherent, and more colloquial text. The annotation process is significantly different as well, as our dataset reflects the annotation preferences of real-world website authors. It is also significantly larger in size, being over 100 times larger than CoNLL-YAGO. Recently, a number of Twitter-based datasets were compiled as well (Meij et al., 2012; Fromreide et al., 2014). These represent a much more extreme case than our dataset in terms of noise, shortness and spelling variations, and are much smaller in size. Due to the unique nature of tweets, proposed algorithms tend to be substantially different from algorithms used for other NED tasks. 3 We prepare our dataset from the local-context version of Wikilinks1 , and resolve ground-truth links using a Wikipedia dump from April 20162 . We use the page and redirect tables for resolution, and keep the database pageid column as a unique identifier for Wikipedia entities. We discard mentions where the ground-truth"
K17-1008,P14-1133,0,0.0216889,"bvious next choice was Monopoly. I played until I became a successful Captain of Industry.” Introduction Named Entity Disambiguation (NED) is the task of linking mentions of entities in text to a given knowledge base, such as Freebase or Wikipedia. NED is a key component in Entity Linking (EL) systems, focusing on the disambiguation task itself, independently from the tasks of Named Entity Recognition (detecting mention bounds) and Candidate Generation (retrieving the set of potential candidate entities). NED has been recognized as an important component in NLP tasks such as semantic parsing (Berant and Liang, 2014). Current research on NED is mostly driven by a number of standard datasets, such as CoNLLYAGO (Hoffart et al., 2011), TAC KBP (Ji et al., 2010) and ACE (Bentivogli et al., 2010). These datasets are based on news corpora and Wikipedia, which are naturally coherent, well-structured, and rich in context. Global disambiguation models This short fragment is considerably less structured and with a more personal tone than a typical news article. It references the entity Monopoly (Game), however expressions such as “experiment” and “Industry” can distract a naive disambiguation model because they are"
K17-1008,E06-1002,0,0.37683,"Missing"
K17-1008,P16-1059,0,0.457102,"ate the useful signals (e.g. “indoor games”, “played”) from the noisy ones. We therefore propose a new model that leverages local contextual information to disambiguate entities. Our neural approach (based on RNNs with attention) leverages the vast amount of training data in WikilinksNED to learn representations 58 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 58–68, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics local approach on standard datasets (Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016). However, global approaches are difficult to apply in domains where only short and noisy text is available, as often occurs in social media, questions and answers, and other short web documents. For example, Huang et al. (2014) collected many tweets from the same author in order to apply a global disambiguation method. Since this work focuses on disambiguating entities within short fragments of text, our algorithmic approach tries to extract as much information from the local context, without resorting to external signals. for entity and context, allowing it to extract signals from noisy and"
K17-1008,Q15-1011,0,0.181478,"f entity assignments within a document. For example the local component of the GLOW algorithm (Ratinov et al., 2011) was used as part of the relational inference system suggested by Cheng and Roth (2013). Similarly, Globerson et al. (2016) achieved state-of-the-art results by extending the local-based selective-context model of Lazic et al. (2015) with an attention-like coherence model. Global models can tap into highlydiscriminative semantic signals (e.g. coreference and entity relatedness) that are unavailable to local methods, and have significantly outperformed the Noisy Data Chisholm and Hachey (2015) showed that despite the noisy nature of web data, augmenting Wikipedia-derived data with weblinks from the Wikilinks corpus (Singh et al., 2012) can improve performance on standard datasets. In our work, we find noisy web data to be a unique and challenging test case for disambiguation. We therefore use Wikilinks to construct a new stand-alone disambiguation benchmark that focuses on noisy text, rather than use it for training alone. Moreover, we differ from Chisholm at el. by taking a neural approach that implicitly discovers useful signals from contexts, instead of manually crafting feature"
K17-1008,P13-2006,0,0.451729,"t approaches. In addition, we evaluate our algorithm on CoNLLYAGO (Hoffart et al., 2011), a dataset of annotated news articles. We use a simple domain adaptation technique since CoNLL-YAGO lacks a large enough training set for our model, and achieve comparable results to other state-of-the-art methods. These experiments highlight the difference between the two datasets, indicating that our NED benchmark is substantially more challenging. Code and data used for our experiments can be found at https://github.com/ yotam-happy/NEDforNoisyText 2 Neural Approaches The first neural approach for NED (He et al., 2013) used stacked autoencoders to learn a similarity measure between mention-context structures and entity candidates. More recently, convolutional neural networks (CNNs) were employed for learning semantic similarity between context, mention, and candidate inputs (Sun et al., 2015; Francis-Landau et al., 2016). Neural embedding techniques have also inspired a number of works that measure entitycontext relatedness using their embeddings (Yamada et al., 2016; Hu et al., 2015). In this paper, we train a recurrent neural network (RNN) model, which unlike CNNs and embeddings, is designed to exploit th"
K17-1008,N15-1098,1,0.899289,"Missing"
K17-1008,D11-1072,0,0.77286,"Missing"
K17-1008,P14-1036,0,0.106254,"verages the vast amount of training data in WikilinksNED to learn representations 58 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 58–68, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics local approach on standard datasets (Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016). However, global approaches are difficult to apply in domains where only short and noisy text is available, as often occurs in social media, questions and answers, and other short web documents. For example, Huang et al. (2014) collected many tweets from the same author in order to apply a global disambiguation method. Since this work focuses on disambiguating entities within short fragments of text, our algorithmic approach tries to extract as much information from the local context, without resorting to external signals. for entity and context, allowing it to extract signals from noisy and unexpected context patterns. While convolutional neural networks (Sun et al., 2015; Francis-Landau et al., 2016) and probabilistic attention (Lazic et al., 2015) have been applied to the task, this is the first model to use RNNs"
K17-1008,D16-1046,0,0.0223753,"thout the global component. Due to the long running time of this system, we only evaluated their method on the smaller test set, which contains 10,000 randomly sampled instances from the full 320,000-example test set. Finally, we include the Most Probable Sense (MPS) baseline, which selects the entity that was seen most with the given mention during training. Domain Adaptation We used a simple domain adaptation technique where we first trained our model on an available large corpus of label data derived from Wikipedia, and then trained the resulting model on the smaller training set of CoNLL (Mou et al., 2016). The Wikipedia corpus was built by extracting all cross-reference links along with their context, resulting in over 80 million training examples. We trained our model with All-Entity corrupt sampling for 1 epoch on this data. The resulting model was then adapted to CoNLL-YAGO by training 1 epoch on CoNLLYAGO’s training set, where corrupt examples were produced by considering all possible candidates for each mention as corrupt-samples (NearMisses corrupt sampling). Additional Features We proceeded to use the model in a similar setting to Yamada et al. (2016) where a Gradient Boosting Regressio"
K17-1008,Q15-1036,0,0.343609,"questions and answers, and other short web documents. For example, Huang et al. (2014) collected many tweets from the same author in order to apply a global disambiguation method. Since this work focuses on disambiguating entities within short fragments of text, our algorithmic approach tries to extract as much information from the local context, without resorting to external signals. for entity and context, allowing it to extract signals from noisy and unexpected context patterns. While convolutional neural networks (Sun et al., 2015; Francis-Landau et al., 2016) and probabilistic attention (Lazic et al., 2015) have been applied to the task, this is the first model to use RNNs and a neural attention model for NED. RNNs account for the sequential nature of textual context while the attention model is applied to reduce the impact of noise in the text. Our experiments show that our model significantly outperforms existing state-of-the-art NED algorithms on WikilinksNED, suggesting that RNNs with attention are able to model short and noisy context better than current approaches. In addition, we evaluate our algorithm on CoNLLYAGO (Hoffart et al., 2011), a dataset of annotated news articles. We use a sim"
K17-1008,N15-1026,0,0.0923423,"dered in order to separate the useful signals (e.g. “indoor games”, “played”) from the noisy ones. We therefore propose a new model that leverages local contextual information to disambiguate entities. Our neural approach (based on RNNs with attention) leverages the vast amount of training data in WikilinksNED to learn representations 58 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 58–68, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics local approach on standard datasets (Guo and Barbosa, 2014; Pershina et al., 2015; Globerson et al., 2016). However, global approaches are difficult to apply in domains where only short and noisy text is available, as often occurs in social media, questions and answers, and other short web documents. For example, Huang et al. (2014) collected many tweets from the same author in order to apply a global disambiguation method. Since this work focuses on disambiguating entities within short fragments of text, our algorithmic approach tries to extract as much information from the local context, without resorting to external signals. for entity and context, allowing it to extrac"
K17-1008,P14-2050,1,0.796552,"algorithm by Mikolov et al. (2013) that simultaneously trains both word and entity vectors. Training We assume our model is only given training examples for correct entity assignments and therefore use corrupt-sampling, where we automatically generate examples of wrong assignments. For each context-entity pair (c, e), where e is the correct assignment for c, we produce k corrupt examples with the same context c but with a different, corrupt entity e0 . We considered two alternatives for corrupt sampling and provide an empirical comparison of the two approaches (Section 5): We used word2vecf4 (Levy and Goldberg, 2014a), which allows one to train word and context embeddings using arbitrary definitions of ”word” and ”context” by providing a dataset of word-context pairs (w, c), rather than a textual corpus. In our usage, we define a context as an entity e. To compile a dataset of (w, e) pairs, we consider every word w that appeared in the Wikipedia article describing entity e. We limit our vocabularies to words that appeared at least 20 times in the corpus and entities that contain at least 20 words in their articles. We ran the process for 10 epochs and produced vectors of 300 dimensions; other hyperparame"
K17-1008,P11-1138,0,0.917131,"s paper, we train a recurrent neural network (RNN) model, which unlike CNNs and embeddings, is designed to exploit the sequential nature of text. We also utilize an attention mechanism, inspired by results from Lazic et al. (2015) that successfully used a probabilistic attention-like model for NED. Related Work Local vs Global NED Early work on Named Entity Disambiguation, such as Bunescu and Pas¸ca (2006) and Mihalcea and Csomai (2007) focused on local approaches where each mention is disambiguated separately using hand-crafted features. While local approaches provide a hard-tobeat baseline (Ratinov et al., 2011), recent work has largely focused on global approaches. These disambiguate all mentions within a document simultaneously by considering the coherency of entity assignments within a document. For example the local component of the GLOW algorithm (Ratinov et al., 2011) was used as part of the relational inference system suggested by Cheng and Roth (2013). Similarly, Globerson et al. (2016) achieved state-of-the-art results by extending the local-based selective-context model of Lazic et al. (2015) with an attention-like coherence model. Global models can tap into highlydiscriminative semantic si"
K17-1008,K16-1025,1,0.939563,"a used for our experiments can be found at https://github.com/ yotam-happy/NEDforNoisyText 2 Neural Approaches The first neural approach for NED (He et al., 2013) used stacked autoencoders to learn a similarity measure between mention-context structures and entity candidates. More recently, convolutional neural networks (CNNs) were employed for learning semantic similarity between context, mention, and candidate inputs (Sun et al., 2015; Francis-Landau et al., 2016). Neural embedding techniques have also inspired a number of works that measure entitycontext relatedness using their embeddings (Yamada et al., 2016; Hu et al., 2015). In this paper, we train a recurrent neural network (RNN) model, which unlike CNNs and embeddings, is designed to exploit the sequential nature of text. We also utilize an attention mechanism, inspired by results from Lazic et al. (2015) that successfully used a probabilistic attention-like model for NED. Related Work Local vs Global NED Early work on Named Entity Disambiguation, such as Bunescu and Pas¸ca (2006) and Mihalcea and Csomai (2007) focused on local approaches where each mention is disambiguated separately using hand-crafted features. While local approaches provid"
K17-1008,D11-1141,0,\N,Missing
K17-1034,P15-2061,0,0.037345,"015) proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do. Related Work We are interested in a particularly harsh zero-shot learning scenario: given labeled examples for N relation types during training, extract relations of a new type RN +1 at test time. The only information we have about RN +1 are parametrized questions. This setting differs from prior art in relation extraction. Bronstein et al. (2015) explore a similar zero-shot setting for event-trigger identification, in 334 3 that might participate in this relation.1 Each time the model returns a non-null answer a for a given question qe , it extracts the relation RN +1 (e, a). Ultimately, all we need to do for a new relation is define our information need in the form of a question.2 Our approach provides a naturallanguage API for application developers who are interested in incorporating a relation-extraction component in their programs; no linguistic knowledge or pre-defined schema is needed. To implement our approach, we require two"
K17-1034,D16-1146,0,0.0118323,"Missing"
K17-1034,D15-1076,1,0.160901,"ontext can therefore be defined by: Discussion Some recent QA datasets were collected by expressing knowledge-base assertions in natural language. The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.g. educated at(T uring, P rinceton)), collecting roughly 100,000 natural-language questions to support QA against a knowledge graph. Morales et al. (2016) used a similar process to collect questions from Wikipedia infoboxes, yielding the 15,000-example InfoboxQA dataset. For the task of identifying predicate-argument structures, QASRL (He et al., 2015) was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences. In these efforts, the costs scale linearly in the number of instances, requiring significant investments for large datasets. In contrast, schema querification can generate an enormous amount of data for a fraction of the cost by labeling at the relation level; as ev"
K17-1034,P16-1145,0,0.276022,"7), pages 333–342, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics which RN +1 is specified by a set of trigger words at test time. They generalize by measuring the similarity between potential triggers and the given seed set using unsupervised methods. We focus instead on slot filling, where questions are more suitable descriptions than trigger words. in data and models. We use distant supervision for a relatively large number of relations (120) from Wikidata (Vrandeˇci´c, 2012), which are easily gathered in practice via the WikiReading dataset (Hewlett et al., 2016). We introduce a crowdsourcing approach for gathering and verifying the questions for each relation. This process produced about 10 questions per relation on average, yielding a dataset of over 30,000,000 questionsentence-answer examples in total. Because questions are paired with relation types, not instances, this overall procedure has very modest costs. The key modeling challenge is that most existing reading-comprehension problem formulations assume the answer to the question is always present in the given text. However, for relation extraction, this premise does not hold, and the model ne"
K17-1034,P11-1055,1,0.180397,"Missing"
K17-1034,N16-1104,0,0.00903376,"to train a reading comprehension model through our reduction. However, at test time, we are asked about a previously unseen relation type RN +1 . Rather than providing labeled data for the new relation, we simply list questions that define the relation’s slot values. Assuming we learned a good reading comprehension model, the correct values should be extracted. Our zero-shot setup includes innovations both Introduction Relation extraction systems populate knowledge bases with facts from an unstructured text corpus. When the type of facts (relations) are predefined, one can use crowdsourcing (Liu et al., 2016) or distant supervision (Hoffmann et al., 2011) to collect examples and train an extraction model for each relation type. However, these approaches are incapable of extracting relations that were not specified in advance and observed during training. In this paper, we propose an alternative approach for relation extraction, which can potentially extract facts of new types that were neither specified nor observed a priori. 333 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 333–342, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Associatio"
K17-1034,P16-1105,0,0.0341351,"ways improves performance (Question Ensemble). We create an ensemble by sampling 3 questions per test instance and predicting the answer for each. We then choose the answer with the highest sum of confidence scores. In addition to our model, we compare three other systems. The first is a random baseline that chooses a named entity in the sentence that does not appear in the question (Random NE). We also reimplement the RNN Labeler that was shown to have good results on the extractive portion of WikiReading (Hewlett et al., 2016). Lastly, we retrain an off-the-shelf relation extraction system (Miwa and Bansal, 2016), which has shown promising results on a number of benchmarks. This system (and many like it) represents relations as indicators, and cannot extract unseen relations. Experiments To understand how well our method can generalize to unseen data, we design experiments for unseen entities (Section 6.1), unseen question templates (Section 6.2), and unseen relations (Section 6.3). Evaluation Metrics Each instance is evaluated by comparing the tokens in the labeled answer set with those of the predicted span.6 Precision is the true positive count divided by the number of times the system returned a n"
K17-1034,E17-1058,0,0.0181729,"extracting facts from text. While open IE systems need no relation-specific training data, they often treat different phrasings as different relations. In this work, we hope to extract a canonical slot value independent of how the original text is phrased. Universal schema (Riedel et al., 2013) represents open IE extractions and knowledge-base facts in a single matrix, whose rows are entity pairs and columns are relations. The redundant schema (each knowledge-base relation may overlap with multiple natural-language relations) enables knowledge-base population via matrix completion techniques. Verga et al. (2017) predict facts for entity pairs that were not observed in the original matrix; this is equivalent to extracting seen relation types with unseen entities (see Section 6.1). Rockt¨aschel et al. (2015) and Demeester et al. (2016) use inference rules to predict hidden knowledge-base relations from observed naturallanguage relations. This setting is akin to generalizing across different manifestations of the same relation (see Section 6.2) since a natural-language description of each target relation appears in the training data. Moreover, the information about the unseen relations is a set of expli"
K17-1034,D16-1199,0,0.0336193,"i is to end at that index. Assuming the answer exists, we can transform these confidence scores into pseudo-probability distributions pstart , pend via softmax. The probability of each i-to-j-span of the context can therefore be defined by: Discussion Some recent QA datasets were collected by expressing knowledge-base assertions in natural language. The Simple QA dataset (Bordes et al., 2015) was created by annotating questions about individual Freebase facts (e.g. educated at(T uring, P rinceton)), collecting roughly 100,000 natural-language questions to support QA against a knowledge graph. Morales et al. (2016) used a similar process to collect questions from Wikipedia infoboxes, yielding the 15,000-example InfoboxQA dataset. For the task of identifying predicate-argument structures, QASRL (He et al., 2015) was proposed as an open schema for semantic roles, in which the relation between an argument and a predicate is expressed as a natural-language question containing the predicate (“Where was someone educated?”) whose answer is the argument (“Princeton”). The authors collected about 19,000 question-answer pairs from 3,200 sentences. In these efforts, the costs scale linearly in the number of instan"
K17-1034,D14-1162,0,0.111821,"een relations (Section 6.3). Evaluation Metrics Each instance is evaluated by comparing the tokens in the labeled answer set with those of the predicted span.6 Precision is the true positive count divided by the number of times the system returned a non-null answer. Recall is the true positive count divided by the number of instances that have an answer. 6.1 Unseen Entities We show that our reading-comprehension approach works well in a typical relation-extraction setting by testing it on unseen entities and texts. Hyperparameters In our experiments, we initialized word embeddings with GloVe (Pennington et al., 2014), and did not fine-tune them. The typical training set was an order of 1 million examples, for which 3 epochs were enough for convergence. All training sets had a ratio of 1:1 positive and negative examples, which was chosen to match the test sets’ ratio. Setup We partitioned our dataset along entities in the question, and randomly clustered each entity into one of three groups: train, dev, or test. For instance, Alan Turing examples appear only in training, while Steve Jobs examples are exclusive to test. We then sampled 1,000,000 examples for train, 1,000 for dev, and 10,000 for test. This p"
K17-1034,D16-1264,0,0.0721379,"the example set, the name of the relation (e.g. country), and another instance where it was hidden. Out of a potential 54 question templates, 40 were unique on average. In the verification phase, we measure the question templates’ quality by sampling additional sentences and instantiating each question template with the example entity e. Annotators are then asked to answer the question from the sentence s, or mark it as unanswerable; if the annotators’ anNegative Examples To support relation extraction, our dataset deviates from recent reading comprehension formulations (Hermann et al., 2015; Rajpurkar et al., 2016), and introduces negative examples – question-sentence pairs that have no answers (A = ∅). Following the methodology of InfoboxQA (Morales et al., 2016), we generate negative examples by matching (for the same entity e) a question q that pertains to one relation with a sentence s that expresses another relation. We also assert that the sentence does not contain the answer to q. For instance, we match “Who 3 We used this relatively lenient measure because many annotators selected the correct answer, but with a slightly incorrect span; e.g. “American businessman” instead of “businessman”. We the"
K17-1034,N13-1008,0,0.0781624,"answer to “Where” is a location), as well as detecting relation paraphrases to a certain extent. We also find that there are many feasible cases that our model does not quite master, providing an interesting challenge for future work. 2 Open information extraction (open IE) (Banko et al., 2007) is a schemaless approach for extracting facts from text. While open IE systems need no relation-specific training data, they often treat different phrasings as different relations. In this work, we hope to extract a canonical slot value independent of how the original text is phrased. Universal schema (Riedel et al., 2013) represents open IE extractions and knowledge-base facts in a single matrix, whose rows are entity pairs and columns are relations. The redundant schema (each knowledge-base relation may overlap with multiple natural-language relations) enables knowledge-base population via matrix completion techniques. Verga et al. (2017) predict facts for entity pairs that were not observed in the original matrix; this is equivalent to extracting seen relation types with unseen entities (see Section 6.1). Rockt¨aschel et al. (2015) and Demeester et al. (2016) use inference rules to predict hidden knowledge-b"
K17-1034,N15-1118,0,0.0672247,"Missing"
K17-1034,D15-1174,0,0.0166238,"et of explicit inference rules, as opposed to implicit natural-language questions. Our zero-shot scenario, in which no manifestation of the test relation is observed during training, is substantially more challenging (see Section 6.3). In universal-schema terminology, we add a new empty column (the target knowledgebase relation), plus a few new columns with a single entry each (reflecting the textual relations in the sentence). These columns share no entities with existing columns, making the rest of the matrix irrelevant. To fill the empty column from the others, we match their descriptions. Toutanova et al. (2015) proposed a similar approach that decomposes natural-language relations and computes their similarity in a universal schema setting; however, they did not extend their method to knowledge-base relations, nor did they attempt to recover out-of-schema relations as we do. Related Work We are interested in a particularly harsh zero-shot learning scenario: given labeled examples for N relation types during training, extract relations of a new type RN +1 at test time. The only information we have about RN +1 are parametrized questions. This setting differs from prior art in relation extraction. Bron"
N15-1098,J10-4006,0,0.065573,"okens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dataset entry contains two w"
N15-1098,W11-2501,0,0.453212,"different goal in mind, affecting word-pair generation and annotation. For example, 2 Following Caron (2001), we used the square root √ of the eigenvalue matrix Σk for representing words: Mk = Uk Σk . 3 http://bitbucket.org/yoavgo/word2vecf 971 both of Baroni’s datasets are designed to capture hypernyms, while other datasets try to capture broader notions of lexical inference (e.g. causality). Table 1 provides metadata on each dataset, and the description below explains how each one was created. (Kotlerman et al., 2010) Manually annotated lexical entailment of distributionally similar nouns. (Baroni and Lenci, 2011) a.k.a. BLESS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailin"
N15-1098,E12-1004,0,0.750923,"y. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexic"
N15-1098,W09-0215,0,0.133534,"f ever), hypernymy (cat → animal), and other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on context"
N15-1098,P14-1113,0,0.552171,"inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these supervised methods learn whether y is a “prototypical hypernym” (i.e. a category), regardless of x, rather than learning a concrete relation between x and y. Our mathematical anal"
N15-1098,S13-1035,0,0.0223919,"Missing"
N15-1098,C92-2082,0,0.839194,")) = (~x~y · x~s y~s ) 2 (~xx~s · ~y y~s ) 1−α 2 (4) While these methods reduce match error – match error = 0.618 · recall versus the previous regression curve of match error = 0.935 · recall – their overall performance is only incrementally better than that of linear methods (Table 5). This improvement is also, partially, a result of the nonlinearity introduced in these kernels. 974 7 The Limitations of Contextual Features A (de)motivating example can be seen in §4.2. A typical y often has such+1 as a dominant feature, whereas x tends to appear with such−2 . These features are relics of the Hearst (1992) pattern “y such as x”. However, contextual features of single words cannot capture the joint occurrence of x and y in that pattern; instead, they record only this observation as two independent features of different words. In that sense, contextual features are inherently handicapped in capturing relational information, requiring supervised methods to harness complementary information from more sophisticated features, such as textual patterns that connect x with y (Snow et al., 2005; Turney, 2006). Acknowledgements This work was supported by the Adolf Messer Foundation, the Google Research Aw"
N15-1098,S12-1047,0,0.0690378,"notated lexical entailment of distributionally similar nouns. (Baroni and Lenci, 2011) a.k.a. BLESS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailing or not. (Levy et al., 2014) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a f"
N15-1098,P14-2050,1,0.0441489,"ntation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dataset entry contains two words (x, y) and a label whether x entails y. Note that each dataset"
N15-1098,W14-1610,1,0.664954,"SS. Created by selecting unambiguous word pairs and their semantic relations from WordNet. Following Roller et al. (2014), we labeled noun hypernyms as positive examples and used meronyms, noun cohyponyms, and random noun pairs as negative. (Baroni et al., 2012) Created in a similar fashion to BLESS. Hypernym pairs were selected as positive examples from WordNet, and then permutated to generate negative examples. (Turney and Mohammad, 2014) Based on a crowdsourced dataset of 79 semantic relations (Jurgens et al., 2012). Each semantic relation was linguistically annotated as entailing or not. (Levy et al., 2014) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a feature vector: concat (~x ⊕~y ) (Baroni et al., 2012), diff (~y − ~x) (Roller et al., 2014; We"
N15-1098,P98-2127,0,0.433791,"ions over Wikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets f"
N15-1098,C94-1049,0,0.186491,"Missing"
N15-1098,J07-2002,0,0.0122586,"ikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows different context types (Levy and Goldberg, 2014).3 2.2 Labeled Datasets We used 5 labeled datasets for evaluation. Each dat"
N15-1098,E14-1054,0,0.287818,"ds, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these supervised methods learn whether y is a “prototypical hypernym” (i.e. a category), regardless of x, rather than learning a concrete relation between x and y. Our mathematical analysis reveals that said methods ignore the interaction between x and y, explaining our empirical findings. We modify them accordingly by incorporating the similarity between"
N15-1098,C14-1097,0,0.772314,"sks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney and Mohammad, 2014; Rimell, 2014). In this paper, we argue that these supervised methods do not, in fact, learn to recognize lexical inference. Our experiments reveal that much of their previously perceived success stems from lexical memorizing. Further experiments show that these super"
N15-1098,E14-4008,0,0.286564,"other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, based on contextual features, have also been tested (Turney an"
N15-1098,P93-1034,0,0.626283,",657 Table 1: Datasets evaluated in this work. 2.1 Word Representations We built 9 word representations over Wikipedia (1.5 billion tokens) using the cross-product of 3 types of contexts and 3 representation models. 2.1.1 Context Types Bag-of-Words Uses 5 tokens to each side of the target word (10 context words in total). It also employs subsampling (Mikolov et al., 2013a) to increase the impact of content words. Positional Uses only 2 tokens to each side of the target word, and decorates them with their position (relative to the target word); e.g. the−1 is a common positional context of cat (Schütze, 1993). Dependency Takes all words that share a syntactic connection with the target word (Lin, 1998; Padó and Lapata, 2007; Baroni and Lenci, 2010). We used the same parsing apparatus as in (Levy and Goldberg, 2014). 2.1.2 Representation Models PPMI A word-context positive pointwise mutual information matrix M (Niwa and Nitta, 1994). SVD We reduced M ’s dimensionality to k = 500 using Singular Value Decomposition (SVD).2 SGNS Skip-grams with negative sampling (Mikolov et al., 2013b) with 500 dimensions and 5 negative samples. SGNS was trained using a modified version of word2vec that allows differe"
N15-1098,J06-3003,0,0.298018,"Missing"
N15-1098,W03-1011,0,0.661401,"as causality (f lu → f ever), hypernymy (cat → animal), and other notions of lexical entailment. The distributional approach to automatically recognize these relations relies on representing each word x as a vector ~x of contextual features: other words that tend to appear in its vicinity. Such features are typically used in word similarity tasks, where cosine similarity is a standard similarity measure between two word vectors: sim(x, y) = cos(~x, ~y ). Many unsupervised distributional methods of recognizing lexical inference replace cosine similarity with an asymmetric similarity function (Weeds and Weir, 2003; Clarke, 2009; Kotlerman et al., 2010; Santus et al., 2014). Supervised methods, reported to perform better, try to learn the asymmetric operator from a training set. The various supervised methods differ by the way they represent each candidate pair of words (x, y): Baroni et al. (2012) use concatenation ~x ⊕ ~y , others (Roller et al., 2014; Weeds Chris Biemann§ Ido Dagan† § Language Technology Lab Technische Universität Darmstadt Darmstadt, Germany {remus,biem}@cs.tu-darmstadt.de et al., 2014; Fu et al., 2014) take the vectors’ difference ~y − ~x, and more sophisticated representations, ba"
N15-1098,C14-1212,0,0.797224,"4) Based on manually annotated entailment graphs of subject-verb-object tuples (propositions). Noun entailments were extracted from entailing tuples that were identical except for one of the arguments, thus propagating the existence/absence of proposition-level entailment to the noun level. This dataset is the most realistic dataset, since the original entailment annotations were made in the context of a complete proposition. 2.3 Supervised Methods We tested 4 compositions for representing (x, y) as a feature vector: concat (~x ⊕~y ) (Baroni et al., 2012), diff (~y − ~x) (Roller et al., 2014; Weeds et al., 2014; Fu et al., 2014), only x (~x), and only y (~y ). For each composition, we trained two types of classifiers, tuning hyperparameters with a validation set: logistic regression with L1 or L2 regularization, and SVM with a linear kernel or quadratic kernel. 3 Negative Results Based on the above setup, we present three negative empirical results, which challenge the claim that the methods presented in §2.3 are learning a relation between x and y. In addition to our setup, these results were also reproduced in preliminary experDataset Kotlerman 2010 Bless 2011 Baroni 2012 Turney 2014 Levy 2014 Lex"
N15-1098,C98-2122,0,\N,Missing
N18-2017,D15-1075,1,0.69331,". This suggests that, despite recently reported progress, natural language inference remains an open problem. Introduction Natural language inference (NLI; also known as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. T"
N18-2017,P17-2097,0,0.0400512,"ICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered wh"
N18-2017,P16-1223,0,0.0889145,"Missing"
N18-2017,N18-2123,0,0.0357975,"Missing"
N18-2017,D17-1070,0,0.0753495,"Missing"
N18-2017,N16-1098,0,0.0292386,"yMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across multiple NLI datasets. Indeed, annotation artifacts are not unique to the NLI datasets, and the danger of such biases should be carefully considered when annotating new datasets. to use them for evaluating NLI models (in addition to the original benc"
N18-2017,D16-1244,0,0.193867,"Missing"
N18-2017,S18-2023,0,0.153649,"Missing"
N18-2017,D16-1264,0,0.144407,"Missing"
N18-2017,W17-1609,0,0.0464368,"Missing"
N18-2017,D17-1215,0,0.24022,"Missing"
N18-2017,K17-1004,1,0.376046,"f discourse markers such as because. Once again, we observe that the example from the SNLI annotation guidelines does just that, by adding the purpose clause to catch a stick (Table 3). Contradiction. Negation words such as nobody, no, never and nothing are strong indicators of contradiction.3 Other (non-negative) words appear to be part of heuristics for contradicting whatever information is displayed in the premise; sleeping contradicts any activity, and naked (further down the list) contradicts any description of clothing. 3 Similar findings were observed in the ROC story cloze annotation (Schwartz et al., 2017). 109 Model DAM ESIM DIIN Full SNLI Hard Easy MultiNLI Matched Full Hard Easy MultiNLI Mismatched Full Hard Easy 84.7 85.8 86.5 69.4 71.3 72.7 72.0 74.1 77.0 72.1 73.1 76.5 92.4 92.6 93.4 55.8 59.3 64.1 85.3 86.2 87.6 56.2 58.9 64.4 85.7 85.2 86.8 Table 5: Performance of high-performing NLI models on the full, Hard, and Easy NLI test sets. 4 Re-evaluating NLI Models not be as straightforward to eliminate annotation artifacts once the dataset has been collected. First, after removing the Easy examples, Hard examples might not necessarily be artifact-free. For instance, removing all contradictin"
N18-2017,E17-2068,0,0.0173405,"9 35.2 52.3 Table 2: Performance of a premise-oblivious text classifier on NLI. The MultiNLI benchmark contains two test sets: matched (in-domain examples) and mismatched (out-of-domain examples). A majority baseline is presented for reference. 3.1 To see whether the use of certain words is indicative of the inference class, we compute the pointwise mutual information (PMI) between each word and class in the training set: To determine the degree to which such artifacts exist, we train a model to predict the label of a given hypothesis without seeing the premise. Specifically, we use fastText (Joulin et al., 2017), an off-the-shelf text classifier that models text as a bag of words and bigrams, to predict the entailment label of the hypothesis.1 This classifier is completely oblivious to the premise. Table 2 shows that a significant portion of each test set can be correctly classified without looking at the premise, well beyond the most-frequentclass baseline.2 Our finding demonstrates that it is possible to perform well on these datasets without modeling natural language inference. 3 Lexical Choice PMI(word, class) = log p(word, class) p(word, ·)p(·, class) We apply add-100 smoothing to the raw statis"
N18-2017,N18-1101,1,0.645347,"wn as recognizing textual entailment, or RTE) is a widely-studied task in natural language processing, to which many complex semantic tasks, such as question answering and text summarization, can be reduced (Dagan et al., 2006). Given a pair of sentences, a premise p and a hypothesis h, the goal is to determine whether or not p semantically entails h. The problem of acquiring large amounts of labeled inference data was addressed by Bowman et al. (2015), who devised a method for crowdsourcing high-agreement entailment annotations en masse, creating the SNLI and later the genrediverse MultiNLI (Williams et al., 2018) datasets. In this process, crowd workers are presented with F h is definitely true given p h might be true given p h is definitely not true given p 2 Annotation Artifacts are Common We conjecture that the framing of the annotation task has a significant effect on the language generation choices that crowd workers make when authoring hypotheses, producing certain patterns in the data. We call these patterns annotation artifacts. These authors contributed equally to this work. 107 Proceedings of NAACL-HLT 2018, pages 107–112 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Compu"
N18-2017,S14-2055,0,0.0249089,"that such models rely heavily on annotation artifacts in the hypothesis to make their predictions. A natural question to ask is whether it is possible to select a set of NLI training and test samples which do not contain easy-to-exploit artifacts. One solution might be to filter Easy examples from the training set, retaining only Hard examples. However, initial experiments suggest that it might 5 Discussion We reflect on our results and relate them to other work that also analyzes annotation artifacts in NLP datasets, drawing three main conclusions. Many datasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark"
N18-2017,P12-2031,0,0.0251786,"emonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this work, Poliak et al. (2018) uncovered similar annotation biases across"
N18-2017,P16-2041,1,0.826417,"atasets contain annotation artifacts. Lai and Hockenmaier (2014) demonstrated that lexical features such as the presence of negation, word overlap, and hypernym relations are highly predictive of entailment classes in the SICK dataset (Marelli et al., 2014). Chen et al. (2016) revealed problems with the CNN/DailyMail dataset (Hermann et al., 2015) which resulted from apply4 The MultiNLI models were trained on MultiNLI data alone (as opposed to a blend of MultiNLI and SNLI data). 5 github.com/allenai/allennlp 6 github.com/nyu-mll/multiNLI 7 goo.gl/kCeZXm 110 ing automatic tools for annotation. Levy and Dagan (2016) showed that a relation inference benchmark (Zeichner et al., 2012) is severely biased towards distributional methods, since it was created using DIRT (Lin and Pantel, 2001). Schwartz et al. (2017) and Cai et al. (2017) showed that certain biases are prevalent in the ROC stories cloze task (Mostafazadeh et al., 2016), which allow models trained on the endings alone, and not the story prefix, to yield state-of-the-art results. Rudinger et al. (2017) revealed that elicited hypotheses in SNLI contain evidence of various gender, racial, religious, and aged-based stereotypes. In parallel to this wo"
N18-2017,N15-1098,1,0.787039,"lopment of additional challenging benchmarks that expose the true performance levels of state-of-the-art NLI models. Acknowledgments This research was supported in part by the DARPA CwC program through ARO (W911NF15-1-0543) and a hardware gift from NVIDIA Corporation. SB acknowledges gift support from Google and Tencent Holdings and support from Samsung Research. References Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. 2016. Analyzing the behavior of visual question answering models. In Proc. of EMNLP. https: //aclweb.org/anthology/D16-1203. Supervised models leverage annotation artifacts. Levy et al. (2015) demonstrated that supervised lexical inference models rely heavily on artifacts in the datasets, particularly the tendency of some words to serve as prototypical hypernyms. Agrawal et al. (2016); Jabri et al. (2016); Goyal et al. (2017) all showed that state-of-the-art visual question answering (Antol et al., 2015) systems leverage annotation biases in the dataset. Cirik et al. (2018) find that complex models for referring expression recognition achieve high performance without any text input. In parallel to this work, Dasgupta et al. (2018) found that the InferSent model (Conneau et al., 201"
N18-2017,D16-1203,0,\N,Missing
N18-2017,P18-1224,0,\N,Missing
N19-1362,W11-2501,0,0.0688554,"ocus mostly on relations between pairs of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inf"
N19-1362,D18-1454,0,0.034005,"Missing"
N19-1362,Q17-1010,0,0.0645686,"about a quarter of the overall gain. This suggests that while the bulk of the signal is mined from the pair-context interactions, there is also valuable information in other interactions as well. We also test whether specific pre-training of word pair representations is useful by replacing pair2vec embeddings with the vector offsets of pre-trained word embeddings (Unsupervised: Pair Dist). We follow the PairDistance method for word analogies (Mikolov et al., 2013b), and represent the pair (x, y) as the L2 normalized difference of single-word vectors: (x − y)/kx − yk. We use the same fastText (Bojanowski et al., 2017) word vectors with which we initialized pair2vec before training. We observe a gain of only 0.34 F1 over the baseline. 5 Ablating parts of pair2vec shows that all components of the model (Section 2) are useful. We ablate each component and report the EM and F1 on the development set of SQuAD 2.0 (Table 6). The full model, which uses a 4-layer MLP for R(x, y) and trains with multivariate negative sampling, achieves the highest F1 of 72.68. We experiment with two alternative composition functions, a 2-layer MLP (Composition: 2 Layers) and element-wise multiplication (CompoAnalysis In Section 4,"
N19-1362,D15-1075,0,0.0291221,"◦ bi ; ˆ a (12) In the later layers, ainf is recontextualized using a BiGRU and self attention. Finally a prediction layer predicts the start and end tokens. BiDAF++ with pair2vec To add our pair vectors, we simply concatenate ri (3) to ainf (12): i   ainf = ai ; bi ; ai ◦ bi ; ˆ a; ri (13) i 3.3 Benchmark ESIM + pair2vec Matched 79.68 Mismatched 78.80 81.03 80.12 ∆ +1.35 +1.32 Table 4: Performance on MultiNLI, with and without pair2vec. All models have ELMo. Natural Language Inference For NLI, we augment the ESIM model (Chen et al., 2017), which was previously state-of-theart on both SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) benchmarks. ESIM Let a and b be the outputs of the premise and hypothesis encoders respectively (in place of the standard p and h notations). The inference layer’s inputs ainf (and binf i j ) are defined similarly to the generic model’s in (7):   ainf = ai ; bi ; ai ◦ bi ; ai − bi (14) i In the later layers, ainf and binf are projected, recontextualized, and converted to a fixed-length vector for each sentence using multiple pooling schemes. These vectors are then passed on to an output layer, which predicts the class. ESIM with pair2vec To add our pair"
N19-1362,P18-1224,0,0.124062,"h notations). The inference layer’s inputs ainf (and binf i j ) are defined similarly to the generic model’s in (7):   ainf = ai ; bi ; ai ◦ bi ; ai − bi (14) i In the later layers, ainf and binf are projected, recontextualized, and converted to a fixed-length vector for each sentence using multiple pooling schemes. These vectors are then passed on to an output layer, which predicts the class. ESIM with pair2vec To add our pair vectors, we simply concatenate ri (3) to ainf (14): i   ainf = ai ; bi ; ai ◦ bi ; ai − bi ; ri (15) i A similar augmentation of ESIM was recently proposed in KIM (Chen et al., 2018). However, their pair vectors are composed of WordNet features, while our pair embeddings are learned directly from text (see further discussion in Section 6). 4 Table 3: Performance on SQuAD 2.0 and adversarial SQuAD (AddSent and AddOneSent) benchmarks, with and without pair2vec. All models have ELMo. Experiments For experiments on QA (Section 4.1) and NLI (Section 4.2), we use our full model which includes multivariate and typed negative sampling. We discuss ablations in Section 4.3 Data We use the January 2018 dump of English Wikipedia, containing 96M sentences to train pair2vec. We restric"
N19-1362,P17-1152,0,0.57696,"ise mutual information (PMI) among x, y, and their context c using a variant of negative sampling (Mikolov et al., 2013a). Making R(x, y) a 3597 Proceedings of NAACL-HLT 2019, pages 3597–3608 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics compositional function on individual words alleviates the sparsity that necessarily comes with embedding pairs of words, even at a very large scale. We show that our embeddings can be added to existing cross-sentence inference models, such as BiDAF++ (Seo et al., 2017; Clark and Gardner, 2018) for QA and ESIM (Chen et al., 2017) for NLI. Instead of changing the word embeddings that are fed into the encoder, we add the pretrained pair representations to higher layers in the network where cross sentence attention mechanisms are used. This allows the model to use the background knowledge that the pair embeddings implicitly encode to reason about the likely relationships between the pairs of words it aligns. Experiments show that simply adding our wordpair embeddings to existing high-performing models, which already use ELMo (Peters et al., 2018), results in sizable gains. We show 2.72 F1 points over the BiDAF++ model (C"
N19-1362,W04-3205,0,0.0608506,"ts. First, our goal is to represent word pairs, not individual words. Second, our new PMI formulation models the trivariate word-word-context distribution. Experiments show that our pair embeddings can complement single-word embeddings. Mining Textual Patterns There is extensive literature on mining textual patterns to predict relations between words (Hearst, 1992; Snow et al., 2005; Turney, 2005; Riedel et al., 2013; Van de Cruys, 2014; Toutanova et al., 2015; Shwartz and Dagan, 2016). These approaches focus mostly on relations between pairs of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes"
N19-1362,P18-1078,0,0.0386834,"Missing"
N19-1362,W11-1303,0,0.0775844,"Missing"
N19-1362,D14-1004,0,0.0554251,"Missing"
N19-1362,C18-1225,0,0.129932,"Missing"
N19-1362,W18-2501,1,0.868432,"Missing"
N19-1362,N16-2002,0,0.0411555,"erb 3pSg:Ved Verb Ving:Ved Verb Inf:Ved Noun+less Substance Meronym 3CosAdd +pair2vec α∗ 1.2 1.8 0.1 0.7 4.0 49.1 61.1 58.5 4.8 3.8 86.1 44.6 42.0 31.7 28.4 61.7 73.3 70.1 16.0 14.5 0.9 0.8 0.9 1.0 0.8 0.6 0.5 0.5 0.2 0.6 Table 7: The top 10 analogy relations for which interpolating with pair2vec improves performance. α∗ is the optimal interpolation parameter for each relation. 5.1 Quantitative Analysis: Word Analogies Word Analogy Dataset Given a word pair (a, b) and word x, the word analogy task involves predicting a word y such that a : b :: x : y. We use the Bigger Analogy Test Set (BATS, Gladkova et al., 2016) which contains four groups of relations: encyclopedic semantics (e.g., personprofession as in Einstein-physicist), lexicographic semantics (e.g., antonymy as in cheap-expensive), derivational morphology (e.g., noun forms as in oblige-obligation), and inflectional morphology (e.g., noun-plural as in bird-birds). Each group contains 10 sub-relations. Method We interpolate pair2vec and 3CosAdd (Mikolov et al., 2013b; Levy et al., 2014) scores on fastText embeddings, as follows: score(y) = α · cos(ra,b , rx,y ) + (1 − α) · cos(b − a + x, y) (16) where a, b, x, and y represent fastText embeddings6"
N19-1362,P18-2103,0,0.0246428,"swering We experiment on the SQuAD 2.0 QA benchmark (Rajpurkar et al., 2018), as well as the adversarial datasets of SQuAD 1.1 (Rajpurkar et al., 2016; Jia and Liang, 2017). Table 3 shows the performance of BiDAF++, with ELMo , before and after adding pair2vec. Experiments on SQuAD 2.0 show that our pair representations improve performance by 2.72 F1. Moreover, adding pair2vec also results in better generalization on the adversarial SQuAD datasets with gains of 7.14 and 6.11 F1. 4.2 Natural Language Inference We report the performance of our model on MultiNLI and the adversarial test set from Glockner et al. (2018) in Table 5. We outperform the 5 Like in word2vec, subsampling reduces the size of the dataset and speeds up training. For this, we define the word pair probability as the product of unigram probabilities. 3601 100 Rule-based Models WordNet Baseline 85.8 80 Models with GloVe ESIM (Chen et al., 2017) KIM (Chen et al., 2018) ESIM + pair2vec 77.0 87.7 92.9 Models with ELMo ESIM (Peters et al., 2018) ESIM + pair2vec 84.6 93.4 Accuracy Accuracy Model EM (∆) 40 0 0.0 69.20 72.68 Composition: 2 Layers Composition: Multiply Objective: Bivariate NS Unsupervised: Pair Dist 68.35 (-0.85) 67.10 (-2.20) 68"
N19-1362,C92-2082,0,0.431945,"be learned with word pair vectors (pair2vec1 ), 1 https://github.com/mandarjoshi90/ pair2vec Y which are trained unsupervised, and which significantly improve performance when added to existing cross-sentence attention mechanisms. Unlike single-word representations, which typically model the co-occurrence of a target word x with its context c, our word-pair representations are learned by modeling the three-way cooccurrence between words (x, y) and the context c that ties them together, as seen in Table 1. While similar training signals have been used to learn models for ontology construction (Hearst, 1992; Snow et al., 2005; Turney, 2005; Shwartz et al., 2016) and knowledge base completion (Riedel et al., 2013), this paper shows, for the first time, that large scale learning of pairwise embeddings can be used to directly improve the performance of neural cross-sentence inference models. More specifically, we train a feedforward network R(x, y) that learns representations for the individual words x and y, as well as how to compose them into a single vector. Training is done by maximizing a generalized notion of the pointwise mutual information (PMI) among x, y, and their context c using a varia"
N19-1362,P18-1003,0,0.241061,"tains pairs occurring within a sentence. For better generalization to cross-sentence tasks, where the pair distribution differs from that of the training data, we need a multivariate objective that captures the full three-way (x, y, c) interaction. Multivariate Negative Sampling We introduce negative sampling of target words, x and y, in addition to negative sampling of contexts c (Table 2, J3N S ). Our new objective also converges to a novel multivariate generalization of PMI, different from previous PMI extensions that were inspired by information theory (Van de Cruys, 2011) and heuristics (Jameel et al., 2018).2 Following Levy and Goldberg (2014), we can show that when replacing target words in addition to contexts, our objective will converge3 to the optimal value in Equation 1: R(x, y) · C(c) = log 2 3 P (x, y, c) Zx,y,c (1) See supplementary material for their exact formulations. A full proof is provided in the supplementary material. where Zx,y,c , the denominator, is: Zx,y,c = kc P (·, ·, c)P (x, y, ·) + kx P (x, ·, ·)P (·, y, c) + ky P (·, y, ·)P (x, ·, c) (2) This optimal value deviates from previous generalizations of PMI by having a linear mixture of marginal probability products in its de"
N19-1362,D17-1215,0,0.0279102,"moves all out-of-vocabulary words in the corpus. We consider each word pair within a window of 5 in the preprocessed corpus, and subsample5 instances based on pair probability with a threshold of 5·10−7 . We define the context as one word each to the left and right, and all the words in between each pair, replacing both target words with placeholders X and Y (see Table 1). More details can be found in the supplementary material. 4.1 Question Answering We experiment on the SQuAD 2.0 QA benchmark (Rajpurkar et al., 2018), as well as the adversarial datasets of SQuAD 1.1 (Rajpurkar et al., 2016; Jia and Liang, 2017). Table 3 shows the performance of BiDAF++, with ELMo , before and after adding pair2vec. Experiments on SQuAD 2.0 show that our pair representations improve performance by 2.72 F1. Moreover, adding pair2vec also results in better generalization on the adversarial SQuAD datasets with gains of 7.14 and 6.11 F1. 4.2 Natural Language Inference We report the performance of our model on MultiNLI and the adversarial test set from Glockner et al. (2018) in Table 5. We outperform the 5 Like in word2vec, subsampling reduces the size of the dataset and speeds up training. For this, we define the word pa"
N19-1362,W14-1610,1,0.897363,"Missing"
N19-1362,K17-1034,1,0.85942,"nce layer effectively learns word-pair relationships from training data, and it should, therefore, help to augment its input with pair2vec. We augment ainf (7) with the pair vectors ri,j (3) by i concatenating a weighted average of the pair vectors ri,j involving ai , where the weights are the same αi,j computed via attention in (5): X ri = αi,j ri,j (8) j ainf i  = ai ; bi ; ri  (9) The symmetric term binf is defined analogously. j 3.2 Question Answering We augment the inference layer in the BiDAF++ model with pair2vec. BiDAF++ is an improved version of the BiDAFNoAnswer (Seo et al., 2017; Levy et al., 2017) which includes selfattention and ELMo embeddings from Peters et al. (2018). We found this variant to be stronger than the baselines presented in Rajpurkar et al. (2018) by over 2.5 F1. We use BiDAF++ as a baseline since its architecture is typical for QA systems, and, until recently, was state-of-the-art on SQuAD 2.0 and other benchmarks. BiDAF++ Let a and b be the outputs of the passage and question encoders respectively (in place of the standard p and q notations). The inference layer’s inputs ainf are defined similarly to i 3600 the generic model’s in (7), but also contain an aggregation o"
N19-1362,D17-1257,0,0.0581715,"Missing"
N19-1362,P18-1076,0,0.0232903,"enchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inference tasks, Weissenborn et al. (2017), Bauer et al. (2018), and Mihaylov and Frank (2018) dynamically refine word representations by reading assertions from ConceptNet and Wikipedia abstracts. Our approach, on the other hand, relies on a relatively simple extension of existing cross-sentence inference models. Furthermore, we do not need to dynamically retrieve and process knowledge base facts or Wikipedia texts, and just pretrain our pair vectors in advance. KIM (Chen et al., 2017) integrates word-pair vectors into the ESIM model for NLI in a very similar way to ours. However, KIM’s wordpair vectors contain only hand-engineered wordrelation indicators from WordNet, whereas our wor"
N19-1362,N13-1090,0,0.737968,"ey, 2005; Shwartz et al., 2016) and knowledge base completion (Riedel et al., 2013), this paper shows, for the first time, that large scale learning of pairwise embeddings can be used to directly improve the performance of neural cross-sentence inference models. More specifically, we train a feedforward network R(x, y) that learns representations for the individual words x and y, as well as how to compose them into a single vector. Training is done by maximizing a generalized notion of the pointwise mutual information (PMI) among x, y, and their context c using a variant of negative sampling (Mikolov et al., 2013a). Making R(x, y) a 3597 Proceedings of NAACL-HLT 2019, pages 3597–3608 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics compositional function on individual words alleviates the sparsity that necessarily comes with embedding pairs of words, even at a very large scale. We show that our embeddings can be added to existing cross-sentence inference models, such as BiDAF++ (Seo et al., 2017; Clark and Gardner, 2018) for QA and ESIM (Chen et al., 2017) for NLI. Instead of changing the word embeddings that are fed into the encoder, we add the pretraine"
N19-1362,N18-1202,1,0.901698,"and it should, therefore, help to augment its input with pair2vec. We augment ainf (7) with the pair vectors ri,j (3) by i concatenating a weighted average of the pair vectors ri,j involving ai , where the weights are the same αi,j computed via attention in (5): X ri = αi,j ri,j (8) j ainf i  = ai ; bi ; ri  (9) The symmetric term binf is defined analogously. j 3.2 Question Answering We augment the inference layer in the BiDAF++ model with pair2vec. BiDAF++ is an improved version of the BiDAFNoAnswer (Seo et al., 2017; Levy et al., 2017) which includes selfattention and ELMo embeddings from Peters et al. (2018). We found this variant to be stronger than the baselines presented in Rajpurkar et al. (2018) by over 2.5 F1. We use BiDAF++ as a baseline since its architecture is typical for QA systems, and, until recently, was state-of-the-art on SQuAD 2.0 and other benchmarks. BiDAF++ Let a and b be the outputs of the passage and question encoders respectively (in place of the standard p and q notations). The inference layer’s inputs ainf are defined similarly to i 3600 the generic model’s in (7), but also contain an aggregation of the elements in a, with better-aligned elements receiving larger weights:"
N19-1362,P18-2124,0,0.141089,"the pair vectors ri,j (3) by i concatenating a weighted average of the pair vectors ri,j involving ai , where the weights are the same αi,j computed via attention in (5): X ri = αi,j ri,j (8) j ainf i  = ai ; bi ; ri  (9) The symmetric term binf is defined analogously. j 3.2 Question Answering We augment the inference layer in the BiDAF++ model with pair2vec. BiDAF++ is an improved version of the BiDAFNoAnswer (Seo et al., 2017; Levy et al., 2017) which includes selfattention and ELMo embeddings from Peters et al. (2018). We found this variant to be stronger than the baselines presented in Rajpurkar et al. (2018) by over 2.5 F1. We use BiDAF++ as a baseline since its architecture is typical for QA systems, and, until recently, was state-of-the-art on SQuAD 2.0 and other benchmarks. BiDAF++ Let a and b be the outputs of the passage and question encoders respectively (in place of the standard p and q notations). The inference layer’s inputs ainf are defined similarly to i 3600 the generic model’s in (7), but also contain an aggregation of the elements in a, with better-aligned elements receiving larger weights: µ = softmaxi (max si,j ) j X ˆ ai = µi a i Benchmark ∆ BiDAF + pair2vec SQuAD 2.0 EM F1 65.66"
N19-1362,D16-1264,0,0.0570696,"words. Preprocessing removes all out-of-vocabulary words in the corpus. We consider each word pair within a window of 5 in the preprocessed corpus, and subsample5 instances based on pair probability with a threshold of 5·10−7 . We define the context as one word each to the left and right, and all the words in between each pair, replacing both target words with placeholders X and Y (see Table 1). More details can be found in the supplementary material. 4.1 Question Answering We experiment on the SQuAD 2.0 QA benchmark (Rajpurkar et al., 2018), as well as the adversarial datasets of SQuAD 1.1 (Rajpurkar et al., 2016; Jia and Liang, 2017). Table 3 shows the performance of BiDAF++, with ELMo , before and after adding pair2vec. Experiments on SQuAD 2.0 show that our pair representations improve performance by 2.72 F1. Moreover, adding pair2vec also results in better generalization on the adversarial SQuAD datasets with gains of 7.14 and 6.11 F1. 4.2 Natural Language Inference We report the performance of our model on MultiNLI and the adversarial test set from Glockner et al. (2018) in Table 5. We outperform the 5 Like in word2vec, subsampling reduces the size of the dataset and speeds up training. For this,"
N19-1362,N13-1008,0,0.0738311,"are trained unsupervised, and which significantly improve performance when added to existing cross-sentence attention mechanisms. Unlike single-word representations, which typically model the co-occurrence of a target word x with its context c, our word-pair representations are learned by modeling the three-way cooccurrence between words (x, y) and the context c that ties them together, as seen in Table 1. While similar training signals have been used to learn models for ontology construction (Hearst, 1992; Snow et al., 2005; Turney, 2005; Shwartz et al., 2016) and knowledge base completion (Riedel et al., 2013), this paper shows, for the first time, that large scale learning of pairwise embeddings can be used to directly improve the performance of neural cross-sentence inference models. More specifically, we train a feedforward network R(x, y) that learns representations for the individual words x and y, as well as how to compose them into a single vector. Training is done by maximizing a generalized notion of the pointwise mutual information (PMI) among x, y, and their context c using a variant of negative sampling (Mikolov et al., 2013a). Making R(x, y) a 3597 Proceedings of NAACL-HLT 2019, pages"
N19-1362,W16-5309,0,0.0214379,"s of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inference tasks, Weissenborn et al. ("
N19-1362,N18-1101,0,0.0469063,", ainf is recontextualized using a BiGRU and self attention. Finally a prediction layer predicts the start and end tokens. BiDAF++ with pair2vec To add our pair vectors, we simply concatenate ri (3) to ainf (12): i   ainf = ai ; bi ; ai ◦ bi ; ˆ a; ri (13) i 3.3 Benchmark ESIM + pair2vec Matched 79.68 Mismatched 78.80 81.03 80.12 ∆ +1.35 +1.32 Table 4: Performance on MultiNLI, with and without pair2vec. All models have ELMo. Natural Language Inference For NLI, we augment the ESIM model (Chen et al., 2017), which was previously state-of-theart on both SNLI (Bowman et al., 2015) and MultiNLI (Williams et al., 2018) benchmarks. ESIM Let a and b be the outputs of the premise and hypothesis encoders respectively (in place of the standard p and h notations). The inference layer’s inputs ainf (and binf i j ) are defined similarly to the generic model’s in (7):   ainf = ai ; bi ; ai ◦ bi ; ai − bi (14) i In the later layers, ainf and binf are projected, recontextualized, and converted to a fixed-length vector for each sentence using multiple pooling schemes. These vectors are then passed on to an output layer, which predicts the class. ESIM with pair2vec To add our pair vectors, we simply concatenate ri (3)"
N19-1362,W16-5304,0,0.012518,"ivariate distribution of target words and contexts is modeled. Our work deviates from the word embedding literature in two major aspects. First, our goal is to represent word pairs, not individual words. Second, our new PMI formulation models the trivariate word-word-context distribution. Experiments show that our pair embeddings can complement single-word embeddings. Mining Textual Patterns There is extensive literature on mining textual patterns to predict relations between words (Hearst, 1992; Snow et al., 2005; Turney, 2005; Riedel et al., 2013; Van de Cruys, 2014; Toutanova et al., 2015; Shwartz and Dagan, 2016). These approaches focus mostly on relations between pairs of nouns (perhaps with the exception of VerbOcean (Chklovski and Pantel, 2004)). More recently, they have been expanded to predict relations between unrestricted pairs of words (Jameel et al., 2018; Espinosa Anke and Schockaert, 2018), assuming that each word-pair was observed together during pretraining. Washio and Kato (2018a,b) relax this assumption with a compositional model that can represent any pair, as long as each word appeared (individually) in the corpus. These methods are evaluated on either intrinsic relation prediction ta"
N19-1362,P17-1132,0,0.0242905,"(individually) in the corpus. These methods are evaluated on either intrinsic relation prediction tasks, such as BLESS (Baroni and Lenci, 2011) and CogALex (Santus et al., 2016), or knowledge-base population benchmarks, e.g. FB15 (Bordes et al., 2013). To the best of our knowledge, our work is the first to integrate pattern-based methods into modern highperforming semantic models and evaluate their impact on complex end-tasks like QA and NLI. Integrating Knowledge in Complex Models Ahn et al. (2016) integrate Freebase facts into a language model using a copying mechanism over fact attributes. Yang and Mitchell (2017) modify the LSTM cell to incorporate WordNet and NELL knowledge for event and entity extraction. For cross-sentence inference tasks, Weissenborn et al. (2017), Bauer et al. (2018), and Mihaylov and Frank (2018) dynamically refine word representations by reading assertions from ConceptNet and Wikipedia abstracts. Our approach, on the other hand, relies on a relatively simple extension of existing cross-sentence inference models. Furthermore, we do not need to dynamically retrieve and process knowledge base facts or Wikipedia texts, and just pretrain our pair vectors in advance. KIM (Chen et al."
N19-1362,P16-1226,0,0.0463128,"Missing"
N19-1362,D15-1174,0,0.205549,"g, we typically replace x and y by sampling from their unigram distributions. In addition to this, we also sample uniformly from the top 100 words according to cosine similarity using distributional word vectors. This is done to encourage the model to learn relations between specific instances as opposed to more general types. For example, using California as a negative sample for Oregon helps the model to learn that the pattern “X is located in Y” fits the pair (Portland, Oregon), but not the pair (Portland, California). Similar adversarial constraints were used in knowledge base completion (Toutanova et al., 2015) and word embeddings (Li et al., 2017).4 3 Integrating pair2vec into Models We first present a general outline for incorporating pair2vec into attention-based architectures, and then discuss changes made to BiDAF++ and ESIM. The key idea is to inject our pairwise representations into the attention layer by reusing the cross-sentence attention weights. In addition to attentive pooling over single word representations, we also pool over cross-sentence word pair embeddings (Figure 1). 4 Applying typed sampling also changes the value to which our objective will converge, and will replace the unigr"
N19-1362,N18-1102,0,0.3372,"ong with their element-wise product, are fed into a four-layer perceptron: R(x, y) = M LP 4 (x, y, x ◦ y) The context c = c1 ...cn is encoded as a ddimensional vector using the function C(c). C(c) embeds each token ci with a lookup matrix Ec , contextualizes it with a single-layer Bi-LSTM, and then aggregates the entire context with attentive pooling: ci = Ec (ci ) h1 ...hn = BiLSTM(c1 ...cn ) w = softmaxi (khi ) X C(c) = wi Whi i where W ∈ Rd×d and k ∈ Rd . All parameters, including the lookup tables Ea and Ec , are trained. Our representation is similar to two recentlyproposed frameworks by Washio and Kato (2018a,b), but differs in that: (1) they use dependency paths as context, while we use surface form; (2) they encode the context as either a lookup table or the last state of a unidirectional LSTM. We also use a different objective, which we discuss next. 2.2 Objective To optimize our representation functions, we consider two variants of negative sampling (Mikolov et al., 2013a): bivariate and multivariate. The original bivariate objective models the two-way distribution of context and (monolithic) word pair co-occurrences, while our multivariate extension models the three-way distribution of word-"
N19-1362,D18-1058,0,0.396801,"ong with their element-wise product, are fed into a four-layer perceptron: R(x, y) = M LP 4 (x, y, x ◦ y) The context c = c1 ...cn is encoded as a ddimensional vector using the function C(c). C(c) embeds each token ci with a lookup matrix Ec , contextualizes it with a single-layer Bi-LSTM, and then aggregates the entire context with attentive pooling: ci = Ec (ci ) h1 ...hn = BiLSTM(c1 ...cn ) w = softmaxi (khi ) X C(c) = wi Whi i where W ∈ Rd×d and k ∈ Rd . All parameters, including the lookup tables Ea and Ec , are trained. Our representation is similar to two recentlyproposed frameworks by Washio and Kato (2018a,b), but differs in that: (1) they use dependency paths as context, while we use surface form; (2) they encode the context as either a lookup table or the last state of a unidirectional LSTM. We also use a different objective, which we discuss next. 2.2 Objective To optimize our representation functions, we consider two variants of negative sampling (Mikolov et al., 2013a): bivariate and multivariate. The original bivariate objective models the two-way distribution of context and (monolithic) word pair co-occurrences, while our multivariate extension models the three-way distribution of word-"
P13-2080,N12-1021,0,0.0900373,"Missing"
P13-2080,S13-2045,1,0.930106,"t is called partial textual entailment, because we are only interested in recognizing whether a single element of the hypothesis is entailed. To differentiate the two tasks, we will refer to the original textual entailment task as complete textual entailment. Partial textual entailment was first introduced by Nielsen et al. (2009), who presented a machine learning approach and showed significant improvement over baseline methods. Recently, a public benchmark has become available through the Joint Student Response Analysis and 8th Recognizing Textual Entailment (RTE) Challenge in SemEval 2013 (Dzikovska et al., 2013), on which we focus in this paper. Our goal in this paper is to investigate the idea of partial textual entailment, and assess whether Textual entailment is an asymmetric relation between two text fragments that describes whether one fragment can be inferred from the other. It thus cannot capture the notion that the target fragment is “almost entailed” by the given text. The recently suggested idea of partial textual entailment may remedy this problem. We investigate partial entailment under the faceted entailment model and the possibility of adapting existing textual entailment methods to thi"
P13-2080,R11-1063,1,0.82929,"rate movement in the body. H: The main job of muscles is to move bones. Lexical Inference This feature checks whether both facet words, or semantically related words, appear in T . We use WordNet (Fellbaum, 1998) with the Resnik similarity measure (Resnik, 1995) and count a facet term wi as matched if the similarity score exceeds a certain threshold (0.9, empirically determined on the training set). Both w1 and w2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first pars"
P13-2080,P11-2098,1,0.802976,"mainly on lexical inference and syntax. We examined three representative modules that reflect these levels: Exact Match, Lexical Inference, and Syntactic Inference. Exact Match We represent T as a bag-of-words containing all tokens and lemmas appearing in the text. We then check whether both facet lemmas w1 , w2 appear in the text’s bag-of-words. Exact matching was used as a baseline in previous recognizing textual entailment challenges (Bentivogli et al., 2011), and similar methods of lemmamatching were used as a component in recognizing textual entailment systems (Clark and Harrison, 2010; Shnarch et al., 2011). Task Definition In order to tackle partial entailment, we need to find a way to decompose a hypothesis. Nielsen et al. (2009) defined a model of facets, where each such facet is a pair of words in the hypothesis and the direct semantic relation connecting those two words. We assume the simplified model that was used in RTE-8, where the relation between the words is not explicitly stated. Instead, it remains unstated, but its interpreted meaning should correspond to the manner in which the words are related in the hypothesis. For example, in the sentence “the main job of muscles is to move bo"
P13-2080,S12-1051,0,0.0646707,"Missing"
P13-2080,P12-3013,1,0.842323,"2 must match for this module’s entailment decision to be positive. Syntactic Inference This module builds upon the open source1 Bar-Ilan University Textual Entailment Engine (BIUTEE) (Stern and Dagan, 2011). BIUTEE operates on dependency trees by applying a sequence of knowledge-based transformations that converts T into H. It determines entailment according to the “cost” of generating the hypothesis from the text. The cost model can be automatically tuned with a relatively small training set. BIUTEE has shown state-of-the-art performance on previous recognizing textual entailment challenges (Stern and Dagan, 2012). Since BIUTEE processes dependency trees, both T and the facet must be parsed. We therefore extract a path in H’s dependency tree that represents the facet. This is done by first parsing H, and then locating the two nodes whose words compose the facet. We then find their lowest common ancestor (LCA), and extract the path P from w1 to The facet (muscles, move) refers to the agent role in H, and is expressed by T . However, the facet (move, bones), which refers to a theme or direct object relation in H, is unaddressed by T . 3 Entailment Modules Recognizing Faceted Entailment Our goal is to inv"
P14-2050,N09-1003,0,0.682178,"Missing"
P14-2050,W13-3520,0,0.510599,"ques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies. Previous work on neural word embedding"
P14-2050,Q13-1033,1,0.350488,"ow but not directly related to the target word (e.g. Australian is not used as the context for discovers). In addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientists are subjects. We thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity. Dependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in. This is facilitated by recent advances in parsing technology (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013) that allow parsing to syntactic dependencies with very high speed and near state-of-the-art accuracy. After parsing each sentence, we derive word contexts as follows: for a target word w with modifiers m1 , . . . , mk and a head h, we consider the contexts (m1 , lbl1 ), . . . , (mk , lblk ), (h, lblh−1 ), 4 Experiments and Evaluation We experiment with 3 training conditions: B OW5 (bag-of-words contexts with k = 5), B OW2 (same, with k = 2) and D EPS (dependency-based syntactic contexts). We modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to t"
P14-2050,J10-4006,0,0.0160132,"de.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, and by p(D = 0|w, c) = 1 − p(D = 1|w, c) the probability that (w, c) did not. The distribution is modeled as: following work in sparse vector-space models (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees. The different kinds of contexts produce noticeably different embeddings, and induce different word similarities. In particular, the bag-ofwords nature of the contexts in the “original” S KIP G RAM model yield broad topical similarities, while the dependency-based contexts yield more functional similarities of a cohyponym nature. This effect is demonstrated using both qualitative and quantitative analysis (Section 4). The neural word-embeddings are considered opaque, in the sense that it"
P14-2050,W14-1618,1,0.448246,"Missing"
P14-2050,J92-4003,0,0.109988,"hat the word “pizza” is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris (1954), stating that words in similar contexts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 1 code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, a"
P14-2050,P98-2127,0,0.0379453,"ement no. 287923 (EXCITEMENT). 1 code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, and by p(D = 0|w, c) = 1 − p(D = 1|w, c) the probability that (w, c) did not. The distribution is modeled as: following work in sparse vector-space models (Lin, 1998; Pad´o and Lapata, 2007; Baroni and Lenci, 2010), we experiment with syntactic contexts that are derived from automatically produced dependency parse-trees. The different kinds of contexts produce noticeably different embeddings, and induce different word similarities. In particular, the bag-ofwords nature of the contexts in the “original” S KIP G RAM model yield broad topical similarities, while the dependency-based contexts yield more functional similarities of a cohyponym nature. This effect is demonstrated using both qualitative and quantitative analysis (Section 4). The neural word-embed"
P14-2050,W12-3308,1,0.490614,"Missing"
P14-2050,N13-1090,0,0.742971,"sociation between the word and a particular context (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) for a comprehensive survey). In some works, the dimensionality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful wo"
P14-2050,W08-1301,0,0.0130058,"Missing"
P14-2050,J07-2002,0,0.0171713,"Missing"
P14-2050,P10-1044,0,0.00942085,"Missing"
P14-2050,P10-1045,0,0.00522992,"Missing"
P14-2050,D11-1014,0,0.218583,"reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large word and context vocabularies. Previous work o"
P14-2050,N03-1033,0,0.0548877,"ny negative contexts to sample for every correct one) was 15. 2 word2vec’s implementation is slightly more complicated. The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic window size. In addition, the window size is not fixed to k but is sampled uniformly in the range [1, k] for each word. 304 Target Word All embeddings were trained on English Wikipedia. For D EPS, the corpus was tagged with parts-of-speech using the Stanford tagger (Toutanova et al., 2003) and parsed into labeled Stanford dependencies (de Marneffe and Manning, 2008) using an implementation of the parser described in (Goldberg and Nivre, 2012). All tokens were converted to lowercase, and words and contexts that appeared less than 100 times were filtered. This resulted in a vocabulary of about 175,000 words, with over 900,000 distinct syntactic contexts. We report results for 300 dimension embeddings, though similar trends were also observed with 600 dimensions. 4.1 batman hogwarts turing florida object-oriented Qualitative Evaluation Our first evaluation is qualitative: we manua"
P14-2050,P10-1040,0,0.346783,"nality of the sparse word-context vectors is reduced, using techniques such as SVD (Bullinaria and Levy, 2007) or LDA (Ritter et al., 2010; S´eaghdha, 2010; Cohen et al., 2012). Most recently, it has been proposed to represent words as dense vectors that are derived by various training methods inspired from neural-network language modeling (Bengio et al., 2003; Collobert and Weston, 2008; Mnih and Hinton, 2008; Mikolov et al., 2011; Mikolov et al., 2013b). These representations, referred to as “neural embeddings” or “word embeddings”, have been shown to perform well across a variety of tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). Word embeddings are easy to work with because they enable efficient computation of word similarities through low-dimensional matrix operations. Among the state-of-the-art wordembedding methods is the skip-gram with negative sampling model (S KIP G RAM), introduced by Mikolov et al. (2013b) and implemented in the word2vec software.1 Not only does it produce useful word representations, but it is also very efficient to train, works in an online fashion, and scales well to huge copora (billions of words) as well as very large w"
P14-2050,P08-1086,0,0.00356561,"is a good argument for the verb “eat”, we cannot infer that “hamburger” is also a good argument. We thus seek a representation that captures semantic and syntactic similarities between words. A very common paradigm for acquiring such representations is based on the distributional hypothesis of Harris (1954), stating that words in similar contexts have similar meanings. Based on the distributional hypothesis, many methods of deriving word representations were explored in the NLP community. On one end of the spectrum, words are grouped into clusters based on their contexts (Brown et al., 1992; Uszkoreit and Brants, 2008). On the other end, words ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 1 code.google.com/p/word2vec/ 302 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 302–308, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics a large body of text. Consider a word-context pair (w, c). Did this pair come from the data? We denote by p(D = 1|w, c) the probability that (w, c) came from the data, and by p(D = 0|w, c) = 1 − p(D"
P14-2050,C98-2122,0,\N,Missing
P14-2050,C12-1059,1,\N,Missing
P14-5008,W07-1401,1,0.865172,"esources for these languages (assuming that the EDAs themselves are largely language-independent). These are provided by the language-independent knowledge acquisition tools which we offer alongside the platform (cf. Section 3.2). EOP Evaluation Results for the three EDAs included in the EOP platform are reported in Table 1. Each line represents an EDA, the language and the dataset on which the EDA was evaluated. For brevity, we omit here the knowledge resources used for each EDA, even though knowledge configuration clearly affects performance. The evaluations were performed on RTE-3 dataset (Giampiccolo et al., 2007), where the goal is to maximize accuracy. We (manually) translated it to German and Italian for evaluations: in both cases the results fix a reference for the two languages. The two new datasets for German and English are available both as part of the EOP distribution and independently5 . The transformation-based EDA was also evaluated on RTE-6 dataset (Bentivogli et al., 2010), in which the goal is to maximize the F1 measure. The results of the included EDAs are higher than median values of participated systems in RTE-3, and they are competing with state-of-the-arts in RTE-6 results. To the b"
P14-5008,P98-2127,0,0.00988239,"otivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-structured and well-documented form. A subclass of this group is formed by researchers who want to set up a RTE infrastructure for languages in which it does"
P14-5008,P09-1051,1,0.820811,"art of or all of the semantic processing, such as Question Answering or Intelligent Tutoring. Such users require a system that is as easy to deploy as possible, which motivates our offer of the EOP platform as a library. They also require a system that provides good quality at a reasonable efficiency as well as guidance as to the best choice of parameters. The latter point is realized through our results archive in the official EOP Wiki on the EOP site. Table 1: EDAs results for specific domains. Particularly, the EOP platform includes a language independent tool to build Wikipedia resources (Shnarch et al., 2009), as well as a language-independent framework for building distributional similarity resources like DIRT (Lin and Pantel, 2002) and Lin similarity(Lin, 1998). 3.3 Use Case 2: Textual Entailment Development. This category covers researchers who are interested in Recognizing Textual Entailment itself, for example with the goal of developing novel algorithms for detecting entailment. In contrast to the first category, this group need to look ”under the hood” of the EOP platform and access the source code of the EOP. For this reason, we have spent substantial effort to provide the code in a well-s"
P14-5008,P12-1030,1,0.845888,"raphrasing patterns at the predicate-argument level that cannot be captured by purely lexical rules. Formally, each syntactic rule consists of two dependency tree fragments plus a mapping from the variables of the LHS tree to the variables of the RHS tree.4 2.3 In the EOP we include a transformation based inference system that adopts the knowledge based transformations of Bar-Haim et al. (2007), while incorporating a probabilistic model to estimate transformation confidences. In addition, it includes a search algorithm which finds an optimal sequence of transformations for any given T/H pair (Stern et al., 2012). Edit distance EDA involves using algorithms casting textual entailment as the problem of mapping the whole content of T into the content of H. Mappings are performed as sequences of editing operations (i.e., insertion, deletion and substitution) on text portions needed to transform T into H, where each edit operation has a cost associated with it. The underlying intuition is that the probability of an entailment relation between T and H is related to the distance between them; see Kouylekov and Magnini (2005) for a comprehensive experimental study. Configuration Files The EC components can b"
P14-5008,P07-2045,0,\N,Missing
P14-5008,C98-2122,0,\N,Missing
P16-2041,S13-1035,0,0.0207886,"Missing"
P16-2041,C14-1207,0,0.0353119,"Missing"
P16-2041,D15-1113,0,0.0273263,"Missing"
P16-2041,E14-1057,0,0.0438097,"Missing"
P16-2041,Q15-1027,0,0.0333362,"Missing"
P16-2041,P14-2050,1,0.787903,"Missing"
P16-2041,W14-1610,1,0.924145,"Missing"
P16-2041,marelli-etal-2014-sick,0,0.0766008,"Missing"
P16-2041,S07-1009,0,0.115542,"Missing"
P16-2041,P13-1131,1,0.899585,"Missing"
P16-2041,N13-1008,0,0.156982,"Missing"
P16-2041,N15-1118,0,0.0309087,"Missing"
P16-2041,D10-1106,0,0.349506,"Missing"
P16-2041,P07-1058,1,0.840671,"Missing"
P16-2041,W15-1501,1,0.877236,"Missing"
P16-2041,P15-1146,0,0.0611783,"Missing"
P16-2041,D12-1018,1,0.906476,"Missing"
P16-2041,P15-2070,0,0.0961797,"Missing"
P16-2041,P12-2031,1,0.768861,"Missing"
P16-2041,P11-1062,1,\N,Missing
P16-2041,S13-1002,0,\N,Missing
P16-2041,Q13-1015,0,\N,Missing
P16-2041,P13-1158,0,\N,Missing
P16-2041,P14-1061,0,\N,Missing
P16-2041,D13-1160,0,\N,Missing
P18-1009,D17-1284,0,0.106987,"Missing"
P18-1009,N06-2015,0,0.0905723,"Missing"
P18-1009,D17-1018,1,0.935109,"to prevent malaria?”). We annotate a dataset of about 6,000 mentions via crowdsourcing (Section 2.1), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section 2.2). 88 2.1 Crowdsourcing Entity Types To capture multiple domains, we sample sentences from Gigaword (Parker et al., 2011), OntoNotes (Hovy et al., 2006), and web articles (Singh et al., 2012). We select entity mentions by taking maximal noun phrases from a constituency parser (Manning et al., 2014) and mentions from a coreference resolution system (Lee et al., 2017). We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate fine-grained types, we require at least one general type (e.g. person, organization, location) and two specific types (e.g. doctor, fish, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet (Miller, 1995) to expand these types automatically by generating all their synonyms and hypernyms based on the most common sense, and ask five different annotators to validate the generate"
P18-1009,P14-5010,0,0.0118187,"resolution and question answering (e.g. “Which philanthropist is trying to prevent malaria?”). We annotate a dataset of about 6,000 mentions via crowdsourcing (Section 2.1), and demonstrate that using an large type vocabulary substantially increases annotation coverage and diversity over existing approaches (Section 2.2). 88 2.1 Crowdsourcing Entity Types To capture multiple domains, we sample sentences from Gigaword (Parker et al., 2011), OntoNotes (Hovy et al., 2006), and web articles (Singh et al., 2012). We select entity mentions by taking maximal noun phrases from a constituency parser (Manning et al., 2014) and mentions from a coreference resolution system (Lee et al., 2017). We provide the sentence and the target entity mention to five crowd workers on Mechanical Turk, and ask them to annotate the entity’s type. To encourage annotators to generate fine-grained types, we require at least one general type (e.g. person, organization, location) and two specific types (e.g. doctor, fish, religious institute), from a type vocabulary of about 10K frequent noun phrases. We use WordNet (Miller, 1995) to expand these types automatically by generating all their synonyms and hypernyms based on the most com"
P18-1009,E17-1075,0,0.237414,"Missing"
P18-1009,N07-1071,0,0.0168201,"defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related to selectional preference (Resnik, 1996; Pantel et al., 2007; Zapirain et al., 2013; de Cruys, 2014), where the goal is to induce semantic generalizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the arguments’ types, which allows us to capture more nuanced types. For example, not every type that fits “He played the violin in his room” is also suitable for “He played the violin in the Carnegie Hall”. Entity typing here can be connected to argument finding in semantic role labeling. To deal with noisy distant supervision for KB population and entity typing, researcher"
P18-1009,D15-1103,0,0.108925,"growing attention, and is used in many applications (Gupta et al., 2017; Ren et al., 2017; Yaghoobzadeh et al., 2017b; Raiman and Raiman, 2018). Researchers studied typing in varied contexts, including mentions in specific sentences (as we consider) (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015; Dong et al., 2015; Schutze et al., 2017), corpus-level prediction (Yaghoobzadeh and Sch¨utze, 2016), and lexicon level (given only a noun phrase with no context) (Yao et al., 2013). Recent work introduced fine-grained type ontologies (Rabinovich and Klein, 2017; Murty et al., 2017; Corro et al., 2015), defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related to selectional preference (Resnik, 1996;"
P18-1009,D14-1004,0,0.0608809,"Missing"
P18-1009,P17-2052,0,0.107655,"76.8 66.1 71.8 7 Fine-grained NER has received growing attention, and is used in many applications (Gupta et al., 2017; Ren et al., 2017; Yaghoobzadeh et al., 2017b; Raiman and Raiman, 2018). Researchers studied typing in varied contexts, including mentions in specific sentences (as we consider) (Ling and Weld, 2012; Gillick et al., 2014; Yogatama et al., 2015; Dong et al., 2015; Schutze et al., 2017), corpus-level prediction (Yaghoobzadeh and Sch¨utze, 2016), and lexicon level (given only a noun phrase with no context) (Yao et al., 2013). Recent work introduced fine-grained type ontologies (Rabinovich and Klein, 2017; Murty et al., 2017; Corro et al., 2015), defined using Wikipedia categories (100), Freebase types (1K) and WordNet senses (16K). However, they focus on named entities, and data has been challenging to gather, often approximating gold annotations with distant supervision. In contrast, (1) our ontology contains any frequent noun phrases that depicts a type, (2) our task goes beyond named entities, covering every noun phrase (even pronouns), and (3) we provide crowdsourced annotations which provide context-sensitive, fine grained type labels. Contextualized fine-grained entity typing is related"
P18-1009,Q14-1037,0,0.0693507,"e sentence. Table 1 shows three examples that exhibit a rich variety of types at different granularities. Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns. Incorporating fine-grained entity types has improved entity-focused downstream tasks, such as relation extraction (Yaghoobzadeh et al., 2017a), question answering (Yavuz et al., 2016), query analysis (Balog and Neumayer, 2012), and coreference resolution (Durrett and Klein, 2014). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difficult to cover all posIntroduction Entities can often be described by very fine grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “criminal” due to the “robbing” and “arresting,” while “John” is more likely a “victim” because he was “robbed.” Such fine-grained types (victim, c"
P18-1009,D16-1144,0,0.719416,"We predict every type t for which yt &gt; 0.5, or arg max yt if there is no such type. Multitask Objective The distant supervision sources provide partial supervision for ultra-fine types; KBs often provide more general types, while head words usually provide only ultra-fine types, without their generalizations. In other words, the absence of a type at a different level of abstraction does not imply a negative signal; e.g. when the head word is “inventor”, the model should not be discouraged to predict “person”. Prior work used a customized hinge loss (Abhishek et al., 2017) or max margin loss (Ren et al., 2016a) to improve robustness to noisy or incomplete supervision. We propose a multitask objective that reflects the characteristic of our training dataset. Instead of updating all labels for each example, we divide labels into three bins (general, fine, and ultra-fine), and update labels only in bin containing at least one positive label. Specifically, the training objective is to minimize J where t is the target vector at each granularity: Model We design a model for predicting sets of types given a mention in context. The architecture resembles the recent neural AttentiveNER model (Shimaoka et a"
P18-1009,E17-1111,0,0.0716907,"Missing"
P18-1009,D11-1141,0,0.271575,"Missing"
P18-1009,D16-1015,0,0.109002,"form noun phrases that describe appropriate types for the role the target entity plays in the sentence. Table 1 shows three examples that exhibit a rich variety of types at different granularities. Our task effectively subsumes existing finegrained named entity typing formulations due to the use of a very large type vocabulary and the fact that we predict types for all noun phrases, including named entities, nominals, and pronouns. Incorporating fine-grained entity types has improved entity-focused downstream tasks, such as relation extraction (Yaghoobzadeh et al., 2017a), question answering (Yavuz et al., 2016), query analysis (Balog and Neumayer, 2012), and coreference resolution (Durrett and Klein, 2014). These systems used a relatively coarse type ontology. However, manually designing the ontology is a challenging task, and it is difficult to cover all posIntroduction Entities can often be described by very fine grained types. Consider the sentences “Bill robbed John. He was arrested.” The noun phrases “John,” “Bill,” and “he” have very specific types that can be inferred from the text. This includes the facts that “Bill” and “he” are both likely “criminal” due to the “robbing” and “arresting,” w"
P18-1009,E17-2119,0,0.117556,"Missing"
P18-1009,P15-2048,0,0.237857,"Missing"
P18-1009,C12-2133,0,0.234817,"f distant supervision: automatically extracted nominal head words from raw text (Section 3.2). Using head words as a form of distant supervision provides fine-grained information about named entities and nominal mentions. While a KB may link “the 44th president of the United States” to many types such as author, lawyer, and professor, head words provide only the type “president”, which is relevant in the context. We bypass the challenge of automatically linking entities to Wikipedia by exploiting existing hyperlinks in web pages (Singh et al., 2012), following prior work (Ling and Weld, 2012; Yosef et al., 2012). Since our heuristic extraction of types from the definition sentence is somewhat noisy, we use a more conservative entity linking policy4 that yields a signal with similar overall accuracy to KB-linked data. 2 Data from: https://github.com/ shimaokasonse/NFGEC 3 We extract types by applying a dependency parser (Manning et al., 2014) to the definition sentence, and taking nouns that are dependents of a copular edge or connected to nouns linked to copulars via appositive or conjunctive edges. 4 Only link if the mention contains the Wikipedia entity’s name and the entity’s name contains the men"
P18-1009,N04-1002,0,0.150987,"Missing"
P18-1009,D12-1042,0,0.0575254,"ere the goal is to induce semantic generalizations on the type of arguments a predicate prefers. Rather than focusing on predicates, we condition on the entire sentence to deduce the arguments’ types, which allows us to capture more nuanced types. For example, not every type that fits “He played the violin in his room” is also suitable for “He played the violin in the Carnegie Hall”. Entity typing here can be connected to argument finding in semantic role labeling. To deal with noisy distant supervision for KB population and entity typing, researchers used multi-instance multi-label learning (Surdeanu et al., 2012; Yaghoobzadeh et al., 2017b) or custom losses (Abhishek et al., 2017; Ren et al., 2016a). Our multitask objective handles noisy supervision by pooling different distant supervision sources across different levels of granularity. Table 6: Results on the OntoNotes fine-grained entity typing test set. The first two models (AttentiveNER++ and AFET) use only KB-based supervision. LNR uses a filtered version of the KBbased training set. Our model uses all our distant supervision sources. Model Training Data Performance Acc. MaF1 MiF1 ONTO WIKI HEAD Attn. NER 3 3 3 3 46.5 53.7 63.3 72.8 58.3 68.0 Ou"
P18-2003,P17-1080,0,0.308427,"erties from the vector alone. Specifically, we predict the word’s part of speech (POS), as well as the first (parent), second (grand-parent), and third level (great-grandparent) constituent labels of the given word. Figure 1 shows how these labels correspond to an example constituency tree. 2.2 Analyzed Models We consider four different forms of supervision. Table 1 summarizes the differences in data, architecture, and hyperparameters.1 Our methodology follows Shi et al. (2016), who run syntactic feature prediction experiments over a number of different shallow machine translation models, and Belinkov et al. 2017a; 2017b, who use a similar process to study the morphological, part-of-speech, and semantic features learned by deeper machine translation encoders. We extend upon prior work by considering training signals for models other than machine translation, and by applying more stratified word-level syntactic tasks. 2.1 (1) Dependency Parsing We train a four-layer version of the Stanford dependency parser (Dozat and Manning, 2017) on the Universal Dependencies English Web Treebank (Silveira et al., 2014). We ran the parser with 4 bidirectional LSTM layers (the default is 3), yielding a UAS of 91.5 an"
P18-2003,I17-1001,0,0.16282,"Missing"
P18-2003,D14-1162,0,0.0801549,"Missing"
P18-2003,P17-1171,0,0.0180853,"els encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what syntactic information is stored in each word vector, we try to predict a series of constituency-based prop14 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 14–19 c Melbourne, Australia, July 15 - 20, 2018. 2018 Associa"
P18-2003,N18-1202,1,0.721736,"for example, in SRL we see that the parent constituent task peaks one layer after POS, and the grand-parent and great-grandparent tasks peak on the layer after that. One possible explanation is that each layer leverages the shallower syntactic information learned in the previous layer in order to construct a more abstract syntactic representation. In SRL and language modeling, it seems as though the syntactic information is then replaced by taskspecific information (semantic roles, word probabilities), perhaps making it redundant. This observation may also explain a modeling decision in ELMo (Peters et al., 2018), where injecting the contextualized word representations from a pre-trained language model was shown to boost performance on a wide variety of NLP tasks. ELMo represents each word using a task-specific weighted sum of the language model’s hidden layers, i.e. rather than use only the top layer, it selects which of the language model’s internal layers contain the most relevant information for the task at hand. Our results confirm that, in general, different types of information manifest at different layers, suggesting that post-hoc layer selection can be beneficial. Figure 3: Comparison between"
P18-2003,W13-3516,0,0.0426484,"Missing"
P18-2003,N09-1025,0,0.0490487,"emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what syntactic information is stored in each word vector, we try to predict a series of constituency-based prop14 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short P"
P18-2003,W05-0639,0,0.044504,"soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what syntactic information is stored in each word vector, we try to predict a series of constituency-based prop14 Proceedings of the 56th Annual Meeting of the Association for Computationa"
P18-2003,P81-1022,0,0.69212,"Missing"
P18-2003,D16-1159,0,0.408155,"nday” for the POS (green), parent constituent (blue), grandparent constituent (orange), and great-grandparent constituent (red) tasks. erties from the vector alone. Specifically, we predict the word’s part of speech (POS), as well as the first (parent), second (grand-parent), and third level (great-grandparent) constituent labels of the given word. Figure 1 shows how these labels correspond to an example constituency tree. 2.2 Analyzed Models We consider four different forms of supervision. Table 1 summarizes the differences in data, architecture, and hyperparameters.1 Our methodology follows Shi et al. (2016), who run syntactic feature prediction experiments over a number of different shallow machine translation models, and Belinkov et al. 2017a; 2017b, who use a similar process to study the morphological, part-of-speech, and semantic features learned by deeper machine translation encoders. We extend upon prior work by considering training signals for models other than machine translation, and by applying more stratified word-level syntactic tasks. 2.1 (1) Dependency Parsing We train a four-layer version of the Stanford dependency parser (Dozat and Manning, 2017) on the Universal Dependencies Engl"
P18-2003,N18-1108,0,0.0383606,"(red) constituent prediction tasks. itly. Specifically, we observe in Figure 3 that the language model and dependency parser perform nearly identically on the three constituent prediction tasks in the second layer of their respective networks. In deeper layers the parser continues to improve, while the language model peaks at layer 2 and drops off afterwards. These results may be surprising given the findings of Linzen et al. (2016), which found that RNNs trained on language modeling perform below baseline levels on the task of subject-verb agreement. However, the more recent investigation by Gulordava et al. (2018) are in line with our results. They find that language models trained on a number of different languages assign higher probabilities to valid long-distance dependencies than to incorrect ones. Therefore, LMs seem able to induce syntactic information despite being provided with no linguistic annotation. 4 Dependency Arc Prediction We run an additional experiment that seeks to clarify if the representations learned by deep NLP models capture information about syntactic structure. Using the internal representations from a deep RNN, we train a classifier to predict whether two words share an depen"
P18-2003,silveira-etal-2014-gold,0,0.193271,"Missing"
P18-2003,P17-1044,1,0.851187,"h one of four objectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at e"
P18-2003,P17-4012,0,0.199338,"y parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l. To determine what s"
P18-2003,D17-1018,1,0.842022,"ectives: dependency parsing, semantic role labeling, machine translation, or language modeling. In every case, we find a correspondence between network depth and syntactic depth, suggesting that a soft syntactic hierarchy emerges. This effect is robust across all conditions, indicating that the models encode significant amounts of syntax even in the absence of an explicit syntactic training supervision. 1 Introduction Deep recurrent neural networks (RNNs) have effectively replaced explicit syntactic features (e.g. parts of speech, dependencies) in state-of-the-art NLP models (He et al., 2017; Lee et al., 2017; Klein et al., 2017). However, previous work has shown that syntactic information (in the form of either input features or supervision) is useful for a wide variety of NLP tasks (Punyakanok et al., 2005; Chiang et al., 2009), even in the neural setting (Aharoni and Goldberg, 2017; Chen et al., 2017). In this paper, we show that the internal representations of RNNs trained on a variety of NLP tasks encode these syntactic features without explicit supervision. 2 Methodology Given a model that uses multi-layered RNNs, we collect the vector representation xli of each word i at each hidden layer l"
P18-2058,W05-0620,0,0.678935,"Missing"
P18-2058,P18-1191,1,0.824012,"akanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass. Our model builds on a recent coref"
P18-2058,D15-1112,0,0.148413,"Missing"
P18-2058,D17-1128,0,0.0386925,"P PRP PRP I-AM-PRP I-AM-PRP I-AM-PRP Many tourists visit Disney to meet their favorite cartoon characters [predicate] B-ARG0 I-ARG0 O O O B-V B-ARG1 I-ARG1 I-ARG1 I-ARG1 Many tourists visit Disney to meet their favorite cartoon characters [predicate] Figure 1: A comparison of our span-graph structure (top) versus BIO-based SRL (bottom). pruning for efficiency. The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, o"
P18-2058,P17-1044,1,0.911591,"RP I-AM-PRP Many tourists visit Disney to meet their favorite cartoon characters [predicate] B-ARG0 I-ARG0 O O O B-V B-ARG1 I-ARG1 I-ARG1 I-ARG1 Many tourists visit Disney to meet their favorite cartoon characters [predicate] Figure 1: A comparison of our span-graph structure (top) versus BIO-based SRL (bottom). pruning for efficiency. The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves"
P18-2058,D17-1018,1,0.89591,"presentations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipel"
P18-2058,P15-1109,0,0.155871,"B-AM- I-AM- I-AMPRP PRP PRP I-AM-PRP I-AM-PRP I-AM-PRP Many tourists visit Disney to meet their favorite cartoon characters [predicate] B-ARG0 I-ARG0 O O O B-V B-ARG1 I-ARG1 I-ARG1 I-ARG1 Many tourists visit Disney to meet their favorite cartoon characters [predicate] Figure 1: A comparison of our span-graph structure (top) versus BIO-based SRL (bottom). pruning for efficiency. The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predi"
P18-2058,N18-2108,1,0.90354,"Missing"
P18-2058,K17-1041,0,0.0504248,"ting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass. Our model builds on a recent coreference resolution model (Lee et al., 2017), by making central use of learned, contextualized span representations. We use these representations to predict SRL graphs directly over text spans."
P18-2058,P16-1105,0,0.0253707,"ly standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pa"
P18-2058,N18-1202,1,0.771516,"esentations as inputs, and computes a softmax over the label space L. 4 Results with gold predicates To compare with additional previous systems, we also conduct experiments with gold predicates by constraining our predicate beam to be gold predicates only. As shown in Table 2, our model significantly out-performs He et al. (2017), but falls short of Tan et al. (2018), a very recent attention-based (Vaswani et al., 2017) BIO-tagging model that was developed concurrently with our work. By adding the contextualized ELMo representations, we are able to out-perform all previous systems, including Peters et al. (2018), which applies ELMo to the SRL model introduced in He et al. (2017). Experiments We experiment on the CoNLL 2005 (Carreras and M`arquez, 2005) and CoNLL 2012 (OntoNotes 5.0, (Pradhan et al., 2013)) benchmarks, using two SRL setups: end-to-end and gold predicates. In the end-to-end setup, a system takes a tokenized sentence as input, and predicts all the predicates and their arguments. Systems are evaluated on the micro-averaged F1 for correctly predicting (predicate, argument span, label) tuples. For comparison with previous systems, we also report results with gold predicates, in which the c"
P18-2058,W04-2421,0,0.123961,"Missing"
P18-2058,J08-2005,0,0.358856,"imply the union of predicted SRL roles (edges) and their associated text spans (nodes). Our span-graph formulation overcomes a key limitation of semi-markov and BIO-based models (Kong et al., 2016; Zhou and Xu, 2015; Yang and Mitchell, 2017; He et al., 2017; Tan et al., 2018): it can model overlapping spans across different predicates in the same output structure (see Figure 1). The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al.,"
P18-2058,P17-1076,0,0.0230476,"d roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015). To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given. In this more realistic setting, where the predicate must be predicted, our model achieves state-of-the-art performance on PropBank. It also reinforces the strong performance of similar span embedding methods for coreference (Lee et al., 2017), suggesting that this style of models could be used for other span-span relation tasks, such as syntactic parsing (Stern et al., 2017), relation extraction (Miwa and Bansal, 2016), and QA-SRL (FitzGerald et al., 2018). Introduction Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1). They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment. We propose an end-to-end approach for predicting all the predicat"
P18-2058,D18-1548,0,0.273641,"Missing"
P18-2058,P06-4018,0,\N,Missing
P18-2058,D14-1162,0,\N,Missing
P18-2116,P17-1044,1,0.742747,"ile theoretically expressive, is in practice only a minor contributor that clouds the mathematical clarity of the model. By replacing the S-RNN with a context-independent function of the input, we arrive at a much more restricted class of RNNs, where the main recurrence is via the element-wise weighted sums that the gates are computing. We test our hypothesis on NLP problems, where LSTMs are wildly popular at least in part due to their ability to model crucial phenomena such as word order (Adi et al., 2017), syntactic structure (Linzen et al., 2016), and even long-range semantic dependencies (He et al., 2017). We consider four challenging tasks: language modeling, question answering, dependency parsing, and machine translation. Experiments show that while removing the gates from an LSTM can severely hurt performance, replacing the S-RNN with a simple linear transformation of the input results in minimal or no loss in model performance. We also show that, in many cases, LSTMs can be further simplified by removing the output gate, arriving at an even more transparent architecture, where the output is a context-independent function of the weighted sum. Together, these results suggest that the gates’"
P18-2116,P17-1171,0,0.0190863,"pact form: ht = OUTPUT t X wjt ◦ CONTENT(xj )  Language Modeling We evaluate the models on the Penn Treebank (PTB) (Marcus et al., 1993) language modeling benchmark. We use the implementation of Zaremba et al. (2014) from TensorFlow’s tutorial while replacing any invocation of LSTMs with simpler models. We test two of their configurations: medium and large (Table 1). Question Answering For question answering, we use two different QA systems on the Stanford question answering dataset (SQuAD) (Rajpurkar et al., 2016): the Bidirectional Attention Flow model (BiDAF) (Seo et al., 2016) and DrQA (Chen et al., 2017). BiDAF contains 3 LSTMs, which are referred to as the phrase layer, the modeling layer, and the span end encoder. Our experiments replace each of these LSTMs with their simplified counterparts. We directly use the implementation of BiDAF from AllenNLP (Gardner et al., 2017), and all experiments reuse the existing hyperparameters that were tuned for LSTMs. Likewise, we use an open-source implementation of DrQA1 and replace only the LSTMs, while leaving everything else intact. Table 2 shows the results. (9) j=0 where the content layer CONTENT(·) and the output layer OUTPUT(·) are both context-i"
P18-2116,J93-2004,0,\N,Missing
P18-2116,silveira-etal-2014-gold,0,\N,Missing
P18-2116,D16-1053,0,\N,Missing
P18-2116,D16-1264,0,\N,Missing
P18-2116,P17-4012,0,\N,Missing
Q15-1016,N09-1003,0,0.472274,"Missing"
Q15-1016,J10-4006,0,0.0160512,"are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, th"
Q15-1016,P14-1023,0,0.9474,"to a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008). These models represent each word as a ddimensional vector of real numbers, and vectors that are close to each other are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’"
Q15-1016,P12-1015,0,0.291036,"Missing"
Q15-1016,J90-1003,0,0.216701,"exts have been studied (Pad´o and Lapata, 2007; Baroni and Lenci, 2010; Levy and Goldberg, 2014a) this work focuses on fixed-window bag-of-words contexts. 2.1 Explicit Representations (PPMI Matrix) The traditional way to represent words in the distributional approach is to construct a highdimensional sparse matrix M , where each row represents a word w in the vocabulary VW and each column a potential context c ∈ VC . The value of each matrix cell Mij represents the association between the word wi and the context cj . A popular measure of this association is pointwise mutual information (PMI) (Church and Hanks, 1990). PMI is defined as the log ratio between w and c’s joint probability and the product of their marginal probabilities, which can be estimated by: P M I(w, c) = log Pˆ (w,c) Pˆ (w)Pˆ (c) = log #(w,c)·|D| #(w)·#(c) The rows of M PMI contain many entries of wordcontext pairs (w, c) that were never observed in the corpus, for which P M I(w, c) = log 0 = −∞. A common approach is thus to replace the M PMI matrix with M0PMI , in which P M I(w, c) = 0 in cases where #(w, c) = 0. A more consistent approach is to use positive PMI (PPMI), in which all 2.3 negative values are replaced by 0: P P M I(w, c)"
Q15-1016,J15-4004,0,0.840752,"Missing"
Q15-1016,P14-2050,1,0.2029,"ddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, then, is the source of superiority (or perceived superiority) of these recent embeddings? While the focus of the presentation in the wordembedding literature is on the mathematical model and the objective being optimized, other factors affect the results as well. In particular, embedding algorithms suggest some natural hyperparameters that can be t"
Q15-1016,W14-1618,1,0.194781,"ddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding methods consistently outperform the traditional methods by a non-trivial margin on many similarity-oriented tasks. However, state-of-the-art embedding methods are all based on the same bag-of-contexts representation of words. Furthermore, analysis by Levy and Goldberg (2014c) shows that word2vec’s SGNS is implicitly factorizing a word-context PMI matrix. That is, the mathematical objective and the sources of information available to SGNS are in fact very similar to those employed by the more traditional methods. What, then, is the source of superiority (or perceived superiority) of these recent embeddings? While the focus of the presentation in the wordembedding literature is on the mathematical model and the objective being optimized, other factors affect the results as well. In particular, embedding algorithms suggest some natural hyperparameters that can be t"
Q15-1016,W13-3512,0,0.895955,"Missing"
Q15-1016,W14-1619,1,0.218059,"84 .567 .484 Table 6: The average performance of SVD on word similarity tasks given different values of eig, in the vanilla scenario. pared CBOW to the other methods when setting all the hyperparameters to the defaults provided by word2vec (Table 3). With the exception of MSR’s analogy task, CBOW is not the bestperforming method of any other task in this scenario. Other scenarios showed similar trends in our preliminary experiments. While CBOW can potentially derive better representations by combining the tokens in each context window, this potential is not realized in practice. Nevertheless, Melamud et al. (2014) show that capturing joint contexts can indeed improve performance on word similarity tasks, and we believe it is a direction worth pursuing. 6 Hyperparameter Analysis We analyze the individual impact of each hyperparameter, and try to characterize the conditions in which a certain setting is beneficial. 6.1 Harmful Configurations Certain hyperparameter settings might cripple the performance of a certain method. We observe two scenarios in which SVD performs poorly. SVD does not benefit from shifted PPMI. Setting neg &gt; 1 consistently deteriorates SVD’s performance. Levy and Goldberg (2014c) ma"
Q15-1016,N13-1090,0,0.577456,"meaning of a word is at the heart of natural language processing (NLP). While a deep, human-like, understanding remains elusive, many methods have been successful in recovering certain aspects of similarity between words. Recently, neural-network based approaches in which words are “embedded” into a lowdimensional space were proposed by various authors (Bengio et al., 2003; Collobert and Weston, 2008). These models represent each word as a ddimensional vector of real numbers, and vectors that are close to each other are shown to be semantically related. In particular, a sequence of papers by Mikolov et al. (2013a; 2013b) culminated in the skip-gram with negative-sampling training method (SGNS): an efficient embedding algorithm that provides state-of-the-art results on various linguistic tasks. It was popularized via word2vec, a program for creating word embeddings. A recent study by Baroni et al. (2014) conducts a set of systematic experiments comparing word2vec embeddings to the more traditional distributional methods, such as pointwise mutual information (PMI) matrices (see Turney and Pantel (2010) and Baroni and Lenci (2010) for comprehensive surveys). These results suggest that the new embedding"
Q15-1016,J07-2002,0,0.021048,"Missing"
Q15-1016,D14-1162,0,0.144387,", and GloVe. For historical reasons, we refer to PPMI and SVD as “countbased” representations, as opposed to SGNS and GloVe, which are often referred to as “neural” or “prediction-based” embeddings. All of these methods (as well as all other “skip-gram”-based embedding methods) are essentially bag-of-words models, in which the representation of each word reflects a weighted bag of context-words that cooccur with it. Such bag-of-word embedding models were previously shown to perform as well as or better than more complex embedding methods on similarity and analogy tasks (Mikolov et al., 2013a; Pennington et al., 2014). Notation We assume a collection of words w ∈ VW and their contexts c ∈ VC , where VW and VC are the word and context vocabularies, and denote the collection of observed word-context pairs as 212 D. We use #(w, c) to denote the number of times the in D. Similarly, #(w) = P pair (w, c) appears P 0 ) and #(c) = 0 #(w, c 0 0 c ∈VC w ∈VW #(w , c) are the number of times w and c occurred in D, respectively. In some algorithms, words and contexts are embedded in a space of d dimensions. In these cases, each word w ∈ VW is associated with a vector w ~ ∈ Rd and similarly each context c ∈ VC is repres"
S13-2048,S12-1059,1,0.886472,"Missing"
S13-2048,C12-1011,1,0.819518,"Missing"
S13-2048,S13-2045,1,0.847683,"tment Technische Universit¨at Darmstadt § Natural Language Processing Lab Computer Science Department Bar-Ilan University Abstract Our system combines text similarity measures with a textual entailment system. In the main task, we focused on the influence of lexicalized versus unlexicalized features, and how they affect performance on unseen questions and domains. We also participated in the pilot partial entailment task, where our system significantly outperforms a strong baseline. 1 Ido Dagan§ Introduction The Joint Student Response Analysis and 8th Recognizing Textual Entailment Challenge (Dzikovska et al., 2013) brings together two important dimensions of Natural Language Processing: real-world applications and semantic inference technologies. The challenge focuses on the domain of middleschool quizzes, and attempts to emulate the meticulous marking process that teachers do on a daily basis. Given a question, a reference answer, and a student’s answer, the task is to determine whether the student answered correctly. While this is not a new task in itself, the challenge focuses on employing textual entailment technologies as the backbone of this educational application. As a consequence, we formalize"
S13-2048,W01-0515,0,0.0300847,"use semantic similarity measures in order to bridge a possible vocabulary gap between the student and reference answer. We use the ESA measure (Gabrilovich 3 code.google.com/p/dkpro-core-asl/ DKPro Core v1.4.0, TreeTagger models v20130204.0, Stanford parser PCFG model v20120709.0. 5 Using the 750 most frequent n-grams gave good results on the training set, so we also used this number for the test runs. 6 As basic similarity measures, we use greedy string tiling (Wise, 1996) with n = 3, longest common subsequence and longest common substring (Allison and Dix, 1986), and word n-gram containment(Lyon et al., 2001) with n = 2. 4 and Markovitch, 2007) based on concept vectors build from WordNet, Wiktionary, and Wikipedia. Spelling Features As spelling errors might be indicative of the answer quality, we use the number of spelling errors normalized by the text length as an additional feature. Entailment Features We run BIUTEE (Stern and Dagan, 2011) on the test instance (as T ) with each reference answer (as H), which results in an array of numerical entailment confidence values. If there is more than one reference answer, we compute all pairwise confidence scores and add the minimum, maximum, average, an"
S13-2048,R11-1063,1,0.940093,"provide a robust architecture for student response analysis, that can generalize and perform well in multiple domains. Moreover, we are interested in evaluating how well general-purpose technologies will perform in this setting. We therefore approach the challenge by combining two such technologies: DKPro Similarity –an extensive suite of text similarity measures– that has been successfully applied in other settings like the SemEval 2012 task on semantic textual similarity (B¨ar et al., 2012a) or reuse detection (B¨ar et al., 2012b). BIUTEE, the Bar-Ilan University Textual Entailment Engine (Stern and Dagan, 2011), which has shown state-of-the-art performance on recognizing textual entailment challenges. Our systems use both technologies to extract features, and combine them in a supervised model. Indeed, this approach works relatively well (with respect to other entries in the challenge), especially in unseen domains. 2 2.1 Background Text Similarity Text similarity is a bidirectional, continuous function which operates on pairs of texts of any length and returns a numeric score of how similar one text is to the other. In previous work (Mihalcea et al., 285 Second Joint Conference on Lexical and Compu"
W14-1610,P12-3014,1,0.948998,"each directed edge reflects an entailment relation, in the spirit of textual entailment (Dagan et al., 2013). Entailment provides an effective structure for aggregating natural-language based information; it merges semantically equivalent propositions into cliques, and induces specification-generalization edges between them. For example, (aspirin, eliminate, headache) entails, and is more specific than, (headache, respond to, painkiller). We thus propose the task of constructing an entailment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailm"
W14-1610,P98-2127,0,0.360271,"Missing"
W14-1610,D12-1048,0,0.0160256,"les. Though this problem is simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffe"
W14-1610,J12-1003,1,0.938774,"ache, respond to, painkiller). We thus propose the task of constructing an entailment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailment. However, analyzing our data revealed that the lexical inferences captured in WordNet are quite difOpen IE methods extract structured propositions from text. However, these propositions are neither consolidated nor generalized, and querying them may lead to insufficient or redundant information. This work suggests an approach to organize open IE propositions using entailment graphs. The entailment relation unifies equi"
W14-1610,J90-1003,0,0.0898991,"uses only the argument entailment graph (as produced by Opt(Arg) ∧ Opt(P red)) to decide on proposition entailment; i.e. a pair of propositions entail if and only if their arguments entail. Opt(P red) is defined analogously. We used the entire database of 68 million extracted propositions (see Section 4) to create a word-context matrix; context was defined as other words that appeared in the same proposition, and each word was represented as (string, role), role being the location within the proposition, either a1 , p, or a2 . The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990). We used various metrics to measure different types of similarities between each component pair, including: cosine similarity, Lin’s similarity (1998), inclusion (Weeds and Weir, 2003), average precision, and balanced average precision (Kotlerman et al., 2010). Weed’s and Kotlerman’s metrics are directional (asymmetric) and indicate the direction of a potential entailment relation. These features were used for both predicates and arguments. In addition, we used Melamud et al’s (2013) method to learn a context-sensitive model of predicate entailment, which estimates predicate similarity in the"
W14-1610,N13-1018,0,0.0708682,"senseBackground Our work builds upon two major research threads: open IE, and entailment graphs. 88      pij π x log + log . The prior i6=j ij 1−pij 1−π term π is the probability of a random pair of predicates to be in an entailment relation, and can be estimated in advance. The ILP solver searches for the optimal assignment that maximizes the objective function under transitivity constraints, expressed as linear constraints ∀i,j,k xij + xjk − xik ≤ 1. P disambiguated nouns and their hyponymy relations. Berant et al (2012) constructed entailment graphs of predicate templates. Recently, Mehdad et al (2013) built an entailment graph of noun phrases and partial sentences for topic labeling. The notion of proposition entailment graphs, however, is novel. This distinction is critical, because apparently, entailment in the context of specific propositions does not behave like contextoblivious lexical entailment (see Section 8). Berant et al’s work was implemented in Adler et al’s (2012) text exploration demo, which instantiated manually-annotated predicate entailment graphs with arguments, and used an additional lexical resource to determine argument entailment. The combined graphs of predicate and"
W14-1610,P13-1131,1,0.874892,"Missing"
W14-1610,N13-1008,0,0.137916,"induced by proposition entailment. 1 Introduction Open information extraction (open IE) extracts natural language propositions from text without pre-defined schemas as in supervised relation extraction (Etzioni et al., 2008). These propositions represent predicate-argument structures as tuples of natural language strings. Open IE enables knowledge search by aggregating billions of propositions from the web1 . It may also be perceived as capturing an unsupervised knowledge representation schema, complementing supervised knowledge bases such as Freebase (Bollacker et al., 2008), as suggested by Riedel et al (2013). However, language variability obstructs open IE from becoming a viable knowledge representation framework. As it does not consolidate natural language expressions, querying a database of open IE propositions may lead to either insufficient or redundant information. As an illustrative example, 1 See demo: openie.cs.washington.edu 87 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 87–97, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics Figure 1: An excerpt from a proposition entailment graph focused on the topic heada"
W14-1610,D11-1142,0,0.00681112,"ment between language-based predicate-argument tuples. Though this problem is simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, c"
W14-1610,P06-1101,0,0.498542,"ilment graph over a set of open IE propositions (Section 3), which is closely related to Berant et al’s work (2012) who introduced predicate entailment graphs. In contrast, our work explores propositions, which are essentially predicates instantiated with arguments, and thus semantically richer. We provide a dataset of 30 such graphs, which represent 1.5 million pairwise entailment decisions between propositions (Section 4). To approach this task, we extend the state-ofthe-art method for building entailment graphs (Berant et al., 2012) from predicates to complete propositions. Both Snow et al (2006) and Berant et al used WordNet as distant supervision when training a local pairwise model of lexical entailment. However, analyzing our data revealed that the lexical inferences captured in WordNet are quite difOpen IE methods extract structured propositions from text. However, these propositions are neither consolidated nor generalized, and querying them may lead to insufficient or redundant information. This work suggests an approach to organize open IE propositions using entailment graphs. The entailment relation unifies equivalent propositions and induces a specific-to-general structure."
W14-1610,S13-1035,0,0.034538,"s, hypernyms, and (WordNet) entailments as positive examples, and antonyms, hyponyms, and cohyponyms as negative. The global optimization phase then searches for the most probable transitive entailment graph, given the local probability estimations. It does so with an integer linear program (ILP), where each pair of predicates is represented by a binary variable xij , denoting whether there is an entailment edge from i to j. The objective function corresponds to the log likelihood of the assignment: 4 Dataset To construct our dataset of open IE extractions, we found Google’s syntactic ngrams (Goldberg and Orwant, 2013) as a useful source of high-quality propositions. Based on a corpus of 3.5 million English books, it aggregates every syntactic ngram 89 – subtree of a dependency parse – with at most 4 dependency arcs. The resource contains only tree fragments that appeared at least 10 times in the corpus, filtering out many low-quality syntactic ngrams. We extracted the syntactic ngrams that reflect propositions, i.e. subject-verb-object fragments where object modifies the verb with either dobj or pobj. Prepositions in pobj were concatenated to the verb (e.g. use with). In addition, both subject and object m"
W14-1610,W03-1011,0,0.759979,"il. Opt(P red) is defined analogously. We used the entire database of 68 million extracted propositions (see Section 4) to create a word-context matrix; context was defined as other words that appeared in the same proposition, and each word was represented as (string, role), role being the location within the proposition, either a1 , p, or a2 . The matrix was then normalized with pointwise mutual information (Church and Hanks, 1990). We used various metrics to measure different types of similarities between each component pair, including: cosine similarity, Lin’s similarity (1998), inclusion (Weeds and Weir, 2003), average precision, and balanced average precision (Kotlerman et al., 2010). Weed’s and Kotlerman’s metrics are directional (asymmetric) and indicate the direction of a potential entailment relation. These features were used for both predicates and arguments. In addition, we used Melamud et al’s (2013) method to learn a context-sensitive model of predicate entailment, which estimates predicate similarity in the context of the given arguments. We leveraged the Unified Medical Language System (UMLS) to check argument entailment, using the parent and synonym relations. A single feature indicated"
W14-1610,N13-1107,0,0.00699842,"s simpler than sentence-level entailment, it does capture entailment of complete statements, which proves to be quite challenging indeed. 2 Open Information Extraction Research in open IE (Etzioni et al., 2008) has focused on transforming text to predicate-argument tuples (propositions). The general approach is to learn proposition extraction patterns, and use them to create tuples while denoting extraction confidence. Various methods differ in the type of patterns they acquire. For instance, (Banko et al., 2007) and (Fader et al., 2011) used surface patterns, while (Mausam et al., 2012) and (Xu et al., 2013) used syntactic dependencies. Yates and Etzioni (2009) tried to mitigate the issue of language variability (as exemplified in the introduction) by clustering synonymous predicates and arguments. While these clusters do contain semantically related items, they do not necessarily reflect equivalence or implication. For example, coffee, tea, and caffeine may all appear in one cluster, but coffee does not imply tea; on the other hand, separating any element from this cluster removes a valid implication. Entailment, however, can capture the fact that both beverages imply caffeine, but not one anoth"
W14-1610,C98-2122,0,\N,Missing
W14-1618,S12-1047,0,0.295943,"ons were created as described in Section 2. The neural embeddings were created using the word2vec software3 accompanying (Mikolov et al., 2013b). We embedded the vocabulary into a 600 dimensional space, using the state-of-the-art skip-gram architecture, the negative-training approach with 15 negative samples (NEG-15), and sub-sampling of frequent words with a parameter of 10−5 . The parameter settings follow (Mikolov et al., 2013b). 4.1 Closed Vocabulary The S EM E VAL dataset contains the collection of 79 semantic relations that appeared in SemEval 2012 Task 2: Measuring Relation Similarity (Jurgens et al., 2012). Each relation is exemplified by a few (usually 3) characteristic word-pairs. Given a set of several dozen target word pairs, which supposedly have the same relation, the task is to rank the target pairs according to the degree in which this relation holds. This can be cast as an analogy question in the following manner: For example, take the Recipient:Instrument relation with the prototypical word pairs king:crown and police:badge. To measure the degree that a target word pair wife:ring has the same relation, we form the two analogy questions “king is to crown as wife is to ring” and “police"
W14-1618,W13-3520,0,0.0621251,"omputer Science Department Bar-Ilan University Ramat-Gan, Israel {omerlevy,yoav.goldberg}@gmail.com Abstract word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexico:spanish” and the pas"
W14-1618,W09-0201,0,0.0290312,"Missing"
W14-1618,P14-2050,1,0.310233,"ach word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mean of 1595 and a median of 415. Another popular choice of context is the syntactic relations the word participates in (Lin, 1998; Pad´o and Lapata, 2007; Levy and Goldberg, 2014). In this paper, we chose the sequential context as it is compatible with the information available to the state-of-the-art neural embedding method we are comparing against. In this study, we show that similarly to the neural embedding space, the explicit vector space also encodes a vast amount of relational similarity which can be recovered in a similar fashion, suggesting the explicit vector space representation as a competitive baseline for further work on neural embeddings. Moreover, this result implies that the neural embedding process is not discovering novel patterns, but rather is doin"
W14-1618,J10-4006,0,0.0150611,"entations, but how crucial is it for a representation to be dense and low-dimensional at all? An alternative approach to representing words as vectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in di"
W14-1618,P14-1023,0,0.254993,"tal cities have embassies and meetups, while immigration is associated with countries). It is also interesting to observe how the relatively syntactic “superlativity” aspect is captured with many regional possessives (“america’s”, “asia’s”, “world’s”). 10 11 Discussion Mikolov et al. showed how an unsupervised neural network can represent words in a space that “naturally” encodes relational similarities in the form of vector offsets. This study shows that finding analogies through vector arithmetic is actually a form of balancing word similarities, and that, contrary to the recent findings of Baroni et al. (2014), under certain conditions traditional word similarities induced by explicit representations can perform just as well as neural embeddings on this task. Learning to represent words is a fascinating and important challenge with implications to most current NLP efforts, and neural embeddings in particular are a promising research direction. We believe that to improve these representations we should understand how they work, and hope that the methods and insights provided in this work will help to deepen our grasp of current and future investigations of word representations. Related Work Relation"
W14-1618,P98-2127,0,0.163639,"ectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). Th"
W14-1618,D12-1050,0,0.0117192,"s to the cosine similarity between the vectors. The superscript marks the position of the feature relative to the target word. sumes a similar setting but with two types of word similarities, and combines them with products and ratios (similar to 3C OS M UL) to recover a variety of semantic relations, including analogies. Arithmetic combination of explicit word vectors is extensively studied in the context of compositional semantics (Mitchell and Lapata, 2010), where a phrase composed of two or more words is represented by a single vector, computed by a function of its component word vectors. Blacoe and Lapata (2012) compare different arithmetic functions across multiple representations (including embeddings) on a range of compositionality benchmarks. To the best of our knowledge such methods of word vector arithmetic have not been explored for recovering relational similarities in explicit representations. features in the intersection. Table 7 presents the top (most influential) features of each aspect. Many of these features are names of people or places, which appear rarely in our corpus (e.g. Adeliza, a historical queen, and Nzinga, a royal family) but are nonetheless highly indicative of the shared c"
W14-1618,J90-1003,0,0.380721,"hibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions f"
W14-1618,N13-1090,0,0.727908,"d embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexico:spanish” and the pasttense relation in “capture:captured”, “go:went”. Remarkably, Mikolov et al. showed that such relations are ref"
W14-1618,P94-1038,0,0.235412,"rity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus range"
W14-1618,Q13-1029,0,0.0406897,"Missing"
W14-1618,N13-1120,0,0.0138842,"ikolov et al., 2013b). The queen ≈ king − man + woman The recovery of relational similarities using vector arithmetic on RNN-embedded vectors was evaluated on many relations, achieving state-of-the-art results in relational similarity identification tasks ∗ Supported by the European Community’s Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 287923 (EXCITEMENT). 171 Proceedings of the Eighteenth Conference on Computational Language Learning, pages 171–180, c Baltimore, Maryland USA, June 26-27 2014. 2014 Association for Computational Linguistics 2 (Mikolov et al., 2013c; Zhila et al., 2013). It was later demonstrated that relational similarities can be recovered in a similar fashion also from embeddings trained with different architectures (Mikolov et al., 2013a; Mikolov et al., 2013b). Explicit Vector Space Representation We adopt the traditional word representation used in the distributional similarity literature (Turney and Pantel, 2010). Each word is associated with a sparse vector capturing the contexts in which it occurs. We call this representation explicit, as each dimension corresponds to a particular context. For a vocabulary V and a set of contexts C, the result is a"
W14-1618,C94-1049,0,0.558866,"Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney, 2001). The PPMI variant dates back to at least (Niwa and Nitta, 1994), and was demonstrated to perform very well in Bullinaria and Levy (2007). In this work, we take the linear contexts in which words appear. We consider each word surrounding the target word w in a window of 2 to each side as a context, distinguishing between different sequential positions. For example, in the sentence a b c d e the contexts of the word c are a−2 , b−1 , d+1 and e+2 . Each vector’s dimenstion is thus |C |≈ 4 |V |. Empirically, the number of non-zero dimensions for vocabulary items in our corpus ranges between 3 (for some rare tokens) and 474,234 (for the word “and”), with a mea"
W14-1618,J07-2002,0,0.0126484,"Missing"
W14-1618,P93-1024,0,0.31969,"epresenting words as vectors is the distributional similarity representation, or bag of contexts. In this representation, each word is associated with a very highdimensional but sparse vector capturing the contexts in which the word occurs. We call such vector representations explicit, as each dimension directly corresponds to a particular context. These explicit vector-space representations have been extensively studied in the NLP literature (see (Turney and Pantel, 2010; Baroni and Lenci, 2010) and the references therein), and are known to exhibit a large extent of attributional similarity (Pereira et al., 1993; Lin, 1998; Lin and Pantel, 2001; Sahlgren, 2006; Kotlerman et al., 2010). Sij = P P M I(wi , cj ) ( P P M I(w, c) = P M I(w, c) = log 0 P M I(w, c) &lt; 0 P M I(w, c) otherwise P (w,c) P (w)P (c) = log f req(w,c)|corpus| f req(w)f req(c) where |corpus |is the number of items in the corpus, f req(w, c) is the number of times word w appeared in context c in the corpus, and f req(w), f req(c) are the corpus frequencies of the word and the context respectively. The use of PMI in distributional similarity models was introduced by Church and Hanks (1990) and widely adopted (Dagan et al., 1994; Turney"
W14-1618,D11-1014,0,0.0202825,"∗ and Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel {omerlevy,yoav.goldberg}@gmail.com Abstract word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexi"
W14-1618,P10-1040,0,0.405608,"e and Explicit Word Representations Omer Levy∗ and Yoav Goldberg Computer Science Department Bar-Ilan University Ramat-Gan, Israel {omerlevy,yoav.goldberg}@gmail.com Abstract word embeddings are designed to capture what Turney (2006) calls attributional similarities between vocabulary items: words that appear in similar contexts will be close to each other in the projected space. The effect is grouping of words that share semantic (“dog cat cow”, “eat devour”) or syntactic (“cars hats days”, “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language"
W14-1618,J06-3003,0,0.0985635,", “emptied carried danced”) properties, and are shown to be effective as features for various NLP tasks (Turian et al., 2010; Collobert et al., 2011; Socher et al., 2011; Al-Rfou et al., 2013). We refer to such word representations as neural embeddings or just embeddings. Recently, Mikolov et al. (2013c) demonstrated that the embeddings created by a recursive neural network (RNN) encode not only attributional similarities between words, but also similarities between pairs of words. Such similarities are referred to as linguistic regularities by Mikolov et al. and as relational similarities by Turney (2006). They capture, for example, the gender relation exhibited by the pairs “man:woman”, “king:queen”, the language-spoken-in relation in “france:french”, “mexico:spanish” and the pasttense relation in “capture:captured”, “go:went”. Remarkably, Mikolov et al. showed that such relations are reflected in vector offsets between word pairs (apples − apple ≈ cars − car), and that by using simple vector arithmetic one could apply the relation and solve analogy questions of the form “a is to a∗ as b is to —” in which the nature of the relation is hidden. Perhaps the most famous example is that the embedd"
W14-1618,C98-2122,0,\N,Missing
W14-4504,P12-3014,1,0.824939,"rom those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might not have considered a-priori. 4 Discussion In this position paper we outlined a framework for information discovery that leverages and extends Open IE, while ad"
W14-4504,W13-2322,0,0.0170123,"drawbacks. The proposed framework enriches Open IE by representing natural language in a traversable graph, composed of propositions and their semantic interrelations – A Propositional Knowledge Graph (PKG). The resulting structure provides a representation in two levels: locally, at sentence level, by representing the syntactic proposition structure embedded in a single sentence, and globally, at inter-proposition level, where relations are drawn between propositions from discourse, or from various sources. At the sentence level, PKG can be compared to Abstract Meaning Representation (AMR) (Banarescu et al., 2013), which maps a sentence onto a hierarchical structure of propositions (predicate-argument relations) - a “meaning representation”. AMR uses Propbank (Kingsbury and Palmer, 2003) for predicates’ meaning representation, where possible, and ungrounded natural language, where no respective 22 Propbank lexicon entry exists. While AMR relies on a deep semantic interpretation, our sentence level representation is more conservative (and thus, hopefully, more feasible) and can be obtained by syntactic interpretation. At inter-proposition level, PKG can be compared with traditional Knowledge Graphs (suc"
W14-4504,W08-1301,0,0.0604435,"Missing"
W14-4504,S14-1010,0,0.0197595,"rmation from multiple documents on the same topic. PKGs can be a natural platform leveraged by summarization because: (1) they would contain the information from those documents as fine-grained propositions (2) they represent the semantic relations between those propositions. These semantic relations can be leveraged to create high-quality summarizations. For example, the paraphrase (mutual entailment) relation prevents redundancy. Links of a temporal or causal nature can also dictate the order in which each proposition is presented. A recent method of summarizing text with entailment graphs (Gupta et al., 2014) demonstrates the appeal and feasibility of this application. Faceted Search Faceted search allows a user to interactively navigate a PKG. Adler et al. (2012) demonstrate this concept on a limited proposition graph. When searching for “headache” in their demo, the user can drill-down to find possible causes or remedies, and even focus on subcategories of those; for example, finding the foods which relieve headaches. As opposed to the structured query application, retrieval is not fully automated, but rather interactive. It thus allows users to explore and discover new information they might no"
W14-4504,W14-1610,1,0.832308,"texts. However, in a PKG, they are marked as paraphrases (mutually entailing), and both entail an additional proposition from a third source: “Curiosity is a lab”. If one were to query all the 21 propositions that entail “Curiosity is a lab” – e.g. in response to the query “What is Curiosity?” – all three propositions would be retrieved, even though their surface forms may have “functions as” instead of “is” or “laboratory” instead of “lab”. We have recently taken some first steps in this direction, investigating algorithms for constructing entailment edges over sets of related propositions (Levy et al., 2014). Even between simple propositions, recognizing entailment is challenging. We are currently working on new methods that will leverage structured and unstructured data to recognize entailment for Open IE propositions. There are additional relations, besides entailment, that should desirably be represented in PKGs as well. Two such examples are temporal relations (depicted in Figure 1) and causality. Investigating and adapting methods for recognizing and utilizing these relations is intended for future work. 3 Applications An appealing application of knowledge graphs is question answering (QA)."
W14-4504,D12-1048,0,0.0391103,"Open IE systems retrieve only propositions in which both predicates and arguments are instantiated in succession in the surface form. For such propositions, these systems produce independent tuples (typically a (subject, verb, object) triplet) consisting of a predicate and a list of its arguments, all expressed in natural language, in the same way they originally appeared in the sentence. This methodology lacks the ability to represent cases in which propositions are inherently embedded, such as conditionals and propositional arguments (e.g. “Senator Kennedy asked congress to pass the bill”). Mausam et al. (2012) introduced a context analysis layer, extending this 20 representation with an additional field per tuple, which intends to represent the factuality of the extraction, accounting specifically for cases of conditionals and attribution. For instance, the assertion “If he wins five key states, Romney will be elected President” will be represented as ((Romney; will be elected; President) ClausalModifier if; he wins five key states). While these methods capture some of the propositions conveyed by text, they fail to retrieve other propositions expressed by more sophisticated syntactic constructs. C"
W14-4504,W14-2413,1,0.823008,"te (namely “acquired”) does not appear in the surface form. Implicit propositions might be introduced in many other linguistic constructs, such as: appositions (“The company, Random House, doesn’t report its earnings.” implies that Random House is a company), adjectives (“Tall John walked home” implies that John is tall), and possessives (“John’s book is on the table” implies that John has a book). We intend to syntactically identify these implicit propositions, and make them explicit in our representation. For further analysis of syntax-driven proposition representation, see our recent work (Stanovsky et al., 2014). We believe that this extension of Open IE representation is feasibly extractable from syntactic parse trees, and are currently working on automatic conversion from Stanford dependencies (de Marneffe and Manning, 2008) to interconnected propositions as described. 2.2 Consolidating Information across Propositions While Open IE is indeed much more scalable than supervised approaches, it does not consolidate natural language expressions, which leads to either insufficient or redundant information when accessing a repository of Open IE extractions. As an illustrating example, querying the Univers"
W15-1501,D14-1082,0,0.00614467,"vides the same kind of data as LS-SE, but instead of choosing specific target words that tend to be ambiguous as done in LS-SE, the target words here are all the content words in text documents extracted from news and fiction corpora, and are therefore more naturally distributed. LS-CIC is also much larger than LS-SE with over 15K target word instances. 4 4.2 Compared methods We used ukWaC (Ferraresi et al., 2008), a two billion word web corpus, as our learning corpus. We parsed both ukWaC and the sentences in the lexical substitution datasets with Stanford’s Neural Network Dependency Parser (Chen and Manning, 2014).4 Following Levy and Goldberg (2014a), we learned syntax-based skip-gram word and context embeddings using word2vecf (with 600 dimensions and 15 negative sampling), converting all tokens to lowercase, discarding words and syntactic contexts that appear less than 100 times in the corpus and ‘collapsing’ dependencies that include prepositions. This resulted in a vocabulary of about 200K word embeddings and 1M context embeddings. 5 Finally, for every instance in the lexical substitution datasets, we extracted the syntactic contexts of the target word and used each of our measures, Add, BalAdd, M"
W15-1501,P12-1092,0,0.0783026,"model today is skip-gram, introduced in Mikolov et al. (2013) and available as part of the word2vec toolkit.1 word2vec learns for every word type two distinct representations, one as a target and another as a context, both embedded in the same space. However, the context representations are considered internal to the model and are discarded after training. The output word embeddings represent context-insensitive target word types. Few recent models extended word embeddings by learning a distinct representation for each sense of a target word type, as induced by clustering the word’s contexts (Huang et al., 2012; Neelakantan et al., 2014). They then identify the relevant sense(s) for a given word instance, in order to measure contextsensitive similarities. Although these models may be considered for lexical substitution, they have so far been applied only to ‘softer’ word similarity tasks which include topical relations. In this work we propose a simple approach for directly utilizing the skip-gram model for contextsensitive lexical substitution. Instead of discarding the learned context embeddings, we use them in conjunction with the target word embeddings to model target word instances. A suitable"
W15-1501,E14-1057,0,0.341915,"Missing"
W15-1501,P14-2050,1,0.499272,"he same low-dimensional space. In this space, the vector representations of a target and context are pushed closer together the more frequently they co-occur in a learning corpus. Thus, the Cosine distance between them can be viewed as a first-order target-to-context similarity measure, indicative of their syntagmatic compatibility. Indirectly, this also results in assigning similar vector representations to target words that share similar contexts, thereby suggesting the Cosine distance between word embeddings as a second-order target-to-target distributional similarity measure. word2vecf 3 (Levy and Goldberg, 2014a) is an extension of the skip-gram implementation in word2vec, which supports arbitrary types of contexts rather than only word window contexts. Levy and Goldberg (2014a) used word2vecf to produce syntax-based word embeddings, where context elements are the syntactic contexts of the target words. Specifically, for a target word t with modifiers m1 ,...,mk and head h, they considered the context elements (m1 , r1 ),...,(mk , rk ),(h, rh−1 ), where r is the type of the (‘collapsed’) dependency relation between the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation."
W15-1501,W14-1618,1,0.298193,"he same low-dimensional space. In this space, the vector representations of a target and context are pushed closer together the more frequently they co-occur in a learning corpus. Thus, the Cosine distance between them can be viewed as a first-order target-to-context similarity measure, indicative of their syntagmatic compatibility. Indirectly, this also results in assigning similar vector representations to target words that share similar contexts, thereby suggesting the Cosine distance between word embeddings as a second-order target-to-target distributional similarity measure. word2vecf 3 (Levy and Goldberg, 2014a) is an extension of the skip-gram implementation in word2vec, which supports arbitrary types of contexts rather than only word window contexts. Levy and Goldberg (2014a) used word2vecf to produce syntax-based word embeddings, where context elements are the syntactic contexts of the target words. Specifically, for a target word t with modifiers m1 ,...,mk and head h, they considered the context elements (m1 , r1 ),...,(mk , rk ),(h, rh−1 ), where r is the type of the (‘collapsed’) dependency relation between the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation."
W15-1501,Q15-1016,1,0.135652,"tween the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation. Similarly to traditional syntax-based vector space models (Pad´o and Lapata, 2007), they show that these embeddings tend to capture functional word similarity (as in manage ∼ supervise) rather than topi2 While in this work we focus on skip-gram embeddings, we note that there are also other potentially relevant word embedding methods that can generate context representations in addition to the ‘standard’ target word representations. See, for example, GloVe (Pennington et al., 2014) and SVD-based methods (Levy et al., 2015). 3 https://bitbucket.org/yoavgo/word2vecf 2 Figure 1: A 2-dimensional visualization of the gerunds singing, dancing, driving, and healing with their top syntactic contexts in an embedded space. singing and dancing share many similar contexts (e.g. partmod song and dobj jive) and therefore end up with very similar vector representations. cal similarity or relatedness (as in manage ∼ manager). Figure 1 illustrates a syntax-based embedding space using t-SNE (Van der Maaten and Hinton, 2008), which visualizes the similarities in the original higher-dimensional space. 3 Lexical Substitution Model"
W15-1501,S07-1009,0,0.870273,"sed on the popular skip-gram word embedding model. The novelty of our approach is in leveraging explicitly the context embeddings generated within the skip-gram model, which were so far considered only as an internal component of the learning process. Our model is efficient, very simple to implement, and at the same time achieves state-ofthe-art results on lexical substitution tasks in an unsupervised setting. 1 Introduction Lexical substitution tasks have become very popular for evaluating context-sensitive lexical inference models since the introduction of the original task in SemEval-2007 (McCarthy and Navigli, 2007) and additional later variants (Biemann, 2013; Kremer et al., 2014). In these tasks, systems are required to predict substitutes for a target word instance, which preserve its meaning in a given sentential context. Recent models addressed this challenge mostly in an unsupervised setting. They typically generated a word instance representation, which is biased towards its given context, and then identified substitute words based on their similarity to this biased representation. Various types of models were proposed, from sparse syntax-based vector models (Thater et al., 2011), to probabilistic"
W15-1501,D14-1113,0,0.0528929,"gram, introduced in Mikolov et al. (2013) and available as part of the word2vec toolkit.1 word2vec learns for every word type two distinct representations, one as a target and another as a context, both embedded in the same space. However, the context representations are considered internal to the model and are discarded after training. The output word embeddings represent context-insensitive target word types. Few recent models extended word embeddings by learning a distinct representation for each sense of a target word type, as induced by clustering the word’s contexts (Huang et al., 2012; Neelakantan et al., 2014). They then identify the relevant sense(s) for a given word instance, in order to measure contextsensitive similarities. Although these models may be considered for lexical substitution, they have so far been applied only to ‘softer’ word similarity tasks which include topical relations. In this work we propose a simple approach for directly utilizing the skip-gram model for contextsensitive lexical substitution. Instead of discarding the learned context embeddings, we use them in conjunction with the target word embeddings to model target word instances. A suitable substitute for a 1 https://"
W15-1501,J14-3005,0,0.0826541,"Missing"
W15-1501,J07-2002,0,0.0235971,"Missing"
W15-1501,D14-1162,0,0.114645,"type of the (‘collapsed’) dependency relation between the head and the modifier (e.g. dobj, prep of ) and r−1 denotes an inverse relation. Similarly to traditional syntax-based vector space models (Pad´o and Lapata, 2007), they show that these embeddings tend to capture functional word similarity (as in manage ∼ supervise) rather than topi2 While in this work we focus on skip-gram embeddings, we note that there are also other potentially relevant word embedding methods that can generate context representations in addition to the ‘standard’ target word representations. See, for example, GloVe (Pennington et al., 2014) and SVD-based methods (Levy et al., 2015). 3 https://bitbucket.org/yoavgo/word2vecf 2 Figure 1: A 2-dimensional visualization of the gerunds singing, dancing, driving, and healing with their top syntactic contexts in an embedded space. singing and dancing share many similar contexts (e.g. partmod song and dobj jive) and therefore end up with very similar vector representations. cal similarity or relatedness (as in manage ∼ manager). Figure 1 illustrates a syntax-based embedding space using t-SNE (Van der Maaten and Hinton, 2008), which visualizes the similarities in the original higher-dimens"
W15-1501,D13-1198,0,0.19746,"Missing"
W15-1501,I11-1127,0,0.0390161,"Missing"
W18-3024,P17-1080,0,0.0468882,"t LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016b; Adi et al., 2017; Belinkov et al., 2017, among others). Validation Test 0 10 20 30 # Epochs 40 50 Figure 4: Model validation and test accuracy over time during training. Validation improves faster than test, indicating that the model exploits linguistic properties of the data during training. Figure 4 shows that models trained on the unigram and language datasets converge to high validation accuracy faster than high test accuracy. This suggests that models trained on data with linguistic attributes first learn to do well on the training data by exploiting the properties of language and not truly memorizing. Perhaps the model genera"
W18-3024,D16-1248,0,0.206153,"ical structure of language 1 This distribution is adversarial with respect to the Zipfian and natural language training sets. 180 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 180–186 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics paths for the model to minimize its loss, thus offering more training signal than the uniform case, in which the only way to reduce the loss is to learn the memorization function. We further inspect how the LSTM solves the memorization task, and find that some hidden units count the number of inputs. Shi et al. (2016a) analyzed LSTM encoder-decoder translation models and found that similar counting neurons regulate the length of generated translations. Since LSTMs better memorize (and thus better count) on language data than on non-language data, and counting plays a role in encoder-decoder models, our work could also lead to improved training for sequence-to-sequence models in non-language applications (e.g., Schwaller et al., 2017). vocabulary. Our goal is to evaluate the memorization ability of the LSTM, so we freeze the weights of the embedding matrix and the linear output projection during training."
W18-3024,D16-1159,0,0.258984,"ical structure of language 1 This distribution is adversarial with respect to the Zipfian and natural language training sets. 180 Proceedings of the 3rd Workshop on Representation Learning for NLP, pages 180–186 c Melbourne, Australia, July 20, 2018. 2018 Association for Computational Linguistics paths for the model to minimize its loss, thus offering more training signal than the uniform case, in which the only way to reduce the loss is to learn the memorization function. We further inspect how the LSTM solves the memorization task, and find that some hidden units count the number of inputs. Shi et al. (2016a) analyzed LSTM encoder-decoder translation models and found that similar counting neurons regulate the length of generated translations. Since LSTMs better memorize (and thus better count) on language data than on non-language data, and counting plays a role in encoder-decoder models, our work could also lead to improved training for sequence-to-sequence models in non-language applications (e.g., Schwaller et al., 2017). vocabulary. Our goal is to evaluate the memorization ability of the LSTM, so we freeze the weights of the embedding matrix and the linear output projection during training."
W18-3024,P18-2117,0,0.0961116,"counting frequent words or phrases. In the uniform setting, the model has only one path to success: true memorization, and it cannot find an effective way to reduce the loss. In other words, linguistic structure and the patterns of language may provide additional signals that correlate with the label and facilitate learning the memorization task. 183 Accuracy 100 90 80 70 60 50 40 30 20 10 0 ture of language. Blevins et al. (2018) show that the internal representations of LSTMs encode syntactic information, even when trained without explicit syntactic supervision. Also related is the work of Weiss et al. (2018), who demonstrate that LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016"
W18-3024,N18-1108,0,0.051157,"Missing"
W18-3024,P82-1020,0,0.756047,"Missing"
W18-3024,N16-1082,0,0.0475318,"t the internal representations of LSTMs encode syntactic information, even when trained without explicit syntactic supervision. Also related is the work of Weiss et al. (2018), who demonstrate that LSTMs are able to count infinitely, since their cell states are unbounded, while GRUs cannot count infinitely since the activations are constrained to a finite range. One avenue of future work could compare the performance of LSTMs and GRUs on the memorization task. Past studies have also investigated what information RNNs encode by directly examining hidden unit activations (Karpathy et al., 2016; Li et al., 2016; Shi et al., 2016a, among others) and by training an auxiliary classifier to predict various properties of interest from hidden state vectors (Shi et al., 2016b; Adi et al., 2017; Belinkov et al., 2017, among others). Validation Test 0 10 20 30 # Epochs 40 50 Figure 4: Model validation and test accuracy over time during training. Validation improves faster than test, indicating that the model exploits linguistic properties of the data during training. Figure 4 shows that models trained on the unigram and language datasets converge to high validation accuracy faster than high test accuracy. Th"
W18-3024,Q16-1037,0,0.0939639,"Missing"
W18-3024,J93-2004,0,0.0609389,"set the forget gate to 0 and the input gate to 1).3 All input sequences at train and test time are of equal length. To explore the effect of sequence length on LSTM task performance, we experiment with different input sequence lengths (10, 20, 40, 60, . . . , 300). 3 • In the uniform setup, each token in the training dataset is randomly sampled from a uniform distribution over the vocabulary. • In the unigram setup, we modify the uniform data by integrating the Zipfian token frequencies found in natural language data. The input sequences are taken from a modified version of the Penn Treebank (Marcus et al., 1993) with randomly permuted tokens. • In the 5gram, 10gram, and 50gram settings, we seek to augment the unigram setting with Markovian dependencies. We generate the dataset by grouping the tokens of the Penn Treebank into 5, 10, or 50-length chunks and randomly permuting these chunks. Experimental Setup We modify the linguistic properties of the training data and observe the effects on model performance. Further details are found in Appendix A, and we release code for reproducing our results.4 • In the language setup, we assess the effect of using real language. The input sequences here are taken"
W18-5446,N18-1202,0,0.363701,"fic linguistic phenomena, and an online platform for evaluating and comparing models. For some benchmark tasks, training data is plentiful, but for others it is limited or does not match the genre of the test set. GLUE thus favors models that can represent linguistic knowledge in a way that facilitates sample-efficient learning and effective knowledge-transfer across tasks. While none of the datasets in GLUE were created from scratch for the benchmark, four of them feature privately-held test data, which is used to ensure that the benchmark is used fairly. We evaluate baselines that use ELMo (Peters et al., 2018), a powerful transfer learning technique, as well as state-of-the-art sentence representation models. The best models still achieve fairly low absolute scores. Analysis with our diagnostic dataset yields similarly weak performance over all phenomena tested, with some exceptions. Corpus |Train| Task Domain Single-Sentence Tasks CoLA SST-2 8.5k 67k acceptability sentiment misc. movie reviews Similarity and Paraphrase Tasks MRPC STS-B QQP 3.7k 7k 364k MNLI QNLI RTE WNLI 393k 108k 2.5k 634 paraphrase textual sim. paraphrase news misc. online QA Inference Tasks NLI QA/NLI NLI coref./NLI misc. Wikip"
W18-5446,S17-2001,0,0.0943845,"ough it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we expect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should re1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 353 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for"
W18-5446,D16-1264,0,0.443898,"65.1 62.3 65.1 65.1 65.1 65.1 Table 2: Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation (Matthews, 1975). For all other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the online platform. nizing Textual Entailment (RTE; aggregated from Dagan et al. 2006, Bar Haim et al. 2006, Giampiccolo et al. 2007, Bentivogli et al. 2009), as well as versions of SQuAD (Rajpurkar et al., 2016) and Winograd Schema Challenge (Levesque et al., 2011) recast as NLI (resp. QNLI, WNLI). Table 1 summarizes the tasks. Performance on the benchmark is measured per task as well as in aggregate, averaging performance across tasks. Tags Sentence Pair Quantifiers Double Negation I have never seen a hummingbird not flying. I have never seen a hummingbird. Active/Passive Named Entities World Knowledge Diagnostic Dataset To understand the types of knowledge learned by models, GLUE also includes a dataset of hand-crafted examples for probing trained models. This dataset is designed to highlight commo"
W18-5446,D17-1070,0,0.111288,"Missing"
W18-5446,D13-1170,0,0.166913,"l tasks are binary classification, except STS-B (regression) and MNLI (three classes). quire models to share substantial knowledge (e.g., trained parameters) across tasks, while maintaining some task-specific components. Though it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we expect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we"
W18-5446,I05-5002,0,0.264722,"tial knowledge (e.g., trained parameters) across tasks, while maintaining some task-specific components. Though it is possible to train a model per task and evaluate the resulting set of models on this benchmark, we expect that inclusion of several data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should re1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 353 P"
W18-5446,W07-1401,0,0.301573,"71.3 61.1 81.7 75.1 74.7 79.8 75.2 82.3 50.4 61.2 54.1 53.1 58.0 56.4 59.2 65.1 65.1 62.3 65.1 65.1 65.1 65.1 Table 2: Baseline performance on the GLUE tasks. For MNLI, we report accuracy on the matched and mismatched test sets. For MRPC and QQP, we report accuracy and F1. For STS-B, we report Pearson and Spearman correlation. For CoLA, we report Matthews correlation (Matthews, 1975). For all other tasks we report accuracy. All values are scaled by 100. A similar table is presented on the online platform. nizing Textual Entailment (RTE; aggregated from Dagan et al. 2006, Bar Haim et al. 2006, Giampiccolo et al. 2007, Bentivogli et al. 2009), as well as versions of SQuAD (Rajpurkar et al., 2016) and Winograd Schema Challenge (Levesque et al., 2011) recast as NLI (resp. QNLI, WNLI). Table 1 summarizes the tasks. Performance on the benchmark is measured per task as well as in aggregate, averaging performance across tasks. Tags Sentence Pair Quantifiers Double Negation I have never seen a hummingbird not flying. I have never seen a hummingbird. Active/Passive Named Entities World Knowledge Diagnostic Dataset To understand the types of knowledge learned by models, GLUE also includes a dataset of hand-crafted"
W18-5446,N18-1101,1,0.32004,"data-scarce tasks will ultimately render this approach uncompetitive. The nine tasks include two tasks with singlesentence inputs: Corpus of Linguistic Acceptability (CoLA; Warstadt et al. 2018) and Stanford Sentiment Treebank (SST-2; Socher et al. 2013) Three tasks involve detecting semantic similarity: Microsoft Research Paraphrase Corpus (MRPC, (Dolan and Brockett, 2005)), Quora Question Pairs1 (QQP), and Semantic Textual Similarity Benchmark (STS-B; Cer et al. 2017). The remaining four tasks are formatted as natural language inference (NLI) tasks, such as the Multi-Genre NLI corpus (MNLI; Williams et al. 2018) and RecogThe GLUE benchmark GLUE consists of nine English sentence understanding tasks covering a broad range of domains, data quantities, and difficulties. As the goal of GLUE is to spur development of generalizable NLU systems, we design the benchmark such that good performance should re1 data.quora.com/First-Quora-DatasetRelease-Question-Pairs 353 Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353–355 c Brussels, Belgium, November 1, 2018. 2018 Association for Computational Linguistics Model Avg Single-task Multi-task CBoW Skip"
W18-5446,N18-2017,1,0.716956,"et of hand-crafted examples for probing trained models. This dataset is designed to highlight common phenomena, such as the use of world knowledge, logical operators, and lexical entailments, that models must grasp if they are to robustly solve the tasks. Each of the 550 examples is an NLI sentence pair tagged with the phenomena demonstrated. We ensure that the data is reasonably diverse by producing examples for a wide variety of linguistic phenomena, and basing our examples on naturally-occurring sentences from several domains. We validate our data by using the hypothesis-only baseline from Gururangan et al. (2018) and having six NLP researchers manually validate a random sample of the data. Cape sparrows eat seeds, along with soft plant parts and insects. Cape sparrows are eaten. Musk decided to offer up his personal Tesla roadster. Musk decided to offer up his personal car. Table 3: Diagnostic set examples. Systems must predict the relationship between the sentences, either entailment, neutral, or contradiction when one sentence is the premise and the other is the hypothesis, and vice versa. Examples are tagged with the phenomena demonstrated. We group each phenomena into one of four broad categories."
W18-5446,D15-1075,1,\N,Missing
W18-5446,P04-1035,0,\N,Missing
W18-5446,P05-1015,0,\N,Missing
W18-5446,D14-1162,0,\N,Missing
W18-5446,P16-2038,0,\N,Missing
W18-5446,D17-1206,0,\N,Missing
W18-5446,W17-5307,0,\N,Missing
W18-5446,I17-1100,0,\N,Missing
W18-5446,W18-2501,0,\N,Missing
W18-5446,W17-1609,0,\N,Missing
W19-4828,P18-1027,1,0.822387,"nding from an analysis perspective even if that head is not always used when making predictions for some downstream task. chine translation. One possibility for the apparent redundancy in BERT’s attention heads is the use of attention dropout, which causes some attention weights to be zeroed-out during training. 7 Related Work There has been substantial recent work performing analysis to better understand what neural networks learn, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-tra"
W19-4828,W11-1902,0,0.0950967,"Missing"
W19-4828,P17-1080,0,0.0329332,"rained language models achieve very high accuracy when fine-tuned on supervised tasks (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018), but it is not fully understood why. The strong results suggest pre-training teaches the models about the structure of language, but what specific linguistic features do they learn? Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences (Linzen et al., 2016) or examining the internal vector representations of the model through methods such as probing classifiers (Adi et al., 2017; Belinkov et al., 2017). Complementary to these approaches, we 1 Code will be released at https://github.com/ clarkkev/attention-analysis. 276 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 276–286 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Head 1-1 Attends broadly Head 3-1 Attends to next token Head 8-7 Attends to [SEP] Head 11-6 Attends to periods Figure 1: Examples of heads exhibiting the patterns discussed in Section 3. The darkness of a line indicates the strength of the attention weight (some attention weights a"
W19-4828,Q16-1037,0,0.273616,"ng classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention. 1 Introduction Large pre-trained language models achieve very high accuracy when fine-tuned on supervised tasks (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018), but it is not fully understood why. The strong results suggest pre-training teaches the models about the structure of language, but what specific linguistic features do they learn? Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences (Linzen et al., 2016) or examining the internal vector representations of the model through methods such as probing classifiers (Adi et al., 2017; Belinkov et al., 2017). Complementary to these approaches, we 1 Code will be released at https://github.com/ clarkkev/attention-analysis. 276 Proceedings of the Second BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP, pages 276–286 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Head 1-1 Attends broadly Head 3-1 Attends to next token Head 8-7 Attends to [SEP] Head 11-6 Attends to periods Figure 1: Examples of h"
W19-4828,P18-2003,1,0.855456,"di et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and"
W19-4828,N19-1112,0,0.0443214,"ble 3: Results of attention-based probing tasks on dependency parsing. A simple model taking BERT attention maps and GloVe word embeddings as input performs quite well at dependency parsing. *Not directly comparable to our numbers; see text. Overall, our results from probing both individual and combinations of attention heads suggest that BERT learns some aspects syntax purely as a by-product of self-supervised training. Other work has drawn a similar conclusions from examining BERT’s predictions on agreement tasks (Goldberg, 2019) or internal vector representations (Hewitt and Manning, 2019; Liu et al., 2019). Traditionally, syntax-aware models have been developed through architecture design (e.g., recursive neural networks) or from direct supervision from human-curated treebanks. Our findings are part of a growing body of work indicating that indirect supervision from rich pre-training tasks like language modeling can also produce models sensitive to language’s hierarchical structure. 6 Clustering Attention Heads Figure 6: BERT attention heads embedded in twodimensional space. Distance between points approximately matches the average Jensen-Shannon divergences between the outputs of the correspon"
W19-4828,W18-5454,0,0.0260915,"achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and Tiedemann (2018) evaluate the attention heads of a machine translation model on dependency parsing, but only report overall UAS scores instead of investigating heads for specific syntactic relations or using probing classifiers. Marecek and Rosa (2018) propose heuristic ways 8 Conclusion We have proposed a series of an"
W19-4828,J93-2004,0,0.0655926,"Missing"
W19-4828,W18-5444,0,0.0530079,"f the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and Tiedemann (2018) evaluate the attention heads of a machine translation model on dependency parsing, but only report overall UAS scores instead of investigating heads for specific syntactic relations or using probing classifiers. Marecek and Rosa (2018) propose heuristic ways 8 Conclusion We have proposed a series of analysis methods for understanding the attention mechanisms of models and applied them to BERT. While most recent work on model analysis for NLP has focused on probing vector representations or model outputs, we have shown that a substantial amount of linguistic knowledge can be found not only in the hidden states, but also in the attention maps. We think probing attention maps complements these other model analysis techniques, and should be part of the toolkit used by researchers to understand what neural networks learn about l"
W19-4828,P18-1198,0,0.0428708,"ence indicating how likely each other word is to be the syntactic head of the current one. • A right-branching baseline that always predicts the head is to the dependent’s right. • A simple one-hidden-layer network that takes as input the GloVe embeddings for the dependent and candidate head as well as a set of features indicating the distances between the two words.4 • Our attention-and-words probe, but with attention maps from a BERT network with pretrained word/positional embeddings but randomly initialized other weights. This kind of baseline is surprisingly strong at other probing tasks (Conneau et al., 2018). Attention-Only Probe. Our first probe learns a simple linear combination of attention weights. p(i|j) ∝ exp X n k=1 k wk αij + k uk αji   Results are shown in Table 3. We find the Attn + GloVe probing classifier substantially outperforms our baselines and achieves a decent UAS of 77, suggesting BERT’s attention maps have a fairly thorough representation of English syntax. As a rough comparison, we also report results from the structural probe from Hewitt and Manning (2019), which builds a probing classifier on top of BERT’s vector representations rather than attention. The scores are not"
W19-4828,D18-1151,0,0.036082,"ead is not always used when making predictions for some downstream task. chine translation. One possibility for the apparent redundancy in BERT’s attention heads is the use of attention dropout, which causes some attention weights to be zeroed-out during training. 7 Related Work There has been substantial recent work performing analysis to better understand what neural networks learn, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supe"
W19-4828,A94-1016,0,0.0236585,"Missing"
W19-4828,N19-1423,0,0.379541,"BERT’s Attention Kevin Clark† Urvashi Khandelwal† Omer Levy‡ Christopher D. Manning† † Computer Science Department, Stanford University ‡ Facebook AI Research {kevclark,urvashik,manning}@cs.stanford.edu omerlevy@fb.com study1 the attention maps of a pre-trained model. Attention (Bahdanau et al., 2015) has been a highly successful neural network component. It is naturally interpretable because an attention weight has a clear meaning: how much a particular word will be weighted when computing the next representation for the current word. Our analysis focuses on the 144 attention heads in BERT (Devlin et al., 2019), a large pre-trained Transformer (Vaswani et al., 2017) network that has demonstrated excellent performance on many tasks. We first explore generally how the attention heads behave. We find that there are common patterns in their behavior, such as attending to fixed positional offsets or attending broadly over the whole sentence. A surprisingly large amount of BERT’s attention focuses on the deliminator token [SEP], which we argue is used by the model as a sort of no-op. Generally we find that attention heads in the same layer tend to behave similarly. We next probe each attention head for li"
W19-4828,D14-1162,1,0.113256,"j’s syntactic head, αij from word i to word j produced by head k, and n is the number of attention heads. We include both directions of attention: candidate head to dependent as well as dependent to candidate head. The weight vectors w and u are trained using standard supervised learning on the train set. Attention-and-Words Probe. Given our finding that heads specialize to particular syntactic relations, we believe probing classifiers should benefit from having information about the input words. In particular, we build a model that sets the weights of the attention heads based on the GloVe (Pennington et al., 2014) embeddings for the input words. Intuitively, if the dependent and candidate head are “the” and “cat,” the probing classifier should learn to assign most of the weight to the head 8-11, which achieves excellent performance at the determiner relation. The attentionand-words probing classifier assigns the probabil4 indicator features for short distances as well as continuous distance features, with distance ahead/behind treated separately to capture word order 282 Model Right-branching Distances + GloVe Random Init Attn + GloVe Attn Attn + GloVe Structural probe (Hewitt and Manning, 2019) UAS 26"
W19-4828,P16-1078,0,0.0261099,"sed in Section 3. The darkness of a line indicates the strength of the attention weight (some attention weights are so low they are invisible). Our findings show that particular heads specialize to specific aspects of syntax. To get a more overall measure of the attention heads’ syntactic ability, we propose an attention-based probing classifier that takes attention maps as input. The classifier achieves 77 UAS at dependency parsing, showing BERT’s attention captures a substantial amount about syntax. Several recent works have proposed incorporating syntactic information to improve attention (Eriguchi et al., 2016; Chen et al., 2018; Strubell et al., 2018). Our work suggests that to an extent this kind of syntax-aware attention already exists in BERT, which may be one of the reason for its success. 2 Attention weights can be viewed as governing how “important” every other token is when producing the next representation for the current token. BERT is pre-trained on 3.3 billion tokens of unlabeled text to perform two tasks. In the “masked language modeling” task, the model predicts the identities of words that have been masked-out of the input text. In the “next sentence prediction” task, the model predi"
W19-4828,N18-1202,0,0.117725,"ing similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT’s attention. 1 Introduction Large pre-trained language models achieve very high accuracy when fine-tuned on supervised tasks (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018), but it is not fully understood why. The strong results suggest pre-training teaches the models about the structure of language, but what specific linguistic features do they learn? Recent work has investigated this question by examining the outputs of language models on carefully chosen input sentences (Linzen et al., 2016) or examining the internal vector representations of the model through methods such as probing classifiers (Adi et al., 2017; Belinkov et al., 2017). Complementary to these approaches, we 1 Code will be released at https://github.com/ clarkkev/attent"
W19-4828,W18-5426,0,0.0184434,"n, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Ten"
W19-4828,W12-4501,0,0.0425951,".5 (1) poss auxpass ccomp mark prt 7-6 4-10 8-1 8-2 6-7 80.5 82.5 48.8 50.7 99.1 47.7 (1) 40.5 (1) 12.4 (-2) 14.5 (2) 91.4 (-1) 4.3 Accuracy Baseline Coreference Resolution Having shown BERT attention heads reflect certain aspects of syntax, we now explore using attention heads for the more challenging semantic task of coreference resolution. Coreference links are usually longer than syntactic dependencies and state-of-the-art systems generally perform much worse at coreference compared to parsing. Setup. We evaluate the attention heads on coreference resolution using the CoNLL-2012 dataset3 (Pradhan et al., 2012). In particular, we compute antecedent selection accuracy: what percent of the time does the head word of a coreferent mention most attend to the head of one of that mention’s antecedents. We compare against three baselines for selecting an antecedent: • Picking the nearest other mention. Table 1: The best performing attentions heads of BERT on WSJ dependency parsing by dependency type. Numbers after baseline accuracies show the best offset found (e.g., (1) means the word to the right is predicted as the head). We show the 10 most common relations as well as 5 other ones attention heads did we"
W19-4828,N18-1108,0,0.0266004,"rspective even if that head is not always used when making predictions for some downstream task. chine translation. One possibility for the apparent redundancy in BERT’s attention heads is the use of attention dropout, which causes some attention weights to be zeroed-out during training. 7 Related Work There has been substantial recent work performing analysis to better understand what neural networks learn, especially from language model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. The"
W19-4828,W18-5431,0,0.0398025,"t al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention with syntax. Raganato and Tiedemann (2018) evaluate the attention heads of a machine translation model on dependency parsing, but only report overall UAS scores instead of investigating heads for specific syntactic relations or using probing classifiers. Marecek and Rosa (2018) propose heuristic ways 8 Conclusion We have proposed a series of analysis methods for understanding the attention mechanisms of models and applied them to BERT. While most recent work on model analysis for NLP has focused on probing vector representations or model outputs, we have shown that a substantial amount of linguistic knowledge can be found not only in"
W19-4828,P16-1162,0,0.0598324,"Missing"
W19-4828,N19-1357,0,0.0342873,"machine translation models. They also demonstrate that many attention heads can be pruned away without substantially hurting model performance. Interestingly, the important attention heads that remain after pruning tend to be ones with identified behaviors. Michel et al. (2019) similarly show that many of BERT’s attention heads can be pruned. Although our analysis in this paper only found interpretable behaviors in a subset of BERT’s attention heads, these recent works suggest that there might not be much to explain for some attention heads because they have little effect on model perfomance. Jain and Wallace (2019) argue that attention often does not “explain” model predictions. They show that attention weights frequently do not correlate with other measures of feature importance. Furthermore, attention weights can often be substantially changed without altering model predictions. However, our motivation for looking at attention is different: rather than explaining model predictions, we are seeking to understand information learned by the models. For example, if a particular attention head learns a syntactic relation, we consider that an important finding from an analysis perspective even if that head i"
W19-4828,D16-1159,0,0.0214255,"ns of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) without explicitly being trained for the tasks. With regards to analyzing attention, Vig (2019) builds a visualization tool for the BERT’s attention and reports observations about some of the heads’ behaviors, but does not perform any quantitative analysis. Burns et al. (2018) analyze the attention of memory networks to understand model performance on a question answering dataset; we instead aim to understand linguistic information captured in pre-trained models. There has also been some initial work in correlating attention wi"
W19-4828,D18-1548,0,0.029955,"ndicates the strength of the attention weight (some attention weights are so low they are invisible). Our findings show that particular heads specialize to specific aspects of syntax. To get a more overall measure of the attention heads’ syntactic ability, we propose an attention-based probing classifier that takes attention maps as input. The classifier achieves 77 UAS at dependency parsing, showing BERT’s attention captures a substantial amount about syntax. Several recent works have proposed incorporating syntactic information to improve attention (Eriguchi et al., 2016; Chen et al., 2018; Strubell et al., 2018). Our work suggests that to an extent this kind of syntax-aware attention already exists in BERT, which may be one of the reason for its success. 2 Attention weights can be viewed as governing how “important” every other token is when producing the next representation for the current token. BERT is pre-trained on 3.3 billion tokens of unlabeled text to perform two tasks. In the “masked language modeling” task, the model predicts the identities of words that have been masked-out of the input text. In the “next sentence prediction” task, the model predicts whether the second half of the input fo"
W19-4828,P19-1452,0,0.0867982,"Missing"
W19-4828,D18-1317,0,0.0219967,"ion heads be clearly grouped by behavior? We investigate these questions by computing the distances between all pairs of attention heads. Formally, we measure the distance between two heads Hi and Hj as: X JS(Hi (token), Hj (token)) Results are shown in Figure 6. We find that there are several clear clusters of heads that behave similarly, often corresponding to behaviors we have already discussed in this paper. Heads within the same layer are often fairly close to each other, meaning that heads within the layer have similar attention distributions. This finding is a bit surprising given that Tu et al. (2018) show that encouraging attention heads to have different behaviors can improve Transformer performance at matoken∈data Where JS is the Jensen-Shannon Divergence between attention distributions. Using these distances, we visualize the attention heads by applying multidimensional scaling (Kruskal, 1964) to embed each head in two dimensions such that the euclidean distance between embeddings reflects the Jensen-Shannon distance between the corresponding heads as closely as possible. 283 of converting attention scores to syntactic trees, but do not quantitatively evaluate their approach. Concurren"
W19-4828,P19-1580,0,0.0757501,"ging attention heads to have different behaviors can improve Transformer performance at matoken∈data Where JS is the Jensen-Shannon Divergence between attention distributions. Using these distances, we visualize the attention heads by applying multidimensional scaling (Kruskal, 1964) to embed each head in two dimensions such that the euclidean distance between embeddings reflects the Jensen-Shannon distance between the corresponding heads as closely as possible. 283 of converting attention scores to syntactic trees, but do not quantitatively evaluate their approach. Concurrently with our work Voita et al. (2019) identify syntactic, positional, and rare-wordsensitive attention heads in machine translation models. They also demonstrate that many attention heads can be pruned away without substantially hurting model performance. Interestingly, the important attention heads that remain after pruning tend to be ones with identified behaviors. Michel et al. (2019) similarly show that many of BERT’s attention heads can be pruned. Although our analysis in this paper only found interpretable behaviors in a subset of BERT’s attention heads, these recent works suggest that there might not be much to explain for"
W19-4828,P15-1137,0,0.0187034,"h dependent has exactly one head but heads have multiple dependents. We also note heads can disagree with standard annotation conventions while still performing syntactic behavior. For example, head 76 marks ’s as the dependent for the poss relation, while gold-standard labels mark the complement of an ’s as the dependent (the accuracy in Table 1 counts ’s as correct). Such disagreements highlight how these syntactic behaviors in BERT are learned as a by-product of self-supervised training, not by copying a human design. We also show the performance of a recent neural coreference system from (Wiseman et al., 2015). Results. Results are shown in Table 2. We find that one of BERT’s attention heads achieves decent coreference resolution performance, improving by over 10 accuracy points on the stringmatching baseline and performing close to the rule-based system. It is particularly good with nominal mentions, perhaps because it is capable of fuzzy matching between synonyms as seen in the bottom right of Figure 5. 5 Probing Attention Head Combinations Since individual attention heads specialize to particular aspects of syntax, the model’s overall “knowledge” about syntax is distributed across multiple atten"
W19-4828,W18-5448,0,0.0223357,"e model pretraining. One line of research examines the outputs of language models on carefully chosen input sentences (Linzen et al., 2016; Khandelwal et al., 2018; Gulordava et al., 2018; Marvin and Linzen, 2018). For example, the model’s performance at subject-verb agreement (generating the correct number of a verb far away from its subject) provides a measure of the model’s syntactic ability, although it does not reveal how that ability is captured by the network. Another line of work investigates the internal vector representations of the model (Adi et al., 2017; Giulianelli et al., 2018; Zhang and Bowman, 2018), often using probing classifiers. Probing classifiers are simple neural networks that take the internal vector representations of a pre-trained model as input. They are trained to do a supervised task (e.g., part-of-speech tagging). If a probing classifier achieves high accuracy, it suggests that the vector representations reflect the corresponding aspect of language (e.g., low-level syntax). Like our work, some of these studies have also demonstrated neural networks capturing aspects of syntactic structures (Shi et al., 2016; Blevins et al., 2018) or coreference (Tenney et al., 2018, 2019) w"
W19-4828,P18-1117,0,\N,Missing
