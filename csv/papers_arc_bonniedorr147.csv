2020.stoc-1.1,Active Defense Against Social Engineering: The Case for Human Language Technology,2020,-1,-1,18,0,14499,adam dalton,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,0,"We describe a system that supports natural language processing (NLP) components for active defenses against social engineering attacks. We deploy a pipeline of human language technology, including Ask and Framing Detection, Named Entity Recognition, Dialogue Engineering, and Stylometry. The system processes modern message formats through a plug-in architecture to accommodate innovative approaches for message analysis, knowledge representation and dialogue generation. The novelty of the system is that it uses NLP for cyber defense and engages the attacker using bots to elicit evidence to attribute to the attacker and to waste the attacker{'}s time and resources."
2020.stoc-1.2,Adaptation of a Lexical Organization for Social Engineering Detection and Response Generation,2020,1,0,8,0.952381,11651,archna bhatia,Proceedings for the First International Workshop on Social Threats in Online Conversations: Understanding and Management,0,We present a paradigm for extensible lexicon development based on Lexical Conceptual Structure to support social engineering detection and response generation. We leverage the central notions of ask (elicitation of behaviors such as providing access to money) and framing (risk/reward implied by the ask). We demonstrate improvements in ask/framing detection through refinements to our lexical organization and show that response generation qualitatively improves as ask/framing detection performance improves. The paradigm presents a systematic and efficient approach to resource adaptation for improved task-specific performance.
2020.findings-emnlp.247,Learning to Plan and Realize Separately for Open-Ended Dialogue Systems,2020,-1,-1,4,0,6267,sashank santhanam,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"Achieving true human-like ability to conduct a conversation remains an elusive goal for open-ended dialogue systems. We posit this is because extant approaches towards natural language generation (NLG) are typically construed as end-to-end architectures that do not adequately model human generation processes. To investigate, we decouple generation into two separate phases: planning and realization. In the planning phase, we train two planners to generate plans for response utterances. The realization phase uses response plans to produce an appropriate response. Through rigorous evaluations, both automated and human, we demonstrate that decoupling the process into planning and realization performs better than an end-to-end approach."
2020.amta-research.1,A New Approach to Parameter-Sharing in Multilingual Neural Machine Translation,2020,-1,-1,2,1,14088,benyamin ahmadnia,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track),0,None
R19-1003,Bilingual Low-Resource Neural Machine Translation with Round-Tripping: The Case of {P}ersian-{S}panish,2019,0,0,2,1,14088,benyamin ahmadnia,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"The quality of Neural Machine Translation (NMT), as a data-driven approach, massively depends on quantity, quality, and relevance of the training dataset. Such approaches have achieved promising results for bilingually high-resource scenarios but are inadequate for low-resource conditions. This paper describes a round-trip training approach to bilingual low-resource NMT that takes advantage of monolingual datasets to address training data scarcity, thus augmenting translation quality. We conduct detailed experiments on Persian-Spanish as a bilingually low-resource scenario. Experimental results demonstrate that this competitive approach outperforms the baselines."
R19-1004,Enhancing Phrase-Based Statistical Machine Translation by Learning Phrase Representations Using Long Short-Term Memory Network,2019,0,0,2,1,14088,benyamin ahmadnia,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"Phrases play a key role in Machine Translation (MT). In this paper, we apply a Long Short-Term Memory (LSTM) model over conventional Phrase-Based Statistical MT (PBSMT). The core idea is to use an LSTM encoder-decoder to score the phrase table generated by the PBSMT decoder. Given a source sequence, the encoder and decoder are jointly trained in order to maximize the conditional probability of a target sequence. Analytically, the performance of a PBSMT system is enhanced by using the conditional probabilities of phrase pairs computed by an LSTM encoder-decoder as an additional feature in the existing log-linear model. We compare the performance of the phrase tables in the PBSMT to the performance of the proposed LSTM and observe its positive impact on translation quality. We construct a PBSMT model using the Moses decoder and enrich the Language Model (LM) utilizing an external dataset. We then rank the phrase tables using an LSTM-based encoder-decoder. This method produces a gain of up to 3.14 BLEU score on the test set."
W18-3808,{STYLUS}: A Resource for Systematically Derived Language Usage,2018,-1,-1,1,1,14512,bonnie dorr,Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing,0,"We describe a resource derived through extraction of a set of argument realizations from an existing lexical-conceptual structure (LCS) Verb Database of 500 verb classes (containing a total of 9525 verb entries) to include information about realization of arguments for a range of different verb classes. We demonstrate that our extended resource, called STYLUS (SysTematicallY Derived Language USe), enables systematic derivation of regular patterns of language usage without requiring manual annotation. We posit that both spatially oriented applications such as robot navigation and more general applications such as narrative generation require a layered representation scheme where a set of primitives (often grounded in space/motion such as GO) is coupled with a representation of constraints at the syntax-semantics interface. We demonstrate that the resulting resource covers three cases of lexico-semantic operations applicable to both language understanding and language generation."
W18-1404,Lexical Conceptual Structure of Literal and Metaphorical Spatial Language: A Case Study of {``}Push{''},2018,-1,-1,1,1,14512,bonnie dorr,Proceedings of the First International Workshop on Spatial Language Understanding,0,"Prior methodologies for understanding spatial language have treated literal expressions such as {``}Mary pushed the car over the edge{''} differently from metaphorical extensions such as {``}Mary{'}s job pushed her over the edge{''}. We demonstrate a methodology for standardizing literal and metaphorical meanings, by building on work in Lexical Conceptual Structure (LCS), a general-purpose representational component used in machine translation. We argue that spatial predicates naturally extend into other fields (e.g., circumstantial or temporal), and that LCS provides both a framework for distinguishing spatial from non-spatial, and a system for finding metaphorical meaning extensions. We start with MetaNet (MN), a large repository of conceptual metaphors, condensing 197 spatial entries into sixteen top-level categories of motion frames. Using naturally occurring instances of English push , and expansions of MN frames, we demonstrate that literal and metaphorical extensions exhibit patterns predicted and represented by the LCS model."
W18-1408,The Case for Systematically Derived Spatial Language Usage,2018,0,0,1,1,14512,bonnie dorr,Proceedings of the First International Workshop on Spatial Language Understanding,0,"This position paper argues that, while prior work in spatial language understanding for tasks such as robot navigation focuses on mapping natural language into deep conceptual or non-linguistic representations, it is possible to systematically derive regular patterns of spatial language usage from existing lexical-semantic resources. Furthermore, even with access to such resources, effective solutions to many application areas such as robot navigation and narrative generation also require additional knowledge at the syntax-semantics interface to cover the wide range of spatial expressions observed and available to natural language speakers. We ground our insights in, and present our extensions to, an existing lexico-semantic resource, covering 500 semantic classes of verbs, of which 219 fall within a spatial subset. We demonstrate that these extensions enable systematic derivation of regular patterns of spatial language without requiring manual annotation."
W17-2318,Characterization of Divergence in Impaired Speech of {ALS} Patients,2017,-1,-1,2,0.952381,11651,archna bhatia,{B}io{NLP} 2017,0,"Approximately 80{\%} to 95{\%} of patients with Amyotrophic Lateral Sclerosis (ALS) eventually develop speech impairments, such as defective articulation, slow laborious speech and hypernasality. The relationship between impaired speech and asymptomatic speech may be seen as a divergence from a baseline. This relationship can be characterized in terms of measurable combinations of phonological characteristics that are indicative of the degree to which the two diverge. We demonstrate that divergence measurements based on phonological characteristics of speech correlate with physiological assessments of ALS. Speech-based assessments offer benefits over commonly-used physiological assessments in that they are inexpensive, non-intrusive, and do not require trained clinical personnel for administering and interpreting the results."
J13-3004,Computing Lexical Contrast,2013,41,59,2,0.426907,13005,saif mohammad,Computational Linguistics,0,"Knowing the degree of semantic contrast between words has widespread application in natural language processing, including machine translation, information retrieval, and dialogue systems. Manually created lexicons focus on opposites, such as hot and cold. Opposites are of many kinds such as antipodals, complementaries, and gradable. Existing lexicons often do not classify opposites into the different kinds, however. They also do not explicitly list word pairs that are not opposites but yet have some degree of contrast in meaning, such as warm and cold or tropical and freezing. We propose an automatic method to identify contrasting word pairs that is based on the hypothesis that if a pair of words, A and B, are contrasting, then there is a pair of opposites, C and D, such that A and C are strongly related and B and D are strongly related. (For example, there exists the pair of opposites hot and cold such that tropical is related to hot, and freezing is related to cold.) We will call this the contrast hypo..."
W12-3807,Statistical Modality Tagging from Rule-based Annotations and Crowdsourcing,2012,19,15,4,0,90,vinodkumar prabhakaran,Proceedings of the Workshop on Extra-Propositional Aspects of Meaning in Computational Linguistics,0,We explore training an automatic modality tagger. Modality is the attitude that a speaker might have toward an event or state. One of the main hurdles for training a linguistic tagger is gathering training data. This is particularly problematic for training a tagger for modality because modality triggers are sparse for the overwhelming majority of sentences. We investigate an approach to automatically training a modality tagger where we first gathered sentences based on a high-recall simple rule-based modality tagger and then provided these sentences to Mechanical Turk annotators for further annotation. We used the resulting set of training data to train a precise modality tagger using a multi-class SVM that delivers good performance.
J12-2006,Modality and Negation in {SIMT} Use of Modality and Negation in Semantically-Informed Syntactic {MT},2012,45,34,3,1,43387,kathryn baker,Computational Linguistics,0,"This article describes the resource-and system-building efforts of an 8-week Johns Hopkins University Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically Informed Machine Translation (SIMT). We describe a new modality/negation (MN) annotation scheme, the creation of a (publicly available) MN lexicon, and two automated MN taggers that we built using the annotation scheme and lexicon. Our annotation scheme isolates three components of modality and negation: a trigger (a word that conveys modality or negation), a target (an action associated with modality or negation), and a holder (an experiencer of modality). We describe how our MN lexicon was semi-automatically produced and we demonstrate that a structure-based MN tagger results in precision around 86% (depending on genre) for tagging of a standard LDC data set.n n We apply our MN annotation scheme to statistical machine translation using a syntactic framework that supports the inclusion of semantic annotations. Syntactic tags enriched with semantic annotations are assigned to parse trees in the target-language training texts through a process of tree grafting. Although the focus of our work is modality and negation, the tree grafting procedure is general and supports other types of semantic information. We exploit this capability by including named entities, produced by a pre-existing tagger, in addition to the MN elements produced by the taggers described here. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English test set. This finding supports the hypothesis that both syntactic and semantic information can improve translation quality."
2012.amta-keynotes.3,Language Research at {DARPA}-Machine Translation and Beyond,2012,-1,-1,1,1,14512,bonnie dorr,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Keynote Presentations,0,None
N10-1041,Putting the User in the Loop: Interactive Maximal Marginal Relevance for Query-Focused Summarization,2010,6,16,3,0,888,jimmy lin,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,This work represents an initial attempt to move beyond single-shot summarization to interactive summarization. We present an extension to the classic Maximal Marginal Relevance (MMR) algorithm that places a user in the loop to assist in candidate selection. Experiments in the complex interactive Question Answering (ciQA) task at TREC 2007 show that interactively-constructed responses are significantly higher in quality than automatically-generated ones. This novel algorithm provides a starting point for future work on interactive summarization.
baker-etal-2010-modality,A Modality Lexicon and its use in Automatic Tagging,2010,10,35,3,1,43387,kathryn baker,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper describes our resource-building results for an eight-week JHU Human Language Technology Center of Excellence Summer Camp for Applied Language Exploration (SCALE-2009) on Semantically-Informed Machine Translation. Specifically, we describe the construction of a modality annotation scheme, a modality lexicon, and two automated modality taggers that were built using the lexicon and annotation scheme. Our annotation scheme is based on identifying three components of modality: a trigger, a target and a holder. We describe how our modality lexicon was produced semi-automatically, expanding from an initial hand-selected list of modality trigger words and phrases. The resulting expanded modality lexicon is being made publicly available. We demonstrate that one taggerâa structure-based taggerâresults in precision around 86{\%} (depending on genre) for tagging of a standard LDC data set. In a machine translation application, using the structure-based tagger to annotate English modalities on an English-Urdu training corpus improved the translation quality score for Urdu by 0.3 Bleu points in the face of sparse training data."
J10-3003,Generating Phrasal and Sentential Paraphrases: A Survey of Data-Driven Methods,2010,122,168,2,1,16057,nitin madnani,Computational Linguistics,0,"The task of paraphrasing is inherently familiar to speakers of all languages. Moreover, the task of automatically generating or extracting semantic equivalences for the various units of language-words, phrases, and sentences-is an important part of natural language processing (NLP) and is being increasingly employed to improve the performance of several NLP applications. In this article, we attempt to conduct a comprehensive and application-independent survey of data-driven phrasal and sentential paraphrase generation methods, while also conveying an appreciation for the importance and potential use of paraphrases in the field of NLP research. Recent work done in manual and automatic construction of paraphrase corpora is also examined. We also discuss the strategies used for evaluating paraphrase generation techniques and briefly explore some future trends in paraphrase generation."
2010.amta-papers.7,Semantically-Informed Syntactic Machine Translation: A Tree-Grafting Approach,2010,13,9,4,1,43387,kathryn baker,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"We describe a unified and coherent syntactic framework for supporting a semantically-informed syntactic approach to statistical machine translation. Semantically enriched syntactic tags assigned to the target-language training texts improved translation quality. The resulting system significantly outperformed a linguistically naive baseline model (Hiero), and reached the highest scores yet reported on the NIST 2009 Urdu-English translation task. This finding supports the hypothesis (posed by many researchers in the MT community, e.g., in DARPA GALE) that both syntactic and semantic information are critical for improving translation quality{---}and further demonstrates that large gains can be achieved for low-resource languages with different word order than English."
W09-0441,"Fluency, Adequacy, or {HTER}? {E}xploring Different Human Judgments with a Tunable {MT} Metric",2009,15,158,3,1,44205,matthew snover,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"Automatic Machine Translation (MT) evaluation metrics have traditionally been evaluated by the correlation of the scores they assign to MT output with human judgments of translation performance. Different types of human judgments, such as Fluency, Adequacy, and HTER, measure varying aspects of MT performance that can be captured by automatic MT metrics. We explore these differences through the use of a new tunable MT metric: TER-Plus, which extends the Translation Edit Rate evaluation metric with tunable parameters and the incorporation of morphology, synonymy and paraphrases. TER-Plus was shown to be one of the top metrics in NIST's Metrics MATR 2008 Challenge, having the highest average rank in terms of Pearson and Spearman correlation. Optimizing TER-Plus to different types of human judgments yields significantly improved correlations and meaningful changes in the weight of different types of edits, demonstrating significant differences between the types of human judgments."
N09-1066,Using Citations to Generate surveys of Scientific Paradigms,2009,27,81,2,1,13005,saif mohammad,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"The number of research publications in various disciplines is growing exponentially. Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material. In this paper we present the first steps in producing an automatically generated, readily consumable, technical survey. Specifically we explore the combination of citation information and summarization techniques. Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role."
D09-1063,Generating High-Coverage Semantic Orientation Lexicons From Overtly Marked Words and a Thesaurus,2009,33,145,3,1,13005,saif mohammad,Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing,0,"Sentiment analysis often relies on a semantic orientation lexicon of positive and negative words. A number of approaches have been proposed for creating such lexicons, but they tend to be computationally expensive, and usually rely on significant manual annotation and large corpora. Most of these methods use WordNet. In contrast, we propose a simple approach to generate a high-coverage semantic orientation lexicon, which includes both individual words and multi-word expressions, using only a Roget-like thesaurus and a handful of affixes. Further, the lexicon has properties that support the Polyanna Hypothesis. Using the General Inquirer as gold standard, we show that our lexicon has 14 percentage points more correct entries than the leading WordNet-based high-coverage lexicon (SentiWordNet). In an extrinsic evaluation, we obtain significantly higher performance in determining phrase polarity using our thesaurus-based lexicon than with any other. Additionally, we explore the use of visualization techniques to gain insight into the our algorithm beyond the evaluations mentioned above."
W08-0209,Combining Open-Source with Research to Re-engineer a Hands-on Introductory {NLP} Course,2008,18,3,2,1,16057,nitin madnani,Proceedings of the Third Workshop on Issues in Teaching Computational Linguistics,0,"We describe our first attempts to re-engineer the curriculum of our introductory NLP course by using two important building blocks: (1) Access to an easy-to-learn programming language and framework to build hands-on programming assignments with real-world data and corpora and, (2) Incorporation of interesting ideas from recent NLP research publications into assignment and examination problems. We believe that these are extremely important components of a curriculum aimed at a diverse audience consisting primarily of first-year graduate students from both linguistics and computer science. Based on overwhelmingly positive student feedback, we find that our attempts were hugely successful."
bird-etal-2008-acl,The {ACL} {A}nthology Reference Corpus: A Reference Dataset for Bibliographic Research in Computational Linguistics,2008,15,134,3,0,8953,steven bird,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The ACL Anthology is a digital archive of conference and journal papers in natural language processing and computational linguistics. Its primary purpose is to serve as a reference repository of research results, but we believe that it can also be an object of study and a platform for research in its own right. We describe an enriched and standardized reference corpus derived from the ACL Anthology that can be used for research in scholarly document processing. This corpus, which we call the ACL Anthology Reference Corpus (ACL ARC), brings together the recent activities of a number of research groups around the world. Our goal is to make the corpus widely available, and to encourage other researchers to use it as a standard testbed for experiments in both bibliographic and bibliometric research."
D08-1090,Language and Translation Model Adaptation using Comparable Corpora,2008,18,61,2,1,44205,matthew snover,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Traditionally, statistical machine translation systems have relied on parallel bi-lingual data to train a translation model. While bi-lingual parallel data are expensive to generate, monolingual data are relatively common. Yet monolingual data have been under-utilized, having been used primarily for training a language model in the target language. This paper describes a novel method for utilizing monolingual target data to improve the performance of a statistical machine translation system on news stories. The method exploits the existence of comparable text---multiple texts in the target language that discuss the same or similar stories as found in the source language document. For every source document that is to be translated, a large monolingual data set in the target language is searched for documents that might be comparable to the source documents. These documents are then used to adapt the MT system to increase the probability of generating texts that resemble the comparable document. Experimental results obtained by adapting both the language and translation models show substantial gains over the baseline system."
D08-1103,Computing Word-Pair Antonymy,2008,23,77,2,1,13005,saif mohammad,Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing,0,"Knowing the degree of antonymy between words has widespread applications in natural language processing. Manually-created lexicons have limited coverage and do not include most semantically contrasting word pairs. We present a new automatic and empirical measure of antonymy that combines corpus statistics with the structure of a published thesaurus. The approach is evaluated on a set of closest-opposite questions, obtaining a precision of over 80%. Along the way, we discuss what humans consider antonymous and how antonymy manifests itself in utterances."
2008.amta-papers.13,Are Multiple Reference Translations Necessary? Investigating the Value of Paraphrased Reference Translations in Parameter Optimization,2008,-1,-1,3,1,16057,nitin madnani,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In our earlier work (Madnani et al., 2007), we introduced a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and demonstrated that the resulting paraphrases can be used to cut the number of human reference translations needed in half. In this paper, we take the idea a step further, asking how far it is possible to get with just a single good reference translation for each item in the development set. Our analysis suggests that it is necessary to invest in four or more human translations in order to significantly improve on a single translation augmented by monolingual paraphrases."
W07-2312,Measuring Variability in Sentence Ordering for News Summarization,2007,17,23,5,1,16057,nitin madnani,Proceedings of the Eleventh {E}uropean Workshop on Natural Language Generation ({ENLG} 07),0,"The issue of sentence ordering is an important one for natural language tasks such as multi-document summarization, yet there has not been a quantitative exploration of the range of acceptable sentence orderings for short texts. We present results of a sentence reordering experiment with three experimental conditions. Our findings indicate a very high degree of variability in the orderings that the eighteen subjects produce. In addition, the variability of reorderings is significantly greater when the initial ordering seen by subjects is different from the original summary. We conclude that evaluation of sentence ordering should use multiple reference orderings. Our evaluation presents several metrics that might prove useful in assessing against multiple references. We conclude with a deeper set of questions: (a) what sorts of independent assessments of quality of the different reference orderings could be made and (b) whether a large enough test set would obviate the need for such independent means of quality assessment."
W07-0716,Using Paraphrases for Parameter Tuning in Statistical Machine Translation,2007,25,70,4,1,16057,nitin madnani,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Most state-of-the-art statistical machine translation systems use log-linear models, which are defined in terms of hypothesis features and weights for those features. It is standard to tune the feature weights in order to maximize a translation quality metric, using held-out test sentences and their corresponding reference translations. However, obtaining reference translations is expensive. In this paper, we introduce a new full-sentence paraphrase technique, based on English-to-English decoding with an MT system, and we demonstrate that the resulting paraphrases can be used to drastically reduce the number of human reference translations needed for parameter tuning, without a significant decrease in translation quality."
N07-1029,Combining Outputs from Multiple Machine Translation Systems,2007,15,153,6,0,42249,anttiveikko rosti,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"Currently there are several approaches to machine translation (MT) based on different paradigms; e.g., phrasal, hierarchical and syntax-based. These three approaches yield similar translation accuracy despite using fairly different levels of linguistic knowledge. The availability of such a variety of systems has led to a growing interest toward finding better translations by combining outputs from multiple systems. This paper describes three different approaches to MT system combination. These combination methods operate on sentence, phrase and word level exploiting information from -best lists, system scores and target-to-source phrase alignments. The word-level combination provides the most robust gains but the best results on the development test sets (NIST MT05 and the newsgroup portion of GALE 2006 dry-run) were achieved by combining all three methods."
P06-1002,Going Beyond {AER}: An Extensive Analysis of Word Alignments and Their Impact on {MT},2006,18,80,2,1,48721,necip ayan,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,This paper presents an extensive evaluation of five different alignments and investigates their impact on the corresponding MT system output. We introduce new measures for intrinsic evaluations and examine the distribution of phrases and untranslated words during decoding to identify which characteristics of different alignments affect translation. We show that precision-oriented alignments yield better MT output (translating more words and using longer phrases) than recall-oriented alignments.
P06-1021,{PCFG}s with Syntactic and Prosodic Indicators of Speech Repairs,2006,31,14,4,0,11523,john hale,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"A grammatical method of combining two kinds of speech repair cues is presented. One cue, prosodic disjuncture, is detected by a decision tree-based ensemble classifier that uses acoustic cues to identify where normal prosody seems to be interrupted (Lickley, 1996). The other cue, syntactic parallelism, codifies the expectation that repairs continue a syntactic category that was left unfinished in the reparandum (Levelt, 1983). The two cues are combined in a Treebank PCFG whose states are split using a few simple tree transformations. Parsing performance on the Switchboard and Fisher corpora suggests that these two cues help to locate speech repairs in a synergistic way."
P06-1119,Leveraging Reusability: Cost-Effective Lexical Acquisition for Large-Scale Ontology Translation,2006,13,5,2,0,49992,craig murray,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Thesauri and ontologies provide important value in facilitating access to digital archives by representing underlying principles of organization. Translation of such resources into multiple languages is an important component for providing multilingual access. However, the specificity of vocabulary terms in most ontologies precludes fully-automated machine translation using general-domain lexical resources. In this paper, we present an efficient process for leveraging human translations when constructing domain-specific lexical resources. We evaluate the effectiveness of this process by producing a probabilistic phrase dictionary and translating a thesaurus of 56,000 concepts used to catalogue a large archive of oral histories. Our experiments demonstrate a cost-effective technique for accurate machine translation of large ontologies."
N06-1013,A Maximum Entropy Approach to Combining Word Alignments,2006,21,27,2,1,48721,necip ayan,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"This paper presents a new approach to combining outputs of existing word alignment systems. Each alignment link is represented with a set of feature functions extracted from linguistic features and input alignments. These features are used as the basis of alignment decisions made by a maximum entropy approach. The learning method has been evaluated on three language pairs, yielding significant improvements over input alignments and three heuristic combination methods. The impact of word alignment on MT quality is investigated, using a phrase-based MT system."
roark-etal-2006-sparseval,{SP}arseval: Evaluation Metrics for Parsing Speech,2006,12,36,4,0,4293,brian roark,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"While both spoken and written language processing stand to benefit from parsing, the standard Parseval metrics (Black et al., 1991) and their canonical implementation (Sekine and Collins, 1997) are only useful for text. The Parseval metrics are undefined when the words input to the parser do not match the words in the gold standard parse tree exactly, and word errors are unavoidable with automatic speech recognition (ASR) systems. To fill this gap, we have developed a publicly available tool for scoring parses that implements a variety of metrics which can handle mismatches in words and segmentations, including: alignment-based bracket evaluation, alignment-based dependency evaluation, and a dependency evaluation that does not require alignment. We describe the different metrics, how to use the tool, and the outcome of an extensive set of experiments on the sensitivity."
rambow-etal-2006-parallel,Parallel Syntactic Annotation of Multiple Languages,2006,12,10,2,0,1354,owen rambow,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes an effort to investigate the incrementally deepening development of an interlingua notation, validated by human annotation of texts in English plus six languages. We begin with deep syntactic annotation, and in this paper present a series of annotation manuals for six different languages at the deep-syntactic level of representation. Many syntactic differences between languages are removed in the proposed syntactic annotation, making them useful resources for multilingual NLP projects with semantic components."
2006.eamt-1.18,Leveraging Recurrent Phrase Structure in Large-scale Ontology Translation,2006,-1,-1,2,0,49992,craig murray,Proceedings of the 11th Annual conference of the European Association for Machine Translation,0,None
2006.amta-papers.7,Challenges in Building an {A}rabic-{E}nglish {GHMT} System with {SMT} Components,2006,24,12,2,0.424767,517,nizar habash,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"The research context of this paper is developing hybrid machine translation (MT) systems that exploit the advantages of linguistic rule-based and statistical MT systems. Arabic, as a morphologically rich language, is especially challenging even without addressing the hybridization question. In this paper, we describe the challenges in building an Arabic-English generation-heavy machine translation (GHMT) system and boosting it with statistical machine translation (SMT) components. We present an extensive evaluation of multiple system variants and report positive results on the advantages of hybridization."
2006.amta-papers.25,A Study of Translation Edit Rate with Targeted Human Annotation,2006,-1,-1,2,1,44205,matthew snover,Proceedings of the 7th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"We examine a new, intuitive measure for evaluating machine-translation output that avoids the knowledge intensiveness of more meaning-based approaches, and the labor-intensiveness of human judgments. Translation Edit Rate (TER) measures the amount of editing that a human would have to perform to change a system output so it exactly matches a reference translation. We show that the single-reference variant of TER correlates as well with human judgments of MT quality as the four-reference variant of BLEU. We also define a human-targeted TER (or HTER) and show that it yields higher correlations with human judgments than BLEU{---}even when BLEU is given human-targeted references. Our results indicate that HTER correlates with human judgments better than HMETEOR and that the four-reference variants of TER and HTER correlate with human judgments as well as{---}or better than{---}a second human judgment does."
W05-1007,Frame Semantic Enhancement of Lexical-Semantic Resources,2005,13,1,2,1,50483,rebecca green,Proceedings of the {ACL}-{SIGLEX} Workshop on Deep Lexical Acquisition,0,"SemFrame generates FrameNet-like frames, complete with semantic roles and evoking lexical units. This output can enhance FrameNet by suggesting new frames, as well as additional lexical units that evoke existing frames. SemFrame output can also support the addition of frame semantic relationships to WordNet."
W05-0901,A Methodology for Extrinsic Evaluation of Text Summarization: Does {ROUGE} Correlate?,2005,19,27,1,1,14512,bonnie dorr,Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,0,"This paper demonstrates the usefulness of summaries in an extrinsic task of relevance judgment based on a new method for measuring agreement, Relevance-Prediction, which compares subjectsxe2x80x99 judgments on summaries with their own judgments on full text documents. We demonstrate that, because this measure is more reliable than previous gold-standard measures, we are able to make stronger statistical statements about the benefits of summarization. We found positive correlations between ROUGE scores and two different summary types, where only weak or negative correlations were found using other agreement measures. However, we show that ROUGE may be sensitive to the choice of summarization style. We discuss the importance of these results and the implications for future summarization evaluations."
H05-1009,{N}eur{A}lign: Combining Word Alignments Using Neural Networks,2005,27,29,2,1,48721,necip ayan,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"This paper presents a novel approach to combining different word alignments. We view word alignment as a pattern classification problem, where alignment combination is treated as a classifier ensemble, and alignment links are adorned with linguistic features. A neural network model is used to learn word alignments from the individual alignment systems. We show that our alignment combination approach yields a significant 20--34% relative error reduction over the best-known alignment combination technique on English-Spanish and English-Chinese data."
H05-1024,Alignment Link Projection Using Transformation-Based Learning,2005,17,11,2,1,48721,necip ayan,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We present a new word-alignment approach that learns errors made by existing word alignment systems and corrects them. By adapting transformation-based learning to the problem of word alignment, we project new alignment links from already existing links, using features such as POS tags. We show that our alignment link projection approach yields a significantly lower alignment error rate than that of the best performing alignment system (22.6% relative reduction on English-Spanish data and 23.2% relative reduction on English-Chinese data)."
W04-2709,Interlingual Annotation of Multilingual Text Corpora,2004,14,20,3,0,50484,stephen helmreich,Proceedings of the Workshop Frontiers in Corpus Annotation at {HLT}-{NAACL} 2004,0,"This paper describes a multi-site project to annotate six sizable bilingual parallel corpora for interlingual content. After presenting the background and objectives of the effort, we will go on to describe the data set that is being annotated, the interlingua representation language used, an interface environment that supports the annotation task and the annotation process itself. We will then present a preliminary version of our evaluation methodology and conclude with a summary of the current status of the project along with a number of issues which have arisen."
W04-0909,Inducing a semantic frame lexicon from {W}ord{N}et data,2004,11,7,2,1,50483,rebecca green,Proceedings of the 2nd Workshop on Text Meaning and Interpretation,0,"This paper presents SemFrame, a system that automatically induces the names and internal structures of semantic frames. After SemFrame identifies sets of frame-evoking verb synsets, the conceptual density of nodes in the WordNet network for corresponding nouns and noun synsets is computed and analyzed. Conceptually dense nodes are candidates for frame names and frame slots. Ca. 76% of the frame names and 87% of the frame slots generated by SemFrame are rated adequate by human judges."
P04-1048,Inducing Frame Semantic Verb Classes from {W}ord{N}et and {LDOCE},2004,15,24,2,1,50483,rebecca green,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"This paper presents SemFrame, a system that induces frame semantic verb classes from WordNet and LDOCE. Semantic frames are thought to have significant potential in resolving the paraphrase problem challenging many language-based applications.When compared to the handcrafted FrameNet, SemFrame achieves its best recall-precision balance with 83.2% recall (based on SemFrame's coverage of FrameNet frames) and 73.8% precision (based on SemFrame verbs' semantic relatedness to frame-evoking verbs). The next best performing semantic verb classes achieve 56.9% recall and 55.0% precision."
N04-4040,A Lexically-Driven Algorithm for Disfluency Detection,2004,11,43,2,1,44205,matthew snover,Proceedings of {HLT}-{NAACL} 2004: Short Papers,0,"This paper describes a transformation-based learning approach to disfluency detection in speech transcripts using primarily lexical features. Our method produces comparable results to two other systems that make heavy use of prosodic features, thus demonstrating that reasonable performance can be achieved without extensive prosodic cues. In addition, we show that it is possible to facilitate the identification of less frequently disfluent discourse markers by taking speaker style into account."
C04-1137,Identification of Confusable Drug Names: A New Approach and Evaluation Methodology,2004,8,50,2,0,1894,grzegorz kondrak,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper addresses the mitigation of medical errors due to the confusion of sound-alike and look-alike drug names. Our approach involves application of two new methods---one based on orthographic similarity (look-alike) and the other based on phonetic similarity (sound-alike). We present a new recall-based evaluation methodology for determining the effectiveness of different similarity measures on drug names. We show that the new orthographic measure (BI-SIM) outperforms other commonly used measures of similarity on a set containing both look-alike and sound-alike pairs, and that the feature-based phonetic approach (ALINE) outperforms orthographic approaches on a test set containing solely sound-alike confusion pairs. However, an approach that combines several different measures achieves the best results on both test sets."
ayan-etal-2004-multi,Multi-Align: combining linguistic and statistical techniques to improve alignments for adaptable {MT},2004,28,16,2,1,48721,necip ayan,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"An adaptable statistical or hybrid MT system relies heavily on the quality of word-level alignments of real-world data. Statistical alignment approaches provide a reasonable initial estimate for word alignment. However, they cannot handle certain types of linguistic phenomena such as long-distance dependencies and structural differences between languages. We address this issue in Multi-Align, a new framework for incremental testing of different alignment algorithms and their combinations. Our design allows users to tune their systems to the properties of a particular genre/domain while still benefiting from general linguistic knowledge associated with a language pair. We demonstrate that a combination of statistical and linguistically-informed alignments can resolve translation divergences during the alignment process."
reeder-etal-2004-interlingual,Interlingual annotation for {MT} development,2004,16,7,2,0,46657,florence reeder,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"MT systems that use only superficial representations, including the current generation of statistical MT systems, have been successful and useful. However, they will experience a plateau in quality, much like other {``}silver bullet{''} approaches to MT. We pursue work on the development of interlingual representations for use in symbolic or hybrid MT systems. In this paper, we describe the creation of an interlingua and the development of a corpus of semantically annotated text, to be validated in six languages and evaluated in several ways. We have established a distributed, well-functioning research methodology, designed a preliminary interlingua notation, created annotation manuals and tools, developed a test collection in six languages with associated English translations, annotated some 150 translations, and designed and applied various annotation metrics. We describe the data sets being annotated and the interlingual (IL) representation language which uses two ontologies and a systematic theta-role list. We present the annotation tools built and outline the annotation process. Following this, we describe our evaluation methodology and conclude with a summary of issues that have arisen."
W03-0501,Hedge Trimmer: A Parse-and-Trim Approach to Headline Generation,2003,22,169,1,1,14512,bonnie dorr,Proceedings of the {HLT}-{NAACL} 03 Text Summarization Workshop,0,"This paper presents Hedge Trimmer, a HEaDline GEneration system that creates a headline for a newspaper story using linguistically-motivated heuristics to guide the choice of a potential headline. We present feasibility tests used to establish the validity of an approach that constructs a headline by selecting words in order from a story. In addition, we describe experimental results that demonstrate the effectiveness of our linguistically-motivated approach over a HMM-based model, using both human evaluation and automatic metrics for comparing the two approaches."
N03-2026,Desparately Seeking {C}ebuano,2003,2,7,3,0,12879,douglas oard,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Short Papers,0,This paper describes an effort to rapidly develop language resources and component technology to support searching Cebuano news stories using English queries. Results from the first 60 hours of the exercise are presented.
N03-1013,A Categorial Variation Database for {E}nglish,2003,28,63,2,0.833333,517,nizar habash,Proceedings of the 2003 Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe our approach to the construction and evaluation of a large-scale database called CatVar which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications, our categorial-variation resource may serve as an integral part of a diverse range of natural language applications. Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities.We apply the information-retrieval metrics of precision and recall to evaluate the accuracy and coverage of our database with respect to a human-produced gold standard. This evaluation reveals that the categorial database achieves a high degree of precision and recall. Additionally, we demonstrate that the database improves on the linkability of Porter stemmer by over 30%."
2003.mtsummit-systems.9,{C}at{V}ar: a database of categorial variations for {E}nglish,2003,28,63,2,0.833333,517,nizar habash,Proceedings of Machine Translation Summit IX: System Presentations,0,"We present a new large-scale database called {``}CatVar{''} (Habash and Dorr, 2003) which contains categorial variations of English lexemes. Due to the prevalence of cross-language categorial variation in multilingual applications, our categorial-variation resource may serve as an integral part of a diverse range of natural language applications. Thus, the research reported herein overlaps heavily with that of the machine-translation, lexicon-construction, and information-retrieval communities. We demonstrate this database, embedded in a graphical interface; we also show a GUI for user input of corrections to the database."
2003.mtsummit-papers.28,Acquisition of bilingual {MT} lexicons from {OCR}ed dictionaries,2003,-1,-1,3,0,49649,burcu karagolayan,Proceedings of Machine Translation Summit IX: Papers,0,"This paper describes an approach to analyzing the lexical structure of OCRed bilingual dictionaries to construct resources suited for machine translation of low-density languages, where online resources are limited. A rule-based, an HMM-based, and a post-processed HMM-based method are used for rapid construction of MT lexicons based on systematic structural clues provided in the original dictionary. We evaluate the effectiveness of our techniques, concluding that: (1) the rule-based method performs better with dictionaries where the font is not an important distinguishing feature for determining information types; (2) the post-processed stochastic method improves the results of the stochastic method for phrasal entries; and (3) Our resulting bilingual lexicons are comprehensive enough to provide the basis for reasonable translation results when compared to human translations."
2003.mtsummit-eval.1,Evaluation techniques applied to domain tuning of {MT} lexicons,2003,-1,-1,2,1,48721,necip ayan,Workshop on Systemizing MT Evaluation,0,None
dorr-etal-2002-duster,{DUST}er: a method for unraveling cross-language divergences for statistical word-level alignment,2002,23,31,1,1,14512,bonnie dorr,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"The frequent occurrence of divergenceS{---}structural differences between languages{---}presents a great challenge for statistical word-level alignment. In this paper, we introduce DUSTer, a method for systematically identifying common divergence types and transforming an English sentence structure to bear a closer resemblance to that of another language. Our ultimate goal is to enable more accurate alignment and projection of dependency trees in another language without requiring any training on dependency-tree data in that language. We present an empirical analysis comparing the complexities of performing word-level alignments with and without divergence handling. Our results suggest that our approach facilitates word-level alignment, particularly for sentence pairs containing divergences."
habash-dorr-2002-handling,Handling translation divergences: combining statistical and symbolic techniques in generation-heavy machine translation,2002,19,34,2,1,517,nizar habash,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper describes a novel approach to handling translation divergences in a Generation-Heavy Hybrid Machine Translation (GHMT) system. The translation divergence problem is usually reserved for Transfer and Interlingual MT because it requires a large combination of complex lexical and structural mappings. A major requirement of these approaches is the accessibility of large amounts of explicit symmetric knowledge for both source and target languages. This limitation renders Transfer and Interlingual approaches ineffective in the face of structurally-divergent language pairs with asymmetric resources. GHMT addresses the more common form of this problem, source-poor/targetrich, by fully exploiting symbolic and statistical target-language resources. This non-interlingual non-transfer approach is accomplished by using target-language lexical semantics, categorial variations and subcategorization frames to overgenerate multiple lexico-structural variations from a target-glossed syntactic dependency of the source-language sentence. The symbolic overgeneration, which accounts for different possible translation divergences, is constrained by a statistical target-language model."
P01-1032,Mapping Lexical Entries in a Verbs Database to {W}ord{N}et Senses,2001,19,12,3,1,50483,rebecca green,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"This paper describes automatic techniques for mapping 9611 entries in a database of English verbs to WordNet senses. The verbs were initially grouped into 491 classes based on syntactic features. Mapping these verbs into WordNet senses provides a resource that supports disambiguation in multilingual applications such as machine translation and cross-language information retrieval. Our techniques make use of (1) a training set of 1791 disambiguated entries, representing 1442 verb entries from 167 classes; (2) word sense probabilities, from frequency counts in a tagged corpus; (3) semantic similarity of WordNet senses for verbs within the same class; (4) probabilistic correlations between WordNet data and attributes of the verb classes. The best results achieved 72% precision and 58% recall, versus a lower bound of 62% precision and 38% recall for assigning the most frequently occurring WordNet sense, and an upper bound of 87% precision and 75% recall for human judgment."
2001.mtsummit-papers.26,Large scale language independent generation using thematic hierarchies,2001,1,3,2,1,517,nizar habash,Proceedings of Machine Translation Summit VIII,0,Abstract : This paper describes a large-scale language-independent evaluation of the use of Thematic Hierardies in natural language generation. We translate from a corpus of sentences reflecting the full variety of behavior of Levin-based verb classes. The corpus is used as input to a generation system that utilizes the same thematic hierarchy for realizing relative argument surface positions in two languages; English and Spanish. The output was manually evaluated by English and Spanish speakers. The contributions of this work include: (1) an improved thematic hierarchy over an earlier implementation; (2) a large-scale evaluation of the use of thematic hierarchies in two languages; (3) an implementation of a language independent module for natural language generation; and (4) the creation of a single tool for incremental development of multilingual lexicons.
dorr-etal-2000-chinese,{C}hinese-{E}nglish Semantic Resource Construction,2000,12,8,1,1,14512,bonnie dorr,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Abstract : We describe an approach to large-scale construction of a semantic lexicon for Chinese verbs. We leverage off of three existing resources-- a classification of English verbs called EVCA (English Verbs Classes and Alternations), a Chinese conceptual database called HowNet, and a large-machine readable dictionary called Optilex. The resulting lexicon is used for determining appropriate word senses in applications such as machine translation and cross-language information retrieval."
dorr-etal-2000-building,Building a {C}hinese-{E}nglish mapping between verb concepts for multilingual applications,2000,19,8,1,1,14512,bonnie dorr,Proceedings of the Fourth Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper addresses the problem of building conceptual resources for multilingual applications. We describe new techniques for large-scale construction of a Chinese-English lexicon for verbs, using thematic-role information to create links between Chinese and English conceptual information. We then present an approach to compensating for gaps in the existing resources. The resulting lexicon is used for multilingual applications such as machine translation and cross-language information retrieval."
olsen-etal-1998-enhancing,Enhancing automatic acquisition of the thematic structure in a large-scale lexicon for {M}andarin {C}hinese,1998,13,8,2,0.952381,28559,mari olsen,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper describes a refinement to our procedure for porting lexical conceptual structure (LCS) into new languages. Specifically we describe a two-step process for creating candidate thematic grids for Mandarin Chinese verbs, using the English verb heading the VP in the subde{\_}nitions to separate senses, and roughly parsing the verb complement structure to match thematic structure templates. We accomplished a substantial reduction in manual effort, without substantive loss. The procedure is part of a larger process of creating a usable lexicon for interlingual machine translation from a large on-line resource with both too much and too little information."
dorr-etal-1998-thematic,A thematic hierarchy for efficient generation from lexical-conceptual structure,1998,16,14,1,1,14512,bonnie dorr,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,This paper describes an implemented algorithm for syntactic realization of a target-language sentence from an interlingual representation called Lexical Conceptual Structure (LCS). We provide a mapping between LCS thematic roles and Abstract Meaning Representation (AMR) relations; these relations serve as input to an off-the-shelf generator (Nitrogen). There are two contributions of this work: (1) the development of a thematic hierarchy that provides ordering information for realization of arguments in their surface positions; (2) the provision of a diagnostic tool for detecting inconsistencies in an existing online LCS-based lexicon that allows us to enhance principles for thematic-role assignment.
dorr-katsova-1998-lexical,Lexical selection for cross-language applications: combining {LCS} with {W}ord{N}et,1998,14,12,1,1,14512,bonnie dorr,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"This paper describes experiments for testing the power of large-scale resources for lexical selection in machine translation (MT) and cross-language information retrieval (CLIR). We adopt the view that verbs with similar argument structure share certain meaning components, but that those meaning components are more relevant to argument realization than to idiosyncratic verb meaning. We verify this by demonstrating that verbs with similar argument structure as encoded in Lexical Conceptual Structure (LCS) are rarely synonymous in WordNet. We then use the results of this work to guide our implementation of an algorithm for cross-language selection of lexical items, exploiting the strengths of each resource: LCS for semantic structure and WordNet for semantic content. We use the Parka Knowledge-Based System to encode LCS representations and WordNet synonym sets and we implement our lexical-selection algorithm as Parka-based queries into a knowledge base containing both information types."
W97-0219,Structured Lexicons and Semantic Tagging,1997,0,0,1,1,14512,bonnie dorr,"Tagging Text with Lexical Semantics: Why, What, and How?",0,None
P97-1020,Deriving Verbal and Compositonal Lexical Aspect for {NLP} Applications,1997,26,26,1,1,14512,bonnie dorr,35th Annual Meeting of the Association for Computational Linguistics and 8th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,1,"Verbal and compositional lexical aspect provide the underlying temporal structure of events. Knowledge of lexical aspect, e.g., (a)telicity, is therefore required for interpreting event sequences in discourse (Dowty, 1986; Moens and Steedman, 1988; Passoneau, 1988), interfacing to temporal databases (Androutsopoulos, 1996), processing temporal modifiers (Antonisse, 1994), describing allowable alternations and their semantic effects (Resnik, 1996; Tenny, 1994), and selecting tense and lexical items for natural language generation ((Dorr and Olsen, 1996; Klavans and Chodorow, 1992), cf. (Slobin and Bocaz, 1988)). We show that it is possible to represent lexical aspect---both verbal and compositional---on a large scale, using Lexical Conceptual Structure (LCS) representations of verbs in the classes cataloged by Levin (1993). We show how proper consideration of these universal pieces of verb meaning may be used to refine lexical representations and derive a range of meanings from combinations of LCS representations. A single algorithm may therefore be used to determine lexical aspect classes and features at both verbal and sentence levels. Finally, we illustrate how knowledge of lexical aspect facilitates the interpretation of events in NLP applications."
A97-1021,Large-Scale Acquisition of {LCS}-Based Lexicons for Foreign Language Tutoring,1997,20,21,1,1,14512,bonnie dorr,Fifth Conference on Applied Natural Language Processing,0,"We focus on the problem of building large repositories of lexical conceptual structure (LCS) representations for verbs in multiple languages. One of the man results of this work is the definition of a relation between broad semantic classes and LCS meaning components. Our acquisition program---LEXICALL---takes, as input, the result of previous work on verb classification and thematic grid tagging, and outputs LCS representations for different languages. These representations have been ported into English. Arabic and Spanish lexicons, each containing approximately 9000 verbs. We are currently using these lexicons in an operational foreign language tutoring and machine translation."
1997.mtsummit-workshop.3,{S}panish {E}uro{W}ord{N}et and {LCS}-based interlingual {MT},1997,9,13,1,1,14512,bonnie dorr,AMTA/SIG-IL First Workshop on Interlinguas,0,"We present a machine translation framework in which the interlingua{---} Lexical Conceptual Structure (LCS){---}is coupled with a definitional component that includes bilingual (EuroWordNet) links between words in the source and target languages. While the links between individual words are language-specific, the LCS is designed to be a language-independent, compositional representation. We take the view that the two types of information{---}shallower, transfer-like knowledge as well as deeper, compositional knowledge{---}can be reconciled in interlingual machine translation, the former for overcoming the intractability of LCS-based lexical selec- tion, and the latter for relating the underlying semantics of two words cross-linguistically. We describe the acquisition process for these two information types and present results of hand-verification of the acquired lexicon. Finally, we demonstrate the utility of the two information types in interlingual MT."
1997.mtsummit-workshop.4,Toward compact monotonically compositional interlingua using lexical aspect,1997,18,2,1,1,14512,bonnie dorr,AMTA/SIG-IL First Workshop on Interlinguas,0,"We describe a theoretical investigation into the semantic space described by our interlingua (IL), which currently has 191 main verb classes divided into 434 subclasses, represented by 237 distinct Lexical Conceptual Structures (LCSs). Using the model of aspect in Olsen (1994; 1997){---}monotonic aspectual composition{---}we have identified 71 aspectually basic subclasses that are associated with one or more of 68 aspectually non-basic classes via some lexical ({``}type-shifting{''}) rule (Bresnan, 1982; Pinker, 1984; Levin and Rappaport Hovav, 1995). This allows us to refine the IL and address certain computational and theoretical issues at the same time. (1) From a linguistic viewpoint, the expected benefits include a refinement of the aspectual model in (Olsen, 1994; Olsen, 1997) (which provides necessary but not sufficient conditions for aspectual com- position), and a refinement of the verb classifications in (Levin, 1993); we also expect our approach to eventually produce a systematic definition (in terms of LCSs and compositional operations) of the precise meaning components responsible for Levin's classification. (2) Computationally, the lexicon is made more compact."
1997.mtsummit-workshop.13,Using {W}ord{N}et to posit hierarchical structure in Levin{'}s verb classes,1997,9,3,2,0.952381,28559,mari olsen,AMTA/SIG-IL First Workshop on Interlinguas,0,"In this paper we report on experiments using WordNet synset tags to evaluate the semantic properties of the verb classes cataloged by Levin (1993). This paper represents ongoing research begun at the University of Pennsylvania (Rosenzweig and Dang, 1997; Palmer, Rosenzweig, and Dang, 1997) and the University of Maryland (Dorr and Jones, 1996b; Dorr and Jones, 1996a; Dorr and Jones, 1996c). Using WordNet sense tags to constrain the intersection of Levin classes, we avoid spurious class intersections introduced by homonymy and polysemy (run a bath, run a mile). By adding class intersections based on a single shared sense-tagged word, we minimize the impact of the non-exhaustiveness of Levin{'}s database (Dorr and Olsen, 1996; Dorr, To appear). By examining the syntactic properties of the intersective classes, we provide a clearer picture of the relationship between WordNet/EuroWordNet and the LCS interlingua for machine translation and other NLP applications."
W96-0306,Acquisition of Semantic Lexicons: Using Word Sense Disambiguation to Improve Precision,1996,-1,-1,1,1,14512,bonnie dorr,Breadth and Depth of Semantic Lexicons,0,None
C96-1055,Role of Word Sense Disalnbiguation in Lexical Acquisition: Predicting Semantics from Syntactic Cues,1996,20,74,1,1,14512,bonnie dorr,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"This paper addresses the issue of word-sense ambiguity in extraction from machine-readable resources for the construction of large-scale knowledge sources. We describe two experiments: one which ignored word-sense distinctions, resulting in 6.3% accuracy for semantic classification of verbs based on (Levin, 1993); and one which exploited word-sense distinctions, resulting in 97.9% accuracy. These experiments were dual purpose: (1) to validate the central thesis of the work of (Levin, 1993), i.e., that verb semantics and syntactic behavior are predictably related; (2) to demonstrate that a 15-fold improvement can be achieved in deriving semantic information from syntactic cues if we first divide the syntactic cues into distinct groupings that correlate with different word senses. Finally, we show that we can provide effective acquisition techniques for novel word senses using a combination of online sources."
J95-4008,Book Reviews: Compositional translation,1995,-1,-1,1,1,14512,bonnie dorr,Computational Linguistics,0,None
J95-2005,Squibs and Discussions: Efficient Parsing for {K}orean and {E}nglish: A Parameterized Message-Passing Approach,1995,11,6,1,1,14512,bonnie dorr,Computational Linguistics,0,None
J94-4004,Machine Translation Divergences: A Formal Description and Proposed Solution,1994,70,188,1,1,14512,bonnie dorr,Computational Linguistics,0,"There are many cases in which the natural translation of one language into another results in a very different form than that of the original. The existence of translation divergences (i.e., crosslinguistic distinctions) makes the straightforward transfer from source structures into target structures impractical. Many existing translation systems have mechanisms for handling divergent structures but do not provide a general procedure that takes advantage of takes advantage of the systematic relation between lexical-semantic structure and syntactic structure. This paper demonstrates that a systematic solution to the divergence problem can be derived from the formalization of two types of information: (1) the linguistically grounded classes upon which lexical-semantic divergences are based; and (2) the techniques by which lexical-semantic divergences are resolved. This formalization is advantageous in that it facilitates the design and implementation of the system, allows one to make an evaluation of the status of the system, and provides a basis for proving certain important properties about the system."
1994.amta-1.6,The Case for a {MT} Developers{'} Tool with a Two-Component View of the Interlingua,1994,-1,-1,1,1,14512,bonnie dorr,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1994.amta-1.20,A Parameter-Based Message-Passing Parser for {MT} of {K}orean and {E}nglish,1994,-1,-1,2,0,30549,dekang lin,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
1994.amta-1.27,Is {MT} Research Doing Any Good?,1994,-1,-1,2,0,3453,kenneth church,Proceedings of the First Conference of the Association for Machine Translation in the Americas,0,None
P92-1033,A Parameterized Approach to Integrating Aspect With Lexical-Semantics for Machine Translation,1992,21,13,1,1,14512,bonnie dorr,30th Annual Meeting of the Association for Computational Linguistics,1,"This paper discusses how a two-level knowledge representation model for machine translation integrates aspectual information with lexical-semantic information by means of parameterization. The integration of aspect with lexical-semantics is especially critical in machine translation because of the lexical selection and aspectual realization processes that operate during the production of the target-language sentence: there are often a large number of lexical and aspectual possibilities to choose from in the production of a sentence from a lexical semantic representation. Aspectual information from the source-language sentence constrains the choice of target-language terms. In turn, the target-language terms limit the possibilities for generation of aspect. Thus, there is a two-way communication channel between the two processes. This paper will show that the selection/realization processes may be parameterized so that they operate uniformly across more than one language and it will describe how the parameter-based approach is currently being used as the basis for extraction of aspectual information from corpora."
C92-2094,Parameterization of the Interlingua in Machine Translation,1992,13,12,1,1,14512,bonnie dorr,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The task of designing as interlingual machine translation system is difficult, first because the designer must have a knowledge of the principles underlying crosslinguistic distinctions for the languages under consideration, and second because the designer must then be able to incorporate this knowledge effectively into the system. This paper provides a catalog of several types of distinctions among Spanish, English, and German, and describes a parametric approach that characterizes these distinctions, both at the syntactic level and at the lexical-semantic level. The approach described here is implemented in a system called UNITRAN, a machine translation system that translates English, Spanish, and German bidirectionally."
W91-0222,A Two-Level Knowledge Representation for Machine Translation: Lexical Semantics and Tense/Aspect,1991,23,9,1,1,14512,bonnie dorr,Lexical Semantics and Knowledge Representation,0,"This paper proposes a two-level model that integrates contemporary theories of tense and aspect with lexical semantics. The model is intended to be extensible to realms outside of the temporal domain (e.g., the spatial domain). The integration of tense and aspect with lexical-semantics is especially critical in machine translation because of the lexical selection process during generation: there is often a number of lexical connective and tense/aspect possibilities that may be produced from a lexical semantic representation, which, as defined in the model presented here, is largely underspecified. Temporal/aspectual information from the source-language sentence constrains the choice of target-language terms. In turn, the target-language terms limit the possibilities for generation of tense and aspect. Thus, there is a two-way communication channel between the two processes."
P90-1017,Solving Thematic Divergences in Machine Translation,1990,10,31,1,1,14512,bonnie dorr,28th Annual Meeting of the Association for Computational Linguistics,1,"Though most translation systems have some mechanism for translating certain types of divergent predicate-argument structures, they do not provide a general procedure that takes advantage of the relationship between lexical-semantic structure and syntactic structure. A divergent predicate-argument structure is one in which the predicate (e.g., the main verb) or its arguments (e.g., the subject and object) do not have the same syntactic ordering properties for both the source and target language. To account for such ordering differences, a machine translator must consider language-specific syntactic idiosyncrasies that distinguish a target language from a source language, while making use of lexical-semantic uniformities that tie the two languages together. This paper describes the mechanisms used by the UNITRAN machine translation system for mapping an underlying lexical-conceptual structure to a syntactic structure (and vice versa), and it shows how these mechanisms coupled with a set of general linking routines solve the problem of thematic divergence in machine translation."
