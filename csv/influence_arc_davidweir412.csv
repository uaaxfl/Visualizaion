1995.iwpt-1.30,J94-1004,0,0.104021,"Missing"
1997.iwpt-1.6,J93-1002,1,0.880549,"Missing"
1997.iwpt-1.6,1995.iwpt-1.8,1,0.897603,"Missing"
1997.iwpt-1.6,H90-1021,0,0.0197493,"Missing"
1997.iwpt-1.6,J94-1004,0,0.288846,"Missing"
2020.coling-main.36,D14-1206,0,0.026503,"2007). A more granular approach is to use specific HTML tags that are deemed text containing (Bunescu, 2007; Sarode et al., 2019), further filtering may then be applied (Kohlsch¨utter et al., 2010; Kim, 2017). To include HTML tags in sentence based NER, we have a range of options. Blohm (2011) and Miro´nczuk (2018) simplify tags, removing attributes and style, including the tag as one standard token. Soderland (1999) takes a similar approach, including attributes, splitting into tokens by whitespace. Freitag (1998) engineers features to represent aspects of HTML positions relative to a token. Apostolova and Tomuro (2014) combine visual and textual page content into engineered features for NER. Gogar et al. (2016) use a CNN to automatically extract similar features. 3 Approach We conduct NER experiments on the five datasets detailed below. These datasets allow sentences, including HTML tags, to be extracted from the Web while applying a supplied gold standard set of entity label annotations or Distant Supervision. We extract two sets of free text elements and their inner HTML contents from the &lt;body> of raw Web pages, removing script and comment blocks. Set1 contains &lt;p> tags and contents, Set2 contains &lt;p> an"
2020.coling-main.36,U17-1010,0,0.0150489,"t be included in other Websourced NLP tasks, such as Knowledge Base Population or integrated into deep language models trained on the Web. 2 Related Work To extract free text from a Web page, the level of granularity of free text containing elements must be decided. Approaches to free text segmentation range from indiscriminate whole-page approaches to stripping HTML (Bird et al., 2009; Richardson, 2007). A more granular approach is to use specific HTML tags that are deemed text containing (Bunescu, 2007; Sarode et al., 2019), further filtering may then be applied (Kohlsch¨utter et al., 2010; Kim, 2017). To include HTML tags in sentence based NER, we have a range of options. Blohm (2011) and Miro´nczuk (2018) simplify tags, removing attributes and style, including the tag as one standard token. Soderland (1999) takes a similar approach, including attributes, splitting into tokens by whitespace. Freitag (1998) engineers features to represent aspects of HTML positions relative to a token. Apostolova and Tomuro (2014) combine visual and textual page content into engineered features for NER. Gogar et al. (2016) use a CNN to automatically extract similar features. 3 Approach We conduct NER experi"
2020.coling-main.36,P16-1101,0,0.0320854,"n’s employing organisation, rather than the person themselves. Organisations and corresponding key persons are extracted from DBpedia, with the top-5 Web search results from the organisation name processed. Each result is processed three pages deep, with the same matching and exclusion criteria as Persons. This set is evaluated using the same set as Persons. This is a smaller task-focussed set; the three page deep processing is likely to hit more of the types of pages that were labelled in our evaluation set. 3.2 Models We use two recent state-of-the-art NLP models. A Bi-LSTM+CNN+CRF based on Ma and Hovy (2016), with Word2Vec skip-gram embeddings (Mikolov et al., 2013), generated with Gensim (Rehurek and Sojka, 2010) over the five datasets. We generated four variants of embeddings to cover tag Set1 and Set2, Text+Tags and Text-Only, using 100 dimensions, 20 iterations and window 5. We also included pre-trained Stanford GloVe embeddings (Pennington et al., 2014) as a baseline. For our second model, we fine-tuned a BERT (Devlin et al., 2018) transformer using the Hugging Face (Wolf et al., 2019) bertbase-cased language model. These models and embeddings provide a good contrast for our experiments. 4 R"
2020.coling-main.36,P09-1113,0,0.262981,"Missing"
2020.coling-main.36,W13-2001,0,0.0472703,"Missing"
2020.coling-main.36,D14-1162,0,0.0834644,"Persons. This is a smaller task-focussed set; the three page deep processing is likely to hit more of the types of pages that were labelled in our evaluation set. 3.2 Models We use two recent state-of-the-art NLP models. A Bi-LSTM+CNN+CRF based on Ma and Hovy (2016), with Word2Vec skip-gram embeddings (Mikolov et al., 2013), generated with Gensim (Rehurek and Sojka, 2010) over the five datasets. We generated four variants of embeddings to cover tag Set1 and Set2, Text+Tags and Text-Only, using 100 dimensions, 20 iterations and window 5. We also included pre-trained Stanford GloVe embeddings (Pennington et al., 2014) as a baseline. For our second model, we fine-tuned a BERT (Devlin et al., 2018) transformer using the Hugging Face (Wolf et al., 2019) bertbase-cased language model. These models and embeddings provide a good contrast for our experiments. 4 Results and Analysis Our results in Table 2 show increased F1 performance for Text+Tags over every dataset, tagset and model. 1 https://github.com/dstl/re3d https://archive.codeplex.com/?p=swde 3 http://www.dia.uniroma3.it/db/weir/ 2 409 Dataset.Tagset Text-Only LSTM GloVe W2V OrgPersons.1 OrgPersons.2 Persons.1 Persons.2 RE3D.1 RE3D.2 SWDE.1 SWDE.2 WEIR.1"
2020.coling-main.36,I08-5007,0,0.102419,"Missing"
2021.eacl-main.89,2020.acl-main.191,0,0.0430949,", to the existing training set of D. We ensure that the data augmentation does not introduce any lexical overlap with the existing test set, i.e. X ∩ X 0 = ∅. Data augmentation strategies in NLP can roughly be divided into two categories: linguistically grounded augmentation and artificial augmentation. In the former, which has been the dominant paradigm in NLP, any additional instances that are added to a training set have an actual surface form representation, i.e. the data points correspond to actual words or sentences (Kim et al., 2019; Kumar et al., 2019; Gao et al., 2019; Andreas, 2020; Croce et al., 2020). The latter adds instances that are fully or partly artificial, meaning they do not correspond to any words or sentences. In this work we propose methods for both categories, data augmentation via distributional composition adds data points grounded in real language to a training set, and data augmentation based on GANs infers plausible points in latent space, which however, do not correspond to any real linguistic objects. Furthermore, we distinguish between data augmentation and dataset extension, where in the former case we only leverage knowledge from the existing dataset and in the latte"
2021.eacl-main.89,P17-2090,0,0.0246971,"ntified two novel data augmentation techniques for the task of hypernymy detection which have the potential to generate almost limitless quantities of synthetic data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this w"
2021.eacl-main.89,Q15-1016,0,0.0536941,"ker association with the query word, but are frequently still related to it somehow as in the example of akpeteshie, which is a spirit on sugar cane basis, as a neighbour for sugar. 5.2 Data Augmentation as Regularisation In the past, a prominent criticism of distributional methods for hypernymy detection was that such models were found to frequently identify features of a prototypical hypernym in the distributional representations, rather than being able to dynamically focus on the relevant features that are indicative of a hypernymy relation for a specific pair of words (Weeds et al., 2014; Levy et al., 2015b). We therefore briefly investigate whether data augmentation can be used as a regularisation mechanism that helps prevent models from overfitting on prototypical hypernym features. Table 4 shows the results on the Weeds dataset using a hypernym-only FF model with word2vec representations, in comparison to the same model variant that makes use of the hyponym and the hypernym. Ideally, we would hope to see weak performance for the hypernym-only and strong performance on the full model. This would indicate that the classifier does not rely on prototypical features in the hypernym, but is able t"
2021.eacl-main.89,H93-1061,0,0.0971793,"Missing"
2021.eacl-main.89,N15-1098,0,0.023482,"ker association with the query word, but are frequently still related to it somehow as in the example of akpeteshie, which is a spirit on sugar cane basis, as a neighbour for sugar. 5.2 Data Augmentation as Regularisation In the past, a prominent criticism of distributional methods for hypernymy detection was that such models were found to frequently identify features of a prototypical hypernym in the distributional representations, rather than being able to dynamically focus on the relevant features that are indicative of a hypernymy relation for a specific pair of words (Weeds et al., 2014; Levy et al., 2015b). We therefore briefly investigate whether data augmentation can be used as a regularisation mechanism that helps prevent models from overfitting on prototypical hypernym features. Table 4 shows the results on the Weeds dataset using a hypernym-only FF model with word2vec representations, in comparison to the same model variant that makes use of the hyponym and the hypernym. Ideally, we would hope to see weak performance for the hypernym-only and strong performance on the full model. This would indicate that the classifier does not rely on prototypical features in the hypernym, but is able t"
2021.eacl-main.89,D19-1570,0,0.0147497,"niques for the task of hypernymy detection which have the potential to generate almost limitless quantities of synthetic data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this work represents the first application"
2021.eacl-main.89,D19-5543,0,0.0225224,"uantities of synthetic data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this work represents the first application of data augmentation for lexical entailment, a number of alternative approaches have been proposed. M"
2021.eacl-main.89,N19-1117,0,0.0176848,"c data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this work represents the first application of data augmentation for lexical entailment, a number of alternative approaches have been proposed. Most proposals rely"
2021.eacl-main.89,W02-0109,0,0.36524,"Missing"
2021.eacl-main.89,D19-5536,0,0.0347365,"Missing"
2021.eacl-main.89,D17-1022,0,0.0352562,"Missing"
2021.eacl-main.89,P18-2101,0,0.0759725,"similar words, they do not straightforwardly provide a way to distinguish more fine-grained semantic information, such as hypernymy, co-hyponymy and meronymy relationships. This deficiency has attracted substantial attention in the literature, and with regard to the task of hypernymy detection, both unsupervised approaches (Hearst, 1992; Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Rimell, 2014; Nguyen et al., 2017; Chang et al., 2018) and supervised approaches (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Shwartz et al., 2016; Vuli´c and Mrkˇsi´c, 2018; Rei et al., 2018; Kamath et al., 2019) have been proposed. Supervised methods have, however, been severely hampered by a lack of adequate training data. Not only has a paucity of labelled data been an obstacle in the adoption of deep neural networks and other more complex supervised methods, but two compounding problem-specific issues have been identified. First, there is a need to avoid lexical overlap between the training and test sets in order to avoid the lexical memorisation problem (Weeds et al., 2014; Levy et al., 2015a), where a supervised method simply learns the relationships between lexemes rather"
2021.eacl-main.89,E14-1054,0,0.0257918,"of practical applications in a variety of domains such as Healthcare (Bariseviˇcius et al., 2018) or Fashion1 . While distributed representations of words are commonly used to find semantically similar words, they do not straightforwardly provide a way to distinguish more fine-grained semantic information, such as hypernymy, co-hyponymy and meronymy relationships. This deficiency has attracted substantial attention in the literature, and with regard to the task of hypernymy detection, both unsupervised approaches (Hearst, 1992; Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Rimell, 2014; Nguyen et al., 2017; Chang et al., 2018) and supervised approaches (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Shwartz et al., 2016; Vuli´c and Mrkˇsi´c, 2018; Rei et al., 2018; Kamath et al., 2019) have been proposed. Supervised methods have, however, been severely hampered by a lack of adequate training data. Not only has a paucity of labelled data been an obstacle in the adoption of deep neural networks and other more complex supervised methods, but two compounding problem-specific issues have been identified. First, there is a need to avoid lexical overlap between the"
2021.eacl-main.89,C14-1097,0,0.0275285,"Fashion1 . While distributed representations of words are commonly used to find semantically similar words, they do not straightforwardly provide a way to distinguish more fine-grained semantic information, such as hypernymy, co-hyponymy and meronymy relationships. This deficiency has attracted substantial attention in the literature, and with regard to the task of hypernymy detection, both unsupervised approaches (Hearst, 1992; Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Rimell, 2014; Nguyen et al., 2017; Chang et al., 2018) and supervised approaches (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Shwartz et al., 2016; Vuli´c and Mrkˇsi´c, 2018; Rei et al., 2018; Kamath et al., 2019) have been proposed. Supervised methods have, however, been severely hampered by a lack of adequate training data. Not only has a paucity of labelled data been an obstacle in the adoption of deep neural networks and other more complex supervised methods, but two compounding problem-specific issues have been identified. First, there is a need to avoid lexical overlap between the training and test sets in order to avoid the lexical memorisation problem (Weeds et al., 2014; Levy et al.,"
2021.eacl-main.89,D16-1234,0,0.107844,"ributed representations of words are commonly used to find semantically similar words, they do not straightforwardly provide a way to distinguish more fine-grained semantic information, such as hypernymy, co-hyponymy and meronymy relationships. This deficiency has attracted substantial attention in the literature, and with regard to the task of hypernymy detection, both unsupervised approaches (Hearst, 1992; Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Rimell, 2014; Nguyen et al., 2017; Chang et al., 2018) and supervised approaches (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Shwartz et al., 2016; Vuli´c and Mrkˇsi´c, 2018; Rei et al., 2018; Kamath et al., 2019) have been proposed. Supervised methods have, however, been severely hampered by a lack of adequate training data. Not only has a paucity of labelled data been an obstacle in the adoption of deep neural networks and other more complex supervised methods, but two compounding problem-specific issues have been identified. First, there is a need to avoid lexical overlap between the training and test sets in order to avoid the lexical memorisation problem (Weeds et al., 2014; Levy et al., 2015a), where a superv"
2021.eacl-main.89,P18-2057,0,0.0126991,"orpus. We construct negative pairs based on distributional similarity, where we calculate the pairwise cosine similarities between all lexemes in the positive set. Subsequently we use all antecedent (LHS) lexemes from the extracted positive pairs and select the top n most similar words for each antecedent as negative examples6 . Hearst PatPattern based Extension. terns (Hearst, 1992) are textual patterns such as a car is-a vehicle and can be automatically mined from text corpora in an unsupervised way. This has recently been shown to deliver strong performance on the hypernymy detection task (Roller et al., 2018). In this work, we leverage Hearst Patterns to mine additional hyponym-hypernym pairs in order to extend a training set. We treat any extracted noun pairs as additional positive examples and create the negative pairs in the same way as for the WordNet-based approach above. 4 Experiments We evaluate our models on the datasets Weeds (Weeds et al., 2014) and LEDS (Baroni et al., 2012): well-studied and frequently used benchmarks for the hypernymy detection task (Roller et al., 2014; Vilnis and McCallum, 2014; Roller and Erk, 2016; Carmona and Riedel, 2017; Shwartz et al., 2017). Since both datase"
2021.eacl-main.89,E14-4008,0,0.0737305,"es, which has a range of practical applications in a variety of domains such as Healthcare (Bariseviˇcius et al., 2018) or Fashion1 . While distributed representations of words are commonly used to find semantically similar words, they do not straightforwardly provide a way to distinguish more fine-grained semantic information, such as hypernymy, co-hyponymy and meronymy relationships. This deficiency has attracted substantial attention in the literature, and with regard to the task of hypernymy detection, both unsupervised approaches (Hearst, 1992; Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Rimell, 2014; Nguyen et al., 2017; Chang et al., 2018) and supervised approaches (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Shwartz et al., 2016; Vuli´c and Mrkˇsi´c, 2018; Rei et al., 2018; Kamath et al., 2019) have been proposed. Supervised methods have, however, been severely hampered by a lack of adequate training data. Not only has a paucity of labelled data been an obstacle in the adoption of deep neural networks and other more complex supervised methods, but two compounding problem-specific issues have been identified. First, there is a need to avoid lexical overl"
2021.eacl-main.89,P16-1009,0,0.0332467,"old. First, we have identified two novel data augmentation techniques for the task of hypernymy detection which have the potential to generate almost limitless quantities of synthetic data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of"
2021.eacl-main.89,P16-1226,0,0.0403506,"Missing"
2021.eacl-main.89,E17-1007,0,0.0336138,"Missing"
2021.eacl-main.89,N18-1103,0,0.0394837,"Missing"
2021.eacl-main.89,D18-1100,0,0.0152288,"a augmentation techniques for the task of hypernymy detection which have the potential to generate almost limitless quantities of synthetic data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this work represents the"
2021.eacl-main.89,C14-1212,1,0.857458,"us et al., 2018) or Fashion1 . While distributed representations of words are commonly used to find semantically similar words, they do not straightforwardly provide a way to distinguish more fine-grained semantic information, such as hypernymy, co-hyponymy and meronymy relationships. This deficiency has attracted substantial attention in the literature, and with regard to the task of hypernymy detection, both unsupervised approaches (Hearst, 1992; Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Rimell, 2014; Nguyen et al., 2017; Chang et al., 2018) and supervised approaches (Weeds et al., 2014; Roller et al., 2014; Roller and Erk, 2016; Shwartz et al., 2016; Vuli´c and Mrkˇsi´c, 2018; Rei et al., 2018; Kamath et al., 2019) have been proposed. Supervised methods have, however, been severely hampered by a lack of adequate training data. Not only has a paucity of labelled data been an obstacle in the adoption of deep neural networks and other more complex supervised methods, but two compounding problem-specific issues have been identified. First, there is a need to avoid lexical overlap between the training and test sets in order to avoid the lexical memorisation problem (Weeds et al."
2021.eacl-main.89,C04-1146,1,0.694008,"Missing"
2021.eacl-main.89,D19-1670,0,0.0207131,"almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this work represents the first application of data augmentation for lexical entailment, a number of alternative approaches have been proposed. Most proposals rely on supervised methods for injecting an exter1035 nal source of knowledge into distributional representations. Starting with re"
2021.eacl-main.89,P19-1579,0,0.0241951,"ntial to generate almost limitless quantities of synthetic data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this work represents the first application of data augmentation for lexical entailment, a number of alter"
2021.eacl-main.89,D19-5218,0,0.0281408,"sk of hypernymy detection which have the potential to generate almost limitless quantities of synthetic data. Second, we show, rather surprisingly, that adding synthetic data is more effective than adding non-synthetic data in almost all cases. Third, we release a new benchmark evaluation dataset for the lexical entailment task that is not dependent on WordNet. 2 Related Work Data augmentation has recently become a very popular research topic in NLP and has successfully been applied in machine translation systems (Sennrich et al., 2016; Fadaee et al., 2017; Wang et al., 2018; Li et al., 2019; Tong et al., 2019; Matos Veliz et al., 2019; Xia et al., 2019; Gao et al., 2019; Li and Specia, 2019; Liu et al., 2019), but also for tasks such as relation extraction (Can et al., 2019; Yan et al., 2019), text classification (Wei and Zou, 2019), or natural language inference (Kang et al., 2018; Junghyun et al., 2020). Most similar to our usage of GANs for data augmentation is the proposal of Kang et al. (2018) who leverage a GANbased setup together with WordNet for data augmentation for natural language inference. While to the best of our knowledge this work represents the first application of data augmentati"
2021.findings-acl.296,D12-1091,0,0.0379087,"n. Again, DM is the model showing the highest variation in results. This provides further evidences in favour of the lightweight models taken from the KG literature 4.3 Statistical Analysis All correlations were tested for significance, adopting the Holm correction (Holm, 1979) to account for the large number of tests, and we observed no p &lt; .05. As the main interest of our work was the compositional investigation (reported in Table 5), a global comparison was conducted to test whether observed differences in correlations were also significant. We adopted a paired two-tail bootstrap analysis (Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), performed independently between results from the three seeds. Given the large number of comparisons, a Holm correction was adopted within the same Phrase Type. Results (see A.3 for more details) showed that, among all models, the only one that generated a number of insignificant differences was DM, mainly pertaining to different strategies for composing NN items. 4.4 Qualitative Analysis We now investigate the impact of relation representations on word vectors and composition from a qualitative point of view. Here, we focus on the model that quantita"
2021.findings-acl.296,2020.acl-main.617,0,0.0812848,"Missing"
2021.findings-acl.296,2020.acl-main.493,0,0.0210484,"suggest that the three syntactic relations adopted for contextualisation (i.e. amod, dobj, 3349 (a) syn-Rt (b) syn-Rh (c) syn-BiD Figure 1: PCA visualisation of RefE vector space. Images show the same word (•) and add-composed vectors (⇥), in the context of representations composed with the four different syntax-aware (⌅) composition methods. All composed vectors represent the set of phrases from the Mitchell and Lapata (2010) benchmark. nmod) appear to generate as many distinguishable clusters. Despite being limited, these results support evidence for syntactic subspace probed out of mBert (Chi et al., 2020). and mostly in Figure 1c, phrase representations obtained via simple addition mainly lie within the perimeter of the word space. A similar pattern is observed in Figure 1a, with syn-Rt. Phrases composed by using the root as the head of the triple are still fairly close to the word-space perimeter, but tend to abandon its centre. Lastly, Figure 1c shows how bi-directional representations lie scattered fairly distant from the word and add-composed representations. This last observation is contrary to theories suggesting that representations at every level (word, phrase, sentence, etc..) should"
2021.findings-acl.296,W19-0408,0,0.165769,"etric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation. 1 Introduction Representing words in terms of their syntactic co-occurrences has been long proposed, both for count-based (Pad´o and Lapata, 2007; Weir et al., 2016), and neural (Hermann and Blunsom, 2013; Levy and Goldberg, 2014; Komninos and Manandhar, 2016; Czarnowska et al., 2019; Vashishth et al., 2019) models of word meaning. Tested on benchmark word similarity tasks, such models often perform favourably to models based on proximal co-occurrence, particularly when the similarity or substitutability of two words is considered rather than their relatedness (Levy and Goldberg, 2014). However, the real promise of distributional models based on syntactic rather than proximal co-occurrence, is the potential for carrying out syntax-sensitive composition. For example, in the Anchored Packed Tree (APT) model (Weir et al., 2016) lexemes, phrases, and sentences are represented"
2021.findings-acl.296,P18-1128,0,0.0159719,"on in results. This provides further evidences in favour of the lightweight models taken from the KG literature 4.3 Statistical Analysis All correlations were tested for significance, adopting the Holm correction (Holm, 1979) to account for the large number of tests, and we observed no p &lt; .05. As the main interest of our work was the compositional investigation (reported in Table 5), a global comparison was conducted to test whether observed differences in correlations were also significant. We adopted a paired two-tail bootstrap analysis (Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), performed independently between results from the three seeds. Given the large number of comparisons, a Holm correction was adopted within the same Phrase Type. Results (see A.3 for more details) showed that, among all models, the only one that generated a number of insignificant differences was DM, mainly pertaining to different strategies for composing NN items. 4.4 Qualitative Analysis We now investigate the impact of relation representations on word vectors and composition from a qualitative point of view. Here, we focus on the model that quantitative tests indicated as the most promising"
2021.findings-acl.296,2020.acl-main.367,0,0.0188589,"ences are represented as collections of typed occurrences, and composition is carried out by contextualising each element in its syntactic role. This leads to syntaxsensitive representations for phrases. For example, glass window and window glass have different representations due to the different syntactic roles played by each constituent. Alongside count-based models, a variety of neural ones have been proposed to encode syntactic structure, focusing on different depths of the graph (Levy and Goldberg, 2014; Komninos and Manandhar, 2016; Marcheggiani and Titov, 2017; Vashishth et al., 2019; Emerson, 2020)). Of particular note here, Levy and Goldberg (2014) and Komninos and Manandhar (2016) each proposed models (DEP and EXT, respectively) which learn from local dependency relations, by extending the Skip-Gram with Negative sampling (SGNS) architecture from word2vec (Mikolov et al., 2013). Given a tuple of (target, context) words, e.g. (rain,like), a standard SGNS model can be trained to encode the probability of it being a true or a randomly sampled tuple. DEP and EXT, on the other hand, make use of both standard and syntactically contextualised tuples e.g., (rain dobj, like)1 . Whilst DEP was"
2021.findings-acl.296,P13-1088,0,0.0327696,"ture, on the other hand, has proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation. 1 Introduction Representing words in terms of their syntactic co-occurrences has been long proposed, both for count-based (Pad´o and Lapata, 2007; Weir et al., 2016), and neural (Hermann and Blunsom, 2013; Levy and Goldberg, 2014; Komninos and Manandhar, 2016; Czarnowska et al., 2019; Vashishth et al., 2019) models of word meaning. Tested on benchmark word similarity tasks, such models often perform favourably to models based on proximal co-occurrence, particularly when the similarity or substitutability of two words is considered rather than their relatedness (Levy and Goldberg, 2014). However, the real promise of distributional models based on syntactic rather than proximal co-occurrence, is the potential for carrying out syntax-sensitive composition. For example, in the Anchored Packed Tree"
2021.findings-acl.296,D15-1162,0,0.0339875,"https://github.com/lorenzoscottb/ findings_ACL2021 5 using the dataset’s original splits. 3347 MuRE RotE RefE AttE Simlex .38±.01 .35±.01 .36±.01 .36±.01 MEN .45±.00 .54±.00 .54±.00 .54±.00 WS s .42±.01 .59±.00 .57±.00 .58±.01 WS r .21±.03 .30±.02 .30±.01 .29±.00 Adjective Nouns .19±.03 .18±.03 .16±.04 .20±.00 Verb Objects .31±.00 .33±.00 .37±.01 .32±.00 Noun-Noun .13±.01 .20±.02 .14±.02 .18±.00 Table 3: Spearman ⇢s’ (mean ± SE) obtained on all selected benchmarks, for knowledge-graph models trained on WN18RR dataset. A second set of models was trained on the text86 corpus, parsed with spaCy (Honnibal and Johnson, 2015). Following Czarnowska et al. (2019), minimum item count, epochs, NS, optimiser and learning rate were fine-tuned on SimLex. Hyperparameters are selected from the union of the ones proposed in (Balazevic et al., 2019; Czarnowska et al., 2019; Chami et al., 2020). All the models share the same number of dimensions, i.e., n = 300. For a fair comparison, all experiments for this set have been conducted on the vocabulary shared across the models. Final coverage and best hyperparamenters are reported in Appendix A.2 and A.1. All models were trained using NVIDIA Titan V GPUs. 4.2 Word similarity Our"
2021.findings-acl.296,N16-1175,0,0.0457592,"Missing"
2021.findings-acl.296,P14-2050,0,0.255035,"s proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation. 1 Introduction Representing words in terms of their syntactic co-occurrences has been long proposed, both for count-based (Pad´o and Lapata, 2007; Weir et al., 2016), and neural (Hermann and Blunsom, 2013; Levy and Goldberg, 2014; Komninos and Manandhar, 2016; Czarnowska et al., 2019; Vashishth et al., 2019) models of word meaning. Tested on benchmark word similarity tasks, such models often perform favourably to models based on proximal co-occurrence, particularly when the similarity or substitutability of two words is considered rather than their relatedness (Levy and Goldberg, 2014). However, the real promise of distributional models based on syntactic rather than proximal co-occurrence, is the potential for carrying out syntax-sensitive composition. For example, in the Anchored Packed Tree (APT) model (Weir et al."
2021.findings-acl.296,D17-1159,0,0.117909,") model (Weir et al., 2016) lexemes, phrases, and sentences are represented as collections of typed occurrences, and composition is carried out by contextualising each element in its syntactic role. This leads to syntaxsensitive representations for phrases. For example, glass window and window glass have different representations due to the different syntactic roles played by each constituent. Alongside count-based models, a variety of neural ones have been proposed to encode syntactic structure, focusing on different depths of the graph (Levy and Goldberg, 2014; Komninos and Manandhar, 2016; Marcheggiani and Titov, 2017; Vashishth et al., 2019; Emerson, 2020)). Of particular note here, Levy and Goldberg (2014) and Komninos and Manandhar (2016) each proposed models (DEP and EXT, respectively) which learn from local dependency relations, by extending the Skip-Gram with Negative sampling (SGNS) architecture from word2vec (Mikolov et al., 2013). Given a tuple of (target, context) words, e.g. (rain,like), a standard SGNS model can be trained to encode the probability of it being a true or a randomly sampled tuple. DEP and EXT, on the other hand, make use of both standard and syntactically contextualised tuples e."
2021.findings-acl.296,J15-4004,0,0.0146859,"ata to obtain competitive results. We hypothesise that when using KGs alone: i) word similarity tasks might yield high results; ii) compositional evaluation will yield poor results. As for models trained on SyG, we expect to see: i) a generally improved performance on most tasks, when compared to models trained on KGs; ii) larger models to be penalised across benchmarks and for syntactically-contextualised (syn) composition. 4.1 Experimental setup Benchmarks We divide our quantitative experiments between word similarity and composition tasks. For the word similarity tasks, we focus on SimLex (Hill et al., 2015), MEN (Bruni et al., 2014), and both similarity (WS s) and relatedness (WS r) split of the WordSim353 (Finkelstein et al., 2001) datasets. For every word pair, we produce a model’s prediction using cosine similarity (CS). We compare model predictions and human judgements using Spearman’s ⇢. For the compositional investigation, we focus on the Mitchell and Lapata (2010) (ML10) dataset. Items in this benchmark consist of pairs of twotoken phrases (e.g. (pour tea–drink water)) paired with human judgements on their similarity. Phrases are composed using the four different presented strategies and"
2021.findings-acl.296,J07-2002,0,0.17151,"Missing"
2021.findings-acl.296,N18-1202,0,0.0565898,"linguistic item. In our example, to compare pour tea with drink water, this would require us to consider the syntactic root in the context of its dependent i.e., how similar is the verb pour when contextualised by the direct 3345 object tea to the verb drink when contextualised by the direct object water? In models which modify the head of the triple (e.g., (Chami et al., 2020), this would correspond to using the root-astail (Rt) analysis of the phrase. Here, we compare the two strategies empirically. Further, inspired by the growing success of (very large) bi-directional models such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2018) and also by recent evidence from the neuroscientific literature (Mollica et al., 2020; Fedorenko et al., 2020), suggesting that sentence processing strongly relies on identifying and composing smaller units of meaning, such as phrases, regardless of order of their constituents, we also propose a third compositional strategy which is bi-directional in nature. Here, the phrase-level representation is the sum of the root-as-head and the root-as-tail representations, making it more agnostic to the direction of the relation as well as the word order. However, phrases"
2021.findings-acl.296,2020.acl-srw.42,0,0.0995935,"Missing"
2021.findings-acl.296,2020.findings-emnlp.252,0,0.0388452,"tions, generally gave a further boost to performance. This is further evidence that bi-directional information is more informative than uni-directional information, not just in large neural models such as LSTMs and transformers, and supports recent theory from neuroscience which argues that what is crucial for composition is not the overall structure nor the root, but that we can identify a phrase’s constituents and the relation they have (Mollica et al., 2020). Evidence in favour of the fact that composition strongly relies on local dependencies based on syntactic structure was also found by Saphra and Lopez (2020). Such work suggests that LSTMs learn to compose following a hierarchical structure, driven by syntax, and that they rely on the learned short sequences to build longer and more reliable ones. Taken altogether, the evidence from different language-related fields is becoming more compelling that syntax and phrase composition should play an important role in the composition of larger units of meaning. 6 Conclusions and Further Work We have shown how GT models previously proposed for encoding KGs can be adapted to encode syntactic information in a distributional model. We have demonstrated the hi"
2021.findings-acl.296,W14-1601,0,0.0143437,"ng the highest variation in results. This provides further evidences in favour of the lightweight models taken from the KG literature 4.3 Statistical Analysis All correlations were tested for significance, adopting the Holm correction (Holm, 1979) to account for the large number of tests, and we observed no p &lt; .05. As the main interest of our work was the compositional investigation (reported in Table 5), a global comparison was conducted to test whether observed differences in correlations were also significant. We adopted a paired two-tail bootstrap analysis (Berg-Kirkpatrick et al., 2012; Søgaard et al., 2014; Dror et al., 2018), performed independently between results from the three seeds. Given the large number of comparisons, a Holm correction was adopted within the same Phrase Type. Results (see A.3 for more details) showed that, among all models, the only one that generated a number of insignificant differences was DM, mainly pertaining to different strategies for composing NN items. 4.4 Qualitative Analysis We now investigate the impact of relation representations on word vectors and composition from a qualitative point of view. Here, we focus on the model that quantitative tests indicated a"
2021.findings-acl.296,J16-4006,1,0.81899,"ps. The knowledge graphs’ literature, on the other hand, has proposed light-weight models employing different geometric transformations (GTs) to encode edges in a knowledge graph (KG). Our work explores the possibility of adopting this family of models to encode SyGs. Furthermore, we investigate which GT better encodes syntactic relations, so that these representations can be used to enhance phrase-level composition via syntactic contextualisation. 1 Introduction Representing words in terms of their syntactic co-occurrences has been long proposed, both for count-based (Pad´o and Lapata, 2007; Weir et al., 2016), and neural (Hermann and Blunsom, 2013; Levy and Goldberg, 2014; Komninos and Manandhar, 2016; Czarnowska et al., 2019; Vashishth et al., 2019) models of word meaning. Tested on benchmark word similarity tasks, such models often perform favourably to models based on proximal co-occurrence, particularly when the similarity or substitutability of two words is considered rather than their relatedness (Levy and Goldberg, 2014). However, the real promise of distributional models based on syntactic rather than proximal co-occurrence, is the potential for carrying out syntax-sensitive composition. F"
2021.findings-acl.296,2020.acl-main.341,0,0.0278005,"dels to encode syntactic graphs. We focus our investigation on four state of the art models from the knowledge-graphs literature, namely MuRE (Balazevic et al., 2019), and the three GTs-based models proposed by Chami et al. (2020): RotE, RefE and AttE. Despite the simplicity, MuRE has obtained competitive results, when compared to more complex models (Chami et al., 2020)). Rotation has been used to model composition of relation representations (Sun et al., 2019). Attention has been frequently proposed as a plausible mechanism for composition (e.g. Hudson and Manning (2018); Tay et al. (2019); Yin et al. (2020); Russin et al. (2020)), whilst reflection is relatively under-studied (Chami et al., 2020). Furthermore, as discussed in Section 3, these models allow for an interesting comparison, as they can be grouped into three categories: tail modifiers (DM), head modifiers (RotE, RefE, AttE), and full modifiers (MuRE). Hence, we explore some of the transformational properties required to enable the successful encoding of syntactic relations, where success is defined in terms of their potential to support phrasal composition. Our contributions are as follows. First, we show how lighter-weight models bas"
2021.findings-acl.296,W19-4329,0,0.044782,"Missing"
2021.findings-acl.296,W15-4007,0,0.217719,"driven strategies for composition with simple addition. Third, we provide an analysis of which type of GTs better encode relations for syntactic contextualisation and enhanced composition. 3344 2 Related Work Knowledge graphs are complex data structures where nodes are concepts or entities (usually content words like dog or Campari) and edges are relations (e.g. is a, produced in) connecting entities to one another (e.g. dog is a mammal, Campari produced in Italy). Table 2 reports the number of distinct entities, relations and triples for three of the most investigated KGs, namely, FB15k-237 (Toutanova and Chen, 2015) YAGO3-10 (Mahdisoltani et al., 2015), and WN18RR (Dettmers et al., 2018), as well as a syntactic graph (SyG) constructed from the parsed corpus text8. The way these graphs are structured can vary significantly. Chami et al. (2020) showed how, among the presented KGs, only WN18RR has a significantly hierarchical structure. Dataset WNRR18 FB15k-237 YAGO3-10 text8 entities 31k 15k 123k 72k relations 11 237 33 88 triples 87k 272k 1M 12M⇤ graph type KG KG KG SyG Table 2: Statistics for the training splits of different datasets (* number of unique items, with observed repetitions, items raise to 18"
2021.findings-acl.296,D15-1174,0,0.161215,"n different types of geometric transformations (GTs). These include, but are not limited to, stretch (Balazevic et al., 2019), rotation (Sun et al., 2019; Chami et al., 2020), reflection (Chami et al., 2020) and attention (Chami et al., 2020). However, in the KG literature, limited attention has been paid to the compositional nature of phrases. Single-token oriented vocabularies (where New York is represented by New York), used in most KGs, work well for realworld entities, such as people or cities, but are problematic when considering compositional phrases such as small cake. As discussed by Toutanova et al. (2015), treating these phrases in the same way forces the vocabulary to grow immensely, and prevents the model from reasoning over new phrases in a compositional fashion. Hence, developing successful composition strategies is of interest to the KG community as well as more widely in Natural Language Inference (NLI). Given the success that DM and other models have obtained in modelling syntax and syntactically driven composition, we propose to overcome the parameter and word-relation vocabulary problems by using GT models to encode syntactic graphs. We focus our investigation on four state of the art"
2021.findings-acl.296,P16-1136,0,0.0607084,"Missing"
2021.repl4nlp-1.7,S17-2001,0,0.046327,"Missing"
2021.repl4nlp-1.7,D18-2029,0,0.0148146,"omparable sentence representations. The model we propose consists of two components, as shown in Figure 1. Related Work Sentence encoders have been studied extensively in years. Skip-Thought (Kiros et al., 2015) has been trained to predict its surrounding sentences by using current sentence in a self-supervised fashion. Hill et al. (2016) proposed a sequential denoising autoencoder (SDAE) to reconstruct given sentence representations. InferSent (Conneau et al., 2017), on the other hand, used labelled NLI datasets to train a general-purpose sentence encoder in a BiLSTM-based siamese structure. Cer et al. (2018) proposed the Universal Sentence Encoder (USE) model based on transformers (Vaswani et al., 2017), and trained it with both unsupervised tasks and supervised NLI tasks. Inspired by InferSent, Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) produces general-purpose sentence embeddings by fine-tuning BERT on NLI datasets in a siamese structure, showing improved performance on a variety of tasks. Hidden syntax structures in pre-trained models have been well explored. Various probing methods have been used to investigate hidden structures (Clark et al., 2019; Hewitt and Manning, 2019; Jawahar e"
2021.repl4nlp-1.7,S14-2010,0,0.0632017,"Missing"
2021.repl4nlp-1.7,K19-2007,0,0.0262981,"semantic graphs to be less effective than syntactic dependency trees when evaluated on our development set, and as a result, in the experiments below, we restrict our attention to the use of syntactic dependency graphs. RGCN: RGCNs, proposed by (Schlichtkrull et al., 2018), can be viewed as a weighted message passing process. At each RGCN layer, each node’s representation will be updated by collecting information from its neighbours and applying edge-specific weighting: X X 1 hl+1 = ReLU (W0l hli + Wrl hlj ) (1) i c r∈R j∈N r i,r i 1 For semantic graphs, we use the semantic parser produced by Che et al. (2019). 58 where Nir and Wrl are the neighbours of node i and the weight of relation r ∈ R, respectively. ci,r is the normalisation constant and normally set to be |Nir |which is the number of neighbours under relation r. W0l is the self-loop weight. In our case, each sentence is first parsed into a dependency tree, then modelled as a labelled directed graph by an RGCN, where nodes are words and edges are dependency relations. Following Schlichtkrull et al. (2018), we allow information to flow in both directions (from head to dependent and from dependent to head). Following Wu et al. (2021), we pass"
2021.repl4nlp-1.7,S16-1081,0,0.0571294,"Missing"
2021.repl4nlp-1.7,S12-1051,0,0.0442077,"Missing"
2021.repl4nlp-1.7,W19-4800,0,0.132991,"Missing"
2021.repl4nlp-1.7,L18-1269,0,0.014568,"ion of 512. Following SBERT, we evaluate our model on the STS benchmark development set in Spearman rank correlation for every 1, 000 steps during training, and save the best model. 4.2 Evaluation - Unsupervised STS First, we evaluate our model on semantic textual similarity (STS) datasets. Here we use STS12-16 tasks (Agirre et al., 2012, 2013, 2014, 2015, 2016), SICK-Relatedness (SICK-R) (Marelli et al., 2014) test set and STS benchmark (STSb) (Cer et al., 2017) test set. These datasets are labelled from 0 to 5 on semantic relatedness of sentence pairs. We obtain these datasets via SentEval (Conneau and Kiela, 2018). In this evaluation, we test different encoders’ performance without using any task-specific training data. Connect BERT and RGCN: The concatenation of BERT and RGCN’s sentence representations are then passed through a layer normalisation layer to form the final sentence representation. Sentence embeddings of given sentence-pair are then interacted before passing to the final classifier for training. As for the interaction, we use the concatenation of sentence embedding u, v and the elementwise difference |u − v|, which has been found to be the best concatenation mode by Reimers and Gurevych"
2021.repl4nlp-1.7,P19-1356,0,0.0862893,"to both similarity comparison and downstream tasks. In this work, we show that by incorporating structural information into SBERT, the resulting model outperforms SBERT and previous general sentence encoders on unsupervised semantic textual similarity (STS) datasets and transfer classification tasks. 1 Introduction Pre-trained models like BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) have demonstrated promising results across a variety of downstream NLP tasks. Though BERT-like models have been shown to capture hidden syntax structures (Clark et al., 2019; Hewitt and Manning, 2019; Jawahar et al., 2019), recent works have achieved performance improvements on various natural language understanding (NLU) tasks through the use of a graph network that captures syntax and semantics information. Xu and Yang (2019) demonstrate the value of syntax information for pronoun resolution tasks, using Relational Graph Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018) to incorporate syntactic dependency graphs. Wu et al. (2021) argue that semantics has not been brought to the surface of pre-trained models and propose to introduce semantic label information 57 Proceedings of the 6th Workshop on Rep"
2021.repl4nlp-1.7,D17-1070,0,0.0246543,"cation tasks. 2 Model Inspired by Reimers and Gurevych (2019), we train our model in a siamese network to update weights so as to produce similarity-comparable sentence representations. The model we propose consists of two components, as shown in Figure 1. Related Work Sentence encoders have been studied extensively in years. Skip-Thought (Kiros et al., 2015) has been trained to predict its surrounding sentences by using current sentence in a self-supervised fashion. Hill et al. (2016) proposed a sequential denoising autoencoder (SDAE) to reconstruct given sentence representations. InferSent (Conneau et al., 2017), on the other hand, used labelled NLI datasets to train a general-purpose sentence encoder in a BiLSTM-based siamese structure. Cer et al. (2018) proposed the Universal Sentence Encoder (USE) model based on transformers (Vaswani et al., 2017), and trained it with both unsupervised tasks and supervised NLI tasks. Inspired by InferSent, Sentence-BERT (SBERT) (Reimers and Gurevych, 2019) produces general-purpose sentence embeddings by fine-tuning BERT on NLI datasets in a siamese structure, showing improved performance on a variety of tasks. Hidden syntax structures in pre-trained models have be"
2021.repl4nlp-1.7,2021.naacl-main.146,0,0.0339665,"ERT) (Reimers and Gurevych, 2019) produces general-purpose sentence embeddings by fine-tuning BERT on NLI datasets in a siamese structure, showing improved performance on a variety of tasks. Hidden syntax structures in pre-trained models have been well explored. Various probing methods have been used to investigate hidden structures (Clark et al., 2019; Hewitt and Manning, 2019; Jawahar et al., 2019). The impact of external structures on pre-trained models has also been questioned. Glavaˇs and Vuli´c (2021) examined the benefits of incorporating universal dependencies into pre-trained models. Dai et al. (2021) showed that the tree induced from pre-trained models could produce competitive results compared with external trees. However, recent improvements have still been observed on various NLU tasks by incorporating structural information into pre-trained models. Yin et al. (2020) proposed SentiBERT to incorporate constituency tree into BERT for sentiment analysis. Xu and Yang (2019) modelled each sentence as a directed dependency graph by using RGCNs, and achieved large improvements on pronoun resolution. Zhang et al. (2020) proposed a semanticsaware BERT model by further encoding semantic informat"
2021.repl4nlp-1.7,C02-1150,0,0.291189,". For remaining tasks, results are reported on test set. We run 5 times with random seeds and report mean with standard deviation. While the best results for BERT-like models is achieved with problem-specific fine-tuning, an evaluation on transfer tasks provides a way to test the encoder’s generalisation ability and representation quality. Following Reimers and Gurevych (2019), we use SentEval with logistic regression to test different encoders on 8 classification tasks: sentiment analysis, MR (Pang and Lee, 2005); CR (Hu and Liu, 2004); SST-5/SST-2 (Socher et al., 2013); question-type, TREC (Li and Roth, 2002); subjectivity-objectivity, SUBJ (Pang and Lee, 2004); phrase-level opinion polarity, MPQA (Wiebe et al., 2005); and paraphrase detection, MRPC (Dolan et al., 2004). These datasets are provided by SentEval. As shown in Table 2, the proposed model outperforms previous encoders in general though the difference between SBERT and our model is relatively small. Our model performs significantly worse than USE on TREC, which may be due to the fact that USE is pre-trained on question-answering data, which appears to be beneficial to the TREC question-type classification task. Unlike previous poor perf"
2021.repl4nlp-1.7,2020.coling-main.293,0,0.0542338,"Missing"
2021.repl4nlp-1.7,C04-1051,0,0.273727,"like models is achieved with problem-specific fine-tuning, an evaluation on transfer tasks provides a way to test the encoder’s generalisation ability and representation quality. Following Reimers and Gurevych (2019), we use SentEval with logistic regression to test different encoders on 8 classification tasks: sentiment analysis, MR (Pang and Lee, 2005); CR (Hu and Liu, 2004); SST-5/SST-2 (Socher et al., 2013); question-type, TREC (Li and Roth, 2002); subjectivity-objectivity, SUBJ (Pang and Lee, 2004); phrase-level opinion polarity, MPQA (Wiebe et al., 2005); and paraphrase detection, MRPC (Dolan et al., 2004). These datasets are provided by SentEval. As shown in Table 2, the proposed model outperforms previous encoders in general though the difference between SBERT and our model is relatively small. Our model performs significantly worse than USE on TREC, which may be due to the fact that USE is pre-trained on question-answering data, which appears to be beneficial to the TREC question-type classification task. Unlike previous poor performance on STS datasets, BERT-CLS and BERT-AVG produce good results on classification tasks. This shows that the relevant information is encoded in BERT-CLS and BER"
2021.repl4nlp-1.7,2021.ccl-1.108,0,0.082529,"Missing"
2021.repl4nlp-1.7,2021.eacl-main.270,0,0.0715936,"Missing"
2021.repl4nlp-1.7,marelli-etal-2014-sick,0,0.0304654,"Missing"
2021.repl4nlp-1.7,N19-1419,0,0.171138,"embedding method, suited to both similarity comparison and downstream tasks. In this work, we show that by incorporating structural information into SBERT, the resulting model outperforms SBERT and previous general sentence encoders on unsupervised semantic textual similarity (STS) datasets and transfer classification tasks. 1 Introduction Pre-trained models like BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) have demonstrated promising results across a variety of downstream NLP tasks. Though BERT-like models have been shown to capture hidden syntax structures (Clark et al., 2019; Hewitt and Manning, 2019; Jawahar et al., 2019), recent works have achieved performance improvements on various natural language understanding (NLU) tasks through the use of a graph network that captures syntax and semantics information. Xu and Yang (2019) demonstrate the value of syntax information for pronoun resolution tasks, using Relational Graph Convolutional Networks (RGCNs) (Schlichtkrull et al., 2018) to incorporate syntactic dependency graphs. Wu et al. (2021) argue that semantics has not been brought to the surface of pre-trained models and propose to introduce semantic label information 57 Proceedings of"
2021.repl4nlp-1.7,P04-1035,0,0.080605,"set. We run 5 times with random seeds and report mean with standard deviation. While the best results for BERT-like models is achieved with problem-specific fine-tuning, an evaluation on transfer tasks provides a way to test the encoder’s generalisation ability and representation quality. Following Reimers and Gurevych (2019), we use SentEval with logistic regression to test different encoders on 8 classification tasks: sentiment analysis, MR (Pang and Lee, 2005); CR (Hu and Liu, 2004); SST-5/SST-2 (Socher et al., 2013); question-type, TREC (Li and Roth, 2002); subjectivity-objectivity, SUBJ (Pang and Lee, 2004); phrase-level opinion polarity, MPQA (Wiebe et al., 2005); and paraphrase detection, MRPC (Dolan et al., 2004). These datasets are provided by SentEval. As shown in Table 2, the proposed model outperforms previous encoders in general though the difference between SBERT and our model is relatively small. Our model performs significantly worse than USE on TREC, which may be due to the fact that USE is pre-trained on question-answering data, which appears to be beneficial to the TREC question-type classification task. Unlike previous poor performance on STS datasets, BERT-CLS and BERT-AVG produc"
2021.repl4nlp-1.7,N16-1162,0,0.0464987,"Missing"
2021.repl4nlp-1.7,P05-1015,0,0.280244,"gression. For MR, CR, MPQA and SUBJ, we use 10fold cross validation and report accuracy on test-fold. For remaining tasks, results are reported on test set. We run 5 times with random seeds and report mean with standard deviation. While the best results for BERT-like models is achieved with problem-specific fine-tuning, an evaluation on transfer tasks provides a way to test the encoder’s generalisation ability and representation quality. Following Reimers and Gurevych (2019), we use SentEval with logistic regression to test different encoders on 8 classification tasks: sentiment analysis, MR (Pang and Lee, 2005); CR (Hu and Liu, 2004); SST-5/SST-2 (Socher et al., 2013); question-type, TREC (Li and Roth, 2002); subjectivity-objectivity, SUBJ (Pang and Lee, 2004); phrase-level opinion polarity, MPQA (Wiebe et al., 2005); and paraphrase detection, MRPC (Dolan et al., 2004). These datasets are provided by SentEval. As shown in Table 2, the proposed model outperforms previous encoders in general though the difference between SBERT and our model is relatively small. Our model performs significantly worse than USE on TREC, which may be due to the fact that USE is pre-trained on question-answering data, whic"
2021.repl4nlp-1.7,D19-1410,0,0.213394,"also be used as a general sentence encoder, either by using the CLS token (the first token of BERT output) or applying pooling over its outputs. However, this fails to produce sentence embeddings that can be used effectively for similarity comparison. Furthermore, this method of using BERT for similarity comparison is extremely inefficient, requiring sentence pairs to be concatenated and passed to BERT for every possible comparison. In response, Sentence-BERT (SBERT) has been proposed to alleviate this by fine-tuning BERT on natural language inference (NLI) datasets using a siamese structure (Reimers and Gurevych, 2019). General-purpose sentence embeddings are generated which outperform previous sentence encoders on both similarity comparison and transfer tasks. In this paper, we show that it is possible to improve the SBERT sentence encoder through the use of explicit syntactic or semantic structure. Inspired by SBERT’s success in producing general sentence representations and previous efforts on introducing structural information into pre-trained models, we propose a model that combines the two by training a BERT-RGCN model in a siamese structure. Under specific structural supervision, the proposed model i"
2021.repl4nlp-1.7,2020.acl-main.341,0,0.0193334,"ing methods have been used to investigate hidden structures (Clark et al., 2019; Hewitt and Manning, 2019; Jawahar et al., 2019). The impact of external structures on pre-trained models has also been questioned. Glavaˇs and Vuli´c (2021) examined the benefits of incorporating universal dependencies into pre-trained models. Dai et al. (2021) showed that the tree induced from pre-trained models could produce competitive results compared with external trees. However, recent improvements have still been observed on various NLU tasks by incorporating structural information into pre-trained models. Yin et al. (2020) proposed SentiBERT to incorporate constituency tree into BERT for sentiment analysis. Xu and Yang (2019) modelled each sentence as a directed dependency graph by using RGCNs, and achieved large improvements on pronoun resolution. Zhang et al. (2020) proposed a semanticsaware BERT model by further encoding semantic information with BERT using a GRU (Chung et al., 2014). RGCNs have also been used by Wu et al. (2021) to introduce semantic information into RoBERTa, and achieved consistent improvements when fine-tuned on problem-specific datasets. Similar efforts can be seen where researchers try"
2021.repl4nlp-1.7,D13-1170,0,0.00841909,"validation and report accuracy on test-fold. For remaining tasks, results are reported on test set. We run 5 times with random seeds and report mean with standard deviation. While the best results for BERT-like models is achieved with problem-specific fine-tuning, an evaluation on transfer tasks provides a way to test the encoder’s generalisation ability and representation quality. Following Reimers and Gurevych (2019), we use SentEval with logistic regression to test different encoders on 8 classification tasks: sentiment analysis, MR (Pang and Lee, 2005); CR (Hu and Liu, 2004); SST-5/SST-2 (Socher et al., 2013); question-type, TREC (Li and Roth, 2002); subjectivity-objectivity, SUBJ (Pang and Lee, 2004); phrase-level opinion polarity, MPQA (Wiebe et al., 2005); and paraphrase detection, MRPC (Dolan et al., 2004). These datasets are provided by SentEval. As shown in Table 2, the proposed model outperforms previous encoders in general though the difference between SBERT and our model is relatively small. Our model performs significantly worse than USE on TREC, which may be due to the fact that USE is pre-trained on question-answering data, which appears to be beneficial to the TREC question-type class"
2021.repl4nlp-1.7,2020.acl-main.295,0,0.0612273,"Missing"
2021.repl4nlp-1.7,N18-1101,0,0.0618856,"Missing"
C00-1029,C94-2195,0,0.0627943,"Missing"
C00-1029,W99-0631,1,0.834754,"chnique tends to dissipate as counts are passed up the hierarchy. 4 Table 1: Maximum Likelihood Estimates { freq(c; v; r) is the number of (n; v; r) triples in the data in which n is being used to denote c. p^(cjr) = freq(c;r ) freq(r ) =P p^(v jr) = freq(v;r ) freq(r ) =P p^(v jc0; r) = freq(c0 ;v;r ) freq(c0 ;r ) P P 0 v 0 2V freq(c;v ;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) P P P P P = 0 c0 2C freq(c ;v;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) c00 2c0 v 0 2V freq(c00 ;v;r ) c00 2c0 freq(c00 ;v 0 ;r ) The method used for comparing the p(v jc00; r) for c00 in some set c0, is based on the technique in Clark and Weir (1999) used for nding homogeneous sets of concepts in the WordNet noun hierarchy. Rather than directly compare estimates of p(v jc00 ; r), which are likely to be unreliable, we consider the children of c0 , and use estimates based on counts which have accumulated at the children. If c0 has children c01; c02; : : : ; c0 , we compare p(v jc0 ; r) for each i. This is an approximation, but if the p(v jc0 ; r) are similar, then we assume that the p(v jc00 ; r) for c00 in c0 are similar too. To determine whether the children of some hypernym c0 have similar p(v jc0 ), where c0 is the ith child, we apply a"
C00-1029,W95-0103,0,0.0342469,"Missing"
C00-1029,P96-1025,0,0.0322117,"Missing"
C00-1029,P97-1003,0,0.109999,"Missing"
C00-1029,J93-1003,0,0.0179403,"equencies for c0 equal to hnutrimenti, in the object position of eat. The gures in brackets are the expected values, based on the marginal totals in the table. The null hypothesis of the test is that p(v jc0 ; r) is the same for each i. For Table 2 the null hypothesis is that for every child, c0 , of hnutrimenti, the probability p(eat jc0 ; obj) is the same. The log-likelihood 2 statistic corresponding to Table 2 is 4:8. The log-likelihood 2 statistic is used rather than the Pearson's 2 statistic because it is thought to be more appropriate when the counts in the contingency table are low (Dunning, 1993). This tends to occur when the test is being applied to a set of concepts near the foot of the hierarchy.5 We compared n i i i i i i i Fisher's exact test could be used for tables with low counts, but we do not do so because tables dominated by low counts are likely to have a high percentage of noise, due to the way counts for a noun are split among 5 Table 2: Contingency table for children of hnutrimenti c i ^ c ; eat ; obj) freq( ^ c ; obj), freq( ^ c ; eat ; obj) freq( i i i hmilki hmeali 0.0 8.5 hcoursei 1.3 hdishi 5.3 hdelicacyi 0.3 15.4 (0.6) (5.6) (1.7) (5.7) (1.8) 9.0 78.0 24.7 82.3 27"
C00-1029,J93-1005,0,0.166435,"cn(n2 ) c The sense of n2 is chosen which maximises the relevant probability in each potential attachment case. If p(c ; prjv ) is greater than p(c 1 ; prjn1), the attachment is made to v , otherwise to n1 . If n2 is not in WordNet we compare p(prjv ) and p(prjn1). Probabilities of the form p(c; prjv ) and p(c; prjn1) are used rather than p(cjv; pr) and p(cjn1; pr), because the association between the preposition and v and n1 contains useful information. In fact, for a lot of cases this information alone can be used to decide on the correct attachment site. The original corpus-based method of Hindle and Rooth (1993) used exactly this information. Thus the method described here can be thought of as Hindle and Rooth's method with additional classbased information about n2 . In order to estimate p(c ; prjv ) (and p(c 1 ; prjn1)) we apply the same procedure as described in Section 3, rst rewriting the probability using Bayes' rule: p(c ; pr) p(c ; prjv ) = p(v jc ; pr) p(v ) = p(v jc ; pr) p(prjc )p(c ) p(v ) v n v n v v v v v v The probabilities p(c ) and p(v ) can be estimated using maximum likelihood estimates, and p(v jc ; pr) and p(prjc ) can be estimated using maximum likelihood estimates of p(v jtop(c"
C00-1029,J98-2002,0,0.181475,"ch would be too high is hentityi, fact that dog, rather than prize, is often as not all entities are semantically similar with the subject of run, can be used to decide respect to the object position of eat. The problem of choosing an appropriate level on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest in the hierarchy at which to represent a parWe describe a proposal for acquiring such ticular noun sense (given a predicate and arguknowledge, and as in other recent work in this ment position) has been investigated by Resnik area (Resnik, 1993; Li and Abe, 1998), a prob- (1993), Li and Abe (1998) and Ribas (1995). abilistic approach is taken. Using probabilities The learning mechanism presented here is a accords with the intuition that there are no ab- novel approach based on nding semantically solute constraints on the arguments of predi- similar sets of concepts in a hierarchy. We cates, but rather that constraints are satis ed demonstrate the e ectiveness of our approach to a certain degree (Resnik, 1993). Unfortu- using a PP-attachment experiment. nately, de ning probabilities in terms of words leads to a model with a vast number of param- 2 The"
C00-1029,H94-1048,0,0.0209356,"'s method with additional classbased information about n2 . In order to estimate p(c ; prjv ) (and p(c 1 ; prjn1)) we apply the same procedure as described in Section 3, rst rewriting the probability using Bayes' rule: p(c ; pr) p(c ; prjv ) = p(v jc ; pr) p(v ) = p(v jc ; pr) p(prjc )p(c ) p(v ) v n v n v v v v v v The probabilities p(c ) and p(v ) can be estimated using maximum likelihood estimates, and p(v jc ; pr) and p(prjc ) can be estimated using maximum likelihood estimates of p(v jtop(c ; v; pr); pr) and p(prjtop(c ; pr)) respectively.6 We used the training and test data described in Ratnaparkhi et al. (1994), which was taken from the Penn Treebank and has now become the standard data set for this task. The data set consists of tuples of the form (v , n1 , pr, n2 ), together with the attachment site for each tuple. There is also a development set to prevent implicit training on the test set during development. We extracted (v , pr, n2 ) and (n1 , pr, n2 ) v v v v v 6 In Section 4 we only gave the procedure for determining top(cv ; v; pr), but top(cv ; pr) can be determined in an analogous fashion. triples from the training set, and in order to increase the number of training triples, we also extra"
C00-1029,P98-2177,0,0.0346051,"Missing"
C00-1029,E95-1016,0,0.207944,"han prize, is often as not all entities are semantically similar with the subject of run, can be used to decide respect to the object position of eat. The problem of choosing an appropriate level on the attachment site of the relative clause: Fred awarded a prize for the dog that ran the fastest in the hierarchy at which to represent a parWe describe a proposal for acquiring such ticular noun sense (given a predicate and arguknowledge, and as in other recent work in this ment position) has been investigated by Resnik area (Resnik, 1993; Li and Abe, 1998), a prob- (1993), Li and Abe (1998) and Ribas (1995). abilistic approach is taken. Using probabilities The learning mechanism presented here is a accords with the intuition that there are no ab- novel approach based on nding semantically solute constraints on the arguments of predi- similar sets of concepts in a hierarchy. We cates, but rather that constraints are satis ed demonstrate the e ectiveness of our approach to a certain degree (Resnik, 1993). Unfortu- using a PP-attachment experiment. nately, de ning probabilities in terms of words leads to a model with a vast number of param- 2 The Input Data and Semantic eters, resulting in a sparse"
C00-1029,W97-0109,0,0.0393604,"Missing"
C00-1029,C92-2070,0,0.0658946,"ins how we determine similarity classes. The maximum likelihood estimates for the relevant probabilities are given in Table 1.4 4 Finding Similarity-classes First we explain how we determine if a set of concepts has similar p(v jc00 ; r) for each concept c00 in the set. Then we explain how we determine top(c; v; r). Since we are assuming the data is not sense disambiguated, freq(c; v; r) cannot be obtained by simply counting senses. The standard approach, which is adopted here, is to estimate freq(c;v; r) by distributing the count for each noun n in syn(c) evenly among all senses of the noun. Yarowsky (1992) and Resnik (1993) explain how the noise introduced by this technique tends to dissipate as counts are passed up the hierarchy. 4 Table 1: Maximum Likelihood Estimates { freq(c; v; r) is the number of (n; v; r) triples in the data in which n is being used to denote c. p^(cjr) = freq(c;r ) freq(r ) =P p^(v jr) = freq(v;r ) freq(r ) =P p^(v jc0; r) = freq(c0 ;v;r ) freq(c0 ;r ) P P 0 v 0 2V freq(c;v ;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) P P P P P = 0 c0 2C freq(c ;v;r ) 0 0 v 0 2V c0 2C freq(c ;v ;r ) c00 2c0 v 0 2V freq(c00 ;v;r ) c00 2c0 freq(c00 ;v 0 ;r ) The method used for comparing the p(v"
C00-1029,P97-1056,0,0.0349289,"Missing"
C00-1029,C98-2172,0,\N,Missing
C04-1146,W03-1812,0,0.0146853,"le 5: Correlation with compositionality for different similarity measures Compositionality of collocations In its most general sense, a collocation is a habitual or lexicalised word combination. However, some collocations such as strong tea are compositional, i.e., their meaning can be determined from their constituents, whereas others such as hot dog are not. Both types are important in language generation since a system must choose between alternatives but only non-compositional ones are of interest in language understanding since only these collocations need to be listed in the dictionary. Baldwin et al. (2003) explore empirical models of compositionality for noun-noun compounds and verb-particle constructions. Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words. McCarthy et al. (2003) also investigate several tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar in meaning to their constituent parts. They extract co-occurrence data for 111 phras"
C04-1146,briscoe-carroll-2002-robust,0,0.0700259,"ur sets We have described a number of ways of calculating distributional similarity. We now consider whether there is substantial variation in a word’s distributionally nearest neighbours according to the chosen measure. We do this by calculating the overlap between neighbour sets for 2000 nouns generated using different measures from direct-object data extracted from the British National Corpus (BNC). 3.1 Experimental set-up The data from which sets of nearest neighbours are derived is direct-object data for 2000 nouns extracted from the BNC using a robust accurate statistical parser (RASP) (Briscoe and Carroll, 2002). For reasons of computational efficiency, we limit ourselves to 2000 nouns and directobject relation data. Given the goal of comparing neighbour sets generated by different measures, we would not expect these restrictions to affect our findings. The complete set of 2000 nouns (WScomp ) is the union of two sets WShigh and WSlow for which nouns were selected on the basis of frequency: WShigh contains the 1000 most frequently occurring nouns (frequency > 500), and WSlow contains the nouns ranked 3001-4000 (frequency ≈ 100). By excluding mid-frequency nouns, we obtain a clear separation between h"
C04-1146,J92-4003,0,0.0165264,"ept of distributional gnerality and the semantic relation of hyponymy. Finally, we consider the impact that this has on one application of distributional similarity methods (judging the compositionality of collocations). 1 Introduction Over recent years, many Natural Language Processing (NLP) techniques have been developed that might benefit from knowledge of distributionally similar words, i.e., words that occur in similar contexts. For example, the sparse data problem can make it difficult to construct language models which predict combinations of lexical events. Similarity-based smoothing (Brown et al., 1992; Dagan et al., 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events. Other potential applications apply the hypothesised relationship (Harris, 1968) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruse"
C04-1146,P99-1016,0,0.00756697,"ty measure to analysing the statistical and linguistic properties of sets of distributionally similar words returned by different measures. This will make it possible to predict in advance of any experimental evaluation which distributional similarity measures might be most appropriate for a particular application. Further, we explore a problem faced by the automatic thesaurus generation community, which is that distributional similarity methods do not seem to offer any obvious way to distinguish between the semantic relations of synonymy, antonymy and hyponymy. Previous work on this problem (Caraballo, 1999; Lin et al., 2003) involves identifying specific phrasal patterns within text e.g., “Xs and other Ys” is used as evidence that X is a hyponym of Y. Our work explores the connection between relative frequency, distributional generality and semantic generality with promising results. The rest of this paper is organised as follows. In Section 2, we present ten distributional similarity measures that have been proposed for use in NLP. In Section 3, we analyse the variation in neighbour sets returned by these measures. In Section 4, we take one fundamental statistical property (word frequency) and"
C04-1146,W02-0908,0,0.13096,"ns of lexical events. Similarity-based smoothing (Brown et al., 1992; Dagan et al., 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events. Other potential applications apply the hypothesised relationship (Harris, 1968) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain. However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be"
C04-1146,P99-1004,0,0.359404,"that they might be tailored to a particular genre or domain. However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curran and Moens, 2002) or be oriented towards a particular task such as language modelling (Dagan et al., 1999; Lee, 1999). The first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard. Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area. However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). Thus, applying a distributional similarity technique to a new application necessitates evaluating a large number of distributional simil"
C04-1146,P98-2127,0,0.979739,"combinations of lexical events. Similarity-based smoothing (Brown et al., 1992; Dagan et al., 1999) is an intuitively appealing approach to this problem where probabilities of unseen co-occurrences are estimated from probabilities of seen co-occurrences of distributionally similar events. Other potential applications apply the hypothesised relationship (Harris, 1968) between distributional similarity and semantic similarity; i.e., similarity in the meaning of words can be predicted from their distributional similarity. One advantage of automatically generated thesauruses (Grefenstette, 1994; Lin, 1998; Curran and Moens, 2002) over large-scale manually created thesauruses such as WordNet (Fellbaum, 1998) is that they might be tailored to a particular genre or domain. However, due to the lack of a tight definition for the concept of distributional similarity and the broad range of potential applications, a large number of measures of distributional similarity have been proposed or adopted (see Section 2). Previous work on the evaluation of distributional similarity methods tends to either compare sets of distributionally similar words to a manually created semantic resource (Lin, 1998; Curra"
C04-1146,W03-1810,1,0.696641,"not. Both types are important in language generation since a system must choose between alternatives but only non-compositional ones are of interest in language understanding since only these collocations need to be listed in the dictionary. Baldwin et al. (2003) explore empirical models of compositionality for noun-noun compounds and verb-particle constructions. Based on the observation (Haspelmath, 2002) that compositional collocations tend to be hyponyms of their head constituent, they propose a model which considers the semantic similarity between a collocation and its constituent words. McCarthy et al. (2003) also investigate several tests for compositionality including one (simplexscore) based on the observation that compositional collocations tend to be similar in meaning to their constituent parts. They extract co-occurrence data for 111 phrasal verbs (e.g. rip off ) and their simplex constituents (e.g. rip) from the BNC using RASP and calculate the value of simlin between each phrasal verb and its simplex constituent. The test simplexscore is used to rank the phrasal verbs according to their similarity with their simplex constituent. This ranking is correlated with human judgements of the comp"
C04-1146,W03-1011,1,0.907832,"eated semantic resource (Lin, 1998; Curran and Moens, 2002) or be oriented towards a particular task such as language modelling (Dagan et al., 1999; Lee, 1999). The first approach is not ideal since it assumes that the goal of distributional similarity methods is to predict semantic similarity and that the semantic resource used is a valid gold standard. Further, the second approach is clearly advantageous when one wishes to apply distributional similarity methods in a particular application area. However, it is not at all obvious that one universally best measure exists for all applications (Weeds and Weir, 2003). Thus, applying a distributional similarity technique to a new application necessitates evaluating a large number of distributional similarity measures in addition to evaluating the new model or algorithm. We propose a shift in focus from attempting to discover the overall best distributional similarity measure to analysing the statistical and linguistic properties of sets of distributionally similar words returned by different measures. This will make it possible to predict in advance of any experimental evaluation which distributional similarity measures might be most appropriate for a part"
C04-1146,C98-2122,0,\N,Missing
C14-1212,W11-2501,0,0.670317,"an we determine which relationship holds? In Section 2, we discuss existing attempts to address this problem through the use of various directional measures of distributional similarity. This paper considers the effectiveness of various supervised approaches, and makes the following contributions. First, we show that a SVM can distinguish the entailment and co-hyponymy relations, achieving a significant reduction in error rate in comparison to existing state-of-the-art methods based on the notion of distributional generality. Second, by comparing two different data sets, one built from BLESS (Baroni and Lenci, 2011) and the other from WordNet (Fellbaum, 1989), we derive important insights into the requirements of a valid evaluation of supervised approaches, and provide a data set for further research in this area. Third, we show that when learning how to determine an ontological relationship between a pair of similar words by means of the word’s distributional vectors, quite different vector operations are useful when identifying different ontological relationships. In particular, using the difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding the vector"
C14-1212,D10-1115,0,0.0113218,"the sentences, and in which direction, depends on the ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we know which is the hyponym and which is the hypernym? In applying distributional semantics to the problem of textual entailment, there is a need to generalise lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations is crucial if approaches to the composition of distributional representations of meaning that are currently receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual entailment problem. We formulate the challenge as follows: Consider a set of pairs of similar words hA, Bi where one of three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section 2, we discuss existing attempts to address this problem through the use of various directional measures of distributional similarity. This paper considers the effectiveness of"
C14-1212,E12-1004,0,0.611376,"method of Average Precision to the problem of identifying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similarities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric similarity measures previously proposed perform much better than symmetric similarity measures on a directionality detection experiment, and that their method and that of Clarke (2009) outperform the others with statistical significance. They also show that their measure is superior when used for term expansion in an event detection task. Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate that support vector machines can generalise entailment relations between quantifier phrases to entailment involving unseen quantifiers. They compare the performance of their system with the balAPinc measure. The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large corpora to find patterns that are indicative of hyponymy. For example, the pattern “NP X and other NP Y ” is an indication that NP X is a NP Y , i.e. that NP X is a hyponym of NP Y . They use machine learning to i"
C14-1212,P10-1124,0,0.0292029,"utomatically and applied them in a variety of applications, generally with a good deal of success. In early research there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to re"
C14-1212,D10-1029,0,0.0147337,"generally with a good deal of success. In early research there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyp"
C14-1212,P11-1014,1,0.337646,"Curran, 2004) and many researchers have built thesauri (i.e. lists of “nearest neighbours”) automatically and applied them in a variety of applications, generally with a good deal of success. In early research there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relat"
C14-1212,P89-1010,0,0.543369,"nformation was collected for all of the nouns from Wikipedia provided they had occurred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted text using wp2txt4 . This was part-of-speech tagged, lemmatised and dependency parsed using the Malt Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech (nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were extracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The total number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345. Our goal is to build classifiers that establish whether or not a given semantic relation, rel, holds between two similar words A and B. Support vector machines (SVMs), which are effective across a variety of classification scenarios, learn a boundary between two classes from a set of positive and negative example vectors. The two classes correspond to the relation rel holding or not holding. Here, however, we do not start with a single vector, but with tw"
C14-1212,W09-0215,1,0.322645,"the geometric average of Pww and the symmetric similarity measure of Lin (1998) in order to penalise low frequency words. Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identifying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similarities for narrow feature vectors; their measure is called balAPinc. They show that all of the asymmetric similarity measures previously proposed perform much better than symmetric similarity measures on a directionality detection experiment, and that their method and that of Clarke (2009) outperform the others with statistical significance. They also show that their measure is superior when used for term expansion in an event detection task. Baroni et al. (2012) investigate the relation between phrasal and lexical entailment, and demonstrate that support vector machines can generalise entailment relations between quantifier phrases to entailment involving unseen quantifiers. They compare the performance of their system with the balAPinc measure. The Stanford WordNet project (Snow et al., 2004) expands the WordNet taxonomy by analysing large corpora to find patterns that are in"
C14-1212,S12-1089,0,0.0164515,"3 bird 0.36, fish 0.34, creature 0.33, dog 0.31, horse 0.30, insect 0.30, species 0.29, cat 0.29, human 0.28, mammal, 0.28, cattle 0.27, snake 0.27, pig 0.26, rabbit 0.26, elephant 0.25 cat 0.32, animal 0.31, horse 0.29, bird 0.26, rabbit 0.26, pig 0.25, bear 0.26, man 0.25, fish 0.24, boy 0.24, creature 0.24, monkey 0.24, snake 0.24, mouse 0.24, rat 0.23 Table 1: Top 15 neighbours of cat, animal and dog generated using Lin’s similarity measure (Lin, 1998) considering all words and dependency features occurring 100 or more times in Wikipedia. Distributional similarity is being deployed (e.g., Dinu and Thater (2012)) in situations where it can be useful to be able to distinguish between these different relationships. Consider the following two sentences. The cat ran across the road. (1) The animal ran across the road. (2) Sentence 1 textually entails sentence 2, but sentence 2 does not textually entail sentence 1. The ability to determine whether entailment holds between the sentences, and in which direction, depends on the ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we know which is the hyponym and which is the hypernym? In applying distributional semant"
C14-1212,N12-1051,0,0.00913863,"t and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related words. A central problem here is that whilst most measures of distributional similarity are symmetric, some of the important semantic relations are"
C14-1212,W11-0114,0,0.00768727,"direction, depends on the ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we know which is the hyponym and which is the hypernym? In applying distributional semantics to the problem of textual entailment, there is a need to generalise lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations is crucial if approaches to the composition of distributional representations of meaning that are currently receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual entailment problem. We formulate the challenge as follows: Consider a set of pairs of similar words hA, Bi where one of three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section 2, we discuss existing attempts to address this problem through the use of various directional measures of distributional similarity. This paper considers the effectiveness of various supervised approac"
C14-1212,P10-1155,0,0.0191069,"uri (i.e. lists of “nearest neighbours”) automatically and applied them in a variety of applications, generally with a good deal of success. In early research there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automat"
C14-1212,P99-1004,0,0.0493788,"ve important insights into the requirements of a valid evaluation of supervised approaches, and provide a data set for further research in this area. Third, we show that when learning how to determine an ontological relationship between a pair of similar words by means of the word’s distributional vectors, quite different vector operations are useful when identifying different ontological relationships. In particular, using the difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding the vectors works well for the co-hyponym task. 2 Related Work Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She found that this measure improved results in language modelling, when a word’s distribution is smoothed using the distributions of its nearest neighbours. Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words tend to occur in a larger variety of contexts than more specific words. For example, we would expect to be able to replace any occurrence of cat with anim"
C14-1212,S12-1012,0,0.394086,"vB ∗ vA A linear SVM trained on the vector sum vB + vA A linear SVM trained on the vector concatenation vB ⊕ vA A linear SVM trained on the vector vB k nearest neighbours (knn) trained on the vector difference vB − vA .1 < k < 50 width(B) > width(A) → rel(A, B) where width(A) is number of non-zero features in A width(B) > p → rel(A, B) simcos (A, B) > p → rel(A, B) where simcos (A, B) is cosine similarity using PPMI simlin (A, B) > p → rel(A, B) (Lin, 1998) Pww (A, B) > Rww (A, B) → rel(A, B) (Weeds et al., 2004) Pcl (A, B) > Rcl (A, B) → rel(A, B) (Clarke, 2009) invCL(A, B) > p → rel(A, B) (Lenci and Benotto, 2012) balAPinc(A, B) > p → rel(A, B) (Kotlerman et al., 2010) The most frequent label in the training data is assigned to every test point. Table 2: Implemented classifiers 2253 3.3 Data Sets One of key the challenges of this work has been to construct a data set which accurately and validly tests our hypotheses. All four of our datasets detailed below are available online 5 . In order to test our hypotheses, a data set needs to be balanced in many respects in order to prevent the supervised classifiers making use of artefacts of the data. This would not only make it unfair to compare the supervise"
C14-1212,P98-2127,0,0.0984133,"al semantics, drawing on the distributional hypothesis: words that occur in similar contexts tend to have similar meanings (Harris, 1954). There is a large body of work on the use of different similarity measures (Lee, 1999; Weeds and Weir, 2003; Curran, 2004) and many researchers have built thesauri (i.e. lists of “nearest neighbours”) automatically and applied them in a variety of applications, generally with a good deal of success. In early research there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2"
C14-1212,D10-1035,0,0.0238163,"there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related words."
C14-1212,N13-1090,0,0.031662,". The approach is similar to ours in providing a supervised method of learning semantic relations, but relies on having features for occurrences of pairs of terms rather than just vectors for terms themselves. Our approach is therefore more generally applicable to systems which compose distributional representations of meaning. Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution. In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional similarity measure, WeightedCosine2 , performs best. Also of note, Mikolov et al. (2013) propose a vector offset method to capture syntactic and semantic regularities between word representations learnt by a recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis. Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms. Evaluated"
C14-1212,C12-1109,0,0.0197854,"hers have built thesauri (i.e. lists of “nearest neighbours”) automatically and applied them in a variety of applications, generally with a good deal of success. In early research there was much interest in how these automatically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just cons"
C14-1212,P08-1028,0,0.0117054,"er entailment holds between the sentences, and in which direction, depends on the ability to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we know which is the hyponym and which is the hypernym? In applying distributional semantics to the problem of textual entailment, there is a need to generalise lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations is crucial if approaches to the composition of distributional representations of meaning that are currently receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual entailment problem. We formulate the challenge as follows: Consider a set of pairs of similar words hA, Bi where one of three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section 2, we discuss existing attempts to address this problem through the use of various directional measures of distributional similarity. This paper c"
C14-1212,W04-0308,0,0.0306239,"2011), this measure outperforms Pww at both discriminating hypernym test pairs from other types of relation and at determining the direction of the entailment relation. 3 Methodology The code used to perform our experiments has been open sourced, and is available online.3 3.1 Vector Representations Distributional information was collected for all of the nouns from Wikipedia provided they had occurred 100 or more times. We used a Wikimedia dump of Wikipedia from June 2011 and extracted text using wp2txt4 . This was part-of-speech tagged, lemmatised and dependency parsed using the Malt Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech (nsubj, dobj, iobj, conj, amod, nnmod) and also occurring 100 or more times were extracted as features of the POS-tagged and lemmatised nouns. The value of each feature is the positive point wise mutual information (PPMI) (Church and Hanks, 1989) between the noun and the feature. The total number of noun vectors which can be harvested from Wikipedia with these parameters is 124, 345. Our goal is to build classifiers that establish whether or not a given semantic relation, rel, holds between two similar words A an"
C14-1212,N13-1040,0,0.0758774,"ore recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related words. A central problem here is that whilst most measures of distributional similarity are symmetric, some of the important semantic relations are not. The hyponymy relation (and converse hypernymy)"
C14-1212,E14-4008,0,0.55272,"i and Briscoe (2013) note that hyponyms are well suited for lexical substitution. In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional similarity measure, WeightedCosine2 , performs best. Also of note, Mikolov et al. (2013) propose a vector offset method to capture syntactic and semantic regularities between word representations learnt by a recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis. Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS dataset (Baroni and Lenci, 2011), this measure outperforms Pww at both discriminating hypernym test pairs from other types of relation and at determining the direction of the entailment relation. 3 Methodology The code used to perform our experiments has been open sourced, and is available online.3 3.1 Vector R"
C14-1212,P06-1101,0,0.0931762,"Missing"
C14-1212,D12-1110,0,0.0177649,"bility to identify hyponymy. Given a similarity score of 0.29 between cat and animal, how do we know which is the hyponym and which is the hypernym? In applying distributional semantics to the problem of textual entailment, there is a need to generalise lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations is crucial if approaches to the composition of distributional representations of meaning that are currently receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual entailment problem. We formulate the challenge as follows: Consider a set of pairs of similar words hA, Bi where one of three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section 2, we discuss existing attempts to address this problem through the use of various directional measures of distributional similarity. This paper considers the effectiveness of various supervised approaches, and makes the fo"
C14-1212,N13-1133,0,0.0332728,"n-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related words. A central problem here is that whilst most measures of distributional similarity are symmetri"
C14-1212,C08-1107,0,0.0274283,", v) ∗ (1 − Rcl (u, v)) Evaluation on the BLESS data set (Baroni and Lenci, 2011), showed that this measure is better at distinguishing hypernyms from other relations than the measures of Weeds et al. (2004) and Clarke (2009). Geffet and Dagan (2005) proposed an approach based on feature inclusion, which extends the rationale of Weeds et al. (2004) to lexical entailment. Using data from the web they demonstrated a strong correlation between complete inclusion of prominent features and lexical entailment. However, they were unable to assess this using an off-line corpus due to data sparseness. Szpektor and Dagan (2008) found that the Pww measure tends to promote relationships between infrequent words with narrow vectors (i.e. those with relatively few distinct context features). They proposed using the geometric average of Pww and the symmetric similarity measure of Lin (1998) in order to penalise low frequency words. Kotlerman et al. (2010) apply the IR evaluation method of Average Precision to the problem of identifying lexical inference and use the balancing approach of Szpektor and Dagan (2008) to demote similarities for narrow feature vectors; their measure is called balAPinc. They show that all of the"
C14-1212,W03-1011,1,0.652897,"Missing"
C14-1212,C04-1146,1,0.795069,"hen identifying different ontological relationships. In particular, using the difference between the vectors for pairs of words is appropriate for the entailment task, whereas adding the vectors works well for the co-hyponym task. 2 Related Work Lee (1999) noted that the substitutability of one word for another was asymmetric and proposed the alpha-skew divergence measure, an asymmetric version of the Kullback-Leibler divergence measure. She found that this measure improved results in language modelling, when a word’s distribution is smoothed using the distributions of its nearest neighbours. Weeds et al. (2004) proposed a notion of distributional generality, observing that more general words tend to occur in a larger variety of contexts than more specific words. For example, we would expect to be able to replace any occurrence of cat with animal and so all of the contexts of cat must be plausible 2250 contexts for animal. However, not all of the contexts of animal would be plausible for cat, e.g., “the monstrous animal barked at the intruder”. Weeds et al. (2004) attempt to capture this asymmetry by framing word similarity in terms of co-occurrence retrieval (Weeds and Weir, 2003), where precision a"
C14-1212,W14-1502,1,0.78607,"ponymy. Given a similarity score of 0.29 between cat and animal, how do we know which is the hyponym and which is the hypernym? In applying distributional semantics to the problem of textual entailment, there is a need to generalise lexical entailment to phrases and sentences. Thus, the ability to distinguish different semantic relations is crucial if approaches to the composition of distributional representations of meaning that are currently receiving considerable interest (Widdows, 2008; Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010; Grefenstette et al., 2011; Socher et al., 2012; Weeds et al., 2014) are to be applied to the textual entailment problem. We formulate the challenge as follows: Consider a set of pairs of similar words hA, Bi where one of three relationships hold between A and B: A lexically entails B, B lexically entails A or A and B are related by co-hyponymy. Given such a set, how can we determine which relationship holds? In Section 2, we discuss existing attempts to address this problem through the use of various directional measures of distributional similarity. This paper considers the effectiveness of various supervised approaches, and makes the following contributions"
C14-1212,D12-1111,0,0.0121168,"ctors for terms themselves. Our approach is therefore more generally applicable to systems which compose distributional representations of meaning. Most recently, Rei and Briscoe (2013) note that hyponyms are well suited for lexical substitution. In their experiments with smoothing edge scores for parser lexicalisation, they find that a directional similarity measure, WeightedCosine2 , performs best. Also of note, Mikolov et al. (2013) propose a vector offset method to capture syntactic and semantic regularities between word representations learnt by a recurrent neural network language model. Yih et al. (2012) present a method for distinguishing synonyms and antonyms by inducing polarity in a document-term matrix before applying Latent Semantic Analysis. Santus et al. (2014) propose identifying hypernyms using a new measure based on entropy, SLQS, which is based on the hypothesis that the most typical linguistic contexts of a hypernym are less informative than the most typical linguistic contexts of its hyponyms. Evaluated on pairs extracted from the BLESS dataset (Baroni and Lenci, 2011), this measure outperforms Pww at both discriminating hypernym test pairs from other types of relation and at de"
C14-1212,D10-1074,0,0.0119446,"atically generated thesauri compare with human-constructed gold standards such as WordNet and Roget (Lin, 1998; Kilgarriff and Yallop, 2000). More recently, the focus has tended to shift to building thesauri to alleviate the sparse-data problem. Distributional thesauri have been used in a wide variety of areas including sentiment classification (Bollegala et al., 2011), WSD (Miller et al., 2012; Khapra et al., 2010), textual entailment (Berant et al., 2010), predicting semantic compositionality (Bergsma et al., 2010), acquisition of semantic lexicons (McIntosh, 2010), conversation entailment (Zhang and Chai, 2010), lexical substitution (Szarvas et al., 2013), taxonomy induction (Fountain and Lapata, 2012), and parser lexicalisation (Rei and Briscoe, 2013). A primary focus of distributional semantics has been on identifying words which are similar to each other. However, semantic similarity encompasses a variety of different lexico-semantic and topical relations. Even if we just consider nouns, an automatically generated thesaurus will tend to return a mix of synonyms, antonyms, hyponyms, hypernyms, co-hyponyms, meronyms and other topically related words. A central problem here is that whilst most measu"
C14-1212,J90-1003,0,\N,Missing
C14-1212,P05-1014,0,\N,Missing
C14-1212,C98-2122,0,\N,Missing
C14-1212,kilgarriff-yallop-2000-whats,0,\N,Missing
C14-2025,black-etal-2012-data,0,0.0309404,"ght is generated through the iterative interaction of the subject matter expert and the data itself. Method51 combines two strands of existing work. The first body of work employs tailored automatic data analysis, using supervised machine-learning approaches (Carvalho et al., 2011; Papacharissi and de Fatima Oliveira, 2012; Meraz and Papacharissi, 2013; Hopkins and King, 2010; Burnap et al., 2013b). A second body of work focusses on providing user interfaces that enable researchers to customise their processing pipeline, based on the requirements of their investigation (Blessing et al., 2013; Black et al., 2012; Burnap et al., 2013a). 1 Method51 has been released under an open source license, and is available at https://github.com/simonwibberley/method51 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 115 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, pages 115–119, Dublin, Ireland, August 23-29 2014. (a) Pipeline Construction Interface (b) Coding interface 3 Case Studies"
C14-2025,W13-2708,0,0.0128864,"ploration process. Insight is generated through the iterative interaction of the subject matter expert and the data itself. Method51 combines two strands of existing work. The first body of work employs tailored automatic data analysis, using supervised machine-learning approaches (Carvalho et al., 2011; Papacharissi and de Fatima Oliveira, 2012; Meraz and Papacharissi, 2013; Hopkins and King, 2010; Burnap et al., 2013b). A second body of work focusses on providing user interfaces that enable researchers to customise their processing pipeline, based on the requirements of their investigation (Blessing et al., 2013; Black et al., 2012; Burnap et al., 2013a). 1 Method51 has been released under an open source license, and is available at https://github.com/simonwibberley/method51 This work is licensed under a Creative Commons Attribution 4.0 International Licence. Page numbers and proceedings footer are added by the organisers. Licence details: http://creativecommons.org/licenses/by/4.0/ 115 Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: System Demonstrations, pages 115–119, Dublin, Ireland, August 23-29 2014. (a) Pipeline Construction Interface (b) Coding inte"
C14-2025,D11-1136,0,0.169994,"ley et al., 2013). In this paper we present a series of case studies, carried out on Twitter, that illustrate the importance of that agile paradigm, and how they have motivated the development of several additional methodologies, including ‘Twitcident’, ‘Patterns of Use’, and ‘Russian Doll’ analysis. First, we present Method511 , the technological counterpart to our methodological paradigm. 2 Method51 Method51 uses active learning, coupled with a Na¨ıve Bayes model, to allow social scientists to construct chains of linked, bespoke classifiers. The framework, initially an extension of DUALIST (Settles, 2011), utilises an EM step to harness information from large amounts of unlabelled data, and allows the analyst to expedite learning by specifying features that are highly indicative of a class. Using this approach, a classifier can be trained within minutes (Settles, 2011). This enables analysts to evolve the way that the data is being analysed without significant loss of effort. Method51 provides significant additional functionality including collaborative gold standard and classifier construction, processing pipeline construction, data collection and storage, data visualisation, various filterin"
C14-2025,W13-2705,1,0.73678,"sent Method51, a social media analysis software platform with a set of accompanying methodologies. We discuss a series of case studies illustrating the platform’s application, and motivating our methodological proposals. 1 Introduction Social scientists wish to apply language processing technology on social media datasets to answer sociological questions. To that end, the technology should support methodologies that allow analysts to gain valuable insight from the datasets under examination. In previous work we have argued for the importance of agility when dealing with social media datasets (Wibberley et al., 2013). In this paper we present a series of case studies, carried out on Twitter, that illustrate the importance of that agile paradigm, and how they have motivated the development of several additional methodologies, including ‘Twitcident’, ‘Patterns of Use’, and ‘Russian Doll’ analysis. First, we present Method511 , the technological counterpart to our methodological paradigm. 2 Method51 Method51 uses active learning, coupled with a Na¨ıve Bayes model, to allow social scientists to construct chains of linked, bespoke classifiers. The framework, initially an extension of DUALIST (Settles, 2011), u"
C14-2025,P11-2099,0,\N,Missing
C86-1048,P85-1011,1,0.821545,"atural languages, were developed independently. TAG&apos;s deal with a set of elementary trees which are composed by means of an operation called a d j o i n i n g . HG&apos;s are like Context-free Grammars, except for the fact t h a t besides concatenation of strings, s t r i n g w r a p p i n g operations are permitted. TAG&apos;s were first introduced in 1975 by Joshi, Levy and Wakahashi [3]. Joshi [2] investigated some formal and linguistic properties of TAG&apos;s with local constraints. The formulation of local constraints was then modified and formal properties were investigated by Vijay-Shanker and Joshi [9]. The linguistic properties were studied in detail by Kroeh and Joshi [5]. HG&apos;s were first introduced by Pollard [6] in 1983 and their formal properties were investigated by Roach [7]. It was observed t h a t the two systems seemed to possess similar generative power and since they also appear to have the same closure properties [7,9] as well as similar parsing algorithn~ [6,9] a significant amount of indirect evidence existed to suggest t h a t they were formally equivalent. In the present paper, we will attempt to provide a characterization of the formal relationship between HG&apos;s and TAG&apos;s."
C86-1048,P86-1011,1,0.376533,"Missing"
C98-1059,P96-1023,0,0.0777978,"Missing"
C98-1059,1997.iwpt-1.7,0,0.0446835,"habes, 1991) are difficult to parse with efficiently. Each word in the parser's input string introduces an elementary tree into the parse table for each of its possible readings, and there is often a substantial overlap in structure between these trees. A conventional parsing algorithm (VijayShanker and Joshi, 1985) views the trees as independent, and so is likely to duplicate tile processing of this common structure. Parsing could be made more efficient (empirically if not formally), if the shared structure could be identified and processed only once. Recent work by Evans and Weir (1997) and Chen and Vijay-Shanker (1997) addresses this problem from two different perspectives. Evans and Weir (1997) outline a technique for compiling LTAG grammars into automata which are 372 David Weir Cognitive and C o m p u t i n g Sciences University of Sussex Brighton, BN1 9QH, UK David. Weir@cogs.susx.ae. uk then merged to introduce some sharing of structure. Chen and Vijay-Shanker (1997) use underspecified tree descriptions to represent sets of trees during parsing. Tile present paper takes the former approach, but extends our previous work by: • showing how merged automata can be minirnised, so that they share as much str"
C98-1059,1997.iwpt-1.10,0,0.103398,"Missing"
C98-1059,P85-1011,0,0.272334,"ised, so that they share as much structure as possible; • showing that by precompiling additional information, parsing can be broken down into recognition followed by parse recovery; • providing a formal treatment of the algorithms for transforming and minimising the grammar, recognition and parse recovery. In the following sections we outline the basic approach, and describe informally our improvements to the previous account. We then give a formal account of the optimisation process and a possible parsing algorithm that makes use of it 1. 2 Automaton-based parsing Conventional LTAG parsers (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; VijayShanker and Weir, 1993) maintain a p a r s e table, a set of i t e m s corresponding to complete and partial constituents. Parsing proceeds by first seeding the table with items anchored on the input string, and then repeatedly scanning the table for p a r s e r a c t i o n s . Parser actions introduce new items into the table licensed by one or more items already in the table. The main types of parser actions are: 1. extending a constituent by incorporating a complete subconstituent (on the left or 1However, due to lack of space, no proofs and only minimal info"
C98-1059,J93-4002,1,0.875632,"Missing"
C98-1059,P88-1032,0,\N,Missing
C98-1059,1997.iwpt-1.11,1,\N,Missing
D16-1175,N09-1003,0,0.0520077,"lues are greater than or equal to 0. Shifted PPMI (SPPMI) subtracts a constant from any PMI score before applying the PPMI threshold. We experiment with values of 1, 5, 10, 40 and 100 for the shift parameter k. 5.1 Word Similarity Experiments We first evaluate our models on 3 word similarity benchmarks, MEN (Bruni et al., 2014), which is testing for relatedness (e.g. meronymy or holonymy) between terms, SimLex-999 (Hill et al., 2015), which is testing for substitutability (e.g. synonymy, antonymy, hyponymy and hypernymy), and WordSim-353 (Finkelstein et al., 2001), where we use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset. Baroni and Lenci (2011) have shown that untyped models are typically better at capturing relatedness, whereas typed models are better at encoding substitutability. Performance is measured by computing Spearman’s ρ between the cosine similarities of the vector representations and the corresponding aggregated human similarity judgements. For these experiments we keep the number of neighbours that a word vector can consume fixed at 30. This value is based on preliminary experiments on WordSim-353 (see Figure 2) using the st"
D16-1175,J10-4006,0,0.0521034,"rectly enriching sparse distributional vector representations, or have explored its behaviour for semantic composition. Compositional models of distributional semantics have become an increasingly popular topic in the research community. Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et 1692 Background Distributional vector space models can broadly be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation between two words for a co-occurrence event into account. Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad´o and Lapata"
D16-1175,W11-2501,0,0.117129,"ore before applying the PPMI threshold. We experiment with values of 1, 5, 10, 40 and 100 for the shift parameter k. 5.1 Word Similarity Experiments We first evaluate our models on 3 word similarity benchmarks, MEN (Bruni et al., 2014), which is testing for relatedness (e.g. meronymy or holonymy) between terms, SimLex-999 (Hill et al., 2015), which is testing for substitutability (e.g. synonymy, antonymy, hyponymy and hypernymy), and WordSim-353 (Finkelstein et al., 2001), where we use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset. Baroni and Lenci (2011) have shown that untyped models are typically better at capturing relatedness, whereas typed models are better at encoding substitutability. Performance is measured by computing Spearman’s ρ between the cosine similarities of the vector representations and the corresponding aggregated human similarity judgements. For these experiments we keep the number of neighbours that a word vector can consume fixed at 30. This value is based on preliminary experiments on WordSim-353 (see Figure 2) using the static top n neighbour retrieval function and a PPMI shift of k = 40. Figure 2 shows that distribut"
D16-1175,D10-1115,0,0.120078,"s, such as oldish – old, as a back-off strategy in case of data sparsity. However, none of these approaches have used distributional inference as a general technique for directly enriching sparse distributional vector representations, or have explored its behaviour for semantic composition. Compositional models of distributional semantics have become an increasingly popular topic in the research community. Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et 1692 Background Distributional vector space models can broadly be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation b"
D16-1175,D12-1050,0,0.729956,"utional approach for smoothing derivationally related words, such as oldish – old, as a back-off strategy in case of data sparsity. However, none of these approaches have used distributional inference as a general technique for directly enriching sparse distributional vector representations, or have explored its behaviour for semantic composition. Compositional models of distributional semantics have become an increasingly popular topic in the research community. Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et 1692 Background Distributional vector space models can broadly be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text. T"
D16-1175,J90-1003,0,0.164285,"s with other words within a specified spatial context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation between two words for a co-occurrence event into account. Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad´o and Lapata (2007), Erk and Pad´o (2008) and Weir et al. (2016) uses dependency paths to build a structured vector space model. In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014). In the following we will give an explanation of the theory of composition with A PTs as introduced by Weir et al. (2016), which we adopt in this paper. In addition to direct relations between two words, the A PT model also considers inverse and higher order relations. Inverse relations are denoted with a horizontal bar above the dependency relation, such as amod for an inverse adjectival modifier. Higher order dependencies are separated by a colon as in the second order distributional feature dobj:nsubj. The example below"
D16-1175,P94-1038,0,0.789302,"k compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose A PT framework is based on a higher-order dependency-typed vector space, however they do not address the issue of sparsity in their work. 3 Related Work Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al. (1997). In these works the authors are concerned with smoothing the probability estimate for unseen words in bigrams. This is achieved by measuring which unobserved bigrams are more likely than others on the basis of the Kullback-Leibler divergence between bigram distributions. This has led to significantly improved performance on a language modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997). More recently Pad´o et al. (2013) used a distributional approach for smoothing derivationally"
D16-1175,P97-1008,0,0.593732,"o convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose A PT framework is based on a higher-order dependency-typed vector space, however they do not address the issue of sparsity in their work. 3 Related Work Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al. (1997). In these works the authors are concerned with smoothing the probability estimate for unseen words in bigrams. This is achieved by measuring which unobserved bigrams are more likely than others on the basis of the Kullback-Leibler divergence between bigram distributions. This has led to significantly improved performance on a language modelling for speech recognition task, as well as for word-sense disambiguation in machine translation (Dagan et al., 1994; Dagan et al., 1997). More recently Pad´o et al. (2013) used a distributional approach for smoothing derivationally related words, such as"
D16-1175,de-marneffe-etal-2014-universal,0,0.0298743,"Missing"
D16-1175,D08-1094,0,0.284744,"Missing"
D16-1175,W13-0112,0,0.0338233,"ective approaches to composition benefit more from distributional inference than composition by union and highlight the ability of composition by intersection to disambiguate the meaning of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the A PT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We conclude this paper and outline future work in section 6. 2 al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approac"
D16-1175,D14-1163,0,0.598196,"wise multiplication represents a competitive and robust approach in comparison to more sophisticated composition methods. For the final set of experiments on the Mitchell and Lapata (2010) dataset, we present results the A PT model and the untyped model, using composition by union and composition by intersection, with and without distributional inference. We compare our models with the best performing untyped VSMs of Mitchell and Lapata (2010), and Blacoe and Lapata (2012), the best performing A PT model of Weir et al. (2016), as well as with the recently published state-of-the-art methods by Hashimoto et al. (2014), and Wieting et al. (2015), who are using neural network based approaches. For our models, we use the static top n approach as neighbour retrieval function and tune the remaining parameters, the SPPMI shift k (1, 5, 10, 40, 100) and the number of neighbours (10, 30, 50, 100, 500, 1000, 5000), for both model types, and the sliding window size for the untyped VSM (1, 2, 5), on the development portion of the Mitchell and Lapata (2010) dataset. We keep the vector con1698 figuration (k and window size) fixed for all phrase types and only tune the number of neighbours used for DI individually. The"
D16-1175,J15-4004,0,0.0388578,"ation together with a word. P (w, c) P (w)P (c) SP P M I(w, c) = max(P M I(w, c) − log k, 0) P M I(w, c) = log (1) As PMI is negatively unbounded, PPMI is used to ensure that all values are greater than or equal to 0. Shifted PPMI (SPPMI) subtracts a constant from any PMI score before applying the PPMI threshold. We experiment with values of 1, 5, 10, 40 and 100 for the shift parameter k. 5.1 Word Similarity Experiments We first evaluate our models on 3 word similarity benchmarks, MEN (Bruni et al., 2014), which is testing for relatedness (e.g. meronymy or holonymy) between terms, SimLex-999 (Hill et al., 2015), which is testing for substitutability (e.g. synonymy, antonymy, hyponymy and hypernymy), and WordSim-353 (Finkelstein et al., 2001), where we use the version of Agirre et al. (2009), who split the dataset into a relatedness and a substitutability subset. Baroni and Lenci (2011) have shown that untyped models are typically better at capturing relatedness, whereas typed models are better at encoding substitutability. Performance is measured by computing Spearman’s ρ between the cosine similarities of the vector representations and the corresponding aggregated human similarity judgements. For t"
D16-1175,Q14-1041,0,0.0121848,"d representations that only capture a fragment of the meaning of a word. For example the verbs “walking” and “strolling” may occur in many different and possibly disjoint contexts, although both verbs would be equally plausible in numerous cases. This subsequently results in incomplete representations for both lexemes. In addition, models based on counting co-occurrences face the general problem of sparsity in a very high-dimensional vector space. The most common approaches to these challenges have involved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014). The common problem in both of these approaches is that composition becomes a black-box process due to the lack of interpretability of the representations. Count-based models are therefore a very attractive line of work with regards to a number of important long-term research challenges, most notably the development of an adequate model of distributional compositional semantics. In this paper we propose the use of distributional inference (DI) to inject unobserved but plausible distr"
D16-1175,D15-1137,0,0.0183362,"of composition by intersection to disambiguate the meaning of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the A PT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We conclude this paper and outline future work in section 6. 2 al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose A PT framework is based on a higher-order de"
D16-1175,Q15-1016,0,0.0359828,"ors containing fewer than 50 non-zero entries. The raw counts are subsequently transformed to PPMI weights. The untyped vector space model is built from the same lowercased, tokenised and lemmatised Wikipedia corpus. We discard terms with a frequency of less than 50 and apply PPMI to the raw co-occurrence counts. Shifted PPMI We explore a range of different values for shifting the PPMI scores as these have a significant impact 1695 on the performance of the A PT model. The effect of shifting PPMI scores for untyped vector space models has already been explored in Levy and Goldberg (2014), and Levy et al. (2015), thus we only present results for the A PT model. As shown in equation 1, PMI is defined as the log of the ratio of the joint probability of observing a word w and a context c together, and the product of the respective marginals of observing them separately. In our A PT model, a context c is defined as a dependency relation together with a word. P (w, c) P (w)P (c) SP P M I(w, c) = max(P M I(w, c) − log k, 0) P M I(w, c) = log (1) As PMI is negatively unbounded, PPMI is used to ensure that all values are greater than or equal to 0. Shifted PPMI (SPPMI) subtracts a constant from any PMI score"
D16-1175,P98-2127,0,0.120646,"models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation between two words for a co-occurrence event into account. Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad´o and Lapata (2007), Erk and Pad´o (2008) and Weir et al. (2016) uses dependency paths to build a structured vector space model. In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014). In the following we will give an explanation of the theory of composition with A PTs as introduced by Weir et al. (2016), which we adopt in this paper. In addition to direct relations between two words, the A PT model"
D16-1175,P14-5010,0,0.00354437,"structure of our distributional vector space, we retrieve neighbours by querying WordNet (Fellbaum, 1998), and treat synsets with agreeing PoS tags as the nearest neighbours of any target vector. This restricts the retrieved neighbours to synonyms only. 5 Experiments Our model is based on a cleaned October 2013 Wikipedia dump, which excludes all pages with fewer than 20 page views, resulting in a corpus of approximately 0.6 billion tokens (Wilson, 2015). The corpus is lowercased, tokenised, lemmatised, PoS tagged and dependency parsed with the Stanford NLP tools, using universal dependencies (Manning et al., 2014; de Marneffe et al., 2014). We then build our A PT model with first, second and third order relations. We remove distributional features with a count of less than 10, and vectors containing fewer than 50 non-zero entries. The raw counts are subsequently transformed to PPMI weights. The untyped vector space model is built from the same lowercased, tokenised and lemmatised Wikipedia corpus. We discard terms with a frequency of less than 50 and apply PPMI to the raw co-occurrence counts. Shifted PPMI We explore a range of different values for shifting the PPMI scores as these have a significant"
D16-1175,P08-1028,0,0.26764,"tly Pad´o et al. (2013) used a distributional approach for smoothing derivationally related words, such as oldish – old, as a back-off strategy in case of data sparsity. However, none of these approaches have used distributional inference as a general technique for directly enriching sparse distributional vector representations, or have explored its behaviour for semantic composition. Compositional models of distributional semantics have become an increasingly popular topic in the research community. Starting from simple pointwise additive and multiplicative approaches to composition, such as Mitchell and Lapata (2008; 2010), and Blacoe and Lapata (2012), to tensor based models, such as Baroni and Zamparelli (2010), Coecke et 1692 Background Distributional vector space models can broadly be categorised into untyped proximity based models and typed models (Baroni and Lenci, 2010). Examples of the former include Deerwester et al. (1990); Lund and Burgess (1996); Curran (2004); Sahlgren (2006); Bullinaria and Levy (2007) and Turney and Pantel (2010). These models count the number of times every word in a large corpus co-occurs with other words within a specified spatial context window, without leveraging the"
D16-1175,D15-1279,0,0.0299045,"y intersection to disambiguate the meaning of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the A PT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We conclude this paper and outline future work in section 6. 2 al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose A PT framework is based on a higher-order dependency-typed vect"
D16-1175,C94-1049,0,0.0242615,"n a specified spatial context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation between two words for a co-occurrence event into account. Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad´o and Lapata (2007), Erk and Pad´o (2008) and Weir et al. (2016) uses dependency paths to build a structured vector space model. In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014). In the following we will give an explanation of the theory of composition with A PTs as introduced by Weir et al. (2016), which we adopt in this paper. In addition to direct relations between two words, the A PT model also considers inverse and higher order relations. Inverse relations are denoted with a horizontal bar above the dependency relation, such as amod for an inverse adjectival modifier. Higher order dependencies are separated by a colon as in the second order distributional feature dobj:nsubj. The example below illustrates how raw t"
D16-1175,J07-2002,0,0.136285,"Missing"
D16-1175,P13-2128,0,0.0333146,"Missing"
D16-1175,P14-1009,0,0.0805673,"n benefit more from distributional inference than composition by union and highlight the ability of composition by intersection to disambiguate the meaning of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the A PT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We conclude this paper and outline future work in section 6. 2 al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional compos"
D16-1175,D14-1162,0,0.0874446,"strolling” may occur in many different and possibly disjoint contexts, although both verbs would be equally plausible in numerous cases. This subsequently results in incomplete representations for both lexemes. In addition, models based on counting co-occurrences face the general problem of sparsity in a very high-dimensional vector space. The most common approaches to these challenges have involved the use of various techniques for dimensionality reduction (Bullinaria and Levy, 2012; Lapesa and Evert, 2014) or the use of low-dimensional and dense neural word embeddings (Mikolov et al., 2013; Pennington et al., 2014). The common problem in both of these approaches is that composition becomes a black-box process due to the lack of interpretability of the representations. Count-based models are therefore a very attractive line of work with regards to a number of important long-term research challenges, most notably the development of an adequate model of distributional compositional semantics. In this paper we propose the use of distributional inference (DI) to inject unobserved but plausible distributional semantic knowledge into the vector space by leveraging the intrinsic structure of the distributional"
D16-1175,I13-1056,0,0.0192581,"context window, without leveraging the structural information of the text. Typed models on the other hand, take the grammatical relation between two words for a co-occurrence event into account. Early proponents of that approach are Grefenstette (1994) and Lin (1998). More recent work by Pad´o and Lapata (2007), Erk and Pad´o (2008) and Weir et al. (2016) uses dependency paths to build a structured vector space model. In both kinds of models, the raw counts are usually transformed by Positive Pointwise Mutual Information (PPMI) or a variant of it (Church and Hanks, 1990; Niwa and Nitta, 1994; Scheible et al., 2013; Levy and Goldberg, 2014). In the following we will give an explanation of the theory of composition with A PTs as introduced by Weir et al. (2016), which we adopt in this paper. In addition to direct relations between two words, the A PT model also considers inverse and higher order relations. Inverse relations are denoted with a horizontal bar above the dependency relation, such as amod for an inverse adjectival modifier. Higher order dependencies are separated by a colon as in the second order distributional feature dobj:nsubj. The example below illustrates how raw text is processed to ret"
D16-1175,D12-1110,0,0.115163,"on and highlight the ability of composition by intersection to disambiguate the meaning of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the A PT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We conclude this paper and outline future work in section 6. 2 al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose A PT framework is bas"
D16-1175,P15-1150,0,0.0805406,"Missing"
D16-1175,P10-1097,0,0.185918,"Missing"
D16-1175,I11-1127,0,0.203692,"Missing"
D16-1175,W14-1502,1,0.872948,"ntal work, together with our results in section 5. We conclude this paper and outline future work in section 6. 2 al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose A PT framework is based on a higher-order dependency-typed vector space, however they do not address the issue of sparsity in their work. 3 Related Work Our method follows the distributional smoothing approach of Dagan et al. (1994) and Dagan et al. (1997). In these works the authors are concerned with smoothing the probability estimate for unseen words in bigrams. This is achieved by measuri"
D16-1175,J16-4006,1,0.590213,"antic knowledge into the vector space by leveraging the intrinsic structure of the distributional neighbourhood. This results in richer word representations and furthermore mitigates the sparsity effect common in high-dimensional vector spaces, while remaining fully interpretable. Our contributions are as follows: we show that typed and untyped sparse word representations, enriched by distributional inference, lead to performance improvements on several word similarity benchmarks, and that a higher-order dependency-typed vector space model, based on “Anchored Packed Dependency Trees (A PTs)” (Weir et al., 2016), is competitive with the state-of-the-art for adjective-noun, noun-noun and verb-object compositions. Using our method, we are able to bridge the gap in performance between high dimensional interpretable mod1691 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1691–1702, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics els and low dimensional non-interpretable models and offer evidence to support a possible explanation of why high-dimensional models usually perform worse, together with a simple, practical method f"
D16-1175,J15-1010,0,0.014062,"g of a phrase in a local context. The remainder of this paper is structured as follows: we discuss related work in section 2, followed by an introduction of the A PT framework for semantic composition in section 3. We describe distributional inference in section 4 and present our experimental work, together with our results in section 5. We conclude this paper and outline future work in section 6. 2 al. (2010), Grefenstette et al. (2013) and Paperno et al. (2014), and neural network based approaches, such as Socher et al. (2012), Le and Zuidema (2015), Mou et al. (2015) and Tai et al. (2015). Zanzotto et al. (2015) provide a decompositional analysis of how similarity is affected by distributional composition, and link compositional models to convolution kernels. Most closely related to our approach of composition are the works of Thater et al. (2010), Thater et al. (2011) and Weeds et al. (2014), which aim to provide a general model of compositionality in a typed distributional vector space. In this paper we adopt the approach to distributional composition introduced by Weir et al. (2016), whose A PT framework is based on a higher-order dependency-typed vector space, however they do not address the issu"
D16-1175,C98-2122,0,\N,Missing
E09-1034,afonso-etal-2002-floresta,0,0.11534,"es Nonprojective By gap degree Gap Gap Gap degree 2 degree 3 deg. > 3 13 2 1 359 4 1 10 0 0 427 13 0 188 10 2 351 51 14 81 21 10 19 7 5 29 0 0 WellNested 204 20257 856 4850 1552 1711 550 1008 665 By nestedness Mildly Strongly Ill-Nested Ill-Nested 1 0 96 0 8 0 15 0 191 0 7 0 5 0 71 0 20 0 Table 1: Counts of dependency trees classified by gap degree, and mild and strong ill-nestedness (for their gap degree); appearing in treebanks for Arabic (Hajiˇc et al., 2004), Czech (Hajiˇc et al., 2006), Danish (Kromann, 2003), Dutch (van der Beek et al., 2002), Latin (Bamman and Crane, 2006), Portuguese (Afonso et al., 2002), Slovene (Dˇzeroski et al., 2006), Swedish (Nilsson et al., 2005) and Turkish (Oflazer et al., 2003; Atalay et al., 2003). like the fastest known parsers for LTAG, and can be made O(n6 ) if we use unlexicalised dependencies. When the gap degree is greater than 1, the time complexity goes up by a factor of n2 for each extra unit of gap degree, as in parsers for coupled context-free grammars. Most of the non-projective sentences appearing in treebanks are well-nested and have a small gap degree, so this algorithm directly parses the vast majority of the non-projective constructions present in n"
E09-1034,dzeroski-etal-2006-towards,0,0.13524,"Missing"
E09-1034,P99-1059,0,0.353771,"sed to prove the correctness of a parser: for each input string, a parsing schema’s deduction steps allow us to infer a set of items, called valid items for that string. A schema is said to be sound if all valid final items it produces for any arbitrary string are correct for that string. A schema is said to be complete if all correct final items are valid. A correct parsing schema is one which is both sound and complete. In constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions. In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the underlying logic of the parser while abstracting away"
E09-1034,W00-2011,0,0.821295,"only if T is projective. The subtrees induced by nodes wp and wq are interleaved if bwp c ∩ bwq c = ∅ and there are nodes wi , wj ∈ bwp c and wk , wl ∈ bwq c such that i < k < j < l. A dependency tree T is well-nested if it does not contain two interleaved subtrees. A tree that is not well-nested is said to be ill-nested. Note that projective trees are always well-nested, but well-nested trees are not always projective. for well-nested dependency structures of gap degree 1, and prove its correctness. The parser runs in time O(n7 ), the same complexity as the best existing algorithms for LTAG (Eisner and Satta, 2000), and can be optimised to O(n6 ) in the nonlexicalised case; (2) we generalise the previous algorithm to any well-nested dependency structure with gap degree at most k in time O(n5+2k ); (3) we generalise the previous parsers to be able to analyse not only well-nested structures, but also ill-nested structures with gap degree at most k satisfying certain constraints1 , in time O(n4+3k ); and (4) we characterise the set of structures covered by this parser, which we call mildly ill-nested structures, and show that it includes all the trees present in a number of dependency treebanks. 2.2 Depend"
E09-1034,C96-1058,0,0.0911021,"ema is said to be sound if all valid final items it produces for any arbitrary string are correct for that string. A schema is said to be complete if all correct final items are valid. A correct parsing schema is one which is both sound and complete. In constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions. In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the underlying logic of the parser while abstracting away from control structures (the particular model used to create the decisions associated with D-rules). Furthermore, the choice points in the parsing process and the i"
E09-1034,P08-1110,1,0.793227,"Missing"
E09-1034,N09-1061,1,0.851787,"Missing"
E09-1034,P07-1077,0,0.572169,"Spain cgomezr@udc.es David Weir and John Carroll Department of Informatics University of Sussex, United Kingdom {davidw,johnca}@sussex.ac.uk Abstract the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are “close” to being projective, since they contain only a small proportion of nonprojective arcs. This has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (Kuhlmann and Nivre, 2006; Havelka, 2007). Kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky et al., 2005), relating them to lexicalised constituency grammar formalisms. Specifically, he shows that: linear context-free rewriting systems (LCFRS) with fan-out k (VijayShanker et al., 1987; Satta, 1992) induce the set of dependency structures with gap degree at most k − 1; coupled context-free grammars in which the maximal rank of a nonterminal is k (Hotz and Pitsch, 1996) induce the set of well-nested dependency structures with gap degree at most k − 1; and LTAGs (Joshi and"
E09-1034,P07-1021,0,0.686144,"Missing"
E09-1034,P06-2066,0,0.641413,"Universidade da Coru˜na, Spain cgomezr@udc.es David Weir and John Carroll Department of Informatics University of Sussex, United Kingdom {davidw,johnca}@sussex.ac.uk Abstract the problem is intractable in the absence of this assumption (McDonald and Satta, 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice are “close” to being projective, since they contain only a small proportion of nonprojective arcs. This has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (Kuhlmann and Nivre, 2006; Havelka, 2007). Kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky et al., 2005), relating them to lexicalised constituency grammar formalisms. Specifically, he shows that: linear context-free rewriting systems (LCFRS) with fan-out k (VijayShanker et al., 1987; Satta, 1992) induce the set of dependency structures with gap degree at most k − 1; coupled context-free grammars in which the maximal rank of a nonterminal is k (Hotz and Pitsch, 1996) induce the set of well-nested dependency structures with gap degree at most k − 1; and L"
E09-1034,W07-2216,0,0.488419,"Missing"
E09-1034,H05-1066,0,0.694573,"Missing"
E09-1034,P05-1013,0,0.235701,"ately, the general problem of parsing ill-nested structures is NPcomplete, even when the gap degree is bounded: this set of structures is closely related to LCFRS with bounded fan-out and unbounded production length, and parsing in this formalism has been proven to be NP-complete (Satta, 1992). The reason for this high complexity is the problem of unrestricted crossing configurations, appearing when dependency subtrees are allowed to interleave in every possible way. However, just as it has been noted that most non-projective structures appearing in practice are only “slightly” nonprojective (Nivre and Nilsson, 2005), we characterise a sense in which the structures appearing in treebanks can be viewed as being only “slightly” ill-nested. In this section, we generalise the algorithms WG1 and WGk to parse a proper superset of the set of well-nested structures in polynomial time; and give a characterisation of this new set of structures, which includes all the structures in several dependency treebanks. 4.2 Computational complexity The WGk parser runs in time O(n5+2k ): as in the case of WG1 , the deduction step with most free variables is Combine Shrinking Gap Centre, and in this case it has 5 + 2k free ind"
E09-1034,P92-1012,0,0.86861,"ring in practice are “close” to being projective, since they contain only a small proportion of nonprojective arcs. This has led to the study of classes of dependency structures that lie between projective and unrestricted non-projective structures (Kuhlmann and Nivre, 2006; Havelka, 2007). Kuhlmann (2007) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky et al., 2005), relating them to lexicalised constituency grammar formalisms. Specifically, he shows that: linear context-free rewriting systems (LCFRS) with fan-out k (VijayShanker et al., 1987; Satta, 1992) induce the set of dependency structures with gap degree at most k − 1; coupled context-free grammars in which the maximal rank of a nonterminal is k (Hotz and Pitsch, 1996) induce the set of well-nested dependency structures with gap degree at most k − 1; and LTAGs (Joshi and Schabes, 1997) induce the set of well-nested dependency structures with gap degree at most 1. These results establish that there must be polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, since such parsers exist for their corresponding lexicalised constituency-based formali"
E09-1034,P87-1015,1,0.825185,"Missing"
E09-1034,W03-3023,0,0.231743,"valid final items it produces for any arbitrary string are correct for that string. A schema is said to be complete if all correct final items are valid. A correct parsing schema is one which is both sound and complete. In constituency-based parsing schemata, deduction steps usually have grammar rules as side conditions. In the case of dependency parsers it is also possible to use grammars (Eisner and Satta, 1999), but many algorithms use a data-driven approach instead, making individual decisions about which dependencies to create by using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the underlying logic of the parser while abstracting away from control structures (the particular model used to create the decisions associated with D-rules). Furthermore, the choice points in the parsing process and the information we can use to make decisions are"
E09-1034,W03-2405,0,\N,Missing
E17-2085,J10-4006,0,0.048956,"extual features may also be defined 529 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 529–534, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics have features of the same type (e.g., DOBJ). in terms of dependency relations. For example, in a dependency parse of the sentence we would expect to see a direct-object relation from folded to student. Contextual features based on dependency relations may be typed (i.e., include the name of the dependency relation) or untyped (Baroni and Lenci, 2010). Pad´o and Lapata (2007) proposed using dependency paths to define untyped contextual features; here any word in the context which has a dependency path to the target is considered a contextual feature. Weeds et al. (2014) proposed using dependency paths to define typed contextual features which could be used to align representations before composition. This idea is further refined in the A PT framework of Weir et al. (2016). 3 Compositionality of compound nouns Compositionality detection (Reddy et al., 2011) involves deciding whether a given multiword expression is compositional or not i.e.,"
E17-2085,D10-1115,0,0.0400614,"lt from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, they found that using weighted addition outperformed multiplication as a compositionality function. With their optimal settings, they achieved a Spearman’s rank correlation coefficient of 0.714 with the human judgments, which remains the Na¨ıve composition of distributional representations, e.g., using pointwise addition and multiplication, has proved very popular and effective. In an evaluation across 3 different benchmark tasks (Dinu et al., 2013), the lexical function model (Baroni and Zamparelli, 2010) was shown to be consistently the best-performing, but in the composition of adjective-noun phrases, simple additive and multiplicative models were highly competitive. Milajevs et al. (2014) compared neural word representations with count-based vectors on 4 different tasks using a variety of na¨ıve and tensorbased compositional models. The neural word representations consistently outperformed the traditional count-based vectors. Considering the results for the neural word representations, pointwise addition outperformed all of the other compositional models considered on 3 of the tasks. Typed"
E17-2085,W04-0308,0,0.0136012,"ion of student from the perspective of actions (i.e., verbs) which are likely to be carried out by students. This view can be straightforwardly composed with the representation of folded because the representations are aligned i.e., they 530 state-of-the-art on this dataset1 . For consistency with the experiments of Reddy et al. (2011), the corpus used in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008). This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004). It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item. We then created a number of elementary representations for every token in the corpus. 3.1 values. Levy et al. (2015) showed that the use of context distribution smoothing (α = 0.75) in the PMI calculation can lead to performance comparable with state-of-the-art word embeddings on word similarity tasks. We use this modified definition of PMI and experiment"
E17-2085,W13-3206,0,0.0189528,"ed out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, they found that using weighted addition outperformed multiplication as a compositionality function. With their optimal settings, they achieved a Spearman’s rank correlation coefficient of 0.714 with the human judgments, which remains the Na¨ıve composition of distributional representations, e.g., using pointwise addition and multiplication, has proved very popular and effective. In an evaluation across 3 different benchmark tasks (Dinu et al., 2013), the lexical function model (Baroni and Zamparelli, 2010) was shown to be consistently the best-performing, but in the composition of adjective-noun phrases, simple additive and multiplicative models were highly competitive. Milajevs et al. (2014) compared neural word representations with count-based vectors on 4 different tasks using a variety of na¨ıve and tensorbased compositional models. The neural word representations consistently outperformed the traditional count-based vectors. Considering the results for the neural word representations, pointwise addition outperformed all of the other"
E17-2085,I11-1024,0,0.454319,"ons may be typed (i.e., include the name of the dependency relation) or untyped (Baroni and Lenci, 2010). Pad´o and Lapata (2007) proposed using dependency paths to define untyped contextual features; here any word in the context which has a dependency path to the target is considered a contextual feature. Weeds et al. (2014) proposed using dependency paths to define typed contextual features which could be used to align representations before composition. This idea is further refined in the A PT framework of Weir et al. (2016). 3 Compositionality of compound nouns Compositionality detection (Reddy et al., 2011) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts. Reddy et al. (2011) introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level. All judgments are given on a scale of 0 to 5, where 5 is high. For example, the phrase spelling bee is deemed to have high literalness in its use of the first constituent, low literalness in its use of the second constituent and a medium level of liter"
E17-2085,J07-2002,0,0.161072,"Missing"
E17-2085,N15-1099,0,0.0491285,"Missing"
E17-2085,S12-1021,0,0.0221357,"d unaligned A PTs in the composition process, we used a hybrid method where the composed representation is defined as: o Gn (qAδ1 + (1 − q)A1 ), A2 (5) For each word and compound phrase, elementary A PT representations were constructed using the method and recommended settings of Weir et al. (2016). For efficiency, we did not consider paths of length 3 or more. In relation to the construction of the elementary A PTs, the most obvious parameter is the nature of the weight associated with each feature. We consider both the use of probabilities2 and positive pointwise mutual information (PPMI) 1 Hermann et al. (2012) proposed using generative models for modeling the compositionality of noun-noun compounds. Using interpolation to mitigate the sparse data problem, their model beat the baseline of weighted addition on the Reddy et al. (2011) evaluation task when trained on the BNC. However, these results were still significantly lower than those reported by Reddy et al. (2011) using the larger ukWaC corpus. 2 referred to as normalised counts by Weir et al. (2016) INT 3 α = 1 corresponds to the standard definition of PMI used elsewhere. 531 Embedding method cbow, 50d cbow, 100d cbow, 300d skip-gram, 50d skip-"
E17-2085,D13-1147,0,0.0186813,"ly. Thus, a good compositionality function, without any access to the observed co-occurrences of the target phrases, is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Accordingly, as observed elsewhere (Reddy et al., 2011; Salehi et al., 2015; Yazdani et al., 2015), compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. Reddy et al. (2011) carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, they found that using weighted addition outperformed multiplication as a compositionality function. With their optimal settings, they achieved a Spearman’s rank correlation coefficient of 0.714 with the human judgments, which remains the Na¨ıve composition of distri"
E17-2085,S13-1038,0,0.0455679,"Missing"
E17-2085,Q15-1016,0,0.0252971,"ed in this experiment is the same fullyannotated version of the web-derived ukWaC corpus (Ferraresi et al., 2008). This corpus has been tokenised, POS-tagged and lemmatised with TreeTagger (Schmid, 1994) and dependency-parsed with the Malt Parser (Nivre, 2004). It contains about 1.9 billion tokens. In order to create a corpus which contains compound nouns, we further preprocessed the corpus by identifying occurrences of the 90 target compound nouns and recombining them into a single lexical item. We then created a number of elementary representations for every token in the corpus. 3.1 values. Levy et al. (2015) showed that the use of context distribution smoothing (α = 0.75) in the PMI calculation can lead to performance comparable with state-of-the-art word embeddings on word similarity tasks. We use this modified definition of PMI and experiment with α = 0.75 and α = 1.3 Having constructed elementary A PTs, the A PT composition process involves aligning and composing F these elementary APTs. We investigate using INT , which takes the minimumFof each of the constituent’s feature values and UNI , which performs pointwise addition. F Following Reddy et al. (2011), when using the UNI operation, we exp"
E17-2085,L16-1362,0,0.0324394,"Missing"
E17-2085,J16-4006,1,0.816313,"bject relation from folded to student. Contextual features based on dependency relations may be typed (i.e., include the name of the dependency relation) or untyped (Baroni and Lenci, 2010). Pad´o and Lapata (2007) proposed using dependency paths to define untyped contextual features; here any word in the context which has a dependency path to the target is considered a contextual feature. Weeds et al. (2014) proposed using dependency paths to define typed contextual features which could be used to align representations before composition. This idea is further refined in the A PT framework of Weir et al. (2016). 3 Compositionality of compound nouns Compositionality detection (Reddy et al., 2011) involves deciding whether a given multiword expression is compositional or not i.e., whether the meaning can be understood from the literal meaning of its parts. Reddy et al. (2011) introduced a dataset consisting of 90 compound nouns along with human judgments of their literality or compositionally at both the constituent and the phrase level. All judgments are given on a scale of 0 to 5, where 5 is high. For example, the phrase spelling bee is deemed to have high literalness in its use of the first constit"
E17-2085,D15-1201,0,0.0172001,"urred with one or both of the constituents independently. On the other hand, the observed cooccurrences of non-compositional target phrases are much less likely to have occurred with either of the constituents independently. Thus, a good compositionality function, without any access to the observed co-occurrences of the target phrases, is highly likely to return vectors which are similar to observed phrasal vectors for compositional phrases but much less likely to return similar vectors for non-compositional phrases. Accordingly, as observed elsewhere (Reddy et al., 2011; Salehi et al., 2015; Yazdani et al., 2015), compositional methods can be evaluated by correlating the similarity of composed and observed phrase representations with the human judgments of compositionality. A similar idea is also explored by Kiela and Clark (2013) who detect noncompositional phrases by comparing the neighbourhoods of phrases where individual words have been substituted for similar words. Reddy et al. (2011) carried out experiments with a vector space model built from ukWaC (Ferraresi et al., 2008) using untyped co-occurrences (window size=100). Used 3-fold cross-validation, they found that using weighted addition outp"
E17-2085,W14-1502,1,\N,Missing
E95-1011,P91-1042,0,0.0298811,"Missing"
E95-1011,W89-0202,0,0.0558448,"Missing"
E95-1011,P90-1023,0,0.0285987,"Missing"
E95-1011,J92-4005,0,0.0435034,"Missing"
E95-1011,P85-1016,0,0.0768127,"Missing"
E95-1011,C90-2039,0,0.0460152,"Missing"
E95-1011,P85-1017,0,0.0802238,"Missing"
E95-1011,P84-1075,0,0.0257856,"cation-based grammar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our choice of constraints is motivated by showing how they generalize constraints inherent in Linear Indexed Grammar (l_lG). We begin by describing how constraints inherent in I.IG admit tractable processing algorithms and then consider how these constraints can be generalized to a formalism that manipulates trees rather than stacks. The constraints that we identify for the tree-based system can be regarded equally well as constraints on unification-based grammar formalisms such as PArR (Shieber, 1984). Vijay-Shanker and Weir (1993) show that Linear Indexed Grammars (I_IG) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing. This paper describes a formalism that is more powerful than I_IG, but which can also be processed in polynomial time using similar techniques. The formalism, which we refer to as Partially Linear PATR (PI_PATR) manipulates feature structures rather than stacks. 1 Introduction Unification-based grammar formalisms can be viewed as generalizations of Context-Free Grammars (CFG) where the nonterminal symbo"
E95-1011,P85-1018,0,0.0606252,"Missing"
E95-1011,J93-4002,1,0.941465,"mmar formalisms in order to guarantee the existence of polynomial time parsing algorithms. Our choice of constraints is motivated by showing how they generalize constraints inherent in Linear Indexed Grammar (l_lG). We begin by describing how constraints inherent in I.IG admit tractable processing algorithms and then consider how these constraints can be generalized to a formalism that manipulates trees rather than stacks. The constraints that we identify for the tree-based system can be regarded equally well as constraints on unification-based grammar formalisms such as PArR (Shieber, 1984). Vijay-Shanker and Weir (1993) show that Linear Indexed Grammars (I_IG) can be processed in polynomial time by exploiting constraints which make possible the extensive use of structure-sharing. This paper describes a formalism that is more powerful than I_IG, but which can also be processed in polynomial time using similar techniques. The formalism, which we refer to as Partially Linear PATR (PI_PATR) manipulates feature structures rather than stacks. 1 Introduction Unification-based grammar formalisms can be viewed as generalizations of Context-Free Grammars (CFG) where the nonterminal symbols are replaced by an infinite"
E95-1011,P87-1015,1,0.848792,"Missing"
E95-1011,P91-1041,0,\N,Missing
E95-1011,1991.iwpt-1.19,0,\N,Missing
E99-1029,C96-1034,0,0.021412,"enerated by a lexicalized CFG. But does the EDOL have any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage LTAGs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on g r a m m a r size, this could have an adverse effect on parsing efficiency. However, the problem of g r a m m a r size in TAG has to some extent been addressed both with respect to g r a m m a r encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies t h a t are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the g r a m m a r , they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependen"
E99-1029,W98-0108,1,0.847898,"arsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over unbounded distances across the tree if the EDOL Proceedings of EACL '99 S $ NP[¢~ :,¢¢] S (whom) SNP VP e Figure 1: Localizing a filler-gap dependency makes it possible for the gap and its filler to be located within the same elementary structure. This paper presents an investigation into the extent to which the EDOL reduces the need for feature passing in two existing wide-coverage grammars: the XTAG grammar (XTAG-Group, 1995), and the LEXSYS grammar (Carroll et al., 1998). It can be seen as an evaluation of how well these two grammars make use of the EDOL hypothesis with respect to those dependencies that are being checked by the parser. 2 Parsing Unification-Based Grammars In phrase-structure rule-based parsing, each rule corresponds to a local tree. A rule is applied to a sequence of existing contiguous constituents, if they are compatible with the daughters. In the case of context-free grammar (CFG), the compatibility check is just equality of atomic symbols, and an instantiated daughter is merely the corresponding sub-constituent. However, unification-base"
E99-1029,P91-1042,0,0.0464762,"Missing"
E99-1029,P98-1061,1,0.726463,"any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage LTAGs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on g r a m m a r size, this could have an adverse effect on parsing efficiency. However, the problem of g r a m m a r size in TAG has to some extent been addressed both with respect to g r a m m a r encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies t h a t are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the g r a m m a r , they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over unbounded distances across the tree if the EDOL Pro"
E99-1029,P95-1011,1,0.819731,"ed by a CFG can be generated by a lexicalized CFG. But does the EDOL have any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage LTAGs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on g r a m m a r size, this could have an adverse effect on parsing efficiency. However, the problem of g r a m m a r size in TAG has to some extent been addressed both with respect to g r a m m a r encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies t h a t are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the g r a m m a r , they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish fil"
E99-1029,C94-1024,0,0.0291243,"G. But does the EDOL have any other more direct effects on parsing efficiency? On the one hand, it is a consequence of the EDOL that wide-coverage LTAGs are larger than their rule-based counterparts. With larger elementary structures, generalizations are lost regarding the internal structure of the elementary trees. Since parse time depends on g r a m m a r size, this could have an adverse effect on parsing efficiency. However, the problem of g r a m m a r size in TAG has to some extent been addressed both with respect to g r a m m a r encoding (Evans et al., 1995; Candito, 1996) and parsing (Joshi and Srinivas, 1994; Evans and Weir, 1998). On the other hand, if the EDOL hypothesis holds for those dependencies t h a t are being checked by the parser, then the burden of passing feature values around during parsing will be less than in a rule-based framework. If all dependencies that the parser is checking can be stated directly within the elementary structures of the g r a m m a r , they do not need to be computed dynamically during the parsing process by means of feature percolation. For example, there is no need to use a slash feature to establish filler-gap dependencies over unbounded distances across t"
E99-1029,C86-1016,0,0.1196,"Missing"
E99-1029,C90-2039,0,0.0524163,"Missing"
E99-1029,P85-1017,0,0.16914,"Missing"
E99-1029,P95-1021,1,0.821229,"ion of the tree in Figure 2 would cause the .feature structure at the node for the subject to'include pn:+. This, however, does not violate the EDOL hypothesis since this feature is not coreferenced with any other feature in the tree. 3 Analysis of two wide-coverage grammars As we have seen, the EDOL of LTAGs makes it possible, at least in principle, to locally express dependencies which cannot be localized in a CFGbased formalism. In this section we consider two existing grammars: the XTAG grammar, a widecoverage LTAG, and the LEXSYS grammar, a widecoverage D-Tree Substitution G r a m m a r (Rambow et al., 1995). For each g r a m m a r we investigate the extend to which they do not take full advantage of the EDOL and require percolation of features at parse time. There are a number of instances in which dependencies are not localized in the XTAG grammar, most of which involve auxiliary trees. There are Proceedings of EACL '99 three types of auxiliary trees: predicative, modifier and coordination auxiliary trees. In predicative auxiliary trees the anchor is also the head of the tree and becomes the head of the tree resulting from the adjunction. In modifier auxiliary trees, the anchor is not the head"
E99-1029,1995.iwpt-1.8,1,\N,Missing
E99-1029,W96-0209,1,\N,Missing
E99-1029,A94-1009,0,\N,Missing
E99-1029,C96-2183,0,\N,Missing
E99-1029,W97-1306,0,\N,Missing
E99-1029,P91-1041,0,\N,Missing
E99-1029,1991.iwpt-1.19,0,\N,Missing
E99-1029,C98-1059,1,\N,Missing
E99-1029,A97-2017,0,\N,Missing
J01-1004,E91-1005,1,0.838818,"Missing"
J01-1004,1995.iwpt-1.6,1,0.81509,"Missing"
J01-1004,W98-0107,0,0.223773,"Missing"
J01-1004,E99-1029,1,0.906601,"Missing"
J01-1004,W00-2007,1,0.862891,"Missing"
J01-1004,P97-1003,0,0.100262,"Missing"
J01-1004,W98-0116,0,0.0543715,"Missing"
J01-1004,C96-1091,0,0.0632209,"Missing"
J01-1004,W98-0117,0,0.0362597,"Missing"
J01-1004,P95-1013,1,0.883932,"Missing"
J01-1004,W98-0124,0,0.0468347,"Missing"
J01-1004,P83-1020,0,0.179008,"Missing"
J01-1004,W98-0135,1,0.911043,"Missing"
J01-1004,P95-1021,1,0.931775,"Missing"
J01-1004,P92-1010,1,0.861998,"2, we give some formal definitions and in Section 3 discuss some of the formal properties of DSG. In Section 4, we present analyses in DSG for various linguistic constructions in several languages, and compare them to the corresponding LTAG analyses. In Section 5, we discuss the particular problem of modeling syntactic dependency. We conclude with a discussion of some related work and summary. 2. D e f i n i t i o n of D S G D-trees are the primitive elements of a DSG. D-trees are descriptions of trees, in particular, certain types of expressions in a tree description language such as that of Rogers and Vijay-Shanker (1992). In this section we define tree descriptions and substitution of tree descriptions (Section 2.1) and d-trees (Section 2.2) together with some associated terminology and the graphical representation (Section 2.3). We then define d-tree substitution grammars, along with derivations of d-tree substitution grammars (Section 2.4) and languages generated by these grammars (Section 2.5), and close with an informal discussion of path constraints (Section 2.6). 2.1 Tree D e s c r i p t i o n s and S u b s t i t u t i o n In the following, we are interested in a tree description language that provides"
J01-1004,J94-1004,0,0.260946,"Missing"
J01-1004,J92-4004,1,0.961676,"the tree into which adjunction occurs. In LTAG, the lexicalized elementary objects are defined in such a w a y that the structural relationships between the anchor and each of its dependents change during the course of a derivation through the operation of adjunction, as just illustrated. This approach is not the only possibility. An alternative would be to define the relationships between the nodes of the elementary objects in such a w a y that these relationships hold throughout the derivation, regardless of how the derivation proceeds. This perspective on the LTAG formalism was explored in Vijay-Shanker (1992) where, following the principles of d-theory parsing (Marcus, Hindle, and Fleck 1983), LTAG was seen as a system manipulating descriptions of trees rather than as a tree 2 The same analysis holds for wh-movement, but w e use topicalization as an example in order to avoid the superficial complication of the auxiliary n e e d e d in English questions. Sometimes, topicalized sentences s o u n d s o m e w h a t less natural than the corresponding wh-questions, which are always structurally equivalent. 88 Rambow, Vijay-Shanker, and Weir fl&apos;: NP I Peter S NP S D-Tree Substitution Grammars I I I S yo"
J01-1004,1995.iwpt-1.30,1,0.873684,"Missing"
J01-1004,P94-1036,1,\N,Missing
J02-2003,W99-0901,0,0.744355,"h to the problem of estimating the frequencies of senses, by distributing the count for each noun in the data evenly among all senses of the noun:  f (n, v, r) ˆf (c, v, r) = (15) |cn(n)| n∈syn(c) where ˆf (c, v, r) is an estimate of the number of times that concept c appears in position r of verb v, and |cn(n) |is the cardinality of cn(n). This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude technique works surprisingly well. Alternative approaches are described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999), and Ciaramita and Johnson (2000). 4. Using a Chi-Square Test to Compare Probabilities In this section we show how to test whether p(v |c , r) changes significantly when considering a node higher in the hierarchy. Consider the problem of deciding whether p(run |canine, subj) is a good approximation of p(run |dog, subj). (canine is the parent of dog in WordNet.) To do this, the probabilities p(run |ci , subj) are compared using a chi-square test, where the ci are the children of canine. In this case, the null hypothesis of the test is that the probabilities p(run |ci , subj) are t"
J02-2003,W01-0703,0,0.0408907,"c, can be used to estimate p(c |v, r). (Recall that c denotes the set of concepts dominated by c , including c itself.) One possible approach would be simply to substitute c for the individual concept c. This is a poor solution, however, since p(c |v, r) is the conditional probability that 2 Angled brackets are used to denote concepts in the hierarchy. 3 The term predicate is used loosely here, in that the predicate does not have to be a semantic object but can simply be a word form. 4 A recent paper that extends the acquisition of selectional preferences to sense-sense relationships is Agirre and Martinez (2001). 189 Computational Linguistics Volume 28, Number 2 some noun denoting a concept in c appears in position r of verb v. For example, p(animal |run, subj) is the probability that some noun denoting a kind of animal appears in the subject position of the verb run. Probabilities of sets of concepts are obtained by summing over the concepts in the set: p(c |v, r) =  p(c |v, r) (1) c ∈c This means that p(animal |run, subj) is likely to be much greater than p(dog | run, subj) and thus is not a good approximation of p(dog |run, subj). What can be done, though, is to condition on sets o"
J02-2003,W00-1320,0,0.012491,"the extent of generalization, which can be tailored to particular tasks. We have also shown that the task performance is at least as good when using the Pearson chi-square statistic as when using the log-likelihood chi-square statistic. There are a number of ways in which this work could be extended. One possibility would be to use all the classes dominated by the hypernyms of a concept, rather than just one, to estimate the probability of the concept. An estimate would be obtained for each hypernym, and the estimates combined in a linear interpolation. An approach similar to this is taken by Bikel (2000), in the context of statistical parsing. There is still room for investigation of the hidden-data problem when data are used that have not been sense disambiguated. In this article, a very simple approach is taken, 13 χ2 performed slightly better than G2 using the smaller data set also. 204 Clark and Weir Class-Based Probability Estimation which is to split the count for a noun evenly among the noun’s senses. Abney and Light (1999) have tried a more motivated approach, using the expectation maximization algorithm, but with little success. The approach described in Clark and Weir (1999) is show"
J02-2003,A97-1052,0,0.201662,"= = f (r)   v ∈V c ∈C f (c , v , r)    c ∈c f (c , v, r)  ˆ(v |c , r) = f (c ,v,r) =  p   f (c ,r) v ∈V c ∈c f (c , v , r) (12) (13) (14) where f (c, v, r) is the number of (n, v, r) triples in the data in which n is being used to denote c, and V is the set of verbs in the data. The problem is that the estimates are defined in terms of frequencies of senses, whereas the data are assumed to be in the form of (n, v, r) triples: a noun, verb, and argument position. All the data used in this work have been obtained from the British National Corpus (BNC), using the system of Briscoe and Carroll (1997), which consists of a shallow-parsing component that is able to identify verbal arguments. We take a simple approach to the problem of estimating the frequencies of senses, by distributing the count for each noun in the data evenly among all senses of the noun:  f (n, v, r) ˆf (c, v, r) = (15) |cn(n)| n∈syn(c) where ˆf (c, v, r) is an estimate of the number of times that concept c appears in position r of verb v, and |cn(n) |is the cardinality of cn(n). This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude techniq"
J02-2003,J90-1003,0,0.047983,"ich, obj). A feature of the proposed generalization procedure is that comparing probabilities of the form p(v |C, r), where C is a class, is closely related to comparing ratios of probabilities of the form p(C |v, r)/p(C |r) (for a given verb and argument position): p(v |C, r) = p(C |v, r) p(v |r) p(C |r) (10) Note that, for a given verb and argument position, p(v |r) is constant across classes. Equation (10) is of interest because the ratio p(C |v, r)/p(C |r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resnik’s association score, which will be introduced in Section 6. Thus the generalization procedure can be thought of as one that finds “homogeneous” areas of the hierarchy, that is, areas consisting of classes that are associated to a similar degree with the verb (Clark and Weir 1999). Finally, we note that the proposed estimation method does not guarantee that the estimates form a probability distribution over the concepts in the hierarchy, and so a normalization factor is required: psc (c |v, r) =  (c|r) ˆp(v |[c, v, r], r) ˆpˆp(v|r) c ∈C  |r) ˆp(v |[c , v, r],"
J02-2003,C00-1028,0,0.172921,"ng the frequencies of senses, by distributing the count for each noun in the data evenly among all senses of the noun:  f (n, v, r) ˆf (c, v, r) = (15) |cn(n)| n∈syn(c) where ˆf (c, v, r) is an estimate of the number of times that concept c appears in position r of verb v, and |cn(n) |is the cardinality of cn(n). This is the approach taken by Li and Abe (1998), Ribas (1995), and McCarthy (2000).7 Resnik (1998) explains how this apparently crude technique works surprisingly well. Alternative approaches are described in Clark and Weir (1999) (see also Clark [2001]), Abney and Light (1999), and Ciaramita and Johnson (2000). 4. Using a Chi-Square Test to Compare Probabilities In this section we show how to test whether p(v |c , r) changes significantly when considering a node higher in the hierarchy. Consider the problem of deciding whether p(run |canine, subj) is a good approximation of p(run |dog, subj). (canine is the parent of dog in WordNet.) To do this, the probabilities p(run |ci , subj) are compared using a chi-square test, where the ci are the children of canine. In this case, the null hypothesis of the test is that the probabilities p(run |ci , subj) are the same for each child ci . By jud"
J02-2003,W99-0631,1,0.900416,"Note that, for a given verb and argument position, p(v |r) is constant across classes. Equation (10) is of interest because the ratio p(C |v, r)/p(C |r) can be interpreted as a measure of association between the verb v and class C. This ratio is similar to pointwise mutual information (Church and Hanks 1990) and also forms part of Resnik’s association score, which will be introduced in Section 6. Thus the generalization procedure can be thought of as one that finds “homogeneous” areas of the hierarchy, that is, areas consisting of classes that are associated to a similar degree with the verb (Clark and Weir 1999). Finally, we note that the proposed estimation method does not guarantee that the estimates form a probability distribution over the concepts in the hierarchy, and so a normalization factor is required: psc (c |v, r) =  (c|r) ˆp(v |[c, v, r], r) ˆpˆp(v|r) c ∈C  |r) ˆp(v |[c , v, r], r) ˆpˆp(c(v|r) (11) We use psc to denote an estimate obtained using our method (since the technique finds sets of semantically similar senses, or “similarity classes”) and [c, v, r] to denote the class chosen for concept c in position r of verb v; ˆp denotes a relative frequency estimate, and C denotes the set"
J02-2003,C00-1029,1,0.933173,"(NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling. To see how such knowledge can be used to resolve structural ambiguities, consider the following prepositional phrase attachment ambiguity: Example 1 Fred ate strawberries with a spoon. The ambiguity arises because the prepositional phrase with a spoon can attach to either strawberries or ate. The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of “ate-with” than “strawberries-with” (Li and Abe 1998; Clark and Weir 2000). The problem with estimating a probability model defined over a large vocabulary of predicates and noun senses is that this involves a huge number of parameters, which results in a sparse-data problem. In order to reduce the number of parameters, we propose to define a probability model over senses in a semantic hierarchy and ∗ Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail: stephenc@cogsci.ed.ac.uk. † School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK. E-mail: david.weir@cogs.susx.ac.uk. c 2002 Associat"
J02-2003,N01-1013,1,0.0995285,"Missing"
J02-2003,J93-1003,0,0.230062,"re statistic, denoted X2 :  (oij − eij )2 X2 = (16) eij i,j where oij is the observed value for the cell in row i and column j, and eij is the corresponding expected value. An alternative statistic is the log-likelihood chi-square statistic, denoted G2 :8  oij G2 = 2 oij loge (17) eij i,j The two statistics have similar values when the counts in the contingency table are large (Agresti 1996). The statistics behave differently, however, when the table contains low counts, and, since corpus data are likely to lead to some low counts, the question of which statistic to use is an important one. Dunning (1993) argues for the use of G2 rather than X2 , based on an analysis of the sampling distributions of G2 and X2 , and results obtained when using the statistics to acquire highly associated bigrams. We consider Dunning’s analysis at the end of this section, and the question of whether to use G2 or X2 will be discussed further there. For now, we continue with the discussion of how the chi-square test is used in the generalization procedure. For Table 1, the value of G2 is 3.8, and the value of X2 is 2.5. Assuming a level of significance of α = 0.05, the critical value is 12.6 (for six degrees of fre"
J02-2003,C96-1003,0,0.0305241,"Missing"
J02-2003,J98-2002,0,0.313739,"nguage processing (NLP) tasks, such as structural disambiguation and statistical parsing, word sense disambiguation, anaphora resolution, and language modeling. To see how such knowledge can be used to resolve structural ambiguities, consider the following prepositional phrase attachment ambiguity: Example 1 Fred ate strawberries with a spoon. The ambiguity arises because the prepositional phrase with a spoon can attach to either strawberries or ate. The ambiguity can be resolved by noting that the correct sense of spoon is more likely to be an argument of “ate-with” than “strawberries-with” (Li and Abe 1998; Clark and Weir 2000). The problem with estimating a probability model defined over a large vocabulary of predicates and noun senses is that this involves a huge number of parameters, which results in a sparse-data problem. In order to reduce the number of parameters, we propose to define a probability model over senses in a semantic hierarchy and ∗ Division of Informatics, University of Edinburgh, 2 Buccleuch Place, Edinburgh, EH8 9LW, UK. E-mail: stephenc@cogsci.ed.ac.uk. † School of Cognitive and Computing Sciences, University of Sussex, Brighton, BN1 9QH, UK. E-mail: david.weir@cogs.susx."
J02-2003,W97-0808,0,0.0608576,"pseudo-disambiguation task. Our method outperforms these alternatives on the pseudo-disambiguation task, and an analysis of the results shows that the generalization methods of Resnik and Li and Abe appear to be overgeneralizing, at least for this task. Note that the problem being addressed here is the engineering problem of estimating predicate argument probabilities, with the aim of producing estimates that will be useful for NLP applications. In particular, we are not addressing the problem of acquiring selectional restrictions in the way this is usually construed (Resnik 1993; Ribas 1995; McCarthy 1997; Li and Abe 1998; Wagner 2000). The purpose of using a semantic hierarchy for generalization is to overcome the sparse data problem, rather than find a level of abstraction that best represents the selectional restrictions of some predicate. This point is considered further in Section 5. The next section describes the noun hierarchy from WordNet and gives a more precise description of the probabilities to be estimated. Section 3 shows how a class from WordNet can be used to estimate the probability of a noun sense. Section 4 shows how a chi-square test is used as part of the generalization pr"
J02-2003,A00-2034,0,0.25818,"timate the probability of the sense? And second, given a particular noun sense, how can a suitable class be determined? This article offers novel solutions to both problems, and there is a particular focus on the second question, which can be thought of as how to find a suitable level of generalization in the hierarchy.1 The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum 1998), version 1.6. Previous work has considered how to estimate probabilities using classes from WordNet in the context of acquiring selectional preferences (Resnik 1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also addressed the question of how to determine a suitable level of generalization in the hierarchy. Li and Abe use the minimum description length principle to obtain a level of generalization, and Resnik uses a simple technique based on a statistical measure of selectional preference. (The work by Ribas builds on that by Resnik, and the work by McCarthy builds on that by Li and Abe.) We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task. Our method outperforms these alternatives on the pseudo-disambiguation task,"
J02-2003,N01-1011,0,0.00941752,"problem, moving up a node is desirable, and therefore we do not modify the test for tables with low counts. The final issue to consider is which chi-square statistic to use. Dunning (1993) argues for the use of G2 rather than X2 , based on the claim that the sampling distribution of G2 approaches the true chi-square distribution quicker than the sampling distribution of X2 . However, Agresti (1996, page 34) makes the opposite claim: “The sampling distributions of X2 and G2 get closer to chi-squared as the sample size n increases. . . . The convergence is quicker for X2 than G2 .” In addition, Pedersen (2001) questions whether one statistic should be preferred over the other for the bigram acquisition task and cites Cressie and Read (1984), who argue that there are some cases where the Pearson statistic is more reliable than the log-likelihood statistic. Finally, the results of the pseudo-disambiguation experiments presented in Section 7 are at least as good, if not better, when using X2 rather than G2 , and so we conclude that the question of which statistic to use should be answered on a per application basis. 5. The Generalization Procedure The procedure for finding a suitable class, c , to ge"
J02-2003,P93-1024,0,0.557123,"e approach described in Clark and Weir (1999) is shown in Clark (2001) to have some impact on the pseudo-disambiguation task, but only with certain values of the α parameter, and ultimately does not improve on the best performance. Finally, an issue that has not been much addressed in the literature (except by Li and Abe [1996]) is how the accuracy of class-based estimation techniques compare when automatically acquired classes, as opposed to the manually created classes from WordNet, are used. The pseudo-disambiguation task described here has also been used to evaluate clustering algorithms (Pereira, Tishby, and Lee, 1993; Rooth et al., 1999), but with different data, and so it is difficult to compare the results. A related issue is how the structure of WordNet affects the accuracy of the probability estimates. We have taken the structure of the hierarchy for granted, without any analysis, but it may be that an alternative design could be more conducive to probability estimation. Acknowledgments This article is an extended and updated version of a paper that appeared in the proceedings of NAACL 2001. The work on which it is based was carried out while the first author was a D.Phil. student at the University of"
J02-2003,E95-1016,0,0.27864,"can that class be used to estimate the probability of the sense? And second, given a particular noun sense, how can a suitable class be determined? This article offers novel solutions to both problems, and there is a particular focus on the second question, which can be thought of as how to find a suitable level of generalization in the hierarchy.1 The semantic hierarchy used here is the noun hierarchy of WordNet (Fellbaum 1998), version 1.6. Previous work has considered how to estimate probabilities using classes from WordNet in the context of acquiring selectional preferences (Resnik 1998; Ribas 1995; Li and Abe 1998; McCarthy 2000), and this previous work has also addressed the question of how to determine a suitable level of generalization in the hierarchy. Li and Abe use the minimum description length principle to obtain a level of generalization, and Resnik uses a simple technique based on a statistical measure of selectional preference. (The work by Ribas builds on that by Resnik, and the work by McCarthy builds on that by Li and Abe.) We compare our estimation method with those of Resnik and Li and Abe, using a pseudo-disambiguation task. Our method outperforms these alternatives on"
J02-2003,P99-1014,0,0.608779,"ich is close to entity and dominated by entity, has two parents: life form and causal agent. This DAG-like property was responsible for the overgeneralization, and so we removed the link between person and causal agent. This appeared to solve the problem, and the results presented later for the average degree of generalization do not show an overgeneralization compared with those given in Li and Abe (1998). 7. Pseudo-Disambiguation Experiments The task we used to compare the class-based estimation techniques is a decision task previously used by Pereira, Tishby, and Lee (1993) and Rooth et al. (1999). The task is to decide which of two verbs, v and v , is more likely to take a given noun, n, as an object. The test and training data were obtained as follows. A number of verb–direct object pairs were extracted from a subset of the BNC, using the system of Briscoe and Carroll. All those pairs containing a noun not in WordNet were removed, and each verb and argument was lemmatized. This resulted in a data set of around 1.3 million (v, n) pairs. To form a test set, 3,000 of these pairs were randomly selected such that each selected pair contained a fairly frequent verb. (Following Pereira, Ti"
J05-4002,1995.iwpt-1.8,0,0.0125746,"Missing"
J05-4002,J92-4003,0,0.032014,"Missing"
J05-4002,P99-1016,0,0.0266263,"Missing"
J05-4002,W96-0209,0,0.0364362,"Missing"
J05-4002,P89-1010,0,0.310549,"e tokens occurring with w2 that also occur with w1 . Hence, words have the same features as in the type-based CRM, but each feature is given a weight based on its probability of occurrence. Since F(w) =  {c : D(w, c) &gt; 0} = {c : P(c|w) &gt; 0}, it follows that F(w) Dtok (w, c) = 1, and therefore the expressions for precision and recall can be simplified: add (w1 , w2 ) Ptok   D (w , c) =  TP tok 1 = P(c, w1 ) F(w1 ) Dtok (w1 , c) (7)   D (w , c) =  TP tok 2 = P(c, w2 ) F(w2 ) Dtok (w2 , c) (8) TP Radd tok (w1 , w2 ) TP Additive MI-based CRM (Dmi ). Using pointwise mutual information (MI) (Church and Hanks 1989) as the weight function means that a co-occurrence c is considered a feature of word n if the probability of their co-occurrence is greater than would be expected if words occurred independently. In addition, more informative co-occurrences contribute more to the sums in the calculation of precision and recall and hence have more weight. Additive WMI-based CRM (Dwmi ). Weighted mutual information (WMI) (Fung and McKeown 1997) has been proposed as an alternative to MI, particularly when MI might lead to the over-association of low-frequency events. In this function, the pointwise MI is multipli"
J05-4002,C00-1029,1,0.84046,"Missing"
J05-4002,J02-2003,1,0.48215,"ing the harmonic mean in the additive MI-based CRM was 0.312 for high-frequency nouns and 0.153 for low-frequency nouns, and the development-set similarity using the harmonic mean in the additive t-test based CRM was 0.294 for high-frequency nouns and 0.129 for low-frequency nouns. 5.2 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique (Gale, ¨ ¨ Church, and Yarowsky 1992; Schutze 1992; Pereira, Tishby, and Lee 1993; Schutze 1998; Lee 1999; Dagan, Lee, and Pereira 1999; Golding and Roth 1999; Rooth et al. 1999; EvenZohar and Roth 2000; Lee 2001; Clark and Weir 2002) and, in the current setting, we may use a noun’s neighbors to decide which of two co-occurrences is the most likely. Although pseudo-disambiguation is an artificial task, it has relevance in at least two application areas. First, by replacing occurrences of a particular word in a test suite with 465 Computational Linguistics Volume 31, Number 4 a pair of words from which a technique must choose, we recreate a simplified version of the word sense disambiguation task; that is, we choose between a fixed number of homonyms based on local context. The second is in language modeling where we wish t"
J05-4002,W02-0908,0,0.176095,"han another. With this goal in mind, we now turn to the applications of distributional similarity. In the next section, we consider what characteristics of distributional similarity measures are desirable in two different application areas: (1) automatic thesaurus generation and (2) language modeling. 5. Application-Based Evaluation As discussed by Weeds (2003), evaluation is a major problem in this area of research. In some areas of natural language research, evaluation can be performed against a gold standard or against human plausibility judgments. The first of these approaches is taken by Curran and Moens (2002), who evaluate a number of different distributional similarity measures and weight functions against a gold standard thesaurus compiled from Roget’s, the Macquarie thesaurus, and the Moby thesaurus. However, we argue that this approach can only be considered when distributional similarity is required as an approximation to semantic similarity and that, in any case, it is not ideal since it is not Figure 2 Variation (with parameters β and γ) in development set mean similarity between neighbor sets of the additive MI-based CRM and of simlin . 461 Computational Linguistics Volume 31, Number 4 cle"
J05-4002,P93-1022,0,0.17317,"Missing"
J05-4002,C04-1036,0,0.359037,"Missing"
J05-4002,P90-1034,0,0.224341,"neighbor sets produced using distα and Lin’s similarity measure. A third observation is that all of the asymmetric models get closest at high levels of recall for both high- and low-frequency nouns. For example, Figure 1 illustrates the variation in mean similarity between neighbor sets with the parameters β and γ for the additive t-test based model. As can be seen, similarity between neighbor sets is significantly higher at high recall settings (low β) within the model than at highprecision settings (high β), which suggests that distα has high-recall CR characteristics. 4.5 Hindle’s Measure Hindle (1990) proposed an MI-based measure, which he used to show that nouns could be reliably clustered based on their verb co-occurrences. We consider the variant of 458 Weeds and Weir Co-occurrence Retrieval Figure 1 Variation (with parameters β and γ) in development set mean similarity between neighbor sets of the additive t-test based CRM and of distα . Hindle’s Measure proposed by (Lin 1998a), which overcomes the problem associated with calculating MI for word-feature combinations that do not occur: simhind (w1 , w2 ) =  min(I(c, w1 ), I(c, w2 )) (38) T(w1 )∩T(w2 ) where T(w1 ) = {c : I(c, n) &gt; 0}."
J05-4002,J93-1005,0,0.0448555,"Missing"
J05-4002,O97-1002,0,0.0315036,"998a): wn simlin (w1 , w2 ) = max c1 ∈S(w1 )∧c2 ∈S(w2 ) 2 log P(c) c∈sup(c1 )∩sup(c2 ) log (P(c1 )) + log (P(c2 ))  max (49) where S(w) is the set of senses of the word w in WordNet, sup(c) is the set of possibly indirect super-classes of concept c in WordNet, and P(c) is the probability that a randomly selected word refers to an instance of concept c (estimated over some corpus such as SemCor [Miller et al. 1994]). However, in other research (Budanitsky and Hirst 2001; Patwardhan, Banerjee, and Pedersen 2003; McCarthy, Koeling, and Weeds 2004), it has been shown that the distance measure of Jiang and Conrath (1997) (referred to herein as the “JC measure”) is a superior WordNet-based semantic similarity measure: wn distJC (w1 , w2 ) = max c1 ∈S(w1 )∧c2 ∈S(w2 )  max c∈sup(c1 )∩sup(c2 ) 2 log(c) − log P(c1 ) − log P(c2 ) (50) In our work, we make an empirical comparison of neighbors derived using a WordNet-based measure and each of the distributional similarity measures using the technique discussed in Section 3. We have carried out the same experiments using both the Lin measure and the JC measure. Correlation between distributional similarity measures and the WordNet measure tends to be slightly higher"
J05-4002,kilgarriff-yallop-2000-whats,0,0.0220621,"Missing"
J05-4002,P99-1004,0,0.506925,"that do not occur with a given word and thus these zero frequency co-occurrence types are never selected as features. This is advantageous in the computation of similarity, since computing the sums over all co-occurrence types rather than just those co-occurring with at least one of the words is (1) very computationally expensive and (2) due to their vast number, the effect of these zero frequency co-occurrence types tends to outweigh the effect of those co-occurrence types that have actually occurred. Giving such weight to these shared non-occurrences seems unintuitive and has been shown by Lee (1999) to be undesirable in the calculation of distributional similarity. Hence, when using the 448 Weeds and Weir Co-occurrence Retrieval ALLR as the weight function, we use the additional restriction that P(c, w) &gt; 0 when selecting features. 2.4 Difference-Weighted Models In additive models, no distinction is made between features that have occurred to the same extent with each word and features that have occurred to different extents with each word. For example, if two words have the same features, they are considered identical, regardless of whether the feature occurs with the same probability w"
J05-4002,J98-2002,0,0.0949222,"Missing"
J05-4002,P97-1009,0,0.722595,"task. Our last restriction was to only consider 2,000 of the approximately 35,000 nouns occurring in the corpus. This restriction was for computational efficiency and to avoid computing similarities based on the potentially unreliable descriptions of very lowfrequency words. However, since our evaluation is comparative, we do not expect our results to be affected by this or any of the other restrictions. 3.2 Neighbor Set Comparison Technique In several of our experiments, we measure the overlap between two different similarity measures. We use a neighbor set comparison technique adapted from Lin (1997). In order to compare two neighbor sets of size k, we transform each neighbor set so that each neighbor is given a rank score of k − rank. Potential neighbors not within a given rank distance k of the noun score zero. This transformation is required since scores computed on different scales are to be compared and because we wish to only consider neighbors up to a certain rank distance. The similarity between two neighbor sets S and S is computed as the cosine of the rank score vectors:  C(S, S ) = s(w) · s (w) k 2 i=1 i w∈S∩S (28) where s(w) and s (w) are the rank scores of the words wi"
J05-4002,P98-2127,0,0.118503,"milarity. We chose WordNet as our gold standard for semantic similarity since, as discussed by Kilgarriff and Yallop (2000), distributional similarity scores calculated over grammatical relation level context tend to be more similar to tighter thesauri, such as WordNet, than looser thesauri such as Roget’s. 5.1.1 Experimental Set-Up. There are a number of ways to measure the distance between two nouns in the WordNet noun hierarchy (see Budanitsky [1999] for a review). In previous work (Weeds and Weir 2003b), we used the WordNet-based similarity measure first proposed in Lin (1997) and used in Lin (1998a): wn simlin (w1 , w2 ) = max c1 ∈S(w1 )∧c2 ∈S(w2 ) 2 log P(c) c∈sup(c1 )∩sup(c2 ) log (P(c1 )) + log (P(c2 ))  max (49) where S(w) is the set of senses of the word w in WordNet, sup(c) is the set of possibly indirect super-classes of concept c in WordNet, and P(c) is the probability that a randomly selected word refers to an instance of concept c (estimated over some corpus such as SemCor [Miller et al. 1994]). However, in other research (Budanitsky and Hirst 2001; Patwardhan, Banerjee, and Pedersen 2003; McCarthy, Koeling, and Weeds 2004), it has been shown that the distance measure of Jia"
J05-4002,P04-1036,1,0.625615,"y to distinguish between linguistic relations such as synonymy, antonymy, and hyponymy (see Caraballo [1999] and Lin et al. [2003] for work on this). Thus, one may question 1 “You shall know a word by the company it keeps.”(Firth 1957) 440 Weeds and Weir Co-occurrence Retrieval the benefit of automatically generating a thesaurus if one has access to large-scale manually constructed thesauri (e.g., WordNet [Fellbaum 1998], Roget’s [Roget 1911], the Macquarie [Bernard 1990] and Moby2 ). Automatic techniques give us the opportunity to model language change over time or across domains and genres. McCarthy et al. (2004) investigate using distributional similarity methods to find predominant word senses within a corpus, making it possible to tailor an existing resource (WordNet) to specific domains. For example, in the computing domain, the word worm is more likely to be used in its ‘malicious computer program’ sense than in its ‘earthworm’ sense. This domain knowledge will be reflected in a thesaurus automatically generated from a computing-specific corpus, which will show increased similarity between worm and virus and reduced similarity between worm and caterpillar. In other application areas, however, the"
J05-4002,H94-1046,0,0.0122186,"in the WordNet noun hierarchy (see Budanitsky [1999] for a review). In previous work (Weeds and Weir 2003b), we used the WordNet-based similarity measure first proposed in Lin (1997) and used in Lin (1998a): wn simlin (w1 , w2 ) = max c1 ∈S(w1 )∧c2 ∈S(w2 ) 2 log P(c) c∈sup(c1 )∩sup(c2 ) log (P(c1 )) + log (P(c2 ))  max (49) where S(w) is the set of senses of the word w in WordNet, sup(c) is the set of possibly indirect super-classes of concept c in WordNet, and P(c) is the probability that a randomly selected word refers to an instance of concept c (estimated over some corpus such as SemCor [Miller et al. 1994]). However, in other research (Budanitsky and Hirst 2001; Patwardhan, Banerjee, and Pedersen 2003; McCarthy, Koeling, and Weeds 2004), it has been shown that the distance measure of Jiang and Conrath (1997) (referred to herein as the “JC measure”) is a superior WordNet-based semantic similarity measure: wn distJC (w1 , w2 ) = max c1 ∈S(w1 )∧c2 ∈S(w2 )  max c∈sup(c1 )∩sup(c2 ) 2 log(c) − log P(c1 ) − log P(c2 ) (50) In our work, we make an empirical comparison of neighbors derived using a WordNet-based measure and each of the distributional similarity measures using the technique discussed in"
J05-4002,A00-2011,0,0.0220875,"Missing"
J05-4002,P93-1024,0,0.61272,"Missing"
J05-4002,P99-1014,0,0.0312129,"In our experiments, the development-set similarity using the harmonic mean in the additive MI-based CRM was 0.312 for high-frequency nouns and 0.153 for low-frequency nouns, and the development-set similarity using the harmonic mean in the additive t-test based CRM was 0.294 for high-frequency nouns and 0.129 for low-frequency nouns. 5.2 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique (Gale, ¨ ¨ Church, and Yarowsky 1992; Schutze 1992; Pereira, Tishby, and Lee 1993; Schutze 1998; Lee 1999; Dagan, Lee, and Pereira 1999; Golding and Roth 1999; Rooth et al. 1999; EvenZohar and Roth 2000; Lee 2001; Clark and Weir 2002) and, in the current setting, we may use a noun’s neighbors to decide which of two co-occurrences is the most likely. Although pseudo-disambiguation is an artificial task, it has relevance in at least two application areas. First, by replacing occurrences of a particular word in a test suite with 465 Computational Linguistics Volume 31, Number 4 a pair of words from which a technique must choose, we recreate a simplified version of the word sense disambiguation task; that is, we choose between a fixed number of homonyms based on local co"
J05-4002,J98-1004,0,0.0364534,"tter than using the harmonic mean (which is equivalent to Jaccard’s measure). In our experiments, the development-set similarity using the harmonic mean in the additive MI-based CRM was 0.312 for high-frequency nouns and 0.153 for low-frequency nouns, and the development-set similarity using the harmonic mean in the additive t-test based CRM was 0.294 for high-frequency nouns and 0.129 for low-frequency nouns. 5.2 Pseudo-Disambiguation Task Pseudo-disambiguation tasks have become a standard evaluation technique (Gale, ¨ ¨ Church, and Yarowsky 1992; Schutze 1992; Pereira, Tishby, and Lee 1993; Schutze 1998; Lee 1999; Dagan, Lee, and Pereira 1999; Golding and Roth 1999; Rooth et al. 1999; EvenZohar and Roth 2000; Lee 2001; Clark and Weir 2002) and, in the current setting, we may use a noun’s neighbors to decide which of two co-occurrences is the most likely. Although pseudo-disambiguation is an artificial task, it has relevance in at least two application areas. First, by replacing occurrences of a particular word in a test suite with 465 Computational Linguistics Volume 31, Number 4 a pair of words from which a technique must choose, we recreate a simplified version of the word sense disambigua"
J05-4002,W03-1011,1,0.639337,"the difference in extent to which each word appears in each context? Is it enough to know that both words can occur in each context, or do similar words occur in similar contexts with similar probabilities? In order to answer these questions, we take a pragmatic, application-oriented approach to evaluation that is based on the assumption that we want to know which words are distributionally similar because particular applications can make use of this information. However, high performance in one application area is not necessarily correlated with high performance in another application area (Weeds and Weir 2003a). Thus, it is not clear that the same characteristics that make a distributional similarity measure useful in one application will make it useful in another. For example, with regard to the question about symmetry, in some applications we may prefer a word A that can be substituted for word B in all of the contexts in which B occurs. In other applications, we may prefer a word A that can be substituted for word B in all of the contexts in which A occurs. For example, asked for a semantically related word to dog, we might say animal, since animal can generally be used in place of dog, whereas"
J05-4002,C04-1146,1,0.892889,"Missing"
J05-4002,J90-1003,0,\N,Missing
J05-4002,J06-1003,0,\N,Missing
J05-4002,C98-2122,0,\N,Missing
J11-3004,W03-2405,0,0.140806,"Missing"
J11-3004,W06-2922,0,0.801563,"example, the algorithm of Kahane, Nasr, and Rambow (1998) uses a strategy similar to Lombardo and Lesmo (1996), but with the following initializer step instead of the I NITTER and P REDICTOR: I NITTER : [A(•α ), i, i − 1] A(α ) ∈ P ∧ 1 ≤ i ≤ n The initialization step speciﬁed by Kahane, Nasr, and Rambow (1998) differs from this (directly consuming a nonterminal from the input) but this gives an incomplete algorithm. The problem can be ﬁxed either by using the step shown here instead (bottom–up Earley strategy) or by adding an additional step turning it into a bottom–up left-corner parser. 6.2 Attardi (2006) The non-projective parser of Attardi (2006) extends the algorithm of Yamada and Matsumoto (2003), adding additional shift and reduce actions to handle non-projective dependency structures. These extra actions allow the parser to link to nodes that are several positions deep in the stack, creating non-projective links. In particular, Attardi uses six non-projective actions: two actions to link to nodes that are two positions deep, another two actions for nodes that are three positions deep, and a third pair of actions that generalizes the previous ones to n positions deep for any n. Thus, the"
J11-3004,W98-0507,0,0.655301,"dings of intermediate dependency structures are deﬁned as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues. Traditional parsing schemata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules"
J11-3004,P89-1018,0,0.318893,"ditions for the parser’s deduction steps. Side conditions restrict the inference relation by specifying which combinations of values are permissible for the variables appearing in the antecedents and consequent of deduction steps. This parsing schema speciﬁes a recognizer: Given a set of D-rules and an input string w1 . . . wn , the sentence can be parsed (projectively) under those D-rules if and only if the deduction system infers a coherent ﬁnal item. When executing this schema with a deductive engine, the parse forest can be recovered by following back pointers, as in constituency parsers (Billot and Lang 1989). This schema formalizes a parsing logic which is independent of the order and the way linking decisions are taken. Statistical models can be used to determine whether a step linking words a and b in positions i and j—i.e., having (a, i) → (b, j) as a side condition—is executed or not, and probabilities can be attached to items in order to assign different weights to different analyses of the sentence. The side conditions provide an explicit representation of the choice points where probabilistic decisions are made by the control mechanism that is executing the schema. The same principle appli"
J11-3004,W08-2134,0,0.0122277,"MHk parser has the property of being able to parse any possible dependency structure as long as we make k large enough. 6.4 MST Parser (McDonald et al. 2005) McDonald et al. (2005) describe a parser which ﬁnds a nonprojective analysis for a sentence in O(n2 ) time under a strong independence assumption called an edgefactored model: Each dependency decision is assumed to be independent of all the others (McDonald and Satta 2007). Despite the restrictiveness of this model, this maximum spanning tree (MST) parser achieves state-of-the-art performance for projective and non-projective structures (Che et al. 2008; Nivre and McDonald 2008; Surdeanu et al. 2008). The parser considers the weighted graph formed by all the possible dependencies between pairs of input words, and applies an MST algorithm to ﬁnd a dependency tree covering all the words in the sentence and maximizing the sum of weights. The MST algorithm for directed graphs suggested by McDonald et al. (2005) is not fully constructive: It does not work by building structures and combining them into large structures until it ﬁnds the solution. Instead, the algorithm works by using a greedy strategy to select a candidate set of edges for the spa"
J11-3004,P96-1025,0,0.65053,"nal) items are called correct (ﬁnal) items in the original formulation by Sikkel (1997). 3 Derivable items are called valid items in the original formulation by Sikkel (1997). 545 Computational Linguistics Volume 37, Number 3 Figure 2 Representation of the [i, j, h] item in Collins’s parser, together with one of the dependency structures contained in it (left side); and of the antecedents and consequents of an L-L INK step (right side). White rectangles in an item represent intervals of nodes that have been assigned a head by the parser, and dark squares represent nodes that have no head. 3.1 Collins (1996) One of the most straightforward projective dependency parsing strategies was introduced by Collins (1996), and is based on the CYK bottom–up parsing strategy (Kasami 1965; Younger 1967). Collins’s parser works with dependency trees which are linked to each other by creating links between their heads. The schema for this parser maps every set of D-rules G and input string w1 . . . wn to an instantiated dependency parsing system (ICol96 , H, DCol96 ) such that: Item set: The item set is deﬁned as ICol96 = {[i, j, h] |1 ≤ i ≤ h ≤ j ≤ n}, where item [i, j, h] is deﬁned as the set of forests conta"
J11-3004,N06-1021,0,0.0131231,"number of free variables used in deduction steps of Collins’s parser, it is apparent that its time complexity is O(n5 ): There are O(n5 ) combinations of index values with which each of its L INK steps can be executed.5 This complexity arises because a parentless word (head) may appear in any position in the items generated by the parser; the complexity can be reduced to O(n3 ) by ensuring that parentless words only appear at the ﬁrst or last position of an item. This is the idea behind the parser deﬁned by Eisner (1996), which is still in wide use today (McDonald, Crammer, and Pereira 2005; Corston-Oliver et al. 2006). The parsing schema for this algorithm is deﬁned as follows. Item set: The item set is IEis96 = {[i, j, True, False] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, False, True] |0 ≤ i ≤ j ≤ n} ∪{[i, j, False, False] |0 ≤ i ≤ j ≤ n}, where item [i, j, True, False] corresponds to [i, j, j] ∈ ICol96 , item [i, j, False, True] corresponds to item [i, j, i] ∈ ICol96 , and item [i, j, False, False] is deﬁned as the set of forests 5 For this and the rest of the complexity results in this article, we assume that the linking decision associated with a D-rule can be made in constant time. 547 Computational Linguistics Volu"
J11-3004,W98-0511,0,0.113846,"LETER : [A(αB • β ), i, k] Final items: The ﬁnal item set is {[(S• ), 1, n]}. The schema for Lombardo and Lesmo’s parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the S CANNER always moves the dot over the head symbol ∗, rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between conﬁgurations."
J11-3004,P04-1054,0,0.0273384,"in time O(n4+3k ). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism. 1. Introduction Dependency parsing involves ﬁnding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and ˜ Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005), ¨ relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify head–modiﬁer and head–complement relationships, which form the basis of predicate–argument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of dependency parsers ˜ Campus de Elvina, ˜ s/n, 15071 A Coruna, ˜ Spain. ∗ Facultade de Inform´atica, Universidade da Coruna E-mail: cgomezr@udc.es. ∗∗ School"
J11-3004,P05-1067,0,0.0289662,"t includes all gap degree k structures present in several natural language treebanks (which we call mildly ill-nested structures for gap degree k) in time O(n4+3k ). Finally, we illustrate how the parsing schema framework can be applied to Link Grammar, a dependency-related formalism. 1. Introduction Dependency parsing involves ﬁnding the structure of a sentence as expressed by a set of directed links (called dependencies) between individual words. Dependency formalisms have attracted considerable interest in recent years, having been successfully applied to tasks such as machine translation (Ding and Palmer 2005; Shen, Xu, and ˜ Weischedel 2008), textual entailment recognition (Herrera, Penas, and Verdejo 2005), ¨ relation extraction (Culotta and Sorensen 2004; Fundel, Kuffner, and Zimmer 2006), and question answering (Cui et al. 2005). Key characteristics of the dependency parsing approach are that dependency structures specify head–modiﬁer and head–complement relationships, which form the basis of predicate–argument structure, but are not represented explicitly in constituency trees; there is no need for dependency parsers to postulate the existence of non-lexical nodes; and some variants of depend"
J11-3004,C96-1058,0,0.108842,"parsing schemata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) → (b, j), which speciﬁes that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps i"
J11-3004,H05-1036,0,0.112317,"Missing"
J11-3004,P99-1059,0,0.152783,"PANS step is used to join two items that overlap at a single word, which must have a parent in only one of the items, so that the result of joining trees coming from both items (without creating any dependency link) is a well-formed dependency tree. Final items: The set of ﬁnal items is {[0, n, False, True]}. Note that these items represent dependency trees rooted at the beginning-of-sentence marker 0, which acts as a “dummy head” for the sentence. In order for the algorithm to parse sentences correctly, we need to deﬁne D-rules to allow the real sentence head to be linked to the node 0. 3.3 Eisner and Satta (1999) Eisner and Satta (1999) deﬁne an O(n3 ) parser for split head automaton grammars which can be used for dependency parsing. This algorithm is conceptually simpler than Eisner’s (1996) algorithm, because it only uses items representing single dependency trees, avoiding items of the form [i, j, False, False]. Item set: The item set is IES99 = {[i, j, i] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, j] |0 ≤ i ≤ j ≤ n}, where items are deﬁned as in Collins’s parsing schema. Deduction steps: The deduction steps for this parser are the following: [i, j, i] [ j + 1, k, k] R-L INK : (wi , i) → (wk , k) [i, k, k] [i, j, i"
J11-3004,W00-2011,0,0.793589,"isms. Developing efﬁcient dependency parsing strategies for these sets of structures has considerable practical interest, in particular, making it possible to parse directly with dependencies in a data-driven manner rather than indirectly by constructing intermediate constituency grammars and extracting dependencies from constituency parses. In this section, we make four contributions to this enterprise. Firstly, we deﬁne a parser for well-nested structures of gap degree 1, and prove its correctness. The parser runs in time O(n7 ), the same complexity as the best existing algorithms for LTAG (Eisner and Satta 2000), and can be optimized to O(n6 ) in the nonlexicalized case. Secondly, we generalize our algorithm to any well-nested dependency structure with gap degree at most k, resulting in an algorithm with time complexity O(n5+2k ). Thirdly, we generalize the previous parsers in order to include ill-nested structures with gap degree at most k satisfying certain constraints, giving a parser that runs in time O(n4+3k ). Note that parsing unrestricted ill-nested structures, even when the gap degree is bounded, is NP-complete: These structures are equivalent to LCFRS for which the recognition problem is NP"
J11-3004,P08-1110,1,0.88419,"Missing"
J11-3004,N09-1061,1,0.809783,"ures that we call mildly ill-nested for a given gap degree k, and presented an algorithm that can parse these in time O(n3k+4 ). The practical relevance of this set of structures can be seen in the data obtained from several dependency treebanks, showing that all the sentences contained in them are mildly ill-nested for their gap degree, and thus they are parsable with this algorithm. The strategy used by this algorithm for parsing mildly ill-nested structures has been adapted to solve the problem of ﬁnding minimal fan-out binarizations of LCFRS to improve parsing ´ efﬁciency (see Gomez-Rodr´ ıguez et al. 2009). An interesting line of future work would be to provide implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our deﬁnition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to ﬁnd a more grammar-oriented deﬁnition that would provide linguistic insight into this set of structures. An alternative generalization of the conce"
J11-3004,E09-1034,1,0.563747,"ures that we call mildly ill-nested for a given gap degree k, and presented an algorithm that can parse these in time O(n3k+4 ). The practical relevance of this set of structures can be seen in the data obtained from several dependency treebanks, showing that all the sentences contained in them are mildly ill-nested for their gap degree, and thus they are parsable with this algorithm. The strategy used by this algorithm for parsing mildly ill-nested structures has been adapted to solve the problem of ﬁnding minimal fan-out binarizations of LCFRS to improve parsing ´ efﬁciency (see Gomez-Rodr´ ıguez et al. 2009). An interesting line of future work would be to provide implementations of the mildly non-projective dependency parsers presented here, using probabilistic models to guide their linking decisions, and compare their practical performance and accuracy to those of other non-projective dependency parsers. Additionally, our deﬁnition of mildly ill-nested structures is closely related to the way the corresponding parser works. It would be interesting to ﬁnd a more grammar-oriented deﬁnition that would provide linguistic insight into this set of structures. An alternative generalization of the conce"
J11-3004,P07-1077,0,0.188126,"m called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dependency grammars. 7. Mildly Non-Projective Dependency Parsing For reasons of computational efﬁciency, many practical implementations of dependency parsing are restricted to projective structures. However, some natural language sentences appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex: Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contai"
J11-3004,P98-1106,0,0.567797,"Missing"
J11-3004,P07-1021,0,0.460103,"h n. Note that this parsing schema is not correct, because Covington’s algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reﬂected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dep"
J11-3004,P06-2066,0,0.60887,"Missing"
J11-3004,C96-2122,0,0.453149,"a framework, where the encodings of intermediate dependency structures are deﬁned as items, and the operations used to combine them are expressed as inference rules. We begin by addressing a number of preliminary issues. Traditional parsing schemata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we us"
J11-3004,P05-1012,0,0.551096,"˜ s/n, 15071 A Coruna, ˜ Spain. ∗ Facultade de Inform´atica, Universidade da Coruna E-mail: cgomezr@udc.es. ∗∗ School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: J.A.Carroll@sussex.ac.uk. † School of Informatics, University of Sussex, Falmer, Brighton BN1 9QJ, UK. E-mail: D.J.Weir@sussex.ac.uk. Submission received: 21 October 2009; revised submission received: 23 December 2010; accepted for publication: 29 January 2011. © 2011 Association for Computational Linguistics Computational Linguistics Volume 37, Number 3 are able to represent non-projective structures (McDonald et al. 2005), which is important when parsing free word order languages where discontinuous constituents are common. The formalism of parsing schemata, introduced by Sikkel (1997), is a useful tool for the study of constituency parsers, supporting precise, high-level descriptions of parsing algorithms. Potential applications of parsing schemata include devising correctness proofs, extending our understanding of relationships between different algorithms, deriving new variants of existing algorithms, and obtaining efﬁcient implementations ´ automatically (Gomez-Rodr´ ıguez, Vilares, and Alonso 2009). The f"
J11-3004,D07-1013,0,0.145043,"h n. Note that this parsing schema is not correct, because Covington’s algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reﬂected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dep"
J11-3004,H05-1066,0,0.575344,"Missing"
J11-3004,W07-2216,0,0.686108,"Missing"
J11-3004,W03-3017,0,0.804804,"n]}. The schema for Lombardo and Lesmo’s parser is a variant of the Earley constituency parser (cf. Sikkel 1997), with minor changes to adapt it to dependency grammar (for example, the S CANNER always moves the dot over the head symbol ∗, rather than over a terminal symbol). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying CFG parsing schemata of Sikkel (1997): The algorithm of Barbero et al. (1998) can be obtained from the left-corner parser, and the parser described by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Nivre (2003) Nivre (2003) describes a shift-reduce algorithm for projective dependency parsing, later extended by Nivre, Hall, and Nilsson (2004). With linear-time performance and competitive parsing accuracy (Nivre et al. 2006; Nivre and McDonald 2008), it is one of the parsers included in the MaltParser system (Nivre et al. 2007), which is currently widely used (e.g., Nivre et al. 2007; Surdeanu et al. 2008). The parser proceeds by reading the sentence from left to right, using a stack and four different kinds of transitions between conﬁgurations. The transition system deﬁned by all the possible conﬁgur"
J11-3004,N07-1050,0,0.163235,"length n. Note that this parsing schema is not correct, because Covington’s algorithm does not prevent the generation of cycles in the dependency graphs it produces. Quoting Covington (2001, page 99), Because the parser operates one word at a time, unity can only be checked at the end of the whole process: did it produce a tree with a single root that comprises all of the words? Therefore, a postprocessing mechanism is needed to determine which of the generated structures are, in fact, valid trees. In the parsing schema, this is reﬂected by the fact that the schema is complete but not sound. Nivre (2007) uses a variant of this algorithm in which cycle detection is used to avoid generating incorrect structures. Other non-projective parsers not covered here can also be represented under the parsing schema framework. For example, Kuhlmann (2010) presents a deduction system for a non-projective parser which uses a grammar formalism called regular dependency grammars. This deduction system can easily be converted into a parsing schema by associating adequate semantics with items. However, we do not show this here for space reasons, because we would ﬁrst have to explain the formalism of regular dep"
J11-3004,W04-2407,0,0.0290287,"Missing"
J11-3004,W06-2933,0,0.0439191,"Missing"
J11-3004,P08-1108,0,0.0320374,"Missing"
J11-3004,P05-1013,0,0.575223,"es appear to have non-projective syntactic structure, something that arises in many languages (Havelka 2007), and is particularly common in free word order languages such as Czech. Parsing without the projectivity constraint is computationally complex: Although it is possible to parse non-projective structures in quadratic time with respect to input length under a model in which each dependency decision is independent of all the others (as in the parser of McDonald et al. [2005], discussed in Section 6.4), the problem is intractable in the absence of this assumption (McDonald and Satta 2007). Nivre and Nilsson (2005) observe that most non-projective dependency structures appearing in practice contain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, ¨ 2005), relating them to lexicalized constituency grammar forKuhlmann, and Mohl malisms. Speciﬁcally, Kuhlmann shows that linear context-free rewriting systems 562 ´ Gomez-Rodr´ ıguez, Carroll, and Weir Dependency"
J11-3004,P92-1012,0,0.732582,"ontain only small proportions of non-projective arcs. This has led to the study of sub-classes of the class of all non-projective dependency structures (Kuhlmann and Nivre 2006; Havelka 2007). Kuhlmann (2010) investigates several such classes, based on well-nestedness and gap degree constraints (Bodirsky, ¨ 2005), relating them to lexicalized constituency grammar forKuhlmann, and Mohl malisms. Speciﬁcally, Kuhlmann shows that linear context-free rewriting systems 562 ´ Gomez-Rodr´ ıguez, Carroll, and Weir Dependency Parsing Schemata (LCFRS) with fan-out k (Vijay-Shanker, Weir, and Joshi 1987; Satta 1992) induce the set of dependency structures with gap degree at most k − 1; coupled CFG in which the maximal rank of a nonterminal is k (Hotz and Pitsch 1996) induces the set of wellnested dependency structures with gap degree at most k − 1; and ﬁnally, LTAG (Joshi and Schabes 1997) induces the set of well-nested dependency structures with gap degree at most 1. These results establish that there are polynomial-time dependency parsing algorithms for well-nested structures with bounded gap degree, because such parsers exist for their corresponding lexicalized constituency-based formalisms. Developin"
J11-3004,P08-1066,0,0.0656057,"Missing"
J11-3004,1993.iwpt-1.22,0,0.540082,"Missing"
J11-3004,W08-2121,0,0.0915845,"Missing"
J11-3004,P87-1015,1,0.765065,"Missing"
J11-3004,W03-3023,0,0.0944423,"ata are used to deﬁne grammar-driven parsers, in which the parsing process is guided by some set of rules which are used to license deduction steps. For example, an Earley P REDICTOR step is tied to a particular grammar rule, and can only be executed if such a rule exists. Some dependency parsers are also grammardriven. For example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998), and Kahane, Nasr, and Rambow (1998) are based on the formalizations of dependency grammar CFG-like rules described by Hays (1964) and Gaifman (1965). However, many of the algorithms (Eisner 1996; Yamada and Matsumoto 2003) are not traditionally considered to be grammar-driven, because they do not use an explicit formal grammar; decisions about which dependencies to create are taken individually, using probabilistic models (Eisner 1996) or classiﬁers (Yamada and Matsumoto 2003). These are called data-driven parsers. To express such algorithms as deduction systems, we use the notion of D-rules (Covington 1990). D-rules have the form (a, i) → (b, j), which speciﬁes that a word b located at position j in the input string can have the word a in position i as a dependent. Deduction steps in data-driven parsers can be"
J11-3004,E99-1020,0,\N,Missing
J11-3004,J13-2004,0,\N,Missing
J11-3004,C98-1102,0,\N,Missing
J11-3004,dzeroski-etal-2006-towards,0,\N,Missing
J11-3004,D07-1096,0,\N,Missing
J11-3004,afonso-etal-2002-floresta,0,\N,Missing
J16-4006,P14-1023,0,0.0313121,"Missing"
J16-4006,J10-4006,0,0.756742,"ow about each of the lexeme’s distributional features so that this can be achieved. The problem is that the uncontextualized distributional knowledge associated with the different lexemes in the phrase take a different perspective on the feature space. To overcome this we need to: (a) provide a way of structuring the distributional feature space, which we do by typing distributional features with dependency paths; and (b) find a way to systematically modify the perspective that each lexeme has on this structured feature space in such a way that they are all aligned with one another. Following Baroni and Lenci (2010), we use typed dependency relations as the bases for our distributional features, and following Pado´ and Lapata (2007), we include higher-order dependency relations in this space. However, in contrast to previous proposals, the higher-order dependency relations provides structure to the space that is crucial to our definition of composition. Each co-occurrence associated with a lexeme such as wooden is typed by the path in the dependency tree that connects the lexeme wooden with the co-occurring lexeme (e.g., fired). This allows us to encode a lexeme’s distributional knowledge with a hierarch"
J16-4006,D10-1115,0,0.470008,"Missing"
J16-4006,P14-1133,0,0.0795849,"Missing"
J16-4006,D12-1050,0,0.601382,"on two benchmark tasks for phrase-based composition. 5.3.1 Experiment 1: The M&L2010 Data Set. The first experiment uses the M&L2010 data set, introduced by Mitchell and Lapata (2010), which contains human similarity judgments for adjective–noun (AN), noun–noun (NN), and verb–object (VO) combinations on a seven-point rating scale. It contains 108 combinations in each category such as hsocial activity, economic conditioni, htv set, bedroom windowi, and h fight war, win battlei. This data set has been used in a number of evaluations of compositional methods including Mitchell and Lapata (2010), Blacoe and Lapata (2012), Turney (2012), Hermann and Blunsom (2013), and Kiela and Clark (2014). For example, Blacoe and Lapata (2012) show that multiplication in a simple distributional space (referred to here as an untyped VSM) outperforms the distributional memory (DM) method of Baroni and Lenci (2010) and the neural language model (NLM) method of Collobert and Weston (2008). Although often not explicit, the experimental procedure in most of this work would appear to be the calculation of Spearman’s rank correlation coefficient ρ between model scores and individual, non-aggregated, human ratings. For example, if t"
J16-4006,J12-1002,0,0.0448286,"Missing"
J16-4006,P94-1038,0,0.174552,"Missing"
J16-4006,W13-3206,0,0.155145,"Missing"
J16-4006,S12-1089,0,0.254195,"Missing"
J16-4006,D08-1094,0,0.483028,"Missing"
J16-4006,W11-0112,0,0.110623,"Missing"
J16-4006,W13-0112,0,0.356474,"using the phrase-based compositionality benchmarks of Mitchell and Lapata (2008, 2010). 5.1 Instantiating A PTs We have constructed A PT lexicons from three different corpora. r r r clean wiki is a corpus used for the case studies in Section 5.2. This corpus is a cleaned 2013 Wikipedia dump (Wilson 2015) that we have tokenized, part-of-speech-tagged, lemmatized, and dependency-parsed using the Malt Parser (Nivre 2004). This corpus contains approximately 0.6 billion tokens. BNC is the British National Corpus. It has been tokenized, POS-tagged, lemmatized, and dependency-parsed as described in Grefenstette et al. (2013) and contains approximately 0.1 billion tokens. concat is a concatenation of the ukWaC corpus (Ferraresi et al. 2008), a mid-2009 dump of the English Wikipedia and the British National Corpus. This corpus has been tokenized, POS-tagged, lemmatized, and dependency-parsed as described in Grefenstette et al. (2013) and contains about 2.8 billion tokens. Having constructed lexicons, there are a number of hyperparameters to be explored during composition. First there is the composition operation itself. We have explored variants that take a union of the features such as add and max and variants tha"
J16-4006,W10-2805,0,0.105887,"Missing"
J16-4006,P13-1088,0,0.376826,"omposition. 5.3.1 Experiment 1: The M&L2010 Data Set. The first experiment uses the M&L2010 data set, introduced by Mitchell and Lapata (2010), which contains human similarity judgments for adjective–noun (AN), noun–noun (NN), and verb–object (VO) combinations on a seven-point rating scale. It contains 108 combinations in each category such as hsocial activity, economic conditioni, htv set, bedroom windowi, and h fight war, win battlei. This data set has been used in a number of evaluations of compositional methods including Mitchell and Lapata (2010), Blacoe and Lapata (2012), Turney (2012), Hermann and Blunsom (2013), and Kiela and Clark (2014). For example, Blacoe and Lapata (2012) show that multiplication in a simple distributional space (referred to here as an untyped VSM) outperforms the distributional memory (DM) method of Baroni and Lenci (2010) and the neural language model (NLM) method of Collobert and Weston (2008). Although often not explicit, the experimental procedure in most of this work would appear to be the calculation of Spearman’s rank correlation coefficient ρ between model scores and individual, non-aggregated, human ratings. For example, if there are 108 phrase pairs being judged by 6"
J16-4006,J15-4004,0,0.0457875,"Missing"
J16-4006,W14-1503,0,0.0134296,"The M&L2010 Data Set. The first experiment uses the M&L2010 data set, introduced by Mitchell and Lapata (2010), which contains human similarity judgments for adjective–noun (AN), noun–noun (NN), and verb–object (VO) combinations on a seven-point rating scale. It contains 108 combinations in each category such as hsocial activity, economic conditioni, htv set, bedroom windowi, and h fight war, win battlei. This data set has been used in a number of evaluations of compositional methods including Mitchell and Lapata (2010), Blacoe and Lapata (2012), Turney (2012), Hermann and Blunsom (2013), and Kiela and Clark (2014). For example, Blacoe and Lapata (2012) show that multiplication in a simple distributional space (referred to here as an untyped VSM) outperforms the distributional memory (DM) method of Baroni and Lenci (2010) and the neural language model (NLM) method of Collobert and Weston (2008). Although often not explicit, the experimental procedure in most of this work would appear to be the calculation of Spearman’s rank correlation coefficient ρ between model scores and individual, non-aggregated, human ratings. For example, if there are 108 phrase pairs being judged by 6 humans, this would lead to"
J16-4006,P99-1004,0,0.492459,"rences hw, τ0 , w0 i, where w0 can be any lexeme in V, τ0 is the co-occurrence type τ. We can estimate these path probabilities from the co-occurrence counts in C as follows: p(τ |w) = #hw, τ, ∗i #hw, ∗, ∗i (5) where P #hw, τ, ∗i = #hw, τ, w0 i 0 Pw ∈V P 0 #hw, ∗, ∗i = w0 ∈V τ∈R¯ ∗ R∗ #hw, τ, w i p(τ |w) typically falls off rapidly as a function of the length of τ as desired. The similarity of two A PTs, A1 and A2 , which we denote SIM (A1 , A2 ), can be − → − → measured in terms of the similarity of vectors A1 and A2 . The similarity of vectors can be measured in a variety of ways (Lin 1998; Lee 1999; Weeds and Weir 2005; Curran 2004). One popular option involves the use of the cosine measure: SIM (A1 , A2 ) − → − → = cos(A1 , A2 ) (6) It is common to apply cosine to vectors containing positive pointwise mutual information (PPMI) values. If the weights used in the A PTs are counts or probabilities, then they can be transformed into PPMI values at this point. As a consequence of the fact that the different co-occurrence types of the cooccurrences associated with a lexeme are being differentiated, vectorized A PTs are much sparser than traditional vector representations used to model distri"
J16-4006,Q15-1016,0,0.0556387,"Missing"
J16-4006,Q13-1015,0,0.036658,"information about each word from the corpus. Other approaches have been proposed. Clarke (2007, 2012) suggested a contexttheoretic semantic framework, incorporating a generative model that assigned probabilities to arbitrary word sequences. This approach shared with Coecke, Sadrzadeh, and Clark (2011) an ambition to provide a bridge between compositional distributional semantics and formal logic-based semantics. In a similar vein, Garrette, Erk, and Mooney (2011) combined word-level distributional vector representations with logicbased representation using a probabilistic reasoning framework. Lewis and Steedman (2013) also attempted to combine distributional and logical semantics by learning a lexicon for Combinatory Categorial Grammar (CCG; Steedman 2000), which first maps natural language to a deterministic logical form and then performs a distributional clustering over logical predicates based on arguments. The CCG formalism was also used by Hermann and Blunsom (2013) as a means for incorporating syntax-sensitivity into vector space representations of sentential semantics based on recursive auto-encoders (Socher et al. 2011a, 2011b). They achieved this by representing each combinatory step in a CCG pars"
J16-4006,P98-2127,0,0.87371,"e co-occurrences hw, τ0 , w0 i, where w0 can be any lexeme in V, τ0 is the co-occurrence type τ. We can estimate these path probabilities from the co-occurrence counts in C as follows: p(τ |w) = #hw, τ, ∗i #hw, ∗, ∗i (5) where P #hw, τ, ∗i = #hw, τ, w0 i 0 Pw ∈V P 0 #hw, ∗, ∗i = w0 ∈V τ∈R¯ ∗ R∗ #hw, τ, w i p(τ |w) typically falls off rapidly as a function of the length of τ as desired. The similarity of two A PTs, A1 and A2 , which we denote SIM (A1 , A2 ), can be − → − → measured in terms of the similarity of vectors A1 and A2 . The similarity of vectors can be measured in a variety of ways (Lin 1998; Lee 1999; Weeds and Weir 2005; Curran 2004). One popular option involves the use of the cosine measure: SIM (A1 , A2 ) − → − → = cos(A1 , A2 ) (6) It is common to apply cosine to vectors containing positive pointwise mutual information (PPMI) values. If the weights used in the A PTs are counts or probabilities, then they can be transformed into PPMI values at this point. As a consequence of the fact that the different co-occurrence types of the cooccurrences associated with a lexeme are being differentiated, vectorized A PTs are much sparser than traditional vector representations used to mo"
J16-4006,S07-1009,0,0.0850475,"Missing"
J16-4006,N13-1090,0,0.702479,"aditionally been tackled by taking vector representations for words (Turney and Pantel 2010) and combining them using some function to produce a data structure that represents the phrase or sentence. Mitchell and Lapata (2008, 2010) found that simple additive and multiplicative functions applied to proximity-based vector representations were no less effective than more complex functions when performance was assessed against human similarity judgments of simple paired phrases. The word embeddings learned by the continuous bag-of-words model (CBOW) and the continuous skip-gram model proposed by Mikolov et al. (2013a, 2013b) are currently among the most popular forms of distributional word representations. Although using a neural network architecture, the intuitions behind such distributed representations of words are the same as in traditional distributional representations. As argued by Pennington et al. (2014), both count-based and prediction-based models probe the underlying corpus co-occurrences statistics. For example, the CBOW architecture predicts the current word based on context (which is viewed as a bag-of-words) and the skip-gram architecture predicts surrounding words given the current word."
J16-4006,P08-1028,0,0.208785,"contextualized lexemes, and overcomes the potential problem arising from the fact that as larger and larger units are composed, there is less and less external context around to characterize distributional meaning. 5. Experiments In this section we consider some empirical evidence in support of A PTs. First, we consider some of the different ways in which A PTs can be instantiated. Second, we present a number of case studies showing the disambiguating effect of A PT composition in adjective–noun composition. Finally, we evaluate the model using the phrase-based compositionality benchmarks of Mitchell and Lapata (2008, 2010). 5.1 Instantiating A PTs We have constructed A PT lexicons from three different corpora. r r r clean wiki is a corpus used for the case studies in Section 5.2. This corpus is a cleaned 2013 Wikipedia dump (Wilson 2015) that we have tokenized, part-of-speech-tagged, lemmatized, and dependency-parsed using the Malt Parser (Nivre 2004). This corpus contains approximately 0.6 billion tokens. BNC is the British National Corpus. It has been tokenized, POS-tagged, lemmatized, and dependency-parsed as described in Grefenstette et al. (2013) and contains approximately 0.1 billion tokens. concat"
J16-4006,W04-0308,0,0.0950574,"ch A PTs can be instantiated. Second, we present a number of case studies showing the disambiguating effect of A PT composition in adjective–noun composition. Finally, we evaluate the model using the phrase-based compositionality benchmarks of Mitchell and Lapata (2008, 2010). 5.1 Instantiating A PTs We have constructed A PT lexicons from three different corpora. r r r clean wiki is a corpus used for the case studies in Section 5.2. This corpus is a cleaned 2013 Wikipedia dump (Wilson 2015) that we have tokenized, part-of-speech-tagged, lemmatized, and dependency-parsed using the Malt Parser (Nivre 2004). This corpus contains approximately 0.6 billion tokens. BNC is the British National Corpus. It has been tokenized, POS-tagged, lemmatized, and dependency-parsed as described in Grefenstette et al. (2013) and contains approximately 0.1 billion tokens. concat is a concatenation of the ukWaC corpus (Ferraresi et al. 2008), a mid-2009 dump of the English Wikipedia and the British National Corpus. This corpus has been tokenized, POS-tagged, lemmatized, and dependency-parsed as described in Grefenstette et al. (2013) and contains about 2.8 billion tokens. Having constructed lexicons, there are a nu"
J16-4006,J07-2002,0,0.27301,"Missing"
J16-4006,D14-1162,0,0.0770335,"proximity-based vector representations were no less effective than more complex functions when performance was assessed against human similarity judgments of simple paired phrases. The word embeddings learned by the continuous bag-of-words model (CBOW) and the continuous skip-gram model proposed by Mikolov et al. (2013a, 2013b) are currently among the most popular forms of distributional word representations. Although using a neural network architecture, the intuitions behind such distributed representations of words are the same as in traditional distributional representations. As argued by Pennington et al. (2014), both count-based and prediction-based models probe the underlying corpus co-occurrences statistics. For example, the CBOW architecture predicts the current word based on context (which is viewed as a bag-of-words) and the skip-gram architecture predicts surrounding words given the current word. Mikolov et al. (2013c) showed that it is possible to use these models to efficiently learn lowdimensional representations for words that appear to capture both syntactic and semantic regularities. Mikolov et al. (2013b) also demonstrated the possibility of composing skip-gram representations using add"
J16-4006,J98-1004,0,0.704763,"ector) and the meaning of predicates in terms of second-order co-occurrence frequencies with other predicates. These predicate vectors can be obtained by adding argument vectors. For example, the verb catch will contain counts on the dimension for kick introduced by the direct-object ball and counts on the dimension for contract introduced by the direct-object cold. In other words, as in the Erk and Pado´ approach, the vector for a verb can be seen as a vector of similar verbs, thus making this notion of second-order dependency compatible with that used ¨ in work on word sense discrimination (Schutze 1998) rather than referring to secondorder (or higher-order) grammatical dependencies as in this work. Contextualization can then be achieved by multiplication of a second-order predicate vector with a firstorder argument vector because this selects the dimensions that are common to both. ¨ Thater, Furstenau, and Pinkal (2010) presented a more general model where every word is modeled in terms of first-order and second-order co-occurrences and demonstrate high performance at ranking paraphrases. 7. Directions for Future Work 7.1 Representations There are a number of apparent limitations of our appr"
J16-4006,D12-1110,0,0.475015,"Missing"
J16-4006,D11-1014,0,0.0191475,"Missing"
J16-4006,D13-1170,0,0.0335399,"Missing"
J16-4006,W09-2506,0,0.0614926,"Missing"
J16-4006,P10-1097,0,0.57764,"Missing"
J16-4006,I11-1127,0,0.198382,"Missing"
J16-4006,D11-1094,0,0.0389309,"Missing"
J16-4006,J05-4002,1,0.844321,"τ0 , w0 i, where w0 can be any lexeme in V, τ0 is the co-occurrence type τ. We can estimate these path probabilities from the co-occurrence counts in C as follows: p(τ |w) = #hw, τ, ∗i #hw, ∗, ∗i (5) where P #hw, τ, ∗i = #hw, τ, w0 i 0 Pw ∈V P 0 #hw, ∗, ∗i = w0 ∈V τ∈R¯ ∗ R∗ #hw, τ, w i p(τ |w) typically falls off rapidly as a function of the length of τ as desired. The similarity of two A PTs, A1 and A2 , which we denote SIM (A1 , A2 ), can be − → − → measured in terms of the similarity of vectors A1 and A2 . The similarity of vectors can be measured in a variety of ways (Lin 1998; Lee 1999; Weeds and Weir 2005; Curran 2004). One popular option involves the use of the cosine measure: SIM (A1 , A2 ) − → − → = cos(A1 , A2 ) (6) It is common to apply cosine to vectors containing positive pointwise mutual information (PPMI) values. If the weights used in the A PTs are counts or probabilities, then they can be transformed into PPMI values at this point. As a consequence of the fact that the different co-occurrence types of the cooccurrences associated with a lexeme are being differentiated, vectorized A PTs are much sparser than traditional vector representations used to model distributional semantics. T"
J16-4006,C04-1146,1,0.83877,"Missing"
J16-4006,W14-1502,1,0.881905,"Missing"
J16-4006,W12-2704,0,0.0736687,"asks for phrase-based composition. 5.3.1 Experiment 1: The M&L2010 Data Set. The first experiment uses the M&L2010 data set, introduced by Mitchell and Lapata (2010), which contains human similarity judgments for adjective–noun (AN), noun–noun (NN), and verb–object (VO) combinations on a seven-point rating scale. It contains 108 combinations in each category such as hsocial activity, economic conditioni, htv set, bedroom windowi, and h fight war, win battlei. This data set has been used in a number of evaluations of compositional methods including Mitchell and Lapata (2010), Blacoe and Lapata (2012), Turney (2012), Hermann and Blunsom (2013), and Kiela and Clark (2014). For example, Blacoe and Lapata (2012) show that multiplication in a simple distributional space (referred to here as an untyped VSM) outperforms the distributional memory (DM) method of Baroni and Lenci (2010) and the neural language model (NLM) method of Collobert and Weston (2008). Although often not explicit, the experimental procedure in most of this work would appear to be the calculation of Spearman’s rank correlation coefficient ρ between model scores and individual, non-aggregated, human ratings. For example, if t"
J93-4002,P87-1012,0,0.379616,"Missing"
J93-4002,P88-1031,0,0.0664247,"Missing"
J93-4002,P85-1011,0,0.629277,"to the formalisms listed as well as others that have similar characteristics (as outlined below) in their derivational process. Some of the main ideas underlying our scheme have been influenced by the observations that can be made about the constructions used in the proofs of the equivalence of these formalisms and Head Grammars (HG) (Vijay-Shanker 1987; Weir 1988; Vijay-Shanker and Weir 1993). There are similarities between the TAG and HG derivation processes and that of Context-Free Grammars (CFG). This is reflected in common features of the parsing algorithms for HG (Pollard 1984) and TAG (Vijay-Shanker and Joshi 1985) and the CKY algorithm for CFG (Kasami 1965; Younger 1967). In particular, what can happen at each step in a derivation can depend only on which of a finite set of &quot;states&quot; the derivation is in (for CFG these states can be considered to be the nonterminal symbols). This property, which we refer to as the context-freeness property, is important because it allows one to keep only a limited amount of context during the recognition process, * Department of Computer and InformationSciences,Universityof Delaware, Newark, DE 19716. E-mail: vijay@udel.edu. School of Cognitiveand ComputingSciences,Univ"
J93-4002,P88-1034,1,0.892476,"Missing"
J93-4002,H86-1020,0,\N,Missing
J93-4002,E93-1045,1,\N,Missing
J93-4002,P90-1001,1,\N,Missing
N01-1013,W99-0631,1,\N,Missing
N01-1013,J90-1003,0,\N,Missing
N01-1013,A00-2034,0,\N,Missing
N01-1013,J98-2002,0,\N,Missing
N01-1013,W97-0808,0,\N,Missing
N01-1013,C96-1003,0,\N,Missing
N01-1013,E95-1016,0,\N,Missing
N01-1013,W00-1320,0,\N,Missing
N01-1013,C00-1029,1,\N,Missing
N01-1013,C00-1028,0,\N,Missing
N01-1013,N01-1011,0,\N,Missing
N01-1013,A97-1052,0,\N,Missing
N01-1013,P99-1014,0,\N,Missing
N01-1013,W01-0703,0,\N,Missing
N01-1013,P93-1024,0,\N,Missing
N09-1061,N07-1019,0,0.20138,"Missing"
N09-1061,E09-1034,1,0.84924,"Missing"
N09-1061,C92-2066,0,0.278857,"valent to LCFRS that has been introduced for syntax-based machine translation. However, the grammar produced by our algorithm has optimal (minimal) fan-out. This is an important improvement over the result in (Melamed et al., 2004), as this quantity enters into the parsing complexity of both multitext grammars and LCFRS as an exponential factor, and therefore must be kept as low as possible to ensure practically viable parsing. Rank reduction is also investigated in Nesson et al. (2008) for synchronous tree-adjoining grammars, a synchronous rewriting formalism based on tree-adjoining grammars Joshi and Schabes (1992). In this case the search space of possible reductions is strongly restricted by the tree structures specified by the formalism, resulting in simplified computation for the reduction algorithms. This feature is not present in the case of LCFRS. There is a close parallel between the technique used in the M INIMAL -B INARIZATION algorithm and deductive parsing techniques as proposed by Shieber et al. (1995), that are usually implemented by means of tabular methods. The idea of exploiting tabular parsing in production factorization was first expressed in Zhang et al. (2006). In fact, the 546 part"
N09-1061,P06-2066,1,0.79325,"th a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally introduced as a generalization of several so-called mildly context-sensitive grammar formalisms. In the context of machine translation, LCFRS is an interesting generalization of SCFG because it does not restrict the fan-out to 2, allowing productions with arbitrary fan-out (and arbitrary rank). Given an LCFRS, our algorithm computes a strongly equivalent grammar with rank 2 and minHuman Langua"
N09-1061,E09-1055,1,0.585799,"re particularly relevant to this paper. 5.1 The tradeoff between rank and fan-out The algorithm introduced in this paper can be used to transform an LCFRS into an equivalent form with rank 2. This will result into a more efficiently parsable LCFRS, since rank exponentially affects parsing complexity. However, we must take into account that parsing complexity is also influenced by fan-out. Our algorithm guarantees a minimal increase in fan-out. In practical cases it seems such an increase is quite small. For example, in the context of dependency parsing, both G´omezRodr´ıguez et al. (2009) and Kuhlmann and Satta (2009) show that all the structures in several wellknown non-projective dependency treebanks are binarizable without any increase in their fan-out. More in general, it has been shown by Seki et al. (1991) that parsing of LCFRS can be carried out in time O(n|pM |), where n is the length of the input string and pM is the production in the grammar with largest size.3 Thus, there may be cases in which one has to find an optimal tradeoff between rank and fanout, in order to minimize the size of pM . This requires some kind of Viterbi search over the space of all possible binarizations, constructed as des"
N09-1061,P04-1084,1,0.120203,"ions of the original grammar can be reconstructed using some simple homomorphism (c.f. Nijholt, 1980). Our contribution is significant because the existing algorithms for decomposing SCFG, based on Uno and Yagiura (2000), cannot be applied to LCFRS, as they rely on the crucial property that components of biphrases are strictly separated in the generated string: Given a pair of synchronized nonterminal symbols, the material derived from the source nonterminal must precede the material derived from the target nonterminal, or vice versa. The problem that we solve has been previously addressed by Melamed et al. (2004), but in contrast to our result, their algorithm does not guarantee an optimal (minimal) increase in the fanout of the resulting grammar. However, this is essential for the practical applicability of the transformed grammar, as the parsing complexity of LCFRS is exponential in both the rank and the fan-out. Structure of the paper The remainder of the paper is structured as follows. Section 2 introduces the terminology and notation that we use for LCFRS. In Section 3, we present the technical background of our algorithm; the algorithm itself is discussed in Section 4. Section 5 concludes the pa"
N09-1061,P08-1069,1,0.851562,"Missing"
N09-1061,P87-1015,1,0.604286,"Missing"
N09-1061,P06-1123,0,0.0348496,"racted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studies in the context of dependency parsing, where the need for formalisms with higher fan-out has been observed even in standard, single language texts (Kuhlmann and Nivre, 2006). In this paper, we present an algorithm that computes optimal decompositions of rules in the formalism of Linear Context-Free Rewriting Systems (LCFRS) (Vijay-Shanker et al., 1987). LCFRS was originally"
N09-1061,C08-1136,0,0.0189435,"ences therein. One practical problem with this approach, apart from the sheer number of the rules that result from the extraction procedure, is that the parsing complexity of all synchronous formalisms that we are aware of is exponential in the rank of a rule, defined as the number of nonterminals on the righthand side. Therefore, it is important that the rules of the extracted grammar are transformed so as to minimise this quantity. Not only is this beneficial in 539 Optimal algorithms exist for minimising the size of rules in a Synchronous Context-Free Grammar (SCFG) (Uno and Yagiura, 2000; Zhang et al., 2008). However, the SCFG formalism is limited to modelling word-to-word alignments in which a single continuous phrase in the source language is aligned with a single continuous phrase in the target language; as defined below, this amounts to saying that SCFG have a fan-out of 2. This restriction appears to render SCFG empirically inadequate. In particular, Wellington et al. (2006) find that the coverage of a translation model can increase dramatically when one allows a bilingual phrase to stretch out over three rather than two continuous substrings. This observation is in line with empirical studi"
N09-1061,N06-1033,0,0.095259,"ear Context-Free Rewriting Systems Carlos G´omez-Rodr´ıguez1 , Marco Kuhlmann2 , Giorgio Satta3 and David Weir4 1 Departamento de Computaci´on, Universidade da Coru˜na, Spain (cgomezr@udc.es) Department of Linguistics and Philology, Uppsala University, Sweden (marco.kuhlmann@lingfil.uu.se) 2 3 Department of Information Engineering, University of Padua, Italy (satta@dei.unipd.it) 4 Department of Informatics, University of Sussex, United Kingdom (davidw@sussex.ac.uk) Abstract terms of parsing complexity, but smaller rules can also improve a translation model’s ability to generalize to new data (Zhang et al., 2006). Linear Context-free Rewriting Systems (LCFRS) is an expressive grammar formalism with applications in syntax-based machine translation. The parsing complexity of an LCFRS is exponential in both the rank of a production, defined as the number of nonterminals on its right-hand side, and a measure for the discontinuity of a phrase, called fan-out. In this paper, we present an algorithm that transforms an LCFRS into a strongly equivalent form in which all productions have rank at most 2, and has minimal fan-out. Our results generalize previous work on Synchronous Context-Free Grammar, and are pa"
N09-1061,J07-2003,0,\N,Missing
P08-1110,E99-1020,0,0.0947804,"properties (such as correctness), establish relations between them, derive new parsers from existing ones and obtain efficient implementations automatically (G´omez-Rodr´ıguez et al., 2007). The formalism was initially defined for context-free grammars and later applied to other constituencybased formalisms, such as tree-adjoining grammars ∗ Partially supported by Ministerio de Educaci´on y Ciencia and FEDER (TIN2004-07246-C03, HUM2007-66607-C04), Xunta de Galicia (PGIDIT07SIN005206PR, PGIDIT05PXIC10501PN, PGIDIT05PXIC30501PN, Rede Galega de Proc. da Linguaxe e RI) and Programa de Becas FPU. (Alonso et al., 1999). However, since parsing schemata are defined as deduction systems over sets of constituency trees, they cannot be used to describe dependency parsers. In this paper, we define an analogous formalism that can be used to define, analyze and compare dependency parsers. We use this framework to provide uniform, high-level descriptions for a wide range of well-known algorithms described in the literature, and we show how they formally relate to each other and how we can use these relations and the formalism itself to prove their correctness. 1.1 Parsing schemata Parsing schemata (Sikkel, 1997) pro"
P08-1110,W06-2922,0,0.22437,"Missing"
P08-1110,W98-0507,0,0.956481,"e dependency parsers are also grammar2 wi is shorthand for the marked terminal (wi , i). These are used by Sikkel (1997) to link terminal symbols to string positions so that an input sentence can be represented as a set of trees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b"
P08-1110,P89-1018,0,0.557318,"schemata for some well-known dependency parsers. As we can see, we use D-rules as side conditions for deduction steps, since this parsing strategy is not grammar-based. Conceptually, the schema we have just defined describes a recogniser: given a set of Drules and an input string wi . . . wn , the sentence can be parsed (projectively) under those D-rules if and only if this deduction system can infer a correct final item. However, when executing this schema with a deductive engine, we can recover the parse forest by following back pointers in the same way as is done with constituency parsers (Billot and Lang, 1989). Of course, boolean D-rules are of limited interest in practice. However, this schema provides a formalization of a parsing strategy which is independent of the way linking decisions are taken in a particular implementation. In practice, statistical models can be used to decide whether a step linking words a and b (i.e., having a → b as a side condition) is executed or not, and probabilities can be attached to items in order to assign different weights to different analyses of the sentence. The same principle applies to the rest of D-rule-based parsers described in this paper. 3.2 Eis96 (Eisn"
P08-1110,P96-1025,0,0.0603299,"arsers. When defining projective parsers, correct final items will be those containing projective parse trees for w1 . . . wn . This distinction is relevant because the concepts of soundness and correctness of parsing schemata are based on correct final items (cf. section 1.1), and we expect correct projective parsers to produce only projective structures, while nonprojective parsers should find all possible structures including nonprojective ones. 3 Some practical examples 3.1 Col96 (Collins, 96) One of the most straightforward projective dependency parsing strategies is the one described by Collins (1996), directly based on the CYK parsing algorithm. This parser works with dependency trees which are linked to each other by creating links between their heads. Its item set is defined as ICol96 = {[i, j, h] |1 ≤ i ≤ h ≤ j ≤ n}, where an item [i, j, h] is defined as the set of forests containing a single projective dependency tree t such that t is grounded, yield (t) = wi . . . wj and head (t) = wh . For an input string w1 . . . wn , the set of hypotheses is H = {[i, i, i] |0 ≤ i ≤ n + 1}, i.e., the set of forests containing a single dependency tree of the form wi (wi ). This same set of hypothese"
P08-1110,N06-1021,0,0.0432819,"he rest of D-rule-based parsers described in this paper. 3.2 Eis96 (Eisner, 96) By counting the number of free variables used in each deduction step of Collins’ parser, we can conclude that it has a time complexity of O(n5 ). This complexity arises from the fact that a parentless word (head) may appear in any position in the partial results generated by the parser; the complexity can be reduced to O(n3 ) by ensuring that parentless words can only appear at the first or last position of an item. This is the principle behind the parser defined by Eisner (1996), which is still in wide use today (Corston-Oliver et al., 2006; McDonald et al., 971 2005a). The item set for Eisner’s parsing schema is IEis96 = {[i, j, T, F ] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, F, T ] |0 ≤ i ≤ j ≤ n} ∪ {[i, j, F, F ] | 0 ≤ i ≤ j ≤ n}, where each item [i, j, T, F ] is defined as the item [i, j, j] ∈ ICol96 , each item [i, j, F, T ] is defined as the item [i, j, i] ∈ ICol96 , and each item [i, j, F, F ] is defined as the set of forests of the form {t1 , t2 } such that t1 and t2 are grounded, head (t1 ) = wi , head (t2 ) = wj , and ∃k ∈ N(i ≤ k &lt; j)/yield (t1 ) = wi . . . wk ∧ yield (t2 ) = wk+1 . . . wj . Note that the flags b, c in an item [i, j"
P08-1110,W98-0511,0,0.583594,"eduction steps for the schema are shown in Figure 2, and the final item set is {[(S.), 1, n]}. As we can see, the schema for Lombardo and Lesmo’s parser resembles the Earley-style parser in Sikkel (1997), with some changes to adapt it to dependency grammar (for example, the Scanner always moves the dot over the head symbol ∗). Analogously, other dependency parsing schemata based on CFG-like rules can be obtained by modifying context-free grammar parsing schemata of Sikkel (1997) in a similar way. The algorithm by Barbero et al. (1998) can be obtained from the leftcorner parser, and the one by Courtin and Genthial (1998) is a variant of the head-corner parser. 3.6 Pseudo-projectivity Pseudo-projective parsers can generate nonprojective analyses in polynomial time by using a projective parsing strategy and postprocessing the results to establish nonprojective links. For example, the algorithm by Kahane et al. (1998) uses a projective parsing strategy like that of LL96, but using the following initializer step instead of the Initter and Predictor:5 Initter 4 [A(α), i, i − 1] sr A(α) ∈ P ∧ 1 ≤ i ≤ n Relations between dependency parsers The framework of parsing schemata can be used to establish relationships betw"
P08-1110,P81-1022,0,0.738346,"ardo and Lesmo, 96) and other Earley-based parsers The algorithms in the above examples are based on taking individual decisions about dependency links, represented by D-rules. Other parsers, such as that of Lombardo and Lesmo (1996), use grammars with context-free like rules which encode the preferred order of dependents for each given governor, as defined by Gaifman (1965). For example, a rule of the form N (Det ∗ P P ) is used to allow N to have Det as left dependent and P P as right dependent. The algorithm by Lombardo and Lesmo (1996) is a version of Earley’s context-free grammar parser (Earley, 1970) using Gaifman’s dependency grammar, and can be written by using an item set ILomLes = {[A(α.β), i, j] |A(αβ) ∈ P ∧ 1 ≤ i ≤ j ≤ n}, where each item [A(α.β), i, j] represents the set of partial dependency trees rooted at A, where the direct children of A are αβ, and the subtrees rooted at α have yield wi . . . wj . The deduction steps for the schema are shown in Figure 2, and the final item set is {[(S.), 1, n]}. As we can see, the schema for Lombardo and Lesmo’s parser resembles the Earley-style parser in Sikkel (1997), with some changes to adapt it to dependency grammar (for example, the Scan"
P08-1110,P99-1059,0,0.512818,"ent dependency trees where the word in position i or j is the head, while items with both flags set to F represent pairs of trees headed at positions i and j, and therefore correspond to disconnected dependency graphs. Deduction steps4 are shown in Figure 2. The set of final items is {[0, n, F, T ]}. Note that these items represent dependency trees rooted at the BOS marker w0 , which acts as a “dummy head” for the sentence. In order for the algorithm to parse sentences correctly, we will need to define D-rules to allow w0 to be linked to the real sentence head. 3.3 ES99 (Eisner and Satta, 99) Eisner and Satta (1999) define an O(n3 ) parser for split head automaton grammars that can be used 4 Alternatively, we could consider items of the form [i, i + 1, F, F ] to be hypotheses for this parsing schema, so we would not need an Initter step. However, we have chosen to use a standard set of hypotheses valid for all parsers because this allows for more straightforward proofs of relations between schemata. for dependency parsing. This algorithm is conceptually simpler than Eis96, since it only uses items representing single dependency trees, avoiding items of the form [i, j, F, F ]. Its item set is IES99 = {[i,"
P08-1110,C96-1058,0,0.925767,"ees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the semantics of these parsing strat"
P08-1110,P98-1106,0,0.628947,"lso grammar2 wi is shorthand for the marked terminal (wi , i). These are used by Sikkel (1997) to link terminal symbols to string positions so that an input sentence can be represented as a set of trees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent"
P08-1110,C96-2122,0,0.788926,"if such a rule exists. Some dependency parsers are also grammar2 wi is shorthand for the marked terminal (wi , i). These are used by Sikkel (1997) to link terminal symbols to string positions so that an input sentence can be represented as a set of trees which are used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b,"
P08-1110,P05-1012,0,0.159405,"Missing"
P08-1110,H05-1066,0,0.225193,"Missing"
P08-1110,D07-1013,0,0.0646823,"Missing"
P08-1110,W03-3023,0,0.888754,"used as initial items (hypotheses) for the deduction system. Thus, a sentence w1 . . . wn produces a set of hypotheses {{w1 (w1 )}, . . . , {wn (wn )}}. 969 Figure 1: Representation of a dependency structure with a tree. The arrows below the words correspond to its associated dependency graph. based: for example, those described by Lombardo and Lesmo (1996), Barbero et al. (1998) and Kahane et al. (1998) are tied to the formalizations of dependency grammar using context-free like rules described by Hays (1964) and Gaifman (1965). However, many of the most widely used algorithms (Eisner, 1996; Yamada and Matsumoto, 2003) do not use a formal grammar at all. In these, decisions about which dependencies to create are taken individually, using probabilistic models (Eisner, 1996) or classifiers (Yamada and Matsumoto, 2003). To represent these algorithms as deduction systems, we use the notion of D-rules (Covington, 1990). D-rules take the form a → b, which says that word b can have a as a dependent. Deduction steps in non-grammarbased parsers can be tied to the D-rules associated with the links they create. In this way, we obtain a representation of the semantics of these parsing strategies that is independent of"
P08-1110,C98-1102,0,\N,Missing
P11-1014,W06-1615,0,0.614812,"number of baselines and previous cross-domain sentiment classification techniques using the benchmark dataset. For all previous techniques we give the results reported in the original papers. The No Thesaurus baseline simulates the effect of not performing any feature expansion. We simply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. This can be considered to be a lower bound that does not perform domain adaptation. SCL is the structural correspondence learning technique of Blitzer et al. (2006). In SCL-MI, features are selected using the mutual information between a feature (unigram or bigram) and a domain label. After selecting salient features, the SCL algorithm is used to train a binary classifier. SFA is the spectral feature alignment technique of Pan et al. (2010). Both the LSA and FALSA techniques are based on latent semantic analysis (Pan et al., 2010). For the Within-Domain baseline, we train a binary classifier using the labeled data from the target domain. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for"
P11-1014,P07-1056,0,0.937463,"Missing"
P11-1014,P06-4020,1,0.770277,"ines and previous cross-domain sentiment classification techniques using the benchmark dataset. For all previous techniques we give the results reported in the original papers. The No Thesaurus baseline simulates the effect of not performing any feature expansion. We simply train a binary classifier using unigrams and bigrams as features from the labeled reviews in the source domains and apply the trained classifier on the target domain. This can be considered to be a lower bound that does not perform domain adaptation. SCL is the structural correspondence learning technique of Blitzer et al. (2006). In SCL-MI, features are selected using the mutual information between a feature (unigram or bigram) and a domain label. After selecting salient features, the SCL algorithm is used to train a binary classifier. SFA is the spectral feature alignment technique of Pan et al. (2010). Both the LSA and FALSA techniques are based on latent semantic analysis (Pan et al., 2010). For the Within-Domain baseline, we train a binary classifier using the labeled data from the target domain. This upper baseline represents the classification accuracy we could hope to obtain if we were to have labeled data for"
P11-1014,P08-1017,0,0.0228875,"use of unlabeled data enables us to accurately estimate the distribution of words in source and target domains. Our method can learn from a large amount of unlabeled data to leverage a robust cross-domain sentiment classifier. We model the cross-domain sentiment classification problem as one of feature expansion, where we append additional related features to feature vectors that represent source and target domain reviews in order to reduce the mismatch of features between the two domains. Methods that use related features have been successfully used in numerous tasks such as query expansion (Fang, 2008), and document classification (Shen et al., 2009). However, feature expansion techniques have not previously been applied to the task of cross-domain sentiment classification. In our method, we use the automatically created 133 thesaurus to expand feature vectors in a binary classifier at train and test times by introducing related lexical elements from the thesaurus. We use L1 regularized logistic regression as the classification algorithm. (However, the method is agnostic to the properties of the classifier and can be used to expand feature vectors for any binary classifier). L1 regularizati"
P11-1014,P97-1023,0,0.11935,"sensitive thesaurus for feature expansion. Given a labeled or an unlabeled review, we first split the review into individual sentences. We carry out part-of-speech (POS) tagging and lemmatization on each review sentence using the RASP sys134 tem (Briscoe et al., 2006). Lemmatization reduces the data sparseness and has been shown to be effective in text classification tasks (Joachims, 1998). We then apply a simple word filter based on POS tags to select content words (nouns, verbs, adjectives, and adverbs). In particular, previous work has identified adjectives as good indicators of sentiment (Hatzivassiloglou and McKeown, 1997; Wiebe, 2000). Following previous work in cross-domain sentiment classification, we model a review as a bag of words. We select unigrams and bigrams from each sentence. For the remainder of this paper, we will refer to unigrams and bigrams collectively as lexical elements. Previous work on sentiment classification has shown that both unigrams and bigrams are useful for training a sentiment classifier (Blitzer et al., 2007). We note that it is possible to create lexical elements both from source domain labeled reviews as well as from unlabeled reviews in source and target domains. Next, we rep"
P11-1014,W02-1011,0,0.0328941,"learn from multiple source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products. 1 Introduction Users express opinions about products or services they consume in blog posts, shopping sites, or review sites. It is useful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment 132 expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully us"
P11-1014,N04-1041,0,0.0331408,"have similar distributions are semantically similar. We compute f (u, w) as the pointwise mutual information between a lexical element u and a feature w as follows: c(u,w) N P Pn m j=1 c(u,j) i=1 c(i,w) × N N f (u, w) = log ! (1) Here, c(u, w) denotes the number of review sentences in which a lexical element u and a feature w co-occur, n and m respectively denote the total number of lexical elements and the total number of Pn P m features, and N = i=1 j=1 c(i, j). Pointwise mutual information is known to be biased towards infrequent elements and features. We follow the discounting approach of Pantel & Ravichandran (2004) to overcome this bias. Next, for two lexical elements u and v (represented by feature vectors u and v, respectively), we compute the relatedness τ (v, u) of the feature v to the feature u as follows, P w∈{x|f (v,x)>0} f (u, w) w∈{x|f (u,x)>0} f (u, w) τ (v, u) = P . (2) 1. Note that relatedness is an asymmetric measure by the definition given in Equation 2, and the relatedness τ (v, u) of an element v to another element u is not necessarily equal to τ (u, v), the relatedness of u to v. We use the relatedness measure defined in Equation 2 to construct a sentiment sensitive thesaurus in which,"
P11-1014,P02-1053,0,0.0546106,"e source domains. Our method significantly outperforms numerous baselines and returns results that are better than or comparable to previous cross-domain sentiment classification methods on a benchmark dataset containing Amazon user reviews for different types of products. 1 Introduction Users express opinions about products or services they consume in blog posts, shopping sites, or review sites. It is useful for both consumers as well as for producers to know what general public think about a particular product or service. Automatic document level sentiment classification (Pang et al., 2002; Turney, 2002) is the task of classifying a given review with respect to the sentiment expressed by the author of the review. For example, a sentiment classifier might classify a user review about a movie as positive or negative depending on the sentiment 132 expressed in the review. Sentiment classification has been applied in numerous tasks such as opinion mining (Pang and Lee, 2008), opinion summarization (Lu et al., 2009), contextual advertising (Fan and Chang, 2010), and market analysis (Hu and Liu, 2004). Supervised learning algorithms that require labeled data have been successfully used to build sen"
P14-1058,J10-4006,0,0.21112,"., 2011) groups together words that express similar sentiments in • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies 614 sentence unigrams (surface) unigrams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic"
P14-1058,W06-1615,0,0.768872,"ve baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved fr"
P14-1058,D12-1120,0,0.0191454,"words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter words from the training data based on the intuition that longer words are more likely to be examples of productive linguistic processes than shorter words. • Given the distribution wS of a word w in a source domain S, we propose a method for learning its distribution wT in a target domain T . The sentiment of a word can vary from one domain to another. In Structural Correspondence Learning (SCL) (Blitzer et al., 2006; Blitzer et al., 2007), a set of pivots are chosen using pointwise mutual infor"
P14-1058,P07-1056,0,0.942627,"the same word is often associated with negative sentimentbearing words such as superficial or formulaic. Consequently, the distributional representations of the word lightweight will differ considerably between the two domains. In this paper, given the distribution wS of a word w in the source domain S, we propose an unsupervised method for predicting its distribution wT in a different target domain T . The ability to predict how the distribution of a word varies from one domain to another is vital for numerous adaptation tasks. For example, unsupervised cross-domain sentiment classification (Blitzer et al., 2007; Aue and Gamon, 2005) involves using sentiment-labeled user reviews from the source domain, and unlabeled reviews from both the source and the target domains to learn a sentiment classifier for the target domain. Domain adaptation (DA) of sentiment classification becomes extremely challenging when the distributions of words in the source and the target domains are very different, because the features learnt from the source domain labeled reviews might not appear in the target domain reviews that must be classified. By predicting the distribution of a word across different domains, we can find"
P14-1058,P11-1014,1,0.95001,"ntwise mutual information. Linear predictors are then learnt to predict the occurrence of those pivots, and SVD is used to construct a lower dimensional representation in which a binary classifier is trained. Spectral Feature Alignment (SFA) (Pan et al., 2010) also uses pivots to compute an alignment between domain specific and domain independent features. Spectral clustering is performed on a bipartite graph representing domain specific and domain independent features to find a lowerdimensional projection between the two sets of features. The cross-domain sentiment-sensitive thesaurus (SST) (Bollegala et al., 2011) groups together words that express similar sentiments in • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni a"
P14-1058,P98-2127,0,0.0360269,"lving a unigram and a bigram. Consequently, in matrix A, we consider co-occurrences only between unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as representing the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr , and columns, nc , as the raw co-occurrence matrix A. Distribution Prediction In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows. Given a document H, such as a user-review of a product, we split"
P14-1058,P06-4020,1,0.711747,"ied to A. Matrix F has the same number of rows, nr , and columns, nc , as the raw co-occurrence matrix A. Distribution Prediction In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a sentence as follows. Given a document H, such as a user-review of a product, we split H into sentences, and lemmatize each word in a sentence using the RASP system (Briscoe et al., 2006). Using a standard stop word list, we filter out frequent non-content unigrams and select the remainder as unigram features to represent a sentence. Next, we generate bigrams of word lemmas and remove any bigrams that consists only of stop words. Bigram features capture negations more accurately than unigrams, and have been found to be useful for sentiment classification tasks. Table 1 shows the unigram and bigram features we extract for a sentence using this procedure. Using data from a single doNote that in addition to the above-mentioned representation, there are many other ways to represen"
P14-1058,N10-1004,0,0.066055,"o steps. Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain. Without requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a P"
P14-1058,P12-2071,0,0.0242231,"ised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in both the source and target domains) to adapt a POS tagger to a target domain. Choi and Palmer (2012) propose a cross-domain POS tagging method by training two separate models: a generalised model and a domain-specific model. At tagging time, a sentence is tagged by the model that is most similar to that sentence. Huang and Yates (2009) train a Conditional Random Field (CRF) tagger with features retrieved from a smoothing model trained using both source and target domain unlabeled data. Adding latent states to the smoothing model further improves the POS tagging accuracy (Huang and Yates, 2012). Schnabel and Sch¨utze (2013) propose a training set filtering method where they eliminate shorter"
P14-1058,J07-2002,0,0.0299297,"d remove any bigrams that consists only of stop words. Bigram features capture negations more accurately than unigrams, and have been found to be useful for sentiment classification tasks. Table 1 shows the unigram and bigram features we extract for a sentence using this procedure. Using data from a single doNote that in addition to the above-mentioned representation, there are many other ways to represent the distribution of a word in a particular domain (Turney and Pantel, 2010). For example, one can limit the definition of co-occurrence to words that are linked by some dependency relation (Pado and Lapata, 2007), or extend the window of co-occurrence to the entire document (Baroni and Lenci, 2010). Since the method we propose in Section 3.2 to predict the distribution of a word across domains does not depend on the particular 615 Algorithm 1 Learning a prediction model. feature representation method, any of these alternative methods could be used. To reduce the dimensionality of the feature space, and create dense representations for words, we perform SVD on F. We use the left singular vectors corresponding to the k largest singular ˆ of values to compute a rank k approximation F, F. We perform trunc"
P14-1058,J90-1003,0,0.411652,"cooccurrences of bigrams are rare compared to cooccurrences of unigrams, and co-occurrences involving a unigram and a bigram. Consequently, in matrix A, we consider co-occurrences only between unigrams vs. unigrams, and bigrams vs. unigrams. We consider each row in A as representing the distribution of a feature (i.e. unigrams or bigrams) in a particular domain over the unigram features extracted from that domain (represented by the columns of A). We apply Positive Pointwise Mutual Information (PPMI) to the cooccurrence matrix A. This is a variation of the Pointwise Mutual Information (PMI) (Church and Hanks, 1990), in which all PMI values that are less than zero are replaced with zero (Lin, 1998; Bullinaria and Levy, 2007). Let F be the matrix that results when PPMI is applied to A. Matrix F has the same number of rows, nr , and columns, nc , as the raw co-occurrence matrix A. Distribution Prediction In-domain Feature Vector Construction Before we tackle the problem of learning a model to predict the distribution of a word across domains, we must first compute the distribution of a word from a single domain. For this purpose, we represent a word w using unigrams and bigrams that co-occur with w in a se"
P14-1058,P05-1004,0,0.0329206,"niversity of Sussex Falmer, Brighton, BN1 9QJ, UK Introduction The Distributional Hypothesis, summarised by the memorable line of Firth (1957) – You shall know a word by the company it keeps – has inspired a diverse range of research in natural language processing. In such work, a word is represented by the distribution of other words that co-occur with it. Distributional representations of words have been successfully used in many language processing tasks such as entity set expansion (Pantel et al., 2009), part-of-speech (POS) tagging and chunking (Huang and Yates, 2009), ontology learning (Curran, 2005), computing semantic textual similarity (Besanc¸on et al., 1999), and lexical inference (Kotlerman et al., 2012). 1 In this paper, we use the term domain to refer to a collection of documents about a particular topic, for example reviews of a particular kind of product. 613 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 613–623, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics across different domains, and such variations are not captured by models that only learn a single semantic representation for a w"
P14-1058,P07-1033,0,0.288919,"Missing"
P14-1058,D12-1060,0,0.0154934,"ams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon. Ponomareva and Thelwall (2012) represent source and target domain reviews as nodes in a graph and apply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to our task of distribution prediction across domains. The unsupervised DA setting that we consider does not assume the availability of labeled data for the target"
P14-1058,N09-1032,0,0.085558,"uire any labeled data in either of the two steps. Using two popular multi-domain datasets, we evaluate the proposed method in two prediction tasks: (a) predicting the POS of a word in a target domain, and (b) predicting the sentiment of a review in a target domain. Without requiring any task specific customisations, systems based on our distribution prediction method significantly outperform competitive baselines in both tasks. Because our proposed distribution prediction method is unsupervised and task independent, it is potentially useful for a wide range of DA tasks such entity extraction (Guo et al., 2009) or dependency parsing (McClosky et al., 2010). Our contributions are summarised as follows: The POS of a word is influenced both by its context (contextual bias), and the domain of the document in which it appears (lexical bias). For example, the word signal is predominately used as a noun in MEDLINE, whereas it appears predominantly as an adjective in the Wall Street Journal (WSJ) (Blitzer et al., 2006). Consequently, a tagger trained on WSJ would incorrectly tag signal in MEDLINE. Blitzer et al. (2006) append the source domain labeled data with predicted pivots (i.e. words that appear in bo"
P14-1058,I13-1023,0,0.0944857,"Missing"
P14-1058,P11-1013,0,0.0501642,"10; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies 614 sentence unigrams (surface) unigrams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propose a joint sentiment-topic model that imposes a sentiment-prior depending on the occurrence of a word in a sentiment lexicon. Ponomareva and Thelwall (2012) represent source and target domain reviews as nodes in a graph and apply a label propagation algorithm to predict the sentiment labels for target domain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to"
P14-1058,D13-1170,0,0.00551686,"SST) (Bollegala et al., 2011) groups together words that express similar sentiments in • Using the learnt distribution prediction model, we propose a method to learn a crossdomain POS tagger. • Using the learnt distribution prediction model, we propose a method to learn a crossdomain sentiment classifier. To our knowledge, ours is the first successful attempt to learn a model that predicts the distribution of a word across different domains. 2 Related Work Learning semantic representations for words using documents from a single domain has received much attention lately (Vincent et al., 2010; Socher et al., 2013; Baroni and Lenci, 2010). As we have already discussed, the semantics of a word varies 614 sentence unigrams (surface) unigrams (lemma) unigrams (features) bigrams (lemma) bigrams (features) different domains. The created thesaurus is used to expand feature vectors during train and test stages in a binary classifier. However, unlike our method, SCL, SFA, or SST do not learn a prediction model between word distributions across domains. Prior knowledge of the sentiment of words, such as sentiment lexicons, has been incorporated into cross-domain sentiment classification. He et al. (2011) propos"
P14-1058,D13-1016,0,0.020529,"ain reviews from the sentiment labels in source domain reviews. A sentiment lexicon is used to create features for a document. Although incorporation of prior sentiment knowledge is a promising technique to improve accuracy in cross-domain sentiment classification, it is complementary to our task of distribution prediction across domains. The unsupervised DA setting that we consider does not assume the availability of labeled data for the target domain. However, if a small amount of labeled data is available for the target domain, it can be used to further improve the performance of DA tasks (Xiao et al., 2013; Daum´e III, 2007). 3 3.1 This is an interesting and well researched book this, is, an, interesting, and, well, researched, book this, be, an, interest, and, well, research, book interest, well, research, book this+be, be+an, an+interest, interest+and, and+well, well+research, research+book an+interest, interest+and, and+well, well+research, research+book Table 1: Extracting unigram and bigram features. main, we construct a feature co-occurrence matrix A in which columns correspond to unigram features and rows correspond to either unigram or bigram features. The value of the element aij in th"
P14-1058,D07-1118,0,\N,Missing
P14-1058,P09-1056,0,\N,Missing
P14-1058,C98-2122,0,\N,Missing
P14-1058,D09-1098,0,\N,Missing
P17-2069,D16-1175,1,0.518848,"concerning co-occurrences involving that lexeme, is represented with a higher-order dependencytyped structure (the A PT) where paths associated with higher-order dependencies connect vertices associated with weighted lexeme multisets. The central innovation in the compositional theory is that the A PT’s type structure enables the precise alignment of the semantic representation of each of the lexemes being composed. Like other countbased distributional spaces, however, it is prone to considerable data sparsity, caused by not observing all plausible co-occurrences in the given data. Recently, Kober et al. (2016) introduced a simple unsupervised algorithm to infer missing cooccurrence information by leveraging the distributional neighbourhood and ease the sparsity effect in count-based models. In this paper, we generalise distributional inference (DI) in A PTs and show how precisely Offset Representations The basis of how composition is modelled in the A PT framework is the way that the co-occurrences are structured. In characterising the distributional semantics of some lexeme w, rather than just recording a co-occurrence between w and w0 within some context window, we follow Pad´o and Lapata (2007)"
P17-2069,W11-0114,0,0.417397,"2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014). The perhaps most popular approach in the literature to evaluating compositional distributional semantic models is to compare human word and phrase similarity judgements with similarity estimates of composed meaning representations, under the assumption that better distributional representations will perform better at these tasks (Blacoe and Lapata, 2012; Dinu et al., 2013; Erk and Pad´o, 2008; Hashimoto et al., 2014; Hermann and Blunsom, 2013; Kiela et al., 2014; Turney, 2012). ML08 VO 0.22 0.29‡ 0.31†‡ Table 3: Comparison of DI algorithms. ‡ denotes statistical signific"
P17-2069,P14-2050,0,0.0593987,"sating for sparsity, but overwhelming the representations with non-plausible co-occurrence information. A similar effect has also been observed by Erk and Pado (2010) in an exemplarbased model. Experiments For our experiments we re-implemented the standard DI method of Kober et al. (2016) for a direct comparison. We built an order 2 A PT space on the basis of the concatenation of ukWaC, Wackypedia and the BNC (Baroni et al., 2009), pre-parsed with the Malt parser (Nivre et al., 2006). We PPMI transformed the raw co-occurrence counts prior to composition, using a negative SPPMI shift of log 5 (Levy and Goldberg, 2014b). We also experimented with composing normalised counts and applying the PPMI transformation after composition as done by Weeds et al. (2017), however found composing PPMI scores to work better for this task. We evaluate our offset inference algorithm on two popular short phrase composition benchmarks by Mitchell and Lapata (2008) and Mitchell and Lapata (2010), henceforth ML08 and ML10 respectively. The ML08 dataset consists of 120 distinct verb-object (VO) pairs and the ML10 dataset contains 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object pairs. The goal is to compare a mod"
P17-2069,W10-2805,0,0.152686,"exts for word-sense discrimination. Recently Kober et al. (2016) revitalised the idea for compositional distributional semantic models. Composition with distributional semantic models has become a popular research area in recent years. Simple, yet competitive methods, are based on pointwise vector addition or multiplication (Mitchell and Lapata, 2008, 2010). However, these approaches neglect the structure of the text defining composition as a commutative operation. A number of approaches proposed in the literature attempt to overcome this shortcoming by introducing weighted additive variants (Guevara, 2010, 2011; Zanzotto et al., 2010). Another popular strand of work models semantic composition on the basis of ideas arising in formal semantics. Composition in such models is usually implemented as operations on higher-order tensors (Ba5 Conclusion In this paper we have introduced a novel form of distributional inference that generalises the method introduced by Kober et al. (2016). We have shown its effectiveness for semantic composition on two benchmark phrase similarity tasks where we achieved state-of-the-art performance while retaining the interpretability of our model. We have furthermore h"
P17-2069,W11-0115,0,0.0578337,"Missing"
P17-2069,P08-1028,0,0.681042,"r 2 A PT space on the basis of the concatenation of ukWaC, Wackypedia and the BNC (Baroni et al., 2009), pre-parsed with the Malt parser (Nivre et al., 2006). We PPMI transformed the raw co-occurrence counts prior to composition, using a negative SPPMI shift of log 5 (Levy and Goldberg, 2014b). We also experimented with composing normalised counts and applying the PPMI transformation after composition as done by Weeds et al. (2017), however found composing PPMI scores to work better for this task. We evaluate our offset inference algorithm on two popular short phrase composition benchmarks by Mitchell and Lapata (2008) and Mitchell and Lapata (2010), henceforth ML08 and ML10 respectively. The ML08 dataset consists of 120 distinct verb-object (VO) pairs and the ML10 dataset contains 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object pairs. The goal is to compare a model’s similarity estimates to human provided judgements. For both tasks, each phrase pair has been rated by multiple human annotators on a scale between 1 and 7, where 7 indicates maximum similarity. Comparison with human judgements is achieved by calculating Spearman’s ρ between the model’s similarity estimates and the scores of eac"
P17-2069,D14-1163,0,0.0600366,"tperforms 436 the method of Kober et al. (2016) by a statistically significant margin on both datasets. A PT configuration None Standard DI Offset Inference AN 0.35 0.48‡ 0.49‡ ML10 NN VO 0.50 0.39 0.51 0.43‡ 0.52 0.44‡ Avg 0.41 0.47‡ 0.48∗‡ roni and Zamparelli, 2010; Baroni et al., 2014; Coecke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013). Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007);"
P17-2069,D15-1279,0,0.0127042,"016) by a statistically significant margin on both datasets. A PT configuration None Standard DI Offset Inference AN 0.35 0.48‡ 0.49‡ ML10 NN VO 0.50 0.39 0.51 0.43‡ 0.52 0.44‡ Avg 0.41 0.47‡ 0.48∗‡ roni and Zamparelli, 2010; Baroni et al., 2014; Coecke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013). Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (2010, 2011); Weeds et"
P17-2069,P13-1088,0,0.0174707,"as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014). The perhaps most popular approach in the literature to evaluating compositional distributional semantic models is to compare human word and phrase similarity judgements with similarity estimates of composed meaning representations, under the assumption that better distributional representations will perform better at these tasks (Blacoe and Lapata, 2012; Dinu et al., 2013; Erk and Pad´o, 2008; Hashimoto et al., 2014; Hermann and Blunsom, 2013; Kiela et al., 2014; Turney, 2012). ML08 VO 0.22 0.29‡ 0.31†‡ Table 3: Comparison of DI algorithms. ‡ denotes statistical significance at p < 0.01 in comparison to the method without DI, * denotes statistical significance at p < 0.01 in comparison to standard DI and † denotes statistical significance at p < 0.05 in comparison to standard DI. Table 4 shows that offset inference substantially outperforms comparable sparse models by Dinu et al. (2013) on ML08, achieving a new state-ofthe-art, and matches the performance of the stateof-the-art neural network model of Hashimoto et al. (2014) on ML"
P17-2069,nivre-etal-2006-maltparser,0,0.0236999,"co-occurrence events, whereas past that threshold distributional inference degrades to just generic smoothing that is simply compensating for sparsity, but overwhelming the representations with non-plausible co-occurrence information. A similar effect has also been observed by Erk and Pado (2010) in an exemplarbased model. Experiments For our experiments we re-implemented the standard DI method of Kober et al. (2016) for a direct comparison. We built an order 2 A PT space on the basis of the concatenation of ukWaC, Wackypedia and the BNC (Baroni et al., 2009), pre-parsed with the Malt parser (Nivre et al., 2006). We PPMI transformed the raw co-occurrence counts prior to composition, using a negative SPPMI shift of log 5 (Levy and Goldberg, 2014b). We also experimented with composing normalised counts and applying the PPMI transformation after composition as done by Weeds et al. (2017), however found composing PPMI scores to work better for this task. We evaluate our offset inference algorithm on two popular short phrase composition benchmarks by Mitchell and Lapata (2008) and Mitchell and Lapata (2010), henceforth ML08 and ML10 respectively. The ML08 dataset consists of 120 distinct verb-object (VO)"
P17-2069,Q16-1002,0,0.0140639,"of Kober et al. (2016) by a statistically significant margin on both datasets. A PT configuration None Standard DI Offset Inference AN 0.35 0.48‡ 0.49‡ ML10 NN VO 0.50 0.39 0.51 0.43‡ 0.52 0.44‡ Avg 0.41 0.47‡ 0.48∗‡ roni and Zamparelli, 2010; Baroni et al., 2014; Coecke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013). Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (201"
P17-2069,J07-2002,0,0.176991,"Missing"
P17-2069,P14-1009,0,0.0130602,"e ML10 development set. Results Table 3 shows that both forms of distributional inference significantly outperform a baseline without DI. On average, offset inference outperforms 436 the method of Kober et al. (2016) by a statistically significant margin on both datasets. A PT configuration None Standard DI Offset Inference AN 0.35 0.48‡ 0.49‡ ML10 NN VO 0.50 0.39 0.51 0.43‡ 0.52 0.44‡ Avg 0.41 0.47‡ 0.48∗‡ roni and Zamparelli, 2010; Baroni et al., 2014; Coecke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013). Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural inf"
P17-2069,P14-2135,0,0.0309636,"Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014). The perhaps most popular approach in the literature to evaluating compositional distributional semantic models is to compare human word and phrase similarity judgements with similarity estimates of composed meaning representations, under the assumption that better distributional representations will perform better at these tasks (Blacoe and Lapata, 2012; Dinu et al., 2013; Erk and Pad´o, 2008; Hashimoto et al., 2014; Hermann and Blunsom, 2013; Kiela et al., 2014; Turney, 2012). ML08 VO 0.22 0.29‡ 0.31†‡ Table 3: Comparison of DI algorithms. ‡ denotes statistical significance at p < 0.01 in comparison to the method without DI, * denotes statistical significance at p < 0.01 in comparison to standard DI and † denotes statistical significance at p < 0.05 in comparison to standard DI. Table 4 shows that offset inference substantially outperforms comparable sparse models by Dinu et al. (2013) on ML08, achieving a new state-ofthe-art, and matches the performance of the stateof-the-art neural network model of Hashimoto et al. (2014) on ML10, while being full"
P17-2069,J98-1004,0,0.759481,"Missing"
P17-2069,D12-1110,0,0.19718,"Missing"
P17-2069,W14-1502,1,0.83461,"l., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014). The perhaps most popular approach in the literature to evaluating compositional distributional semantic models is to compare human word and phrase similarity judgements with similarity estimates of composed meaning representations, under the assumption that better distributional representations will perform better at these tasks (Blacoe and Lapata, 2012; Dinu et al., 2013; Erk and Pad´o, 2008; Hashimoto et al., 2014; Hermann and Blunsom, 2013; Kiela et al., 2014; Turney, 2012). ML08 VO 0.22 0.29‡ 0.31†‡ Table 3: Comparison of DI algorithms. ‡ denotes statistical significance at p < 0.01 in c"
P17-2069,Q14-1017,0,0.0238767,"Missing"
P17-2069,J16-4006,1,0.913836,"rsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (A PTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in A PTs and infers missing data by the same mechanism that is used for semantic composition. 1 Introduction 2 Anchored Packed Trees (A PTs) is a recently proposed approach to distributional semantics that takes distributional composition to be a process of lexeme contextualisation (Weir et al., 2016). A lexeme’s meaning, characterised as knowledge concerning co-occurrences involving that lexeme, is represented with a higher-order dependencytyped structure (the A PT) where paths associated with higher-order dependencies connect vertices associated with weighted lexeme multisets. The central innovation in the compositional theory is that the A PT’s type structure enables the precise alignment of the semantic representation of each of the lexemes being composed. Like other countbased distributional spaces, however, it is prone to considerable data sparsity, caused by not observing all plausi"
P17-2069,Q15-1025,0,0.0151342,"both datasets. A PT configuration None Standard DI Offset Inference AN 0.35 0.48‡ 0.49‡ ML10 NN VO 0.50 0.39 0.51 0.43‡ 0.52 0.44‡ Avg 0.41 0.47‡ 0.48∗‡ roni and Zamparelli, 2010; Baroni et al., 2014; Coecke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013). Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014). The perhaps most popular approach in"
P17-2069,P10-1097,0,0.440763,"Missing"
P17-2069,Q15-1017,0,0.0125614,"onfiguration None Standard DI Offset Inference AN 0.35 0.48‡ 0.49‡ ML10 NN VO 0.50 0.39 0.51 0.43‡ 0.52 0.44‡ Avg 0.41 0.47‡ 0.48∗‡ roni and Zamparelli, 2010; Baroni et al., 2014; Coecke et al., 2011; Grefenstette et al., 2011; Grefenstette and Sadrzadeh, 2011; Grefenstette et al., 2013; Kartsaklis and Sadrzadeh, 2014; Paperno et al., 2014; Tian et al., 2016; Van de Cruys et al., 2013). Another widespread approach to semantic composition is to use neural networks (Bowman et al., 2016; Hashimoto et al., 2014; Hill et al., 2016; Mou et al., 2015; Socher et al., 2012, 2014; Wieting et al., 2015; Yu and Dredze, 2015), or convolutional tree kernels (Croce et al., 2011; Zanzotto and Dell’Arciprete, 2012; Annesi et al., 2014) as composition functions. The above approaches are applied to untyped distributional vector space models where untyped models contrast with typed models (Baroni and Lenci, 2010) in terms of whether structural information is encoded in the representation as in the models of Erk and Pad´o (2008); Gamallo and Pereira-Fari˜na (2017); Levy and Goldberg (2014a); Pad´o and Lapata (2007); Thater et al. (2010, 2011); Weeds et al. (2014). The perhaps most popular approach in the literature to eva"
P17-2069,I11-1127,0,0.0410885,"Missing"
P17-2069,P16-1121,0,0.022712,"Missing"
P17-2069,C10-1142,0,0.382398,"Missing"
P17-2069,N13-1134,0,0.0306226,"Missing"
P17-2069,D11-1096,0,\N,Missing
P17-2069,W13-3206,0,\N,Missing
P17-2069,D10-1115,0,\N,Missing
P17-2069,D08-1094,0,\N,Missing
P17-2069,P94-1038,0,\N,Missing
P17-2069,P93-1022,0,\N,Missing
P17-2069,J10-4006,0,\N,Missing
P17-2069,2014.lilt-9.5,0,\N,Missing
P17-2069,W17-1901,0,\N,Missing
P17-2069,D12-1050,0,\N,Missing
P87-1015,J84-3005,0,0.15258,"tions whose path sets have nested dependencies. Introduction Much of the study of grammatical systems in computational linguistics has been focused on the weak generative capacity of grammatical forma~sm- Little attention, however, has been paid to the structuraldescriptions that these formalisms can assign to strings, i.e. their strong generative capacity. This aspect of the formalism i s beth linguistically and computationally important. For example, Gazdar (1985) discusses the applicability of Indexed Grammars (IG&apos;s) to Natural Language in terms of the structural descriptions assigned; and Berwick (1984) discusses the strong generative capacity of Lexical-Functional Grammar CLFG) and Government and Bindings grammars (GB). The work of Thatcher (1973) and Rounds (1969) define formal systems that generate tree sets that are related to CFG&apos;s and IG&apos;s. We consider properties of the tree sets generated by CFG&apos;s, Tree Adjoining Grammars (TAG&apos;s), Head GrammarS (HG&apos;s), Categorial Grammars (CG&apos;s), and IG&apos;s. We examine both the complexity of the paths of trees in the tree sets, and the kinds of dependencies that the formalisms can impose between paths. These two properties of the tree sets are not only"
P87-1015,J88-4001,0,\N,Missing
P88-1034,T75-2001,0,0.462793,"hat they produced. Several formalisms that had previously been descn2~d as mildly contextsensitive were found to share a number of properties. In particular, the derivations of a grammar could be represenled with trees that always formed the tree set of a context-free grammar. Formalisms that share these properties were called Linear Context-Free Rewriting Systems Introduction There have been a number of results concerning the relationship between the weak generative capacity (family of string languages) associated with different grammar formalisms; for example, the thecxem of Oaifman, et al. [3] that Classical Categorial Grammars are weakly equivalent to Context-Free Grammars (CFG&apos;s). Mote recently it has been found that there is a class of languages slightly larger than the class of Context-Free languages that is generated by several different formalisms. In pardodar, Tree Adjoining Grammars (TAG&apos;s) and Head Grammars (HG&apos;s) have been shown to be weakly equivalent [15], and these formalism are also equivalent to a reslriction of Indexed Grammars considered by Gazdar [6] called Linear Indexed Grammars (LIG&apos;s) [13]. In this paper, we examine Combinatory Categorial Grammars (CCG&apos;s), an"
P88-1034,C86-1047,0,0.148532,"S&apos;s. This does not, however, nile out the possibility that there may be alternative ways of representing the derivation of CCG&apos;s that will allow for their classification as LCP&apos;RS&apos;s. Extensions to CCG&apos;s have been considered that enable them to compare two unbounded sU&apos;uctures (for example, in [12]). It has been argued that this may be needed in the analysis of certain coordination phenomena in Dutch. In Section 5 we discuss how these additional features increase the power of the formalism. In so doing, we also give an example demonstrating that the Parenthesisfree Categorial Grammar formalism [5,4] is moze powerful that CCG&apos;s as defined here. Extensions to TAG&apos;s (Multicomponent TAG) have been considered for similar *This work was partially mpportedby NSF gnmts MCS-82-19116CER. MCS-82-07294, DCR-84-10413, ARO grant DAA29-84-9-0027. and DARPA gnmt N0014-85-K0018. We are very grateful to Mark Steedmm, ]C Vijay-Shanker and Remo Pare~:hi for helpful disctmiem. 278 Restrictions can be associated with the use of the combinatory rule in R. These restrictions take the form of conswaints on the instantiations of variables in the rules. These can be constrained in two ways. reasons. However, in th"
P88-1034,P86-1012,0,0.0666507,"S&apos;s. This does not, however, nile out the possibility that there may be alternative ways of representing the derivation of CCG&apos;s that will allow for their classification as LCP&apos;RS&apos;s. Extensions to CCG&apos;s have been considered that enable them to compare two unbounded sU&apos;uctures (for example, in [12]). It has been argued that this may be needed in the analysis of certain coordination phenomena in Dutch. In Section 5 we discuss how these additional features increase the power of the formalism. In so doing, we also give an example demonstrating that the Parenthesisfree Categorial Grammar formalism [5,4] is moze powerful that CCG&apos;s as defined here. Extensions to TAG&apos;s (Multicomponent TAG) have been considered for similar *This work was partially mpportedby NSF gnmts MCS-82-19116CER. MCS-82-07294, DCR-84-10413, ARO grant DAA29-84-9-0027. and DARPA gnmt N0014-85-K0018. We are very grateful to Mark Steedmm, ]C Vijay-Shanker and Remo Pare~:hi for helpful disctmiem. 278 Restrictions can be associated with the use of the combinatory rule in R. These restrictions take the form of conswaints on the instantiations of variables in the rules. These can be constrained in two ways. reasons. However, in th"
P88-1034,C86-1048,1,0.67775,"There have been a number of results concerning the relationship between the weak generative capacity (family of string languages) associated with different grammar formalisms; for example, the thecxem of Oaifman, et al. [3] that Classical Categorial Grammars are weakly equivalent to Context-Free Grammars (CFG&apos;s). Mote recently it has been found that there is a class of languages slightly larger than the class of Context-Free languages that is generated by several different formalisms. In pardodar, Tree Adjoining Grammars (TAG&apos;s) and Head Grammars (HG&apos;s) have been shown to be weakly equivalent [15], and these formalism are also equivalent to a reslriction of Indexed Grammars considered by Gazdar [6] called Linear Indexed Grammars (LIG&apos;s) [13]. In this paper, we examine Combinatory Categorial Grammars (CCG&apos;s), an extension of Classical Categorial Grammars developed by Steedman and his collaborators [1,12,9,10,11]. The main result in this paper is (&apos;LCFRS&apos;s) [14]. On the basis of their weak generative capacity, it appears that CCG&apos;s should be classified as mildly contextsensitive. In Section 4 we consider whether CCG&apos;s should be included in the class of LCFRS&apos;s. The derivation tree sets t"
P88-1034,P87-1015,1,\N,Missing
P90-1001,P89-1018,0,0.0613315,"Missing"
P90-1001,P87-1012,0,0.188195,"Missing"
P90-1001,W89-0217,0,0.0618666,"Missing"
P90-1001,P87-1011,0,0.330902,"Missing"
P90-1001,E89-1002,0,\N,Missing
P90-1001,P88-1031,0,\N,Missing
P90-1001,P88-1034,1,\N,Missing
P92-1018,C80-1007,0,0.0883073,"he generative capacity of a number of grammar formalisms. Several were found to share a number of characteristics (described below) and the class of such formalisms was called linear context-free rewriting systems. This paper shows how the class of string languages generated by linear context-free rewriting systems relates to a number of other systems that have been studied by formal language theorists. In particular, we show that the class of string languages generated by linear context-free rewriting systems is equal to the class of output languages of deterministic tree-walking transducers [1]. A number of other equivalences have already been established. In [10] it was shown that linear contextfree rewriting systems and multicomponent tree adjoining grammars [6] generate the same string languages. The multiple context-free grammars of [7] are equivalent to linear context-free systems. This follows *I would like to thank Joost Engelfriet for drawing my attention to context-free hypergraph grammars and their relationship to deterministic tree-walking automata. 136 from the fact that multiple context-free grammars are exactly that subclass of the linear context-free rewriting systems"
P92-1018,P87-1015,1,0.921803,"H davidw @cogs. sussex, ac. uk Abstract We show that the class of string languages generated by linear context-free rewriting systems is equal to the class of output languages of deterministic treewalking transducers. From equivalences that have previously been established we know that this class of languages is also equal to the string languages generated by context-free hypergraph grammars, multicomponent tree-adjoining grammars, and multiple contextfree grammars and to the class of yields of images of the regular tree languages under finite-copying topdown tree transducers. Introduction In [9] a comparison was made of the generative capacity of a number of grammar formalisms. Several were found to share a number of characteristics (described below) and the class of such formalisms was called linear context-free rewriting systems. This paper shows how the class of string languages generated by linear context-free rewriting systems relates to a number of other systems that have been studied by formal language theorists. In particular, we show that the class of string languages generated by linear context-free rewriting systems is equal to the class of output languages of deterministi"
P95-1011,J92-3001,0,0.222787,"such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be added 5, and are annotated to indicate node t y p e : <>indicates an anchor node, and I indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown & Hippisley (1994), Corbett & Fraser (1993), Cahill (1990; 1993), Cahill &: Evans (1990), Fraser &= Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff & Gazdar (1995), Reinhard & Gibbon (1991). 4See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda & Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. Sin fact, [TAG commonly distinguishes two sets of features at each node (top and bottota), but for simplicity we shall assume just one set in this paper. 78 This says that Give is a verb, with vp as its parent, an s as its grandparent and an"
P95-1011,C90-3009,0,0.016452,"there is at least one leaf node labeled with a l e x i c a l category (such lexical leaf nodes are known as a n c h o r s ) . For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be added 5, and are annotated to indicate node t y p e : <>indicates an anchor node, and I indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown & Hippisley (1994), Corbett & Fraser (1993), Cahill (1990; 1993), Cahill &: Evans (1990), Fraser &= Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff & Gazdar (1995), Reinhard & Gibbon (1991). 4See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda & Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. Sin fact, [TAG commonly distinguishes two sets of fea"
P95-1011,E93-1012,0,0.121359,"Missing"
P95-1011,C92-1034,0,0.145083,"vent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the [TAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe ef a/.(1993); Daelemans & Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DA&apos;lR (Evans & Gazdar, 1989a; Evans & Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an [-&apos;lAG. The issue of efficient representation for I_&apos;rAG1 is discussed by Vijay-Shanker & Schabes (1992), who 1As with all fully lexicMized grammar formalisms, there is really no conceptual distinction to be drawn in I_TAG between the lexicon and the grammar: tile gramrnatical rules are just lexical properties. 2See Section 6 for further discussion of these approaches. 77 lexicon fragments for HPSG, PATR and Word Grammar, among others. Second, DATR is not restricted to syntactic description, so one can take advantage of existing analyses of other levels of lexical description, such as phonology, prosody, morphology, compositional semantics and lexical semantics 3. Third, one can exploit existing"
P95-1011,C94-2149,0,0.0266348,"i v e ~z Computing Sciences University of Sussex dav±dw©cogs, susx. ac. uk Introduction The Tree Adjoining G r a m m a r (lAG) formalism was first introduced two decades ago (3oshi et al., 1975), and since then there has been a steady stream of theoretical work using the formalism. But it is only more recently that grammars of non-trivial size have been developed: Abeille, Bishop, Cote & Schabes (1990) describe a feature-based Lexicalized Tree Adjoining G r a m m a r ([_&apos;lAG) for English which subsequently became the basis for the grammar used in the XTAG system, a wide-coverage [_TAGparser (Doran et al., 1994b; Doran et al., 1994a; XTAG Research Group, 1995). The advent of such large grammars gives rise to questions of efficient representation, and the fully lexicalized character of the [TAG formalism suggests that recent research into lexical representation might be a place to look for answers (see for example Briscoe ef a/.(1993); Daelemans & Gazdar(1992)). In this paper we explore this suggestion by showing how the lexical knowledge representation language (LKRL) DA&apos;lR (Evans & Gazdar, 1989a; Evans & Gazdar, 1989b) can be used to formulate a compact, hierarchical encoding of an [-&apos;lAG. The issu"
P95-1011,E91-1024,0,0.0394271,"ollowing LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be added 5, and are annotated to indicate node t y p e : <>indicates an anchor node, and I indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown & Hippisley (1994), Corbett & Fraser (1993), Cahill (1990; 1993), Cahill &: Evans (1990), Fraser &= Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff & Gazdar (1995), Reinhard & Gibbon (1991). 4See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda & Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. Sin fact, [TAG commonly distinguishes two sets of features at each node (top and bottota), but for simplicity we shall assume just one set in this paper. 78 This says that Give is a verb, with vp as its parent, an s as its grandparent and an NP to the left of its parent. It also"
P95-1011,E93-1026,0,0.262646,"odes are known as a n c h o r s ) . For example, the canonical tree for a ditransitive verb such as give is shown in figure 1. Following LTAG conventions (for the time being), the node labels here are gross syntactic category specifications to which additional featural information may be added 5, and are annotated to indicate node t y p e : <>indicates an anchor node, and I indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown & Hippisley (1994), Corbett & Fraser (1993), Cahill (1990; 1993), Cahill &: Evans (1990), Fraser &= Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff & Gazdar (1995), Reinhard & Gibbon (1991). 4See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda & Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. Sin fact, [TAG commonly distinguishes two sets of features at each node (top and bottota), but for simplicity we shall assume just one set in this p"
P95-1011,C94-2177,0,0.52553,"tactic category specifications to which additional featural information may be added 5, and are annotated to indicate node t y p e : <>indicates an anchor node, and I indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown & Hippisley (1994), Corbett & Fraser (1993), Cahill (1990; 1993), Cahill &: Evans (1990), Fraser &= Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff & Gazdar (1995), Reinhard & Gibbon (1991). 4See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda & Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. Sin fact, [TAG commonly distinguishes two sets of features at each node (top and bottota), but for simplicity we shall assume just one set in this paper. 78 This says that Give is a verb, with vp as its parent, an s as its grandparent and an NP to the left of its parent. It also has an NP to its right, and a tree rooted in a P to the right of that, wit"
P95-1011,E93-1065,0,0.0718358,"ation may be added 5, and are annotated to indicate node t y p e : <>indicates an anchor node, and I indicates a substitution node (where a 3See, for example, Bleiching (1992; 1994), Brown & Hippisley (1994), Corbett & Fraser (1993), Cahill (1990; 1993), Cahill &: Evans (1990), Fraser &= Corbett (in press), Gibbon (1992), Kilgarriff (1993), Kilgarriff & Gazdar (1995), Reinhard & Gibbon (1991). 4See, for example, Andry et al. (1992) on compilation, Kilbury et al. (1991) on coding DAGs, Duda & Gebhardi (1994) on dynamic querying, Langer (1994) on reverse querying, and Barg (1994), Light (1994), Light et al. (1993) and Kilbury et al. (1994) on automatic acquisition. And there are at least a dozen different DATR implementations available, on various platforms and programming languages. Sin fact, [TAG commonly distinguishes two sets of features at each node (top and bottota), but for simplicity we shall assume just one set in this paper. 78 This says that Give is a verb, with vp as its parent, an s as its grandparent and an NP to the left of its parent. It also has an NP to its right, and a tree rooted in a P to the right of that, with a PP parent and NP right sister. The implied bottom-up tree structure"
P95-1011,E91-1023,0,\N,Missing
P95-1011,E89-1009,1,\N,Missing
P95-1011,P92-1010,0,\N,Missing
P95-1021,E91-1005,1,0.76567,"n Section 1.2, there are cases where satisfactory analyses cannot be obtained with adjunction. In particular, using adjunction in this way cannot handle cases in which parts of the clausal complement are required to be placed within the structure of the adjoined tree. The DTG operation of subsertion is designed to overcome this limitation. Subsertion can be viewed as a generalization of adjunction in which components of the clausal complement (the subserted structure) which are not substituted can be interspersed within the structure that is the site of the subsertion. Following earlier work (Becket et al., 1991; Vijay-Shanker, 1992), DTG provide a mechanism involving the use of domination links (d-edges) that ensure that parts of the subserted structure that are not substituted dominate those parts that are. Furthermore, there is a need to constrain the way in which the non-substituted components can be interspersed 3. This is done by either using appropriate feature constraints at nodes or by means of subsertion-insertion constraints (see Section 2). We end this section by briefly commenting on the other DTG operation of sister-adjunction. In TAG, modification is performed with adjunction of modifi"
P95-1021,J94-1004,0,0.271262,"hextraction in Kashmiri proceeds as in English, except that the wh-word ends up in sentence-second position, with a topic from the matrix clause in sentence-initial position. This is illustrated in (2a) for a simple clause and in (2b) for a complex clause. (2) a. rameshan kyaa dyutnay t s e RameshzRG whatNOM g a v e yOUDAT What did you give Ramesh? b. rameshan kyaal chu baasaan [ ki RameshzRG what is believeNperf that me kor ti] IZRG do What does Ramesh beheve that I did? claim SUBJ ~ "" ~ O M P seem he I COMP adore SUB~BJ Mary hotdog MOD ~ . ~ O D spicy small Figure 2: Dependency tree for (1) Schabes & Shieber (1994) solve the first problem 1For clarity, we depart from standard TAG notational practice and annotate nodes with lexemes and arcs with grammatical function: 152 Since the moved element does not appear in sentence-initial position, the TAG analysis of English wh-extraction of Kroch (1987; 1989) (in which the matrix clause is adjoined into the embedded clause) cannot be transferred, and in fact no linguistically plausible TAG analysis appears to be available. In the past, variants of TAG have been developed to extend the range of possible analyses. In Multi-Component TAG (MCTAG) (Joshi, 1987), tre"
P95-1021,J92-4004,1,0.799715,"s a separate functional (f-) structure, and dependency grammars (see e.g. Mel'~uk (1988)) use these notions as the principal basis for syntactic representation. We will follow the dependency literature in referring to complementation and modification as syntactic dependency. As observed by Rambow and Joshi (1992), for TAG, the importance of the dependency structure means that not only the derived phrase-structure tree is of interest, but also the operations by which we obtained it from elementary structures. This information is encoded in the derivation tree (Vijay-Shanker, 1987). However, as Vijay-Shanker (1992) observes, the TAG composition operations are not used uniformly: while substitution is used only to add a (nominal) complement, adjunction is used both for modification and (clausal) complementation. Clausal complementation could not be handled uniformly by substitution because of the existence of syntactic phenomena such as long-distance wh-movement in English. Furthermore, there is an inconsistency in the directionality of the operations used for complementation in TAG@: nominal complements are substituted into their governing verb's tree, while the governing verb's tree is adjoined into it"
P95-1021,P95-1013,1,\N,Missing
P95-1021,P94-1036,1,\N,Missing
P98-1061,P96-1023,0,0.0632539,"Missing"
P98-1061,1997.iwpt-1.7,0,0.044821,"chabes, 1991) are difficult to parse with efficiently. Each word in the parser&apos;s input string introduces an elementary tree into the parse table for each of its possible readings, and there is often a substantial overlap in structure between these trees. A conventional parsing algorithm (VijayShanker and Joshi, 1985) views the trees as independent, and so is likely to duplicate the processing of this common structure. Parsing could be made more efficient (empirically if not formally), if the shared structure could be identified and processed only once. Recent work by Evans and Weir (1997) and Chen and Vijay-Shanker (1997) addresses this problem from two different perspectives. Evans and Weir (1997) outline a technique for compiling LTAG grammars into automata which are 372 David W e i r Cognitive and C o m p u t i n g Sciences University of Sussex Brighton, BN1 9QH, UK David.Weir@cogs.susx.ac.uk then merged to introduce some sharing of structure. Chen and Vijay-Shanker (1997) use underspecified tree descriptions to represent sets of trees during parsing. The present paper takes the former approach, but extends our previous work by: • showing how merged automata can be minimised, so that they share as much stru"
P98-1061,1997.iwpt-1.10,0,0.102172,"Missing"
P98-1061,P85-1011,0,0.292552,"ised, so that they share as much structure as possible; • showing that by precompiling additional information, parsing can be broken down into recognition followed by parse recovery; • providing a formal treatment of the algorithms for transforming and minimising the grammar, recognition and parse recovery. In the following sections we outline the basic approach, and describe informally our improvements to the previous account. We then give a formal account of the optimisation process and a possible parsing algorithm that makes use of it 1. 2 Automaton-based parsing Conventional LTAG parsers (Vijay-Shanker and Joshi, 1985; Schabes and Joshi, 1988; VijayShanker and Weir, 1993) maintain a p a r s e table, a set of i t e m s corresponding to complete and partial constituents. Parsing proceeds by first seeding the table with items anchored on the input string, and then repeatedly scanning the table for p a r s e r actions. Parser actions introduce new items into the table licensed by one or more items already in the table. The main types of parser actions are: 1. extending a constituent by incorporating a complete subconstituent (on the left or 1However, due to lack of space, no proofs and only minimal informal de"
P98-1061,J93-4002,1,0.901504,"Missing"
P98-1061,P88-1032,0,\N,Missing
P98-1061,1997.iwpt-1.11,1,\N,Missing
W00-2007,J96-2002,0,0.0593147,"Missing"
W00-2007,P95-1011,1,0.890936,"Missing"
W00-2007,P98-1061,1,0.892923,"Missing"
W00-2007,W98-0139,1,0.885359,"Missing"
W00-2007,E99-1029,1,\N,Missing
W00-2007,C98-1059,1,\N,Missing
W00-2007,1997.iwpt-1.11,1,\N,Missing
W02-2229,W98-0108,1,0.862273,"gs of TAG+6 tree can be viewed as invoking some fragment of computation (an elementary computation). Evans and Weir (1998) showed that elementary computations corresponding to bottom-up parsing can be expressed as finite state automata (FSA). All elementary computations for the supertags associated with a word can be combined into a single FSA. By minimizing this automaton (using standard minimization algorithms) sharing of elementary computation is achieved. The hope is that this will lead to significant reductions in parsing time. To date, this proposal has only received limited evaluation. Carroll et al. (1998a) demonstrated that for a large hand-crafted grammar the number of states was significantly reduced by merging and minimizing the FSA associated with a word. For example, the numbers of states in the automaton for the word come (associated with 133 supertags) was reduced from 898 to 50, for break from 1240 to 68, and give from 2494 to 83. This paper improves on this evaluation in two ways: firstly, the grammar used is automatically acquired, so we are not open to the charge that it was designed to make this technique work particularly well; secondly, we measure parse time, not just numbers of"
W02-2229,E99-1025,0,0.0268917,"Missing"
W02-2229,P95-1011,1,0.824869,"Missing"
W02-2229,P98-1061,1,0.936134,"ithm, each elementary  We are extremely grateful to Fei Xia for providing us with the grammar used in these experiments, and to Fei Xia and Anoop Sarkar for the help and advice they have given. 1. Note that the approach described in this section could be combined with supertag filtering.  c 2002 Olga Shaumyan, John Carroll and David Weir. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 201–205. Universit´a di Venezia. 202 Proceedings of TAG+6 tree can be viewed as invoking some fragment of computation (an elementary computation). Evans and Weir (1998) showed that elementary computations corresponding to bottom-up parsing can be expressed as finite state automata (FSA). All elementary computations for the supertags associated with a word can be combined into a single FSA. By minimizing this automaton (using standard minimization algorithms) sharing of elementary computation is achieved. The hope is that this will lead to significant reductions in parsing time. To date, this proposal has only received limited evaluation. Carroll et al. (1998a) demonstrated that for a large hand-crafted grammar the number of states was significantly reduced b"
W02-2229,C94-1024,0,0.0867669,"Missing"
W02-2229,J93-2004,0,0.0273008,"Missing"
W02-2229,W00-2027,0,0.0292538,"s significantly reduced it is not clear that parse time (as opposed to recognition time) will drop. This is because in order that parse trees be recoverable from the parse table, a considerable amount of book-keeping is required when the table is being completed. This increases both space and time requirements. 4. Experimental evaluation We used a grammar that was automatically induced by Fei Xia (1999) from sections 00–24 of the Wall Street Journal Penn Treebank II corpus (Marcus, Santorini and Marcinkiewicz, 1993). This is very similar to the grammar used by Sarkar, Xia and Joshi (2000) and Sarkar (2000), though slightly larger, containing around around 7,500 elementary trees. We implemented the algorithm described by Evans and Weir (1997) and Evans and Weir (1998), the details of which are not repeated here. Prior to parsing, the grammar is precompiled as follows. For each word, the set of trees that it can anchor is determined. This results in a total of 11,035 distinct tree sets. For each of these tree sets we first build what we refer to as an unmerged FSA. This automaton contains a separate progression of transitions for each of the trees in the set; using these automata for parsing give"
W02-2229,C92-1034,0,0.0621259,"Missing"
W02-2229,1997.iwpt-1.11,1,\N,Missing
W02-2229,C96-1034,0,\N,Missing
W02-2229,C98-1059,1,\N,Missing
W02-2229,W00-1605,0,\N,Missing
W03-1011,1995.iwpt-1.8,0,\N,Missing
W03-1011,W96-0209,0,\N,Missing
W03-1011,H94-1046,0,\N,Missing
W03-1011,J02-4004,0,\N,Missing
W03-1011,P90-1034,0,\N,Missing
W03-1011,P97-1009,0,\N,Missing
W03-1011,P98-2127,0,\N,Missing
W03-1011,C98-2122,0,\N,Missing
W03-1011,P99-1004,0,\N,Missing
W03-1011,P93-1024,0,\N,Missing
W05-1202,W03-1004,0,0.0334469,"m the Pascal Textual Entailment Challenge 7 Proceedings of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 7–12, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics Datasets2 (Dagan et al., 2005) and demonstrate empirically how similarity can be found between corresponding phrases when parts of the phrases cannot be said to be similar. In Section 5, we present our conclusions and directions for further work. 2 Background One well-studied approach to the identification of paraphrases is to employ a lexical similarity function. As noted by Barzilay and Elhadad (2003), even a lexical function that simply computes word overlap can accurately select paraphrases. The problem with such a function is not in the accuracy of the paraphrases selected, but in its low recall. One popular way of improving recall is to relax the requirement for words in each sentence to be identical in form, to being identical or similar in meaning. Methods to find the semantic similarity of two words can be broadly split into those which use lexical resources, e.g., WordNet (Fellbaum, 1998), and those which use a distributional similarity measure (see Weeds (2003) for a review of dis"
W05-1202,1995.iwpt-1.8,0,0.0342649,"end and a printer can be mapped to the path in the ontology relating a print request to its target printer. As well as lexical variation, our previous work (Weeds et al., 2004) allowed a certain amount of syntactic variation via its use of grammatical dependencies and policy templates. For example, the passive “paraphrase” of a sentence can be identified by comparing the sets of grammatical dependency relations produced by a shallow parser such as the RASP 2 http://www.pascal-network.org/Challenges/RTE/ The core lexicon lists a canonical word form for each concept in the ontology. 3 8 parser (Briscoe and Carroll, 1995). In other words, by looking at grammatical dependency relations, we can identify that “John is liked by Mary,” is a paraphrase of “Mary likes John,” and not of “John likes Mary.” Further, where there is a limited number of styles of sentence, we can manually identify and list other templates for matches over the trees or sets of dependency relations. For example, “If C1 then C2” is the same as “C2 if C1”. However, the limitations of this approach, which combines lexical variation, grammatical dependency relations and template matching, become increasingly obvious as one tries to scale up. As"
W05-1202,P89-1010,0,0.0301692,"this provides a close approximation to the KL divergence measure. The result is a number greater than or equal to 0, where 0 indicates that the two distributions are identical. In other words, a smaller distance indicates greater similarity. The reason for choosing this measure is that it can be used to compute the distance between any two co-occurrence vectors independent of any information about other words. This is in contrast to many other measures, e.g., Lin (1998), which use the co-occurrences of features with other words to compute a weighting function such as mutual information (MI) (Church and Hanks, 1989). Since we only have corpus data for the target phrases, it is not possible for us to use such a measure. However, the α-skew divergence measure has been shown (Weeds, 2003) to perform comparably with measures which use MI, particularly for lower frequency target words. 4.3 Results The results, in terms of α-skew divergence scores between pairs of phrases, are shown in Table 2. Each set of three lines shows the similarity score between a pair of phrases and then between respective pairs of components. In the first two sets, the phrases are paraphrases whereas in the second two sets, the phrase"
W05-1202,W02-0908,0,0.0132246,"and “battery (of X) is low” have roughly similar meanings without their component words having similar meanings. Further, this does not appear to be due to either phrase being non-compositional. As noted by Pearce (2001), it is not possible to substitute similar words within noncompositional collocations. In this case, however, both phrases appear to be compositional. Words cannot be substituted between the two phrases because they are composed in different ways. 3 Proposal Recently, there has been much interest in finding words which are distributionally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). Two words are said to be distributionally similar if they appear in similar contexts. For example, the two words apple and pear are likely to be seen as the objects of the verbs eat and peel, and this adds to their distributional similarity. The Distributional Hypothesis (Harris, 1968) proposes a connection between distributional similarity and semantic similarity, which is the basis for a large body of work on automatic thesaurus construction using distributional similarity methods (Curran and Moens, 2002; Weeds, 2003; Geffet and Dagan, 2004). Our p"
W05-1202,C04-1051,0,0.135569,"e calculated using a standard metric (e.g., Lin (1998)). In our work, we extend the notion of distributional similarity from linear paths to trees. This allows us to compute distributional similarity for any part of an expression, of arbitrary length and complexity (although, in practice, we are still limited by data sparseness). Further, we do not make any restrictions as to the number or types of the grammatical relation contexts associated with a tree. 4 Empirical Evidence Practically demonstrating our proposal requires a source of paraphrases. We first looked at the MSR paraphrase corpus (Dolan et al., 2004) since it contains a large number of sentences close enough in meaning to be considered paraphrases. However, inspection of the data revealed that the lexical overlap between the pairs of paraphrasing sentences in this corpus is very high. The average word overlap (i.e., the proportion of exactly identical word forms) calculated over the sentences paired by humans in the training set is 0.70, and the lowest overlap4 for such sentences is 0.3. This high word overlap makes this a poor source of examples for us, since we wish to study similarity between phrases which do not share semantically sim"
W05-1202,C04-1036,0,0.259322,"imilar meanings without their component words having similar meanings. Further, this does not appear to be due to either phrase being non-compositional. As noted by Pearce (2001), it is not possible to substitute similar words within noncompositional collocations. In this case, however, both phrases appear to be compositional. Words cannot be substituted between the two phrases because they are composed in different ways. 3 Proposal Recently, there has been much interest in finding words which are distributionally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). Two words are said to be distributionally similar if they appear in similar contexts. For example, the two words apple and pear are likely to be seen as the objects of the verbs eat and peel, and this adds to their distributional similarity. The Distributional Hypothesis (Harris, 1968) proposes a connection between distributional similarity and semantic similarity, which is the basis for a large body of work on automatic thesaurus construction using distributional similarity methods (Curran and Moens, 2002; Weeds, 2003; Geffet and Dagan, 2004). Our proposal is that just as words have distrib"
W05-1202,P98-2127,0,0.647866,"that “X needs charging” and “battery (of X) is low” have roughly similar meanings without their component words having similar meanings. Further, this does not appear to be due to either phrase being non-compositional. As noted by Pearce (2001), it is not possible to substitute similar words within noncompositional collocations. In this case, however, both phrases appear to be compositional. Words cannot be substituted between the two phrases because they are composed in different ways. 3 Proposal Recently, there has been much interest in finding words which are distributionally similar e.g., Lin (1998), Lee (1999), Curran and Moens (2002), Weeds (2003) and Geffet and Dagan (2004). Two words are said to be distributionally similar if they appear in similar contexts. For example, the two words apple and pear are likely to be seen as the objects of the verbs eat and peel, and this adds to their distributional similarity. The Distributional Hypothesis (Harris, 1968) proposes a connection between distributional similarity and semantic similarity, which is the basis for a large body of work on automatic thesaurus construction using distributional similarity methods (Curran and Moens, 2002; Weeds,"
W05-1202,W04-3206,0,0.0747372,"s the verb send i.e., it has the feature, <mod-of, send>. Having defined these models for the unit of interest, the sub-parse, and for the context of a sub-parse, we can build up co-occurrence vectors for sub-parses in the same way as for words. A co-occurrence vector is a conglomeration (with frequency counts) of all of the co-occurrences of the target unit found in a corpus. The similarity between two such vectors or descriptions can then be found using a standard distributional similarity measure (see Weeds (2003)). The use of distributional evidence for larger units than words is not new. Szpektor et al. (2004) automatically identify anchors in web corpus data. Anchors are lexical elements that describe the context of a sentence and if words are found to occur with the same set of anchors, they are assumed to be paraphrases. For example, the anchor set {Mozart, 1756} is a known anchor set for verbs with the meaning “born in”. However, this use of distributional 9 evidence requires both anchors, or contexts, to occur simultaneously with the target word. This differs from the standard notion of distributional similarity which involves finding similarity between cooccurrence vectors, where there is no"
W05-1202,W03-1011,1,0.805797,"ions5 for each word in the sub-parse are output as features. When the sub-parse is only a word, the process is simplified to finding grammatical relations containing that word. The raw feature file is then converted into a cooccurrence vector by counting the occurrences of each feature type. Table 1 shows the number of feature types and tokens extracted for each phrase. This shows that we have extracted a reasonable number of features for each phrase, since distributional similarity techniques have been shown to work well for words which occur more than 100 times in a given corpus (Lin, 1998; Weeds and Weir, 2003). We then computed the distributional similarity between each co-occurrence vector using the α-skew divergence measure (Lee, 1999). The α-skew divergence measure is an approximation to the KullbackLeibler (KL) divergence meassure between two distributions p and q: D(p||q) = X x 5 p(x)log p(x) q(x) We currently retain all of the distinctions between grammatical relations output by RASP. The α-skew divergence measure is designed to be used when unreliable maximum likelihood estimates (MLE) of probabilities would result in the KL divergence being equal to ∞. It is defined as: distα (q, r) = D(r||"
W05-1202,J90-1003,0,\N,Missing
W05-1202,W07-1401,0,\N,Missing
W05-1202,C98-2122,0,\N,Missing
W05-1202,P99-1004,0,\N,Missing
W07-2304,N07-1021,1,0.83879,"strictly speaking, according to our model the training parameters are part of the traversal algorithm, not the tree. is considerably more expensive than the greedy modes. 2. Greedy generation: make the single most likely decision at each choice point (rule expansion) in a generation process. This is not guaranteed to result in the most likely generation process, but the computational cost is very low. 3 Examples of this control model 3. Greedy roulette-wheel generation: use a nonuniform random distribution proportional to the likelihoods of alternatives. 3.1 Example 1 – pCRU pCRU (Belz, 2006; Belz, 2007) is a probabilistic language generation framework for creating NLG systems that contain a probabilistic model of the entire generation space, represented by a context-free underspecification grammar. The basic idea is to view all generation rules as context-free rules and to estimate a single probabilistic model from a corpus of texts to guide the generation process. In nonprobabilistic mode, the generator operates by taking any sentential form of the grammar as an input and expanding it using the grammar to all possible fully specified forms, which are the outputs. Thus a pCRU grammar looks r"
W07-2304,W02-2119,1,0.852946,"output gives rise to one source of variation of control. We are not interested in what outputs are, but only how they are constructed, and so we think of them purely in terms of the content operations that give rise to them. Adopting this very abstract view, we can conceptualise a generator as having the following principal components: 2 The control model We start with a very general view of the generation process2 . Generation takes an input and produces an output, which is a ‘more linguistically instantiated’ representation of the input (but we will not say precisely what that means — cf. (Evans et al., 2002; McDonald, 1993). In the process of doing this, the generator makes various decisions about the content of its output — it reaches a choice-point at which several options are possible and selects one to follow, and then reaches another choice point, and so on. In fact, this is all any generation algorithm does: visit choice points one after another and make a decision relating to the content of output at each one. Each decision may be constrained by the input, constrained by other decisions already made, or determined by the generation algorithm itself. These constraints may not reduce the nu"
W07-2304,W02-2103,0,0.0357578,"tional grammar for syntax, except that it is used to model deep generation as well as surface realisation. The probabilistic version of pCRU introduces a probability distribution over the generator decisions. This is achieved by using treebank training, that is, estimating a distribution over the expansion rules that encode the generation space from a corpus using two steps3 : Belz (2006) describes an application of the system to weather forecast generation, and compares the different control techniques with human generation and a more traditional generate-and-test probabilistic architecture (Langkilde-Geary, 2002). pCRU can be interpreted using the model introduce here in the following way (cf. the first example given in section 2.1). The generation tree is defined by all the possible derivations according to the grammar. Each node corresponds to a sentential form and each child node is the result of rewriting a single non-terminal using a grammar rule. Thus the content operations are grammar rules. The content structures are sentences in the grammar. The input is itself a sentential form which identifies where in the complete tree to start generating from, and the input constraint algorithm does nothi"
W07-2304,J93-1009,0,0.0372096,"o one source of variation of control. We are not interested in what outputs are, but only how they are constructed, and so we think of them purely in terms of the content operations that give rise to them. Adopting this very abstract view, we can conceptualise a generator as having the following principal components: 2 The control model We start with a very general view of the generation process2 . Generation takes an input and produces an output, which is a ‘more linguistically instantiated’ representation of the input (but we will not say precisely what that means — cf. (Evans et al., 2002; McDonald, 1993). In the process of doing this, the generator makes various decisions about the content of its output — it reaches a choice-point at which several options are possible and selects one to follow, and then reaches another choice point, and so on. In fact, this is all any generation algorithm does: visit choice points one after another and make a decision relating to the content of output at each one. Each decision may be constrained by the input, constrained by other decisions already made, or determined by the generation algorithm itself. These constraints may not reduce the number of choices a"
W07-2304,P05-1008,1,0.908156,"ration process, and correspondingly of generation systems. The simple pipeline model of Reiter and Dale (2000) actually conceals a wide range of underlying approaches to generation (cf. (Mellish et al., 2006, section 2.1)), and a recent initiative to provide a ‘reference architecture’ for such systems (involving some of the present authors) abandoned any attempt to harmonise control aspects of the systems it studied (Mellish et al., 2006). In such a situation, it is difficult to see how any general statements, results or techniques relating to controlling generation can be developed, although Paiva and Evans (2005) report an approach that has the potential for wider applicability. In the present paper, we introduce a view of the generation process which abstracts away from specific generation systems or architectures, to a point at which it is possible to separate control from content decisions in the generation process. This allows us to explore systematically different control strategies for constructing the same content (mapping from an input to the same output), and examine different approaches (e.g. generative, empirical) to the problem of controlling generation. The approach we develop is quite ab"
W10-2806,1993.eamt-1.1,0,0.0298873,"follows. Define sets Λk (k = 0, 1, 2, . . .) inductively as follows: We show that T (V )/Gk (for an appropriate choice of k) captures the essential features of T (V )/I in terms of equivalence: Proposition. Let deg(a − b) = d and let k ≥ d − mindeg(Λ). Then a ≡ b in T (V )/Gk if and only if a ≡ b in T (V )/I. Proof. Since Gk ⊆ I, the equivalence class of an element a in T (V )/I is a superset of the equivalence class of a in T (V )/Gk , which gives the forward implication. The reverse follows from the lemma above. In order to define an inner product on T (V )/Gk , we make use of the result of Berberian (1961) that if M is a finite-dimensional linear subspace of a pre-Hilbert space P , then P = M ⊕ M ⊥ , where M ⊥ is the orthogonal complement of M in P . In our case this implies T (V ) = Gk ⊕ G⊥ k and that every element x ∈ T (V ) has a unique decomposition as x = y + x0k where y ∈ Gk and x0k ∈ G⊥ k . This implies that T (V )/Gk is isomorphic to G⊥ k , and that for each equivalence class [x]k in T (V )/Gk there is a unique corresponding element x0k ∈ G⊥ k such that x0k ∈ [x]k . This element x0k can be thought of as the canonical representation of all elements of [x]k in T (V )/Gk , and can be found"
W10-2806,W01-0514,0,0.0149663,". un form an orthonormal basis for a vector space U and v1 , v2 , . . . vm form an orthonormal basis for vector space V , then the space U ⊗ V has dimensionality nm with an orthonormal basis formed by the set of all ordered pairs (ui , vj ), denoted by ui ⊗ vj , of the individual P basis elements. For P arbitrary elements u = ni=1 αi ui and v = m j=1 βj vj the tensor product of u and v is then given by Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word lev"
W10-2806,W09-0208,0,0.0141212,"eanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., u⊗v = n X m X i αi βj ui ⊗ vj j 38 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38–44, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics For two finite dimensional vector spaces U and V (over a field F ) of dimensionality n and m respectively, the direct sum U ⊕ V is defined as the cartesian product U × V together with the operations (u1 , v1 ) + (u2 , v2"
W10-2806,P98-2127,0,0.164344,"ly speaking, if u1 , u2 , . . . un form an orthonormal basis for a vector space U and v1 , v2 , . . . vm form an orthonormal basis for vector space V , then the space U ⊗ V has dimensionality nm with an orthonormal basis formed by the set of all ordered pairs (ui , vj ), denoted by ui ⊗ vj , of the individual P basis elements. For P arbitrary elements u = ni=1 αi ui and v = m j=1 βj vj the tensor product of u and v is then given by Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While thes"
W10-2806,P04-1036,0,0.0347356,"for formal and complete definitions. Roughly speaking, if u1 , u2 , . . . un form an orthonormal basis for a vector space U and v1 , v2 , . . . vm form an orthonormal basis for vector space V , then the space U ⊗ V has dimensionality nm with an orthonormal basis formed by the set of all ordered pairs (ui , vj ), denoted by ui ⊗ vj , of the individual P basis elements. For P arbitrary elements u = ni=1 αi ui and v = m j=1 βj vj the tensor product of u and v is then given by Vector based techniques have been exploited in a wide array of natural language processing applications (Sch¨utze, 1998; McCarthy et al., 2004; Grefenstette, 1994; Lin, 1998; Bellegarda, 2000; Choi et al., 2001). Techniques such as latent semantic analysis and distributional similarity analyse contexts in which terms occur, building up a vector of features which incorporate aspects of the meaning of the term. This idea has its origins in the distributional hypothesis of Harris (1968), that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be rep"
W10-2806,P08-1028,0,0.0893946,", that words with similar meanings will occur in similar contexts, and vice-versa. However, there has been limited attention paid to extending this idea beyond individual words, so that the distributional meaning of phrases and whole sentences can be represented as vectors. While these techniques work well at the word level, for longer strings, data becomes extremely sparse. This has led to various proposals exploring methods for composing vectors, rather than deriving them directly from the data (Landauer and Dumais, 1997; Foltz et al., 1998; Kintsch, 2001; Widdows, 2008; Clark et al., 2008; Mitchell and Lapata, 2008; Erk and Pado, 2009; Preller and Sadrzadeh, 2009). Many of these approaches use a pre-defined composition operation such as addition (Landauer and Dumais, 1997; Foltz et al., u⊗v = n X m X i αi βj ui ⊗ vj j 38 Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 38–44, c Uppsala, Sweden, 16 July 2010. 2010 Association for Computational Linguistics For two finite dimensional vector spaces U and V (over a field F ) of dimensionality n and m respectively, the direct sum U ⊕ V is defined as the cartesian product U × V together with the operations ("
W10-2806,J98-1004,0,0.553291,"Missing"
W11-0136,D10-1115,0,0.0170845,"results indicate that the learnt parameters tend to get very large when using least squares to find the parameters, leading to poor results; we plan to investigate other methods such as linear optimisation. Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. His model is however much more restrictive than ours in that the value of a component in the product depends only on that same component in the composed vectors, whereas in our model, the value of the component can depend on all components in the composed vectors. Baroni and Zamparelli (2010) took a similar approach, in which adjectives are modelled as matrices acting on the space of nouns, and the matrices are learnt using least squares regression. The algebra products we propose learning are more general than matrix products; in addition we do not need to distinguish between words which are represented as matrices and words which are represented as vectors. 4 Constructing Algebras from Semigroups Whilst the previous two techniques we have discussed are very general, and allow corpus data to be easily incorporated into the composition definition, our implementations are currently"
W11-0136,W10-2806,1,0.386805,"ot commutative. The resulting structure is an associative algebra over a field — or simply an algebra when there is no ambiguity. Clarke (2007) gives a mathematical model of meaning as context, and shows that under this model, the meaning of natural language expressions can be described by an algebra. The framework is also applied to models of textual entailment, and logical and ontological representations of natural language meaning. In this paper, we identify three general techniques for constructing algebras. • Using quotient algebras to impose relations on a free algebra, as described in (Clarke et al., 2010). • Defining finite-dimensional algebras using matrices. Any finite-dimensional algebra can be described in this way; we have investigated the possibility of learning such algebras using least squares regression. • Constructing algebras from a semigroup to give it vector space properties. We sketch a possible method of using this technique, identified by Clarke (2007), to endow logical semantics with a vector space nature. This paper presents a preliminary consideration of these general techniques, and our goal is simply to show that they are worthy of further exploration. 325 red book big boo"
W11-0136,W10-2805,0,0.030123,"least squares, and we have performed some experiments using this method using a corpus extracted from the ukWaC corpus (Ferraresi et al., 2008). We extracted a list of verb adjective ∗ noun sequences, and used latent semantic analysis (Deerwester et al., 1990) to generate ndimensional vectors for the 160 most common adjectives and nouns, and pairs of these adjectives and nouns. Our initial results indicate that the learnt parameters tend to get very large when using least squares to find the parameters, leading to poor results; we plan to investigate other methods such as linear optimisation. Guevara (2010) proposed a related method of learning composition which used linear regression to learn how components compose. His model is however much more restrictive than ours in that the value of a component in the product depends only on that same component in the composed vectors, whereas in our model, the value of the component can depend on all components in the composed vectors. Baroni and Zamparelli (2010) took a similar approach, in which adjectives are modelled as matrices acting on the space of nouns, and the matrices are learnt using least squares regression. The algebra products we propose l"
W13-2705,W11-3702,0,0.0715947,"Missing"
W13-2705,P11-2099,0,0.17242,"Missing"
W13-2705,E12-1062,0,0.209282,"Missing"
W13-2705,W12-0607,0,0.0755526,"Missing"
W13-2705,N12-1066,0,0.495607,"Missing"
W13-2705,D11-1136,0,0.266537,"Missing"
W14-1502,D10-1115,0,0.487232,"Missing"
W14-1502,P99-1004,0,0.620378,"co-occurrences, which are not observed with the phrase. Related Work In this work, we bring together ideas from several different strands of distributional semantics: incorporating syntactic information into the distributional representation of a lexeme; representing phrasal meaning by creating distributional representations through composition; and representing word meaning in context by modifying the distributional representation of a word. The use of syntactic structure in distributional representations is not new. Two of the earliest proponents of distributional semantics, Lin (1998) and Lee (1999) used features based on first order dependency relations between words in their distributional representations. More recently, Pado and Lapata (2007) propose a semantic space based on dependency paths. This model outperformed traditional word-based models which do not take syntax into account in a synonymy relation detection task and a prevalent sense acquisition task. The problem of representing phrasal meaning has traditionally been tackled by taking vector representations for words (Turney and Pantel, 2010) and combining them using some function to pro16 add max mult min gm dp weight:compos"
W14-1502,W13-0104,0,0.0208168,"Association Scores (PLMI) (Scheible et al., 2013) and positive normalised point wise mutual information (PNPMI) (Bouma, 2009). For definitions, see Table 3. Timing of feature weighting. We consider two alternatives: we can weight features before composition so that the composition operation is applied to weighted vectors, or we can compose vectors prior to feature weighting, in which case the composition operation is applied to unweighted vectors, and feature weighting is applied in the context of making a similarity calculation. In other work, the former order is often implied. For example, Boleda et al. (2013) state that they use “PMI to weight the co-occurrence matrix”. However, if we allow the second order, features which might have a zero association score in the context of the the individual lexemes, could be considered significant in the context of the phrase. 3 Evaluation Our experimental evaluation of the approach is based on the assumption, which is commonly made elsewhere, that where there is a reasonable amount of corpus data available for a phrase, this will generate a good estimate of the vector of the phrase. It has been shown (Turney, 2012; Baroni and Zamparelli, 2010) that such “obse"
W14-1502,P98-2127,0,0.836955,"es higher-order grammatical dependency relations as features. We apply the approach to adjective-noun compounds with promising results in the prediction of the vectors for (held-out) observed phrases. 1 Introduction Vector space models of semantics characterise the meaning of a word in terms of distributional features derived from word co-occurrences. The most widely adopted basis for word co-occurrence is proximity, i.e. that two words (or more generally lexemes) are taken to co-occur when they occur together within a certain sized window, or within the same sentence, paragraph, or document. Lin (1998), in contrast, took the syntactic relationship between co-occurring words into account: the distributional features of a word are based on the word’s grammatical dependents as found in a dependency parsed corpus. For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy"
W14-1502,P08-1028,0,0.838603,"erved nouns and adjective-noun phrases. As a consequence of the computational expense of the machine learning techniques involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which enco"
W14-1502,P89-1010,0,0.178882,"orpus data available for a phrase, this will generate a good estimate of the vector of the phrase. It has been shown (Turney, 2012; Baroni and Zamparelli, 2010) that such “observed” vectors are indeed reasonable for adjective-noun and noun-noun compounds. Hence, in order to evaluate the compositional models under consideration here, we compare observed phrasal vectors with inferred phrasal vectors, where the comparison is made using the cosine measure. We note that it is Feature weighting. We consider three options. Much work in this area has used positive pointwise mutual information (PPMI) (Church and Hanks, 1989) to weight the features. However, PPMI is known to over-emphasise low frequency events, and as a result there has been a recent shift towards using positive localised mutual information The geometric mean of x and y is if I(x, y) > 0 P (x,y) where I(x, y) = log P (x).P (y) Vector composition operation. We consider each of the following seven alternatives: pointwise addition (add), pointwise multiplication (mult), pointwise geometric mean2 (gm), pointwise maximum (max), pointwise minimum (min), first argument (hd), second argument (dp). The latter two operations simply return the first (respect"
W14-1502,W04-0308,0,0.0471896,"bute to counts in the constituent vectors for small and child, producing counts for the features amod:nsubj:cry and nsubj:cry, in their respective vectors. To see why these counts should not be included when building the constituent vectors that we compose to produce inferred vectors for the adjective-noun phrase small child, consider the case where all of the evidence for small things being things that can cry and children being things Experimental Settings Our corpus is a mid-2011 dump of WikiPedia. This has been part-of-speech tagged, lemmatised and dependency parsed using the Malt Parser (Nivre, 2004). All major grammatical dependency relations involving open class parts of speech (nsubj, dobj, iobj, conj, amod, advmod, nnmod) have been extracted for all POS-tagged and lemmatised nouns and adjectives occurring 100 or more times. In past work with conventional dependency relation vectors we found that using a feature threshold of 100, weighting features with PPMI and a cosine similarity score work well. For experimental purposes, we have taken 14 spanish modern digital scientific heavy strong similar former british classical military free common short previous subsequent african female medi"
W14-1502,J07-2002,0,0.923008,"igher-Order Dependency Vectors Julie Weeds, David Weir and Jeremy Reffin Department of Informatics University of Sussex Brighton, BN1 9QH, UK {J.E.Weeds, D.J.Weir, J.P.Reffin}@sussex.ac.uk Abstract Pado and Lapata (2007) took this further by considering not just direct grammatical dependents, but also including indirect dependents. Thus, observing the sentence She filled her glass slowly would provide evidence that the word glass has the distributional feature iobj:advmod:slowly where iobj:advmod captures the indirect dependency relationship between glass and slowly in the sentence. Note that Pado and Lapata (2007) included a basis mapping function that gave their framework flexibility as to how to map paths such as iobj:advmod:slowly onto the basis of the vector space. Indeed, the instantiation of their framework that they adopt in their experiments uses a basis mapping function that removes the dependency path to leave just the word, so iobj:advmod:slowly would be mapped to slowly. In this paper, we are concerned with the problem of distributional semantic composition. We show that the idea that the distributional semantics of a word can be captured with higher-order dependency relationships, provides"
W14-1502,W13-0509,0,0.0138708,"weighting and composition appears to depend on the choice of feature association score. In general, it appears better to weight the features and then compose vectors. This is always true when using PNPMI or PLMI. However, using PPMI, the highest performance is achieved by composing the raw vectors using multiplication and then weighing the Table 4: Adjectives considered 32 of the most frequently occurring adjectives (see Table 4). These adjectives include ones which would generally be considered intersective (e.g., female), subsective (e.g,, long) and nonsubsective/intensional (e.g., former) (Pustejovsky, 2013) . For all of these adjectives there are at least 100 adjective-noun phrases which occur at least 100 times in the corpus. We randomly selected 50 of the phrases for each adjective. Note that our proposed method does not require any hyper parameters to be set during training, nor does it require a certain number of phrases per adjective. For the purpose of these experiments we have a list of 1600 adjective-noun phrases, all of which occur at least 100 times in WikiPedia. 4 Results and Discussion Tables 5 and 6 summarise the average cosines for the proposed higher-order dependency approach and"
W14-1502,D08-1094,0,0.286533,"Missing"
W14-1502,I13-1056,0,0.0597977,"of our approach, and in particular, there are three aspects of the model where alternative solutions are available: the choice of which vector composition operation to use; the choice of how to weight dependency features; and the question as to whether feature weighting should take place before or after composition. ( PPMI(x, y) = ( PLMI(x, y) = p 0 otherwise L(x, y) if L(x, y) > 0 0 otherwise P (x,y) where L(x, y) = P (x, y).log( P (x).P (y) ( PNPMI(x, y) = where N (x, y) = N (x, y) if N (x, y) > 0 0 otherwise P (x,y) 1 .log P (x).P −log(P (y) (y) Table 3: Feature Association Scores (PLMI) (Scheible et al., 2013) and positive normalised point wise mutual information (PNPMI) (Bouma, 2009). For definitions, see Table 3. Timing of feature weighting. We consider two alternatives: we can weight features before composition so that the composition operation is applied to weighted vectors, or we can compose vectors prior to feature weighting, in which case the composition operation is applied to unweighted vectors, and feature weighting is applied in the context of making a similarity calculation. In other work, the former order is often implied. For example, Boleda et al. (2013) state that they use “PMI to w"
W14-1502,J98-1004,0,0.599871,"Missing"
W14-1502,D12-1110,0,0.184535,"attempts to rectify this have offered a more complex, non-commutative function — such as weighted addition — or taken the view that some or all words are no longer simple vectors. For example, in the work of Baroni and Zamparelli (2010) and Guevara (2010), an adjective is viewed as a modifying function and represented by a matrix. Coecke et al. (2011) and Grefenstette et al. (2013) also incorporate the notion of function application from formal semantics. They derived function application from syntactic structure, representing functions as tensors and arguments as vectors. The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke et al. (2011) in using syntax to guide the order of phrasal composition. These higher order structures are typically learnt or induced using a supervised machine learning technique. For example, Baroni and Zamparelli (2010) 3 These are referred to as second-order vectors using the terminology of Grefenstette (1994) and Sch¨utze (1998). However, this refers to a second-order affinity between the words and is"
W14-1502,W13-0112,0,0.0728739,"heoretical limitations in their treatment of composition. How can a commutative function such as multiplication or addition provide different interpretations for different word orderings such as window glass and glass window? The majority of attempts to rectify this have offered a more complex, non-commutative function — such as weighted addition — or taken the view that some or all words are no longer simple vectors. For example, in the work of Baroni and Zamparelli (2010) and Guevara (2010), an adjective is viewed as a modifying function and represented by a matrix. Coecke et al. (2011) and Grefenstette et al. (2013) also incorporate the notion of function application from formal semantics. They derived function application from syntactic structure, representing functions as tensors and arguments as vectors. The MV-RNN model of Socher et al. (2012) broadened the Baroni and Zamparelli (2010) approach; all words, regardless of part-of-speech, were modelled with both a vector and a matrix. This approach also shared features with Coecke et al. (2011) in using syntax to guide the order of phrasal composition. These higher order structures are typically learnt or induced using a supervised machine learning tech"
W14-1502,W09-2506,0,0.0802977,"oun phrases. As a consequence of the computational expense of the machine learning techniques involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which encodes its lexical meani"
W14-1502,W10-2805,0,0.354874,":fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy, where an evaluation against a gold standard such as WordNet (Fellbaum, 1998) can be made (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). 11 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11–20, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Zamparelli (2010) and Guevara (2010). Typically, compositional distributional semantic models can be used to generate an (inferred) distributional vector for a phrase from the (observed) distributional vectors of the phrase’s constituents. One of the motivations for doing this is that the observed distributional vectors for most phrases tend to be very sparse, a consequence of the frequency with which typical phrases occur in even large corpora. However, there are phrases that occur sufficiently frequently that a reasonable characterisation of their meaning can be captured with their observed distributional vector. Such phrases"
W14-1502,I11-1127,0,0.25606,"tational expense of the machine learning techniques involved, implementations of these approaches typically require a considerable amount of dimensionality reduction. A long-standing topic in distributional semantics has been the modification of a canonical representation of a lexeme’s meaning to reflect the context in which it is found. Typically, a canonical vector for a lexeme is estimated from all corpus occurrences and the vector then modified to reflect the instance context (Lund and Burgess, 1996; Erk and Pad´o, 2008; Mitchell and Lapata, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011; Van de Cruys et al., 2011; Erk, 2012). As described in Mitchell and Lapata (2008, 2010), lexeme vectors have typically been modified using simple additive and multiplicative compositional functions. Other approaches, however, share with our proposal the use of syntax to drive modification of the distributional representation (Erk and Pad´o, 2008; Thater et al., 2009; Thater et al., 2010; Thater et al., 2011). For example, in the SVS representation of Erk and Pad´o (2008), a word was represented by a set of vectors: one which encodes its lexical meaning in terms of distributionally similar wo"
W14-1502,D11-1094,0,0.103437,"Missing"
W14-1502,W03-1011,1,0.833204,"nal features of a word are based on the word’s grammatical dependents as found in a dependency parsed corpus. For example, observing that the word glass appears as the indirect object of the verb fill, provides evidence that the word glass has the distributional feature iobj:fill, where iobj denotes the inverse indirect object grammatical relation. The use of grammatical dependents as word features has been exploited in the discovery of tight semantic relations, such as synonymy and hypernymy, where an evaluation against a gold standard such as WordNet (Fellbaum, 1998) can be made (Lin, 1998; Weeds and Weir, 2003; Curran, 2004). 11 Proceedings of the 2nd Workshop on Continuous Vector Space Models and their Compositionality (CVSC) @ EACL 2014, pages 11–20, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics Zamparelli (2010) and Guevara (2010). Typically, compositional distributional semantic models can be used to generate an (inferred) distributional vector for a phrase from the (observed) distributional vectors of the phrase’s constituents. One of the motivations for doing this is that the observed distributional vectors for most phrases tend to be very sparse, a co"
W14-1502,J90-1003,0,\N,Missing
W14-1502,P10-1097,0,\N,Missing
W14-1502,C98-2122,0,\N,Missing
W15-2906,N12-1066,0,0.0147495,"sed learning algorithm with several simple modifications. On the other side, we compare the adjusted algorithms with various other semisupervised learning algorithms that aim to leverage the information in the unlabelled data in a different way. We furthermore examine whether we can improve the classifier by extending its language model to include bigrams and trigrams. data. DUALIST introduced the framework which enables non-technical analysts to design bespoke classifiers by labelling documents and features through active learning, with only a few minutes of annotation effort (Settles, 2011; Settles and Zhu, 2012). Wibberley et al. (2013) and Wibberley et al. (2014) showed that the DUALIST architecture can successfully be used for performing ad-hoc analyses in an agile manner. The remainder of this paper is organised as follows: in Section 2 we more generally introduce agile social media analysis, followed by the description of the datasets we use in our empirical evaluation in Section 3. Section 4 describes our approach alongside related work and Section 5 presents our experiments and discusses our findings. In Section 6 we give an overview of future work and we conclude this paper in Section 7. 2 Agi"
W15-2906,D11-1136,0,0.363182,"ent semisupervised learning algorithm with several simple modifications. On the other side, we compare the adjusted algorithms with various other semisupervised learning algorithms that aim to leverage the information in the unlabelled data in a different way. We furthermore examine whether we can improve the classifier by extending its language model to include bigrams and trigrams. data. DUALIST introduced the framework which enables non-technical analysts to design bespoke classifiers by labelling documents and features through active learning, with only a few minutes of annotation effort (Settles, 2011; Settles and Zhu, 2012). Wibberley et al. (2013) and Wibberley et al. (2014) showed that the DUALIST architecture can successfully be used for performing ad-hoc analyses in an agile manner. The remainder of this paper is organised as follows: in Section 2 we more generally introduce agile social media analysis, followed by the description of the datasets we use in our empirical evaluation in Section 3. Section 4 describes our approach alongside related work and Section 5 presents our experiments and discusses our findings. In Section 6 we give an overview of future work and we conclude this p"
W15-2906,P13-1034,0,0.0672129,"Missing"
W15-2906,P13-2005,0,0.0150202,"learning algorithms in conjunction with a Na¨ıve Bayes model, and show how these modifications can improve the performance of bespoke classifiers for a variety of tasks on a large range of datasets. 1 Introduction Natural Language Processing (NLP) on large social media datasets has emerged as a popular theme in the academic NLP community with publications ranging from predicting elections, e.g. (Tumasjan et al., 2010; Marchetti-Bowick and Chambers, 2012), to forecasting box-office revenues for movies, e.g. (Asur and Huberman, 2010) and anticipating the stock market, e.g. (Bollen et al., 2011; Si et al., 2013). More recently, Opinion Mining and Sentiment Analysis on large social media datasets have received an increasing amount of attention outside academia, where a growing number of businesses and public institutions seek to gain insight into public opinion. For example, companies are primarily interested in what is being said about their brand and products, while public organisations are more concerned with analysing reactions to recent events, or with capturing the general political and societal Zeitgeist. The social network Twitter has been a popular target for such analyses as the vast majorit"
W15-2906,P11-1015,0,0.00844194,"essed tweets matching an initial keyword query while others have already been processed by one or more preceding steps in the pipeline. For example, the shell and flood-1 datasets are the result of querying the Twitter API, whereas the duggan-1 dataset has already been cleared of irrelevant tweets, and tweets only containing news links, in two separate preceeding stages of the processing pipeline. We furthermore evaluate our implementations on 2 commonly used NLP benchmark datasets, 20 Newsgroups (Lang, 1995), henceforth “20news”, as an example Topic Classification dataset, and Movie Reviews (Maas et al., 2011), henceforth “reviews”, as an example Sentiment Analysis dataset. Table 1 highlights the extreme imbalance between labelled and unlabelled data and the corresponding differences in vocabulary size. In the Twitter datasets, |VL |is usually one order of magnitude smaller than |VL∪ U |. In comparison, the disparity in vocabulary size between labelled and unlabelled data in the reviews corpus is less than a factor of two. The difference is more extreme when looking at the actual amounts of labelled and unlabelled data, where the Twitter datasets often contain two orders of magnitude more unlabelle"
W15-2906,E12-1062,0,0.0162445,"he analysis of specific datasets. In this study we investigate how the DUALIST architecture can be optimised for agile social media analysis. We evaluate several semi-supervised learning algorithms in conjunction with a Na¨ıve Bayes model, and show how these modifications can improve the performance of bespoke classifiers for a variety of tasks on a large range of datasets. 1 Introduction Natural Language Processing (NLP) on large social media datasets has emerged as a popular theme in the academic NLP community with publications ranging from predicting elections, e.g. (Tumasjan et al., 2010; Marchetti-Bowick and Chambers, 2012), to forecasting box-office revenues for movies, e.g. (Asur and Huberman, 2010) and anticipating the stock market, e.g. (Bollen et al., 2011; Si et al., 2013). More recently, Opinion Mining and Sentiment Analysis on large social media datasets have received an increasing amount of attention outside academia, where a growing number of businesses and public institutions seek to gain insight into public opinion. For example, companies are primarily interested in what is being said about their brand and products, while public organisations are more concerned with analysing reactions to recent even"
W15-2906,D14-1194,0,0.0141999,"improvement in these algorithms. Especially EM-PWF — binary MNB LT is perhaps scaling down the contributions of the unlabelled data too aggressively. This comes at the expense of not leveraging the full potential of the unlabelled documents, but has the advantage of improved stability across varying amounts of unlabelled data as our experiments show. 5.4 mation. The phenomenon is not exclusive to Sentiment Analysis, hashtag expressions frequently categorise a tweet, e.g. “#ArcticOil” in the shell dataset. Such topical information has already been leveraged in a number of previous works, e.g. Weston et al. (2014); Dela Rosa et al. (2011). Therefore, we hypothesise that the potential benefits of bigrams or trigrams cannot be leveraged as effectively for Twitter Sentiment Analysis datasets than for other datasets. 6 Future Work Our results created a multitude of directions for future research. We plan to investigate the reason behind the inconsistent performance of the semi-supervised learning algorithms across our datasets. We are interested whether it is specific dataset characteristics or particular hyperparameter configurations that cause e.g. EM-PWF — BNB LT to be the top performing algorithm on th"
W15-2906,W13-2705,1,0.817228,"ith several simple modifications. On the other side, we compare the adjusted algorithms with various other semisupervised learning algorithms that aim to leverage the information in the unlabelled data in a different way. We furthermore examine whether we can improve the classifier by extending its language model to include bigrams and trigrams. data. DUALIST introduced the framework which enables non-technical analysts to design bespoke classifiers by labelling documents and features through active learning, with only a few minutes of annotation effort (Settles, 2011; Settles and Zhu, 2012). Wibberley et al. (2013) and Wibberley et al. (2014) showed that the DUALIST architecture can successfully be used for performing ad-hoc analyses in an agile manner. The remainder of this paper is organised as follows: in Section 2 we more generally introduce agile social media analysis, followed by the description of the datasets we use in our empirical evaluation in Section 3. Section 4 describes our approach alongside related work and Section 5 presents our experiments and discusses our findings. In Section 6 we give an overview of future work and we conclude this paper in Section 7. 2 Agile Social Media Analysis"
W15-2906,C14-2025,1,0.839972,"ions. On the other side, we compare the adjusted algorithms with various other semisupervised learning algorithms that aim to leverage the information in the unlabelled data in a different way. We furthermore examine whether we can improve the classifier by extending its language model to include bigrams and trigrams. data. DUALIST introduced the framework which enables non-technical analysts to design bespoke classifiers by labelling documents and features through active learning, with only a few minutes of annotation effort (Settles, 2011; Settles and Zhu, 2012). Wibberley et al. (2013) and Wibberley et al. (2014) showed that the DUALIST architecture can successfully be used for performing ad-hoc analyses in an agile manner. The remainder of this paper is organised as follows: in Section 2 we more generally introduce agile social media analysis, followed by the description of the datasets we use in our empirical evaluation in Section 3. Section 4 describes our approach alongside related work and Section 5 presents our experiments and discusses our findings. In Section 6 we give an overview of future work and we conclude this paper in Section 7. 2 Agile Social Media Analysis When beginning an analysis t"
W15-2906,P12-2018,0,\N,Missing
W16-2502,D14-1079,0,0.0300335,"valua10 Fei Huang and Alexander Yates. 2009. Distributional representations for handling sparsity in supervised sequence-labeling. Proceedings of ACL-IJCNLP pages 495–503. a gold standard. We argue that word similarity still has a place in NLP, but researchers should be aware of its limitations. We view the task as a computationally efficient approximate measure of model quality, which can be useful in the early stage of model development. However, research should place less emphasis on word similarity performance and more on extrinsic results such as (Batchkarov, 2015; Huang and Yates, 2009; Milajevs et al., 2014; Schnabel et al., 2015; Turian et al., 2010; Weston et al., 2015). Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. Proceedings of ACL page 1367. Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review 104(2):211. Acknowledgements Minh-Thang Luong, Richard Socher, and Christopher Manning. 2013. Better word representations with recursive neural networks for morphology. Proceedings of CoNLL 104. This work was funded by UK EPSRC project EP/I"
W16-2502,J15-4004,0,0.722331,"rocedures (intrinsic evaluations) attempt to directly measure the “inherent” quality of a word representation. This often takes the form of computing the extent to which a model agrees with human-provided word or phrase similarity scores. This paper highlights the theoretical and practical issues with the word similarity task, which make it a poor measure of the quality of a distributional model. We investigate five commonly used word similarity datasets, RG (Rubenstein and Goodenough, 1965), MC (Miller and Charles, 1991), WS353 (Finkelstein et al., 2001), MEN (Bruni et al., 2014) and SimLex (Hill et al., 2015). Our contributions are as follows. We argue that the notion of lexical similarity is difficult to define outside of the context of a task and Definition of Similarity The notion of similarity is challenging to define precisely. Existing word similarity data sets typically contain a broad range of semantic relations such as synonymy, antonymy, hypernymy, co-hypernymy, meronymy and topical relatedness. Earlier word similarity work such as WS353 does not attempt to differentiate between those. In contrast, more recent work such as MEN and SimLex distinguishes between “similarity” and “relatednes"
W16-2502,P10-1122,0,0.00661554,"Section 2.3)). Mean agreement ranges from κ = 0.21 to κ = 0.62. For comparison, Kim and Hovy (2004) report κ = 0.91 for a binary sentiment task. Gamon et al. (2005) report a κ of 0.7–0.8 for a threeway sentiment task. Wilson et al. (2005) report κ = 0.72 for a four-class short expressions sentiment task, rising to κ = 0.84 if phrases marked as “unsure” are removed. McCormick et al. (2008) report κ = 0.84 for a five-way text classification task. Stolcke et al. (2000) report κ = 0.8 for a 42-label dialogue act tagging task. Toledo et al. (2012) achieve κ = 0.7 for a textual entailment task, and Sammons et al. (2010) report κ = 0.8 to κ = 1 for a domain identification task. All these κ scores are considerably higher than those achieved by WS353 and MEN. 4 WS353 was annotated on a ten-point scale, whereas MEN used a seven-point scale. 5 Averaging is only needed for WS353, which has been annotated by (at least) 13 judges. MEN only provides full annotations for two judges. 9 tion framework should be able to detect the difference between the original and corrupted models. Model performance, as measured by the evaluation method, should be a monotonically decreasing function of the amount of noise added. In the"
W16-2502,P12-1092,0,0.00803074,"hould be used. We experiment with values between 2 and 5. 0.5 0.4 0.3 0.2 0.1 2 3 Bins 4 5 Figure 2: Inter-annotator agreement of WS353, measured in Cohen’s κ. Shaded region shows the mean and one standard deviation around it. A standard deviation is not shown for MEN as only the annotation of a single pair of raters are available. 4 Size of data set Another issue with existing word similarity data sets is their small size. This ranges from 30 to 3000 data points (Miller and Charles, 1991; Rubenstein and Goodenough, 1965; Landauer and Dumais, 1997; Finkelstein et al., 2001; Hill et al., 2015; Huang et al., 2012; Luong et al., 2013; Bruni et al., 2014). Moreover, they only feature a “tidy” subset of all naturally occurring words, free of spelling variation, domain-specific terminology and named entities. The focus is predominantly on relatively high-frequency words, so the quality of the model cannot be quantified fully. In contrast, typical distributional models “in the wild” have a vocabulary of tens or hundreds of thousands of types. For practical applications, users need to understand the entire distributional model, not just the small fraction of it covered by an intrinsic evaluation. A side eff"
W16-2502,D15-1036,0,0.421626,"re not inherently similar or dissimilar. For example, “good acting” and “cluttered set” are highly dissimilar in terms of the sentiment they express towards a theatrical play. However, they are very similar in the context of detecting news items related to the theatre, as both phrases are highly indicative of theatre-related content. It is often unclear what kind of similarity is useful for a downstream problem in advance. Indeed, it has been shown that being able to learn the notion defined by a particular word similarity task does not necessarily translate to superior extrinsic performance (Schnabel et al., 2015). This argument parallels that of von Luxburg et al. (2012, p 2), who argue that “[d]epending on the use to which a clustering is to be put, the same clustering can either be helpful or useless”. The quality of an unsupervised algorithm can therefore only be assessed in the context of an application. pearsonr = 0.69; p = 0 7 Judge 2 6 5 4 3 2 1 0 0 1 2 3 4 Judge 1 5 6 7 8 (a) MEN pearsonr = 0.77; p = 0 10 8 All judges 3 8 Subjectivity and task difficultly 6 4 2 0 When human judges annotate word pairs for similarity, the distinctions in meaning they are asked to make are often very subtle, espe"
W16-2502,J00-3003,0,0.017032,"Missing"
W16-2502,P10-1040,0,0.00661461,"istributional representations for handling sparsity in supervised sequence-labeling. Proceedings of ACL-IJCNLP pages 495–503. a gold standard. We argue that word similarity still has a place in NLP, but researchers should be aware of its limitations. We view the task as a computationally efficient approximate measure of model quality, which can be useful in the early stage of model development. However, research should place less emphasis on word similarity performance and more on extrinsic results such as (Batchkarov, 2015; Huang and Yates, 2009; Milajevs et al., 2014; Schnabel et al., 2015; Turian et al., 2010; Weston et al., 2015). Soo-Min Kim and Eduard Hovy. 2004. Determining the sentiment of opinions. Proceedings of ACL page 1367. Thomas Landauer and Susan Dumais. 1997. A solution to Plato’s problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological Review 104(2):211. Acknowledgements Minh-Thang Luong, Richard Socher, and Christopher Manning. 2013. Better word representations with recursive neural networks for morphology. Proceedings of CoNLL 104. This work was funded by UK EPSRC project EP/IO37458/1 “A Unified Model of Compositional a"
W16-2502,H05-1044,0,0.00417593,"nter-annotator agreement of WS353 and MEN (converted to Cohen’s κ) is shown in Figure 2. Because κ is only applicable when there are exactly two annotators, we report an average κ over all pairwise comparisons5 . A κ score can be computed between each of the 91 pairs of judges (“WS353-P”), or between each judge and the mean across all judges (“WS353-M”) (as in Hill et al. (2015, Section 2.3)). Mean agreement ranges from κ = 0.21 to κ = 0.62. For comparison, Kim and Hovy (2004) report κ = 0.91 for a binary sentiment task. Gamon et al. (2005) report a κ of 0.7–0.8 for a threeway sentiment task. Wilson et al. (2005) report κ = 0.72 for a four-class short expressions sentiment task, rising to κ = 0.84 if phrases marked as “unsure” are removed. McCormick et al. (2008) report κ = 0.84 for a five-way text classification task. Stolcke et al. (2000) report κ = 0.8 for a 42-label dialogue act tagging task. Toledo et al. (2012) achieve κ = 0.7 for a textual entailment task, and Sammons et al. (2010) report κ = 0.8 to κ = 1 for a domain identification task. All these κ scores are considerably higher than those achieved by WS353 and MEN. 4 WS353 was annotated on a ten-point scale, whereas MEN used a seven-point sc"
W16-2502,P09-1056,0,\N,Missing
W16-2502,C04-1200,0,\N,Missing
W16-2502,W13-3512,0,\N,Missing
W17-1910,S07-1002,0,0.0402809,"ences of polysemous instances with the inferred inventory. Separating the sense discrimination task from the problem of sense induction has the advantage of making our task applicable to evaluating compositional distributional semantic models in order to test their ability to appropriately contextualise a polysemous lexeme. Due to not requiring any models to perform an extra step for sense induction, our task is easier to evaluate as no matching between sense clusters identified by a model and some gold standard sense classes needs to be performed, as typically proposed in the WSI literature (Agirre and Soroa, 2007; Manandhar et al., 2010). Most closely related to our task are the Stanford Contextual Word Similarity (SCWS) dataset by Huang et al. (2012) and the Usage Similarity (USim) task by Erk et al. (2009). The goal in both tasks is to estimate the similarity of two polysemous words in context in comparison to human provided gold standard judgements. In the SCWS dataset typically two different lexemes are considered whereas in USim and our task the same lexemes with different contexts are compared. Instead of relying on crowd-sourced human gold-standard similarity judgements, which can be prone to a"
W17-1910,S16-1081,0,0.0316936,"efinition (Kartsaklis et al., 2012; Polajnar and Clark, 2014). These datasets are commonly set up as retrieval tasks, but generally do not test the ability of a model to disambiguate a polysemous word in context, or discriminate multiple definitions of the same word. Our task also provides a novel evaluation for compositional distributional semantic models, where the predominant strategy is to estimate the similarity of two short phrases (Bernardi et al., 2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human provided gold-standard judgements. One problem with these similarity tasks is that the similarity 11 For example the average standard deviation of human ratings in the SCWS dataset is ≈3 on a 10-point scale, and 86 Phrase river bank bank account dry weather dry clothes capital city capital asset power plant garden plant window bar sandwich bar gasoline tank armored tank desert rock rock band word2vec neighbours bank, river, creek, lake, rivers account, bank, accounts, banks, citibank weather, dry, wet weather, wet, unreasonably"
W17-1910,P09-1002,0,0.0310762,"itional distributional semantic models in order to test their ability to appropriately contextualise a polysemous lexeme. Due to not requiring any models to perform an extra step for sense induction, our task is easier to evaluate as no matching between sense clusters identified by a model and some gold standard sense classes needs to be performed, as typically proposed in the WSI literature (Agirre and Soroa, 2007; Manandhar et al., 2010). Most closely related to our task are the Stanford Contextual Word Similarity (SCWS) dataset by Huang et al. (2012) and the Usage Similarity (USim) task by Erk et al. (2009). The goal in both tasks is to estimate the similarity of two polysemous words in context in comparison to human provided gold standard judgements. In the SCWS dataset typically two different lexemes are considered whereas in USim and our task the same lexemes with different contexts are compared. Instead of relying on crowd-sourced human gold-standard similarity judgements, which can be prone to a considerable amount of noise11 , ing of a lexeme. In all cases in our example the word2vec neighbours reflect the intended sense of the polysemous lexeme, providing evidence for the linear substruct"
W17-1910,D11-1129,0,0.0539529,"Missing"
W17-1910,C14-1151,0,0.0244825,"ional semantic models. By composing the representation of a target lexeme with its surrounding context, it should be possible to determine its sense. For example, composing black smear campaign should lead to a compositional representation that is closer to the composed representation of black hatred than to black mood, black sense of humour or black coffee. This essentially uses the similarity of the compositional representation of a lexeme’s context to determine its sense. Similar approaches to word-sense disambiguation have already been successfully used in past works (Akkaya et al., 2012; Basile et al., 2014). 4.1 of a particular sense for a given polysemous lexeme. • The granularity of the sense inventory reflects common language use7 . • The example sentences are typically free of any domain bias wherever possible. • The data is easily accessible via a web API. By using the data from curated resources we were able to avoid a setup as a sentence similarity task and any potentially noisy crowd-sourced human similarity judgements. We were furthermore able to collect data from varying frequency bands, enabling an assessment of the impact of frequency on any model. Figure 1 shows the number of target"
W17-1910,D14-1163,0,0.0949111,"acobacci et al. (2015), which creates word-sense embeddings by performing word-sense disambiguation prior to running word2vec. We note that word2vec and dep2vec use a single vector per word approach and therefore conflate the different senses of a polysemous lexeme. On the other hand, S ENS E MBED utilises Babelfy (Moro et al., 2014) as an external knowledge source to perform word-sense disambiguation and subsequently creates one vector representation per word sense. For composition we use pointwise addition for all models as this has been shown to be a strong baseline in a number of studies (Hashimoto et al., 2014; Hill et al., 2016). We also experimented with pointwise multiplication as composition function but, similar to Hill et al. (2016), found its performance to be very poor (results not reported). We model any out-of-vocabulary items as a vector consisting of all zeros and determine proximity of composed meaning representations in terms of cosine similarity. We lowercase and lemmatise the data in our task but do not perform number or date normalisation, or removal of rare words. form representations for larger units of meaning. In a compositional phrase, the meaning of the whole can be inferred"
W17-1910,W16-2502,1,0.82305,"t time. Table 7 Results on a subsample of the 2-sense noun subtask across frequency bands. phrases with the appropriate BabelNet sense labels prior to composition. We omitted the BabelNet sense labels in the neighbour list for brevity, however they were consistent with the intended sense in all cases. Table 8 supports the view of composition as a way of contextualising the mean85 or relatedness of two sentences is very difficult to judge — especially on a fine-grained scale — even for humans. This frequently results in a relatively high variance of judgements and low interannotator agreement (Batchkarov et al., 2016). The short phrase datasets typically have a fixed structure that only test a very small fraction of the possible grammatical constructions in which a lexeme can occur, and furthermore provide very little context. The use of full sentences remedies the lack of context and grammatical variation, however can still contain a significant level of noise due to the automatic construction of the dataset or the variance in human ratings. In contrast, our task is not set up as a sentence similarity task and therefore avoids the use of human similarity judgements. Our task is similar to word-sense induc"
W17-1910,Q16-1002,0,0.0278351,"which creates word-sense embeddings by performing word-sense disambiguation prior to running word2vec. We note that word2vec and dep2vec use a single vector per word approach and therefore conflate the different senses of a polysemous lexeme. On the other hand, S ENS E MBED utilises Babelfy (Moro et al., 2014) as an external knowledge source to perform word-sense disambiguation and subsequently creates one vector representation per word sense. For composition we use pointwise addition for all models as this has been shown to be a strong baseline in a number of studies (Hashimoto et al., 2014; Hill et al., 2016). We also experimented with pointwise multiplication as composition function but, similar to Hill et al. (2016), found its performance to be very poor (results not reported). We model any out-of-vocabulary items as a vector consisting of all zeros and determine proximity of composed meaning representations in terms of cosine similarity. We lowercase and lemmatise the data in our task but do not perform number or date normalisation, or removal of rare words. form representations for larger units of meaning. In a compositional phrase, the meaning of the whole can be inferred from the meaning of"
W17-1910,P13-2010,0,0.0232278,"ons have previously been used to evaluate compositional distributional semantic models where the goal is to match a dictionary entry with its corresponding definition (Kartsaklis et al., 2012; Polajnar and Clark, 2014). These datasets are commonly set up as retrieval tasks, but generally do not test the ability of a model to disambiguate a polysemous word in context, or discriminate multiple definitions of the same word. Our task also provides a novel evaluation for compositional distributional semantic models, where the predominant strategy is to estimate the similarity of two short phrases (Bernardi et al., 2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human provided gold-standard judgements. One problem with these similarity tasks is that the similarity 11 For example the average standard deviation of human ratings in the SCWS dataset is ≈3 on a 10-point scale, and 86 Phrase river bank bank account dry weather dry clothes capital city capital asset power plant garden plant window bar sandwich bar gasoline tank armored tank deser"
W17-1910,P12-1092,0,0.668938,"the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in context; e.g, word sense disambiguation (Iacobacci et al., 2016) and word similarity in context (Iacobacci et al., 2015). In this paper we consider what happens when distributional representations are composed to In this paper, we investig"
W17-1910,D14-1110,0,0.018006,"ion of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in context; e.g, word sense disambiguation (Iacobacci et al., 2016) and word similarity in context ("
W17-1910,P15-1010,0,0.312615,"show that appropriate sense selection or disambiguation typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in cont"
W17-1910,D10-1113,0,0.0255637,"me the issue of conflating several different senses of an individual lexeme into a single representation. One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try directly inferring a predefined number of senses from data and subsequently label any occurrences of a polysemous lexeme with the inferred inventory. Similar approaches are proposed by Reddy et al. (2011) and Kartsaklis et al. (2013) who show that appropriate sense selection or disambiguation typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on"
W17-1910,P16-1085,0,0.0221049,"btaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in context; e.g, word sense disambiguation (Iacobacci et al., 2016) and word similarity in context (Iacobacci et al., 2015). In this paper we consider what happens when distributional representations are composed to In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated through composition alone. We evaluate the performance of off-the-shelf singlevector and multi-sense vector models on a benchmark phrase similarity task and a novel task for word-sense discrimination. We find that single-sense vector models perform as well or better than multi-sense"
W17-1910,S12-1089,0,0.0245264,"xemes have fewer senses and potentially less subtle sense differences than more frequent lexemes, which would make them easier to discriminate by distributional semantic methods. Results Table 5 shows the results for all context window sizes across all parts-of-speech and number of senses. All models substantially outperform the random baseline for any number of senses. Interestingly the word overlap baseline is competitive for all context window sizes. While it is a very simple method, it has already been found to be a strong baseline for paraphrase detection and semantic textual similarity (Dinu and Thater, 2012). One possible explanation for its robust performance on our task is an occurrence of the one-sense-per-collocation hypothesis (Yarowsky, 1993). The performance of all other models is roughly in the same ballpark for all parts-ofspeech and number of senses, suggesting that they form robust baselines for future models. While the results are relatively mixed for adjectives, word2vec appears to be the strongest model for polysemous nouns and verbs. The perhaps most interesting observation in Table 5 is that word2vec and dep2vec are performing as well or better than S ENS E MBED despite the fact t"
W17-1910,W13-3513,0,0.019728,"AG laboratory, Department of Informatics, University of Sussex Brighton, BN1 9RH, UK {t.kober, j.e.weeds, jw478, j.p.reffin, d.j.weir}@sussex.ac.uk Abstract Therefore a number of proposals have been made to overcome the issue of conflating several different senses of an individual lexeme into a single representation. One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try directly inferring a predefined number of senses from data and subsequently label any occurrences of a polysemous lexeme with the inferred inventory. Similar approaches are proposed by Reddy et al. (2011) and Kartsaklis et al. (2013) who show that appropriate sense selection or disambiguation typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representati"
W17-1910,H93-1061,0,0.298731,"is in the middle band, there is a considerable amount of less frequent lexemes. The most frequent target lexeme in our task is the verb be with ≈1.8m occurrences in Wikipedia, and the least frequent lexeme is the verb ruffle with only 57 occurrences. The average target lexeme frequency is ≈95k for adjectives, and ≈45k−46k for nouns and verbs9 . Task Construction For the construction of our dataset we made use of data from two english dictionaries (Oxford Dictionary and Collins Dictionary), accessible via their respective web APIs6 , as well as examples from the sense annotated corpus SemCor (Miller et al., 1993). Our use of dictionary data is motivated by a number of favourable properties which make it a very suitable data source for our proposed task: Figure 1: Binned frequency distribution of the polysemous target lexemes in our task. • The content is of very high-quality and curated by expert lexicographers. 7 The Oxford dictionary lists 5 different senses for the noun “bank”, whereas WordNet 3.0 lists 10 synsets, for example distinguishing “bank” as the concept for a financial institution and “bank” as a reference to the building where financial transactions take place. 8 We removed any articles"
W17-1910,P08-1028,0,0.0609194,"ere the goal is to match a dictionary entry with its corresponding definition (Kartsaklis et al., 2012; Polajnar and Clark, 2014). These datasets are commonly set up as retrieval tasks, but generally do not test the ability of a model to disambiguate a polysemous word in context, or discriminate multiple definitions of the same word. Our task also provides a novel evaluation for compositional distributional semantic models, where the predominant strategy is to estimate the similarity of two short phrases (Bernardi et al., 2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human provided gold-standard judgements. One problem with these similarity tasks is that the similarity 11 For example the average standard deviation of human ratings in the SCWS dataset is ≈3 on a 10-point scale, and 86 Phrase river bank bank account dry weather dry clothes capital city capital asset power plant garden plant window bar sandwich bar gasoline tank armored tank desert rock rock band word2vec neighbours bank, river, creek, lake, rivers account, bank, accounts"
W17-1910,D16-1175,1,0.852999,"ecent years, it is an important natural language understanding problem and can provide important insights into the process of semantic composition. 2 3 Phrase Similarity Our first evaluation task is the benchmark phrase similarity task of Mitchell and Lapata (2010). This dataset consists of 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object (VO) pairs. The task is to compare a compositional model’s similarity estimates with human judgements by computing Spearman’s ρ. An average ρ of 0.470.48 represents the current state-of-the-art performance on this task (Hashimoto et al., 2014; Kober et al., 2016; Wieting et al., 2015). For single-sense representations, the strategy for carrying out this task is simple. For each phrase in each pair, we compose the constituent representations and then compute the similarity of each pair of phrases using the cosine similarity. For multi-sense representations, we adapted the strategy which has been used successfully in various word similarity experiments (Huang et al., 2012; Iacobacci et al., 2015). Typically, for each word pair, all pairs of senses are considered and the similarity of the word pair is considered to be Evaluating Distributional Models of"
W17-1910,P14-2050,0,0.0428618,"been used successfully in various word similarity experiments (Huang et al., 2012; Iacobacci et al., 2015). Typically, for each word pair, all pairs of senses are considered and the similarity of the word pair is considered to be Evaluating Distributional Models of Composition For evaluation we use several readily available off-the-shelf word embeddings, that have already been shown to work well for a number of different NLP applications. We compare the 300-dimensional skip-gram word2vec (Mikolov et al., 2013) word embeddings2 to the dependency based version of word2vec — henceforth dep2vec3 (Levy and Goldberg, 2014) — and the 1 Our task is available from https://github.com/ tttthomasssss/sense2017 2 Available from: https://code.google.com/p/ word2vec/ 3 Available from: https://levyomer. wordpress.com/2014/04/25/ dependency-based-word-embeddings/ 4 Available from: sensembed/ 80 http://lcl.uniroma1.it/ Phrase 1 buy land close eye wave hand drink water european state high point the similarity of the closest pair of senses. The fact that this strategy works well suggests that when humans are asked to judge word similarity, the pairing automatically primes them to select the closest senses. Extending this to"
W17-1910,W13-3210,0,0.0274129,"Missing"
W17-1910,D15-1200,0,0.0202724,"formance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on benchmark word similarity tasks (Huang et al., 2012; Iacobacci et al., 2015) and some downstream processing tasks such as part-of-speech tagging and relation identification (Li and Jurafsky, 2015), they have been weaker than the single-vector representations at predicting the compositionality of multi-word expressions (Salehi et al., 2015), and at tasks which require the meaning of a word to be considered in context; e.g, word sense disambiguation (Iacobacci et al., 2016) and word similarity in context (Iacobacci et al., 2015). In this paper we consider what happens when distributional representations are composed to In this paper, we investigate whether an a priori disambiguation of word senses is strictly necessary or whether the meaning of a word in context can be disambiguated thro"
W17-1910,Q14-1019,0,0.0378263,"of all observed meanings of a lexeme in a given text corpus. 79 Proceedings of the 1st Workshop on Sense, Concept and Entity Representations and their Applications, pages 79–90, c Valencia, Spain, April 4 2017. 2017 Association for Computational Linguistics S ENS E MBED model4 by Iacobacci et al. (2015), which creates word-sense embeddings by performing word-sense disambiguation prior to running word2vec. We note that word2vec and dep2vec use a single vector per word approach and therefore conflate the different senses of a polysemous lexeme. On the other hand, S ENS E MBED utilises Babelfy (Moro et al., 2014) as an external knowledge source to perform word-sense disambiguation and subsequently creates one vector representation per word sense. For composition we use pointwise addition for all models as this has been shown to be a strong baseline in a number of studies (Hashimoto et al., 2014; Hill et al., 2016). We also experimented with pointwise multiplication as composition function but, similar to Hill et al. (2016), found its performance to be very poor (results not reported). We model any out-of-vocabulary items as a vector consisting of all zeros and determine proximity of composed meaning r"
W17-1910,S10-1011,0,0.0320686,"ances with the inferred inventory. Separating the sense discrimination task from the problem of sense induction has the advantage of making our task applicable to evaluating compositional distributional semantic models in order to test their ability to appropriately contextualise a polysemous lexeme. Due to not requiring any models to perform an extra step for sense induction, our task is easier to evaluate as no matching between sense clusters identified by a model and some gold standard sense classes needs to be performed, as typically proposed in the WSI literature (Agirre and Soroa, 2007; Manandhar et al., 2010). Most closely related to our task are the Stanford Contextual Word Similarity (SCWS) dataset by Huang et al. (2012) and the Usage Similarity (USim) task by Erk et al. (2009). The goal in both tasks is to estimate the similarity of two polysemous words in context in comparison to human provided gold standard judgements. In the SCWS dataset typically two different lexemes are considered whereas in USim and our task the same lexemes with different contexts are compared. Instead of relying on crowd-sourced human gold-standard similarity judgements, which can be prone to a considerable amount of n"
W17-1910,P14-1009,0,0.021371,"ating the similarity between two lexemes in context, but identifying the sentences that use the same sense of a given polysemous lexeme. 7 paradigm, due to its focus on assessing the ability of a model to appropriately contextualise the meaning of a word. Our task furthermore provides another evaluation option away from intrinsic evaluations which are based on often noisy human similarity judgements, while also not being embedded in a downstream task. In future work we aim to extend our evaluation to more complex compositional distributional semantic models such as the lexical function model (Paperno et al., 2014) or the Anchored Packed Dependency Tree framework (Weir et al., 2016). We furthermore want to investigate how far the sense-discriminating ability of composition can be leveraged for other tasks. Conclusion While elementary multi-sense representations of words might capture a more fine grained semantic picture of a polysemous word, that advantage does not appear to transfer to distributional composition in a straightforward way. Our experiments on a popular phrase similarity benchmark and our novel word-sense discrimination task have demonstrated that semantic composition does not appear to be"
W17-1910,marelli-etal-2014-sick,0,0.0208289,"nar and Clark, 2014). These datasets are commonly set up as retrieval tasks, but generally do not test the ability of a model to disambiguate a polysemous word in context, or discriminate multiple definitions of the same word. Our task also provides a novel evaluation for compositional distributional semantic models, where the predominant strategy is to estimate the similarity of two short phrases (Bernardi et al., 2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014) in comparison to human provided gold-standard judgements. One problem with these similarity tasks is that the similarity 11 For example the average standard deviation of human ratings in the SCWS dataset is ≈3 on a 10-point scale, and 86 Phrase river bank bank account dry weather dry clothes capital city capital asset power plant garden plant window bar sandwich bar gasoline tank armored tank desert rock rock band word2vec neighbours bank, river, creek, lake, rivers account, bank, accounts, banks, citibank weather, dry, wet weather, wet, unreasonably warm dry, clothes, clothing, rinse thoroug"
W17-1910,D16-1174,0,0.0447337,"Missing"
W17-1910,S07-1009,0,0.0363294,"by Arora et al. (2016), and suggesting that distributional composition is able to recover sense specific information from a polysemous lexeme. The very fine-grained sense-level vector space of S ENS E MBED is giving rise to a very focused neighbourhood, however there does not seem to be any advantage over word2vec from a qualitative point of view when using simple additive composition. 6 Related Work The perhaps most popular tasks for evaluating the ability of a model to capture or encode the different senses of a polysemous lexeme in a given context are the english lexical substitution task (McCarthy and Navigli, 2007) and the Microsoft sentence completion challenge (Zweig and Burges, 2011). Both tasks require any model to fill an appropriate word into a pre-defined slot in a given sentential context. The sentence completion challenge provides a list of candidate words while the english lexical substitution task does not. However, neither task focuses on polysemy and the english lexical substitution task conflates the problems of discriminating word senses and finding meaning preserving substitutes. Dictionary definitions have previously been used to evaluate compositional distributional semantic models whe"
W17-1910,E14-1025,0,0.0306856,"to fill an appropriate word into a pre-defined slot in a given sentential context. The sentence completion challenge provides a list of candidate words while the english lexical substitution task does not. However, neither task focuses on polysemy and the english lexical substitution task conflates the problems of discriminating word senses and finding meaning preserving substitutes. Dictionary definitions have previously been used to evaluate compositional distributional semantic models where the goal is to match a dictionary entry with its corresponding definition (Kartsaklis et al., 2012; Polajnar and Clark, 2014). These datasets are commonly set up as retrieval tasks, but generally do not test the ability of a model to disambiguate a polysemous word in context, or discriminate multiple definitions of the same word. Our task also provides a novel evaluation for compositional distributional semantic models, where the predominant strategy is to estimate the similarity of two short phrases (Bernardi et al., 2013; Grefenstette and Sadrzadeh, 2011; Kartsaklis and Sadrzadeh, 2014; Mitchell and Lapata, 2008; Mitchell and Lapata, 2010) or sentences (Agirre et al., 2016; Huang et al., 2012; Marelli et al., 2014"
W17-1910,W04-2406,0,0.0736297,"n word-sense discrimination has suffered from the absence of a benchmark task as well as a clear evaluation methodology. For example Sch¨utze (1998) evaluated his model on a dataset consisting of 20 polysemous words (10 naturally ambiguous lexemes and 10 artificially ambiguous “pseudo-lexemes”) in terms of accuracy for coarse grained sense distinctions, and an information retrieval task. Pantel and Lin (2002), and Van de Cruys (2008) used automatically extracted words from various newswire sources and evaluated the output of their models in comparison to WordNet and EuroWordNet, respectively. Purandare and Pedersen (2004) used a subset of the words from the S ENSEVAL -2 task and evaluated their models in terms of precision, recall and F1-score of how well available sense tags match with clusters discovered by their algorithms. Akkaya et al. (2012) used the concatenation of the S ENSEVAL -2 and S ENSEVAL -3 tasks and evaluated their models in terms of cluster purity and accuracy. Finally, Moen et al. (2013) used the semantic textual similarity (STS) 2012 task, which is based on human judgements of the similarity between two sentences. One contribution of our work is a novel wordsense discrimination task, evalua"
W17-1910,I11-1079,0,0.0208808,"Reffin and David Weir TAG laboratory, Department of Informatics, University of Sussex Brighton, BN1 9RH, UK {t.kober, j.e.weeds, jw478, j.p.reffin, d.j.weir}@sussex.ac.uk Abstract Therefore a number of proposals have been made to overcome the issue of conflating several different senses of an individual lexeme into a single representation. One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try directly inferring a predefined number of senses from data and subsequently label any occurrences of a polysemous lexeme with the inferred inventory. Similar approaches are proposed by Reddy et al. (2011) and Kartsaklis et al. (2013) who show that appropriate sense selection or disambiguation typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori"
W17-1910,N10-1013,0,0.0230391,"conflating several different senses of an individual lexeme into a single representation. One approach (Reisinger and Mooney, 2010; Huang et al., 2012) is to try directly inferring a predefined number of senses from data and subsequently label any occurrences of a polysemous lexeme with the inferred inventory. Similar approaches are proposed by Reddy et al. (2011) and Kartsaklis et al. (2013) who show that appropriate sense selection or disambiguation typically improves performance for composition of noun phrases (Reddy et al., 2011) and verb phrases (Kartsaklis et al., 2013). Dinu and Lapata (2010) proposed a model that represents the meaning of a word as a probability distribution over latent senses which is modulated based on contextualisation, and report improved performance on a word similarity task and the lexical substitution task. Other approaches leverage an existing lexical resource such as BabelNet or WordNet to obtain sense labels a priori to creating word representations (Iacobacci et al., 2015), or as a postprocessing step after obtaining initial word representations (Chen et al., 2014; Pilehvar and Collier, 2016). While these approaches have exhibited strong performance on"
W17-1910,N15-1099,0,0.0612783,"Missing"
W17-1910,J98-1004,0,0.599699,"Missing"
W17-1910,C08-1117,0,0.0261266,"Missing"
W17-1910,J16-4006,1,0.846326,"sentences that use the same sense of a given polysemous lexeme. 7 paradigm, due to its focus on assessing the ability of a model to appropriately contextualise the meaning of a word. Our task furthermore provides another evaluation option away from intrinsic evaluations which are based on often noisy human similarity judgements, while also not being embedded in a downstream task. In future work we aim to extend our evaluation to more complex compositional distributional semantic models such as the lexical function model (Paperno et al., 2014) or the Anchored Packed Dependency Tree framework (Weir et al., 2016). We furthermore want to investigate how far the sense-discriminating ability of composition can be leveraged for other tasks. Conclusion While elementary multi-sense representations of words might capture a more fine grained semantic picture of a polysemous word, that advantage does not appear to transfer to distributional composition in a straightforward way. Our experiments on a popular phrase similarity benchmark and our novel word-sense discrimination task have demonstrated that semantic composition does not appear to benefit from a fine grained sense inventory, but that the ability to co"
W17-1910,Q15-1025,0,0.0254211,"n important natural language understanding problem and can provide important insights into the process of semantic composition. 2 3 Phrase Similarity Our first evaluation task is the benchmark phrase similarity task of Mitchell and Lapata (2010). This dataset consists of 108 adjective-noun (AN), 108 noun-noun (NN) and 108 verb-object (VO) pairs. The task is to compare a compositional model’s similarity estimates with human judgements by computing Spearman’s ρ. An average ρ of 0.470.48 represents the current state-of-the-art performance on this task (Hashimoto et al., 2014; Kober et al., 2016; Wieting et al., 2015). For single-sense representations, the strategy for carrying out this task is simple. For each phrase in each pair, we compose the constituent representations and then compute the similarity of each pair of phrases using the cosine similarity. For multi-sense representations, we adapted the strategy which has been used successfully in various word similarity experiments (Huang et al., 2012; Iacobacci et al., 2015). Typically, for each word pair, all pairs of senses are considered and the similarity of the word pair is considered to be Evaluating Distributional Models of Composition For evalua"
W17-1910,H93-1052,0,0.714522,"ributional semantic methods. Results Table 5 shows the results for all context window sizes across all parts-of-speech and number of senses. All models substantially outperform the random baseline for any number of senses. Interestingly the word overlap baseline is competitive for all context window sizes. While it is a very simple method, it has already been found to be a strong baseline for paraphrase detection and semantic textual similarity (Dinu and Thater, 2012). One possible explanation for its robust performance on our task is an occurrence of the one-sense-per-collocation hypothesis (Yarowsky, 1993). The performance of all other models is roughly in the same ballpark for all parts-ofspeech and number of senses, suggesting that they form robust baselines for future models. While the results are relatively mixed for adjectives, word2vec appears to be the strongest model for polysemous nouns and verbs. The perhaps most interesting observation in Table 5 is that word2vec and dep2vec are performing as well or better than S ENS E MBED despite the fact that the former conflate the senses of a polysemous lexeme in a single vector representation. Figure 2 shows the average performance of all mode"
W98-0108,1997.iwpt-1.6,1,0.831515,"Missing"
W98-0108,A94-1009,0,0.1648,"Missing"
W98-0108,J96-2002,0,0.221573,"bring together, and evaluate, a variety of current NLP techniques, including the organisation of grammars into inheritance hierarchies for compact representation, exploitation of diverse precompilation techniques for efficient parsing, and use of statistical analysis to disambiguate parse results. In conjunction with this we are using several existing tools and resources, such as the lexicon developed in the Alvey Natural Language Tools project (Briscoe et al., 1987), lexical frequency information from the SPARKLE project 2 , and an established lexical knowledge representation language DATR (Evans and Gazdar, 1996a) to represent the grammar. The overall architecture of LExSvs is shown in Figure 1 and the following sections discuss each of. the system&apos;s main components. 2 The morphological analyser The text is first tokenised and then a sentencesplitter is applied to it to determine likely sentence boundaries. The resulting sentences are tagged with extended part-of-speech (PoS) labels using a first-order HMM tagger (Elworthy, 1994) trained on the SUSANNE corpus (Sam.pson, 1995). The SUSANNE lexicon is augmented with open-class words from tlie LOB corpus and the tagger incorporates a part-of-speech gues"
W98-0108,P98-1061,1,0.583256,"Missing"
W98-0108,P95-1011,1,0.874552,"Missing"
W98-0108,W97-0808,0,0.0448682,"tly increases its disambiguation accuracy. Ve intend also to incorporate this information into the system &lt;lescribe&lt;l in this paper, at the point where lemmas a.re associated with tree families: each lemma / family combination would have a separate probability. Carroll and Weir ( 1997) ou tline other alternative probabilistic models, some of which we also inten&lt;l to investigate. The same shallow phrase-structure parser is also providing data for the acquisition of selectional preferences, at present again just for verbs, and only for NP and PP subject, direct and indirect verbal complements (McCarthy, 1997). The technique uses the WordNet hypernym hierarchy (Fellbaum, 1998) in tandem with Minimum Description Length learning (Rissanen, 1978) to induce semantic classes of nominal heads at an appropriate level of abstraction. We have results of acquisition from a 10 million word extract from the British National Corpus, and will augment the lex.icon with the acquired selectionai frequencies and use them during parsing as a further source of disambiguation information. 5 The parser We have implemented a simple bottom-up parsing algorithm which is being used for grammar development . The parser simul"
W98-0108,P95-1021,1,0.929691,"ny whose probabilities are below a preset factor of the most probable. The thresholding technique allows us to fine-tune the trade-off between the costs of incorrect tagging and processing complexity due to lexical ambiguity. After tagging, a lemmatiser finds the lemma, or base form, corresponding to each word-label pair, using an enhanced version of the GATE project stemmer (Cunningham et al., 1995). Finally, the lemma and PoS label are combined with syntactic information associated with the word &apos;s morphological form ( e.g. number for nouns). 3 The grammar Lexicalized D-Tree Grammar (LDTG) (Rambow et al., 1995) is a variant of LTAG. The primitive elements of LDTG are called elementary d-trees and are combined together to form larger structures during a derivation. Although, for convenience, we present d-trees graphically as though they were conventional trees, they are more correctly thought of as expressions in a tree description logic (Rogers and VijayShanker, 1992). These expressions partially describe trees by asserting various relationships between nodes: parenthood, domination, precedence (indicating that one node is to the left of anoth~r ), equality and inequality. There are two substitution"
W98-0108,P92-1010,0,0.0638642,"Missing"
W98-0108,W98-0139,1,0.780908,"Missing"
W98-0108,W98-1114,1,\N,Missing
W99-0631,E95-1016,0,0.188611,"oose a h y p e r n y m for each alternative sense, we employ a novel technique which uses a X2 test to measure the homogeneity of sets of concepts in the hierarchy. 1 Introduction Knowledge of the constraints a verb places on the semantic types of its a r g u m e n t s (variously called selectional restrictions, selectional preferences, selectional constraints) is of use in m a n y areas of n a t u r a l language processing, particularly structural disambiguation. Recent t r e a t m e n t s of selectional restrictions have been probabilistic in n a t u r e (Resnik, 1993), (Li and Abe, 1998), (Ribas, 1995), (McCarthy, 1997), and estimation of the relevant probabilities has required corpus-based counts of the n u m b e r of times word senses, or concepts, appear in the different a r g u m e n t positions of verbs. A difficulty arises due to the absence of a large volume of sense d i s a m b i g u a t e d data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more t h a n one sense. The techniques in Resnik (1993), Li and Abe (1998) and Ribas (1995) simply distribute the count equally a m o n g the alternative senses of a noun. A b n e y and Li"
W99-0631,A97-1052,0,0.135345,". Using the previous example, < e n t i t y &gt; is too high to represent either sense of chicken because the children of < e n t i t y &gt; are not all associated in the same way with eat. The set consisting of the children of <meat&gt;, however, is homogeneous with respect to the object position of eat, and so <meat&gt; is not too high a level of representation. The measure of homogeneity we use is detailed in Section 5. 2 The Input Data and Semantic Hierarchy The input data used to estimate frequencies and probabilities over the semantic hierarchy has been obtained from the shallow parser described in Briscoe and Carroll (1997). The data consists of a multiset of 'co-occurrence triples', each triple consisting of a noun lemma, verb lemma, and argument position. We refer to the data as follows: let the universe of verbs, argument positions and nouns that can appear in the input data be denoted = { V l , . . . ,Vkv }, 1Z---- { r l , . . . , r k n } and Af = { n l , . . . , nk~¢ }, respectively. Note that in our treatment of selectional restrictions, we do not attempt to distinguish between alternative senses of verbs. We also assume that each instance of a noun in the data refers to one, and only one, concept. We use"
W99-0631,C92-2070,0,0.0774675,"v and position r, in the following formula we use [c, v, r] to denote the set of concepts top(c, v, r). The re_ ^ rn+l. estimated frequency treq (c, v, r) is given as follows. freq(n, v, r) I cn(n)l nEsyn(c) where freq(n, v, r) is the number times the triple (n,v,r) appears in the data, and [ cn(n)] is the cardinality of an(n). Although this approach can give inaccurate estimates, the counts given to the incorrect senses will disperse randomly throughout the hierarchy as noise, and by accumulating counts up the hierarchy we will tend to gather counts from the correct senses of related words (Yarowsky, 1992; Resnik, 1993). To see why, consider two instances of possible triples in the data, drink wine and drink water. (This example is adapted from Resnik (1993).) The word water is a member of seven synsets in WordNet 1.6, and wine is a member of two synsets. Thus each sense of water will be incremented by 0.14 counts, and each sense of wine will be incremented by 0.5 counts. Now although the incorrect senses of these words will receive counts, those concepts in the hierarchy which dominate more than one of the senses, such as <beverage&gt;, will accumulate more substantial counts. However, although"
W99-0631,J93-1003,0,0.085173,"< e n t i t y &gt; is {entity, something}, and the words entity and something may well appear in the argument positions of verbs in the corpus. Furthermore, for a concept c, we distinguish between the set of words t h a t can be used to denote c (the synset of c), and the set of words t h a t can be used to denote concepts in L 6 3 p(nlc, v, r) Now we have a model for the input data: p(n, v, r) = p(v,r)p(niv ,r) = p(v,r) p(clv, rlp(ntc, v,r) cecn(n) Note t h a t for c ¢ cn(n), p(nlc, v, r) = O. The association norm (and similar measures such as the mutual information score) have been criticised (Dunning, 1993) because these scores can be greatly over-estimated when frequency counts are low. This problem is overcome to some extent in the scheme presented below since, generally speaking, we only calculate the association norms for concepts that have accumulated a significant count. The association norm can be estimated using m a x i m u m likelihood estimates of the probabilities as follows. The Measure of A s s o c i a t i o n We measure the association between argum e n t positions of verbs and sets of concepts using the a s s o c i a t i o n n o r m (Abe and Li, 1996). 7 For C C C, v E V a n d r E"
W99-0631,J98-2002,0,0.160036,"ount. In order to choose a h y p e r n y m for each alternative sense, we employ a novel technique which uses a X2 test to measure the homogeneity of sets of concepts in the hierarchy. 1 Introduction Knowledge of the constraints a verb places on the semantic types of its a r g u m e n t s (variously called selectional restrictions, selectional preferences, selectional constraints) is of use in m a n y areas of n a t u r a l language processing, particularly structural disambiguation. Recent t r e a t m e n t s of selectional restrictions have been probabilistic in n a t u r e (Resnik, 1993), (Li and Abe, 1998), (Ribas, 1995), (McCarthy, 1997), and estimation of the relevant probabilities has required corpus-based counts of the n u m b e r of times word senses, or concepts, appear in the different a r g u m e n t positions of verbs. A difficulty arises due to the absence of a large volume of sense d i s a m b i g u a t e d data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more t h a n one sense. The techniques in Resnik (1993), Li and Abe (1998) and Ribas (1995) simply distribute the count equally a m o n g the alternative senses of a noun. A"
W99-0631,W97-0808,0,0.724897,"r n y m for each alternative sense, we employ a novel technique which uses a X2 test to measure the homogeneity of sets of concepts in the hierarchy. 1 Introduction Knowledge of the constraints a verb places on the semantic types of its a r g u m e n t s (variously called selectional restrictions, selectional preferences, selectional constraints) is of use in m a n y areas of n a t u r a l language processing, particularly structural disambiguation. Recent t r e a t m e n t s of selectional restrictions have been probabilistic in n a t u r e (Resnik, 1993), (Li and Abe, 1998), (Ribas, 1995), (McCarthy, 1997), and estimation of the relevant probabilities has required corpus-based counts of the n u m b e r of times word senses, or concepts, appear in the different a r g u m e n t positions of verbs. A difficulty arises due to the absence of a large volume of sense d i s a m b i g u a t e d data, as the counts have to be estimated from the nouns which appear in the corpus, most of which will have more t h a n one sense. The techniques in Resnik (1993), Li and Abe (1998) and Ribas (1995) simply distribute the count equally a m o n g the alternative senses of a noun. A b n e y and Light (1998) have at"
