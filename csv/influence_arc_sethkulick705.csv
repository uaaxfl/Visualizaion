1995.iwpt-1.27,J94-4005,0,0.122132,"Missing"
1995.iwpt-1.27,A88-1019,0,0.285476,"Missing"
1995.iwpt-1.27,C94-2149,1,0.901602,"Missing"
1995.iwpt-1.27,C94-1024,1,0.916414,"Missing"
1995.iwpt-1.27,P95-1021,0,0.0276083,"Missing"
1995.iwpt-1.27,P95-1037,0,0.135882,"Missing"
1995.iwpt-1.27,J81-4005,0,0.153699,"Missing"
1995.iwpt-1.27,1995.iwpt-1.22,0,0.0644192,"Missing"
1995.iwpt-1.27,C88-2121,0,0.207128,"Missing"
1995.iwpt-1.27,E93-1040,0,0.161212,"Missing"
1995.iwpt-1.27,C69-0101,0,0.710421,"Missing"
1995.iwpt-1.27,H93-1025,0,\N,Missing
1995.iwpt-1.27,C90-3029,0,\N,Missing
2006.bcs-1.4,W02-0504,0,0.339727,"Missing"
2006.bcs-1.4,P05-1071,0,0.0484791,"ve perfective verb), and 33 as PV (perfective verb). As a NOUN, it has the vowels “kutub”, as a PV_PASS it has “kutib”, and as PV it has “katab”. Therefore, in this case the vowelization does not add much beyond the Part of Speech tags. It is possible therefore that the full benefits of vocalization can only be seen in the context of a wider NLP pipeline than just the parser, including in particular the Part-of-Speech tagger. Related to this, Habash (2005) reports a drop in ambiguity when considering tokens only within the same Part of Speech tag. Also notable in the connection is the work of Habash & Rambow (2005), who describe an integrated approach to tokenization, Part of Speech tagging and morphological disambiguation. Of particular interest is the close connection between POS tagging and morphological disambiguation. While this connection is clearly related to the concerns expressed here, they do not include a step of diacritazation. In our view, the categorization of the ambiguities resolved by diacritic restoration discussed in Section 2 deserves detailed empirical analysis based on the data the parser is using. As just discussed, there is a close connection between POS tags and some diacritic r"
2006.bcs-1.4,2006.bcs-1.4,1,0.0530913,"Missing"
2006.bcs-1.4,W05-0711,0,0.546586,"above, used the bare text, there has been very little work examining whether a parser can make use of vocalized text. The Arabic Treebank now gives us a corpus to carry out such experiments, as we discuss in the following subsection. However, in addition to exploring which diacritic information is useful for the parser, we must also be concerned with what might be available to the parser outside the context of these experiments and outside the context of Treebank research. As discussed in Section 3, there is a growing body of work on diacritic restoration. However, Zitouni, et al. (2006) and Nelken & Shieber (2005) both report that it is much harder to restore the diacritics representing case information. This is not surprising, since as Nelken & Shieber write, “including case information naturally yields proportionally worse accuracy. Since case markings encode higher-order grammatical information, they would require a more powerful grammatical model than offered by finite state methods”. While noting these concerns and issues, for the experiments reported here we continue to train and test the data as it is tokenized following the stage of POS annotation, and we continue to use the gold tags, while va"
2006.bcs-1.4,W04-1612,0,0.663917,"of Pennsylvania, USA {maamouri,bies,skulick}@ldc.upenn.edu Arabic diacritization (referred to sometimes as vocalization or vowelling), defined as the full or partial representation of short vowels, shadda (consonantal length or germination), tanween (nunation or definiteness), and hamza (the glottal stop and its support letters), is still largely understudied in the current NLP literature. In this paper, the lack of diacritics in standard Arabic texts is presented as a major challenge to most Arabic natural language processing tasks, including parsing. Recent studies (Messaoudi, et al. 2004; Vergyri & Kirchhoff 2004; Zitouni, et al. 2006 and Maamouri, et al. forthcoming) about the place and impact of diacritization in text-based NLP research are presented along with an analysis of the weight of the missing diacritics on Treebank morphological and syntactic analyses and the impact on parser development. Keywords: Arabic NLP, Arabic diacritics, Diacritization, Modern Standard Arabic (MSA), Treebanks, Linguistic Annotation, Parsing 1 INTRODUCTION Arabic NLP research, focusing mainly on Modern Standard Arabic (MSA) in this paper, faces two major challenges, not necessarily shared with many other natural lang"
2006.bcs-1.4,P06-1073,0,0.743734,"mouri,bies,skulick}@ldc.upenn.edu Arabic diacritization (referred to sometimes as vocalization or vowelling), defined as the full or partial representation of short vowels, shadda (consonantal length or germination), tanween (nunation or definiteness), and hamza (the glottal stop and its support letters), is still largely understudied in the current NLP literature. In this paper, the lack of diacritics in standard Arabic texts is presented as a major challenge to most Arabic natural language processing tasks, including parsing. Recent studies (Messaoudi, et al. 2004; Vergyri & Kirchhoff 2004; Zitouni, et al. 2006 and Maamouri, et al. forthcoming) about the place and impact of diacritization in text-based NLP research are presented along with an analysis of the weight of the missing diacritics on Treebank morphological and syntactic analyses and the impact on parser development. Keywords: Arabic NLP, Arabic diacritics, Diacritization, Modern Standard Arabic (MSA), Treebanks, Linguistic Annotation, Parsing 1 INTRODUCTION Arabic NLP research, focusing mainly on Modern Standard Arabic (MSA) in this paper, faces two major challenges, not necessarily shared with many other natural languages: the first is it"
bies-etal-2006-linguistic,A00-2018,0,\N,Missing
bies-etal-2006-linguistic,graff-bird-2000-many,0,\N,Missing
bies-etal-2006-linguistic,N01-1016,0,\N,Missing
bies-etal-2006-linguistic,W02-1007,0,\N,Missing
bies-etal-2006-linguistic,N04-4032,0,\N,Missing
bies-etal-2006-linguistic,J03-4003,0,\N,Missing
bies-etal-2006-linguistic,N06-1024,1,\N,Missing
bies-etal-2006-linguistic,P04-1005,0,\N,Missing
bies-etal-2006-linguistic,cieri-etal-2004-fisher,0,\N,Missing
D07-1116,N07-2014,1,0.898226,"Missing"
D07-1116,W05-0711,0,0.0816376,"Missing"
D07-1116,W04-1612,0,0.15119,"Missing"
D07-1116,P06-1073,0,0.324405,"Missing"
D07-1116,P97-1003,0,\N,Missing
kulick-etal-2010-consistent,maamouri-etal-2010-speech,1,\N,Missing
kulick-etal-2010-consistent,W04-1606,0,\N,Missing
kulick-etal-2010-consistent,maamouri-etal-2008-diacritic,1,\N,Missing
kulick-etal-2012-developments,E03-1068,0,\N,Missing
kulick-etal-2012-developments,P00-1058,0,\N,Missing
kulick-etal-2012-developments,P03-2041,0,\N,Missing
kulick-etal-2012-developments,P10-2014,0,\N,Missing
kulick-etal-2012-developments,C10-1045,0,\N,Missing
kulick-etal-2012-developments,P11-2122,1,\N,Missing
L16-1405,W13-2301,1,0.860938,"y in order to handle OOV items, on the assumption that whatever lexicon we used would necessarily be lacking in coverage for the project annotation, even if the suffix sequences could be made close to complete. This framework builds upon our experience with development of Arabic analyzers starting with the Arabic Treebank (Kulick et al., 2010) and moving to the Egyptian Arabic Treebank (Maamouri et al., 2014), in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was greatly increased in order to handle OOV items, as the CALIMA analyzer (Habash et al., 2012; Maamouri et al., 2014) was simultaneously being developed. However, even then this handling was suboptimal because there was no general classification of what suffix combinations could 3 A"
L16-1405,W12-2301,0,0.0292489,"in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was greatly increased in order to handle OOV items, as the CALIMA analyzer (Habash et al., 2012; Maamouri et al., 2014) was simultaneously being developed. However, even then this handling was suboptimal because there was no general classification of what suffix combinations could 3 Another possible analysis is millet/NOUN+ler/3P, due to the ambiguity of ler, which we have not discussed here. 2555 follow stems with particular morphological properties. In contrast, the work reported here integrates this annotation need into the development of the morphological specification, instead of being added on after the analyzer development. 5. Lexicon-Analyzer Tradeoffs, and Integration of Existi"
L16-1405,E09-2008,0,0.223454,"ixes in the analyzer. Also, the interaction between the lexicon and the morphological specification could be, and indeed was, different for each language, depending on the morphological characteristics of the language and the available resources. For example, since Hausa has far less suffixation than Turkish, for Hausa the lexicon held multiple entries for a stem containing the effects of the derivational morphology, with much less information in the morphological specification than for Turkish. While morphological analyzer development tools such as XFST (Beesley and Karttunen, 2003) or Foma (Hulden, 2009) can be extremely powerful and very useful for grammar organization, they can also be quite fragile and complex when dealing with various phenomena such as longdistance dependencies between morphemes, so they could not be used exclusively for this project. We therefore adopted a hybrid approach, described in Section 3. 3. STATE_NOUN ; STATE_NOUN ; STATE_NOUN ; STATE_NOUN ; LEXICON STATE_NOUN +STATE-NOUN:x NOUN_HYPOCORISTIC ; +STATE-NOUN:x NOUN_PLURAL ; LEXICON NOUN_HYPOCORISTIC +CIk/DIM:x # ; LEXICON NOUN_PLURAL NOUN_POSS_FROM_STEM ; +lEr/PLURAL:x NOUN_POSS_FROM_PLURAL ; LEXICON NOUN_POSS_FROM"
L16-1405,kulick-etal-2010-consistent,1,0.828918,"based on flexible key/value entries in a map. This makes it more convenient than in SAMA to classify the possible stems for a suffix, including in terms of the morphological properties that would be needed by a stem to take a particular suffix sequence. We used this property in order to handle OOV items, on the assumption that whatever lexicon we used would necessarily be lacking in coverage for the project annotation, even if the suffix sequences could be made close to complete. This framework builds upon our experience with development of Arabic analyzers starting with the Arabic Treebank (Kulick et al., 2010) and moving to the Egyptian Arabic Treebank (Maamouri et al., 2014), in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was grea"
L16-1405,maamouri-etal-2014-developing,1,0.850053,"convenient than in SAMA to classify the possible stems for a suffix, including in terms of the morphological properties that would be needed by a stem to take a particular suffix sequence. We used this property in order to handle OOV items, on the assumption that whatever lexicon we used would necessarily be lacking in coverage for the project annotation, even if the suffix sequences could be made close to complete. This framework builds upon our experience with development of Arabic analyzers starting with the Arabic Treebank (Kulick et al., 2010) and moving to the Egyptian Arabic Treebank (Maamouri et al., 2014), in which work moved from using an extensive already-existing analyzer for Modern Standard Arabic (MSA) to the simultaneous annotation and development of an analyzer for Egyptian Arabic (Eskander et al., 2013). Even as part of the earlier annotation using SAMA for MSA, “wildcard” solutions were used, which provided possible analyses for a word even when it had an unknown stem. For SAMA, however, this was limited to the use of proper names. For Egyptian Arabic Treebank annotation, the use of wildcards in the annotation was greatly increased in order to handle OOV items, as the CALIMA analyzer"
L16-1405,L16-1521,0,0.216201,"; +STATE-NOUN:x NOUN_PLURAL ; LEXICON NOUN_HYPOCORISTIC +CIk/DIM:x # ; LEXICON NOUN_PLURAL NOUN_POSS_FROM_STEM ; +lEr/PLURAL:x NOUN_POSS_FROM_PLURAL ; LEXICON NOUN_POSS_FROM_STEM NOUN_CASE +(I)m/POSS_1S NOUN_CASE ... LEXICON NOUN_POSS_FROM_PLURAL NOUN_CASE +(I)m/POSS_1S NOUN_CASE ... ; ; ; ; Figure 1: Foma specification for sequences of abstract morphemes in Turkish Development of the Morphological Analysis Specification The starting point of the morphological specification was the information in the more general grammatical sketch that was developed for each language as part of the project (Strassel and Tracey, 2016). The grammatical sketch is simply a description of the main characteristics of a given language - morphology, syntax, etc., similar to the information that could be found in any overview of a language. It was written from the perspective of being used by a NLP system developer, not by a linguist. 3.1. LEXICON NOUN_STEM ... NOUN_STEM_%02/NOUN:x .... NOUN_STEM_%05/NOUN:x .... NOUN_STEM_%08/NOUN:x .... NOUN_STEM_%11/NOUN:x .... Abstract Morpheme Organization While we do not use Foma (Hulden, 2009) for the full analyzer specification, as mentioned above, we do utilize it as a convenient way to im"
L16-1405,W05-1106,0,0.0915031,"Missing"
maamouri-etal-2008-diacritic,W04-1602,1,\N,Missing
maamouri-etal-2008-diacritic,P05-1071,0,\N,Missing
maamouri-etal-2010-speech,kulick-etal-2010-consistent,1,\N,Missing
maamouri-etal-2010-speech,maamouri-etal-2008-enhancing,1,\N,Missing
maamouri-etal-2010-speech,W04-1602,1,\N,Missing
maamouri-etal-2012-expanding,maamouri-etal-2010-speech,1,\N,Missing
maamouri-etal-2012-expanding,kulick-etal-2010-consistent,1,\N,Missing
maamouri-etal-2012-expanding,C10-1045,0,\N,Missing
maamouri-etal-2014-developing,C10-1045,0,\N,Missing
maamouri-etal-2014-developing,P06-1086,1,\N,Missing
maamouri-etal-2014-developing,habash-etal-2012-conventional,1,\N,Missing
maamouri-etal-2014-developing,N13-1044,1,\N,Missing
maamouri-etal-2014-developing,pasha-etal-2014-madamira,1,\N,Missing
maamouri-etal-2014-developing,W13-2301,1,\N,Missing
maamouri-etal-2014-developing,maamouri-etal-2006-developing,1,\N,Missing
maamouri-etal-2014-developing,W12-2301,1,\N,Missing
maamouri-etal-2014-developing,L06-1000,0,\N,Missing
N06-1024,H91-1060,0,0.166861,"ndexed with their antecedents. (NP *)s are used for several purposes in the Penn Treebank. Among the most common are passivization “(NP-1 I) was captured (NP *-1),” and control “(NP-1 I) tried (NP *-1) to get the best results.” Under this representation the above sentence would look like “(NP-1 The dragon) 0 (NP-2 I) am trying (NP *-2) to slay (NP *T*-1) is green.” Despite their importance, these annotations have largely been ignored in statistical parsing work. The importance of returning this information for most real applications of parsing has been greatly obscured by the Parseval metric (Black et al., 1991), which explicitly ignores both function tags and null elements. Because much statistical parsing research has been driven until recently by this metric, which has never been updated, the crucial role of parsing in recovering semantic structure has been generally ignored. An early exception to this was (Collins, 1997) itself, where Model 2 used function tags during the training process for heuristics to identify arguments (e.g., the TMP tag on the NP in Figure 1 disqualifies the NP-TMP from being treated as an argument). However, after this use, the tags are ignored, not included in the models"
N06-1024,A00-2031,0,0.442288,"Missing"
N06-1024,P04-1082,0,0.820493,"e–based 9 (PSLB) work on recovering empty categories has fallen into two classes: those which integrate empty category recovery into the parser (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b) and those which recover empty categories from parser output in a post–processing step (Johnson, 2002; Levy and Manning, 2004). Levy and Manning note that thus far no PSLB post–processing approach has come close to matching the integrated approach on the most numerous types of empty categories. However, there is a rule–based post–processing approach consisting of a set of entirely hand– designed rules (Campbell, 2004) which has better 9 As above, we consider only that work which both inputs and outputs phrase–structure trees. This notably excludes Jijkoun and de Rijke (Jijkoun and de Rijke, 2004), who have a system which seems to match the performance of Dienes and Dubey. However, they provide only aggregate statistics over all the types of empty categories, making any sort of detailed comparison impossible. Finally, it is not clear that their numbers are in fact comparable to those of Dienes and Dubey on parsed data because the metrics used are not quite equivalent, particularly for (NP *)s: among other d"
N06-1024,A00-2018,0,0.458205,"ifying less than ten lines of code. The second stage achieves state-of-the-art performance on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods. 1 Introduction The trees in the Penn Treebank (Bies et al., 1995) are annotated with a great deal of information to make various aspects of the predicate-argument structure easy to decode, including both function tags and markers of “empty” categories that represent displaced constituents. Modern statistical parsers such as (Collins, 2003) and (Charniak, 2000) however ignore much of this information and return only an We would like to thank Fernando Pereira, Dan Bikel, Tony Kroch and Mark Liberman for helpful suggestions. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-C-0022, and in part by the National Science Foundation under grants NSF IIS-0520798 and NSF EIA 02-05448 and under an NSF graduate fellowship. impoverished version of the trees. While there has been some work in the last few years on enriching the output of state-of-the-art parsers that output Penn Treeban"
N06-1024,P97-1003,0,0.305225,"ying (NP *-2) to slay (NP *T*-1) is green.” Despite their importance, these annotations have largely been ignored in statistical parsing work. The importance of returning this information for most real applications of parsing has been greatly obscured by the Parseval metric (Black et al., 1991), which explicitly ignores both function tags and null elements. Because much statistical parsing research has been driven until recently by this metric, which has never been updated, the crucial role of parsing in recovering semantic structure has been generally ignored. An early exception to this was (Collins, 1997) itself, where Model 2 used function tags during the training process for heuristics to identify arguments (e.g., the TMP tag on the NP in Figure 1 disqualifies the NP-TMP from being treated as an argument). However, after this use, the tags are ignored, not included in the models, and absent from the parser output. Collins’ Model 3 attempts to recover traces of Wh-movement, with limited success. 3 Function Tags: Approach Our system for restoring function tags is a modification of Collins’ Model 2. We use the (Bikel, 2004) 2 Excepting empty units (e.g. “$ 1,000,000 *U*”), which are not very in"
N06-1024,J03-4003,0,0.359739,"derlying parser, modifying less than ten lines of code. The second stage achieves state-of-the-art performance on the recovery of empty categories by combining a linguistically-informed architecture and a rich feature set with the power of modern machine learning methods. 1 Introduction The trees in the Penn Treebank (Bies et al., 1995) are annotated with a great deal of information to make various aspects of the predicate-argument structure easy to decode, including both function tags and markers of “empty” categories that represent displaced constituents. Modern statistical parsers such as (Collins, 2003) and (Charniak, 2000) however ignore much of this information and return only an We would like to thank Fernando Pereira, Dan Bikel, Tony Kroch and Mark Liberman for helpful suggestions. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-C-0022, and in part by the National Science Foundation under grants NSF IIS-0520798 and NSF EIA 02-05448 and under an NSF graduate fellowship. impoverished version of the trees. While there has been some work in the last few years on enriching the output of state-of-the-art parsers tha"
N06-1024,W03-1005,0,0.762892,"a, Dan Bikel, Tony Kroch and Mark Liberman for helpful suggestions. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-C-0022, and in part by the National Science Foundation under grants NSF IIS-0520798 and NSF EIA 02-05448 and under an NSF graduate fellowship. impoverished version of the trees. While there has been some work in the last few years on enriching the output of state-of-the-art parsers that output Penn Treebank-style trees with function tags (e.g. (Blaheta, 2003)) or empty categories (e.g. (Johnson, 2002; Dienes and Dubey, 2003a; Dienes and Dubey, 2003b), only one system currently available, the dependency graph parser of (Jijkoun and de Rijke, 2004), recovers some representation of both these aspects of the Treebank representation; its output, however, cannot be inverted to recover the original tree structures. We present here a parser,1 the first we know of, that recovers full Penn Treebank-style trees. This parser uses a minimal modification of the Collins parser to recover function tags, and then uses this enriched output to achieve or better state-of-theart performance on recovering empty categories. We focus h"
N06-1024,P03-1055,0,0.792327,"a, Dan Bikel, Tony Kroch and Mark Liberman for helpful suggestions. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-C-0022, and in part by the National Science Foundation under grants NSF IIS-0520798 and NSF EIA 02-05448 and under an NSF graduate fellowship. impoverished version of the trees. While there has been some work in the last few years on enriching the output of state-of-the-art parsers that output Penn Treebank-style trees with function tags (e.g. (Blaheta, 2003)) or empty categories (e.g. (Johnson, 2002; Dienes and Dubey, 2003a; Dienes and Dubey, 2003b), only one system currently available, the dependency graph parser of (Jijkoun and de Rijke, 2004), recovers some representation of both these aspects of the Treebank representation; its output, however, cannot be inverted to recover the original tree structures. We present here a parser,1 the first we know of, that recovers full Penn Treebank-style trees. This parser uses a minimal modification of the Collins parser to recover function tags, and then uses this enriched output to achieve or better state-of-theart performance on recovering empty categories. We focus h"
N06-1024,N03-1014,0,0.0203768,"Missing"
N06-1024,P04-1040,0,0.287223,"Missing"
N06-1024,P02-1018,0,0.836514,"Fernando Pereira, Dan Bikel, Tony Kroch and Mark Liberman for helpful suggestions. This work was supported in part under the GALE program of the Defense Advanced Research Projects Agency, Contract No. HR001106-C-0022, and in part by the National Science Foundation under grants NSF IIS-0520798 and NSF EIA 02-05448 and under an NSF graduate fellowship. impoverished version of the trees. While there has been some work in the last few years on enriching the output of state-of-the-art parsers that output Penn Treebank-style trees with function tags (e.g. (Blaheta, 2003)) or empty categories (e.g. (Johnson, 2002; Dienes and Dubey, 2003a; Dienes and Dubey, 2003b), only one system currently available, the dependency graph parser of (Jijkoun and de Rijke, 2004), recovers some representation of both these aspects of the Treebank representation; its output, however, cannot be inverted to recover the original tree structures. We present here a parser,1 the first we know of, that recovers full Penn Treebank-style trees. This parser uses a minimal modification of the Collins parser to recover function tags, and then uses this enriched output to achieve or better state-of-theart performance on recovering empt"
N06-1024,P04-1042,0,0.184662,"be seen from Figure 2 and repeated in 1, this is a very rare category. 187 processing approaches. The linguistic sophistication of the work of (Musillo and Merlo, 2005) then provides an added boost in performance over simple integration. 6 Empty Categories: Approach Most learning–based, phrase–structure–based 9 (PSLB) work on recovering empty categories has fallen into two classes: those which integrate empty category recovery into the parser (Dienes and Dubey, 2003a; Dienes and Dubey, 2003b) and those which recover empty categories from parser output in a post–processing step (Johnson, 2002; Levy and Manning, 2004). Levy and Manning note that thus far no PSLB post–processing approach has come close to matching the integrated approach on the most numerous types of empty categories. However, there is a rule–based post–processing approach consisting of a set of entirely hand– designed rules (Campbell, 2004) which has better 9 As above, we consider only that work which both inputs and outputs phrase–structure trees. This notably excludes Jijkoun and de Rijke (Jijkoun and de Rijke, 2004), who have a system which seems to match the performance of Dienes and Dubey. However, they provide only aggregate statisti"
N06-1024,W05-1509,0,0.113639,"Missing"
N10-1094,J03-4003,0,\N,Missing
N10-1094,P00-1058,0,\N,Missing
N12-1031,P09-2056,0,0.0224385,"ructure. Following the motivation discussed in Section 1, the next step is straightforward - to adapt the algorithm to work on conversion from a dependency representation of the Arabic Treebank to the phrase structure representation necessary for the annotation pipeline. Following this, we will then experiment with parsing the Arabic dependency representation, converting to phrase structure, and evaluating the resulting phrase structure representation as usual for parsing evaluation. We will also experiment with dependency parsing for the PTB dependency representation discussed in this paper. Habash and Roth (2009) discuss an already-existing dependency representation of parts of the ATB and it will be interesting to compare the conversion accuracy using the different dependency representations, although we 313 expect that there will not be any major differences in the representations. One other aspect of future work is to implement the algorithm in Wang and Zong (2010), using our own dependency representation, since this would allow a precise investigation of what the phrase structure parser is contributing as compared to our automatic conversion. We note that this work also experimented with dependenc"
N12-1031,W08-1007,0,0.0225921,"correct some of the POS tag errors in the PTB (Manning, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson and Nugues, 2007). Our goal is very different. Instead of en1 There are other details not discussed here. For example, we do not automatically assign a P NP supertag to the head child of an NP, since such a head can legitimately be, e.g, a JJ, in which case we make the supertag P ADJP, on the reasoning that it would be encoding “too much” to treat it as P NP. Instead, we rely on the DS and such labels as SBJ or OBJ to determine when to project it as NP in the converted PS. 308 coding the phrase structure in the dependency tree via complex tags such as"
N12-1031,W07-2444,0,0.0311914,"errors in the PTB (Manning, 2011). For example, if that has the (incorrect) tag DT in the complementizer position, it still receives the new POS tag P COMP. This procedure results in a set of 30 supertags, and Table 1 shows how they are partitioned into 14 projection types. These supertags and projection types are the basis of our DS-to-PS conversion, as discussed further in Section 2.2. We note here a brief comparison with earlier work on “hybrid” representations, which encode a PS representation inside a DS one, in order to convert from the latter to the former. (Hall and Nivre, 2008; Johan Hall and Nilsson, 2007; Johansson and Nugues, 2007). Our goal is very different. Instead of en1 There are other details not discussed here. For example, we do not automatically assign a P NP supertag to the head child of an NP, since such a head can legitimately be, e.g, a JJ, in which case we make the supertag P ADJP, on the reasoning that it would be encoding “too much” to treat it as P NP. Instead, we rely on the DS and such labels as SBJ or OBJ to determine when to project it as NP in the converted PS. 308 coding the phrase structure in the dependency tree via complex tags such as SBARQ in Johansson and Nugues"
N12-1031,W07-2416,0,0.157017,"to the Arabic treebank as well. We expect this to be successful because the ATB has some fundamental similarities to the PTB in spite of the language difference (Maamouri and Bies, 2004). As mentioned above, one goal in our DS to PS conversion work is to base it on a minimal DS representation. By “minimal”, we mean that it does not include information that is redundant, together with our conversion code, with the implicit information in the dependency structure itself. As discussed more in Section 2.1, we aim to make our dependency representation simpler than “hybrid” representations such as Johansson and Nugues (2007). The reason for our interest in this minimal representation is parsing. We do not want to require the parser to recover such a complex dependency representations, when it is, in fact, unnecessary, as we believe our approach shows. The benefit of this approach can only be seen when this line of work is extended to experiments with parsing and Arabic conversion. The work described here is just the first step in this process. A conversion scheme, such as ours, necessarily relies on some details of the annotation content in the DS and PS representations, and so our algorithm is not an algorithm d"
N12-1031,W04-1602,1,0.79084,"rabic, we are first focusing on 305 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 305–314, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics a conversion routine for the English PTB because it is so well-established and the results are easier to interpret. The intent is then to transfer this conversion algorithm work to the Arabic treebank as well. We expect this to be successful because the ATB has some fundamental similarities to the PTB in spite of the language difference (Maamouri and Bies, 2004). As mentioned above, one goal in our DS to PS conversion work is to base it on a minimal DS representation. By “minimal”, we mean that it does not include information that is redundant, together with our conversion code, with the implicit information in the dependency structure itself. As discussed more in Section 2.1, we aim to make our dependency representation simpler than “hybrid” representations such as Johansson and Nugues (2007). The reason for our interest in this minimal representation is parsing. We do not want to require the parser to recover such a complex dependency representatio"
N12-1031,J93-2004,0,0.0422264,"lines. The resulting system significantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing. 1 Introduction and Motivation Recent years have seen a significant increase in interest in dependency treebanks and dependency parsing. Since the standard training and test set for English parsing is a phrase structure (PS) treebank, the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can However, our concern is somewhat different. We are specifically interested in experimenting with dependency parsing of Arabic as a step in the annotation of the A"
N12-1031,H94-1020,1,0.510763,"system significantly outperforms previous work in such automatic conversion. We also achieve comparable results to a system using a phrase-structure parser for the conversion. A comparison with our system using either the part-of-speech tags or the supertags provides some indication of what the parser is contributing. 1 Introduction and Motivation Recent years have seen a significant increase in interest in dependency treebanks and dependency parsing. Since the standard training and test set for English parsing is a phrase structure (PS) treebank, the Penn Treebank (PTB) (Marcus et al., 1993; Marcus et al., 1994), the usual approach is to convert this to a dependency structure (DS) treebank, by means of various heuristics for identifying heads in a PS tree. The resulting DS representation is then used for training and parsing, with results reported on the DS representation. Our goal in this paper is to go in the reverse direction, from the DS to PS representation, by finding a minimal DS representation from which we can However, our concern is somewhat different. We are specifically interested in experimenting with dependency parsing of Arabic as a step in the annotation of the Arabic Treebank, which"
N12-1031,D08-1052,0,0.431252,"Missing"
N12-1031,C10-2148,0,0.295097,"d the word y to its immediate right is its parent in the dependency tree and y has one of the verbal POS tags, then x receives the supertag P AUX, and otherwise P PP. Any word with the POS tag JJ, JJR, or JJS, receives the supertag P ADJP, and so on. The results for Xia and Palmer (2001) and Xia et al. (2009) were reported using an unlabeled version of evalb, so to compare properly we also report our results for Section 00 using an unlabeled evaluation of the run using the POS tags (USE-POS-UNLABEL), while all the other results use a labeled evaluation. We also compare our system with that of Wang and Zong (2010). Unlike the three other systems (including ours), this was not based on an automatic conversion from a gold dependency tree to phrase structure, but rather used the gold dependency tree as additional input for a phrase structure parser (the Berkeley parser). 4.1 Analysis While our system was developed using Section 24, the f-measure results for USE-SUPER are virtually identical across all four sections (96.4, 96.7, 96.7, 96.5). Interestingly, there is more variation in the Error type problem with PTB annotation ambiguous ADVP placement incorrect use of “single token rule” FRAG/X multiple leve"
N12-1031,H01-1014,0,0.280737,"process. A conversion scheme, such as ours, necessarily relies on some details of the annotation content in the DS and PS representations, and so our algorithm is not an algorithm designed to take as input any arbitrary DS representation. However, the fundamentals of our dependency representation are not radically different than others - e.g. we make an auxiliary verb the child of the main verb, instead of the other way, but such choices can be adjusted for in the conversion. To evaluate the success of this conversion algorithm, we follow the same evaluation procedure as Xia et al. (2009) and Xia and Palmer (2001). We convert the PTB to a DS, and then use our algorithm to convert the DS back to a PS representation. The original PS and the converted-from-DS PS are then compared, in exactly the same way as parser output is compared with the original (gold) tree. We will show that our results in this area are a significant improvement above previous efforts. A key aspect of this work is that our DS-to-PS conversion encodes many of the properties of the PTB annotation guidelines (Bies et al., 1995), both 306 globally and for specific XP projections. The PTB guidelines are built upon broad decisions about P"
N12-1031,P00-1058,0,\N,Missing
N13-1061,P10-1075,0,0.014887,"As mentioned above, there is a certain class of inconsistencies which KBM will not pinpoint precisely, which requires adopting the “external check” from Kulick et al. (2012). The abstraction on inconsistency types described in Section 4 can also be taken further. For example, one might want to examine in particular inconsistency types that arise from PP attachment or that have to do with the PRN function tag. One main area for future work is the application of this work to parser evaluation as well as IAA. For this area, there is some connection to the work of Goldberg and Elhadad (2010) and Dickinson (2010), which are both concerned with examining dependency structures of more than one edge. The connection is that those works are focused on dependency representations, and ithe KBM system does phrase structure analysis using a TAG-like derivation tree, which strongly resembles a dependency tree (Rambow and Joshi, 1997). There is much in this area of common concern that is worth examining further. Acknowledgments actual The trees in (4) and (5) show two of the 88 nuclei grouped into the first inconsistency type. As with The word renaissance and The term renaissance in the English web corpus, nucle"
N13-1061,W10-2927,0,0.0210956,"to improve the current approach. As mentioned above, there is a certain class of inconsistencies which KBM will not pinpoint precisely, which requires adopting the “external check” from Kulick et al. (2012). The abstraction on inconsistency types described in Section 4 can also be taken further. For example, one might want to examine in particular inconsistency types that arise from PP attachment or that have to do with the PRN function tag. One main area for future work is the application of this work to parser evaluation as well as IAA. For this area, there is some connection to the work of Goldberg and Elhadad (2010) and Dickinson (2010), which are both concerned with examining dependency structures of more than one edge. The connection is that those works are focused on dependency representations, and ithe KBM system does phrase structure analysis using a TAG-like derivation tree, which strongly resembles a dependency tree (Rambow and Joshi, 1997). There is much in this area of common concern that is worth examining further. Acknowledgments actual The trees in (4) and (5) show two of the 88 nuclei grouped into the first inconsistency type. As with The word renaissance and The term renaissance in the Engl"
N13-1061,P11-2122,1,0.932454,"consisting of syntactic structure with words as the terminals, is by its nature more complex and therefore more prone to error than many other annotation tasks. However, high annotation consistency is crucial to providing reliable training and testing data for parsers and linguistic research. Error detection is therefore an important area of research, and the importance of work such as Dickinson and Meurers (2003) is that errors and annotation inconsistencies might be automatically discovered, and once discovered, be targeted for subsequent quality control. A recent approach to this problem (Kulick et al., 2011; Kulick et al., 2012) (which we will call the KBM system) improves upon Dickinson and Meurers (2003) by decomposing the full syntactic tree into smaller units, using ideas from Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997). This allows the comparison to be based on small syntactic units instead of string n-grams, improving the detection of inconsistent annotation. The KBM system, like that of Dickinson and Meurers (2003) before it, is based on the notion of comparing identical strings. In the general case, this is a problematic assumption, since annotation inconsistencies are missed"
N13-1061,kulick-etal-2012-developments,1,0.819494,"tic structure with words as the terminals, is by its nature more complex and therefore more prone to error than many other annotation tasks. However, high annotation consistency is crucial to providing reliable training and testing data for parsers and linguistic research. Error detection is therefore an important area of research, and the importance of work such as Dickinson and Meurers (2003) is that errors and annotation inconsistencies might be automatically discovered, and once discovered, be targeted for subsequent quality control. A recent approach to this problem (Kulick et al., 2011; Kulick et al., 2012) (which we will call the KBM system) improves upon Dickinson and Meurers (2003) by decomposing the full syntactic tree into smaller units, using ideas from Tree Adjoining Grammar (TAG) (Joshi and Schabes, 1997). This allows the comparison to be based on small syntactic units instead of string n-grams, improving the detection of inconsistent annotation. The KBM system, like that of Dickinson and Meurers (2003) before it, is based on the notion of comparing identical strings. In the general case, this is a problematic assumption, since annotation inconsistencies are missed because of superficial"
N13-1061,P00-1058,0,\N,Missing
P05-1061,C00-1030,0,0.0121031,"(IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involvMost relation extraction systems focus on the specific problem of extracting binary relations, such as the employee of relation or protein-protein interaction relation. Very little work has been done in recognizing and extracting more complex relations. We define a complex relation as any n-ary relation among n typed entities. The relation is defined by the schema (t1 , . . . , tn ) where ti ∈ T are entity types. An i"
P05-1061,J02-3001,0,0.0155543,"ere has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is always associated with a verb in the sentence that specifies the relation type. Though this system is very general, it is limited by the fact that the design ignores relations not expressed by a verb, as the employee of relation in“John Smith, CEO of Inc. Corp., announced he will resign”. Most relation extraction systems work primarily on a sentential level and never consider relations that cross sentences or paragraphs. Since current data sets typically only annotate intra-sentence relations, this has not yet proven to be a problem. Complex Relations Recall that a complex n"
P05-1061,N01-1025,0,0.012856,"lated, and the second stage scores maximal cliques in that graph as potential complex relation instances. We evaluate the new method against a standard baseline for extracting genomic variation relations from biomedical text. 1 Introduction Most research on text information extraction (IE) has focused on accurate tagging of named entities. Successful early named-entity taggers were based on finite-state generative models (Bikel et al., 1999). More recently, discriminatively-trained models have been shown to be more accurate than generative models (McCallum et al., 2000; Lafferty et al., 2001; Kudo and Matsumoto, 2001). Both kinds of models have been developed for tagging entities such as people, places and organizations in news material. However, the rapid development of bioinformatics has recently generated interest on the extraction of biological entities such as genes (Collier et al., 2000) and genomic variations (McDonald et al., 2004b) from biomedical literature. The next logical step for IE is to begin to develop methods for extracting meaningful relations involvMost relation extraction systems focus on the specific problem of extracting binary relations, such as the employee of relation or protein-p"
P05-1061,A00-2030,0,0.00792299,"hallow parse representation. This approach — enumerating all possible entity pairs and classifying each as positive or negative — is the standard method in relation extraction. The main differences among systems are the choice 492 of trainable classifier and the representation for instances. For binary relations, this approach is quite tractable: if the relation schema is (t 1 , t2 ), the number of potential instances is O(|t1 ||t2 |), where |t |is the number of entity mentions of type t in the text under consideration. One interesting system that does not belong to the above class is that of Miller et al. (2000), who take the view that relation extraction is just a form of probabilistic parsing where parse trees are augmented to identify all relations. Once this augmentation is made, any standard parser can be trained and then run on new sentences to extract new relations. Miller et al. show such an approach can yield good results. However, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrat"
P05-1061,P04-1055,0,0.0134126,"rsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume at most one relation per sentence, ruling out cases where entities participate in multiple relations, which is a common occurrence in our data. McDonald et al. (2004a) use a rulebased parser combined with a rule-based relation identifier to extract generic binary relations between biological entities. As in predicate-argument extraction (Gildea and Jurafsky, 2002), each relation is always associated with a verb in the sentence"
P05-1061,W04-2401,0,0.0245476,"owever, it can be argued that this method will encounter problems when considering anything but binary relations. Complex relations would require a large amount of tree augmentation and most likely result in extremely sparse probability estimates. Furthermore, by integrating relation extraction with parsing, the system cannot consider long-range dependencies due to the local parsing constraints of current probabilistic parsers. The higher the arity of a relation, the more likely it is that entities will be spread out within a piece of text, making long range dependencies especially important. Roth and Yih (2004) present a model in which entity types and relations are classified jointly using a set of global constraints over locally trained classifiers. This joint classification is shown to improve accuracy of both the entities and relations returned by the system. However, the system is based on constraints for binary relations only. Recently, there has also been many results from the biomedical IE community. Rosario and Hearst (2004) compare both generative and discriminative models for extracting seven relationships between treatments and diseases. Though their models are very flexible, they assume"
P05-1061,J96-1002,0,0.0349033,"Missing"
P08-2053,W07-0812,0,0.0255709,"Missing"
P08-2053,J04-4004,0,\N,Missing
P08-2053,J03-4003,0,\N,Missing
P10-2063,P05-1071,0,0.174292,"noun or noun + possessive pronoun. One approach taken to this problem is to use a morphological analyzer such as BAMA-v2.0 (Buckwalter, 2004) or SAMA-v3.1 (Maamouri et al., 2009c)1 , which generates a list of all possible morphological analyses for a given token. Machine learning approaches can model separate aspects of a solution (e.g., “has a pronominal clitic”) and then combine them to select the most appropriate solution from among this list. A benefit of this approach is that by picking a single solution from the morphological analyzer, the part-ofspeech and tokenization comes as a unit (Habash and Rambow, 2005; Roth et al., 2008). 2 Background The Arabic Treebank (ATB) contains a full morphological analysis of each “source token”, a whitespace/punctuation-delimited string from the source text. The SAMA analysis includes four fields, as shown in the first part of Table 1.2 TEXT is the actual source token text, to be analyzed. VOC is the vocalized form, including diacritics. Each VOC segment has associated with it a POS tag and 2 This is the analysis for one particular instance of ktbh. The same source token may receive another analysis elsewhere in the treebank. 1 SAMA-v3.1 is an updated version of"
P10-2063,kulick-etal-2010-consistent,1,0.814755,"he POS tag is also taken into account, there can be only one match among all the (open or closed-class) regexes. We say a source token “pos-matches” a regex if the TEXT matches and POS tags match, and “textmatches” if the TEXT matches the regex regardless of the POS. During training, the pos-matching DEM[infl], [IV|PV|CV]SUFF DO[infl] (2) Collapsing of some ATB core tags, as listed in Table 2. These two steps result in a total of 40 reduced core tags, and each tree token has exactly one such reduced core tag. We work with the ATB3-v3.2 release of the ATB (Maamouri et al., 2009b), which 3 See (Kulick et al., 2010) for a detailed discussion of how this splitting is done and how the tree token TEXT field (called INPUT STRING in the ATB releases) is created. 343 (REGEX #1) [w|f]lm w: [PART, CONJ, SUB CONJ, PREP] f: [CONJ, SUB CONJ, CONNEC PART, RC PART] lm: [NEG PART] (REGEX #2) [w|f]lm w: and f: same as above lm: [REL ADV,INTERROG ADV] which is the name of the stem part of the regular expression, along with the characters that match the stem. The stem name encodes whether there is a prefix or suffix, but does not include a POS tag. However, the source token pos-matches exactly one of the regular expressi"
P10-2063,P08-2030,0,0.0585798,"pronoun. One approach taken to this problem is to use a morphological analyzer such as BAMA-v2.0 (Buckwalter, 2004) or SAMA-v3.1 (Maamouri et al., 2009c)1 , which generates a list of all possible morphological analyses for a given token. Machine learning approaches can model separate aspects of a solution (e.g., “has a pronominal clitic”) and then combine them to select the most appropriate solution from among this list. A benefit of this approach is that by picking a single solution from the morphological analyzer, the part-ofspeech and tokenization comes as a unit (Habash and Rambow, 2005; Roth et al., 2008). 2 Background The Arabic Treebank (ATB) contains a full morphological analysis of each “source token”, a whitespace/punctuation-delimited string from the source text. The SAMA analysis includes four fields, as shown in the first part of Table 1.2 TEXT is the actual source token text, to be analyzed. VOC is the vocalized form, including diacritics. Each VOC segment has associated with it a POS tag and 2 This is the analysis for one particular instance of ktbh. The same source token may receive another analysis elsewhere in the treebank. 1 SAMA-v3.1 is an updated version of BAMA, with many sign"
P11-2122,E03-1068,0,0.189571,"ackground and Motivation 2.1 Previous Work - DECCA Introduction The internal consistency of the annotation in a treebank is crucial in order to provide reliable training and testing data for parsers and linguistic research. Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurring more than once in a corpus may occur with different “labels” (taken to be constituen"
P11-2122,P03-2041,0,0.026495,"to its having different etrees, differing in their node label for the substitution node. If the nucleus under comparison includes the verb but not any words from the complement, the inclusion of the different substitution nodes would cause irrelevant differences for that particular nucleus comparison. We solve these problems by mapping down the in the derivation tree. 5 A related approach is taken by Kato and Matsubara (2010), who compare partial parse trees for different instances of the same sequence of words in a corpus, resulting in rules based on a synchronous Tree Substitution Grammar (Eisner, 2003). We suspect that there are some major differences between our approaches regarding such issues as the representation of adjuncts, but we leave such a comparison for future work. System DECCA Us nuclei 24,319 54,496 n-grams 1,158,342 not used instances 2,966,274 605,906 Table 1: Data examined by the two systems for the ATB System DECCA Us-internal nuclei found 4,140 9,984 non-duplicate nuclei found unknown 4,272 types of inconsistency unknown 1,911 Table 2: Annotation inconsistencies reported for the ATB representation of the etrees in a derivation tree fragment to form a “reduced” derivation"
P11-2122,C10-1045,0,0.34162,"ic Treebank and how this approach leads to high precision of error detection. 1 2 Background and Motivation 2.1 Previous Work - DECCA Introduction The internal consistency of the annotation in a treebank is crucial in order to provide reliable training and testing data for parsers and linguistic research. Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurrin"
P11-2122,P10-2014,0,0.232375,"A Introduction The internal consistency of the annotation in a treebank is crucial in order to provide reliable training and testing data for parsers and linguistic research. Treebank annotation, consisting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurring more than once in a corpus may occur with different “labels” (taken to be constituent node labels), and such differences in labels"
P11-2122,W10-4420,1,0.849308,"isting of syntactic structure with words as the terminals, is by its nature more complex and thus more prone to error than other annotation tasks, such as part-of-speech tagging. Recent work has therefore focused on the importance of detecting errors in the treebank (Green and Manning, 2010), and methods for finding such errors automatically, e.g. (Dickinson and Meurers, 2003b; Boyd et al., 2007; Kato and Matsubara, 2010). We present here a new approach to this problem that builds upon Dickinson and Meurers (2003b), by integrating the perspective on treebank consistency checking and search in Kulick and Bies (2010). The approach in Dickinson and Meurers (2003b) has certain limitations and complications that are inherent in examining only strings of words. To overThe basic idea behind the work in (Dickinson and Meurers, 2003a; Dickinson and Meurers, 2003b) is that strings occurring more than once in a corpus may occur with different “labels” (taken to be constituent node labels), and such differences in labels might be the manifestation of an annotation error. Adopting their terminology, a “variation nucleus” is the string of words with a difference in the annotation (label), while a “variation n-gram” i"
P11-2122,P00-1058,0,\N,Missing
P14-2108,P12-3029,0,0.062886,"Missing"
P14-2108,P06-1055,0,0.614895,"ure 4: An example of clausal structure, without VP. (6) (a) NP # Files 81 4 4 5 94 4 Data split We split the data into four sections, as shown in Table 2. The validation section consists of the four files beginning with “a” or “v” (spanning the years 1711-1860), the development section consists of the four files beginning with “l” (1753-1866), the test section consists of the five files beginning with “f” (1749-1900), and the training section consists of the remaining 81 files (1712-1913). The data split sizes used here for the PPCMBE closely approximate that used for the PTB, as described in Petrov et al. (2006).7 For this first work, we used a split that was roughly the same as far as timespans across the four sections. In future work, we will do a more proper cross-validation evaluation. Reduced+NPs As discussed in Section 2.2.2, noun modifiers are sisters to the noun, instead of being adjoined, as in the PTB. As a result, there are fewer NP brackets in the PPCMBE than there would be if the PTBstyle were followed. To evaluate the effect of the difference in annotation guidelines on the parsing score, we added PTB-style NP brackets to the reduced corpus described in Section 3.1. For example, (3a) in"
P14-2109,D08-1052,0,0.02555,"ow does the parser do on non-recursive NPs, separate from NPs resulting from modification? On PP attachment?” etc. Answering such questions is the goal of this work, which combines two strands of research. First, inspired by the tradition of Tree Adjoining Grammar-based research (Joshi and Schabes, 1997; Bangalore and Joshi, 2010), we use a decomposition of the full trees into “elementary trees” (henceforth “etrees”), with a derivation tree that records how the etrees relate to each other, as in Kulick et al. (2011). In particular, we use the “spinal” structure approach of (Shen et al., 2008; Shen and Joshi, 2008), where etrees are constrained to be unary-branching. 2 Framework for analyzing parsing performance We first describe the use of the regexes in tree decomposition, and then give some examples of in1 We refer only to the WSJ treebank portion of OntoNotes, which is roughly a subset of the Penn Treebank (Marcus et al., 1999) with annotation revisions including the addition of NML nodes. 2 We parse (c) while training on (a) to follow the procedure in Petrov and McDonald (2012) 668 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 668–673,"
P14-2109,N12-1031,1,0.896225,"Missing"
P14-2109,N13-1061,1,0.836283,"as well. For example, “make” is a match for VP-t in Figures 2 and 3, and is also a match for the span as well, since in both derivation trees it includes the words “make. . .Florida”. It is this matching for span as well as head that allows us to compare our results to evalb. We call the match just for the head the “Fh” score and the match that also includes the span information the “F-s” score. The F-s score roughly corresponds to the evalb score. However, the F2.4 Comparison with previous work This work is in the same basic line of research as the inter-annotator agreement analysis work in Kulick et al. (2013). However, that work did not utilize regexes, and focused on comparing sequences of identical strings. The current work scores on general categories of structures, without 6 A regex intermediate in a etree, such as VP-t above, is considered to have a default null attachment. Also, the attachment score is not relevant for regexes that already express a recursive structure, such as NP-modr. In Figure 2, NP-t in etree #a5 is considered as having the attachment to #a3. 670 regex NP-t VP-t PP-t S-vp NP-modr VP-aux SBAR-s ADVP-t NML-t ADJP-t QP-t NP-crd VP-crd S-crd SQ-v FRAG-nt Sections 2-21 (Onton"
P14-2109,D12-1096,0,0.0613251,"Missing"
P14-2109,J03-4003,0,\N,Missing
P14-2109,P11-2122,1,\N,Missing
P95-1029,P86-1037,0,0.0119429,"le theoretically possible, it becomes quite problematic to actually implement. The solution given in this paper is to use a higher-order logic programming language, AProlog, that already implements these concepts, called &quot;abstract syntax&quot; in (Miller, 1991) and &quot;higher-order abstract syntax&quot; in (Pfenning and Elliot, 1988). This allows a natural and elegant implementation of the grammatical theory, with only one lexical entry for and. This paper is meant to be viewed as furthering the exploration of the utility of higher-order logic programming for computational linguistics - see, for example, (Miller & Nadathur, 1986), (Pareschi, 1989), and (Pereira, 1990). 2 CCG CCG is a grammatical formalism in which there is a one-to-one correspondence between the rules of composition 1 at the level of syntax and logical form. Each word is (perhaps ambiguously) assigned a category and LF, and when the syntactical operations assign a new category to a constituent, the corresponding semantic operations produce a new LF for that constituent as well. The CCG rules shown in Figure 1 are implemented in the system described 1In the genera] sense, not specifically the CCG rule for function composition. 214 ........ harry ~. . ."
P95-1029,P89-1005,0,0.0552894,"sting cases that CCG can theoretically handle. The problem in general, and for CCG in particular, is that the implementation language does not have sufficient expressive power to allow a more direct encoding. The solution given in this paper is to show how advances in logic programming allow the implementation of semantic theories in a 213 When (2) is applied to the predicate, (15) will result after 13-reduction. However, under first-order unification, this needs to simulated by having the variable z in Az.run(z) unify both with Bill and John, and this is not possible. See (Jowsey, 1990) and (Moore, 1989) for a thorough discussion. (Moore, 1989) suggests that the way to overcome this problem is to use explicit A-terms and encode /~-reduction to perform the needed reduction. For example, the logical form in (3) would be produced, where Xtm(X) is the representation of Az.run (z). (3) and (apply (Iun(X), j ohn). apply (lun(l),bill) ) This would then be reduced by the clauses for apply to result in (lb). For this small example, writing such an apply predicate is not difficult. However, as the semantic terms become more complex, it is no trivial matter to write ~-reduction that will correctly"
P95-1029,P92-1027,0,0.485026,"apply to result in (lb). For this small example, writing such an apply predicate is not difficult. However, as the semantic terms become more complex, it is no trivial matter to write ~-reduction that will correctly handle variable capture. Also, if at some point it was desired to determine if the semantic forms of two different sentences were the same, a predicate would be needed to compare two lambda forms for a-equivalence, which again is not a simple task. Essentially, the logic variable X is meant to be interpreted as a bound variable, which requires an additional layer of programming. (Park, 1992) proposes a solution within first-order unification that can handle not only sentence (la), but also more complex examples with determiners. The method used is to introduce spurious bindings that subsequently get removed. For example, the semantics of (4a) would be (4b), which would then get simplified to (4c). (4a) A farmer and every senator talk (4b) exists(X1 , f a n a e r ( I 1 ) a( exists (x2, (x2=xl) ataZk (X2)) ) ) Function Application (>): I/Y:F Y:y =>g:Fy Function Application (<): Y:y IY:F=>I:Fy Function Composition (> B): X/Y:F Y/Z:G=>X/Z: )tx.F(Gx) Function Composition (< B): Y:G"
P95-1029,P95-1028,0,0.0110447,"nd then composed with found, with the result shown in the following query: ?- compose (abe p  ( a p p p h a r r y &apos; ) ) (abe o b j  (abe sub (found&apos; sub o b j ) ) ) M. M = (abe x  (app (abs sub(found ~ sub x)) harry&apos; )). derivation fails to yield all available quantifier scopings. We do not address here the further question of how the remaining scoped readings axe derived. Alternatives that appear compatible with the present approach are quantitier movement (Hobbs & Shieber, 1987), type-ralsing at LF (Paxtee & Rooth, 1983), or the use of disambiguated quantifers in the derivation itself (Park, 1995). 11There are other clauses, not shown here, that determine the direction of the CCG rule. For either direction, however, the semantics axe the same and both directiona.I rules call these clauses for the semantic computation. rip. s. conj. noun. Figure 5: Implementation of the CCG category system At this point a further/~-reduction is needed. Note however this is not at all the same problem of writing a /~-reducer in Prolog. Instead it is a simple matter of using the meta-level ~-reduction to eliminate ~-redexes to produce the final result (abe x(found I h a r r y x)). We won&apos;t show the compl"
P95-1029,P88-1034,0,0.0150002,"e fact that AProlog is a typed language leads to a good deal of formal clutter if this method is used. 1°The LF for the determiner has the form of a Montagovian generalized quantifier, giving rise to one fully scoped logical form for the sentence. It should be stressed that this particular kind of LF is assumed here purely for the sake of illustration, to make the point that composition at the level of derivation and LF are oneto-one. Section 4 contains an example for which such a sit is not established if this schema should actually produce an unbounded family of rules. See (Weir, 1988) and (Weir and Joshi, 1988) for a discussion of the implications for automata-theoretic power of generalized coordination and composition, and (Gazda~, 1988) for linguistic axguments that languages like Dutch may require this power, and (Steedman, 1990) for some further discussion of the issue. In this paper we use the generalized rule to illustrate the elegance of the representation, but it is an easy change to implement a bounded coordination rule. eThe ,I~ notation is used because of the combinatory logic background of CCG. See (Steedman, 1990) for details. 7defined as the unification of simply typed A-terms, modulo"
P95-1029,J87-1005,0,\N,Missing
W04-2704,S01-1001,0,0.0687376,"Missing"
W04-2704,W98-0315,1,0.884365,"Missing"
W04-2704,kingsbury-palmer-2002-treebank,1,0.773736,"ng or annotation. The Proposition Bank is designed as a broad-coverage resource to facilitate the development of more general systems. It focuses on the argument structure of verbs, and provides a complete corpus annotated with semantic roles, including participants traditionally viewed as arguments and adjuncts. Correctly identifying the semantic roles of the sentence constituents is a crucial part of interpreting text, and in addition to forming a component of the information extraction problem, can serve as an intermediate step in machine translation or automatic summarization. 1 PropBank (Kingsbury & Palmer, 2002) is an annotation of the Wall Street Journal portion of the Penn Treebank II (Marcus, 1994) with `predicate-argument&apos; structures, using sense tags for highly polysemous words and semantic role labels for each argument. An important goal is to provide consistent semantic role labels across different syntactic realizations of the same verb, as in the window in [ARG0 John] broke [ARG1 the window] and [ARG1 The window] broke. PropBank can provide frequency counts for (statistical) analysis or generation components in a machine translation system, but provides only a shallow semantic analysis in th"
W04-2704,miltsakaki-etal-2004-penn,1,0.886331,"Missing"
W04-2704,W04-2807,1,\N,Missing
W04-2704,J00-4005,0,\N,Missing
W04-2704,W01-1605,0,\N,Missing
W04-2704,W03-1707,1,\N,Missing
W04-2704,kipper-etal-2004-extending,1,\N,Missing
W04-3111,kingsbury-palmer-2002-treebank,1,\N,Missing
W04-3111,P03-1002,0,\N,Missing
W04-3111,N03-1028,0,\N,Missing
W04-3111,P96-1008,0,\N,Missing
W04-3111,tateisi-tsujii-2004-part,0,\N,Missing
W05-0304,A00-2030,0,0.0185228,"uents, it makes such a mapping easier. The treebank annotation has been modified from the Penn Treebank guidelines in various ways, such as greater structure for prenominal modifiers. Again, while this would have been done regardless of the mapping of entities, it does make such a mapping more successful. Previous work on integrating syntactic structure with entity information, as well as relation infor21 Proceedings of the Workshop on Frontiers in Corpus Annotation II: Pie in the Sky, pages 21–28, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics mation, is described in (Miller et al., 2000). Our work is in much the same spirit, although we do not integrate relation annotation into the syntactic trees. PubMed abstracts are quite different from the newswire sources used in that earlier work, with several consequences discussed throughout, such as the use of discontinuous entities. Section 2 discusses some of the main issues around the development of the guidelines for entity annotation, and Section 3 discusses some of the changes that have been made for the treebank guidelines. Section 4 describes the annotation workflow and the resulting merged representation. Section 5 evaluates"
W06-0609,I05-1081,1,0.820744,"p (NP (NP musical acts) (PP for (NP a new record label))))) .) the original PropBank annotation, a further link is provided, which specifies the relative pronoun as being of “semantic type” answers. (9) Original PropBank annotation: Arg1: [NP *T*-6] -&gt; which -&gt; answers rel: have Arg0: [NP-SBJ *-3] -&gt; we In PropBank, the different pieces of the utterance, including the trace under the verb said, were concatenated This additional link between which and answers is important for many applications that make use of preferences for semantic types of verb arguments, such as Word Sense Disambiguation (Chen & Palmer 2005). In the new annotation scheme, annotators will first label traces as arguments: (12) Original PropBank annotation: ARG1: [ Among other things] [ Mr. Azoff] [ would develop musical acts for a new record label] [ [*T*-1]] ARG0: they rel: said (10) Revised PropBank annotation (stage 1): Rel: have Arg1: [*T*-6] Arg0: [NP-SBJ *-3] Under the new approach, in stage one, Treebank annotation will introduce not a trace of the S clause, but rather *?*, an empty category indicating ellipsis. In stage three, PropBank annotators will link this null element to the S node, but the resulting chain will not be"
W06-0609,H94-1020,1,0.457429,"o tasks: part-of-speech tagging and syntactic annotation. The first task is to provide a part-of-speech tag for every token. Particularly relevant for PropBank work, verbs in any form (active, passive, gerund, infinitive, etc.) are marked with a verbal part of speech (VBP, VBN, VBG, VB, etc.). (Marcus, et al. 1993; Santorini 1990) The syntactic annotation task consists of marking constituent boundaries, inserting empty categories (traces of movement, PRO, pro), showing the relationships between constituents (argument/adjunct structures), and specifying a particular subset of adverbial roles. (Marcus, et al. 1994; Bies, et al. 1995) Constituent boundaries are shown through syntactic node labels in the trees. In the simplest case, a node will contain an entire constituent, complete with any associated arguments or modifiers. However, in structures involving syntactic movement, sub-constituents may be displaced. In these cases, Treebank annotation represents the original position with a trace and shows the relationship as co-indexing. In (1) below, for example, the direct object of entail is shown with the trace *T*, which is coindexed to the WHNP node of the question word what. Introduction The PropBan"
W06-0609,J93-2004,0,0.0290163,"ismatches between the syntactic bracketing and the semantic role labeling that can be found, and our plans for reconciling them. 1 1.1 Treebank The Penn Treebank annotates text for syntactic structure, including syntactic argument structure and rough semantic information. Treebank annotation involves two tasks: part-of-speech tagging and syntactic annotation. The first task is to provide a part-of-speech tag for every token. Particularly relevant for PropBank work, verbs in any form (active, passive, gerund, infinitive, etc.) are marked with a verbal part of speech (VBP, VBN, VBG, VB, etc.). (Marcus, et al. 1993; Santorini 1990) The syntactic annotation task consists of marking constituent boundaries, inserting empty categories (traces of movement, PRO, pro), showing the relationships between constituents (argument/adjunct structures), and specifying a particular subset of adverbial roles. (Marcus, et al. 1994; Bies, et al. 1995) Constituent boundaries are shown through syntactic node labels in the trees. In the simplest case, a node will contain an entire constituent, complete with any associated arguments or modifiers. However, in structures involving syntactic movement, sub-constituents may be dis"
W06-0609,J05-1004,1,0.470057,"Missing"
W10-4420,J03-4003,0,0.0613377,"lem of comparing different annotations of the same data, such as determining where gold trees and parser output differ. Another such case is that of comparing inter-annotator agreement files during corpus construction. In both cases the typical need is to recognize which syntactic structures the two sets of trees are agreeing or disagreeing on. For this purpose it would be useful to be able to state queries in a way that relates to the decisions that annotators actually make, or that a parser mimics. We refer to this earlier work for arguments that (parent, head, sister) relations as in e.g. (Collins, 2003) are not sufficient, and that what is needed is the ability to state queries in terms of small chunks of syntactic structure. The solution we take is to use an extracted tree grammar, inspired by Tree Adjoining Grammar 2 Elementary Tree Extraction The work described and all our examples are taken from the Arabic Treebank, part 3, v3.2 (ATB3-v3.2) (Maamouri et al., 2010). As discussed above, we are aiming for an analysis of the trees that is directly expressed in terms of the core syntactic constructions. Towards this end we utilize ideas from the long line of TAG-based research that aims to id"
W10-4420,N06-1024,1,0.853985,"lication of structure in a typical treebank representation. From the perspective of database organization, the representation of the etree templates can be perhaps be viewed as a type of database “normalization”, in which duplicate tree structure information is placed in a separate table. A significant aspect of this decomposition of the parse output is that the tree decomposition relies upon the presence of function tags to help determine the argument status of nodes, and therefore what should be included in an elementary tree. We therefore use a modification of Bikel parser as described in (Gabbard et al., 2006), so that the output contains function tags. However, inaccuracy in the function tag recovery by the parser could certainly affect the formation of the elementary trees resulting from Runs 1 and 2. We do not include empty categories for the parser output, while they are present in the Gold trees.9 There are 929 etree templates in total, combining those for the three versions, with those for Run 1 and Run 2 overlapping almost entirely. The extracted tokens, etree templates, etree instances, and derivation trees are stored in a MySQL database for later search. The derivation tree is implemented"
W10-4420,N10-1094,1,0.728979,"e elementary trees, can be used as part of this analysis of parallel annotations over the same text. Recent work has proposed the use of an extracted tree grammar as the basis for treebank analysis, in which queries are stated over the elementary trees, which are small chunks of syntactic structure. In this work we integrate search over the derivation tree with this approach in order to analyze differences between two sets of annotation on the same text, an important problem for parser analysis and evaluation of inter-annotator agreement. 1 Introduction In earlier work (Kulick and Bies, 2009; Kulick and Bies, 2010; Kulick et al., 2010) we have described the need for a treebank search capability that compares two sets of trees over the same tokens. Our motivation is the problem of comparing different annotations of the same data, such as determining where gold trees and parser output differ. Another such case is that of comparing inter-annotator agreement files during corpus construction. In both cases the typical need is to recognize which syntactic structures the two sets of trees are agreeing or disagreeing on. For this purpose it would be useful to be able to state queries in a way that relates to t"
W10-4420,C88-2147,0,0.398666,"ull tree is shown in Figure 1, and the extracted elementary trees2 and derivation tree in Figure 2. (The ˆ symbol at the node NP[t]-SBJ in tree #1 indicates that it is a substitution node.) The extracted trees are the four trees numbered #1–#4. These trees are in effect the nodes in the derivation tree showing how the four elementary trees connect to each other. We briefly mention three unusual features of this extraction, and refer the reader to (Kulick and Bies, 2009) for detail and justification.3 Figure 2: Elementary Trees and Derivation Tree for the Tree Decomposition in Figure 1 system (Vijay-Shanker and Joshi, 1988) to handle function tags. 2. Etree #2 consists of two anchors, rather than splitting up the tree decomposition further. This is because this is an instance of the idafa (”construct state”) construction in Arabic, in which two or more words are grouped tightly together. 3. During the extraction process, additional information is added to the nodes in some cases, as further attributes for the “top” and “bottom” information, parallel to the function tag information. In this case, the root of etree #2 has the “bottom” attribute IDAFATOP, meaning that it is the top of an idafa structure, and the lo"
W10-4420,P00-1058,0,\N,Missing
W13-2301,P11-2062,1,0.825864,"s, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknown/malformed annotations with"
W13-2301,N13-1049,1,0.895093,"Missing"
W13-2301,N13-1066,1,0.784259,"currently considers words out of context, such a correction is not preferred because it requires more character edits (see Figure 2). We acknowledge this to be a limitation and plan to address it in the future. 11 The Levenshtein edit distance is defined as the minimum number of single-character edits (insertion, deletion and substitution) required to change one string into the other. For Arabic words and morphemes, we modify the cost of substitutions involving two phonologically or orthographically similar letters to count as half edits. We acquire the list of such letter substitutions from Eskander et al. (2013), who report them as the most frequent source of errors in Egyptian Arabic orthography. We map all diacritic-only morphemes to empty morphemes in both ways at a cost of half edit also. For POS tag edit distance, we use the standard definition of Levenshtein edit distance. Edit cost is an area where a lot of tuning could be done and we plan to explore it in the future. R AW Analysis D IAC M ORPH POS L EM Incorrect Annotation hayiÂaj∼iluh ha+yi+Âaj∼il+uh ñÊg. AJ ë hyAjlw Correct Annotation HayiÂaj∼iluwA Ha+yi+Âaj∼il+uwA FUT_PART+IV3MS+IV+IVSUFF_SUBJ:P FUT_PART+IV3P+IV+IVSUFF_SUBJ:P Âaj∼ill1 Âaj∼"
W13-2301,P05-1071,1,0.660451,"to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknown/malformed annotations with the morphological analyzer they use, thus forcing a reading on the word to make the unknown/malformed annotation usable. In our work, we address the cleaning issue directly. We intend to make these automatic corrections and extensions available in the future so that they can be used in future disambiguation tools. Maamouri et al. (2009) describ"
W13-2301,P06-1086,1,0.865193,"Missing"
W13-2301,habash-etal-2012-conventional,1,0.831293,"y and affixationally, and several classes of attachable clitics. For example, the Egyptian Arabic word AëñJ.JºJ ëð wi+ha+yi-ktib-uw+hA3 ‘and they will write it’ has two proclitics (+ ð wi+ ‘and’ and + è ha+ ‘will’), one prefix - ø yi- ‘3rd person’, one suffix ð- -uw ‘masculine plural’ and one pronominal enclitic Aë+ +hA ‘it/her’. The word is considered an inflected form of the lemma katab ‘write [lit. he wrote]’. An important challenge for NLP work on dialectal Arabic in general is the lack of an orthographic standard. Egyptian Arabic writers are often inconsistent even in their own writing (Habash et al., 2012a), e.g., the future particle h Ha appears as a separate word or as a proclitic + h/+ë Ha+/ha+, reflecting different pronunciations. Arabic orthography in general drops diacritical marks that mark short vowels and gemination. However in analyses, we want these diacritics to be indicated. Moreover, some letters in Arabic (in general) are often spelled inconsistently which leads to an increase in both sparsity (multiple forms of the same word) and ambiguity (same form corresponding to multiple words), e.g., vari ˇ are often writants of Hamzated Alif, @ Â or @ A, ten without their Hamza ( Z ’): @"
W13-2301,W12-2301,1,0.867863,"y and affixationally, and several classes of attachable clitics. For example, the Egyptian Arabic word AëñJ.JºJ ëð wi+ha+yi-ktib-uw+hA3 ‘and they will write it’ has two proclitics (+ ð wi+ ‘and’ and + è ha+ ‘will’), one prefix - ø yi- ‘3rd person’, one suffix ð- -uw ‘masculine plural’ and one pronominal enclitic Aë+ +hA ‘it/her’. The word is considered an inflected form of the lemma katab ‘write [lit. he wrote]’. An important challenge for NLP work on dialectal Arabic in general is the lack of an orthographic standard. Egyptian Arabic writers are often inconsistent even in their own writing (Habash et al., 2012a), e.g., the future particle h Ha appears as a separate word or as a proclitic + h/+ë Ha+/ha+, reflecting different pronunciations. Arabic orthography in general drops diacritical marks that mark short vowels and gemination. However in analyses, we want these diacritics to be indicated. Moreover, some letters in Arabic (in general) are often spelled inconsistently which leads to an increase in both sparsity (multiple forms of the same word) and ambiguity (same form corresponding to multiple words), e.g., vari ˇ are often writants of Hamzated Alif, @ Â or @ A, ten without their Hamza ( Z ’): @"
W13-2301,N13-1044,1,0.931926,"ust the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknown/malformed annotations with the morphological analyzer they use, thus forcing a reading on the word to make the unknown/malformed annotation usable. In our work, we address the cleaning issue directly. We intend to make these automatic corrections and extensions available in the future so that they can be used in future disambiguation tools. Maamouri et al. (2009) described a set of manual and automatic techniques used to improve on the quality of"
W13-2301,I08-2131,0,0.0212473,"on 6 presents some statistics on the Egyptian Arabic corpus annotated in one unified representation resulting from our correction and extension work. 1 ARZ is the language code for Egyptian Arabic, http://www-01.sil.org/iso639-3/ documentation.asp?id=arz 2 Related Work Much work has been done on automatic spelling correction. Both supervised and unsupervised approaches have been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependen"
W13-2301,N06-2015,0,0.0158196,"g a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “synchronize” unknow"
W13-2301,W06-1648,0,0.0236005,"Arabic corpus annotated in one unified representation resulting from our correction and extension work. 1 ARZ is the language code for Egyptian Arabic, http://www-01.sil.org/iso639-3/ documentation.asp?id=arz 2 Related Work Much work has been done on automatic spelling correction. Both supervised and unsupervised approaches have been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological inf"
W13-2301,J93-2004,0,0.0472249,"urces developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise. We present two methods for automatic correction and extension of morphological annotations, and demonstrate their success on three divergent Egyptian Arabic corpora. 1 Introduction Annotated corpora are essential for most research in natural language processing (NLP). For example, the development of treebanks, such as the Penn Treebank and the Penn Arabic Treebank, has been essential in pushing research on partof-speech (POS) tagging and parsing of English and Arabic (Marcus et al., 1993; Maamouri et al., 2004). The creation of such resources tends to be quite expensive and time consuming: guidelines need to be developed, annotators hired, trained, and regularly evaluated for quality control. For languages with complex morphologies, limited resources and tools, and/or lack of standard grammars, such as any of the Dialectal Arabic (DA) varieties, developing annotated resources can be a challenging task. As a result, annotated resources developed under time/money constraints for such languages tend to tradeoff depth of representation with degree of noise. In the extremes, we fi"
W13-2301,mohamed-etal-2012-annotating,0,0.0579208,"tations: corrections of rich noisy annotations and extensions of clean but shallow ones. We present our work on Egyptian Arabic, an important Arabic dialect with limited resources, and rich and ambiguous morphology. Resulting from this effort is the largest Egyptian Arabic corpus annotated in one common representation by pooling resources from three very different sources: a non-final, pre-release version of the ARZ1 corpora from the Linguistic Data Consortium (LDC) (Maamouri et al., 2012g), the LDC’s CallHome Egypt transcripts (Gadalla et al., 1997) and CMU’s Egyptian Arabic corpus (CMUEAC) (Mohamed et al., 2012). Although the paper focuses on Arabic, the basic problem is relevant to other languages, especially spontaneously written colloquial language forms such as those used in social media. The general solutions we propose are language independent given availability of specific language resources. Next we discuss some related work and relevant linguistic facts (Sections 2 and 3, respectively). Section 4 presents our annotation correction technique; and Section 5 presents out annotation extension technique. Finally, Section 6 presents some statistics on the Egyptian Arabic corpus annotated in one un"
W13-2301,J96-1003,0,0.0812661,"n extension technique. Finally, Section 6 presents some statistics on the Egyptian Arabic corpus annotated in one unified representation resulting from our correction and extension work. 1 ARZ is the language code for Egyptian Arabic, http://www-01.sil.org/iso639-3/ documentation.asp?id=arz 2 Related Work Much work has been done on automatic spelling correction. Both supervised and unsupervised approaches have been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with t"
W13-2301,palmer-etal-2008-pilot,1,0.772222,"ve been used employing a variety of tools, resources, and heuristics, e.g., morphological analyzers, language models, annotated data and edit-distance measures, respectively (Kukich, 1992; Oflazer, 1996; Shaalan et al., 2003; Hassan et al., 2008; Kolak and Resnik, 2002; Magdy and Darwish, 2006). Our work is different from these approaches in that it extends beyond spelling of word forms to deeper annotations. However, we use some of these techniques to correct not just the words, but also malformed POS tags. A number of efforts exist on treebank enrichment for many languages including Arabic (Palmer et al., 2008; Hovy et al., 2006; Alkuhlani and Habash, 2011; Alkuhlani et al., 2013). Our morphological extension effort is similar to Alkuhlani et al. (2013)’s work except that they start with tokenizations, reduced POS tags and dependency trees and extend them to full morphological information. There has been a lot of work on Arabic POS tagging and morphological disambiguation (Habash and Rambow, 2005; Smith et al., 2005; Hajiˇc et al., 2005; Habash, 2010; Habash et al., 2013). The work by Habash et al. (2013) uses one of the resources we improve on in this paper. In their work, they simply attempt to “"
W13-2301,H05-1060,0,0.0446191,"Missing"
W14-2904,N13-1061,1,0.891051,"Missing"
W15-0809,doddington-etal-2004-automatic,1,0.296897,"nre NW DF NW DF Documents 77 74 101 99 Tokens 44,962 70,427 50,997 169,740 Table 1. Event Nugget Data Profile While the Light ERE and KBP Event Argument tasks rely on character offsets for annotation and scoring, the Event Nugget Tuple Scorer 2 (Liu, Mitamura & Hovy, 2015) requires tokenized data. Therefore, prior to annotation, all selected documents were automatically tokenized in the Penn English Treebank style. No manual correction was performed on the tokenization due to time constraints. 8 8.1 Corpus and Consistency Analysis Corpus Experience with event annotation for Light ERE and ACE (Doddington et al., 2004) and related tasks suggests that a major challenge for annotation consistency is poor recall – human annotators are not highly consistent in recognizing that a mention has occurred. To reduce the impact of this known issue for the Event Nugget task, two anno2 Event Nugget Tuple refers to the tuple made up of the nugget, event type/subtype, and realis. tators independently labeled each document (two first pass annotation passes, referred to as FP1 and FP2 below); a senior annotator then adjudicated discrepancies to create a gold standard. The team consisted of four first pass annotators, two of"
W15-0809,W15-0807,1,0.678904,"ed improvement in annotation quality in the workflow by comparing the adjudicated (ADJ) and first (FP1 and FP2) passes, shown in the columns ADJ vs. FP1 and FP2 in Table 3. The noticeable improvement in score shows the advantage of including adjudication as part of the annotation process. (For IAA purposes, there is obviously no gold or system, but in order to use the scorer we arbitrarily treated one file as the “gold”.) FP1 vs. FP2 Consistency Analysis We examined annotation consistency and quality by comparing different passes of the eval set annotation using the Event Nugget Tuple Scorer (Liu, Mitamura, & Hovy, 2015) developed for the event nugget evaluation task. This scorer treats one file as “gold” and the other as “system”, and matches each nugget in the gold file to one or more nuggets in the system file. This mapping is based on the overlap of the nugget spans. By nugget span, we 3 16 event nuggets in the training set did not receive a realis attribute, due to annotation error. 72 ADJ vs. FP1 78.2 71.7 63.2 ADJ vs. FP2 Span 69.0 89.3 Type 68.2 84.3 Realis 60.0 85.7 Table 3. Scores for Event Nugget Eval Set Annotation To gain some further insight into these numbers we expanded the analysis in two di"
W15-0809,E12-2021,0,\N,Missing
W15-0812,bies-etal-2014-incorporating,1,0.734052,"r Light ERE annotation effort also includes creating fully annotated resources in Chinese and Spanish in addition to English, with a portion of the annotation being cross-lingual. We developed a Chinese-English parallel Light ERE corpus which consists of approximately 100K words of Chinese data along with the corresponding English translation, both annotated in Light ERE. Portions of the parallel data have had other layers of annotation performed on it, particularly Chinese Treebank (CTB) on the Chinese side (Zhang and Xue, 2012) as well as English-Chinese Treebank (ECTB) on the English side (Bies et al., 2014). Light ERE annotation is in progress for Spanish on a dataset which is currently being annotated for Spanish Treebank as well. Multiple levels of annotation, such as ERE and treebank, that are keyed to the same dataset should together provide a resource that is expected to facilitate experimentation with machine learning methods that jointly manipulate the multiple levels. 2.2 TAC KBP Event Evaluations The Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards and Technology (NIST) that was developed to encourage research in natural language p"
W15-0812,doddington-etal-2004-automatic,1,0.813222,"nships among them. Given the variety of approaches and evaluations within DEFT, we set out to define an annotation task that would be supportive of multiple research directions and evaluations, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased a"
W15-0812,W14-2904,1,0.767479,"to be met by the end of this year. 4.1 Smart Data Selection In an attempt to minimize annotator effort on documents with insufficient content, documents were fed into the annotation pipeline in descending order of event trigger density, defined as the number of event triggers per 1,000 tokens. Triggers were automatically tagged using a deep neural network based tagger trained on the ACE 2005 annotations (Walker et al., 2006) with orthographic and word 96 Rich ERE Challenges and Next Steps Inter-Annotator Agreement Work on inter-annotator agreement (IAA) will be based on the method outlined in Kulick et al. (2014), which described a matching algorithm used at each level of the annotation hierarchy, from entity mentions to events. This work focused on the evaluation for entity, relation, and event mentions, as well as for entities overall. The algorithm for entity mention mapping is based on the span for an entity mention, while the mapping for relation and event mentions is more complex, based on the mapping of the arguments, which in turn depends on the entity mention mapping. IAA work will be conducted on dual annotation for Rich ERE. Analysis will be reported in the future. 5 Conclusion Rich ERE ann"
W15-0812,W15-0809,1,0.3802,"collection and add it to a new or existing knowledge base. In 2014, TAC KBP moved into the events domain with the addition of the Event Argument Extraction (EAE) evaluation, in which systems were required to extract mentions of entities from unstructured text and indicate the roles they played in events as supported by text (Ellis et al., 2014). Additionally, TAC KBP 2014 also conducted a pilot evaluation on Event Nugget Detection (END), in which systems were required to detect event nugget tuples, consisting of an event trigger, the type and subtype classification, and the realis attribute (Mitamura et al., 2015). TAC KBP 2015 EAE and END evaluations both plan to expand the tasks such that event tuples would be grouped together or linked to one another to show event identity, either by linking event arguments that participate in the same event (EAE) or by grouping event nuggets that refer to the same event (END). Such expansion in both evaluations would require identification of event coreference, which is a challenging issue in both ACE and Light ERE. The transition from Light ERE to Rich ERE tackles this challenge with the addition of event hoppers. 3 Transition from Light ERE to Rich ERE The simpli"
W15-0812,J14-4007,0,0.017686,"ons, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased approach, starting with a fairly lightweight implementation and introducing additional complexity over time. In the first phase of the program, we defined Light ERE as a simplified form"
W15-0812,W11-0419,0,0.0236293,"election process have been very encouraging, with annotators reporting much richer documents on average, compared to the prior approach in which no ranking was imposed. 4.2 One of the challenges in event annotation is to determine the level of granularity that will be distinguished as sub-event vs. event hopper. We observed this issue in our pilot Rich ERE annotation, and the goal is to have sub-event annotation be a relationship between event hoppers in the future. In order to represent the relations between event hoppers, we are planning the addition of a notion such as Narrative Container (Pustejovsky and Stubbs, 2011) to capture non-identity eventevent relations such as causality, part-whole, precedence, enablement, etc. Event hoppers will serve as a level between individual event mentions and Narrative Containers. Event hoppers will be grouped into Narrative Containers, and so relations will be between event hoppers, instead of between individual event mentions. More specific relations between individual event mentions can then be derived from the event-event relations between the event hoppers within narrative containers or from relations between narrative containers. 4.3 The overall target for this phas"
W15-0812,strassel-etal-2010-darpa,1,0.908256,"notation task that would be supportive of multiple research directions and evaluations, and that would provide a useful foundation for more specialized annotation tasks like inference and anomaly. The resulting Entities, Relations and Events (ERE) annotation task has evolved over the course of the program, from a fairly lightweight treatment of entities, relations and events in text, to a richer representation of phenomena of interest to the program. While previous approaches such as ACE (Doddington et al., 2004), LCTL (Simpson et al., 2008), OntoNotes (Pradhan et al., 2007), Machine Reading (Strassel et al., 2010), TimeML (Boguraev and Ando, 2005), Penn Discourse Treebank (Prasad et al., 2014), and Rhetorical Structure Theory (Mann and Thompson, 1988) laid some of the groundwork for this type of resource, the DEFT program requires annotation of complex and hierarchical event structures that go beyond any of the existing (and partially-overlapping) task definitions. Recognizing the effort required to define such an annotation task for multiple languages and genres, we decided to adopt a multi-phased approach, starting with a fairly lightweight implementation and introducing additional complexity over ti"
W15-0812,wright-etal-2012-annotation,1,0.855178,"ent hopper in Rich ERE, and all tagged event mentions that refer to the same event occurrence will be grouped into the same event hopper. Event hoppers will allow annotators to group together more event mentions and therefore also label more event arguments in Rich ERE. This richer annotation will lead to a more complete knowledge base and better support for the Event Argument Linking and END evaluations in 2015, when one of the goals is to evaluate event identity. 3.2 Development of an Annotation GUI for Rich ERE The Rich ERE annotation tool was developed following the framework described in Wright et al. (2012), allowing for rapid development of a new interface for Rich ERE. Numerous features were included “for free” in that they were developed for previous interfaces, and therefore required no additional development time. One important example of this is the representation of annotated text extents with underlines that can overlap arbitrarily, be color coded based on other annotations (e.g., entity type), and allow the user to click to navigate among the annotations. An important feature developed specifically for the Rich ERE tool is a “reference annotation”, which is essentially one widget pointi"
W15-0812,W12-6306,0,0.0230474,"ACE, only attested actual events are annotated (no irrealis events or arguments). Our Light ERE annotation effort also includes creating fully annotated resources in Chinese and Spanish in addition to English, with a portion of the annotation being cross-lingual. We developed a Chinese-English parallel Light ERE corpus which consists of approximately 100K words of Chinese data along with the corresponding English translation, both annotated in Light ERE. Portions of the parallel data have had other layers of annotation performed on it, particularly Chinese Treebank (CTB) on the Chinese side (Zhang and Xue, 2012) as well as English-Chinese Treebank (ECTB) on the English side (Bies et al., 2014). Light ERE annotation is in progress for Spanish on a dataset which is currently being annotated for Spanish Treebank as well. Multiple levels of annotation, such as ERE and treebank, that are keyed to the same dataset should together provide a resource that is expected to facilitate experimentation with machine learning methods that jointly manipulate the multiple levels. 2.2 TAC KBP Event Evaluations The Text Analysis Conference (TAC) is a series of workshops organized by the National Institute of Standards a"
W15-0812,W14-2907,1,\N,Missing
W19-6808,L18-1293,0,0.0456535,"Missing"
W19-6808,kulick-etal-2010-consistent,1,0.673071,"Missing"
