2003.mtsummit-papers.53,W03-1502,1,0.772759,"re NEs have been manually or automatically annotated. Starting from a bilingual corpus where NEs are automatically tagged for each language, NE pairs are aligned in order to minimize a multi-feature alignment cost including the transliteration cost, the NE tagging cost, and word-based translation cost. These features are designed to capture the semantic or phonetic similarities between NE pairs as well as NE tagging confidence, and are derived from several information sources using unsupervised and partly supervised methods. A greedy search algorithm is applied to minimize the alignment cost (Huang et al., 2003). Online NE translation is specially designed for translating NEs which appear in the given test document, but are not covered by the Offline translation. The missing source NEs and target NE translations are “retrieved” cross-lingually from topic-relevant documents (w.r.t. the test document). Relevant documents are retrieved from a monolingual corpus using a 1st-pass translation of the test document as the query. NEs in the retrieved documents are extracted and aligned with source NEs according to their transliteration cost. The NE pairs with minimum transliteration cost are considered as tra"
2003.mtsummit-papers.53,J93-2003,0,0.119946,"ical machine translation system. This system combines phrase-tophrase translations extracted from a bilingual corpus using different alignment approaches. Special methods to extract and align named entities are used. We show how a manual lexicon can be incorporated into the statistical system in an optimized way. Experiments on Chinese-toEnglish and Arabic-to-English translation tasks are presented. 1 Introduction Statistical machine translation is currently the most promising approach to large vocabulary text translation. In the spirit of the Candide system developed in the early 90s at IBM (Brown et al., 1993), a number of statistical machine translation systems have been presented in the last few years (Wang and Waibel, 1998), (Och and Ney, 2000), (Yamada and Knight, 2000). These systems share the basic underlying principles of applying a translation model to capture the lexical and word reordering relationships between two languages, complemented by a target language model to drive the search process through translation model hypotheses. Their primary differences lie in the structure and source of their translation models. Whereas the original IBM system was based on purely word-based translation"
2003.mtsummit-papers.53,W02-1018,0,0.110892,"Missing"
2003.mtsummit-papers.53,P03-1041,1,0.837652,"Missing"
2003.mtsummit-papers.53,C96-2141,1,0.3498,"four different approaches to phrase pair extraction, each of which will be described below. We also describe our technique for adding generalization power by allowing for overlapping phrases. 2.1 From Viterbi Path of HMM Word Alignment A simple approach to extract phrase translations from a bilingual corpus is to harvest the Viterbi path generated by a word alignment model. A number of probabilistic word alignment models have been proposed (Brown et al., 1993) (Och and Ney, 2000) and shown to be effective for statistical machine translation. We use the HMM-based alignment model introduced in (Vogel et al., 1996) which estimates position alignment probabilities in addition to lexical probabilities. The HMM-based alignment model is based on relative positions: it addresses the likelihood that the word at source position j +1 is aligned to target position i0 when source position j is aligned to target position i. The Viterbi path can be used not only to map source words to target words, i.e. building a statistical lexicon, but also to map source phrases to target phrases. For each source phrase ranging from positions j1 to j2 the corresponding target phrase is given by imin = minj {i = a(j)} and imax ="
2003.mtsummit-papers.53,P00-1004,1,0.851861,"Missing"
2003.mtsummit-papers.53,J97-3002,0,0.054098,"ng from positions j1 to j2 the corresponding target phrase is given by imin = minj {i = a(j)} and imax = maxj {i = a(j)}, where j = j1 ...j2 . This is a very simple criterion which does not test if the source phrase actually aligns to two or more noncontiguous sequences of words in the target sentence. Due to the potential for alignment errors, such a test would be unreliable. However, by preventing the length of the aligned target phrase from exceeding the length of the source phrase by a given factor, the problem of non-contiguous alignments can be reduced. 2.2 From Bilingual Bracketing In (Wu, 1997) a word alignment model was proposed which adds additional alignment restrictions over the IBM-style alignment models. The bilingual bracketing builds an hierarchical alignment, which can be viewed as a simple top-down bilingual parse: split source and target segment into two halves f˜l , f˜r and e˜l , e˜r . Then either align f˜l to e˜l and f˜r to e˜r , which is called a straight alignment, or align f˜l to e˜r and f˜r to e˜l , called a reversed alignment. Repeat this for each aligned segment pair down to the word level. At each level the optimization is over the split points and the direction,"
2003.mtsummit-papers.53,W03-0303,1,0.85137,"Missing"
2003.mtsummit-papers.53,P02-1040,0,\N,Missing
2003.mtsummit-papers.53,P01-1067,0,\N,Missing
2003.mtsummit-papers.53,P00-1056,0,\N,Missing
2016.amta-researchers.8,D11-1033,0,0.260007,"high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity betwee"
2016.amta-researchers.8,W15-3003,0,0.0599708,"ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domai"
2016.amta-researchers.8,K16-1031,1,0.892142,"15) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain and four language directions, show that this SSCNN method yields signiﬁcantly higher BLEU scores for the"
2016.amta-researchers.8,W12-3131,0,0.0978263,"an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO"
2016.amta-researchers.8,P14-1129,0,0.0302692,"a subset of data to be used for training an SMT system from a bilingual corpus, the user must specify the number N of sentence pairs to be chosen. The N sentence pairs with the highest global scores S(s, t) will be selected. This method is symmetrical - the roles of the source-language and target-language sides of the corpus are the same - and bilingual, because the IBM model 1 measures the degree to which each target sentence t is a good translation of its partner s, and vice versa. 2.2 Data Selection with Neural Net Joint Model (NNJM) The Neural Network Joint Model (NNJM), as described in (Devlin et al., 2014), is a joint language and translation model based on a feedforward neural net (NN). It incorporats a wide span of contextual information from the source sentence, in addition to the traditional n-gram information from preceding target-language words. Speciﬁcally, when scoring a target word wi , the NNJM inputs not only the n − 1 preceding words wi−n+1 , ..., wi−1 , but also 2m + 1 source words: the source word si most closely aligned with wi along with the m source words si−m , ..., si−1 to the left of si and the m source words si+1 , ..., si+m to the right of si . The NNJMs used in our experi"
2016.amta-researchers.8,P13-2119,0,0.133955,"picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language mod"
2016.amta-researchers.8,2015.mtsummit-papers.10,0,0.686015,"of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT system. Experiments described in that paper, covering three different types of test domain"
2016.amta-researchers.8,2012.amta-papers.7,1,0.926825,"Missing"
2016.amta-researchers.8,2010.eamt-1.26,0,0.0690526,"ent over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URF"
2016.amta-researchers.8,N15-1011,0,0.0239437,"de up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al"
2016.amta-researchers.8,P14-1062,0,0.00850774,"on and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts"
2016.amta-researchers.8,D14-1181,0,0.00555345,"pair is made up of equal contributions from these four models. Since this metric contains information about the translation relationship between each source sentence and its target counterpart, and since the ways in which the source and target languages are used are mirror images of each other, the NNJM data selection method is both bilingual and symmetrical. 2.3 Data Selection with Semi-Supervised CNN As described in more detail in (Chen and Huang, 2016), we were inspired by the success of convolutional neural networks (CNNs) applied to image and text classiﬁcation (Krizhevsky et al., 2012; Kim, 2014; Johnson and Zhang, 2015a,b) to use CNNs to classify training sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation"
2016.amta-researchers.8,W04-3250,0,0.208233,"f-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property for Arabic-to-English. In the table, the bilingua"
2016.amta-researchers.8,D07-1036,0,0.0723693,"Missing"
2016.amta-researchers.8,2011.iwslt-papers.5,0,0.0415047,"_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language model perplexity between two language models trained on in-domain and out-domain data, respectively. Furthermore, (Duh et al., 2013) extends these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). While some previous work considers achieving the two goals simultaneously, such as (Mansour et al., 2011) which uses IBM model 1 and a language model to do data selection, (Durrani et al., 2015) uses a neural network joint model to select the in-domain data. In a recent paper (Chen and Huang, 2016), we describe one type of neural network for carrying out data selection: a semi-supervised Convolutional Neural Network (SSCNN) that is trained on the in-domain set to score one side of each sentence in a general-domain bilingual corpus (either the source side or the target side) for its suitability as training material for an SMT system. The highest-scoring sentence pairs are chosen to train the SMT s"
2016.amta-researchers.8,P10-2041,0,0.263994,"election, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the difference in language m"
2016.amta-researchers.8,J05-4003,0,0.19056,"pairs, can beneﬁt NMT much more than SMT.We observed a BLEU improvement over 3 points on an English-to-French WMT task when Bi-SSCNNs were used. 1 Introduction When building a statistical machine translation (SMT) system, it is important to choose bilingual training data that are of high quality 1 and that are typical of the domain in which the SMT system will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-lang"
2016.amta-researchers.8,W11-2124,0,0.186448,"nce. Essentially, it scores the extent to which both the source and target sentence are in-domain, but does not in any way penalize bad translations. We say that such a method is “symmetric”: it incorporates equal amounts of information from the source and the target language, but it is not “bilingual”: it does not incorporate information about the quality of translations. The main motivation for this paper is to explore CNN-based data selection techniques that are bilingual. It is based on semi-supervised CNNs that use bitokens as units instead of source or target words (Marino et al., 2006; Niehues et al., 2011). For the bitoken semi-supervised CNN, we should use the abbreviation “Bi-SSCNN”. We also experiment with the bilingual method that combines IBM model 1 and language model (LM) scores and neural network joint model. In this paper, we carried out experiments reported on two language pairs: Chinese-toEnglish and Arabic-to-English. We ﬁx the number of training sentences to be chosen for the data selection techniques so that they can be fairly compared, and measure the BLEU score on test data from the resulting MT systems. It turns out that three techniques have roughly the same performance in ter"
2016.amta-researchers.8,P02-1040,0,0.0972421,"ence as the criterion. This is considered to be a state-of-the-art data selection method for domain adaptation (Axelrod et al., 2011). The “sum LM” variant uses the sum of the source and target LM scores for a sentence pair. 2. SSCNN: Data selection by semi-supervised CNN based on monolingual tokens (Section 2.3) 3. IBM-LM: Data selection by both IBM and language models (Section 2.1) 4. NNJM: Data selection by neural network joint models (Section 2.2) 5. Bi-SSCNN: Data selection by bitoken based semi-supervised CNN (Section 2.4) 3.3 Experimental results We evaluated the system using the BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we apply the bootstrap resampling test to do signiﬁcance testing. Table 2 summarizes the results for each task. The number of selected sentence pairs for each language pair (1.8 million pairs for Chinese-to-English, and 1.4 million pairs for Arabic-to-English) was decided on the basis of tests on held-out data using the IBM-LM method. That is, 1.8 million was the value of N that maximized the BLEU score of the ﬁnal SMT system when IBM-LM was used to select N sentence pairs as training data for Chinese-to-English, and 1.4 had the same property fo"
2016.amta-researchers.8,W16-2323,0,0.0611686,"Missing"
2016.amta-researchers.8,2014.amta-researchers.3,1,0.757845,"h SSCNNs that take as input the bitokens of (Marino et al., 2006; Niehues et al., 2011). 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S Figure 2: Bitoken sequence. The paper (Niehues et al., 2011) describes a “bilingual language model” (biLM): the idea that SMT systems would beneﬁt from wider contextual information from the source sentence. BiLMs provide this context by aligning each target word in the training data with source words to create bitokens. An n-gram bitoken LM for the sequence of target words is then trained. Figure 2 (taken from (Stewart et al., 2014)) shows how a bitoken sequence is obtained from a word-aligned sentence pair for the English to French language pair. Unaligned target words (e.g., French word “d´’’ in the example) are aligned with NULL. Unaligned source words (e.g., “very”) are dropped. A source word aligned with more than one target word (e.g., “we”) aligned with two instances of “nous” is duplicated: each target word aligned with it receives a copy of that source word. The word embeddings for bitokens are learned directly by word2vec, treating each bitoken as a word. For instance, in the French sentence shown in Figure 2,"
2016.amta-researchers.8,P15-2058,0,0.0114968,"ng sentences in either the source language or the target language as in-domain or out-of-domain. Convolutional neural networks (CNNs) (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolutional and pooling layers; each computation unit of the convolutional layer processes a small region of the input data. When CNNs are applied to text input, the convolution layers process small regions of a document, i.e., a sequence of words. CNNs are now used in many text classiﬁcation tasks (Kalchbrenner et al., 2014; Johnson and Zhang, 2015b; Wang et al., 2015). Chen and Huang (2016) use CNNs to classify sentence pairs to in-domain and out-of-domain sentence pairs. In many of these studies, the ﬁrst layer of the network converts words to word embeddings using table lookup; the embeddings are sometimes pre-trained on an unnlabeled data. The embeddings remain ﬁxed during subsequent model training. A CNN trained with small number of labled data and pre-trained word embeddings on large unlabeled data is termed “semi-supervised”. Because we were interested in data selection scenarios where only small amounts of in-domain data are available, we chose to u"
2016.amta-researchers.8,I08-2088,0,0.0573278,"e two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for bilingual parallel sentences using the d"
2016.amta-researchers.8,C04-1059,0,0.0497328,"will operate. In previous work, these two goals of data selection, i.e., picking high-quality data and picking data that ensure the SMT system is well-adapted to a given domain, have often been achieved separately. For instance, the papers (Munteanu and Marcu, 2005; Khadivi and Ney, 2005; Okita et al., 2009; Jiang et al., 2010; Denkowski et al., 2012) focus on reducing the noise in the data. They use different scoring functions, such as language model perplexity, word alignment score, or IBM model 1 score, to score each sentence pair, top scored sentence pairs are selected. While the papers (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) focus on domain adaptation. They all select monolingual or bilingual data that 1 Within each sentence pair, the target-language sentence is a good translation of the source sentence 3URFHHGLQJVRI$07$YRO075HVHDUFKHUV 7UDFN $XVWLQ2FW1RY_S are similar to the in-domain data according to some criterion. These state-of-the-art adaptive data selection approaches (Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015) search for b"
2020.acl-main.304,Q17-1010,0,0.0288087,"dings are obtained by average pooling. We feed the token embeddings into the BiLSTM-CRF for decoding. The hidden size of the BiLSTM layer is 256 for the monolingual teacher models and 600 or 800 for the multilingual student model depending on the dataset as larger hidden size for the multilingual model results in better performance in our experiment. The settings of teacher and student models are as follows: • Monolingual Teachers: Each teacher is trained with a dataset of a specific language. We use M-BERT concatenated with languagespecific Flair (Akbik et al., 2018) embeddings and fastText (Bojanowski et al., 2017) word embeddings as token embeddings2 for all the monolingual teacher models. • Multilingual Student: The student model is trained with the datasets of all the languages combined. We only use M-BERT as token embeddings for the multilingual student model. Training For model training, the mini-batch size is set to 2000 tokens. We train all models with SGD optimizer with a learning rate of 0.1 and anneal the learning rate by 0.5 if there is no improvements on the development set for 10 epochs. For all models, we use a single NVIDIA Tesla V100 GPU for training including the student model. We tune"
2020.acl-main.304,P19-1595,0,0.0318413,"9: end for 10: 11: while S < S do 12: S = S + 1. ˆ do 13: for mini-batch (x, y, pˆ) sampled from D 14: Compute the KD loss LKD (x, pˆ). 15: Compute the golden target loss LNLL (x, y). 16: Compute the final loss L = λLKD + (1 − λ)LNLL . 17: Update θ: θ = θ - η ∗ ∂L/∂θ . 18: if λ − τ > 0 do 19: Update interpolation factor λ: λ = λ − τ 20: else 21: Update interpolation factor λ: λ = 0 22: end if 23: end while learns from the gold targets and pseudo targets in training by optimizing the following loss function: LALL = λLKD + (1 − λ)LNLL where λ decreases from 1 to 0 throughout training following Clark et al. (2019), LKD is one of the Eq. 5, 8, 9, 13 or an averaging of Eq. 9, 13. The overall distillation process is summarized in Algorithm 1. 4 4.1 • CoNLL NER: We collect the corpora of 4 languages from the CoNLL 2002 and 2003 shared task (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) • WikiAnn NER (Pan et al., 2017): The dataset contains silver standard NER tags that are annotated automatically on 282 languages that exist in Wikipedia. We select the data of 8 languages from different language families or from different language subgroups of IndoEuropean languages. We randomly choose 5000 sen"
2020.acl-main.304,N19-1383,0,0.0165646,"n be formulated as sequence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual"
2020.acl-main.304,D19-1672,0,0.0149242,"n be formulated as sequence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual"
2020.acl-main.304,D16-1139,0,0.550108,"model and then trains a weak student model through mimicking the output probabilities (Hinton et al., 2015; Lan et al., 2018; Mirzadeh et al., 2019) or hidden states (Romero 3317 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3317–3330 c July 5 - 10, 2020. 2020 Association for Computational Linguistics et al., 2014; Seunghyun Lee, 2019) of the teacher model. The student model can achieve an accuracy comparable to that of the teacher model and usually has a smaller model size through KD. Inspired by KD applied in neural machine translation (NMT) (Kim and Rush, 2016) and multilingual NMT (Tan et al., 2019), our approach contains a set of monolingual teacher models, one for each language, and a single multilingual student model. Both groups of models are based on BiLSTM-CRF (Lample et al., 2016; Ma and Hovy, 2016), one of the state-of-the-art models in sequence labeling. In BiLSTM-CRF, the CRF layer models the relation between neighbouring labels which leads to better results than simply predicting each label separately based on the BiLSTM outputs. However, the CRF structure models the label sequence globally with the correlations between neighboring label"
2020.acl-main.304,D16-1180,0,0.191514,"Missing"
2020.acl-main.304,N18-1202,0,0.0486673,"Missing"
2020.acl-main.304,P19-1493,0,0.0592419,"Missing"
2020.acl-main.304,D18-1061,0,0.0394979,"Missing"
2020.acl-main.304,P19-1015,0,0.136206,"quence labeling problems and these tasks can provide extra information to many downstream tasks and products such as searching engine, chat-bot and syntax parsing (Jurafsky and Martin, 2009). Most of the previ∗ Kewei Tu is the corresponding author. This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. ous work on sequence labeling focused on monolingual models, and the work on multilingual sequence labeling mainly focused on cross-lingual transfer learning to improve the performance of low-resource or zero-resource languages (Johnson et al., 2019; Huang et al., 2019a; Rahimi et al., 2019; Huang et al., 2019b; Keung et al., 2019), but their work still trains monolingual models. However, it would be very resource consuming considering if we train monolingual models for all the 7,000+ languages in the world. Besides, there are languages with limited labeled data that are required for training. Therefore it is beneficial to have a single unified multilingual sequence labeling model to handle multiple languages, while less attention is paid to the unified multilingual models due to the significant difference between different languages. Recently, Multilingual BERT (M-BERT) (Devlin"
2020.acl-main.304,C18-1327,0,0.0130226,"and multilingual BiLSTM-Softmax model with token-level KD based on Eq. 4 as Softmax and Token for reference. Table 2, 3, and 4 show the effectiveness of our approach on 4 tasks over 25 datasets. In all the tables, we report scores averaged over 5 runs. Observation #0. BiLSTM-Softmax models perform inferior to BiLSTM-CRF models in most cases in the multilingual setting: The results show that the BiLSTM-CRF approach is stronger than the BiLSTM-Softmax approach on three of the four tasks, which are consistent with previous work on sequence labeling (Ma and Hovy, 2016; Reimers and Gurevych, 2017; Yang et al., 2018). The token-level KD approach performs almost the same as the BiLSTM-Softmax baseline in most of the tasks except the Aspect Extraction task. Observation #1. Monolingual teacher models outperform multilingual student models: This is probably because the monolingual teacher models are based on both multilingual embeddings M-BERT and strong monolingual embeddings (Flair/fastText). The monolingual embedding may provide additional information that is not available to the multilingual student models. Furthermore, note that the learning problem faced by a multilingual student model is much more diff"
2020.acl-main.304,W12-1908,0,\N,Missing
2020.acl-main.304,N09-1010,0,\N,Missing
2020.acl-main.304,W03-0419,0,\N,Missing
2020.acl-main.304,N16-1030,0,\N,Missing
2020.acl-main.304,P18-1129,0,\N,Missing
2020.acl-main.304,C18-1139,0,\N,Missing
2020.acl-main.304,W18-6125,0,\N,Missing
2020.acl-main.304,W02-2024,0,\N,Missing
2020.acl-main.304,N19-2023,0,\N,Missing
2020.acl-main.304,N19-1078,0,\N,Missing
2020.acl-main.304,N19-1423,0,\N,Missing
2020.acl-main.304,D19-1441,0,\N,Missing
2020.acl-main.304,D19-1374,0,\N,Missing
2020.acl-main.304,D19-1138,0,\N,Missing
2020.acl-main.713,P15-1017,0,0.0709233,".ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual f"
2020.acl-main.713,Q16-1026,0,0.188115,"alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact t"
2020.acl-main.713,N19-1423,0,0.0415352,"is the node type label. Each edge eij = hi, j, lij i ∈ E is represented similarly, whereas i and j denote the indices of involved nodes. For example, in Figure 2, the trigger “injured” is represented as h7, 7, INJUREi, the entity mention “Kashmir region” is represented as h10, 8000 11, LOCi, and the event-argument edge between them is h2, 3, PLACEi. 3 Approach As Figure 2 illustrates, our O NE IE framework extracts the information network from a given sentence in four steps: encoding, identification, classification, and decoding. We encode the input sentence using a pre-trained BERT encoder (Devlin et al., 2019) and identify entity mentions and event triggers in the sentence. After that, we compute the type label scores for all nodes and pairwise edges among them. During decoding, we explore possible information networks for the input sentence using beam search and return the one with the highest global score. 3.1 Encoding Given an input sentence of L words, we obtain the contextualized representation xi for each word using a pre-trained BERT encoder. If a word is split into multiple word pieces (e.g., Mondrian → Mon, ##dr, ##ian), we use the average of all piece vectors as its word representation. W"
2020.acl-main.713,Q14-1037,0,0.0657049,"Missing"
2020.acl-main.713,P19-1136,0,0.0490037,"th global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum span width. 6 Conclusions and Future Work"
2020.acl-main.713,D19-1041,0,0.0484095,"s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum span width. 6 Conclusions and Future Work We propose a joint end-to-end IE framework that incorporates global features to capture the interdep"
2020.acl-main.713,P05-1051,1,0.749578,"Missing"
2020.acl-main.713,H05-1003,1,0.706275,"Missing"
2020.acl-main.713,P17-1085,0,0.0964285,"s efforts, we propose a joint neural framework with global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum sp"
2020.acl-main.713,P16-4011,0,0.0585262,", Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role"
2020.acl-main.713,D14-1198,1,0.882531,"Missing"
2020.acl-main.713,P16-1200,0,0.0255855,"hich leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual for an ELECT event t"
2020.acl-main.713,D15-1102,0,0.0400079,"n with Global Features Ying Lin1 , Heng Ji1 , Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1"
2020.acl-main.713,N19-1308,0,0.2067,"ctions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual for an ELECT event to have two PERSON arguments. Most existing joint neural models fo"
2020.acl-main.713,D14-1200,0,0.0778197,"r Information Extraction with Global Features Ying Lin1 , Heng Ji1 , Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and"
2020.acl-main.713,W04-2401,0,0.391549,"fies “camp” as a facility, and a 8006 DIE event triggered by “dying” in the following sentence “Russia hints ‘peace camp’ alliance with Germany and France is dying by Dmitry Zaks.”. The IE community is lacking of newer data sets with end-to-end annotations. Unfortunately, the annotation quality of the ACE data set is not perfect due to some long-term debates on the annotation guideline; e.g., Should “government” be tagged as a GPE or an ORG? Should “dead” be both an entity and event trigger? Should we consider designator word as part of the entity mention or not? 5 Related Work Previous work (Roth and Yih, 2004; Li et al., 2011) encodes inter-dependency among knowledge elements as global constraints in an integer linear programming framework to effectively remove extraction errors. Such integrity verification results can be used to find knowledge elements that violate the constraints and identify possible instances of detector errors or failures. Inspired by these previous efforts, we propose a joint neural framework with global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instan"
2020.acl-main.713,P19-1131,0,0.0515604,"hts are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not restricted by the maximum span width. 6 Conclusions and Future Work We propose a joint end-to-end IE frame"
2020.acl-main.713,D19-1585,0,0.387379,"and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example where the local argument role classifier predicts a redundant PERSON edge. The model should be able to avoid such mistakes if it is capable of learning and leveraging the fact that it is unusual for an ELECT event to have two PERSON arguments. Most existing jo"
2020.acl-main.713,N16-1033,0,0.106581,"res Ying Lin1 , Heng Ji1 , Fei Huang2 , Lingfei Wu3 1 University of Illinois at Urbana-Champaign 2 Alibaba DAMO Academy 3 IBM Research {yinglin8,hengji}@illinois.edu, f.huang@alibaba-inc.com, wuli@us.ibm.com Abstract which leads to the error propagation problem and disallows interactions among components in the pipeline. As a solution, some researchers propose joint inference and joint modeling methods to improve local prediction (Roth and Yih, 2004; Ji and Grishman, 2005; Ji et al., 2005; Sil and Yates, 2013; Li et al., 2014; Durrett and Klein, 2014; Miwa and Sasaki, 2014; Lu and Roth, 2015; Yang and Mitchell, 2016; Kirschnick et al., 2016). Due to the success of deep learning, neural models have been widely applied to various IE subtasks (Collobert et al., 2011; Chiu and Nichols, 2016; Chen et al., 2015; Lin et al., 2016). Recently, some efforts (Wadden et al., 2019; Luan et al., 2019) revisit global inference approaches by designing neural networks with embedding features to jointly model multiple subtasks. However, these methods use separate local task-specific classifiers in the final layer and do not explicitly model the interdependencies among tasks and instances. Figure 1 shows a real example whe"
2020.acl-main.713,P17-1113,0,0.0759927,"red by these previous efforts, we propose a joint neural framework with global features in which the weights are learned during training. Similar to (Li et al., 2014)’s method, O NE IE also uses global features to capture cross-subtask and cross-instance interdependencies, while our features are languageindependent and do not rely on other NLP tools such as dependency parsers. Our methods also differ in local features, optimization methods, and decoding procedures. Some recent efforts develop joint neural models to perform extraction of two IE subtasks, such as entity and relation extraction (Zheng et al., 2017; Katiyar and Cardie, 2017; Bekoulis et al., 2018; Fu et al., 2019; Luan et al., 2019; Sun et al., 2019) and event and temporal relation extraction (Han et al., 2019). Wadden et al. (2019) design a joint model to extract entities, relations and events based on BERT and dynamic span graphs. Our framework extends (Wadden et al., 2019) by incorporating global features based on cross-subtask and crossinstance constraints. Unlike (Wadden et al., 2019) that uses a span-based method to extract mentions, we adopt a CRF-based tagger in our framework because it can extract mentions of any length, not re"
2020.emnlp-demos.1,D19-3029,0,0.0179153,"s the possibility of bridging the gap between tasks with single architecture and providing future insight for unified natural language understanding. Furthermore, as there is a lack of practical and stable toolkit to support the implementation, deployment and evaluation of those tasks, we develop a toolkit which is a compliment for existing toolsets such as Spacy2 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018) for knowledge embedding, Stanford OpenIE (Angeli et al., 2015) for open information extraction, and OpenNER (Han et al., 2019) for relation extraction. Introduction A large number of natural language processing (NLP) tasks exist to analyze various aspects of human language. Most of them focus on tokenlevel classification (e.g., named entity recognition, slot filling, and argument role classification) or sentence-level understanding (e.g., relation classification, intent detection, and event classification). Previous researchers usually use specifically designed neural network architectures for those tasks. Note that most of those tasks share similar encoder and decoder modules (Jiang et al., 2019). It is beneficial t"
2020.emnlp-demos.1,P15-1034,0,0.031967,"ction and design a simple unified model. Our prototype model studies the possibility of bridging the gap between tasks with single architecture and providing future insight for unified natural language understanding. Furthermore, as there is a lack of practical and stable toolkit to support the implementation, deployment and evaluation of those tasks, we develop a toolkit which is a compliment for existing toolsets such as Spacy2 for named entity recognition (NER), TagMe (Ferragina and Scaiella, 2010) for entity linking (EL), OpenKE (Han et al., 2018) for knowledge embedding, Stanford OpenIE (Angeli et al., 2015) for open information extraction, and OpenNER (Han et al., 2019) for relation extraction. Introduction A large number of natural language processing (NLP) tasks exist to analyze various aspects of human language. Most of them focus on tokenlevel classification (e.g., named entity recognition, slot filling, and argument role classification) or sentence-level understanding (e.g., relation classification, intent detection, and event classification). Previous researchers usually use specifically designed neural network architectures for those tasks. Note that most of those tasks share similar enco"
2020.emnlp-demos.1,W05-0620,0,0.379297,"Missing"
2020.emnlp-demos.1,P15-1017,0,0.0228527,"RT 0.873 0.878 0.888 0.918 0.986 0.969 0.967 0.970 0.973 0.970 0.732 0.741 0.755 0.809 0.928 0.944 0.942 0.942 0.948 0.952 0.975 0.926 0.911 0.936 0.950 0.961 0.807 0.789 0.822 0.834 0.882 OpenUE 0.985 0.988 0.930 0.953 0.960 0.874 Table 1: Evaluation results of slot filling and intent detection on SNIPS-NLU and ATIS datasets. Model NYT WebNLG SKE ChMedIE Tagging CopyR HRL CasRel 42.0 58.7 68.3 89.6 28.3 37.1 66.0 91.8 78.4 81.0 OpenUE 89.9 89.9 79.3 81.2 guage and Intelligence Challenge7 . DuEE contains 65 event types and 121 argument roles. We compare our OpenUE with three baseliens. DMCNN (Chen et al., 2015) uses dynamic multipooling to keep multiple events’ information. dbRNN (Sha et al., 2018) adds dependency which bridges over Bi-LSTM for event extraction. JMEE (Liu et al., 2018) proposes an approach which jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. From Table 3 we observe that OpenUE can archive comparable results with JMEE, Table 2: Evaluation results of entity and relation extraction on NYT, WebNLG, SKE and ChMedIE datasets. ACE Type Arg DuE"
2020.emnlp-demos.1,D18-1156,0,0.0182444,"834 0.882 OpenUE 0.985 0.988 0.930 0.953 0.960 0.874 Table 1: Evaluation results of slot filling and intent detection on SNIPS-NLU and ATIS datasets. Model NYT WebNLG SKE ChMedIE Tagging CopyR HRL CasRel 42.0 58.7 68.3 89.6 28.3 37.1 66.0 91.8 78.4 81.0 OpenUE 89.9 89.9 79.3 81.2 guage and Intelligence Challenge7 . DuEE contains 65 event types and 121 argument roles. We compare our OpenUE with three baseliens. DMCNN (Chen et al., 2015) uses dynamic multipooling to keep multiple events’ information. dbRNN (Sha et al., 2018) adds dependency which bridges over Bi-LSTM for event extraction. JMEE (Liu et al., 2018) proposes an approach which jointly extract multiple event triggers and arguments by introducing syntactic shortcut arcs to enhance information flow and attention-based graph convolution networks to model graph information. From Table 3 we observe that OpenUE can archive comparable results with JMEE, Table 2: Evaluation results of entity and relation extraction on NYT, WebNLG, SKE and ChMedIE datasets. ACE Type Arg DuEE Type Arg DMCNN dbRNN JMEE 69.1 71.9 73.7 53.5 58.7 60.3 80.2 79.5 OpenUE 75.5 60.5 86.2 85.3 Model 4.3 Table 3: Evaluation results of event extraction extraction on ACE and DuE"
2020.emnlp-demos.1,2020.acl-main.141,0,0.104902,"Missing"
2020.emnlp-demos.1,S16-1002,0,0.0712692,"Missing"
2020.emnlp-demos.1,P17-1017,0,0.0318801,"wn } in the training set, where wi ∈ x is the word token in sentence x, we first construct input sentence in the form: {[CLS], w1 , w2 , . . . , wn , [SEP]}. Then we leverage the output of [CLS] representation to encode the entire sentence information. We apply an MLP layer with a cross-entropy loss to perform sentence classification. In OpenUE, we have also implemented other common encoders such as XLNet (Yang et al., 2019). 3.3 Experiment and Evaluation Relational Triple Extraction We carry out experiments on four datasets of relational triple extraction: NYT (Riedel et al., 2010), WebNLG (Gardent et al., 2017), SKE and ChMedIE. NYT dataset was originally produced by the distant supervision method. It consists of 1.18M sentences with 24 predefined relation types. WebNLG dataset was originally created for Natural Language Generation (NLG) tasks and adapted by (Zeng et al., 2018) for relational triple extraction task. It contains 246 predefined relation types. Different from the two previous English datasets, SKE is a Chinese dataset for information extraction, which is released in the 2019 Language and Intelligence Challenge4 . SKE contains 50 relation types, and training texts exceed 200,000. We bui"
2020.emnlp-demos.1,N18-2118,0,0.0194741,"NONE class and BIO annotation schema, we classify each sentence into 67 categories in event classification and 37 categories in argument extraction. DuEE is a Chinese dataset for event extraction, which is released in the 2020 Lan7 https://aistudio.baidu.com/aistudio/ competition/detail/32 8 https://github.com/snipsco/ nlu-benchmark/ 6 https://catalog.ldc.upenn.edu/ LDC2006T06 5 Figure 3: An example of the online system. dict the utterance intent. Attention BiRNN (Liu and Lane, 2016) proposes a RNN based encoderdecoder model for joint intent detection and slot filling. Slot-gated Full Atten (Goo et al., 2018) proposes a slot gate that focuses on learning the relationship between intent and slot attention vectors in order to obtain better semantic frame results by the global optimization. Capsule-NLU (Zhang et al., 2018a) proposes a capsule-based neural network model that accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. Joint-BERT (Chen et al., 2019) adapts the standard BERT classification, and token classification pipeline to jointly model the slot and intent. From Table 1, we observe that OpenUE can archive comparable performance with Capsule-NLU. In summa"
2020.emnlp-demos.1,E12-2021,0,0.0324494,"Missing"
2020.emnlp-demos.1,P18-1047,0,0.020992,"with a cross-entropy loss to perform sentence classification. In OpenUE, we have also implemented other common encoders such as XLNet (Yang et al., 2019). 3.3 Experiment and Evaluation Relational Triple Extraction We carry out experiments on four datasets of relational triple extraction: NYT (Riedel et al., 2010), WebNLG (Gardent et al., 2017), SKE and ChMedIE. NYT dataset was originally produced by the distant supervision method. It consists of 1.18M sentences with 24 predefined relation types. WebNLG dataset was originally created for Natural Language Generation (NLG) tasks and adapted by (Zeng et al., 2018) for relational triple extraction task. It contains 246 predefined relation types. Different from the two previous English datasets, SKE is a Chinese dataset for information extraction, which is released in the 2019 Language and Intelligence Challenge4 . SKE contains 50 relation types, and training texts exceed 200,000. We build our training set, development set, and test set by randomly selecting 50,000, 5,000, and 5,000 texts. ChMedIE is also a Chinese dataset for information extraction in the medical domain. We craw corpus from the Chinese health website5 and build this dataset via distant"
2020.emnlp-demos.1,N19-1306,1,0.70544,"fix bugs in the future. This toolkit may benefit both researchers and industry developers. We highlight our contributions as follows: 2 OpenUE is designed for various tasks, including relational triple extraction, slot filling, intent detection, event extraction, and knowledge extraction from the Web, etc. As shown in Figure 1, we give some examples of these application scenarios. 2.1 Relational Triple Extraction Relational Triple Extraction is an essential task in Information Extraction (IE) for Natural Language Processing (NLP) and Knowledge Graph (KG) (Zhang et al., 2018b; Yu et al., 2017; Zhang et al., 2019; Huang et al., 2020; Nan et al., 2020; Zhang et al., 2020a; Ye et al., 2020; Zhang et al., 2020b), which is aimed at detecting a pair of entities along with their relations from unstructured text. For instance, there is a sentence “Paris is known as the romantic capital of France.”, and in this example, an ideal relational triple extraction system should extract the relational triple hParis, Capital of, Francei, in which Capital of is the relation between Paris and France. In this paper, we provide a simple implementation which firstly classifies relations with the sentence and then conduct s"
2020.emnlp-demos.1,D18-1120,1,0.924976,"meet new requests, add new tasks, and fix bugs in the future. This toolkit may benefit both researchers and industry developers. We highlight our contributions as follows: 2 OpenUE is designed for various tasks, including relational triple extraction, slot filling, intent detection, event extraction, and knowledge extraction from the Web, etc. As shown in Figure 1, we give some examples of these application scenarios. 2.1 Relational Triple Extraction Relational Triple Extraction is an essential task in Information Extraction (IE) for Natural Language Processing (NLP) and Knowledge Graph (KG) (Zhang et al., 2018b; Yu et al., 2017; Zhang et al., 2019; Huang et al., 2020; Nan et al., 2020; Zhang et al., 2020a; Ye et al., 2020; Zhang et al., 2020b), which is aimed at detecting a pair of entities along with their relations from unstructured text. For instance, there is a sentence “Paris is known as the romantic capital of France.”, and in this example, an ideal relational triple extraction system should extract the relational triple hParis, Capital of, Francei, in which Capital of is the relation between Paris and France. In this paper, we provide a simple implementation which firstly classifies relation"
2020.emnlp-main.288,D17-1047,1,0.961554,"ied based on the extracted opinion features and contextual information. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/"
2020.emnlp-main.288,W14-4012,0,0.0629142,"Missing"
2020.emnlp-main.288,P19-1520,0,0.0555027,"shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” Thi"
2020.emnlp-main.288,S14-2076,0,0.186602,"We set the hidden size of GRU to 32 or 64. The batch size is set to 64 or 96. The dropout rate is selected from 0.3 to 0.8, with a step size of 0.1. The dimension of the aspect indicator is selected from {50, 70, 90}. The value of γ in the position decay function is selected from {1,2,3}. The number of layer of GRU is selected from {1,2,3}. We adopt Adam (Kingma and Ba, 2014) to optimize our model with a learning rate of 0.008. All hyper-parameters are selected based on the best performance on the development set. 3.2 Baselines Our MCRF-SA model is compared with the following methods2 . SVM (Kiritchenko et al., 2014) is a support vector machine based method that integrates surface, lexicon, and parse features. ATAELSTM (Wang et al., 2016) is an LSTM (Hochreiter and Schmidhuber, 1997) based model, which has an extra attention to perform soft-selection over the context words. MemNet (Tang et al., 2016) introduces a deep memory network to implement attention mechanisms to learn the relatedness of context words towards the aspect. IAN (Ma et al., 2017) utilizes two LSTM based attention models to learn both context and aspect representations interactively. SA-LSTM-P (Wang and Lu, 2018) employs structured atten"
2020.emnlp-main.288,N16-1030,0,0.0546551,"pect, L is the maximum length of sentences across all datasets, γ is a hyper-parameter and a larger value enables more influence from the context words that are close to the aspect. Then, the decayed contextual word representation is as follows: rt = f (t) ht (4) 2.4 Multi-CRF Structured Attention We use multiple linear-chain CRFs to intensively incorporate structure dependencies to capture the corresponding opinion spans of an aspect. In particular, we create a latent label (Wang and Lu, 2018) z ∈ {Y es, N o} to indicate whether each context word belongs to part of opinion spans. Similar to (Lample et al., 2016), given the sentence representation x, the CRF is defined as: 3562 P (z|x) = P exp(score(z, x)) 0 z0 exp(score(z , x)) (5) where score(z, x) is a score function that is defined as the summation of transition scores and emission scores from the Bi-GRU: n n X X score(z, x) = Tzt ,zt+1 + Et,zt (6) t=0 Marginal Inference The latent labels introduced in the CRF layer show whether the word influences the given aspect’s sentiment. Intuitively, we can understand that the marginal probabilities on the Y es label indicate the influence of the current context word on the aspect word’s sentiment. By using"
2020.emnlp-main.288,D19-1550,1,0.70703,"a and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017), which extend the"
2020.emnlp-main.288,P18-1087,1,0.827072,"ion. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider th"
2020.emnlp-main.288,P17-1036,0,0.0264457,"tal results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “"
2020.emnlp-main.288,C18-1096,0,0.0811402,"am between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017),"
2020.emnlp-main.288,D19-1466,1,0.829529,"ans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables,"
2020.emnlp-main.288,P19-1048,0,0.0158908,"on Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the o"
2020.emnlp-main.288,K19-1091,0,0.0151803,"niversity of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) engage structured attention networks (Kim et al., 2017), which extend the previous attentio"
2020.emnlp-main.288,E17-2091,0,0.0842539,"/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of the previous works (Wang et al., 2016; Chen et al., 2017; Liu and Zhang, 2017; Yang et al., 2017; Li et al., 2018c; He et al., 2018; Li and Lu, 2019; Hu et al., 2019) adopt attention mechanism (Bahdanau et al., 2015) to capture the semantic relatedness among the context words and the aspect, and learn aspect-specific features for sentiment classification. However, it is challenging for attention-based approaches to consider an opinion span as a whole during feature extraction because they are overreliant on neural models to learn the contextstructural information and perform feature extraction over individual hidden representations. Previous work (Wang and Lu, 2018) en"
2020.emnlp-main.288,P13-1172,0,0.159276,"veness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I wor"
2020.emnlp-main.288,P18-2094,0,0.0301115,"further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables"
2020.emnlp-main.288,2020.emnlp-main.183,1,0.842257,"sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at https://github.com/ xuuuluuu/Aspect-Sentiment-Classification For example, consider the review sentence “Food is usually very good, though occasionally I worry about freshness of raw vegetables in side orders.” This review mentions two aspects: Food and raw vegetables, and for ASC, the objective is to give a positive sentiment on Food and a negative sentiment on raw vegetables. Most of"
2020.emnlp-main.288,D19-1464,0,0.313505,"lized. The input representation xt is as follows: xt = [wtword ; wtas ] (1) 2.2 Aspect-Specific Contextualized Representation We employ a bi-directional GRU (Cho et al., 2014) to generate the contextualized representation. Since the input representation has already contained the aspect information, the aspect-specific contextualized representation is obtained by concatenating the hidden states from both directions: → − ← − ht = [ht ; ht ] (2) → − where ht is the hidden state from the forward GRU ← − and ht is from the backward. 2.3 Position Decay Following the previous work (Li et al., 2018a; Zhang et al., 2019; Tang et al., 2019), we also use a position decay function to reduce the influence of Figure 1: MCRF-SA Architecture. the context words on the aspect as it goes further away from the aspect. We propose a higher-order decay function, which is more sensitive to distance, and the sensitivity can be tuned by γ on different datasets.  L−i+t γ  t&lt;i ( L ) f (t) = 1 (3) i≤t≤j   L−t+j γ ( L ) j&lt;t where i and j are the starting and ending position of an aspect, L is the maximum length of sentences across all datasets, γ is a hyper-parameter and a larger value enables more influence from the contex"
2020.emnlp-main.288,D14-1162,0,0.0869837,"r, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With the development set, we tune our model hyperparameters using an open-source black-box tuner (Alberto and Giacomo, 2018). We set the hidden size of GRU to 32 or 64. The batch size is set to 64 or 96. The dropout rate is selected from 0.3 to 0.8, with a step size of 0.1. The dimension of the aspect indicator is selected from {50, 70, 90}. The value of γ in the position decay function is selected"
2020.emnlp-main.288,S15-2082,0,0.314936,"Missing"
2020.emnlp-main.288,S14-2004,0,0.101049,". #Neg. #Pos. #Neu. #Neg. #Pos. #Neu. #Neg. 1796 539 666 368 94 139 728 196 196 824 383 717 161 72 149 340 167 128 808 29 228 147 5 44 340 28 195 1106 54 406 191 9 60 474 29 127 t=1 where T is a transition matrix and Tzt ,zt+1 denotes the transition score from label zt to zt+1 . Et,zt denotes the emission score of label zt at the t-th position, and the score is obtained from a linear layer, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With th"
2020.emnlp-main.288,D16-1021,0,0.584365,"5 1106 54 406 191 9 60 474 29 127 t=1 where T is a transition matrix and Tzt ,zt+1 denotes the transition score from label zt to zt+1 . Et,zt denotes the emission score of label zt at the t-th position, and the score is obtained from a linear layer, which takes rt as input and returns a vector whose length is label size. 2.4.1 Dataset Experiments Experimental Setup Our proposed MCRF-SA model is evaluated on four benchmark datasets: SemEval 2014 Task4 (Pontiki et al., 2014), SemEval 2015 Task12 (Pontiki et al., 2015) and SemEval 2016 Task 5 (Pontiki et al., 2016). Following the previous works (Tang et al., 2016; Chen et al., 2017; Wang and Lu, 2018; Table 1: Statistics of datasets. He et al., 2018), we remove a few examples that have conflicting labels. Detailed statistics of the datasets can be found in Table 1. We use the 300d GloVe (Pennington et al., 2014) to initialize our word embeddings. One-sixth of instances are randomly selected from the original training dataset as the development dataset, and the model is only trained with the remaining data. With the development set, we tune our model hyperparameters using an open-source black-box tuner (Alberto and Giacomo, 2018). We set the hidden siz"
2020.emnlp-main.288,P19-1053,0,0.124683,"Missing"
2020.emnlp-main.288,D16-1058,0,0.484669,"get is then classified based on the extracted opinion features and contextual information. The experimental results on four datasets demonstrate the effectiveness of the proposed model, and our further analysis shows that our model can capture aspect-specific opinion spans.1 1 Introduction Aspect Based Sentiment Analysis (ABSA) (Pang and Lee, 2008; Liu, 2012) is an extensively studied sentiment analysis task on a fine-grained semantic level, i.e., opinion targets explicitly mentioned in sentences. Previous ABSA studies focused on a few sub-tasks, such as Aspect Sentiment Classification (ASC) (Wang et al., 2016; Chen et al., 2017; Ma et al., 2018), Aspect Term Extraction (ATE) (Li et al., 2018b; He et al., 2017), Aspect and Opinion Co-Extraction (Liu et al., 2013; Wang et al., 2017; Xu et al., 2018; Dai and Song, 2019), E2EABSA (a joint task of ASC and ATE) (Li et al., 2019a; He et al., 2019; Li et al., 2019b), Aspect Sentiment Triplet Extraction (ASTE) (Peng et al., 2019; Xu et al., 2020), etc. ASC analyzes the sentiment polarity of given aspects/targets in a review. ∗ Lu Xu is under the Joint PhD Program between Alibaba and Singapore University of Technology and Design. 1 Our code is released at h"
2020.emnlp-main.485,C18-1139,0,0.0252366,"mation. We use these settings for a better understanding of how the decoders perform on each task when the encoders capture different levels of contextual information. Decoder We use the MaxEnt approach, the traditional CRF approach and AINs with the first-order and factorized second-order CRFs for decoding. We denote these approaches by MaxEnt, CRF, AIN-1O and AIN-F2O respectively. We set the iteration number M to 3 in AINs because we find that more iterations do not result in further improvement in accuracy. Hyper-parameters For the hyper-parameters, we follow the settings of previous work (Akbik et al., 2018). We use Stochastic Gradient Descent for optimization with a fixed learning rate of 0.1 and a batch size of 32. We fix the hidden size of the CNN and BiLSTM layer to 512 and 256 respectively, and the kernel size of CNN to 3. We anneal the learning rate by 0.5 if there is no improvement in the development sets for 10 epochs when training. Evaluation We use F1 score to evaluate the NER, slot filling and chunking tasks and use accuracy to evaluate the POS tagging task. We convert the BIO 6022 format into BIOES format for NERs, slot filling and chunking datasets and use the official release of CoN"
2020.emnlp-main.485,Q17-1010,0,0.0139758,"rsal POS tag annotations with 8 languages for experiments. The list of treebanks is shown in Table 3. We use the standard training/development/test split for experiments. Slot Filling Slot filling is a task that interprets user commands by extracting relevant slots, which can be formulated as a sequence labeling task. We use the Air Travel Information System (ATIS) (Hemphill et al., 1990) dataset for the task. 4 . 3.2 Settings Embeddings For word embeddings in the NER, chunking and slot filling experiments, we use the same word embedding as in Lample et al. (2016) except that we use fastText (Bojanowski et al., 2017) embedding for Dutch which we find significantly improves the accuracy (more than 5 F1 scores on CoNLL NER). We use fastText embeddings for all UD tagging experiments. For character embedding, we use a single layer character CNN with a hidden size of 50, because Yang et al. (2018) empirically showed that it has competitive performance with character LSTM. We concatenate the word embedding and character CNN output for the final word representation. 3 https://lindat.mff.cuni.cz/repository/xmlui/handle/ 11234/1-2837 4 We use the same dataset split as https://github.com/sz128/ slot_filling_and_int"
2020.emnlp-main.485,P17-1178,0,0.0569513,"to a CRF (Lafferty et al., 2001) decoder layer to produce final predictions. The CRF layer is a linear-chain structure that models the relation between neighboring labels. In the traditional CRF approach, exact probabilistic inference algorithms such as the forward-backward and Viterbi algorithms are applied for training and prediction respectively. In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently. In practice, we sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace"
2020.emnlp-main.485,D17-1283,0,0.0132944,"ctively. In many sequence labeling tasks, the CRF layer leads to better results than the simpler method of predicting each label independently. In practice, we sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference (MFVI) to approximately decode the linear-chain CRF. MFVI iteratively passes messages am"
2020.emnlp-main.485,P19-1454,1,0.646152,"e Viterbi decoding and one iteration of our MFVI inference on the CRF model. Yi is the random variable representing the i-th label with three possible values. The illustrated vectors represent Viterbi scores and Qi distributions respectively. i ˜ has the same shape as U in where the matrix U Eq. 2. The factor graph of our factorized secondorder CRF is shown at the bottom of Figure 1. The update formula is similar to that of our first-order approach but with more neighbors: ′ Qm i (yi |x)∝ exp{ψu (x, yi )+s (i−2, i, m) +s(i−1, i, m)+s(i+1, i, m)+s′ (i+2, i, m)} et al., 2016; Chen et al., 2018; Wang et al., 2019) using the MFVI algorithm for solving intractable problems of densely connected probabilistic models to get better accuracy, we propose to employ the MFVI algorithm to accelerate tractable inference of sequence-structured probabilistic models. As far as we know, this is the first attempt of using approximate inference on tractable models for speedup with GPU parallelization. The time complexity of each iteration of the MFVI algorithm is O(nL2 ), which is on par with the time complexity of the exact probabilistic inference algorithms. However, in each iteration, the update of each distribution"
2020.emnlp-main.485,C18-1327,0,0.119662,"sometimes require very fast sequence labelers for training (e.g., on huge datasets like WikiAnn (Pan et al., 2017)) and prediction (e.g. for low latency online serving). The BiLSTM encoder and the CRF layer both contain sequential computation and require O(n) time over n input words even when parallelized on GPU. A common practice to improve the speed of the encoder is to replace the BiLSTM with a CNN structure (Collobert et al., 2011; Strubell et al., 2017), distill larger encoders into smaller ones (Tsai et al., 2019; Mukherjee and Awadallah, 2020) or in other settings (Tu and Gimpel, 2018; Yang et al., 2018; Tu and Gimpel, 2019; Cui and Zhang, 2019). The CRF layer, however, is more difficult to replace because of its superior accuracy compared with faster alternatives in many tasks. In order to achieve sublinear time complexity on the CRF layer, we must parallelize the CRF prediction over the tokens. In this paper, we apply Mean-Field Variational Inference (MFVI) to approximately decode the linear-chain CRF. MFVI iteratively passes messages among neighboring labels to update their distributions locally. Unlike the exact probabilistic inference algorithms, MFVI can be parallelized over different"
2020.emnlp-main.700,D19-1255,1,0.855705,"nell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 2018) and GPT-2 (Radford et al., 2019) use a leftto-right Transformer deco"
2020.emnlp-main.700,W11-0609,0,0.0284332,"ERNIE-GENLARGE d PALM PALMLARGE BLEU-4 15.16 16.38 22.88 22.28 24.03 22.78 24.11 MTR 19.12 20.25 24.94 25.13 26.31 25.02 25.85 RG-L 44.48 51.80 50.58 52.36 50.96 52.38 Table 4: Question generation results on the SQuAD dataset. MTR is short for METEOR and RG is short for ROUGE. a (Du and Cardie, 2018); b (Zhao et al., 2018); c (Dong et al., 2019); d (Xiao et al., 2020). 3.6 Fine-tuning on Response Generation Conversational response generation aims to produce a flexible response to a conversation (Vinyals and Le, 2015). Following MASS, we conduct experiments on the Cornell Movie Dialog corpus5 (Danescu-Niculescu-Mizil and Lee, 2011) that contains 140K conversation pairs, and use the training/test splits provided by the dataset. The same training hyperparameters from generative QA fine-tuning are adopted on the response generation task. We report the results in perplexity following (Vinyals and Le, 2015) (lower is better). We compare PALM with the competing methods including the baseline trained on the data pairs available and the pre-trained BERT+LM and MASS. Following MASS, we train every model on 10K pairs randomly sampled and all 110K training pairs. As shown in Table 5, PALM significantly performs better than all the"
2020.emnlp-main.700,P18-1177,0,0.0595509,"Missing"
2020.emnlp-main.700,N19-1409,0,0.0206538,"gradation resulted from ablating pre-training clearly demonstrates the power of PALM in leveraging an unlabeled corpus for downstream generation. 4 Related Work ELMo (Peters et al., 2018) is an early prominent pre-training method based on bidirectional LSTMs. It concatenates left-only and right-only representations, but does not pre-train interactions between these features. GPT (Radford, 2018), GPT2 (Radford et al., 2019) and GPT-3 (Brown et al., 2020) are proposed to base language modeling on the Transformer architecture, and use only the Transformer decoder for pre-training. Edunov et al. (Edunov et al., 2019) examine different strategies (e.g., ELMo) to add contextualized embeddings to sequence-to-sequence models, and observe the most improvement by adding the learned embeddings to the encoder. BERT (Devlin et al., 2018) introduces Masked Language Modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). However, BERT does not make predict"
2020.emnlp-main.700,D18-1443,0,0.0254376,"d, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 20"
2020.emnlp-main.700,P18-1031,0,0.0189712,"fine-tuning where generation is more than reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-theart results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to com"
2020.emnlp-main.700,D18-1054,0,0.0464293,"Missing"
2020.emnlp-main.700,W04-1013,0,0.0214703,"user queries issued to the Bing search engine and the contextual passages are from real web documents. The data has been split into a training set (153,725 QA pairs), a dev set (12,467 QA pairs) and a test set (101,092 questions with unpublished answers). To evaluate the generative capability, we focus on the Q&A + Natural Language Generation task, the goal of which is to provide the best answer available in natural language that could be used by a smart device / digital assistant. The answers are human-generated and not necessarily sub-spans of the contextual passages, so we use the ROUGE-L (Lin, 2004) metric for our evaluation to measure the quality of generated answers against the ground truth. We fine-tune the pre-trained PALM on the MARCO training set for 10 epochs. We set the batch size to 64, the learning rate to 1e-5, and the maximum input length to 512. The other hyperparameters are kept the same as pre-training. In fine-tuning PALM, the encoder takes as input x a contextual passage concatenated with a question at the end, and the decoder takes an answer as input y. During decoding, we use beam search with a beam of size 5. Table 2 presents the answer generation results on the test"
2020.emnlp-main.700,D19-1387,0,0.0578447,"Missing"
2020.emnlp-main.700,2021.ccl-1.108,0,0.129719,"Missing"
2020.emnlp-main.700,P19-1220,0,0.19461,"ing in the two stages, respectively. 2.3 Copying Tokens from Context In a human-written document, subsequent text often refers back to entities and tokens present earlier in the preceding text. Therefore, it would increase coherence of text generated in downstream to incorporate the copy mechanism into pre-training on an unlabeled corpus. This allows the model to learn from pre-training when and how to copy tokens in generating text, and the knowledge is transferred to downstream fine-tuning. PALM incorporates the copy mechanism by plugging in the pointer-generator network (See et al., 2017b; Nishida et al., 2019) on top of the decoder in Transformer. Figure 2 illustrates the pointer-generator network, which allows every token to be either generated from a vocabulary or copied from context in generating text. Extended vocabulary distribution. Let the extended vocabulary, V , be the union of words in the vocabulary and all tokens present in context. P v (yt ) then denotes the probability distribution of the t-th word token, yt , over the extended vocabulary, defined as: P v (yt ) = softmax(W e (W v st + bv )), (2) where st denotes the output representation of t-th token from the decoder. The output embe"
2020.emnlp-main.700,N18-1202,0,0.137717,"han reconstructing original text. An extensive set of experiments show that PALM achieves new state-of-theart results on a variety of language generation benchmarks covering generative question answering (Rank 1 on the official MARCO leaderboard), abstractive summarization on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on w"
2020.emnlp-main.700,D15-1044,0,0.0675287,"on on CNN/DailyMail as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been propose"
2020.emnlp-main.700,P17-1099,0,0.640208,"as well as Gigaword, question generation on SQuAD, and conversational response generation on Cornell Movie Dialogues. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language gen"
2020.emnlp-main.700,P18-1178,0,0.0422221,"Missing"
2020.emnlp-main.700,D16-1264,0,0.0618804,"Dong et al., 2019), T5 (Raffel et al., 2019), BART (Lewis et al., 2019), PEGASUS (Zhang et al., 2019) and ERNIE-GEN (Xiao et al., 2020). By consistently outperforming the pre-training methods, PALM confirms its effectiveness in leveraging unsupervision signals for language generation. 3.5 Fine-tuning on Question Generation We conduct experiments for the answer-aware question generation task. Given an input passage and an answer span, question generation aims to generate a question that leads to the answer. Following the practice in (Zhao et al., 2018; Dong et al., 2019), we use the SQuAD 1.1 (Rajpurkar et al., 2016) dataset, and the BLEU-4, METEOR and ROUGEL metrics for evaluation. As shown in Table 4, PALM outperforms all previous question generation systems and achieves a new state-of-the-art result on BLEU-4 and ROUGE-L for question generation on the SQuAD 1.1 dataset. Method CorefNQGa MP-GSNb UNILMc ERNIE d ERNIE-GENLARGE d PALM PALMLARGE BLEU-4 15.16 16.38 22.88 22.28 24.03 22.78 24.11 MTR 19.12 20.25 24.94 25.13 26.31 25.02 25.85 RG-L 44.48 51.80 50.58 52.36 50.96 52.38 Table 4: Question generation results on the SQuAD dataset. MTR is short for METEOR and RG is short for ROUGE. a (Du and Cardie, 20"
2020.emnlp-main.700,D18-1424,0,0.0990213,"lf-supervised pre-training has achieved great success in a wide range of natural language understanding (NLU) tasks (Dai and Le, 2015; Howard and Ruder, 2018; Radford, 2018; Peters et al., 2018; Devlin et al., 2018). Different from language understanding, language generation aims at generating natural language sentences, including tasks like neural machine translation (Bahdanau et al., 2015; Vaswani et al., 2017), abstractive summarization (Rush et al., 2015; See et al., 2017a; Gehrmann et al., 2018), generative question answering (QA) (Tan et al., 2017; Bi et al., 2019), question generation (Zhao et al., 2018) and conversational response generation (Vinyals and Le, 2015). Many of the language generation tasks require the models to read and to comprehend a given document, based on which output text is generated. In this paper, we present PALM, a novel approach to Pre-training an Autoencoding&autoregressive Language Model for text generation based on reading comprehension of textual context. Recently, several pre-training methods have been proposed for language generation. GPT (Radford, 2018) and GPT-2 (Radford et al., 2019) use a leftto-right Transformer decoder to generate a text sequence token-by-"
2020.emnlp-main.700,P17-4012,0,\N,Missing
2020.emnlp-main.700,E17-2047,0,\N,Missing
2020.emnlp-main.700,P18-1015,0,\N,Missing
2020.emnlp-main.700,N19-1423,0,\N,Missing
2020.emnlp-main.700,D19-1304,0,\N,Missing
2020.findings-emnlp.236,N19-1078,0,0.0199641,"sed quadrilinear potential function based on the vector representations of two neighboring labels and two neighboring words consistently achieves the best performance. 1 BiLSTM Encoder h1 h2 h3 … hn Word Representations x1 x2 x3 … xn Figure 1: Neural architecture for sequence labeling Introduction Sequence labeling is the task of labeling each token of a sequence. It is an important task in natural language processing and has a lot of applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003; Xin et al., 2018), Named Entity Recognition (NER) (Ritter et al., 2011; Akbik et al., 2019), Chunking (Tjong Kim Sang and Buchholz, 2000; Suzuki et al., 2006). The neural CRF model is one of the most widelyused approaches to sequence labeling and can achieve superior performance on many tasks (Collobert et al., 2011; Chen et al., 2015; Ling et al., 2015; Ma and Hovy, 2016; Lample et al., 2016a). It often employs an encoder such as a BiLSTM to compute the contextual vector representation of each word in the input sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vecto"
2020.findings-emnlp.236,D15-1141,0,0.026453,"ral architecture for sequence labeling Introduction Sequence labeling is the task of labeling each token of a sequence. It is an important task in natural language processing and has a lot of applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003; Xin et al., 2018), Named Entity Recognition (NER) (Ritter et al., 2011; Akbik et al., 2019), Chunking (Tjong Kim Sang and Buchholz, 2000; Suzuki et al., 2006). The neural CRF model is one of the most widelyused approaches to sequence labeling and can achieve superior performance on many tasks (Collobert et al., 2011; Chen et al., 2015; Ling et al., 2015; Ma and Hovy, 2016; Lample et al., 2016a). It often employs an encoder such as a BiLSTM to compute the contextual vector representation of each word in the input sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vector representation of the current word) and a transition function (of the previous and current labels) (Liu et al., 2018; Yang et al., 2018). ∗ Softmax or various CRFs Kewei Tu and Yong Jiang are the corresponding authors. In this paper, we design"
2020.findings-emnlp.236,D19-1422,0,0.136821,"t sequence. The potential function at each position of the input sequence in a neural CRF is typically decomposed into an emission function (of the current label and the vector representation of the current word) and a transition function (of the previous and current labels) (Liu et al., 2018; Yang et al., 2018). ∗ Softmax or various CRFs Kewei Tu and Yong Jiang are the corresponding authors. In this paper, we design a series of increasingly expressive potential functions for neural CRF models. First, we compute the transition function from label embeddings (Ma et al., 2016; Nam et al., 2016; Cui and Zhang, 2019) instead of label identities. Second, we use a single potential function over the current word and the previous and current labels, instead of decomposing it into the emission and transition functions, leading to more expressiveness. We also employ tensor decomposition in order to keep the potential function tractable. Thirdly, we take the representations of additional neighboring words as input to the potential function, instead of solely relying on the BiLSTM to capture contextual information. To empirically evaluate different approaches, we conduct experiments on four well-known sequence la"
2020.findings-emnlp.356,N19-1078,0,0.122839,"Missing"
2020.findings-emnlp.356,C18-1139,0,0.258865,"ettings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are noncontextual embeddings helpful when the models are equipped with contextual embeddings? 2. When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold? 3. Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated? 2 2.1 Introduction In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2019; Martin et al., 2019) over approaches that use static non-contextual word embeddings (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014). Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in ∗ Yong Jiang and Kewei Tu are the corresponding authors. ‡ : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. Model Architecture Sequen"
2020.findings-emnlp.356,Q17-1010,0,0.0461714,"t use contextual word embeddings such as ELMo (Peters et al., 2018) since Akbik et al. (2018) showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy. 2 We do not use the pooled version of Flair due to its slower speed in training. Non-contextual Word Embeddings (NWEs) The most common approach to the NWEs is Word2vec (Mikolov et al., 2013), which is a skipgram model learning word representations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy (Yang e"
2020.findings-emnlp.356,Q16-1026,0,0.0341466,"esentations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character sequence of each word. We follow this approach as it usually results in better accuracy (Yang et al., 2018). 3 Experiments and Results For simplicity, we use M to represent M-BERT embeddings, F to represent Flair embeddings, W to represent fastText embeddings, C to represent non-contextual character embeddings, All to represents the concatenation of all types of embeddings and the operator “+” to represent the concatenation operation. We use the MaxEnt approach for all experiments3 . Due to the space limit, some detail"
2020.findings-emnlp.356,L16-1262,0,0.0773898,"Missing"
2020.findings-emnlp.356,P17-1178,0,0.0164893,"M to represent M-BERT embeddings, F to represent Flair embeddings, W to represent fastText embeddings, C to represent non-contextual character embeddings, All to represents the concatenation of all types of embeddings and the operator “+” to represent the concatenation operation. We use the MaxEnt approach for all experiments3 . Due to the space limit, some detailed experiment settings, extra experiments and discussions are included in the appendix. 3.1 Settings Datasets We use datasets from three multilingual sequence labeling tasks over 8 languages in our experiments: WikiAnn NER datasets (Pan et al., 2017), UD Part-Of-Speech (POS) tagging datasets (Nivre et al., 2016), and CoNLL 2003 chunking datasets (Tjong Kim Sang and De Meulder, 2003). We use language-specific fastText and Flair embeddings depending on the dataset. Embedding Concatenation Since experimenting on all 15 concatenation combinations of the four embeddings is not essential for evaluating the effectiveness of each kind of embeddings, we experiment on the following 7 concatenations: F, F+W, 3 We find that the observations from the MaxEnt experiments do not change in all experiments with the CRF approach. 3993 Relative Scores 10 5 0"
2020.findings-emnlp.356,D14-1162,0,0.0877623,"use the Flair embeddings due to their high accuracy for sequence labeling task2 . 1 We do not use contextual word embeddings such as ELMo (Peters et al., 2018) since Akbik et al. (2018) showed that concatenating Flair embeddings with ELMo embeddings cannot further improve the accuracy. 2 We do not use the pooled version of Flair due to its slower speed in training. Non-contextual Word Embeddings (NWEs) The most common approach to the NWEs is Word2vec (Mikolov et al., 2013), which is a skipgram model learning word representations by predicting neighboring words. Based on this approach, GloVe (Pennington et al., 2014) creates a co-occurrence matrix for global information and fastText (Bojanowski et al., 2017) represents each word as an n-gram of characters. We use fastText in our experiments as there are pretrained embeddings for 294 languages. Non-contextual Character Embeddings (NCEs) Using character information to represent the embeddings of word is proposed by Santos and Zadrozny (2014) with a lot of following work using a CNN structure to encode character representation (dos Santos and Guimarães, 2015; Chiu and Nichols, 2016; Ma and Hovy, 2016). Lample et al. (2016) utilized BiLSTM on the character se"
2020.findings-emnlp.356,N18-1202,0,0.293936,". In rich-resources settings, does combining different kinds of contextual embeddings result in a better sequence labeler? Are noncontextual embeddings helpful when the models are equipped with contextual embeddings? 2. When we train models in low-resource and cross-domain settings, do the conclusions from the rich-resource settings still hold? 3. Can sequence labelers automatically learn the importance of each kind of embeddings when they are concatenated? 2 2.1 Introduction In recent years, sequence labelers equipped with contextual embeddings have achieved significant accuracy improvement (Peters et al., 2018; Akbik et al., 2018; Devlin et al., 2019; Martin et al., 2019) over approaches that use static non-contextual word embeddings (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014). Different types of embeddings have different inductive biases to guide the learning process. However, little work has been done to study how to concatenate these contextual embeddings and non-contextual embeddings to build better sequence labelers in ∗ Yong Jiang and Kewei Tu are the corresponding authors. ‡ : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. Model"
2020.iwslt-1.1,P18-4020,0,0.0279555,"Missing"
2020.iwslt-1.1,2005.iwslt-1.19,0,0.174539,"Missing"
2020.iwslt-1.1,2020.iwslt-1.11,0,0.0306756,"Missing"
2020.iwslt-1.1,W18-6319,0,0.0215087,"Missing"
2020.iwslt-1.1,2013.iwslt-papers.14,0,0.069836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.9,0,0.053316,"Missing"
2020.iwslt-1.1,rousseau-etal-2014-enhancing,0,0.0549836,"Missing"
2020.iwslt-1.1,2020.iwslt-1.22,0,0.0613503,"Missing"
2020.tacl-1.7,P19-1470,0,0.0616403,"et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin et al., 2018) to enhance language representation for reading comprehension (Yang et al., 2019a) and other knowledge-driven NLP tasks like entity typing and relation classification (Zhang et al., 2019). Besides, Sun et al. (2019) improved BERT on Chinese NLP tasks by multi-stage knowledge masking strategy to integrate phrase and entity level knowledge into the language representation. Moreover, Bosselut et al. (2019) transferred the implicit knowledge from GPT-2 by fine-tuning the model to generate an object given the subject and a relation as input in commonsense knowledge graphs, that is, automatic knowledge base construction. However, the low novelty of the generated objects showed that it could still be difficult for GPT-2 to generate commonsense texts solely based on its implicit knowledge. Therefore, we target integrating external knowledge into GPT-2 for generating more reasonable commonsense stories. 2.2 Pretraining Recently, large-scale pretraining models have been widely developed in various NLP"
2020.tacl-1.7,D15-1075,0,0.0379736,"e dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge to enrich the limited source information. Commonsense knowledge has been demonstrated to significantly improve dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin et al., 2018) to enha"
2020.tacl-1.7,N18-1204,0,0.0395001,"github.com/thu-coai/CommonsenseStoryGen, and demo is available at http://coai.cs.tsinghua. edu.cn/static/CommonsenseStoryGen. 94 et al., 2014), we build our model based on GPT-2 because of its simplicity and broad applicability. short text descriptions (Jain et al., 2017). Different from these studies, we consider the setting of open-ended story generation from only a limited leading context in this paper. For this task, prior studies have attempted to build specific sentence representations by modeling story entities and events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mi"
2020.tacl-1.7,N19-1423,0,0.0269491,"Missing"
2020.tacl-1.7,P18-1082,0,0.410495,"the sentences in a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence. 1 Introduction Story generation is a strong indicator of machine understanding of natural language. It is often approached as selecting a sequence of events to form a story with a reasonable logic or plot. Although existing generative models (Roemmele, 2016; Fan et al., 2018; Fan et al., 2019) can generate stories with good local coherence, they ∗ Corresponding author: Minlie Huang. 93 Transactions of the Association for Computational Linguistics, vol. 8, pp. 93–108, 2020. https://doi.org/10.1162/tacl a 00302 Action Editor: Noah Smith. Submission batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. these knowledge bases, which can provide additional crucial information for story generation. Empirical experiments demonstrate that training with millions of such examples hel"
2020.tacl-1.7,P19-1254,0,0.347048,"a reasonable story, we use multi-task learning, which combines a discriminative objective to distinguish true and fake stories during fine-tuning. Automatic and manual evaluation shows that our model can generate more reasonable stories than state-of-the-art baselines, particularly in terms of logic and global coherence. 1 Introduction Story generation is a strong indicator of machine understanding of natural language. It is often approached as selecting a sequence of events to form a story with a reasonable logic or plot. Although existing generative models (Roemmele, 2016; Fan et al., 2018; Fan et al., 2019) can generate stories with good local coherence, they ∗ Corresponding author: Minlie Huang. 93 Transactions of the Association for Computational Linguistics, vol. 8, pp. 93–108, 2020. https://doi.org/10.1162/tacl a 00302 Action Editor: Noah Smith. Submission batch: 10/2019; Revision batch: 11/2019; Published 2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. these knowledge bases, which can provide additional crucial information for story generation. Empirical experiments demonstrate that training with millions of such examples helps improve the cohe"
2020.tacl-1.7,P19-1134,0,0.0494618,"Missing"
2020.tacl-1.7,K17-1034,0,0.0131358,"is difficult to match the events extracted from the training data with those stored in KB. (2) Learning and utilizing multi-hop triples in knowledge graphs is costly in time because of the large-scale size. (3) Most of KB triples do not appear in the task-specific training data, so that those absent triples are not fully utilized in existing models. Fortunately, our model is trained on the knowledge bases directly, which can effectively ease these limitations. We transform the commonsense triples in ConceptNet and ATOMIC into readable natural language sentences using a template-based method (Levy et al., 2017), as illustrated in Table 2. We do not use roughly concatenated triples in order to avoid introducing additional special tokens (e.g., UsedFor in ConceptNet and oEffect in ATOMIC), or break the syntactic features contained in the pretrained language model (Alt et al., 2019), which are essential for following story generation. And then the language model is post-trained on the transformed sentences to learn commonsense knowledge between entities and events by minimizing the negative likelihood of predicting the next token: LKG = − |r | X logP (rt |r<t ), Figure 2: An example of fake story const"
2020.tacl-1.7,N16-1014,0,0.158784,"n and fully connected layers. Radford et al. (2019) used a 12-layer decoder-only transformer (GPT-2) (i.e., a left-to-right language model) with masked self-attention heads which are constrained in that every token can only attend to its left context. Formally, the objective in this stage is to minimize the following negative likelihood: LGP T = − |u| X logP (ut |u<t ), (1) P (ut |u<t ) = softmax(HL t W + b), (2) notable work for dialog generation (Zhou et al., 2018). To leverage commonsense knowledge in pretrained language models, we resort to existing large-scale knowledge bases ConceptNet (Li et al., 2016b) and ATOMIC (Sap et al., 2019). The ConceptNet dataset2 consists of triples obtained from the Open Mind Common Sense entries in ConceptNet 5 (Speer and Havasi, 2012). It contains 34 relations in total and represents each knowledge triple by R = (h, r, t), meaning that head concept h has the relation r with tail concept t for example, (cross street, Causes, accident). And the ATOMIC dataset3 is an atlas of everyday commonsense reasoning containing a mass of textual description of inferential knowledge organized as typed if-then triples. For example, a typical if-then triple is (PersonX pays P"
2020.tacl-1.7,P16-1137,0,0.15463,"n and fully connected layers. Radford et al. (2019) used a 12-layer decoder-only transformer (GPT-2) (i.e., a left-to-right language model) with masked self-attention heads which are constrained in that every token can only attend to its left context. Formally, the objective in this stage is to minimize the following negative likelihood: LGP T = − |u| X logP (ut |u<t ), (1) P (ut |u<t ) = softmax(HL t W + b), (2) notable work for dialog generation (Zhou et al., 2018). To leverage commonsense knowledge in pretrained language models, we resort to existing large-scale knowledge bases ConceptNet (Li et al., 2016b) and ATOMIC (Sap et al., 2019). The ConceptNet dataset2 consists of triples obtained from the Open Mind Common Sense entries in ConceptNet 5 (Speer and Havasi, 2012). It contains 34 relations in total and represents each knowledge triple by R = (h, r, t), meaning that head concept h has the relation r with tail concept t for example, (cross street, Causes, accident). And the ATOMIC dataset3 is an atlas of everyday commonsense reasoning containing a mass of textual description of inferential knowledge organized as typed if-then triples. For example, a typical if-then triple is (PersonX pays P"
2020.tacl-1.7,P11-2057,0,0.017414,"d events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge to enrich the limited source information. Commonsense knowledge has been demonstrated to significantly improve dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin"
2020.tacl-1.7,P19-1373,0,0.0670514,"e pretraining models have been widely developed in various NLP tasks. Some work leveraged pretraining to provide better language representations at the word level (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) or sentence level (Le and Mikolov, 2014; Kiros et al., 2015) for various downstream task-specific architectures. However, Radford et al. (2018) and Devlin et al. (2018) suggest that these complex task-specific architectures are no longer necessary, and it is sufficient to merely fine-tune pretrained task-independent transformer language models for downstream tasks. Mehri et al. (2019) explored different pretraining methods based on language models for dialogue context representation learning. Furthermore, Radford et al. (2019) demonstrate pretrained language models (i.e., GPT-2) can perform downstream tasks better than state-of-the-art models even in a zero-shot setting (i.e., without any finetuning on task-specific data). Wolf et al. (2019) fine-tuned GPT-2 for personalized conversation generation, which obtains very competitive results in the challenge. However, as previous studies (See et al., 2019; Holtzman et al., 2019) observed, transferring GPT-2 directly to open-en"
2020.tacl-1.7,D17-1195,0,0.0376794,"able at https:// github.com/thu-coai/CommonsenseStoryGen, and demo is available at http://coai.cs.tsinghua. edu.cn/static/CommonsenseStoryGen. 94 et al., 2014), we build our model based on GPT-2 because of its simplicity and broad applicability. short text descriptions (Jain et al., 2017). Different from these studies, we consider the setting of open-ended story generation from only a limited leading context in this paper. For this task, prior studies have attempted to build specific sentence representations by modeling story entities and events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), read"
2020.tacl-1.7,N16-1098,0,0.503158,"as trying to get home but the other passengers would not let her go. He thought she was going to die because of her weight. She was trying to get home but the other passengers would not let her go. The bus driver would not let her go. Fine-tuned GPT-2: I was on my way to a party. I was on my way to a party and I ’d gotten out of my seat, and started driving. I got a flat tire, so I stopped driving. I drove to the party and had a great time. Table 1: Story examples generated by human and GPT-2 models. The stories written by the pretrained GPT-2 and fine-tuned GPT-2 (post-trained on ROCStories [Mostafazadeh et al., 2016b]) suffer from repetition (in italic), bad inter-sentence coherence to the context (e.g., ignoring key entities such as accident in bold), as well as conflicting logic (underlined, e.g., first stopped driving and then drove to the party), in spite of their good fluency and intra-sentence coherence. • We propose a knowledge-enhanced pretraining model for commonsense story generation by extending GPT-2 with external commonsense knowledge. The model is post-trained on the knowledge examples constructed from ConceptNet and ATOMIC, thereby improving long-range coherence of generated stories. • To"
2020.tacl-1.7,W16-2505,0,0.0960944,"as trying to get home but the other passengers would not let her go. He thought she was going to die because of her weight. She was trying to get home but the other passengers would not let her go. The bus driver would not let her go. Fine-tuned GPT-2: I was on my way to a party. I was on my way to a party and I ’d gotten out of my seat, and started driving. I got a flat tire, so I stopped driving. I drove to the party and had a great time. Table 1: Story examples generated by human and GPT-2 models. The stories written by the pretrained GPT-2 and fine-tuned GPT-2 (post-trained on ROCStories [Mostafazadeh et al., 2016b]) suffer from repetition (in italic), bad inter-sentence coherence to the context (e.g., ignoring key entities such as accident in bold), as well as conflicting logic (underlined, e.g., first stopped driving and then drove to the party), in spite of their good fluency and intra-sentence coherence. • We propose a knowledge-enhanced pretraining model for commonsense story generation by extending GPT-2 with external commonsense knowledge. The model is post-trained on the knowledge examples constructed from ConceptNet and ATOMIC, thereby improving long-range coherence of generated stories. • To"
2020.tacl-1.7,P02-1040,0,0.107429,"ch sentence with RAKE algorithm (Rose et al., 2010). Skeleton-based Model with Reinforcement Learning (SKRL): The model first generates a compressed story including the most critical phrases, called skeleton, and then generates a story conditioned upon the skeleton. The skeleton is automatically learned by reinforcement learning (Xu et al., 2018). 4.4 Automatic Evaluation Evaluation Metrics We adopted the following automatic metrics to evaluate the generation performance in the entire test set. (1) Perplexity (PPL). Smaller perplexity scores indicate better fluency in general. (2) BLEU. BLEU (Papineni et al., 2002) evaluates n-gram overlap between a generated story and a human-written story. However, BLEU is usually inappropriate for openended text generation (Fan et al., 2018) because there are multiple plausible stories for the same input but only one story is given in the dataset. And BLEU scores will become extremely low for large n. We thus experimented with n = 1,2. (3) Coverage. To access the effect of incorporating commonsense knowledge, we calculated the coverage score as the average number of commonsense triples matched in each generated story, which requires both head and tail entities/events"
2020.tacl-1.7,D14-1162,0,0.0837457,"on as input in commonsense knowledge graphs, that is, automatic knowledge base construction. However, the low novelty of the generated objects showed that it could still be difficult for GPT-2 to generate commonsense texts solely based on its implicit knowledge. Therefore, we target integrating external knowledge into GPT-2 for generating more reasonable commonsense stories. 2.2 Pretraining Recently, large-scale pretraining models have been widely developed in various NLP tasks. Some work leveraged pretraining to provide better language representations at the word level (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) or sentence level (Le and Mikolov, 2014; Kiros et al., 2015) for various downstream task-specific architectures. However, Radford et al. (2018) and Devlin et al. (2018) suggest that these complex task-specific architectures are no longer necessary, and it is sufficient to merely fine-tune pretrained task-independent transformer language models for downstream tasks. Mehri et al. (2019) explored different pretraining methods based on language models for dialogue context representation learning. Furthermore, Radford et al. (2019) demonstrate pretrained language models (i.e."
2020.tacl-1.7,N18-1202,0,0.015855,"e knowledge graphs, that is, automatic knowledge base construction. However, the low novelty of the generated objects showed that it could still be difficult for GPT-2 to generate commonsense texts solely based on its implicit knowledge. Therefore, we target integrating external knowledge into GPT-2 for generating more reasonable commonsense stories. 2.2 Pretraining Recently, large-scale pretraining models have been widely developed in various NLP tasks. Some work leveraged pretraining to provide better language representations at the word level (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018) or sentence level (Le and Mikolov, 2014; Kiros et al., 2015) for various downstream task-specific architectures. However, Radford et al. (2018) and Devlin et al. (2018) suggest that these complex task-specific architectures are no longer necessary, and it is sufficient to merely fine-tune pretrained task-independent transformer language models for downstream tasks. Mehri et al. (2019) explored different pretraining methods based on language models for dialogue context representation learning. Furthermore, Radford et al. (2019) demonstrate pretrained language models (i.e., GPT-2) can perform d"
2020.tacl-1.7,K19-1079,0,0.320773,"a University, Beijing 100084, China 2 School of Software, Beihang University, Beijing, China 1 Institute for Artificial Intelligence, State Key Lab of Intelligent Technology and Systems 1 Beijing National Research Center for Information Science and Technology j-guan19@mails.tsinghua.edu.cn,f-huang18@mails.tsinghua.edu.cn, extsuioku@gmail.com, zxy-dcs@tsinghua.edu.cn, aihuang@tsinghua.edu.cn Abstract are still struggling to plan a coherent plot and maintain a reasonable event sequence throughout the story, or they are often biased towards generating a limited set of stories with generic plots (See et al., 2019) (e.g., I have a great time), even when using the powerful generative model OpenAI’s GPT-2 (Radford et al., 2019), as shown in Table 1. Pretrained GPT-2 has been shown to capture useful semantic and syntactic features (Alt et al., 2019), as demonstrated by state-of-theart performance on some generation tasks such as machine translation and text summarization (Radford et al., 2019). However, compared with such tasks whose source inputs have contained sufficient information to generate desired target texts, story generation is a typical openended generation task, where only very limited informat"
2020.tacl-1.7,D19-1321,1,0.78663,"re are multiple plausible stories for the same input but only one story is given in the dataset. And BLEU scores will become extremely low for large n. We thus experimented with n = 1,2. (3) Coverage. To access the effect of incorporating commonsense knowledge, we calculated the coverage score as the average number of commonsense triples matched in each generated story, which requires both head and tail entities/events appears in the same story. (4) Repetition. We measured the redundancy of stories by computing repetition-4, the percentage of generated stories that repeat at least one 4-gram (Shao et al., 2019). (5) Distinct. To measure the generation diversity, we adopted distinct-4 (Li et al., 2016a), the ratio of distinct 4-grams to all the generated 4-grams. Decomposed Model with Semantic Role Labeling (DSRL): It first generates a predicateargument structure conditioned upon the beginning and then generates a story by surface realization on top of the structure. The structures are identified by semantic role labelling (Fan et al., 2019). We also made comparisons with GPT-2 in different settings as follows: GPT-2 (Scratch): The network architecture is the same as GPT-2, but the model is only trai"
2020.tacl-1.7,speer-havasi-2012-representing,0,0.235598,"e knowledge for expanding a reasonable story, handling the causal relationships, as well as deciding the temporal orders between entities and events in context. Explicitly introducing external commonsense knowledge has been shown helpful to improve language understanding and long-range coherence of generated texts (Zhou et al., 2018; Guan et al., 2019; Yang et al., 2019b). For example, for the entities in the given context of Table 1, many potentially related concepts (e.g., run over, cross street) can be inferred and predicted based on external commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). These knowledge bases contain abundant semantic knowledge of concepts and inferential knowledge for commonsense reasoning. We enhance GPT-2 with such knowledge by post-training the model on the knowledge examples constructed from • We conduct extensive experiments with automatic and manual evaluation. Results show that our model can generate more reasonable stories than strong baselines, particularly in terms of logicality and global coherence.1 2 Related Work 2.1 Neural Story Generation Many existing neural story generation models generated stories by condition"
2020.tacl-1.7,P18-1213,0,0.0457733,"Missing"
2020.tacl-1.7,P18-1043,0,0.0289463,"n sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge to enrich the limited source information. Commonsense knowledge has been demonstrated to significantly improve dialogue generation (Zhou et al., 2018), story ending generation (Guan et al., 2019), and essay generation from given topics (Yang et al., 2019b). And recently, some work also attempted to integrate external commonsense knowledge into pretrained models such as BERT (Devlin et al., 2018) to enhance language represent"
2020.tacl-1.7,C16-1100,0,0.0422215,"Missing"
2020.tacl-1.7,D18-1462,0,0.0892502,"build our model based on GPT-2 because of its simplicity and broad applicability. short text descriptions (Jain et al., 2017). Different from these studies, we consider the setting of open-ended story generation from only a limited leading context in this paper. For this task, prior studies have attempted to build specific sentence representations by modeling story entities and events to simplify the dependencies between sentences (Ji et al., 2017; Clark et al., 2018). Another line is to decompose story generation into separate steps (Martin et al., 2018; Fan et al., 2018; Wang et al., 2016; Xu et al., 2018; Yao et al., 2019; Fan et al., 2019). These models usually focused on first planning story sketches and then generating sentences from the sketches. However, improving pretrained models to generate commonsense stories is yet to be well investigated. 2.3 Commonsense Knowledge Incorporating commonsense knowledge is necessary and beneficial for language inference (LoBue and Yates, 2011; Bowman et al., 2015; Rashkin et al., 2018b), reading comprehension (Mihaylov and Frank, 2018; Rashkin et al., 2018a), and particularly for open-ended language generation, which usually requires external knowledge"
2020.tacl-1.7,D16-1023,0,0.0130188,"previous studies (See et al., 2019; Holtzman et al., 2019) observed, transferring GPT-2 directly to open-ended text generation still suffers from several issues such as repetition or lack of knowledge and inter-sentence coherence with different decoding algorithms. Besides, although Song et al. (2019) and Dong et al. (2019) extended the language model to support an encoder-decoder framework (Sutskever 2.4 Multi-Task Learning Incorporating other auxiliary task objectives to complement the primary goal has been shown to improve the performance in many NLP tasks such as sentiment classification (Yu and Jiang, 2016) and conversation generation (Zhao et al., 2017). Recently, multi-task learning was also used to pretrain language models to capture dependencies in context (Devlin et al., 2018; Mehri et al., 2019) and further improve pretrained models’ representation power during fine-tuning (Wolf et al., 2019). 95 3 Methodology The task in this work can be defined as follows: Given a one-sentence story beginning X as the leading context, the model should continue to complete a K -sentence story Y with a reasonable plot. The sentences in a generated story should have reasonable logical connections, causal re"
2020.tacl-1.7,P19-1139,0,0.0511831,"Missing"
2020.tacl-1.7,P19-1226,0,0.376298,"istinguish true stories from auto-constructed fake stories. The auxiliary task makes the model implicitly capture the causal, temporal dependencies between sentences and inter-sentence coherence, and lead to less repetition. dependent commonsense knowledge for expanding a reasonable story, handling the causal relationships, as well as deciding the temporal orders between entities and events in context. Explicitly introducing external commonsense knowledge has been shown helpful to improve language understanding and long-range coherence of generated texts (Zhou et al., 2018; Guan et al., 2019; Yang et al., 2019b). For example, for the entities in the given context of Table 1, many potentially related concepts (e.g., run over, cross street) can be inferred and predicted based on external commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). These knowledge bases contain abundant semantic knowledge of concepts and inferential knowledge for commonsense reasoning. We enhance GPT-2 with such knowledge by post-training the model on the knowledge examples constructed from • We conduct extensive experiments with automatic and manual evaluation. Results show t"
2020.tacl-1.7,P17-1061,0,0.0273739,"l., 2019) observed, transferring GPT-2 directly to open-ended text generation still suffers from several issues such as repetition or lack of knowledge and inter-sentence coherence with different decoding algorithms. Besides, although Song et al. (2019) and Dong et al. (2019) extended the language model to support an encoder-decoder framework (Sutskever 2.4 Multi-Task Learning Incorporating other auxiliary task objectives to complement the primary goal has been shown to improve the performance in many NLP tasks such as sentiment classification (Yu and Jiang, 2016) and conversation generation (Zhao et al., 2017). Recently, multi-task learning was also used to pretrain language models to capture dependencies in context (Devlin et al., 2018; Mehri et al., 2019) and further improve pretrained models’ representation power during fine-tuning (Wolf et al., 2019). 95 3 Methodology The task in this work can be defined as follows: Given a one-sentence story beginning X as the leading context, the model should continue to complete a K -sentence story Y with a reasonable plot. The sentences in a generated story should have reasonable logical connections, causal relationships, and temporal dependencies with each"
2020.tacl-1.7,P19-1193,0,0.423439,"istinguish true stories from auto-constructed fake stories. The auxiliary task makes the model implicitly capture the causal, temporal dependencies between sentences and inter-sentence coherence, and lead to less repetition. dependent commonsense knowledge for expanding a reasonable story, handling the causal relationships, as well as deciding the temporal orders between entities and events in context. Explicitly introducing external commonsense knowledge has been shown helpful to improve language understanding and long-range coherence of generated texts (Zhou et al., 2018; Guan et al., 2019; Yang et al., 2019b). For example, for the entities in the given context of Table 1, many potentially related concepts (e.g., run over, cross street) can be inferred and predicted based on external commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012) and ATOMIC (Sap et al., 2019). These knowledge bases contain abundant semantic knowledge of concepts and inferential knowledge for commonsense reasoning. We enhance GPT-2 with such knowledge by post-training the model on the knowledge examples constructed from • We conduct extensive experiments with automatic and manual evaluation. Results show t"
2021.acl-long.142,N19-1078,0,0.0383627,"l together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is defined to be a special start symbol. WT"
2021.acl-long.142,C18-1139,0,0.337962,"uch to the Supreme Court . Senate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP"
2021.acl-long.142,D19-1539,0,0.0937785,"Missing"
2021.acl-long.142,D19-1195,0,0.0140693,"nslation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts 1807 based on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-super"
2021.acl-long.142,P18-1015,0,0.0136535,"neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we use BERTScore to re-rank the retrieved texts instead as BERTScore evaluates semantic correlations between the texts 1807 based on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approa"
2021.acl-long.142,D18-1217,0,0.0495751,"at can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. Sun (2013) and Xu et al. (2013) have extensively studied various multiview learning approaches. Hu et al. (2021) shows the effectiveness of multi-view learning on crosslingual structured prediction tasks. Recently, Clark et al. (2018) proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the external contexts and therefore we do not need to construct auxiliary prediction modules in the model. Moreover, CVT cannot"
2021.acl-long.142,P19-1082,0,0.0282248,"rength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual scoring functions to re-rank the retrieved texts. Instead, we us"
2021.acl-long.142,D18-1111,0,0.0779912,"Table 3: A comparison of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that the tf-idf weighting"
2021.acl-long.142,2020.acl-main.747,0,0.0789365,"(Liu et al., 2019) for token representations which is the default configuration in the code5 of BERTScore (Zhang et al., 2020). For token representations in the NER model, 2 the accuracy of a query counts 1.0 if all the entities in the query are correctly recognized and 0.0 otherwise. 3 If the descriptions are not available, we use the titles of the results instead. 4 We determined that 6 is a reasonable number based on preliminary experiments. 5 https://github.com/Tiiiger/bert_score 1804 we use pretrained Bio-BERT (Lee et al., 2020) for datasets from the biomedical domain and use XLMRoBERTa (Conneau et al., 2020) for datasets from other domains. Training During training, we fine-tune the pretrained contextual embeddings by AdamW (Loshchilov and Hutter, 2018) optimizer with a batch size of 4. We use a learning rate of 5 × 10−6 to update the parameters in the pretrained contextual embeddings. For the CRF layer parameters, we use a learning rate of 0.05. We train the NER models for 10 epochs for the datasets in Social Media and Biomedical domains while we train the NER models for 5 epochs for other datasets for efficiency as these datasets have more training sentences. 3.2 Results We experiment on the fo"
2021.acl-long.142,2021.acl-long.207,1,0.753104,"ased on pretrained contextual embeddings. 6 Multi-View Learning Multi-View Learning is a technique applied to inputs that can be split into multiple subsets. Co-training (Blum and Mitchell, 1998) and co-regularization (Sindhwani and Niyogi, 2005) train a separate model for each view. These approaches are semi-supervised learning techniques that require two independent views of the data. The model with higher confidence is applied to construct additional labeled data by predicting on unlabeled data. Sun (2013) and Xu et al. (2013) have extensively studied various multiview learning approaches. Hu et al. (2021) shows the effectiveness of multi-view learning on crosslingual structured prediction tasks. Recently, Clark et al. (2018) proposed Cross-View Training (CVT), which trains a unified model instead of multiple models and targets at minimizing the KL divergence between the probability distributions of the model and auxiliary prediction modules. Comparing with CVT, CL targets at improving the accuracy of two kinds of inputs rather than only one of them. We also propose to minimize the distance of token representations between different views in addition to KL-divergence. Besides, CL utilizes the e"
2021.acl-long.142,W17-4418,0,0.101114,"Missing"
2021.acl-long.142,N19-1423,0,0.0665368,"ate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in wh"
2021.acl-long.142,P19-1236,0,0.15668,"ar dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets. Wang et al. (2019) fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets. • Biomedical: We use BC5CDR (Li et al., 2016) and NCBI-disease (Do˘gan et al., 2014) datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following Nooralahzadeh et al. (2019). • Science and Technology: We use CBS SciTech News dataset collected by Jia et al. (2019). The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain. • E-commerce: We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training. We show the statistics of the datasets in Table 1. Annotations of the E-commerce dataset We manually labeled the user queries through crowdsourcing from www.aliexpress"
2021.acl-long.142,2020.coling-main.207,0,0.0253611,"t work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al., 2020) are usual sco"
2021.acl-long.142,N16-1030,0,0.0914812,"ds the texts to a transformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is"
2021.acl-long.142,2020.acl-main.45,0,0.0391605,"Missing"
2021.acl-long.142,2021.ccl-1.108,0,0.0533332,"Missing"
2021.acl-long.142,2020.coling-main.78,0,0.312318,"• CL-L2 represents minimizing the L2 distance between token representations (Eq. 5). • CL-KL represents minimizing the KL divergence (Eq. 8) between CRF output distributions. Besides, we also compare our approaches with previous state-of-the-art approaches over entity-level F1 scores6 . During the evaluation, our approaches are evaluated using inputs without external contexts (W / O C ONTEXT) and inputs with them (W / C ONTEXT). We report the results averaged over 5 runs in our experiments. The results are listed in 6 We do not compare the results from previous work such as Yu et al. (2020); Luoma and Pyysalo (2020); Yamada et al. (2020) that utilizes the document-level contexts in CoNLL-03 NER here. We conduct a comparison with these approaches in Appendix A. Table 27 . With the external contexts, our models with CL outperform previous state-of-the-art approaches on most of the datasets. Our approaches significantly outperform the baseline that is trained without external contexts with only one exception. Comparing with LUKE, our approaches and our baseline outperform LUKE in all the cases. The possible reason is that LUKE is pretrained only using long word sequences, which makes the model prone to fail"
2021.acl-long.142,P16-1101,0,0.175555,"nsformer-based model together with the input sentence. Finally, we calculate the negative likelihood loss LNLL and LNLL-EXT together with the CL loss (either LCL-L2 or LCL-KL ). ternal contexts: ˜ = [sep_token; x ˆ1; · · · ; x ˆl] x where sep_token is a special token representing a separate of sentences in the transformer-based pretrained contextual embeddings (for example, “[SEP]” in BERT). 2.2 NER Model We solve the NER task as a sequence labeling problem. We apply a neural model with a CRF layer, which is one of the most popular state-of-the-art approaches to the task (Lample et al., 2016; Ma and Hovy, 2016; Akbik et al., 2019). In the sequence labeling model, the input sentence x is fed into a transformer-based pretrained contextual embeddings model to get the token representations {v1 , · · · , vn } by vi =embedi (x). The token representations are fed into a CRF layer to get the conditional probability pθ (y|x): ψ(y 0 , y, vi ) = exp(WyT vi + by0 ,y ) (1) n Q ψ(yi−1 , yi , vi ) i=1 pθ (y|x) = n P Q 0 , y0 , v ) ψ(yi−1 i i y 0 ∈Y(x) i=1 where ψ is the potential function and θ represents the model parameters. Y(x) denotes the set of all possible label sequences given x. y0 is defined to be a spe"
2021.acl-long.142,2020.emnlp-demos.2,0,0.0268364,"Missing"
2021.acl-long.142,2020.emnlp-main.107,0,0.0631329,"Missing"
2021.acl-long.142,D19-6125,0,0.0628019,"ulder, 2003) dataset and CoNLL++ (Wang et al., 2019) dataset. The CoNLL-03 dataset is the most popular dataset for NER. CoNLL++ is a revision of the CoNLL-03 datasets. Wang et al. (2019) fixed annotation errors on the test set by professional annotators and improved the quality of the training data through their CrossWeigh approach. We use the standard dataset split for these datasets. • Biomedical: We use BC5CDR (Li et al., 2016) and NCBI-disease (Do˘gan et al., 2014) datasets, which are two popular biomedical NER datasets. We merge the training and development data as training set following Nooralahzadeh et al. (2019). • Science and Technology: We use CBS SciTech News dataset collected by Jia et al. (2019). The dataset only contains the test set with the same label set as the CoNLL-03 dataset. We use the dataset to evaluate the effectiveness of crossdomain transferability from the news domain. • E-commerce: We collect and annotate an internal dataset from one anonymous E-commerce website. The dataset contains 25 named entity labels for goods in short texts. We also collect 300,000 unlabeled sentences for semi-supervised training. We show the statistics of the datasets in Table 1. Annotations of the E-comme"
2021.acl-long.142,N18-1202,0,0.0156664,"ibuster and confirm Neil Gorsuch to the Supreme Court . Senate Republicans deployed the nuclear option on Wednesday to drastically reduce the time it takes to confirm hundreds of President Trump s nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at htt"
2021.acl-long.142,W16-3919,0,0.0224422,"Missing"
2021.acl-long.142,W02-2024,0,0.403859,"Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve th"
2021.acl-long.142,2020.acl-main.304,1,0.865963,"(θ) = LCL-L2 (θ) + LCL-KL (θ) in Eq. 9). Results in Table 7 show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model Related Work Named Entity Recognition Named Entity Recognition (Sundheim, 1995) has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF (Lafferty et al., 2001) to achieve state-of-the-art accuracy (Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieva"
2021.acl-long.142,2021.acl-long.206,1,0.769048,"Missing"
2021.acl-long.142,2020.findings-emnlp.356,1,0.898596,"(θ) = LCL-L2 (θ) + LCL-KL (θ) in Eq. 9). Results in Table 7 show that the external context can help to improve the accuracy even when the NER model is trained without the contexts. However, when the model is trained with the external contexts, the accuracy of the model Related Work Named Entity Recognition Named Entity Recognition (Sundheim, 1995) has been studied for decades. Most of the work takes NER as a sequence labeling problem and applies the linear-chain CRF (Lafferty et al., 2001) to achieve state-of-the-art accuracy (Ma and Hovy, 2016; Lample et al., 2016; Akbik et al., 2018, 2019; Wang et al., 2020b). Recently, the improvement of accuracy mainly benefits from stronger token representations such as pretrained contextual embeddings such as BERT (Devlin et al., 2019), Flair (Akbik et al., 2018) and LUKE (Yamada et al., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieva"
2021.acl-long.142,2021.acl-long.46,1,0.801203,"Missing"
2021.acl-long.142,D19-1519,0,0.105801,"‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search queries, tweets and short comments in various domains such as social media and E-commerce domains. When professional annotators annotate ambiguous named entities in such cases, they usually rely on domain knowledge for disambiguation. This kind of knowledge can often be found through a search engine. Moreover, when the annotators are not sure about a certain entity, they are usually encouraged to find related knowledge through a search engine (Wang et al., 2019). Therefore, we believe that NER models can benefit from such a process as well. In this paper, we propose to improve NER models by retrieving texts related to the input sentence by an off-the-shelf search engine. We re-rank the retrieved texts according to their semantic relevance to the input sentence and select several top-ranking texts as the external contexts. Consequently, we concatenate the input sentence and external contexts together as a new retrieval-based input view and feed it to the pretrained contextual embedding 1800 Proceedings of the 59th Annual Meeting of the Association for"
2021.acl-long.142,2020.acl-main.144,0,0.18718,"n of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that the tf-idf weighting gives high weight"
2021.acl-long.142,2020.emnlp-main.523,0,0.252158,"Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search queries, tweets and short"
2021.acl-long.142,2020.acl-main.577,0,0.336047,"nominees . Label: Group Figure 1: A motivating example from WNUT-17 dataset. The retrieved texts help the model to correctly predict the named entities of “democrats” and “republican”. Introduction ∗ Retrieved Texts: senate democrats eliminated the nuclear option when they had the majority a few years ago , over republican objections . Label: Non Entity Pretrained contextual embeddings such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018) and BERT (Devlin et al., 2019) have significantly improved the accuracy of Named Entity Recognition (NER) models. Recent work (Devlin et al., 2019; Yu et al., 2020; Yamada et al., 2020) found that including document-level contexts of the target sentence in the input of contextual embeddings methods can further boost the accuracy of NER models. However, there are a lot of application scenarios Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/CLNER. ‡ Input Sentence: in which document-level contexts are unavailable in practice. For example, there are sometimes no available contexts in users’ search quer"
2021.acl-long.142,N18-1120,0,0.146753,"core. BS: BERTScore. Table 3: A comparison of different approaches in transfer learning. The models are trained on the CoNLL-03 dataset. Approach CL-L2 CL-KL CL–L2 +S EMI CL-KL+S EMI SE 59.95 61.79 4.1 Comparison of Re-ranking Approaches Various re-ranking approaches may affect the token representations of the model. We compare our approach with three other re-ranking approaches. The first is the ranking from the search engine without any re-ranking approaches. The second is reranking through a fuzzy match score. The approach has been widely applied in a lot of previous work (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020). The third is BERTScore with tf-idf importance weighting which makes rare words more indicative than common words in scoring. We train our models (W / C ONTEXT) with external contexts from these re-ranking approaches and report the averaged and best results on WNUT17 in Table 5. Our results show that re-ranking with BERTScore performs the best, which shows the semantic relevance is helpful for the performance. However, for BERTScore with the tf-idf weighting, the accuracy of the model drops significantly (with p < 0.05). The possible reason might be that"
2021.acl-long.142,P19-1336,0,0.0214626,"Missing"
2021.acl-long.142,W18-5713,0,0.0243972,"l., 2020). Very recent work (Yu et al., 2020; Yamada et al., 2020) utilizes the strength of pretrained contextual embeddings over long-range dependency and encodes the document-level contexts for token representations to achieve state-of-the-art accuracy on CoNLL 2002/2003 NER datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Improving Models through Retrieval Retrieving related texts from a certain database (such as the training set) has been widely applied in tasks such as neural machine translation (Gu et al., 2018; Zhang et al., 2018; Xu et al., 2020), text generation (Weston et al., 2018; Kim et al., 2020), semantic parsing (Hashimoto et al., 2018; Guo et al., 2019). Most of the work uses the retrieved texts to guide the generation or refine the retrieved texts through the neural model, while we take the retrieved texts as the contexts of the input sentence to improve the semantic representations of the input tokens. For the re-ranking models, fuzzy match score (Gu et al., 2018; Zhang et al., 2018; Hayati et al., 2018; Xu et al., 2020), attention mechanisms (Cao et al., 2018; Cai et al., 2019), and dot products between sentence representations (Lewis et al., 2020; Xu et al.,"
2021.acl-long.206,N19-1078,0,0.0939277,"Missing"
2021.acl-long.206,J88-1003,0,0.685257,"CE can find a strong word representation on a single GPU with only a few GPU-hours for structured prediction tasks. In comparison, a lot of NAS approaches require dozens or even thousands of GPU-hours to search for good neural architectures for their corresponding tasks. Empirical results show that ACE outperforms strong baselines. Furthermore, when ACE is applied to concatenate pretrained contextualized embeddings fine-tuned on specific tasks, we can achieve state-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are t"
2021.acl-long.206,C18-1139,0,0.623884,"rms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embeddi"
2021.acl-long.206,N19-1423,0,0.618458,"hieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embedding learning methods that ope"
2021.acl-long.206,D19-1539,0,0.111357,"Missing"
2021.acl-long.206,Q17-1010,0,0.0604138,"diction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of contextualized character embeddings and"
2021.acl-long.206,W06-2920,0,0.0403544,"ur search design can usually lead to better results compared to both of the baselines. 4.3.2 Comparison With State-of-the-Art approaches As we have shown, ACE has an advantage in searching for better embedding concatenations. We further show that ACE is competitive or even stronger than state-of-the-art approaches. We additionally use XLNet (Yang et al., 2019) and RoBERTa as the candidates of ACE. In some tasks, we have several additional settings to better compare with previous work. In NER, we also conduct a comparison on the revised version of German datasets in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). Recent work such as Yu et al. (2020) and Yamada et al. (2020) utilizes document contexts in the datasets. We follow their work and extract document embeddings for the transformer-based embeddings. Specifically, we follow the fine-tune process of Yamada et al. (2020) to fine-tune the transformer-based embeddings over the document except for BERT and M-BERT embeddings. For BERT and M-BERT, we follow the document extraction process of Yu et al. (2020) because we find that the model with such document embeddings is significantly stronger than the model trained with the fine-tuning process of Yam"
2021.acl-long.206,2020.acl-main.777,0,0.0351125,"Missing"
2021.acl-long.206,D18-1217,0,0.10887,"Missing"
2021.acl-long.206,2020.acl-main.607,1,0.770928,"Missing"
2021.acl-long.206,D16-1139,0,0.0528067,"also find that the PTB dataset used by Mrini et al. (2020) is not identical to the dataset in previous work such as Zhang et al. (2020) and Wang and Tu (2020). ‡ : For reference, we confirmed with the authors of He and Choi (2020) that they used a different data pre-processing script with previous work. . ACE No discount (Eq. 5) Simple (Eq. 4) D EV 93.18 92.98 92.89 T EST 90.00 89.90 89.82 embeddings that are not very useful in the concatenation. Moreover, ACE models can be used to guide the training of weaker models through techniques such as knowledge distillation in structured prediction (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a, 2021b), leading to models that are both stronger and faster. Table 5: Comparison of reward functions. NER POS AE CHK All Random ACE All+Weight Ensemble Ensembledev Ensembletest 92.4 92.6 93.0 92.7 92.2 92.2 92.7 90.6 91.3 91.7 90.4 90.6 90.8 91.4 73.2 74.7 75.6 73.7 68.1 70.2 73.9 96.7 96.7 96.8 96.7 96.5 96.7 96.7 DP SDP UAS LAS ID OOD 96.7 95.1 94.3 90.8 96.8 95.2 94.4 90.8 96.9 95.3 94.5 90.9 96.7 95.1 94.3 90.7 96.1 94.3 94.1 90.3 96.8 95.2 94.3 90.7 96.8 95.2 94.4 90.8 7 Table 6: A comparison among All, Random, ACE, All+Weight and Ensemble. CHK:"
2021.acl-long.206,D19-1279,0,0.0134518,"e each edge represents the inputs and outputs between these nodes. In ACE, we represent each 2645 embedding candidate as a node. The input to the nodes is the input sentence x, and the outputs are the embeddings v l . Since we concatenate the embeddings as the word representation of the task model, there is no connection between nodes in our search space. Therefore, the search space can be significantly reduced. For each node, there are a lot of options to extract word features. Taking BERT embeddings as an example, Devlin et al. (2019) concatenated the last four layers as word features while Kondratyuk and Straka (2019) applied a weighted sum of all twelve layers. However, the empirical results (Devlin et al., 2019) do not show a significant difference in accuracy. We follow the typical usage for each embedding to further reduce the search space. As a result, each embedding only has a fixed operation and the resulting search space contains 2L −1 possible combinations of nodes. In NAS, weight sharing (Pham et al., 2018a) shares the weight of structures in training different neural architectures to reduce the training cost. In comparison, we fixed the weight of pretrained embedding candidates in ACE except for"
2021.acl-long.206,D16-1180,0,0.0966408,"PTB dataset used by Mrini et al. (2020) is not identical to the dataset in previous work such as Zhang et al. (2020) and Wang and Tu (2020). ‡ : For reference, we confirmed with the authors of He and Choi (2020) that they used a different data pre-processing script with previous work. . ACE No discount (Eq. 5) Simple (Eq. 4) D EV 93.18 92.98 92.89 T EST 90.00 89.90 89.82 embeddings that are not very useful in the concatenation. Moreover, ACE models can be used to guide the training of weaker models through techniques such as knowledge distillation in structured prediction (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a, 2021b), leading to models that are both stronger and faster. Table 5: Comparison of reward functions. NER POS AE CHK All Random ACE All+Weight Ensemble Ensembledev Ensembletest 92.4 92.6 93.0 92.7 92.2 92.2 92.7 90.6 91.3 91.7 90.4 90.6 90.8 91.4 73.2 74.7 75.6 73.7 68.1 70.2 73.9 96.7 96.7 96.8 96.7 96.5 96.7 96.7 DP SDP UAS LAS ID OOD 96.7 95.1 94.3 90.8 96.8 95.2 94.4 90.8 96.9 95.3 94.5 90.9 96.7 95.1 94.3 90.7 96.1 94.3 94.1 90.3 96.8 95.2 94.3 90.7 96.8 95.2 94.4 90.8 7 Table 6: A comparison among All, Random, ACE, All+Weight and Ensemble. CHK: chunking. performs al"
2021.acl-long.206,N16-1030,0,0.474791,"pendency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of contextualized character embeddings and achieved strong performance in sequence labeling tasks. Recently, Devlin et al. (2019) proposed BERT, which encodes contextualized sub-word information by Transformers (Vaswani et al., 2017) and s"
2021.acl-long.206,D19-5505,0,0.0269117,"Missing"
2021.acl-long.206,N18-1088,0,0.0142024,"e hyper-parameter of each structure and decide the input order of each structure. Evolutionary algorithms have been applied to architecture search for many decades (Miller et al., 1989; Angeline et al., 1994; Stanley and Miikkulainen, 2002; Floreano et al., 2008; Jozefowicz et al., 2015). The algorithm repeatedly generates new populations through recombination and mutation operations and selects survivors through competing among the population. Recent work with evolutionary algorithms differ in the method on parent/survivor selection and population generation. For example, Real et al. (2017), Liu et al. (2018a), Wistuba (2018) and Real et al. (2019) applied tournament selection (Goldberg and Deb, 1991) for the parent selection while Xie and Yuille (2017) keeps all parents. Suganuma et al. (2017) and Elsken et al. (2018) chose the best model while Real et al. (2019) chose several latest models as survivors. 3 Given an embedding concatenation generated from the controller, the task model is trained over the task data and returns a reward to the controller. The controller receives the reward to update its parameter and samples a new embedding concatenation for the task model. Figure 1 shows the gener"
2021.acl-long.206,2020.emnlp-demos.2,0,0.0370801,"the controller for 50 steps. Table 5 shows that both the discount factor and the binary vector |at − ai |for the task are helpful in both development and test datasets. 4 Please refer to Appendix for more details about the embeddings. 5 We compare ACE with other fine-tuned embeddings in Appendix. 2649 Baevski et al. (2019) Straková et al. (2019) Yu et al. (2020) Yamada et al. (2020) XLM-R+Fine-tune ACE+Fine-tune de 85.1 86.4 87.7 88.3 de06 90.3 91.4 91.7 NER en 93.5 93.4 93.5 94.3 94.1 94.6 es 88.8 90.3 89.3 95.9 nl 92.7 93.7 95.3 95.7 Owoputi et al. (2013) Gui et al. (2017) Gui et al. (2018) Nguyen et al. (2020) XLM-R+Fine-tune ACE+Fine-tune Ritter 90.4 90.9 91.2 90.1 92.3 93.4 POS ARK 93.2 92.4 94.1 93.7 94.4 TB-v2 94.6 92.8 95.2 95.4 95.8 Table 2: Comparison with state-of-the-art approaches in NER and POS tagging. † : Models are trained on both train and development set. C HUNK CoNLL 2000 Akbik et al. (2018) Clark et al. (2018) Liu et al. (2019b) Chen et al. (2020) XLM-R+Fine-tune ACE+Fine-tune AE 14Lap 14Res 15Res 16Res Xu et al. (2018)† Xu et al. (2019) Wang et al. (2020a) Wei et al. (2020) XLM-R+Fine-tune ACE+Fine-tune 96.7 97.0 97.3 95.5 97.0 97.3 84.2 84.3 82.7 85.9 87.4 84.6 87.1 90.5 92.0 72"
2021.acl-long.206,S15-2153,0,0.0506143,"Missing"
2021.acl-long.206,S14-2008,0,0.0159013,"thousands of GPU-hours to search for good neural architectures for their corresponding tasks. Empirical results show that ACE outperforms strong baselines. Furthermore, when ACE is applied to concatenate pretrained contextualized embeddings fine-tuned on specific tasks, we can achieve state-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embedding"
2021.acl-long.206,N13-1039,0,0.0434432,"Missing"
2021.acl-long.206,D14-1162,0,0.0909253,"-of-the-art accuracy on 6 structured prediction tasks including Named Entity Recognition (Sundheim, 1995), Part-Of-Speech tagging (DeRose, 1988), chunking (Tjong Kim Sang and Buchholz, 2000), aspect extraction (Hu and Liu, 2004), syntactic dependency parsing (Tesnière, 1959) and semantic dependency parsing (Oepen et al., 2014) over 21 datasets. Besides, we also analyze the advantage of ACE and reward function design over the baselines and show the advantage of ACE over ensemble models. 2 2.1 Related Work Embeddings Non-contextualized embeddings, such as word2vec (Mikolov et al., 2013), GloVe (Pennington et al., 2014), and fastText (Bojanowski et al., 2017), help lots of NLP tasks. Character embeddings (Santos and Zadrozny, 2014) are trained together with the task and applied in many structured prediction tasks (Ma and Hovy, 2016; Lample et al., 2016; Dozat and Manning, 2018). For pretrained contextualized embeddings, ELMo (Peters et al., 2018), a pretrained contextualized word embedding generated with multiple Bidirectional LSTM layers, significantly outperforms previous state-of-the-art approaches on several NLP tasks. Following this idea, Akbik et al. (2018) proposed Flair embeddings, which is a kind of"
2021.acl-long.206,N18-1202,0,0.0524695,"ow that our approach outperforms strong baselines and achieves state-of-the-art performance with fine-tuned embeddings in all the evaluations.1 1 Introduction Recent developments on pretrained contextualized embeddings have significantly improved the performance of structured prediction tasks in natural ∗ Yong Jiang and Kewei Tu are the corresponding authors. : This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/ACE. ‡ language processing. Approaches based on contextualized embeddings, such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-"
2021.acl-long.206,S15-2082,0,0.0548003,"Missing"
2021.acl-long.206,S14-2004,0,0.0702456,"Missing"
2021.acl-long.206,2020.acl-main.304,1,0.783349,", such as ELMo (Peters et al., 2018), Flair (Akbik et al., 2018), BERT (Devlin et al., 2019), and XLM-R (Conneau et al., 2020), have been consistently raising the state-of-the-art for various structured prediction tasks. Concurrently, research has also showed that word representations based on the concatenation of multiple pretrained contextualized embeddings and traditional non-contextualized embeddings (such as word2vec (Mikolov et al., 2013) and character embeddings (Santos and Zadrozny, 2014)) can further improve performance (Peters et al., 2018; Akbik et al., 2018; Straková et al., 2019; Wang et al., 2020b). Given the ever-increasing number of embedding learning methods that operate on different granularities (e.g., word, subword, or character level) and with different model architectures, choosing the best embeddings to concatenate for a specific task becomes non-trivial, and exploring all possible concatenations can be prohibitively demanding in computing resources. Neural architecture search (NAS) is an active area of research in deep learning to automatically search for better model architectures, and has achieved state-of-the-art performance on various tasks in computer vision, such as im"
2021.acl-long.207,P19-1299,0,0.110715,"situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019). To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity. However, these similaritybased approaches are heuristic-based, and cannot well learn the confidence level of multiple source models. In this paper, we propose to leverage a small number of labeled target data to selectively transfer the knowledge from multiple source models. 2661 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2661–2674 August 1–6, 2021."
2021.acl-long.207,D18-1217,0,0.335977,"a new task-specific model in the target language. Both the aggregation model and target task-specific model can map the inputs to the structured outputs but there exists a tradeoff. The aggregation model generally has strong cross-lingual ability since source models are firstly well trained1 , but has lower flexibility since source models are usually frozen. Instead, the target taskspecific model tends to be more flexible and has strong capacity but has poor performance since the model is easily over-fitted on the small training sample. Inspired by previous work on multi/cross-view learning (Clark et al., 2018; Jiang et al., 2019; Fei and Li, 2020), we regard the aggregation model (aggregated source view) and the target taskspecific model (target view) as two views since they both can map the input sentence to structured outputs. We propose a novel multi-view framework to achieve a good trade-off between the two views. To capture the diverse strength and weakness of multiple source models, we propose three approaches to obtain the aggregated source view from language/sentence/sub-structure level in a coarse-to-fine manner. By encouraging two views to influence each other, the proposed framework can"
2021.acl-long.207,2020.acl-main.747,0,0.194106,"Missing"
2021.acl-long.207,J88-1003,0,0.0882324,"ews to interact with each other, our framework can dynamically adjust the confidence level of each source model and improve the performance of both views during training. Experiments for three structured prediction tasks on sixteen data sets show that our framework achieves significant improvement over all existing approaches, including these with access to additional source language data. 1 Introduction Structured prediction is the task of mapping input sentences to structured outputs. It is a fundamental task in natural language processing and has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019;"
2021.acl-long.207,D17-1005,0,0.0176857,"base model for all approaches. We run each approach five times and report the averaged accuracy for POS tagging, f1-score for NER, and unlabelled attachment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability di"
2021.acl-long.207,D15-1166,0,0.0559339,"el Aggregation We simply introduce a trainable probability vector αlang , which is depicted on the bottom right part of 2663 the Figure 1. The final output distribution of the aggregated source view can be computed as, pS (y|x) = K X (1) (k) αlang k=1 · αsub (xi ) = ] (t) Softmax(hi WKTi ) Then the aggregation distribution becomes, pS (y|x) = n X K Y i=1 k=1 (k) αsub (xi ) · p(k) s (yi |x) In this approach, our target model acts as a selector to dynamically assess the multiple source models on sub-structure level. Sentence-level Aggregation In this section, we leverage an attention mechanism (Luong et al., 2015; Vaswani et al., 2017) to learn the weight of each source model on an input sentence, as shown on the top right part of Figure 1. Firstly, we use the internal states of the [CLS] (t) token as sentence representation. Secondly, h0 from the target model T is used as a query to attend (k) h0 from the k-th source model Sk to produce the probabilities αsent (x) ∈ RK . (1) (t) αsent (x) = Softmax(h0 WKT0 ) where K0 is the concatenation of sentence representations from K source models, and W ∈ Rd×d is the bilinear weight matrix. Then the probabilities are utilized to compute the aggregation distribu"
2021.acl-long.207,D18-1061,0,0.0650358,"Missing"
2021.acl-long.207,P11-2052,0,0.097664,"Missing"
2021.acl-long.207,P19-1015,0,0.453502,"require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.e., domain, word order, capitalization, and script style. However, in practice, we are more likely to encounter the situation where some source languages are not as similar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahi"
2021.acl-long.207,D17-1038,0,0.0266652,"d weakness of different source models (see Sec.1 for more discussion.). Sub-structure-level Aggregation We further propose a fine-grained aggregation approach on sub-structure level, which is also based on the attention mechanism. As shown in the left part of Figure 1, for token xi in a given sentence x, (t) we use its representation hi as the query to attend the corresponding representation from each source 3 We also try many metrics of measuring the similarity between two probability distributions, e.g., mean squared error (MSE) (Wu et al., 2020), Cosine, and Jensen-Shannon divergence (JS) (Ruder and Plank, 2017), and we find KL perform best. 2664 2. KD assigns equal importance to multiple source models, which can be seen as a fixed uniform vector in our language-level aggregation approach. 3. Besides language-level aggregation, we propose two fine-grained aggregation strategies to dynamically balance the information from source models. 4. To achieve the previously described goal, our approach has trainable parameters in the aggregation component and our multi-view learning framework can jointly learn the parameters of two views. 2.5 Training and Inference Strategies Following previous work on cross-l"
2021.acl-long.207,P18-1096,0,0.552775,"ompare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also provide the maximal results of DT on language level (DT-Max(lang)) 6 . Hard-KD The hard knowledge distillation approaches first pred"
2021.acl-long.207,D18-1548,0,0.053178,"Missing"
2021.acl-long.207,W02-2024,0,0.719114,"2020), we conduct the experiments in a leaveone-out setting in which we hold out one language as the target language and the others as the source languages. To simulate the low-resources scenario, for each training set in a specific target language, we randomly select fifty sentences 4 with the gold annotations and discard the annotations of the remaining sentences to construct the training set. We randomly select six languages from Universal Dependencies Treebanks (v2.2)5 for dependency parsing and POS tagging tasks. We use the datasets from CoNLL 2002 and CoNLL 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for NER tasks. We utilize the base cased multilingual BERT (Devlin et al., 2019) as base model for all approaches. We run each approach five times and report the averaged accuracy for POS tagging, f1-score for NER, and unlabelled attachment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-thear"
2021.acl-long.207,Q14-1005,0,0.0892508,"has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics, i.e., domain, word ord"
2021.acl-long.207,2021.acl-long.142,1,0.835839,"Missing"
2021.acl-long.207,2020.acl-main.581,0,0.0860103,"milar to the target language and may lead to worse performance (Rosenstein et al., 2005; Rahimi et al., 2019) (we provide an example in the Appendix A). To tackle this challenging problem, most of the previous works do majority voting (Plank and Agi´c, 2018) and truth inference on hard predictions of multiple sources (Rahimi et al., 2019). To better incorporate target language information, some recent works train a new model on the target unlabeled data with hard/soft predictions from multiple source models, such as mixture-of-experts model (Chen et al., 2019) and knowledge distillation (KD) (Wu et al., 2020), and assign weights to multiple sources based on language similarity. However, these similaritybased approaches are heuristic-based, and cannot well learn the confidence level of multiple source models. In this paper, we propose to leverage a small number of labeled target data to selectively transfer the knowledge from multiple source models. 2661 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2661–2674 August 1–6, 2021. ©2021 Association for Computational Linguistics In"
2021.acl-long.207,D19-1077,0,0.0368048,"Missing"
2021.acl-long.207,2020.repl4nlp-1.16,0,0.0169926,"o it, we focus on the cross-lingual scenario and our two views are a target task-specific model and the aggregation of multiple pre-trained source models. Contextual Multilingual Language Model Trained on massive unlabeled data of hundreds of monolingual corpus, the contextual multilingual models (Devlin et al., 2019; Conneau et al., 2020) learn common representations for multiple languages. Though cross-lingual transfer learning significantly benefits from these models (Pires et al., 2019; Wu and Dredze, 2019b), large gaps still remain between low and high-resources setups (Hu et al., 2020a; Wu and Dredze, 2020). 6 Conclusion We propose a novel multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Experimental results show that our approaches achieve state-of-the-art performances on all tasks. Moreover, even compared to approaches with extra resources like source language data, our substructure-level approach still shows significant improvements. Acknowledgement This work was supported by the National Natural Science Foundation of China (61976139) and by Alibaba Group through Alibaba Innovative Research Program. We thank Yuting Zh"
2021.acl-long.207,P95-1026,0,0.702103,"g. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also provide the maximal results of DT on language level (DT-Max(lang))"
2021.acl-long.207,N01-1026,0,0.292596,"language processing and has many applications, i.e., sequence labeling (DeRose, 1988; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020b), dependency parsing (Chen and Manning, 2014; Dozat and Manning, 2016; Ahmad et al., 2019) and semantic role labeling (van der Plas et al., 2011; Strubell et al., 2018; Cai and Lapata, 2020). ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. To achieve strong performance, structured prediction models mostly require manually labeled data that are costly to obtain in general. Cross-lingual transfer learning (Yarowsky and Ngai, 2001; Wang and Manning, 2014; Guo et al., 2018; Lin et al., 2019; Hu et al., 2021) recently attracted attention for tackling that problem, by transferring the knowledge from high-resource languages to low-resource ones. Existing works can be categorized into two types: single-source transfer and multi-source transfer. The former is limited to transferring knowledge from one source language and generally results in inferior performance than the latter (McDonald et al., 2011; Rahimi et al., 2019), especially when the target language is similar to multiple source language over various characteristics"
2021.acl-long.207,N18-1089,0,0.0259681,"hment score (UAS) and labeled attachment score (LAS) for dependency parsing. More details can be found in the Appendix B.1. 3.1 We compare the results of the target view of our language/sentence/sub-structure-level approaches which are denoted as Ours-lang/sent/sub respectively, with a large amount of previous state-of-theart cross-lingual baselines: direct fine-tuning (DTfinetuning), direct transfer (DT), hard knowledge distillation (hard-KD) (Liu et al., 2017), soft knowledge distillation (soft-KD) (Hinton et al., 2015; Wu et al., 2020), unified multilingual model (UMM) which is similar to (Yasunaga et al., 2018; Akbik et al., 2019), and bootstrapping approaches (Yarowsky, 1995; Zhou and Li, 2005; McClosky et al., 2006; Ruder and Plank, 2018) based on UMM. DT-finetuning We directly fine-tune the taskspecific view on fifty labeled data. DT In DT, there is only test data in the target language. Therefore, we evaluate this approach in three ways: 1) using the mean probability distribution of source models (DT-mean); 2) using the maximal probability distribution of source models over the sub-structure level (DT-max). 3) evaluating each source model and voting on the sub-structure level (DT-vote). We also"
2021.acl-long.220,2020.emnlp-main.129,0,0.301282,"fore Life Divorce Marry (65) (26229) Before Figure 1: Low-resource Event Detection w.r.t. Event Correlation in FewEvent (Deng et al., 2020) Dataset. Event Detection (ED) (Chen et al., 2015) is the task to extract structure information of events from unstructured texts. For example, in the event mention “Jack is married to the Iraqi microbiologist known as Dr. Germ.”, an ED model should identify the event type as ‘Marry’ where the word ‘married’ triggers the event. The extracted events with canonical structure facilitate various social applications, such as biomedical science (Li et al., 2019; Wang et al., 2020c), financial analysis (Deng et al., 2019; Liang et al., 2020), fake news detection (Wang et al., 2018; Nikiforos et al., 2020) and so on. As a non-trivial task, ED suffers from the lowresource issues. On the one hand, the maldistribu† (30) Demonstrate Introduction ∗ Riot SubSuper Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type. Most of current methods to ED rely heavily on training instances, and almost ignore the correlation of event types. Hence, they tend to suffer from data scarcity and fail to handle new unseen event types. T"
2021.acl-long.220,2020.louhi-1.10,0,0.186586,"fore Life Divorce Marry (65) (26229) Before Figure 1: Low-resource Event Detection w.r.t. Event Correlation in FewEvent (Deng et al., 2020) Dataset. Event Detection (ED) (Chen et al., 2015) is the task to extract structure information of events from unstructured texts. For example, in the event mention “Jack is married to the Iraqi microbiologist known as Dr. Germ.”, an ED model should identify the event type as ‘Marry’ where the word ‘married’ triggers the event. The extracted events with canonical structure facilitate various social applications, such as biomedical science (Li et al., 2019; Wang et al., 2020c), financial analysis (Deng et al., 2019; Liang et al., 2020), fake news detection (Wang et al., 2018; Nikiforos et al., 2020) and so on. As a non-trivial task, ED suffers from the lowresource issues. On the one hand, the maldistribu† (30) Demonstrate Introduction ∗ Riot SubSuper Event Detection (ED) aims to identify event trigger words from a given text and classify it into an event type. Most of current methods to ED rely heavily on training instances, and almost ignore the correlation of event types. Hence, they tend to suffer from data scarcity and fail to handle new unseen event types. T"
2021.acl-long.220,D19-1582,0,0.107848,"em and propose a novel ontology-based model, OntoED, that encodes intra and inter structures of events. • We provide a novel ED framework based on ontology embedding with event correlations, which interoperates symbolic rules with popular deep neural networks. • We build a new dataset OntoEvent for ED. Extensive experimental results demonstrate that our model can achieve better performance on the overall, few-shot, and zero-shot setting. 2 Related Work Traditional approaches to ED are mostly based on neural networks (Chen et al., 2015; Nguyen et al., 2016; Liu et al., 2018; Wang et al., 2019; Yan et al., 2019; Cui et al., 2020; Shen et al., 2020; Lou et al., 2021), and ignore correlation knowledge of event types, especially in low-resource scenarios. Most previous low-resource ED methods (Peng et al., 2016) have been based on supervised learning. However, supervised-based methods are too dependent on data, and fail to be applied to new types without additional annotation efforts. Another popular methods for low-resource ED are based on meta learning. Deng et al. (2020); Lai et al. (2020); Shen et al. (2021) reformulate ED as a few-shot learning problem to extend ED with limited labeled samples to"
2021.acl-long.220,2020.emnlp-main.430,0,0.0362614,"establish proper linkages between event types and instances. Specifically, each instance Xi in T is denoted as a token sequence Xi = {xji |j ∈ [1, L]} with maximum L tokens, where the event trigger xti are annotated. We expect to predict the index t (1 ≤ t ≤ L) and the event label ei for each instance respectively. Besides, we utilize a multi-faceted event-event relation set R = RH t RT t RC for event ontology population and learning. Thereinto, RH = {S UB S UPER, S UPER S UB, C O S UPER1 } denotes a set of relation labels defined in the subevent relation extraction task (Wang et al., 2020a; Yao et al., 2020). RT = {B EFORE, A FTER, E QUAL2 } denotes a set of temporal relations (Han et al., 2020). RC = {C AUSE, C AUSED B Y} denotes a set of causal relations (Ning et al., 2018). 3.2 Model Overview In this paper, we propose a general framework called OntoED with three modules: (1) Event Detection (Ontology Population), (2) Event Ontology Learning, and (3) Event Correlation Inference. Figure 2 shows the key idea of the three modules. 3 Event Correlation Inference New Event Correlations for Low-Resource Events 1 Event Detection (Ontology Population) Instances of Event Types and Relations The input of"
2021.acl-long.220,2020.acl-main.678,0,0.0611405,"Missing"
2021.acl-long.237,D19-1243,0,0.0558251,"Missing"
2021.acl-long.237,2020.acl-main.679,0,0.124454,"tion (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020). Table 1 lists several typical score functions. However, these scores can be easily influenced by word frequencies, sentence structures, and other 3037 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 3037–3049 August 1–6, 2021. ©2021 Association for Computational Linguistics factors, which can mislead the models and make existing methods oversensitive to lexical perturbations (Abdou et al., 2020; Tamborrino et al., 2020). Figure 1 shows two examples. The correct choices are paraphrased via synonym replacement or structure transformation. In these examples, the baseline (Pro-A) produces much lower scores for the paraphrased choices and chooses the wrong choices. Since existing methods can be easily distracted by irrelevant factors such as lexical perturbations, we argue that a commonsense question answering method should focus on the answers’ semantics and assign similar scores to synonymous choices. To this end, we introduce a novel SEmantic-based Question Answering model, SEQA, whic"
2021.acl-long.237,2020.emnlp-main.11,0,0.0152985,"ch has no restriction on the query types. Thus, Self-Talk can be applied to a wide range of domains. Despite the introduction of auxiliary information, these methods are essentially dependent on language model scores, so they are still sensitive to lexical perturbations. Besides directly using pre-trained LMs, some recent efforts have been dedicated to automatically constructing task-specific data to train commonsense reasoners in zero-shot settings. Wang et al. (2019) and Kocijan et al. (2019) provide some rules to construct labeled training data from large corpus for pronoun disambiguation. Banerjee and Baral (2020), Moghimifar et al. (2020) and Ma et al. (2020) collect training data based on knowledge bases, such as Atomic (Sap et al., 2019a). Though effective, they are limited by the specific task settings or highly dependent on the task-related knowledge bases, which makes them difficult to transfer to other commonsense reasoning tasks. 3 And it then takes the statement as a prompt to calculate the generative probability of each choice. Note that the templates for rewriting is not the focus of this paper, and hence we directly use the templates of previous work (Shwartz et al., 2020; Tamborrino et al."
2021.acl-long.237,2020.coling-main.467,0,0.0294948,"e query types. Thus, Self-Talk can be applied to a wide range of domains. Despite the introduction of auxiliary information, these methods are essentially dependent on language model scores, so they are still sensitive to lexical perturbations. Besides directly using pre-trained LMs, some recent efforts have been dedicated to automatically constructing task-specific data to train commonsense reasoners in zero-shot settings. Wang et al. (2019) and Kocijan et al. (2019) provide some rules to construct labeled training data from large corpus for pronoun disambiguation. Banerjee and Baral (2020), Moghimifar et al. (2020) and Ma et al. (2020) collect training data based on knowledge bases, such as Atomic (Sap et al., 2019a). Though effective, they are limited by the specific task settings or highly dependent on the task-related knowledge bases, which makes them difficult to transfer to other commonsense reasoning tasks. 3 And it then takes the statement as a prompt to calculate the generative probability of each choice. Note that the templates for rewriting is not the focus of this paper, and hence we directly use the templates of previous work (Shwartz et al., 2020; Tamborrino et al., 2020) for our method and"
2021.acl-long.237,N16-1098,0,0.0192474,"Evaluation results, including the original selection accuracy before attack, the accuracy after attack, the attack success rate, the percentage of perturbed words with respect to the original sentence length in successful attacks, and the semantic similarity between the original and paraphrased choices. GPT-2, RoBERTa and SRoBERTa refer to GPT-2-xlarge, RoBERTa-large (Liu et al., 2019) and SentenceRoBERTa-large, respectively. 4 4.1 Experiments 4.3 Datasets We conducted experiments on four multiplechoice commonsense question answering tasks, COPA (Roemmele et al., 2011), StoryClozeTest (SCT) (Mostafazadeh et al., 2016), SocialIQA (Sap et al., 2019b) and CosmosQA (Huang et al., 2019). For each instance, only one choice is correct. See Appendix for more description about datasets. For COPA, we reported the results on its test set. As the test sets of another three datasets are hidden, for convenience of analysis, we reported the experiment results on their development sets. 4.2 Baselines We employed five strong baselines. Table 1 shows three of them, Pro-A, Pro-Q and MI-QA. There is no explicit auxiliary information used in these three methods, while another two baselines rely on explicit information suppleme"
2021.acl-long.237,2020.findings-emnlp.369,0,0.020767,"fore, it is vital to study unsupervised commonsense question answering without relying on any labeled downstream task data. In this paper, we investigate multiple-choice commonsense question answering tasks in an unsupervised setting: given a question and a set of answer choices, a model is required to predict the most reasonable answer choice for the question, but without access to any labeled task data. Introduction Pre-trained language models have been widely used for commonsense question answering. Finetuning pre-trained models on task-specific data produces many state-of-the-art results (Wang et al., 2020; * Equal † contribution Corresponding author: Minlie Huang. Many existing unsupervised methods tackle these tasks by scoring each answer choice using a language model, e.g., estimating the generative probability of the answer choice conditioned on the question (Trinh and Le, 2018; Shwartz et al., 2020; Bosselut and Choi, 2019; Tamborrino et al., 2020). Table 1 lists several typical score functions. However, these scores can be easily influenced by word frequencies, sentence structures, and other 3037 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and t"
2021.acl-long.237,D14-1162,0,0.0871353,"Missing"
2021.acl-long.237,N19-1094,0,0.021734,"generalize to different domains. Self-Talk (Shwartz et al., 2020) breaks the limit by extracting knowledge from GPT-2 (Radford et al., 2019), which has no restriction on the query types. Thus, Self-Talk can be applied to a wide range of domains. Despite the introduction of auxiliary information, these methods are essentially dependent on language model scores, so they are still sensitive to lexical perturbations. Besides directly using pre-trained LMs, some recent efforts have been dedicated to automatically constructing task-specific data to train commonsense reasoners in zero-shot settings. Wang et al. (2019) and Kocijan et al. (2019) provide some rules to construct labeled training data from large corpus for pronoun disambiguation. Banerjee and Baral (2020), Moghimifar et al. (2020) and Ma et al. (2020) collect training data based on knowledge bases, such as Atomic (Sap et al., 2019a). Though effective, they are limited by the specific task settings or highly dependent on the task-related knowledge bases, which makes them difficult to transfer to other commonsense reasoning tasks. 3 And it then takes the statement as a prompt to calculate the generative probability of each choice. Note that the t"
2021.acl-long.237,D19-1250,0,0.0572008,"Missing"
2021.acl-long.237,P19-1487,0,0.013742,"2018; Tamborrino et al., 2020) , which is denoted as Probability-Q (Pro-Q) in Table 1. Some recent work claims that external knowledge can benefit commonsense reasoning. Besides static knowledge bases (KBs), such as ConceptNet (Speer et al., 2017) and Atomic (Sap et al., 2019a), there are also numerous studies treating LMs as dynamic KBs. Petroni et al. (2019) shows that LMs can be used for KB completion. And Davison et al. (2019) shows that BERT can distinguish true and fake ConceptNet triplets. Further, the extracted knowledge can work as complementary information for answering a question. Rajani et al. (2019) proposes a model for Com3038 1 PBERT (Q|A) , Q|Q| i PBERT (Qi |Q/i , A). monSenseQA (Talmor et al., 2019) that generates explanations for questions, which are then used as additional inputs. The shortcoming of this approach is that it requires collecting human explanations for each new dataset to fine-tune LMs. Some following researches explore unsupervised explanation/knowledge generator. CGA (Bosselut and Choi, 2019) employs COMET (Bosselut et al., 2019) to generate intermediate inferences which are then used to score the choice. However, COMET is limited by a small set of question types so"
2021.acl-long.237,D19-1410,0,0.0762639,"probability of observing the choice’s semantics. A choice’s semantic score can be obtained by summing the generative probabilities of sentences that have the same semantic meanings with the choice, where the sentences are called the choice’s supporters. However, it is hard to obtain the supporters which have exactly the same semantic meanings with the choice, so we reformulate the semantic score into a soft version as explained in Section 3.2. Each supporter is weighed by the semantic similarity to the answer choice, which can be computed with some off-the-shelf models, such as SentenceBERT (Reimers and Gurevych, 2019). Since the supporters and their weights depend on the semantics rather than the surface form of the answer choice, by this means, the effects of the distracting factors can be largely suppressed. Moreover, synonymous choices are likely to share the same set of supporters, so their scores are expected to be stably close. Our contributions in this paper are summarized as follows: • We propose a semantic-based question answering model (SEQA) for robust commonsense question answering in an unsupervised setting. Instead of directly scoring the answer choices, our method first generates some plausi"
2021.acl-long.237,D19-1454,0,0.0543383,"Missing"
2021.acl-long.237,2020.emnlp-main.373,0,0.0376012,"Missing"
2021.acl-long.237,N19-1421,0,0.0193007,"laims that external knowledge can benefit commonsense reasoning. Besides static knowledge bases (KBs), such as ConceptNet (Speer et al., 2017) and Atomic (Sap et al., 2019a), there are also numerous studies treating LMs as dynamic KBs. Petroni et al. (2019) shows that LMs can be used for KB completion. And Davison et al. (2019) shows that BERT can distinguish true and fake ConceptNet triplets. Further, the extracted knowledge can work as complementary information for answering a question. Rajani et al. (2019) proposes a model for Com3038 1 PBERT (Q|A) , Q|Q| i PBERT (Qi |Q/i , A). monSenseQA (Talmor et al., 2019) that generates explanations for questions, which are then used as additional inputs. The shortcoming of this approach is that it requires collecting human explanations for each new dataset to fine-tune LMs. Some following researches explore unsupervised explanation/knowledge generator. CGA (Bosselut and Choi, 2019) employs COMET (Bosselut et al., 2019) to generate intermediate inferences which are then used to score the choice. However, COMET is limited by a small set of question types so that CGA is difficult to generalize to different domains. Self-Talk (Shwartz et al., 2020) breaks the lim"
2021.acl-long.237,2020.acl-main.357,0,0.0465758,"Missing"
2021.acl-long.308,P17-1178,0,0.0223577,"ision training using 64 Nvidia Telsa V100 32GB GPUs. Appendix A shows additional details. 5 http://opus.nlpl.eu/ 5.1 Experiments on Cross-lingual Understanding Tasks Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details about the datasets. Fine-tuning Setting Following previous works (Conneau et al., 201"
2021.acl-long.308,W18-6319,0,0.012074,"ize Baseline 42.9 Liu et al. (2020a) 43.8 WMT14 En-De BLEU SacreBLEU 30 29 40.4 41.8 28.7 30.1 27.8 29.5 Randomly Initialize + More Bilingual Data* Baseline* 30.6 29.5 Cross-lingual Model Initialize mBART 43.2 mRASP 44.3 XLM-R 43.8 VECO 44.5 29.1 29.9 30.6 41.0 41.7 41.2 42.0 30.0 30.3 30.9 31.7 sacreBLEU Model 28 27 26 VECO Init. XLM-R Init. Random Init. 25 10 15 20 25 Epochs 30 35 Table 3: (left) Results on machine translation. (right) Learning curves of different initialization methods. tokenized SacreBLEU 7 to avoid the influence of different tokenization and normalization between models (Post, 2018). Fine-tuning Setting We fine-tune our model using fairseq 8 toolkit and adopt comparable training settings with baselines. We run WMT 14 EnDe and En-Fr MT experiments on 16 and 32 V100 GPUs, respectively. The batch size is 64k for EnDe and 256k for En-Fr. The total training updates are set to 100k. The learning rate is 1e-4/2e-4, with linear warm-up over the first 16k steps and linear decay. We average the last 10 checkpoints and use beam search with a beam size of 5. Baselines We consider two types of Transformer baselines: randomly initialized and cross-lingual models initialized. For rando"
2021.acl-long.308,D19-1071,0,0.0281093,"lable in the downstream task. Specifically, we concatenated the two repreL sentations [HL x : Sx ] to predict the label of x, L L [Hy : Sy ] to predict the label of y. 4 . 3.2 For pre-trained encoders like XLM, it is not a trivial problem to incorporate them into the sequenceto-sequence architecture – the mainstream backbone model of generation tasks (Zhu et al., 2020). One of the drawbacks or challenges could be that the encoder-to-decoder attention is not pre-trained. Therefore, the parameters of the decoder need to be re-adjusted along with the encoder in the following fine-tuning process (Ren et al., 2019). However, under the framework of V E C O , the cross-attention is jointly pre-trained along with the whole network, making it easy to provide full initialization for sequence-to-sequence models. Specifically, the self-attention module is used to initialize both the corresponding modules in the encoder and decoder for contextual modeling, while the cross-attention module is used to initialize the encoder-to-decoder attention. It’s okay whether you continue to tie the self-attention parameters during fine-tuning. Directly pre-training a sequenceto-sequence model like mBART (Liu et al., 2020b) c"
2021.acl-long.308,D19-1382,0,0.0282293,"rgence. Then, we jointly train the whole model. We pre-train our model with mixed-precision training using 64 Nvidia Telsa V100 32GB GPUs. Appendix A shows additional details. 5 http://opus.nlpl.eu/ 5.1 Experiments on Cross-lingual Understanding Tasks Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details"
2021.acl-long.308,W17-2512,0,0.015424,"s Experimental Setup Downstream Tasks We conduct cross-lingual NLU evaluations on XTREME (Hu et al., 2020), a representative massively multilingual benchmark that consists of 9 understanding tasks over 40 languages. XTREME tasks can be classified into four different categories: (1) sentence-pair classification: XNLI (Conneau et al., 2018), PAWS-X (Yang et al., 2019); (2) structured prediction: POS (Nivre et al., 2018), Wikiann NER (Pan et al., 2017); (3) question answering: XQuAD (Artetxe et al., 2020), MLQA (Lewis et al., 2020), TyDiQA (Clark et al., 2020); (4) sentence retrieval: BUCC 2018 (Zweigenbaum et al., 2017), Tatoeba (Artetxe and Schwenk, 2019). Tasks in the first three categories are provided: 1) golden training corpus in English, 2) translated training corpus in other languages, and 3) dev/test set in all languages. For sentence retrieval tasks, no training datasets are provided. We refer the reader to Hu et al. (2020) for additional details about the datasets. Fine-tuning Setting Following previous works (Conneau et al., 2019; Hu et al., 2020), we consider two typical fine-tuning settings: (1) Cross-lingual Transfer which fine-tunes the pre-trained model using English golden data only and dire"
2021.acl-long.380,N10-1083,0,0.12678,"Missing"
2021.acl-long.380,P19-1299,0,0.0180284,"asible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the"
2021.acl-long.380,2020.acl-main.747,0,0.0968285,"Missing"
2021.acl-long.380,J88-1003,0,0.0958873,"dels and the true labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions fro"
2021.acl-long.380,N19-1423,0,0.125683,"gorithm, which alternates between updating a posterior distribution and optimizing model parameters. To empirically evaluate our proposed approaches, we extensively conduct experiments on four sequence labeling tasks of twenty-one datasets. Our two proposed approaches, especially the latent variable model, outperform several strong baselines. 2 Background 2.1 Sequence Labeling Given a sentence x = x1 , . . . , xn , its word representations are extracted from the pre-trained embeddings and passed into a sentence encoder such as BiLSTM, Convolutional Neural Networks (CNN) and multilingual BERT (Devlin et al., 2019) to obtain a sequence of contextual features. Without considering the dependencies between predicted labels, the Softmax layer computes the conditional probability as follows, Pθ (y|x) = n Y Pθ (yi |x) i=1 Given the gold sequence y∗ = y1∗ , . . . , yn∗ , the general training objective is to minimize the negative log-likelihood of the sequence, J (θθ ) = − log Pθ (y∗ |x) = − n X J (θθ ) = − log Pθ (ˆ y|x) = − Cross-Lingual/Domain Transfer Supervised models fail when labeled data are absent. Learning from imperfect predictions from rich-resource sources is a viable approach to tackle the problem"
2021.acl-long.380,P19-1266,0,0.0448562,"Missing"
2021.acl-long.380,D18-1498,0,0.0522747,"Missing"
2021.acl-long.380,N06-2015,0,0.188956,"tagging task, we use Universal Dependencies treebanks (UD) v2.44 and randomly select five anguages together with the English dataset. The whole datasets are English (En), Catalan (Ca), Indonesian (Id), Hindi (Hi), Finnish (Fi), and Russian (Ru). For the Aspect Extraction task, we select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016). For the NER task, we evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Cross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new model. Inference For inference, we use Q(y) to obtain ypred 4 lowing Wu et al. (2020), the source model are previously trained on its"
2021.acl-long.380,2020.findings-emnlp.236,1,0.851087,"glish portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new model. Inference For inference, we use Q(y) to obtain ypred 4 lowing Wu et al. (2020), the source model are previously trained on its corresponding training data. We use the BIO scheme for CoNLL and OntoNotes NER tasks and Aspect Extraction. We run each model three times and report the average accuracy for the POS tagging task and F1-score for the other tasks. 2, Multi-source Setup The following approaches are applicable for multi-source setup, Pφ (ˆ y(u) |y, u) u=1 Experiments We use the multilingual BERT (mBERT) as our word representations3 as the sentence encoder. Fol2 Another choice is to use Pθ (y|x), however, we found that utilizing Q(y) generally achieves better perform"
2021.acl-long.380,2021.acl-long.207,1,0.894793,"2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source lan"
2021.acl-long.380,N19-1383,0,0.0949712,", 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled"
2021.acl-long.380,D17-1302,0,0.0180989,"s widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings. 7 Conclus"
2021.acl-long.380,N16-1030,0,0.0408353,"latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang"
2021.acl-long.380,2020.acl-main.193,0,0.46218,"guages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels. We then make the matrix trainable, which leads to further expressiveness and connects minimum risk training to learning latent 4909 Proceedings of the 59th Annual Meeting of the Associ"
2021.acl-long.380,P16-1101,0,0.0435505,"l learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et"
2021.acl-long.380,P17-1135,0,0.0379881,"Missing"
2021.acl-long.380,P19-1493,0,0.0143402,"Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the strong cross-lingual ability of the contextual multilingual embeddings. 7 Conclusion In this paper, we propose two approaches to the zero-shot sequence labeling problem. Our MRT approach uses a fixed matrix to model the relations between the predicted labels from the source models and the true labels. Our LVM approach uses trainable matrices to model these label relations. We extensively verify the effectiveness of our approaches on both single-source and multisource transfer over both cross-lingual and crossdomain sequence labeling problems. Experiments show that MRT and LVM general"
2021.acl-long.380,P19-1015,0,0.109868,"ed data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models the relations between the noisy predictions from the source models and the true labels. We then make the matrix trainable, which leads to further expressiveness and connects minimum risk training to learning latent 4909 Proceedings of the 59th Annual Me"
2021.acl-long.380,W09-1119,0,0.0197473,"connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsk"
2021.acl-long.380,D11-1141,0,0.00798819,"mum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sources (such as crosslingual, cross-domain transfer) (Yarowsky and Ngai, 2001; Guo"
2021.acl-long.380,P18-1096,0,0.0210148,"pose a multi-view framework to selectively transfer knowledge from multiple sources by utilizing a small amount of labeled dataset. Crossdomain adaption is widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP task"
2021.acl-long.380,P16-1159,0,0.0343896,"hard predictions and the target model’s soft predictions, Methodology Minimum Risk Training In supervised learning, minimum risk training aims to minimize the expected error (risk) concerning the conditional probability, X J (θθ ) = Pθ (y|x)R(y∗ , y) y∈Y(x) where R(y∗ , y) is the risk function that measures the distance between the gold sequence y∗ and the candidate sequence y, and Y(x) denotes the collection of all the possible label sequences given the sentence x. The risk function can be defined in many ways depending on specific applications, such as the BLEU score in machine translation (Shen et al., 2016). However, in our setting, there are no gold labels to compute R(y∗ , y). Instead, we assume there are multiple pretrained source models which can be used to predict hard labels, and we define the risk function as R(ˆ y, y) to measure the difference between pseudo label sequence 4910 ˆ predicted by source models and the candidate y sequence y. The objective function becomes, x y∈Y(x) Conventional minimum risk training is intractable which is mainly due to the combination of two reasons: first, the set of candidate label sequences Y(x) is exponential in size and intractable to enumerate; second"
2021.acl-long.380,N03-1031,0,0.424391,"Missing"
2021.acl-long.380,N12-1052,0,0.0777677,"Missing"
2021.acl-long.380,W02-2024,0,0.400509,"hree tasks to conduct the cross-lingual sequence labeling task, which are POS tagging, NER, and Aspect Extraction. For the POS tagging task, we use Universal Dependencies treebanks (UD) v2.44 and randomly select five anguages together with the English dataset. The whole datasets are English (En), Catalan (Ca), Indonesian (Id), Hindi (Hi), Finnish (Fi), and Russian (Ru). For the Aspect Extraction task, we select the restaurant domain over subtask 1 in the SemEval-2016 shared task (Pontiki et al., 2016). For the NER task, we evaluate our models on the CoNLL 2002 and 2003 shared tasks (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003). Cross-Domain Sequence Labeling We use English portion of the OntoNotes (v5) (Hovy et al., 2006), which contains six domains: broadcast conversation (bc), broadcast news (bn), magazine (mz), newswire (nw), and web (wb). More details can be found in the Appendix A.1. 4.2 Approaches Single-source Setup The following approaches are applicable for single-source setup, • DT: we use the pre-trained source model to directly predict the pseudo labels on the target unlabeled data. • Hard: we use the pseudo labels from DT on the target unlabeled data to train a new"
2021.acl-long.380,N03-1033,0,0.314546,"rue labels. By making the risk function trainable, we draw a connection between minimum risk training and latent variable model learning. We propose a unified learning algorithm based on the expectation maximization (EM) algorithm. We extensively evaluate our proposed approaches on cross-lingual/domain sequence labeling tasks over twenty-one datasets. The results show that our approaches outperform state-of-the-art baseline systems. 1 Introduction Sequence labeling is an important task in natural language processing. It has many applications such as Part-of-Speech Tagging (POS) (DeRose, 1988; Toutanova et al., 2003) and Named Entity Recognition (NER) (Ratinov and Roth, 2009; Ritter et al., 2011; Lample et al., 2016; Ma and Hovy, 2016; Hu et al., 2020). Approaches to sequence labeling are mostly based on supervised learning, which relies heavily on labeled data. However, the labeled data is generally expensive and hard to obtain (for lowresource languages/domains), which means that these supervised learning approaches fail in many cases. ∗ Corresponding authors. ‡ Work was done when Zechuan Hu was interning at Alibaba DAMO Academy. Learning knowledge from imperfect predictions from other rich-resource sou"
2021.acl-long.380,Q14-1005,0,0.0759383,"nd Ngai, 2001; Guo et al., 2018; Huang et al., 2019; Hu et al., 2021) is a feasible and efficient way to tackle the low-resource problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the im"
2021.acl-long.380,2020.emnlp-main.639,0,0.0335027,"from multiple sources by utilizing a small amount of labeled dataset. Crossdomain adaption is widely studied (Steedman et al., 6 The CoNLL NER datasets have 11 labels (9 entity labels, a padding label and an ending label). 4916 F1 De Nl Es 74.1 80.6 76.1 74.0 80.5 75.9 73.9 80.4 75.8 73.7 80.2 75.6 73.6 2 3 4 10 80.1 2 3 4 τ 10 τ 75.4 2 3 4 10 τ Figure 3: The performance of MRT approach in single-source setup with soft predictions on three NER datasets by varying different τ . 2003). Existing works include bootstrapping approaches (Ruder and Plank, 2018), mixture-ofexperts (Guo et al., 2018; Wright and Augenstein, 2020), and consensus network (Lan et al., 2020). Other previous work (Kim et al., 2017; Guo et al., 2018; Huang et al., 2019) utilized labeled data in the source domain to learn desired information. However, our proposed approaches do not require any source labeled data or parallel texts. Contextual Multilingual Embeddings Embeddings like mBERT (Devlin et al., 2019), XLM (CONNEAU and Lample, 2019) and XLM-R (Conneau et al., 2020) which are trained on many languages, make great progress on cross-lingual learning for multiple NLP tasks. Recent works (Wu and Dredze, 2019; Pires et al., 2019) show the"
2021.acl-long.380,2020.acl-main.581,0,0.0664042,"t languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced by one or more source models on target unlabeled data , and propose two novel approaches. We start by introducing a novel approach based on the minimum risk training framework. We design a new decomposable risk function parameterized by a fixed matrix that models th"
2021.acl-long.380,D19-1077,0,0.0498863,"trained on its corresponding training data. We use the BIO scheme for CoNLL and OntoNotes NER tasks and Aspect Extraction. We run each model three times and report the average accuracy for the POS tagging task and F1-score for the other tasks. 2, Multi-source Setup The following approaches are applicable for multi-source setup, Pφ (ˆ y(u) |y, u) u=1 Experiments We use the multilingual BERT (mBERT) as our word representations3 as the sentence encoder. Fol2 Another choice is to use Pθ (y|x), however, we found that utilizing Q(y) generally achieves better performance. 3 Following previous work (Wu and Dredze, 2019; Wu et al., 2020), we fine-tune mBERT’s parameters. 4913 • Hard-Cat: we apply DT with all the source languages/domains, mix the resulting pseudo labels from all the sources on the unlabeled target data, and train a new model. • Hard-Vote: we do majority voting at the token level on the pseudo labels from DT with each source and train a new model. 4 https://universaldependencies.org/ C O NLL NER English German Dutch Spanish Avg. A SPECT E XTRACTION English Spanish Dutch Russian Turkish Avg. S INGLE - SOURCE : The following approaches have access to hard predictions: — 72.17 79.54 75.13 75.61 D"
2021.acl-long.380,N15-1069,0,0.0277639,"rce problem. It transfers knowledge from rich-resource languages/domains to low-resource ones. One typical approach to this problem is utilizing existing systems to provide predicted results for the zero-shot datasets. However, due to the gap between the source and the target languages/domains, this approach may fail to recover the true labels. Several previous approaches try to alleviate this problem by relying heavily on cross-lingual information (e.g., parallel text (Wang and Manning, 2014; Ni et al., 2017)), labeled data in source languages (Chen et al., 2019), and prior domain knowledge (Yang and Eisenstein, 2015) for different kinds of zero-shot scenarios. However, these approaches are designed to be specific, and might not be generalizable to other kinds of settings where the required resources are expensive to obtain or not available due to data privacy (Wu et al., 2020). Instead, we want a learning framework that can address the zero-shot learning problem in a unified perspective. In this work, we consider two widely explored settings in which we have access to: 1) the imperfect hard predictions (Rahimi et al., 2019; Lan et al., 2020); 2) the imperfect soft predictions (Wu et al., 2020), produced b"
2021.acl-long.42,2020.emnlp-main.700,1,0.875727,"text. We incorporate the tasks of object detection and image captioning into pretraining with a unified Transformer encoderdecoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established visionlanguage downstream tasks to demonstrate the effectiveness of this novel VLP paradigm. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020). Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns ∗ corresponding author general cross-modal representations from massive image-text pairs, and fine-tunes vision-language pre-training (VLP) models on task-specific data achieving state-of-the-art results on various downstream V+L tasks. Most existing mainstream VLP models adopt a two-step training method, which firstly extracts semantic visual features using a pr"
2021.acl-long.42,2021.ccl-1.108,0,0.0728569,"Missing"
2021.acl-long.42,2020.acl-main.703,0,0.0990917,"Missing"
2021.acl-long.42,D19-1514,0,0.141889,"ng with a unified Transformer encoderdecoder architecture for enhancing visual learning. An extensive set of experiments have been conducted on well-established visionlanguage downstream tasks to demonstrate the effectiveness of this novel VLP paradigm. 1 Introduction Self-supervised pre-training has achieved great success in a wide range of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020). Recent studies (Li et al., 2019; Lu et al., 2019; Chen et al., 2019; Tan and Bansal, 2019; Li et al., 2020b; Yu et al., 2020) have also witnessed the progress of self-supervised pretraining on vision-and-language tasks, which learns ∗ corresponding author general cross-modal representations from massive image-text pairs, and fine-tunes vision-language pre-training (VLP) models on task-specific data achieving state-of-the-art results on various downstream V+L tasks. Most existing mainstream VLP models adopt a two-step training method, which firstly extracts semantic visual features using a pre-trained object detection model, and then combines the derived object-centric representati"
2021.acl-long.42,Q14-1006,0,0.405181,"effective for vision-language pre-training. 2 Related Work Self-supervised pre-training has substantially advanced the performance across a variety of natural language understanding (Devlin et al., 2018; Liu et al., 2019; Wang et al., 2019; Lan et al., 2019) and text generation tasks (Song et al., 2019; Lewis et al., 2019; Bi et al., 2020). Inspired by language model pre-training, several researchers propose Visionlanguage pre-training(VLP) models on large-scale image-text pairs, which has proved effective for a wide range of vision-language (VL) tasks, such as VQA (Antol et al., 2015), NLVR (Young et al., 2014), Cross-modal Retrieval (Suhr et al., 2018). The current VLP models mainly take two-step training pipeline, which consists of extracting semantic visual features by object detector and training the cross-modal pre-training model to align text and visual features. In this kind of method, there are mainly two broad directions to conduct visionlanguage pre-training. The first line uses a singlestream transformer architecture (Vaswani et al., 2017) to model both image and text representations in a unified semantic space such as VLBERT (Su et al., 2019), UNITER (Chen et al., 2019) and OSCAR (Li et"
2021.acl-long.46,C18-1139,0,0.441402,"ank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al., 2020c). For Case 3, we use MBERT embeddings for the teacher. Also for Case 3, we fine-tune the teacher model on the training set of the four Indo-European languages from the WikiAnn dataset and train student models on the four additional languages. For the teacher models in Case 1b and 2b, we simply use the same embeddings as the student because there is already huge performance gap between the teacher and student in these settings and hen"
2021.acl-long.46,2020.iwpt-1.2,0,0.0868451,"Missing"
2021.acl-long.46,Q17-1010,0,0.0229249,"or English and 5000 sentences for each of the other languages. We split the datasets by 3:1:1 for training/development/test. For Case 1b and 2b, we use Penn Treebank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Zadrozny, 2014) as the word representations for stronger monolingual teacher models (Wang et al., 2020c). For Case 3, we use MBERT embeddings for the teacher. Also for Case 3, we fine-tune the teacher model on the training set of the four Indo-European languages from the WikiAnn dataset and train student models on the four additional languages. For the teacher models in Case 1b and"
2021.acl-long.46,P19-1595,0,0.0149707,"of a large teacher model. The typical KD objective function is the cross-entropy between the output distributions predicted by the teacher model and the student model: X LKD = − Pt (y|x) log Ps (y|x) (2) y∈Y(x) where Pt and Ps are the teacher’s and the student’s distributions respectively. During training, the student jointly learns from the gold targets and the distributions predicted by the teacher by optimizing the following objective function: Lstudent = λLKD + (1 − λ)Ltarget where λ is an interpolation coefficient between the target loss Ltarget and the structural KD loss LKD . Following Clark et al. (2019); Wang et al. (2020a), one may apply teacher annealing in training by decreasing λ linearly from 1 to 0. Because KD does not require gold labels, unlabeled data can also be used in the KD loss. 551 3 Structural Knowledge Distillation When performing knowledge distillation on structured prediction, a major challenge is that the structured output space is exponential in size, leading to intractable computation of the KD objective in Eq. 2. However, if the scoring function of the student model can be factorized into scores of substructures (Eq. 1), then we can derive the following factorized form"
2021.acl-long.46,2020.acl-main.747,0,0.135739,"Missing"
2021.acl-long.46,N19-1423,0,0.273421,"This work was conducted when Xinyu Wang was interning at Alibaba DAMO Academy. 1 Our code is publicly available at https://github. com/Alibaba-NLP/StructuralKD. ♠ as online serving. An interesting and viable solution to this problem is knowledge distillation (KD) (Buciluˇa et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015), which can be used to transfer the knowledge of a large model (the teacher) to a smaller model (the student). In the field of natural language processing (NLP), for example, KD has been successfully applied to compress massive pretrained language models such as BERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) into much smaller and faster models without significant loss in accuracy (Tang et al., 2019; Sanh et al., 2019; Tsai et al., 2019; Mukherjee and Hassan Awadallah, 2020). A typical approach to KD is letting the student mimic the teacher model’s output probability distributions on the training data by using the cross-entropy objective. For structured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label se"
2021.acl-long.46,K17-3002,0,0.015241,"l for named entity recognition (NER) that is based on large pretrained contextualized embeddings to a smaller CRF model with static embeddings that is more suitable for fast online serving. For a CRF student model described in section 2.1, if we absorb the emission score Se (yi , x) into the transition score St ((yi−1 , yi ), x) at each position i, then the substructure space Us (x) contains every two adjacent labels {(yi−1 , yi )} for i=1, . . . , n, with n beCase 1b: Graph-based Dependency Parsing ⇒ Dependency Parsing as Sequence Labeling In this case, we use the biaffine parser proposed by Dozat et al. (2017) as the teacher and the sequence labeling approach proposed by Strzyz et al. (2019) as the student for the dependency parsing task. The biaffine parser is one of the state-of-the-art models, while the sequence labeling parser provides a good speed-accuracy tradeoff. There is a big gap in accuracy between the two models and therefore KD can be used to improve the accuracy of the sequence labeling parser. Here we follow the head-selection formulation of dependency parsing without the tree constraint. The dependency parse tree y is represented by hy1 , . . . , yn i, where n is the sentence length"
2021.acl-long.46,P19-1266,0,0.0536502,"Missing"
2021.acl-long.46,D16-1139,0,0.172232,"py objective. For structured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics margi"
2021.acl-long.46,D16-1180,0,0.12717,"ructured prediction problems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over"
2021.acl-long.46,P19-1233,0,0.0774946,"ictions from marginal distributions of the CRF teacher still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve sign"
2021.acl-long.46,P16-1101,0,0.0363014,"ons between adjacent labels. While predictions from marginal distributions of the CRF teacher still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that"
2021.acl-long.46,P18-1130,0,0.017162,"s-lingual transfer in Case 3, we use the four Indo-European languages as the source for the teacher model and additionally select four languages from different language families as the target for the student models.2 We use the standard training/development/test split for the CoNLL datasets. For WikiAnn, we follow the sampling of Wang et al. (2020a) with 12000 sentences for English and 5000 sentences for each of the other languages. We split the datasets by 3:1:1 for training/development/test. For Case 1b and 2b, we use Penn Treebank (PTB) 3.0 and follow the same pre-processing pipeline as in Ma et al. (2018). For unlabeled data, we sample sentences that belong to the same languages of the labeled data from the WikiAnn datasets for Case 1a, 2a and 4 and we sample sentences from the target languages of WikiAnn datasets for Case 3. We use the BLLIP corpus3 as the unlabeled data for Case 1b and 2b. Models For the student models in all the cases, we use fastText (Bojanowski et al., 2017) word embeddings and character embeddings as the word representation. For Case 1a, 2a and 4, we concatenate the multilingual BERT, Flair (Akbik et al., 2018), fastText embeddings and character embeddings (Santos and Za"
2021.acl-long.46,2020.acl-main.202,0,0.036762,"Missing"
2021.acl-long.46,P17-1178,0,0.0170604,"e i-th word as its head and with its length larger than 1. It is intractable to compute such marginal probabilities by enumerating all the output structures, but we can tractably compute them using dynamic programming. See supplementary material for a detailed description of our dynamic programming method. 4 Experiments We evaluate our approaches described in Section 3 on NER (Case 1a, 2a, 3, 4) and dependency parsing (Case 1b, 2b). 4.1 Settings Datasets We use CoNLL 2002/2003 datasets (Tjong Kim Sang, 2002; Tjong Kim Sang and De Meulder, 2003) for Case 1a, 2a and 4, and use WikiAnn datasets (Pan et al., 2017) for Case 1a, 2a, 3, and 4. The CoNLL datasets contain the corpora of four Indo-European languages. We use the same four languages from the WikiAnn datasets. For cross-lingual transfer in Case 3, we use the four Indo-European languages as the source for the teacher model and additionally select four languages from different language families as the target for the student models.2 We use the standard training/development/test split for the CoNLL datasets. For WikiAnn, we follow the sampling of Wang et al. (2020a) with 12000 sentences for English and 5000 sentences for each of the other language"
2021.acl-long.46,P19-1493,0,0.0162243,"re consecutive labels {(yi−1 , yi )}. In contrast, a MaxEnt model predicts the label probability distribution Ps (yi |x) of each token independently and hence the substructure space Us (x) consists of every individual label {yi }. To calculate the substructure marginal of the teacher Pt (yi |x), we can again utilize the forwardbackward algorithm: Teacher Factorization Produces More Fine-grained Substructures than Student Factorization Case 3: MaxEnt ⇒ Linear-Chain CRF Here we consider KD in the opposite direction of Case 2a. An example application is zero-shot crosslingual NER. Previous work (Pires et al., 2019; Wu and Dredze, 2019) has shown that multilingual BERT (M-BERT) has strong zero-shot crosslingual transferability in NER tasks. Many such models employ a MaxEnt decoder. In scenarios requiring fast speed and low computation cost, however, we may want to distill knowledge from such models to a model with much cheaper static monolingual embeddings while compensating the performance loss with a linear-chain CRF decoder. As described in Case 1a, the substructures of a linear-chain CRF model are consecutive labels {(yi−1 , yi )}. Because of the label independence and local normalization in the Max"
2021.acl-long.46,N19-1335,0,0.0202894,"le head-selection first-order approach (Dozat and Manning, 2017). Such speed-accuracy tradeoff as seen in sequence labeling and dependency parsing also occurs in many other structured prediction tasks. This makes KD an interesting and very useful technique that can be used to circumvent this tradeoff to some extent. 6.2 Knowledge Distillation in Structured Prediction KD has been applied in many structured prediction tasks in the fields of NLP, speech recognition and computer vision, with applications such as neural machine translation (Kim and Rush, 2016; Tan et al., 2019), sequence labeling (Tu and Gimpel, 2019; Wang et al., 2020a), connectionist temporal classification (Huang et al., 2018), image semantic segmentation (Liu et al., 2019a) and so on. In KD for structured prediction tasks, how to handle the exponential number of structured outputs is a main challenge. To address this difficult problem, recent work resorts to approximation of the KD objective. Kim and Rush (2016) proposed sequence-level distillation through predicting K-best sequences of the teacher in neural machine translation. Kuncoro et al. (2016) proposed to use multiple greedy parsers as teachers and generate the probability dist"
2021.acl-long.46,P19-1454,1,0.926723,"to the following form without the need for calculating the student partition function Zs (x). LKD = − X Pt (u|x) × logPs (u|x) (6) u∈Us (x) In all the cases except Case 1a and Case 3, the student model is locally normalized and hence we can follow this form of objective. 3.2 parse tree independently. A second-order dependency parser scores pairs of dependency arcs with a shared token. The substructures of second-order parsing are therefore all the dependency arc pairs with a shared token. It has been found that secondorder extensions of the biaffine parser often have higher parsing accuracy (Wang et al., 2019; Zhang et al., 2020; Wang et al., 2020d; Wang and Tu, 2020). Therefore, we may take a second-order dependency parser as the teacher to improve a sequence labeling parser. Here we consider the second-order dependency parser of Wang and Tu (2020). It employs mean field variational inference to estimate the probabilities of arc existence Pt (hi |x) and uses a first-order biaffine model to estimate the probabilities of arc labels Pt (li |x). Therefore, the substructure marginal can be calculated in the same way as Eq. 5. 3.3 Student Factorization Produces More Fine-grained Substructures than Teac"
2021.acl-long.46,2020.acl-main.304,1,0.125515,"oblems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over these substructure"
2021.acl-long.46,2020.emnlp-main.485,1,0.0750727,"oblems, however, the output space is exponentially large, making the cross-entropy objective intractable to compute and optimize directly. Take sequence labeling for example. If the size of the label set is L, then there are Ln possible label sequences for a sentence of n words and it is infeasible to compute the cross-entropy by enumerating the label sequences. Previous approaches to structural KD either choose to perform KD on local decisions or substructures instead of on the full output structure, or resort to Top-K approximation of the objective (Kim and Rush, 2016; Kuncoro et al., 2016; Wang et al., 2020a). In this paper, we derive a factorized form of the structural KD objective based on the fact that almost all the structured prediction models factorize the scoring function of the output structure into scores of substructures. If the student’s substructure space is polynomial in size and the teacher’s 550 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 550–564 August 1–6, 2021. ©2021 Association for Computational Linguistics marginal distributions over these substructure"
2021.acl-long.46,2021.acl-long.206,1,0.538582,"still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wan"
2021.acl-long.46,2021.acl-long.142,1,0.721594,"still outperform MaxEnt, those of the NER-as-parsing teacher clearly underperform MaxEnt. This provides an explanation as to why Struct. KD in Case 4 has equal or even lower accuracy than the Token KD baseline in Case 2a in Table 3. 6 6.1 Related Work Structured Prediction In this paper, we use sequence labeling and dependency parsing as two example structured prediction tasks. In sequence labeling, a lot of work applied the linear-chain CRF and achieved state-of-the-art performance in various tasks (Ma and Hovy, 2016; Akbik et al., 2018; Liu et al., 2019b; Yu et al., 2020; Wei et al., 2020; Wang et al., 2021a,b). Meanwhile, a lot of other work used the MaxEnt layer instead of the CRF for sequence labeling (Devlin et al., 2019; Conneau et al., 2020; Wang et al., 2020b) because MaxEnt makes it easier to fine-tune pretrained contextual embeddings in training. Another advantage of MaxEnt in comparison with CRF is its speed. Yang et al. (2018) showed that models equipped with the CRF are about two times slower than models with the MaxEnt layer in sequence labeling. In dependency parsing, recent work shows that second-order CRF parsers achieve significantly higher accuracy than first-order parsers (Wan"
2021.acl-long.493,N19-1423,0,0.522311,"oduction Document understanding is an essential problem in NLP, which aims to read and analyze textual documents. In addition to plain text, many realworld applications require to understand scanned documents with rich text. As shown in Figure 1, such scanned documents contain various structured information, like tables, digital forms, receipts, and invoices. The information of a document image is usually presented in natural language, but the format can be organized in many ways from multicolumn layout to various tables/forms. Inspired by the recent development of pretrained language models (Devlin et al., 2019; Liu et al., 2019; Wang et al., 2019) in various NLP tasks, recent studies on document image pretraining (Zhang et al., 2020; Xu et al., 2019) have pushed the limits of a variety of document image understanding tasks, which learn the interaction between text and layout information across scanned document images. Xu et al. (2019) propose LayoutLM, which is a pre-training method of text and layout for document image understanding tasks. It uses 2Dposition embeddings to model the word-level layout information. However, it is not enough to model the word-level layout information, and the model sh"
2021.acl-long.493,D18-1476,0,0.0575342,"Missing"
2021.acl-long.493,2021.ccl-1.108,0,0.0536789,"Missing"
2021.acl-long.493,D16-1264,0,0.0509289,"tasks, each of which contains form images. These three tasks are form understanding task, document visual question answering task, and document image classification task. For the form understanding task, StructuralLM predicts B, I, E, S, O tags for each token, and then uses sequential labeling to find the four types of entities including the question, answer, header, or other. For the document visual question answering task, we treat it as an extractive QA task and build a token-level classifier on the top of token representations, which is usually used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016; Wang et al., 2018). For the document image classification task, StructuralLM predicts the class labels using the representation of the [CLS] token. 3 Experiments 3.1 Pre-training Configuration Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006). It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pretraining dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Ther"
2021.acl-long.493,D19-1348,0,0.0269467,"6315 Figure 5: Examples of the output of LayoutLM and StructuralLM on the FUNSD dataset. The division of |means that the two phrases are independent labels. et al., 2005; Wei et al., 2013), they are usually time-consuming to design manually features and difficult to obtain a high-level abstract semantic context. In addition, these methods usually relied on visual cues but ignored textual information. 4.2 Deep Learning Approaches Nowadays, deep learning methods have become the mainstream for many machine learning problems (Yang et al., 2017; Borges Oliveira and Viana, 2017; Katti et al., 2018; Soto and Yoo, 2019). (Yang et al., 2017) propose a pixel-by-pixel classification to solve the document semantic structure extraction problem. Specifically, they propose a multimodal neural network that considers visual and textual information, while this work is an end-toend approach. (Katti et al., 2018) first propose a fully convolutional encoder-decoder network to predict a segmentation mask and bounding boxes. In this way, the model significantly outperforms approaches based on sequential text or document images. In addition, (Soto and Yoo, 2019) incorporate contextual information into the Faster R-CNN model"
2021.acl-long.493,P18-1158,1,0.847945,"tains form images. These three tasks are form understanding task, document visual question answering task, and document image classification task. For the form understanding task, StructuralLM predicts B, I, E, S, O tags for each token, and then uses sequential labeling to find the four types of entities including the question, answer, header, or other. For the document visual question answering task, we treat it as an extractive QA task and build a token-level classifier on the top of token representations, which is usually used in Machine Reading Comprehension (MRC) (Rajpurkar et al., 2016; Wang et al., 2018). For the document image classification task, StructuralLM predicts the class labels using the representation of the [CLS] token. 3 Experiments 3.1 Pre-training Configuration Pre-training Dataset. Following LayoutLM, we pre-train StructuralLM on the IIT-CDIP Test Collection 1.0 (Lewis et al., 2006). It is a large-scale scanned document image dataset, which contains more than 6 million documents, with more than 11 million scanned document images. The pretraining dataset (IIT-CDIP Test Collection) only contains pure texts while missing their corresponding bounding boxes. Therefore, we need to re"
2021.acl-short.111,N19-1423,0,0.049222,"w a slot is roughly operated in the current dialog context and connected with all possible tokens regarding its values in the schema. The dialog context encoder is used for the parameter initialization of the base part of a DST model. The pre-trained corpus is constructed from MultiWOZ2.1 dialogs (Eric et al., 2020) and the off-the-shelf synthesized dialogs (Campagna et al., 2020), which contains 337,346 dialog data in total. We also leverage the language modelling (LM) loss as an auxiliary loss Laux to learn contextual representations of natural language. To be specific, we use the MLM loss (Devlin et al., 2019) as Laux for transformer-based DST modes and the summation of both forward and backward LM losses (Peters et al., 2018) for RNN-based DST models. We only use the original MultiWOZ2.1 dialogs to optimize Laux , considering that synthesized data is not suitable for natural language modelling. However, both the original and synthesized data are used to optimize Lseq and Lcls . 3.3 The Review Module The process of review often help a learner consolidate difficult concepts newly learned. We design a review module to consider mispredicted examples as the concepts that the DST model has not grasped d"
2021.acl-short.111,2020.lrec-1.53,0,0.516868,"mation, a curriculum module that optimizes the model with CL, and a review module that augments mispredicted data to reinforce the CL training. We show that our proposed approach improves DST performance over both a transformerbased and RNN-based DST model (TripPy and TRADE) and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a str"
2021.acl-short.111,W19-5932,0,0.0172906,"and achieves new state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et"
2021.acl-short.111,2020.sigdial-1.4,0,0.0310204,"Missing"
2021.acl-short.111,C18-1105,0,0.0278682,"al., 2020), reading comprehension (Tay et al., 2019) and open-domain chatbots (Bao et al., 2020; Cai et al., 2020; Su et al., 2020). Yet, the research on using CL in task-oriented dialog systems is limited. There has been some work (Saito, 2018; Zhao et al., 2021) on using CL in dialog policy learning, but applying CL to DST has not been investigated. Learning a structural inductive bias during pretraining has been shown beneficial in downstream tasks that require parsing semantics, such as textto-SQL (Yu et al., 2021) and table cell recognition (Wang et al., 2020). There are also many works (Hou et al., 2018; Yoo et al., 2020; Yin et al., 2020) on dialog augmentation. We aim to integrate these methods to build a general CL framework for DST. 6 Conclusion In this paper, we propose a model-agnostic framework named as schema-aware curriculum learning for DST, which exploits both the curriculum 883 References Siqi Bao, Huang He, Fan Wang, Hua Wu, Haifeng Wang, Wenquan Wu, Zhen Guo, Zhibin Liu, and Xinchao Xu. 2020. Plato-2: Towards building an open-domain chatbot via curriculum learning. arXiv preprint arXiv:2006.16779. Yoshua Bengio, J´erˆome Louradour, Ronan Collobert, and Jason Weston. 2009. Curri"
2021.acl-short.111,2020.acl-main.567,0,0.0308017,"state-of-the-art results on WOZ2.0 and MultiWOZ2.1. 1 Figure 1: An easy and a hard dialog example for DST. Introduction Dialog state tracking (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and ha"
2021.acl-short.111,2020.acl-main.53,0,0.0359253,"users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and has been shown useful in various other problems (Wang et al., 2021). DST training examples also vary greatly in their difficulty levels. As shown in Figure 1,"
2021.acl-short.111,P19-1546,0,0.0447439,"Missing"
2021.acl-short.111,2020.acl-main.41,0,0.0724363,"Missing"
2021.acl-short.111,N18-1202,0,0.017011,"in the schema. The dialog context encoder is used for the parameter initialization of the base part of a DST model. The pre-trained corpus is constructed from MultiWOZ2.1 dialogs (Eric et al., 2020) and the off-the-shelf synthesized dialogs (Campagna et al., 2020), which contains 337,346 dialog data in total. We also leverage the language modelling (LM) loss as an auxiliary loss Laux to learn contextual representations of natural language. To be specific, we use the MLM loss (Devlin et al., 2019) as Laux for transformer-based DST modes and the summation of both forward and backward LM losses (Peters et al., 2018) for RNN-based DST models. We only use the original MultiWOZ2.1 dialogs to optimize Laux , considering that synthesized data is not suitable for natural language modelling. However, both the original and synthesized data are used to optimize Lseq and Lcls . 3.3 The Review Module The process of review often help a learner consolidate difficult concepts newly learned. We design a review module to consider mispredicted examples as the concepts that the DST model has not grasped during CL, and utilize a schema-based data augmenter to produce similar cases from the examples. Specifically, the DST m"
2021.acl-short.111,W18-5707,0,0.0427423,"Missing"
2021.acl-short.111,2020.acl-main.563,0,0.0626527,"Missing"
2021.acl-short.111,N10-1116,0,0.0129957,"into our curriculum design: 1) current dialog turn number t; 2) the total token number of (Rt , Ut ); 3) the number of mentioned name entities like ‘hotel names’ in Zt ; 4) the number of newly added or changed slots in Yt . We set the maximum values of above factors as 7/50/4/6 respectively, and normalize all factors into rtrul,i ∈ [0, 1], where i indicates the i-th factor. Finally, the hybrid difficulty calculated P4score isrul,i jointly as rthyb = P α0 rmod + α r , where t i=1 i t 4 hyb r ∈ [0, 1] and i=0 αi = 1. 3.1.2 The Training Scheduler We adopt a widely used strategy called baby step (Spitkovsky et al., 2010) to organize the scored data for CL. Specifically, we divide the score uniformly into N intervals and distribute the sorted data into N buckets accordingly. The optimization starts from the easiest bucket as the initial training stage. After reaching a fixed number of maximum epochs or convergence, the next bucket is merged 880 Figure 2: An overview of the SaCLog training procedures. into the current training subset and shuffled for the next training stage. In our experiment, we set the maximum number of epochs as 3, and treat as the convergence if the training loss ceases to decrease and the"
2021.acl-short.111,P19-1486,0,0.0621828,"Missing"
2021.acl-short.111,E17-1042,0,0.0497929,"Missing"
2021.acl-short.111,P19-1078,0,0.138292,"ng (DST) extracts users’ goals in task-oriented dialog systems, where dialog states are often represented in terms of a set of slot-value pairs (Williams et al., 2016; Eric et al., 2020). Due to the language variety of multi-turn dialogs, the concepts of slots and values are often indirectly expressed in the conversation (such as co-references, ellipsis, and diverse appearances), which are a major bottleneck for improving DST performance (Gao et al., 2019; Hu et al., 2020). Many existing DST methods have focused on designing better model architectures to tackle the problems (Dai et al., 2018; Wu et al., 2019; Kim et al., 2020), but still neglect the full exploitation of two important aspects of structural information. The first is curriculum structure in a dataset. Such a structure relies on a measure of the difficulty of examples, which can be used to guide the ∗ Corresponding author model training in an easy-to-hard manner, imitating the meaningful learning order in human curricula. This paradigm is called curriculum learning (CL) (Bengio et al., 2009) and has been shown useful in various other problems (Wang et al., 2021). DST training examples also vary greatly in their difficulty levels. As"
2021.acl-short.111,2020.findings-emnlp.95,0,0.0267103,"icitly via multi-round interactions, requiring a complex inference process to find the value ‘golden house’ referred by the slot ‘restaurant-name’. However, CL has been rarely studied in DST, and models are often trained with dialog data in a random order. In addition, schema structure is prominent in multi-domain task-oriented dialogs. A schema is specified by a collection of all possible slots and their values, which describes semantic relations among them. Some previous work utilized the structure via an extra schema graph in a regular training process (Chen et al., 2020; Zhu et al., 2020; Wu et al., 2020). We propose to incorporate schema information into CL through a pre-curriculum process, in which a DST model can be pre-trained with schema-related objectives to prepare for upcom879 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 879–885 August 1–6, 2021. ©2021 Association for Computational Linguistics ing DST examples. To reinforce the CL training, we can also expand those examples with frequent mispredictions during CL based upon the schema, enabling the"
2021.acl-short.111,2020.acl-main.542,0,0.0871591,"Missing"
2021.acl-short.111,2020.acl-main.620,0,0.0747792,"Missing"
2021.acl-short.111,2020.findings-emnlp.68,0,0.0237234,"her intention implicitly via multi-round interactions, requiring a complex inference process to find the value ‘golden house’ referred by the slot ‘restaurant-name’. However, CL has been rarely studied in DST, and models are often trained with dialog data in a random order. In addition, schema structure is prominent in multi-domain task-oriented dialogs. A schema is specified by a collection of all possible slots and their values, which describes semantic relations among them. Some previous work utilized the structure via an extra schema graph in a regular training process (Chen et al., 2020; Zhu et al., 2020; Wu et al., 2020). We propose to incorporate schema information into CL through a pre-curriculum process, in which a DST model can be pre-trained with schema-related objectives to prepare for upcom879 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 879–885 August 1–6, 2021. ©2021 Association for Computational Linguistics ing DST examples. To reinforce the CL training, we can also expand those examples with frequent mispredictions during CL based upon the sch"
2021.bionlp-1.20,W19-1909,0,0.0659497,"Missing"
2021.bionlp-1.20,D19-1371,0,0.129138,"bing dataset shows that our model has better ability to model medical knowledge. … treated with glycerin show reduced inflammation after 2 hours. C0017861 1,2,3-Propanetriol C0011603 dermatitis R176722500 (C0017861, may_prevent, C0011603) Figure 1: An example of the biomedical sentence. Two entities “glycerin” and “inflammation” are linked to C0017861 (1,2,3-Propanetriol) and C0011603 (dermatitis) respectively with a relation triplet (C0017861, may_prevent, C0011603) in UMLS. over general-domain PLMs (Lee et al., 2020; Peng et al., 2019; Gu et al., 2020; Shin et al., 2020; Lewis et al., 2020; Beltagy et al., 2019; Alsentzer et al., 2019). Secondly, unlike training language models (LMs) with unlabeled text, many works explore training the model with structural knowledge (i.e. triplets and facts) for better language understanding (Zhang et al., 2019; Peters et al., 2019; Févry et al., 2020; Wang et al., 2019). In this work, we propose to combine the above two strategies for a better Knowledge enhanced Biomedical pretrained Language Model (KeBioLM). As an applied discipline that needs a lot of facts and evidence, the biomedical and clinical fields have accumulated data and knowledge from a very early age"
2021.bionlp-1.20,W04-1213,0,0.403753,"Missing"
2021.bionlp-1.20,N19-1423,0,0.0447065,"em. To capture both the textual and conUMLS relation triplets. The probing results ceptual information, several knowledge-enhanced show that our KeBioLM absorbs more knowl- PLMs are proposed. edge than other biomedical PLMs. Entities are used for bridging tokens and knowledge graphs. Zhang et al. (2019) align tokens 2 Related Work and entities within sentences, and aggregate token and entity representations via two multi-head 2.1 Biomedical PLMs self-attentions. KnowBert (Peters et al., 2019) and Models like ELMo (Peters et al., 2018) and BERT Entity as Experts (EAE) (Févry et al., 2020) use (Devlin et al., 2019) show the effectiveness of the the entity linker to perform entity disambiguation paradigm of first pre-training an LM on the unla- for candidate entity spans and enhance token repbeled text then fine-tuning the model on the down- resentations using entity embeddings. Inspired by stream NLP tasks. However, direct application of entity-enhanced PLMs, we follow the model of the LMs pre-trained on the encyclopedia and web EAE to inject biomedical knowledge into KeBioLM by performing entity detection and linking. 1 Our codes and model can be found at https:// github.com/GanjinZero/KeBioLM. Relatio"
2021.bionlp-1.20,2020.emnlp-main.400,0,0.331261,"ence. Two entities “glycerin” and “inflammation” are linked to C0017861 (1,2,3-Propanetriol) and C0011603 (dermatitis) respectively with a relation triplet (C0017861, may_prevent, C0011603) in UMLS. over general-domain PLMs (Lee et al., 2020; Peng et al., 2019; Gu et al., 2020; Shin et al., 2020; Lewis et al., 2020; Beltagy et al., 2019; Alsentzer et al., 2019). Secondly, unlike training language models (LMs) with unlabeled text, many works explore training the model with structural knowledge (i.e. triplets and facts) for better language understanding (Zhang et al., 2019; Peters et al., 2019; Févry et al., 2020; Wang et al., 2019). In this work, we propose to combine the above two strategies for a better Knowledge enhanced Biomedical pretrained Language Model (KeBioLM). As an applied discipline that needs a lot of facts and evidence, the biomedical and clinical fields have accumulated data and knowledge from a very early age (Ashburner et al., 2000; Stearns et al., 2001). One of the most representative work is Unified Medical Language System (UMLS) (Boden1 Introduction reider, 2004) that contains more than 4M entities Large-scale pretrained language models (PLMs) with their synonyms and defines over"
2021.bionlp-1.20,2020.emnlp-main.372,0,0.437607,"direct application of entity-enhanced PLMs, we follow the model of the LMs pre-trained on the encyclopedia and web EAE to inject biomedical knowledge into KeBioLM by performing entity detection and linking. 1 Our codes and model can be found at https:// github.com/GanjinZero/KeBioLM. Relation triplets provide intrinsic knowledge be181 tween entity pairs. KEPLER (Wang et al., 2019) learns the knowledge embeddings through relation triplets while pretraining. K-BERT (Liu et al., 2020) converts input sentences into sentence trees by relation triplets to infuse knowledge. In the biomedical domain, He et al. (2020) inject disease knowledge to existing PLMs by predicting diseases names and aspects on Wikipedia passages. Michalopoulos et al. (2020) use UMLS synonyms to supervise masked language modeling. We propose KeBioLM to infuse various kinds of biomedical knowledge from UMLS including but not limited to diseases. 3 Approach In this paper, we assume to access an entity set E = {e1 , ..., et }. For a sentence x = {x1 , ..., xn }, we assume some spans m = (xi , ..., xj ) can be grounded to one or more entities in E. We further assume the disjuncture of these spans. In this paper, we use UMLS to set the"
2021.bionlp-1.20,2020.clinicalnlp-1.17,0,0.13834,"s on a collected probing dataset shows that our model has better ability to model medical knowledge. … treated with glycerin show reduced inflammation after 2 hours. C0017861 1,2,3-Propanetriol C0011603 dermatitis R176722500 (C0017861, may_prevent, C0011603) Figure 1: An example of the biomedical sentence. Two entities “glycerin” and “inflammation” are linked to C0017861 (1,2,3-Propanetriol) and C0011603 (dermatitis) respectively with a relation triplet (C0017861, may_prevent, C0011603) in UMLS. over general-domain PLMs (Lee et al., 2020; Peng et al., 2019; Gu et al., 2020; Shin et al., 2020; Lewis et al., 2020; Beltagy et al., 2019; Alsentzer et al., 2019). Secondly, unlike training language models (LMs) with unlabeled text, many works explore training the model with structural knowledge (i.e. triplets and facts) for better language understanding (Zhang et al., 2019; Peters et al., 2019; Févry et al., 2020; Wang et al., 2019). In this work, we propose to combine the above two strategies for a better Knowledge enhanced Biomedical pretrained Language Model (KeBioLM). As an applied discipline that needs a lot of facts and evidence, the biomedical and clinical fields have accumulated data and knowledge"
2021.bionlp-1.20,2020.emnlp-main.479,0,0.0334509,"classified as, and used_by is converted to is used by. Commonly, different relation triplets can generate same query since triplets may overlap (s, r, −) or (−, r, o) with each other. We deduplicate all repeat queries and randomly choose at most 200 queries from all relation types in UMLS. After deduplication, one query can have multiple CUIs as answers. For example: • Q: [MASK] may treat essential tremor. • A1 : C0282321: propranolol hydrochloride • A2 : C0033497: propranolol We summarize our generated UMLS relation probing dataset in Table 5. Unlike LAMA (Petroni et al., 2019) and X-FACTR (Jiang et al., 2020) that contain less than 50 kinds of relation, our probing task is a more difficult task requiring a model to decode entities over 900 kinds of relations. SciBERT ClinicalBERT BlueBERT KeBioLM Type 1 13.92 4.19 4.67 14.01 Type 2 1.01 0.33 0.39 1.48 Overall 2.75 0.79 1.02 3.26 Table 6: Results of the probing test in terms of Recall@5. 4.6.3 Evaluation Metric Since multiple correct CUIs exist for one query, we consider a model answering the query correctly if any decoded tokens in any [MASK] length hit any of the correct CUIs. We evaluate the probing results by the relation-level macro-recall@5."
2021.bionlp-1.20,W19-2011,0,0.0170698,"periments on 5 NER datasets and 3 RE datasets. Results demonstrate that our KeBioLM achieves the best performance on both NER and RE tasks. text usually fails on the biomedical domain, because of the distinctive terminologies and idioms. The gap between general and biomedical domains inspires the researchers to propose LMs specially tailored for the biomedical domain. BioBERT (Lee et al., 2020) is the most widely used biomedical PLM which is trained on PubMed abstracts and PMC articles. It outperforms vanilla BERT in named entity recognition, relation extraction, and question answering tasks. Jin et al. (2019) train BioELMo with PubMed abstracts, and find features extracted by BioELMo contain entity-type and relational information. Different training corpora have been used for enhancing performance of sub-domain tasks. ClinicalBERT (Alsentzer et al., 2019), BlueBERT (Peng et al., 2019) and bio-lm (Lewis et al., 2020) utilize clinical notes MIMIC to improve clinical-related downstream tasks. SciBERT (Beltagy et al., 2019) uses papers from the biomedical and computer science domain as training corpora with a new vocabulary. KeBioLM is trained on PubMed abstracts to adapt to PubMedrelated downstream t"
2021.bionlp-1.20,W19-5034,0,0.0243437,"elations domain text (i.e. PubMed and MIMIC for biomedi- can provide information for better text understandcal domain) can further improve downstream tasks ing (Xu et al., 2018; Yuan et al., 2020). ∗ To this end, we propose to improve biomedical Work done at Alibaba DAMO Academy. † Corresponding author. PLMs with explicit knowledge modeling. Firstly, 180 Proceedings of the BioNLP 2021 workshop, pages 180–190 June 11, 2021. ©2021 Association for Computational Linguistics we process the PubMed text to link entities to the knowledge base. We apply an entity recognition and linking tool ScispaCy (Neumann et al., 2019) to annotate 660M entities in 3.5M documents. Secondly, we implement a knowledge enhanced language model based on Févry et al. (2020), which performs a text-only encoding and a text-entity fusion encoding. Text-only encoding is responsible for bridging text and entities. Text-entity fusion encoding fuses information from tokens and knowledge from entities. Finally, two objectives as entity extraction and linking are added to learn better entity representations. To be noticed, we initialize the entity embeddings with TransE (Bordes et al., 2013), which leverages not only entity but also relatio"
2021.bionlp-1.20,W19-5006,0,0.379205,"rate the effectiveness of our approach. Further analysis on a collected probing dataset shows that our model has better ability to model medical knowledge. … treated with glycerin show reduced inflammation after 2 hours. C0017861 1,2,3-Propanetriol C0011603 dermatitis R176722500 (C0017861, may_prevent, C0011603) Figure 1: An example of the biomedical sentence. Two entities “glycerin” and “inflammation” are linked to C0017861 (1,2,3-Propanetriol) and C0011603 (dermatitis) respectively with a relation triplet (C0017861, may_prevent, C0011603) in UMLS. over general-domain PLMs (Lee et al., 2020; Peng et al., 2019; Gu et al., 2020; Shin et al., 2020; Lewis et al., 2020; Beltagy et al., 2019; Alsentzer et al., 2019). Secondly, unlike training language models (LMs) with unlabeled text, many works explore training the model with structural knowledge (i.e. triplets and facts) for better language understanding (Zhang et al., 2019; Peters et al., 2019; Févry et al., 2020; Wang et al., 2019). In this work, we propose to combine the above two strategies for a better Knowledge enhanced Biomedical pretrained Language Model (KeBioLM). As an applied discipline that needs a lot of facts and evidence, the biomedical"
2021.bionlp-1.20,N18-1202,0,0.286612,"anguage Model (KeBioLM). As an applied discipline that needs a lot of facts and evidence, the biomedical and clinical fields have accumulated data and knowledge from a very early age (Ashburner et al., 2000; Stearns et al., 2001). One of the most representative work is Unified Medical Language System (UMLS) (Boden1 Introduction reider, 2004) that contains more than 4M entities Large-scale pretrained language models (PLMs) with their synonyms and defines over 900 kinds of are proved to be effective in many natural language relations. Figure 1 shows an example. There are processing (NLP) tasks (Peters et al., 2018; Devlin two entities “glycerin” and “inflammation” that et al., 2019). However, there are still many works can be linked to C0017861 (1,2,3-Propanetriol) that explore multiple strategies to improve the and C0011603 (dermatitis) respectively with a PLMs. Firstly, in specialized domains (i.e biomedi- may_prevent relation in UMLS. As the most imporcal domain), many works demonstrate that using in- tant facts in biomedical text, entities and relations domain text (i.e. PubMed and MIMIC for biomedi- can provide information for better text understandcal domain) can further improve downstream tasks"
2021.bionlp-1.20,D19-1005,0,0.0294019,"Missing"
2021.bionlp-1.20,D19-1250,0,0.0203114,"f, classified_as is converted to is classified as, and used_by is converted to is used by. Commonly, different relation triplets can generate same query since triplets may overlap (s, r, −) or (−, r, o) with each other. We deduplicate all repeat queries and randomly choose at most 200 queries from all relation types in UMLS. After deduplication, one query can have multiple CUIs as answers. For example: • Q: [MASK] may treat essential tremor. • A1 : C0282321: propranolol hydrochloride • A2 : C0033497: propranolol We summarize our generated UMLS relation probing dataset in Table 5. Unlike LAMA (Petroni et al., 2019) and X-FACTR (Jiang et al., 2020) that contain less than 50 kinds of relation, our probing task is a more difficult task requiring a model to decode entities over 900 kinds of relations. SciBERT ClinicalBERT BlueBERT KeBioLM Type 1 13.92 4.19 4.67 14.01 Type 2 1.01 0.33 0.39 1.48 Overall 2.75 0.79 1.02 3.26 Table 6: Results of the probing test in terms of Recall@5. 4.6.3 Evaluation Metric Since multiple correct CUIs exist for one query, we consider a model answering the query correctly if any decoded tokens in any [MASK] length hit any of the correct CUIs. We evaluate the probing results by th"
2021.bionlp-1.20,P19-1139,0,0.0601786,"gure 1: An example of the biomedical sentence. Two entities “glycerin” and “inflammation” are linked to C0017861 (1,2,3-Propanetriol) and C0011603 (dermatitis) respectively with a relation triplet (C0017861, may_prevent, C0011603) in UMLS. over general-domain PLMs (Lee et al., 2020; Peng et al., 2019; Gu et al., 2020; Shin et al., 2020; Lewis et al., 2020; Beltagy et al., 2019; Alsentzer et al., 2019). Secondly, unlike training language models (LMs) with unlabeled text, many works explore training the model with structural knowledge (i.e. triplets and facts) for better language understanding (Zhang et al., 2019; Peters et al., 2019; Févry et al., 2020; Wang et al., 2019). In this work, we propose to combine the above two strategies for a better Knowledge enhanced Biomedical pretrained Language Model (KeBioLM). As an applied discipline that needs a lot of facts and evidence, the biomedical and clinical fields have accumulated data and knowledge from a very early age (Ashburner et al., 2000; Stearns et al., 2001). One of the most representative work is Unified Medical Language System (UMLS) (Boden1 Introduction reider, 2004) that contains more than 4M entities Large-scale pretrained language models (P"
2021.bionlp-1.20,2020.emnlp-main.379,0,0.380603,"ch. Further analysis on a collected probing dataset shows that our model has better ability to model medical knowledge. … treated with glycerin show reduced inflammation after 2 hours. C0017861 1,2,3-Propanetriol C0011603 dermatitis R176722500 (C0017861, may_prevent, C0011603) Figure 1: An example of the biomedical sentence. Two entities “glycerin” and “inflammation” are linked to C0017861 (1,2,3-Propanetriol) and C0011603 (dermatitis) respectively with a relation triplet (C0017861, may_prevent, C0011603) in UMLS. over general-domain PLMs (Lee et al., 2020; Peng et al., 2019; Gu et al., 2020; Shin et al., 2020; Lewis et al., 2020; Beltagy et al., 2019; Alsentzer et al., 2019). Secondly, unlike training language models (LMs) with unlabeled text, many works explore training the model with structural knowledge (i.e. triplets and facts) for better language understanding (Zhang et al., 2019; Peters et al., 2019; Févry et al., 2020; Wang et al., 2019). In this work, we propose to combine the above two strategies for a better Knowledge enhanced Biomedical pretrained Language Model (KeBioLM). As an applied discipline that needs a lot of facts and evidence, the biomedical and clinical fields have accumulate"
2021.bionlp-1.20,P82-1020,0,0.804331,"Missing"
2021.emnlp-main.185,D15-1075,0,0.25344,",shaohan.ljh,jian.sun,f.huang,luo.si}@alibaba-inc.com Abstract different learning schemes. Kiros et al. (2015); Logeswaran and Lee (2018); Hill et al. (2016) train Learning sentence embeddings from dialogues sentence encoders in a self-supervised manner with has drawn increasing attention due to its low web pages and books. Conneau et al. (2017); Cer annotation cost and high domain adaptabilet al. (2018); Reimers and Gurevych (2019) proity. Conventional approaches employ the pose to learn sentence embeddings on the supersiamese-network for this task, which obtains vised datasets such as SNLI (Bowman et al., 2015) the sentence embeddings through modeling and MNLI (Williams et al., 2018). Although the the context-response semantic relevance by apsupervised-learning approaches achieve better perplying a feed-forward network on top of the formance, they suffer from high cost of annotation sentence encoders. However, as the semantic textual similarity is commonly measured in building the training dataset, which makes them through the element-wise distance metrics (e.g. hard to adapt to other domains or languages. cosine and L2 distance), such architecture Recently, learning sentence embeddings from yields"
2021.emnlp-main.185,D18-2029,0,0.0906045,"mance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presents two limitations: (1) It yields a large gap between training and evaluating, since the semantic textual similarity is commonly measured by th"
2021.emnlp-main.185,L18-1269,0,0.0142213,"MNLI datasets, achieving the new state-of-the-art performance. Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in this area. Wu et al. (2020); Giorgi et al. (2020); Meng et al. (2021) incorporate the data augmentation methods including the word-level deletion, reordering, substitution, and the sentencelevel corruption into the pre-training of deep Transformer models to improve the sentence representation ability, achieving significantly better performance than BERT especially on the sentence-level tasks (Wang et al., 2018; Cer et al., 2017; Conneau and Kiela, 2018). Gao et al. (2021) apply a twice independent dropout to obtain two same-source embeddings from a single sentence as input. Through 3 Problem Formulation optimizing their cosine distance, SimCSE achieves Suppose that we have a dialogue remarkable gains over the previous baselines. Yan K = {Si }i=1 , where Si = et al. (2021) empirically study more data augmen- dataset D tation strategies in learning sentence embeddings, {u1 , · · · , uk−1 , r, uk+1 , · · · , ut } is the i-th dialogue session in D with t turn utterances. r is the and it also achieves remarkable performance as SimCSE. In this wor"
2021.emnlp-main.185,D17-1070,0,0.0178248,"sponse matching relationships. Our work is closely related to their works. We propose a novel dialogue-based contrastive learning approach, which directly models the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence. 2.2 Supervised Learning Approaches The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets (Bowman et al., 2015; Williams et al., 2018). Conneau et al. (2017) demonstrate the superior performance of the supervised learning model on both the STS-benchmark (Cer et al., 2017) and the SICK-R tasks (Marelli et al., 2014). Based on this observation, Cer et al. (2018) further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task (Henderson et al., 2017; Kiros et al., 2015), and the NLI classification task, achieving significant improvement over InferSent. Reimers and Gurevych (2019) employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI data"
2021.emnlp-main.185,N19-1423,0,0.476909,"on measures, demonstrating the multi-turn dialogue context can improve ing its effectiveness. Further quantitative exthe sentence embedding performance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic relevance, thereby learning the sentence embeddings. However, such architecture presen"
2021.emnlp-main.185,2021.emnlp-main.552,0,0.0500556,"Missing"
2021.emnlp-main.185,2020.acl-main.740,0,0.0309319,"Missing"
2021.emnlp-main.185,2020.emnlp-main.733,0,0.076929,"are inherently limited and hard to achieve further improvement. Recently, the pre-trained language models such as BERT (Devlin et al., 2019) and GPT (Radford et al.) yield strong performances across many downstream tasks (Wang et al., 2018). However, BERT’s embeddings show poor performance without fine-tuning and many efforts have been devoted to alleviating this issue. Zhang et al. (2020) propose a self-supervised learning approach that derives meaningful BERT sentence embeddings by maximizing the mutual information between the global sentence embedding and all its local context embeddings. Li et al. (2020) argue that BERT induces a non-smooth anisotropic semantic space. They propose to use a flow-based generative module to transform BERT’s embeddings into isotropic semantic space. Similar to this work, Su et al. (2021) replace the flow-based generative module with a simple but efficient linear mapping layer, achieving competitive results with reported experiments in BERT-flow. For dialogue, Yang et al. (2018) train a siamese transformer network with single-turn inputresponse pairs extracted from Reddit. Such architecture is further extended in (Reimers and Gurevych, 2019) by replacing the trans"
2021.emnlp-main.185,marelli-etal-2014-sick,0,0.0249141,"ls the context-response matching relationships without an intermediate MLP. We also consider the interactions between each utterance in the dialogue context and the response instead of simply treating the dialogue context as a long sequence. 2.2 Supervised Learning Approaches The supervised learning approaches mainly focus on training classification models with the SNLI and the MNLI datasets (Bowman et al., 2015; Williams et al., 2018). Conneau et al. (2017) demonstrate the superior performance of the supervised learning model on both the STS-benchmark (Cer et al., 2017) and the SICK-R tasks (Marelli et al., 2014). Based on this observation, Cer et al. (2018) further extend the supervised learning to the multi-task learning by introducing the QA prediction task, the Skip-Thought-like task (Henderson et al., 2017; Kiros et al., 2015), and the NLI classification task, achieving significant improvement over InferSent. Reimers and Gurevych (2019) employ BERT as sentence encoders in the siamese-network and finetune them with the SNLI and the MNLI datasets, achieving the new state-of-the-art performance. Lately, the contrastive self-supervised learning approaches have shown their effectiveness and merit in t"
2021.emnlp-main.185,D14-1162,0,0.0885511,"ce embedding to the isotropic semantic space. For BERT, we use the [CLS] token embedding (denoted as BERT-CLS) and the average of the sequence output embeddings (denoted as BERT-avg) as the sentence embedding, and the same is true for domain-adaptive BERT. It should be noted that in related sentence embedding researches, domainadaptive BERT is rarely considered since the training datasets are relatively small. Fortunately, the large-scale dialogue datasets allow us to explore whether the domain-adaptive pre-training is helpful for our tasks. We also adopt the average of GloVe word embeddings (Pennington et al., 2014) (denoted as Avg. GloVe) as the sentence embedding to compare with our results. 5.2.2 Dialogue-based self-supervised learning methods In this line, we mainly consider the siamesenetworks commonly applied in dialogue-based researches. Considering none of the previous works (Yang et al., 2018; Henderson et al., 2020) employs the pre-trained language model as encoder, we re2401 Model Microsoft Corpus Corr. MAP MRR Jing Dong Corpus Corr. MAP MRR E-commerce Corpus Corr. MAP MRR Self-supervised models Avg. GloVe embeddings BERT-CLS BERT-avg BERT-flow BERT-whitening 36.64 22.34 40.95 45.56 26.70 31.5"
2021.emnlp-main.185,2020.findings-emnlp.196,0,0.0767416,"Missing"
2021.emnlp-main.185,N16-1162,0,0.0261775,"Missing"
2021.emnlp-main.185,D19-1410,0,0.130655,"in terms of MAP and Henderson et al. (2020) demonstrate that introducSpearman’s correlation measures, demonstrating the multi-turn dialogue context can improve ing its effectiveness. Further quantitative exthe sentence embedding performance. However, periments show that our approach achieves they concatenate the multi-turn dialogue context better performance when leveraging more diinto a long token sequence, failing to model interalogue context and remains robust when less sentence semantic relationships among the uttertraining data is provided. ances. Recently, more advanced methods such as (Reimers and Gurevych, 2019) achieve better per1 Introduction formance by employing BERT (Devlin et al., 2019) Sentence embeddings are used with success for as the sentence encoder. These works have in coma variety of NLP applications (Cer et al., 2018) mon that they employ a feed-forward network with and many prior methods have been proposed with a non-linear activation on top of the sentence en2396 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2396–2406 c November 7–11, 2021. 2021 Association for Computational Linguistics coders to model the context-response semantic rele"
2021.emnlp-main.185,W18-5446,0,0.0419368,"Missing"
2021.emnlp-main.185,N18-1101,0,0.11829,"learning schemes. Kiros et al. (2015); Logeswaran and Lee (2018); Hill et al. (2016) train Learning sentence embeddings from dialogues sentence encoders in a self-supervised manner with has drawn increasing attention due to its low web pages and books. Conneau et al. (2017); Cer annotation cost and high domain adaptabilet al. (2018); Reimers and Gurevych (2019) proity. Conventional approaches employ the pose to learn sentence embeddings on the supersiamese-network for this task, which obtains vised datasets such as SNLI (Bowman et al., 2015) the sentence embeddings through modeling and MNLI (Williams et al., 2018). Although the the context-response semantic relevance by apsupervised-learning approaches achieve better perplying a feed-forward network on top of the formance, they suffer from high cost of annotation sentence encoders. However, as the semantic textual similarity is commonly measured in building the training dataset, which makes them through the element-wise distance metrics (e.g. hard to adapt to other domains or languages. cosine and L2 distance), such architecture Recently, learning sentence embeddings from yields a large gap between training and evaluatdialogues has begun to attract inc"
2021.emnlp-main.185,2021.acl-long.393,0,0.026849,"closer, resulting in a decrease in Spearman’s correlation. However, as all positive samples in the candidates have identical labels, such degradation may not be fully reflected through the ranking metric (e.g. MAP) or even be covered as the number of retrieved positive samples changes. Impact of negative samples. We vary the number of negative samples for each positive sample within {1, 4, 9, 19}. Table 4 shows the experimental results, from which we find that both metrics improve slightly when the number of negative samples increases. Considering the similar observation in (Gao et al., 2021; Yan et al., 2021), we conclude this phenomenon may be related to the discrete nature of language. Specifically, as the generation of the sentence embeddings in our approach is guided and constrained by the token-level interaction mechanism, our model is more robust than the other contrastive learning approaches and is even effective when only one negative sample is provided. quality of the dialogue-based sentence embeddings. Evaluation results show that DialogueCSE achieves the best result over the baselines while adding no additional parameters. In the next step, we will study how to introduce more interactio"
2021.emnlp-main.185,W18-3022,0,0.356865,"esponse learning methods promising to achieve competembedding (i.e. the context-free embedding) itive or even superior performance against the according to the guidance of the multi-turn supervised-learning methods, especially under the context-response matching matrices. Then it low-resource conditions. pairs each context-aware embedding with its corresponding context-free embedding and fiWhile promising, the issue of how to effectively nally minimizes the contrastive loss across exploit the dialogues for this task has not been sufall pairs. We evaluate our model on three ficiently explored. Yang et al. (2018) propose to multi-turn dialogue datasets: the Microsoft train an input-response prediction model on Reddit Dialogue Corpus, the Jing Dong Dialogue dataset (Al-Rfou et al., 2016). Since they build their Corpus, and the E-commerce Dialogue Corarchitecture based on the single-turn dialogue, the pus. Evaluation results show that our apmulti-turn dialogue history is not fully exploited. proach significantly outperforms the baselines across all three datasets in terms of MAP and Henderson et al. (2020) demonstrate that introducSpearman’s correlation measures, demonstrating the multi-turn dialogue co"
2021.emnlp-main.185,2020.emnlp-main.124,0,0.0212252,"nces of the context in the Dialogue Corpus (ECD) (Zhang et al., 2018). To corpus. Hill et al. (2016) propose to predict the evaluate our model, we introduce two types of neighboring sentences as bag-of-words instead of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) step-by-step decoding. Logeswaran and Lee (2018) perform sentence-level modeling by retrieving the task. Here we do not adopt the standard semantic ground-truth sentence from candidates under the textual similarity (STS) task (Cer et al., 2017) for two reasons: (1) As revealed in (Zhang et al., 2020), given context, achieving consistently better performance compared to the previous token-level modthe sentence embedding performance varies greatly eling approaches. The datasets used in these works as the domain of the training data changes. As a 1 dialogue dataset is always about several certain doAll the datasets will be publicly available at mains, evaluating on the STS benchmark may mis- https://github.com/wangruicn/DialogueCSE 2397 are typically built upon the corpus of web pages and books (Zhu et al., 2015). As the semantic connections are relatively weak in these corpora, the model pe"
2021.emnlp-main.185,C18-1317,0,0.0983426,"ches. • Extensive experiments show that DialogueCSE significantly outperforms the baselines, establishing the state-of-the-art results. 2 Related Work 2.1 Self-supervised Learning Approaches We train our model on three multi-turn dialogue datasets: the Microsoft Dialogue Corpus (MDC) Early works on sentence embeddings mainly focus on the self-supervised learning approaches. Kiros (Li et al., 2018), the Jing Dong Dialogue Corpus et al. (2015) train a seq2seq network by decod(JDDC) (Chen et al., 2020), and the E-commerce ing the token-level sequences of the context in the Dialogue Corpus (ECD) (Zhang et al., 2018). To corpus. Hill et al. (2016) propose to predict the evaluate our model, we introduce two types of neighboring sentences as bag-of-words instead of tasks: the semantic retrieval (SR) task and the dialogue-based semantic textual similarity (D-STS) step-by-step decoding. Logeswaran and Lee (2018) perform sentence-level modeling by retrieving the task. Here we do not adopt the standard semantic ground-truth sentence from candidates under the textual similarity (STS) task (Cer et al., 2017) for two reasons: (1) As revealed in (Zhang et al., 2020), given context, achieving consistently better per"
2021.emnlp-main.205,P19-1335,0,0.0609584,"Missing"
2021.emnlp-main.205,2020.findings-emnlp.228,0,0.0213909,"enting each entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the zero-shot scenario to place more emphasis on reading entity descriptions. BLINK (Wu et al., 2020) proposes a bi-encoder to encode the"
2021.emnlp-main.205,2021.naacl-main.86,0,0.0194072,"fer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the zero-shot scenario to place more emphasis on reading entity descriptions. BLINK (Wu et al., 2020) proposes a bi-encoder to encode the descriptions and enhance the bi-encoder by distilling the knowledge from the cross-encoder. Yao et al. (2020) repeats the position embedding to solve the long-range modeling problem in entity descriptions. Zhang and Stratos (2021) demonstrates that hard negatives can enhance the contrast when training an EL model. 5 Conclusion In this work, we propose a novel approach to construct multi-view representations from descriptions, which shows promising results on four EL datasets. Extensive results demonstrate the effectiveness of multi-view representations and the heuristic search strategy. In the future, we will explore more reliable and efficient approaches to construct views. Acknowledgement This work was supported by Alibaba Group through Alibaba Innovative Research Program and was also supported by the Key Research an"
2021.emnlp-main.205,2020.emnlp-main.519,0,0.403904,"probEntity retrieval, which aims at disambiguating abilities, e.g., p(entity|mention) (Le and Titov, mentions to canonical entities from massive 2018). Ganea and Hofmann (2017) and Yamada KBs, is essential for many tasks in natural lanet al. (2016) build entity embedding from the local guage processing. Recent progress in entity retrieval shows that the dual-encoder structure context of hyperlinks in entity pages or entity-entity is a powerful and efficient framework to nomico-occurrences. Those embedding-based methods nate candidates if entities are only identified by were extended by BLINK (Wu et al., 2020) and descriptions. However, they ignore the propDEER (Gillick et al., 2019) to two-tower dualerty that meanings of entity mentions diverge encoders (Khattab and Zaharia, 2020), which enin different contexts and are related to varicode mentions and descriptions of entities into highous portions of descriptions, which are treated dimensional vectors respectively. Candidates are equally in previous works. In this work, we retrieved by nearest neighbor search (Andoni and propose Multi-View Entity Representations (MuVER), a novel approach for entity retrieval Indyk, 2008; Johnson et al., 2019) for"
2021.emnlp-main.205,2020.emnlp-main.523,0,0.0338565,"entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the zero-shot scenario to place more emphasis on reading entity descriptions. BLINK (Wu et al., 2020) proposes a bi-encoder to encode the descriptions and enhance"
2021.emnlp-main.205,K16-1025,0,0.0210626,"2. BLINK performance on these datasets is reported in its official Github repository2 . We report the In-KB accuracy in Table 2 and observe that MuVER out-performs BLINK on all datasets except the recall@100 on WNED-WIKI. 4 Related Work Representing each entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual info"
2021.emnlp-main.205,Q17-1028,0,0.0220008,"t the In-KB accuracy in Table 2 and observe that MuVER out-performs BLINK on all datasets except the recall@100 on WNED-WIKI. 4 Related Work Representing each entity with a fixed-sized vector has been a common approach in Entity Linking. Ganea and Hofmann (2017) defines a word-entity conditional distribution and samples positive words 2 https://github.com/facebookresearch/ BLINK from it. The representations of those positive words aim to approximate the entity embeddings compared with random words. Yamada et al. (2016) models the relatedness between entities into entity representations. NTEE (Yamada et al., 2017) trains entity representations by predicting the relevant entities for a given context in DBPedia abstract corpus. Ling et al. (2020) and Yamada et al. (2020) pre-train variants of the transformer-based model by maximizing the consistency between the context of the mentions and the corresponding entities. Those entity representations suffer from a cold-start problem that they cannot link mentions to unseen entities. Another line of work is to generate entity representations using entity textual information, such as entity descriptions. Logeswaran et al. (2019) introduces an EL dataset in the z"
2021.emnlp-main.205,N18-1071,0,0.0548116,"Missing"
2021.emnlp-main.232,N19-1423,0,0.643806,"this paradigm. However, when transferred to downstream tasks, the pre-trained model is responsible for encoding the original sequence without noise, and is expected to obtain noise invariant representations. Such pretrain-finetune discrepancy not only impedes fast fine-tuning, but also may result in suboptimal sequence representations, thus affecting the performance in downstream tasks. Introduction To remedy this, we present ContrAstive PreTraining (CAPT) to learn noise invariant (or deRecently, pre-trained self-supervised models such noised) sequence representations. The core idea as BERT (Devlin et al., 2019) have attracted of CAPT is to enhance the consistency between an increasing amount of attention in natural lansemantic representations of the original sequence guage processing and vision-language processand that of corresponding corrupted version (e.g. ing. Benefiting from common knowledge conthe masked sequence) via unsupervised instancetained in massive unlabeled data (Liu et al., 2019), wise training signals. As shown in Figure 1, our the pretraining-finetuning framework has become approach strives to pull the representation of the ∗ Equal Contribution. corrupted sequence towards that of t"
2021.emnlp-main.232,N18-1202,0,0.055898,"Pre-trained Language Representations. This work includes VideoBERT (Sun et al., 2019), Vitask strives to build linguistic representations ben- sualBERT (Li et al., 2019b), UNITER (Chen et al., efiting various downstream tasks. One line of re- 2019), Unicoder-VL (Li et al., 2019a), etc. In consearch focuses on autoregressive (AR) pre-training, trast, the other line such as ViLBERT (Lu et al., while the other centers on denoising autoencoding 2019) and LXMERT (Tan and Bansal, 2019) fo(DAE). Representative work of AR pre-training cuses on the two-stream architecture. They first sepincludes ELMo (Peters et al., 2018) and GPT (Rad- arately encode visual and textual features and then ford, 2018), which aim to predict the next word interact with each other in the co-attention layers. based on previous tokens but lack the modeling As for pre-training tasks, different work exhibits of bidirectional context. The other research line commonalities, all focusing on MRM, MLM, and is built upon DAE, which strives to reconstruct several specific tasks (e.g. ITM). However, most the original sequence based on the corrupted input of these tasks are prone to learning noise covariby jointly attending to both the left and"
2021.emnlp-main.232,P19-1644,0,0.0146394,"and We perform evaluation on three benchmark tasks: textual features and then introduces a cross-modal VQA, GQA, and NLVR2 . VQA (Goyal et al., 2017) layer to integrate them. Same as Section 3.1, we construct the corrupted version x ˆ of the original in- aims to select the correct answer based on both the question and its paired image, while GQA (Hudput x by masking part of visual features or textual son and Manning, 2019) shares the same task setwords. In addition to the proposed CAPT which can learn sequence-level representations, follow- ting but require more reasoning. The goal of NLVR2 (Suhr et al., 2019) is to predict whether ing (Tan and Bansal, 2019), we also adopt three the statement correctly describes the two images. other pre-training tasks to learn more fine-grained word/region-level representations. These tasks in- All three tasks use accuracy (Acc) as the evaluation clude: masked language modeling (MLM) that pre- metric. dicts the masked words based on the corrupted For VQA and GQA, we add extra multi-layer input x ˆ, masked region modeling (MRM) that pre- perceptrons (MLP) that take the representation of dicts the masked visual region objects based on the [CLS] as the input to perfo"
2021.emnlp-main.232,D19-1514,0,0.206054,"tive sampling from a task-specific fine-tuning methods (e.g. formulat- memory queue, resulting in superior model perforing QNLI as a ranking task or using multi-task mance therein. More analysis about the influence fine-tuning), increasing the difficulty for a direct of memory queue can be found in Section 5.1. 2926 Model VQA NLVR2 GQA test-dev test-std test-dev test-std dev test-p SOTA (No pre-training) ViLBERT (Lu et al., 2019) VisualBERT (Li et al., 2019b) VL-BERT (Su et al., 2019) 70.63 70.55 70.80 71.79 70.90 70.92 71.00 72.22 55.8 – – – 56.1 – – – 54.80 – 67.40 – 53.50 – 67.00 – LXMERT (Tan and Bansal, 2019) CAPT (Ours) 72.42 72.78 72.54 73.03 59.95 60.48 60.33 60.93 74.82 75.12 74.41 75.13 Table 3: Comparison to the state-of-the-art systems with the single model on VQA, GQA and NLVR2 . The results of both VQA and GQA are reported on the “test-dev” split (used for validation on the official server) and the “test-std” split (used for maintaining the public leaderboard). The NLVR2 results are reported on the local dev set (“dev”) and the public test set (“test-p”). The results of baselines except LXMERT are obtained from prior work. 4 4.1 Experiments on Vision-language Tasks Implementation architec"
2021.emnlp-main.338,N19-1423,0,0.185995,"gnment put and output spaces over languages. For example, tools (Tiedemann et al., 2014; Zhang et al., 2019) the universal dependency project (McDonald et al., (thanks to the universal word forms). 2013) constructs a universal output space for cross• we don’t really need to perform the reordering lingual dependency parsers, and cross-lingual word action. Instead, the correct order can be implicrepresentation learning algorithms helps aligning itly encoded by multi-task learning: word order word forms of different languages (Conneau et al., information accesses the model as a supervision 2017; Devlin et al., 2019). signal. Beyond word form, word order is another imThe separation of reordering module and structured portant factor in cross-lingual structured prediction prediction module provides a new way to both ex∗ This work was conducted when Tao Ji was interning at plore and transfer order information. Alibaba DAMO Academy. 1 We suggest a distillation framework (Hinton https://github.com/AntNLP/zero-shot-structuredprediction. et al., 2015) for learning the reordering module. 4109 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4109–4120 c November 7–11, 2"
2021.emnlp-main.338,D11-1006,0,0.116698,"Missing"
2021.emnlp-main.338,P18-2077,0,0.0542468,"Missing"
2021.emnlp-main.338,P19-1311,0,0.0114392,"rom a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the in"
2021.emnlp-main.338,D17-1302,0,0.0291786,"c, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the"
2021.emnlp-main.338,P13-2017,0,0.0181681,"Missing"
2021.emnlp-main.338,D15-1039,0,0.0427418,"Missing"
2021.emnlp-main.338,N19-1385,0,0.0512246,"ur experiments, we will select one of the re- ing blocks are directly obtained from order objec−→ ←− ordering signals to train the model and compare it tive (e.g., M and M ). Since we jointly perform with each other. In addition, we use single-head reordering and structured prediction. The data for self-attention to reduce computation because pre- learning word order is constrained by the corpus liminary experiments show that multiple heads are size of structured prediction task (e.g., treebanks). not helpful for reordering blocks. Michel et al. On the other hand, there are massive unlabelled (2019) has also shown that replacing multi-head sentences which can help build a more powerful with single-head does not hurt performance. reordering module. To use those unlabelled data, We cross stack reordering blocks with original one challenge is that directly feeding them into the Transformer blocks to build the complete encoder joint learning model could be problematic since the (Figure 3). The reordering blocks estimate prob- severe imbalance between structured prediction sigability p(os |ωs ) by learning linear word order on nals and reordering signals would make the model the source langua"
2021.emnlp-main.338,P15-2040,0,0.0128549,"al performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the target language’s and Yi Gao for their comments on writing. This word order. Tiedemann and Agic (2016); Wang and Eisner (2016, 2018a) create a"
2021.emnlp-main.338,K17-3009,0,0.0255242,"and three structured prediction models on the train set of Universal Dependencies (UD) English-EWT treebank (v2.2) (Nivre et al., 2018). We use the development set and test set of the UD English-EWT treebank to validate source language performance. Following Ahmad et al. (2019a)’s setup, we take 30 other languages as target languages, and use the development set and test set of their treebanks to evaluate target languages performance. For the reordering model, a Base train set is UD English, and an Extra set is automatically annotated raw texts (Ginter et al., 2017) generated by UDPipe v2.0 (Straka and Straková, 2017) from CommonCrawl and Wikipedia. Each sentence is automatic tokenization and syntactic annotations (include UPOS). The hyperparameters we used in word reordering task and downstream tasks are summarized in Appendix B. The statistics of the UD treebanks are summarized in Appendix C. 6.1 Performances of the Reordering Model Our Models and Baselines We explore the input features’ influence, order representations , and unlabeled data size to the reordering model. For input features, we utilize MUSE, mBERT, and optional upos features. For order representations, we utilize −→ ←− an undirected (M ) o"
2021.emnlp-main.338,W14-1614,0,0.0271603,"Missing"
2021.emnlp-main.338,Q16-1035,0,0.0206985,"t al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpful comments and suggestions, thank Peng Li creasing the intersection with the target language’s and Yi Gao for their comments on writing. This word order. Tiedemann and Agic (2016); Wang and Eisner (2016, 2018a) create a high-quality syn- research is funded by the NSFC (62076097) and the 2020 East China Normal University Future Scithetic treebank to increase source data. But data entists and Outstanding Scholars Incubation Proaugmentation requires expert knowledge to build gramme (WLKXJ2020). The corresponding autreebank and extra train time. It does not apply to thors are Tao Ji, Yuanbin Wu and Xiaoling Wang. a larger number of target languages. Annotation projection relies on cross-language annotation mapping using parallel corpus and automatic alignment References (Rasooli and Collins, 201"
2021.emnlp-main.338,Q18-1046,0,0.0122369,"with a bag of words input. The reordering module is distilled from a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpfu"
2021.emnlp-main.338,D18-1163,0,0.0139835,"with a bag of words input. The reordering module is distilled from a task-generic, unsupervised, and large-scale pre-trained reordering teacher. Experiments show that, our model can significantly improve cross-lingual performances on three tasks without obviously hurting source language performance. Future work contains two parts: extending to multi-source transfer and extending to more structured prediction tasks such as NER which requires span-level reordering. There has been a lot of recent research on the cross-lingual transfer of structured prediction tasks, including dependency parsing (Wang and Eisner, 2018a; Ahmad et al., 2019b), and POS tagging (He et al., 2019; Kim et al., 2017). Early work built delexicalized models to direct transfer, but at the expense of performance (McDonald et al., 2011; Rosa and Žabokrtský, 2015). With the development of cross-lingual word embedding techniques (Conneau et al., 2017; Devlin et al., 2019), the recent work has utilized it to retain lexical information (Ahmad et al., 2019b; Wang et al., 2019). Acknowledgments Data augmentation can enrich the word order that appears on the source language, thereby in- The authors wish to thank the reviewers for their helpfu"
2021.emnlp-main.338,D19-1575,0,0.132284,"rders (e.g., SVO or SOV). To share annotations tured prediction. Current sentence encoders among them, we need to handle word order dis(e.g., RNN, Transformer with position emcrepancies carefully: if a model learned on the beddings) are usually word order sensitive. source language is tightly coupled with the source Even with uniform word form representations language word order, performances on target lan(MUSE, mBERT), word order discrepancies guages could be hurt as their word order could be may hurt the adaptation of models. This paper builds structured prediction models with incompatible (Wang et al., 2019). On the other side, bag-of-words inputs. It introduces a new reif one completely drops word order (e.g., bag-ofordering module to organize words following words), the source language (and target languages) the source language order, which learns taskperformances might be poor as order-sensitive feaspecific reordering strategies from a generaltures could be essential. Trade-offs have been made purpose order predictor model. Experiments by using weak word order information (e.g., relon zero-shot cross-lingual dependency parsing, ative positions instead of absolute positions (AhPOS tagging, and"
2021.emnlp-main.338,D19-1092,0,0.0263429,"Missing"
2021.emnlp-main.339,D16-1211,0,0.0132871,"so we can pack them into the batch dimension to obtain an O(1) training complexity. Hence, USE can uniformly extract full structure features efficiently. Comparing to Previous Encoders We divide previous work into three encoding methods: top-k, stack-LSTM, and binary vector. Top-k methods (Chen and Manning, 2014; Weiss et al., 2015) capture the conjunction of only few 1∼3 in-structure items. It extracts only partial structural information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t )."
2021.emnlp-main.339,K19-2007,0,0.0213531,"information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t ). Vector methods (Zhang et al., 2017) use two binary vectors to model whether each element is in a σ or 2 Note that we use action embedding matrix A instead of a β. It can efficiently encode some outside parts of X when encoding action list α. 3 stack and buffer but loss the information of inside Similar to Equation 2, we give the formulation for the other data structures in Appendix B. position. 4124 We compare existing"
2021.emnlp-main.339,D14-1082,0,0.0793963,"ng transition states with O(1) additional complexity (with respect to basic feature extractors). Experiments on the PTB and UD show that our proposed method significantly improves the test speed and achieves the best transition-based model, and is comparable to state-of-the-art methods. 1 1 Introduction configuration input classifier output buffer stack s-invariant subtree action s-dependent update action action Figure 1: An overview of our transition-based parser. efficiently. However, challenges appear when we try to have the cake and eat it. For example, traditional template-based methods (Chen and Manning, 2014) are fast, but only encode partial information of structures (e.g., few top items on stacks and buffers). Structure-based networks (e.g., StackRNN (Dyer et al., 2015)) rely on carefully designed network architecture to get a full encoding of structures (actually, they still miss some off-structure information, see our discussions in Section 4.2), but they are usually slow (e.g., not easy to batch). Furthermore, different structures have different ways of update (stacks are first-in-last-serve, buffers are first-in-first-serve), it also takes efforts to design different encoders and ways of fus"
2021.emnlp-main.339,P18-1130,0,0.197097,"ffer, and action list, which means that augmenting their information is helpful. Considering the performance gain and computational cost of adding heads, we finally use a total of 8 structural heads. The parser double attent to the stack, buffer and action list. Lexical Encoder UAS Dev LAS Glove + BiLSTM + Xformer 95.72 95.81 95.84 93.79 93.87 93.93 95.71 95.93 95.99 94.05 94.21 94.28 Bert + finetune M&H20 95.90 95.97 95.78 93.97 94.02 93.74 96.21 96.28 96.11 94.56 94.60 94.33 UAS Test LAS Table 2: Lexical encoder comparison on PTB. M&H20: Mohammadshahi and Henderson (2020). Speed Parser Type Ma et al. (2018) Dozat and Manning (2017) Ji et al. (2019) Zhang et al. (2020) T G G‡ G‡ 183 496 403 466 Our arc-hybrid parser T 918 Table 3: Parsing speed comparison on PTB test set. The ‡ indicates high-order graph-based parsers. independent Glove embeddings (Pennington et al., 2014) in the arc-hybrid system. We learn the context via BiLSTM or Transformer encoder. The results show that encoding context can further improve performance and the Transformer encoder is better than BiLSTM. The second part reports the use of contextual Bert networks (Devlin et al., 2019). The introduction of Bert networks and in p"
2021.emnlp-main.339,2020.findings-emnlp.294,0,0.272388,"d (UAS) In a fair comparison, our three unified structure and labeled attachment scores (LAS). For evaluencoding (USE) parsers all achieve significant imations on PTB, five punctuation symbols (“ ” : , provements on PTB. This demonstrates the benefit .) are excluded, while on UD, we use the official of complete structural information by our unified evaluation script. encoding. Hyper-parameters For structure-invariant part, Secondly, we compare with strong graph-based we directly adopt most parameter settings of Ji parsers. The second part of Table 1 contains two et al. (2019) and Zhang et al. (2020), including first-order parsers and two high-order parsers (in pretrained embeddings, BiLSTM, and CharCNN. the red cell). Our USE parsers beat the first-order For structure-dependent part, we use a total of 8 methods, but underperform the high-order methstructural heads, allocating two each for the stack, ods which capture high-order features by graph buffer and action list, one for the subtree’s edges neural networks and TreeCRF. However, speed exand one for the edges’ labels. Our pre-experiments periments show that USE is about 2 times faster show that stacking 6 layers of USE yields the bes"
2021.emnlp-main.339,W03-3017,0,0.0665993,"Missing"
2021.emnlp-main.339,W04-0308,0,0.0423912,"Missing"
2021.emnlp-main.339,J08-4003,0,0.0344169,"contain its lexical form and part-of-speech tag, while its structure-dependent view indicates that w is now sitting on the buffer and its distance to the buffer head is p. When w is detached from buffer and attached to the stack, its structure-dependent ∗ This work was conducted when Tao Ji was interning at view will switch to “sitting on the stack” while its Alibaba DAMO Academy. 1 https://github.com/AntNLP/trans-dep-parser. structure-invariant view stay unchanged. A unified 4121 Transition systems have been successfully applied in many fields of NLP, especially parsing (dependency parsing (Nivre, 2008), constituent parsing (Watanabe and Sumita, 2015), and semantic parsing (Yin and Neubig, 2018)). Basically, a transition system takes a series of actions which attach or detach some items (e.g., sentence words, intermediate outputs) to or from some structures (e.g., stacks, buffers, partial trees). Given a set of action series, a classifier is trained to predict the next action given a current configuration of structures in the transition system. The performances of the final system strongly depend on how well the classifier encodes those transition system configurations. Ideally, a good confi"
2021.emnlp-main.339,D19-1145,0,0.124754,"ing completeness and efficiency, we find that with structure indicators, it is relatively easy to encode a structure completely: one only needs to decompose the structure into identifiable subparts. In fact, we can use them to track some parts of structures which are not revealed in previous work (e.g., words have been popped out from stacks). It runs in the same manner as templated-based models, thus the decoding efficiency is guaranteed. We also note that using structure indicator is different from existing ways to include structure information into neural network models (Shaw et al., 2018; Wang et al., 2019; Shiv and Quirk, 2019): it encodes dynamical structures (changing with transition system running) rather than static structures (e.g., fixed parse trees). We can easily implement the unified structure encoding with existing multi-head attention networks (MHA, (Vaswani et al., 2017)). It is also easy to fuse encodings of different structures with multilayer MHA. We conduct experiments on the English Penn Treebank 3.0 and Universal Dependencies v2.2, show that the unified structure encoder is able to help us achieving state-of-the-art transitionbased parser (even competitive to the best graphba"
2021.emnlp-main.339,P15-1113,0,0.0211638,"of-speech tag, while its structure-dependent view indicates that w is now sitting on the buffer and its distance to the buffer head is p. When w is detached from buffer and attached to the stack, its structure-dependent ∗ This work was conducted when Tao Ji was interning at view will switch to “sitting on the stack” while its Alibaba DAMO Academy. 1 https://github.com/AntNLP/trans-dep-parser. structure-invariant view stay unchanged. A unified 4121 Transition systems have been successfully applied in many fields of NLP, especially parsing (dependency parsing (Nivre, 2008), constituent parsing (Watanabe and Sumita, 2015), and semantic parsing (Yin and Neubig, 2018)). Basically, a transition system takes a series of actions which attach or detach some items (e.g., sentence words, intermediate outputs) to or from some structures (e.g., stacks, buffers, partial trees). Given a set of action series, a classifier is trained to predict the next action given a current configuration of structures in the transition system. The performances of the final system strongly depend on how well the classifier encodes those transition system configurations. Ideally, a good configuration encoder should encode transition system"
2021.emnlp-main.339,P15-1032,0,0.0233944,"tructure). Vaswani et al. (2017) noted that a multi-head attention layer has a constant number (O(1)) of sequentially executed operations, which means that efficient GPU-based computing is possible. In training, the USE calculations at different moments are independent of each other, so we can pack them into the batch dimension to obtain an O(1) training complexity. Hence, USE can uniformly extract full structure features efficiently. Comparing to Previous Encoders We divide previous work into three encoding methods: top-k, stack-LSTM, and binary vector. Top-k methods (Chen and Manning, 2014; Weiss et al., 2015) capture the conjunction of only few 1∼3 in-structure items. It extracts only partial structural information. Since the feature template is fixed, it is easy (l−1) Since ct contains the complete structural infor- to batchify. Stack-LSTM methods (Dyer et al., 2015; Ballesteros et al., 2016) can efficiently repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree w"
2021.emnlp-main.339,D17-1175,0,0.01276,"repmation, the lth layer’s USE module can interact resent all in-structure items, via the PUSH(·) and with other structures and output a more informative (l) (l) (l) (l) representation o∗ = USE(q∗ , K∗ , V∗ ). Then, POP(·) functions. But it loses the information of outside parts and subtree which cannot be treated we obtain a high layer configuration representation as a stack. Besides, Che et al. (2019) point out that by combining these output vectors: its batch computation is very inefficient. Binary (l) (l) (l) (l) (l) (l) ct = MLP(oσ,t ⊕ oβ,t ⊕ oα,t ⊕ oTarc ,t ⊕ oTrel ,t ). Vector methods (Zhang et al., 2017) use two binary vectors to model whether each element is in a σ or 2 Note that we use action embedding matrix A instead of a β. It can efficiently encode some outside parts of X when encoding action list α. 3 stack and buffer but loss the information of inside Similar to Equation 2, we give the formulation for the other data structures in Appendix B. position. 4124 We compare existing work with our USE encoder in terms of the coverage of structure features and GPU computing friendly (in Figure 4). Overall, USE does not lose any structural information and more efficient than previous feature ex"
2021.emnlp-main.749,2020.emnlp-main.634,0,0.269945,"free or task-driven mask is performed on the gradients of the non-child network, resetting them to zero (grey diagonal grids). poor generalization ability in transferring to out-ofdomain data or other related tasks (Mahabadi et al., 2021; Aghajanyan et al., 2021). Preventing the fine-tuned models to deviate too much from the pretrained weights (i.e., with less 1 Introduction knowledge forgetting), is proved to be effective to mitigate the above challenges (Gouk et al., 2020). Pretrained Language Models (PLMs) have had a remarkable effect on the natural language process- For instance, RecAdam (Chen et al., 2020) introing (NLP) landscape recently (Devlin et al., 2019; duces L2 distance penalty between the fine-tuned weights and their pretrained weights. In addition, Liu et al., 2019; Clark et al., 2020). Pretraining and fine-tuning have become a new paradigm of NLP, Mixout (Lee et al., 2020) randomly replaces part of the model parameters with their pretrained weights dominating a large variety of tasks. during fine-tuning. The core idea behind them is Despite its great success, how to adapt such large-scale pretrained language models with mil- to utilize the pretrained weights to regularize the lions"
2021.emnlp-main.749,P07-1033,0,0.323932,"Missing"
2021.emnlp-main.749,N19-1423,0,0.452812,"of the non-child network, resetting them to zero (grey diagonal grids). poor generalization ability in transferring to out-ofdomain data or other related tasks (Mahabadi et al., 2021; Aghajanyan et al., 2021). Preventing the fine-tuned models to deviate too much from the pretrained weights (i.e., with less 1 Introduction knowledge forgetting), is proved to be effective to mitigate the above challenges (Gouk et al., 2020). Pretrained Language Models (PLMs) have had a remarkable effect on the natural language process- For instance, RecAdam (Chen et al., 2020) introing (NLP) landscape recently (Devlin et al., 2019; duces L2 distance penalty between the fine-tuned weights and their pretrained weights. In addition, Liu et al., 2019; Clark et al., 2020). Pretraining and fine-tuning have become a new paradigm of NLP, Mixout (Lee et al., 2020) randomly replaces part of the model parameters with their pretrained weights dominating a large variety of tasks. during fine-tuning. The core idea behind them is Despite its great success, how to adapt such large-scale pretrained language models with mil- to utilize the pretrained weights to regularize the lions to billions of parameters to various scenarios, fine-tu"
2021.emnlp-main.749,2021.acl-long.378,0,0.0719407,"Missing"
2021.emnlp-main.749,2020.acl-main.197,0,0.0430013,"ons. Effective and generalizable fine-tuning. With a mass of parameters, fine-tuning large PLMs tend to achieve degenerated performance due to overfitting and have poor generalization ability, especially on small datasets (Devlin et al., 2019; Phang et al., 2018; Lee et al., 2020). Therefore, different finetuning techniques have been proposed. Some of them utilize the pretrained weights to regularize the deviation of the fine-tuned model (Lee et al., 2020; Daumé III, 2007; Chen et al., 2020), while others compress the output information (Mahabadi et al., 2021) or injects noise into the input (Jiang et al., 2020; Aghajanyan et al., 2021). Moreover, Zhang et al. (2021) and Mosbach et al. (2021) point out that the omission of bias correction in the Adam optimizer used in Devlin et al. (2019) is also responsible for the degenerated results. Orthogonal to these methods, C HILD -T UNING address the problems by detecting the child network within the model in a task-free or task-driven way. It only updates parameters within the child network via a gradient mask, which is proved to be effective in adapting large PLMs to various tasks, along with better generalization ability. as possible parameters to mainta"
2021.emnlp-main.749,2021.acl-long.47,0,0.219485,"r generalization performance by large margins. Vanilla Backward ?! + Forward ?! CHILD-TUNING Backward ?"" ∆?! Task-Free or Task-Driven Gradients Mask Pretrained Weights = ∆?! ?"" Weights at 1-th Iteration Figure 1: The illustration of C HILD -T UNING. Left: It forwards on the whole network while backwarding on a subset of network (i.e., child network). Right: To achieve this, a task-free or task-driven mask is performed on the gradients of the non-child network, resetting them to zero (grey diagonal grids). poor generalization ability in transferring to out-ofdomain data or other related tasks (Mahabadi et al., 2021; Aghajanyan et al., 2021). Preventing the fine-tuned models to deviate too much from the pretrained weights (i.e., with less 1 Introduction knowledge forgetting), is proved to be effective to mitigate the above challenges (Gouk et al., 2020). Pretrained Language Models (PLMs) have had a remarkable effect on the natural language process- For instance, RecAdam (Chen et al., 2020) introing (NLP) landscape recently (Devlin et al., 2019; duces L2 distance penalty between the fine-tuned weights and their pretrained weights. In addition, Liu et al., 2019; Clark et al., 2020). Pretraining and fine-tu"
2021.emnlp-main.749,2020.acl-main.703,0,0.0198146,"so similar. The reason may be that both SST2 and CoLA belongs to a single sentence classification task, while others are in a different format of sentence-pair classification tasks. 5 Related Work Explosion of PLMs. There has been an explosion of studies on Pretrained Language Models (PLMs). Devlin et al. (2019) propose BERT that is pretrained on large quantities of unannotated corpus with self-supervised tasks. Many PLMs also emerged such as GPT-2 (Radford et al., 2018), GPT3 (Brown et al., 2020), ELECTRA (Clark et al., 2020), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019), and BART (Lewis et al., 2020). The number of parameters of PLMs also explodes. BERTLARGE has 340 millions of parameters, and the number for GPT-3 is even up to 175 billions. Effective and generalizable fine-tuning. With a mass of parameters, fine-tuning large PLMs tend to achieve degenerated performance due to overfitting and have poor generalization ability, especially on small datasets (Devlin et al., 2019; Phang et al., 2018; Lee et al., 2020). Therefore, different finetuning techniques have been proposed. Some of them utilize the pretrained weights to regularize the deviation of the fine-tuned model (Lee et al., 2020;"
2021.findings-acl.138,2021.ccl-1.108,0,0.0263718,"Missing"
2021.findings-acl.138,W17-4902,0,0.0258789,"(b) An example of word alignments between the source and target sentences. Arrows connect aligned words (identical or relevant), and blue words are not aligned. (c) NAST’s generation process. Step 1: generate the index of aligned words. [Mask] is a placeholder for unaligned words. Step 2: generate the transferred sentence non-autoregressively. Text style transfer aims at changing the text style while preserving the style-irrelevant contents, which has a wide range of applications, e.g., sentiment transfer (Shen et al., 2017), text formalization (Rao and Tetreault, 2018), and author imitation (Jhamtani et al., 2017). Due to the lack of parallel training data, most works focus on unsupervised text style transfer using non-parallel stylistic data. The cycle consistency loss (Zhu et al., 2017), a.k.a. the back-translation loss (Lample et al., 2018, author: Minlie Huang. Not great, but good atmosphere and great service (b) Observation of Word Alignment Source: Not terrible , but Introduction * Corresponding Not terrible, but not very good 2019), has been widely adopted by unsupervised text style transfer models (Dai et al., 2019; He et al., 2020; Yi et al., 2020). Specifically, the cycle loss minimizes the r"
2021.findings-acl.138,P19-1041,0,0.370615,"based models. To the best of our knowledge, we are the first to introduce a non-autoregressive generator to an unsupervised generation task. • Experiments show that incorporating NAST in cycle-loss-based models significantly improves the overall performance and the speed of training and inference. In further analysis, we find that NAST provides better optimization of the cycle loss and learns explainable word alignments. 2 Related Work Unsupervised Text Style Transfer We categorize style transfer models into three types. The first type (Shen et al., 2017; Zhao et al., 2018; Yang et al., 2018; John et al., 2019) disentangles the style and content representations, and then combines the content representations with the target style to generate the transferred sentence. However, the disentangled representations are limited in capacity and thus hardly scalable for long sentences (Dai et al., 2019). The second type is the editing-based method (Li et al., 2018; Wu et al., 2019a,b), which edits the source sentence with several discrete operations. The operations are usually trained separately and then constitute a pipeline. These methods are highly explainable, but they usually need to locate and replace th"
2021.findings-acl.138,2020.acl-main.639,0,0.0266396,"ation seems similar to a pipeline, NAST is trained in an end-to-end fashion with the cycle loss. All transferred words in NAST are generated, not copied, which is essentially different from these methods. The third type is based on the cycle loss. Zhang et al. (2018); Lample et al. (2019) introduce the back translation method into style transfer, where the model is directly trained with the cycle loss after a proper initialization. The following works (Dai et al., 2019; Luo et al., 2019; He et al., 2020; Yi et al., 2020) further adopt a style loss to improve the style control. A recent study (Zhou et al., 2020) explores the word-level information for style transfer, which is related to our motivation. However, they focus on word-level style relevance in designing novel objectives, while we focus on modeling word alignments and the non-autoregressive architecture. Non-Autoregressive Generation Non-AutoRegressive (NAR) generation is first introduced in machine translation for parallel decoding with low latency (Gu et al., 2018). The NAR generator assumes that each token is generated independently of each other conditioned on the input sentence, which sacrifices the generation quality in exchange for t"
2021.naacl-main.144,P17-4017,0,0.0467976,"Missing"
2021.naacl-main.144,N19-1423,0,0.126883,"ng et al. (2019a) extract the semanthat our proposed unified model achieves superior tic representations from a pre-trained SRL model performance compared with previously proposed BMESO-based works. Our contributions are: (i) and feed them into the opinion mining model, achieving substantial improvements. Zhang et al. we propose a unified span-based model for opinion (2020) incorporate the powerful contextual repremining in the end-to-end fashion that also supports sentations of bi-directional encoder representations the given-expression setting, (ii) we successfully from Transformers (BERT) (Devlin et al., 2019) integrate syntactic constituents knowledge into our and external dependency syntactic knowledge. model with MTL and GCN, achieving promising improvements, (iii) detailed analyses demonstrate To solve or alleviate the weaknesses of the prethe effectiveness of our unified model and the use- viously proposed BMESO-based models, we profulness of integrating constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classifica"
2021.naacl-main.144,Q19-1019,0,0.0464782,"Missing"
2021.naacl-main.144,P18-2058,0,0.0219271,"edly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion roles , and R is the set of opinion relations (holder"
2021.naacl-main.144,P14-1062,0,0.0186777,"char representation, and contextual word representation to compose the model input, denoted as: xi = embword ⊕ repchar ⊕ repcontext , (1) wi wi wi |s Encoder Layer. + M LPbexp (hb ) + M LPeexp (he ), srol = M LP rol (spanrol b,e ) (5) + M LPbrol (hb ) + M LPerol (he ). We can observe that for a sentence with n words, the numbers of candidate spans for expressions and roles are both n∗(n+1) , while the number of 2 gold expressions and roles are much fewer. To alleviate the unbalanced number of gold samples where ⊕ means the concatenate operation. We use the convolutional neural networks (CNN) (Kalchbrenner et al., 2014) to generate the character representations over the characters of words. 1797 1 We omit the process of span boundary module in Figure 2 for clarity. O Classification Layer MLP Holder OM Target MLP Encoder MLP Representation Layer OM Constituent MTL Input seriously needs equipment for detecting drugs GCN Constituent Encoder Input Layer GCN Input OM Encoder Layer Encoder Input MTL+GCN GCN Figure 2: The model architecture of our unified span-based opinion mining model (left) and syntactic constituent integration methods (right). and negative samples, we adapt the focal loss that is widely used in"
2021.naacl-main.144,P16-1087,0,0.342622,"rporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinio"
2021.naacl-main.144,D14-1162,0,0.0862699,"62.04 53.27 57.76 Proportional F1 Holder Target Overall 46.62 34.29 55.62 41.65 48.90 61.20 49.88 55.68 Table 1: Experimental results of our span-based opinion mining model and comparison with previous works on the MPQA2.0 dataset in the end-to-end setting. “-” means results are not reported in their paper. Exact P R F1 Zhang et al. (2019b) 60.21 48.52 53.04 S PAN OM 64.85 52.60 58.06 S PAN OM+BERT 67.15 60.63 63.71 Models Table 2: Results and comparison of the expression prediction on the exact metric in the end-to-end setting. 5.2 Hyper-parameters. We employ the 300-dimension GloVe vector (Pennington et al., 2014) as our pre-trained word embeddings. The character embeddings are randomly initialized and a CNN with kernel sizes of 3, 4, 5 is used to capture the character representations. For the contextual representations, we extract the representations from the base BERT by making a weighted summation over the last four layer outputs. The hidden size of the BiLSTM layer is set to 300 and we employ 2-layer BiLSTMs to encode the input representations. The dimension of opinion expression and role representations is 300 and the hidden size of expression, role, and relation classifiers is 150. We use 3-layer"
2021.naacl-main.144,P13-1161,0,0.210861,"thod. In addition, incorporating the syntactic constituents achieves promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap"
2021.naacl-main.144,Q14-1039,0,0.0244727,"n Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1795–1804 June 6–11, 2021. ©2021 Association for Computational Linguistics belongs to an expression, 0 otherwise), thus one sample is expanded n times if one sentence has n expressions, which is inefficient (Marasovi´c and Frank, 2018; Zhang et al., 2020). 2) The BMESObased method is weak to capture long-range dependencies and prefers to predict shorter opinion role spans (Zhang et al., 2020). 2 Related Work There are several task settings for opinion mining in the community: 1) Breck et al. (2007); Yang and Cardie (2014) focus on labeling the expressions. 2) Katiyar and Cardie (2016); Zhang et al. (2019b); Quan et al. (2019) discover the opinion structures in the end-to-end setting, i.e, based on the systemMotivated by the span-based representations of atic expressions. 3) Marasovi´c and Frank (2018); opinion expressions and roles, we propose a unified Zhang et al. (2019a, 2020) identify the opinion span-based opinion mining model (S PAN OM) that roles based on the given expressions. Our work can solve or alleviate the aforementioned weak- follows the end-to-end setting and also supports nesses. First, we tre"
2021.naacl-main.144,P18-1249,0,0.388081,"ing constituent syntactic knowl- pose a new method to unifiedly model the opinedge on the long-distance opinion roles. ion expressions and roles, which treats the expres1796 sion identification, role identification, and opinion relation classification as an MTL problem. Besides, to boost the opinion mining performance and motivated by the span-based task formalism, we explore to incorporate syntactic constituents into our model. Utilizing span-based representations have been investigated for many other NLP tasks, such as named entity recognition (NER) (Tan et al., 2020), constituency parsing (Kitaev and Klein, 2018), and semantic role labeling (SRL) (He et al., 2018). Generally, NER is a single span classification problem, constituency parsing is a span-based structure prediction problem, and SRL is a word-span classification problem. Different from them, in our methodology, OM is a span-span classification problem. 3 The S PAN OM Model 3.1 Task Definition. Given an input sentence s = w1 , w2 , ..., wn , our model aims to predict the gold-standard opinion structures Y ⊆ E × O × R, where E = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of expressions, O = {wi , ..., wj |1 ≤ i ≤ j ≤ n} is the set of opinion ro"
2021.naacl-main.144,2020.acl-main.297,1,0.617524,"promising improvements over the strong baseline enhanced by contextualized word representations. Holder Target John is happy because he loves being Enderly Park . Holder Target Figure 1: An example of OM, where the blue, yellow, and green blocks denote the opinion expressions, holders, and targets, respectively. MPQA (Wiebe et al., 2005) uses span-based annotations to represent opinion expressions and roles. Figure 1 gives an example of its opinion structures with two opinion expressions and related roles. Previous OM works (Yang and Cardie, 2013; Katiyar and Cardie, 2016; Quan et al., 2019; Zhang et al., 2020) mainly treat it as a BMESO-style tagging problem, which converts opinion expressions and opinion roles (holder/target) into BMESObased labels and uses a linking module to connect the predicted expressions and roles. The B, M, and E represent the beginning, middle, and ending word of a role, S denotes a single-word role, and O denotes other words. However, this kind of method is not perfect for the end-to-end OM setting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpi"
2021.naacl-main.144,N19-1066,0,0.12942,"etting, because one word can only belong to one opinion role (one word has only one label), while there exist overlap1 Introduction ping opinion structures between different expresOpinion mining (OM), which aims to find the opin- sions in one sentence. Figure 1 gives an example, ion structures of “Who expressed what opinions in which some overlapped opinion relations have towards what.” in one sentence, has achieved much been discarded by previous works (Katiyar and attention in recent years (Katiyar and Cardie, 2016; Cardie, 2016), such as [happy, he loves being EnMarasovi´c and Frank, 2018; Zhang et al., 2019b, derly Park, Target] and [loves, he, Holder]. There 2020). The opinion analysis has many NLP appli- are also other works which focus only on predicting cations, such as social media monitoring (Bollen opinions roles based on the gold-standard expreset al., 2011) and e-commerce applications (Cui sions, which also follow the BMESO-based method et al., 2017). The commonly used benchmark (Marasovi´c and Frank, 2018; Zhang et al., 2020). However, they also suffer from some weaknesses: ∗ Rui Wang’s contributions were carried out while at 1) the expressions are usually fed into the model inAlibaba"
2021.naacl-main.144,N18-1054,0,0.041995,"Missing"
2021.naacl-main.144,J93-2004,0,0.0741147,"F1 score of 67.66. Finally, we try to combine the two kinds of methods and the results are shown in the last major row. It is clear that combining the MTL method with OntoNotes and the GCN method with ParserPTB achieves better results than the reversed one. Therefore, our constituent-enhanced opinion mining model follows this combination. Besides, we can also see the relative lower results of “OntoNotes+PTB” in “+MTL” and “+GCN” settings, which is strange Which source of constituent knowledge is better? There are two main constituent syntax corpus in the community, i.e., Penn Treebank (PTB) (Marcus et al., 1993) and OntoNotes5.0 (Weischedel et al., 2013). The PTB corpus contains about 39k training data and mainly focuses on news data, while the OntoNotes5.0 corpus contains about 75k training data and focuses on multi-domain data (news, web, telephone conversation, and etc.). It is a worthy question to explore which is better for our span-based OM model, or what kind of combination is better. We compare them with various combinations on the BERT-based model, whose results are shown in Table 5. First, the sec7 We use the code of Kitaev and Klein (2018) to train the OntoNotes conond major row shows the"
2021.naacl-main.144,D18-1244,0,0.0229945,"information to expressions and roles. 4.2 The GCN Method. The MTL method enhances our OM model from the aspect of model representative ability by jointly modeling opinion mining and partial constituency parsing. We argue that modeling the syntactic constituent structure is also beneficial for OM because it provides valuable syntactic information for a sentence. Therefore, we try to employ the recently popular GCN (Kipf and Welling, 2016) to encode the constituent structure. However, the conventional GCN is not suitable for constituency trees, because it usually works on the dependency trees (Zhang et al., 2018, 2020) where the nodes are the surface words in a sentence. While, in constituent trees, there exists a certain number of non-terminal nodes3 , such as “NP”, “VP”, “SBAR” and so on. So it is hard to directly apply conventional GCN on the constituent trees. In the following, we first introduce the definition and workflow of typical GCN and then describe our modification. Formally, we denote an undirected graph as G = (V, E), where V and E are the set of nodes and edges, respectively. The GCN computation flow of node v ∈ V at l-th layer is defined as: ! X l hlv = ρ Wl hl−1 (11) u +b , u∈N (v) 3"
C10-1056,P05-1033,0,0.076389,"ned with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to discriminatively rescore the baseline MT system’s phrase library: boost good phrase translations while prune bad ones. This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin. 1 Introduction Statistical Machine Translation (SMT) systems, including phrase-based (Och and Ney 2002; Koehn et. al. 2003), syntax-based (Yamada and Knight 2001; Galley et. al. 2004) or hybrid systems (Chiang 2005; Zollmann and Venugopal 2006), are typically built with bilingual phrase pairs, which are extracted from parallel sentences with word alignment. Due to the noises in the bilingual sentence pairs and errors from automatic word alignment, the extracted phrase pairs may contain errors, such as • dropping content words (the $num countries ,||个:&lt;null>), • length mismatch (along the lines of the ||的:of) • content irrelevance (the next $num years, || 水平:level 方面:aspect 所:&lt;null>) These incorrect phrase pairs compete with correct phrase pairs during the decoding process, and are often selected when th"
C10-1056,P08-1010,0,0.113927,"hes have been proposed over the past decade for the purpose of improving the phrase pair quality for SMT. For example, a term weight based model was presented in (Zhao, et al., 2004) to rescore phrase translation pairs. It models the translation probability with similarities between the query (source phrase) and document (target phrase). Significant improvement was obtained in the translation performance. In (Johnson, et al., 2007; Yang and Zheng, 2009), a statistical significance test was used to heavily prune the phrase table and thus achieved higher precision and better MT performance. In (Deng, et al., 2008), a generic phrase training algorithm was proposed with the focus on phrase extraction. Multiple feature functions are utilized based on information metrics or word alignment. The feature parameters are optimized to directly maximize the end-to-end system performance. Significant improvement was reported for a small MT task. But when the phrase table is large, such as in a large-scale SMT system, the computational cost of tuning with this approach will be high due to many iterations of phrase extraction and re-decoding. In this paper we attempt to improve the quality of the phrase table using"
C10-1056,N04-1035,0,0.124544,"Missing"
C10-1056,N03-1017,0,0.0944154,"Missing"
C10-1056,J03-1002,0,0.00759879,"Missing"
C10-1056,P02-1040,0,0.0786456,"Missing"
C10-1056,W06-3602,0,0.0457746,"Missing"
C10-1056,P01-1067,0,0.0275997,"nd a target phrase pair are introduced. These features are combined with linear regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to discriminatively rescore the baseline MT system’s phrase library: boost good phrase translations while prune bad ones. This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin. 1 Introduction Statistical Machine Translation (SMT) systems, including phrase-based (Och and Ney 2002; Koehn et. al. 2003), syntax-based (Yamada and Knight 2001; Galley et. al. 2004) or hybrid systems (Chiang 2005; Zollmann and Venugopal 2006), are typically built with bilingual phrase pairs, which are extracted from parallel sentences with word alignment. Due to the noises in the bilingual sentence pairs and errors from automatic word alignment, the extracted phrase pairs may contain errors, such as • dropping content words (the $num countries ,||个:&lt;null>), • length mismatch (along the lines of the ||的:of) • content irrelevance (the next $num years, || 水平:level 方面:aspect 所:&lt;null>) These incorrect phrase pairs compete with correct phrase pairs during"
C10-1056,P09-2060,0,0.317812,"e pair will be selected for the final translation). As a result, the translation quality is degraded when these incorrect phrase pairs are selected. Various approaches have been proposed over the past decade for the purpose of improving the phrase pair quality for SMT. For example, a term weight based model was presented in (Zhao, et al., 2004) to rescore phrase translation pairs. It models the translation probability with similarities between the query (source phrase) and document (target phrase). Significant improvement was obtained in the translation performance. In (Johnson, et al., 2007; Yang and Zheng, 2009), a statistical significance test was used to heavily prune the phrase table and thus achieved higher precision and better MT performance. In (Deng, et al., 2008), a generic phrase training algorithm was proposed with the focus on phrase extraction. Multiple feature functions are utilized based on information metrics or word alignment. The feature parameters are optimized to directly maximize the end-to-end system performance. Significant improvement was reported for a small MT task. But when the phrase table is large, such as in a large-scale SMT system, the computational cost of tuning with"
C10-1056,2004.tmi-1.9,0,0.0808571,"Missing"
C10-1056,W04-3227,0,0.250511,"contain systematic alignment errors) or certain model costs are low (for example, when some source content words are translated into target function words in an incorrect phrase pair, the language model cost of the incorrect pair may be small, making it more likely that the pair will be selected for the final translation). As a result, the translation quality is degraded when these incorrect phrase pairs are selected. Various approaches have been proposed over the past decade for the purpose of improving the phrase pair quality for SMT. For example, a term weight based model was presented in (Zhao, et al., 2004) to rescore phrase translation pairs. It models the translation probability with similarities between the query (source phrase) and document (target phrase). Significant improvement was obtained in the translation performance. In (Johnson, et al., 2007; Yang and Zheng, 2009), a statistical significance test was used to heavily prune the phrase table and thus achieved higher precision and better MT performance. In (Deng, et al., 2008), a generic phrase training algorithm was proposed with the focus on phrase extraction. Multiple feature functions are utilized based on information metrics or wor"
C10-1056,W06-3119,0,0.0289699,"ar regression model and neural network to predict the quality score of the phrase translation pair. These phrase scores are used to discriminatively rescore the baseline MT system’s phrase library: boost good phrase translations while prune bad ones. This approach not only significantly improves machine translation quality, but also reduces the model size by a considerable margin. 1 Introduction Statistical Machine Translation (SMT) systems, including phrase-based (Och and Ney 2002; Koehn et. al. 2003), syntax-based (Yamada and Knight 2001; Galley et. al. 2004) or hybrid systems (Chiang 2005; Zollmann and Venugopal 2006), are typically built with bilingual phrase pairs, which are extracted from parallel sentences with word alignment. Due to the noises in the bilingual sentence pairs and errors from automatic word alignment, the extracted phrase pairs may contain errors, such as • dropping content words (the $num countries ,||个:&lt;null>), • length mismatch (along the lines of the ||的:of) • content irrelevance (the next $num years, || 水平:level 方面:aspect 所:&lt;null>) These incorrect phrase pairs compete with correct phrase pairs during the decoding process, and are often selected when their counts are high (if they c"
C10-1056,D07-1103,0,\N,Missing
C10-1056,J93-2003,0,\N,Missing
C10-1056,P03-1021,0,\N,Missing
C16-1054,C12-1029,0,0.0193289,"d rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and"
C16-1054,W08-1105,0,0.0909924,"Missing"
C16-1054,C12-1056,0,0.0179621,"t al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compress"
C16-1054,D14-1076,1,0.850198,"n summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user"
C16-1054,N15-1145,1,0.827258,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N15-1079,1,0.837783,"related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post"
C16-1054,N10-1134,0,0.0299569,"s kind. 2 Related Work Our work is closely related to the following aspects: ILP based summarization method, dependency tree based sentence compression by considering extra information, and mining social media for document summarization. Recently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summa"
C16-1054,W04-1013,0,0.0084025,"Missing"
C16-1054,W09-1801,0,0.0273222,"mance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used"
C16-1054,D07-1047,0,0.024065,"Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articles and compress them to form a summary, and the other is that we directly use sentences from the posts as the summary. 3 Corpus Construction For our work, we manually collected popular"
C16-1054,P13-1136,0,0.0210716,"arization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of sentence compression for automatic document summarization (Knight and Marcu, 2000; Zajic et al., 2007; Chali and Hasan, 2012; Wang et al., 2013). The compressed summaries can be generated through a pipeline approach that combines a generic sentence compression model with a summary sentence pre-selection or post-selection step. In addition, joint summarization and sentence compression method attracts lots of attention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for j"
C16-1054,C14-1083,1,0.847348,"mmarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines to help document summarization. Tsukamoto et al. (2015) proposed a method for efficiently collecting posts that are only implicitly related to an announcement post, taking into account retweets on Twitter in particular. Our work involves the two aspects when using post information: one is that we utilize post information to help choose sentences from new articl"
C16-1054,C08-1124,0,0.0362832,"ttention these years. (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011; Li et al., 2014) are typical work in this area. Their focus is to leverage the ILP technique to jointly select and compress sentences for multi-document summarization. In our work, we consider posts as summary related information and then use them for joint sentence compression and summarization. Although there is little work about generating summaries by considering extra information on Facebook data, there is some similar work done on Twitter or other resources. Unsupervised method was tried for summarization by (Wong et al., 2008). (Phelan et al., 2011) used tweets to recommend news articles based on user preferences. (Gao et al., 2012) produced cross-media news summaries by capturing the ˇ complementary information from both sides. Kothari et al. (2013) and Stajner et al. (2013) investigated detecting news comments from Twitter for extending news information provided. Wei and Gao (2014) derived external features based on a collection of relevant tweets to assist the ranking of the original sentences for highlight generation. In addition to tweets, Svore et al. (2007) leveraged Wikipedia and query log of search engines"
C16-1054,D12-1022,0,0.0161237,"ently optimization methods have been widely used in extractive summarization. McDonald (2007) first introduced sentence level ILP for summarization. Later Gillick et al. (2009) revised it to conceptbased ILP, which is similar to the Budgeted Maximal Coverage problem in (Khuller et al., 1999). Then other optimization methods have been used in summarization (Lin and Bilmes, 2010; Davis et al., 2012; Li et al., 2015b; Li et al., 2015a). In the concept-based ILP summarization methods, how to determine the concepts and measure their weights are the two key factors impacting the system performance. Woodsend and Lapata (2012) utilized ILP to jointly optimize different aspects including content selection, surface realization, and rewrite rules in summarization. Galanis et al. (2012) used ILP to jointly maximize the importance of the sentences and their diversity in the summary. In this work, we leverage the unsupervised ILP framework from Gillick et al. (2009) as our summarization system and incorporate post information to help boost summarization performance. Sentence compression techniques are widely used in summarization in order to generate abstractive summaries. Previous research has shown the effectiveness of"
C16-1054,P11-1049,0,\N,Missing
D07-1029,P06-1067,1,0.268308,"tems. The rest of the paper is organized as follows: In section 2, we briefly introduce several baseline MT systems whose outputs are used in the system combination. In section 3, we present the proposed hierarchical system combination framework. We will describe word and phrase combination and pruning, decoding path imitation and sentence translation selection. We show our experimental results in section 4 and conclusions in section 5. 2 Baseline MT System Overview In our experiments, we take the translation outputs from multiple MT systems. These include phrase-based statistical MT systems (Al-Onaizan and Papineni, 2006) (Block) and (Hewavitharana et al., 2005) (CMU SMT) , a direct translation model (DTM) system (Ittycheriah and Roukos, 2007) and a hierarchical phrased-based MT system (Hiero) (Chiang, 2005). Different translation frameworks are adopted by different decoders: the DTM decoder combines different features (source words, morphemes and POS tags, target words and POS tags) in a maximum entropy framework. These features are integrated with a phrase translation table for flexible distortion model and word selection. The CMU SMT decoder extracts testset-specific bilingual phrases on the fly with PESA a"
D07-1029,P05-1033,0,0.0350014,"rarchical system combination framework. We will describe word and phrase combination and pruning, decoding path imitation and sentence translation selection. We show our experimental results in section 4 and conclusions in section 5. 2 Baseline MT System Overview In our experiments, we take the translation outputs from multiple MT systems. These include phrase-based statistical MT systems (Al-Onaizan and Papineni, 2006) (Block) and (Hewavitharana et al., 2005) (CMU SMT) , a direct translation model (DTM) system (Ittycheriah and Roukos, 2007) and a hierarchical phrased-based MT system (Hiero) (Chiang, 2005). Different translation frameworks are adopted by different decoders: the DTM decoder combines different features (source words, morphemes and POS tags, target words and POS tags) in a maximum entropy framework. These features are integrated with a phrase translation table for flexible distortion model and word selection. The CMU SMT decoder extracts testset-specific bilingual phrases on the fly with PESA algorithm. The Hiero system extracts context-free grammar rules for long range constituent reordering. We select the IBM block decoder to re-translate the test set for glass-box system combin"
D07-1029,P05-3026,0,0.129996,"idence rescaling schemes ((Tidhar and Kuss1 http://www.darpa.mil/ipto/programs/gale/index.htm 277 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 277–286, Prague, June 2007. 2007 Association for Computational Linguistics ner, 2000) and (Nomoto, 2004)). (Mellebeek et al., 2006) decomposes source sentences into meaningful constituents, translates them with component MT systems, then selects the best segment translation and combine them based on majority voting, language models and confidence scores. (Jayaraman and Lavie, 2005) proposed another black-box system combination strategy. Given single top-one translation outputs from multiple MT systems, their approach reconstructs a phrase lattice by aligning words from different MT hypotheses. The alignment is based on the surface form of individual words, their stems (after morphology analysis) and part-of-speech (POS) tags. Aligned words are connected via edges. The algorithm finds the best alignment that minimizes the number of crossing edges. Finally the system generates a new translation by searching the lattice based on alignment information, each system’s confide"
D07-1029,E06-1005,0,0.38634,"nation strategy. Given single top-one translation outputs from multiple MT systems, their approach reconstructs a phrase lattice by aligning words from different MT hypotheses. The alignment is based on the surface form of individual words, their stems (after morphology analysis) and part-of-speech (POS) tags. Aligned words are connected via edges. The algorithm finds the best alignment that minimizes the number of crossing edges. Finally the system generates a new translation by searching the lattice based on alignment information, each system’s confidence scores and a language model score. (Matusov et al., 2006) and (Rosti et al., 2007) constructed a confusion network from multiple MT hypotheses, and a consensus translation is selected by redecoding the lattice with arc costs and confidence scores. In this paper, we introduce our hierarchical system combination strategy. This approach allows combination on word, phrase and sentence levels. Similar to glass-box combination, each MT system provides detailed information about the translation process, such as which source word(s) generates which target word(s) in what order. Such information can be combined with existing word and phrase translation table"
D07-1029,2006.amta-papers.13,0,0.111486,"Missing"
D07-1029,H94-1026,0,0.0449994,"ding strategies of multiple systems as well as their outputs and produces translations better than any single system output. More recently, within the GALE1 project, multiple MT systems have been developed in each consortium, thus system combination becomes more important. Traditionally, system combination has been conducted in two ways: glass-box combination and black-box combination. In the glass-box combination, each MT system provides detailed decoding information, such as word and phrase translation pairs and decoding lattices. For example, in the multi-engine machine translation system (Nirenburg and Frederking, 1994), target language phrases from each system and their corresponding source phrases are recorded in a chart structure, together with their confidence scores. A chart-walk algorithm is used to select the best translation from the chart. To combine words and phrases from multiple systems, it is preferable that all the systems adopt similar preprocessing strategies. In the black-box combination, individual MT systems only output their top-N translation hypotheses without decoding details. This is particularly appealing when combining the translation outputs from COTS MT systems. The final translati"
D07-1029,P04-1063,0,0.139284,"ividual MT systems only output their top-N translation hypotheses without decoding details. This is particularly appealing when combining the translation outputs from COTS MT systems. The final translation may be selected by voted language models and appropriate confidence rescaling schemes ((Tidhar and Kuss1 http://www.darpa.mil/ipto/programs/gale/index.htm 277 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational c Natural Language Learning, pp. 277–286, Prague, June 2007. 2007 Association for Computational Linguistics ner, 2000) and (Nomoto, 2004)). (Mellebeek et al., 2006) decomposes source sentences into meaningful constituents, translates them with component MT systems, then selects the best segment translation and combine them based on majority voting, language models and confidence scores. (Jayaraman and Lavie, 2005) proposed another black-box system combination strategy. Given single top-one translation outputs from multiple MT systems, their approach reconstructs a phrase lattice by aligning words from different MT hypotheses. The alignment is based on the surface form of individual words, their stems (after morphology analysis)"
D07-1029,P03-1021,0,0.0826628,"ere t(fj |ei ) is the word translation probabilities, estimated based on word alignment frequencies over all the training data. i and j are word positions in target and source phrases. − log p(e|f ), a source-to-target word translation cost, calculated similar to − log p(f |e); S(e, f ), a phrase translation cost estimated according to their relative alignment frequency in the bilingual training data, S(e, f ) = − log P (e|f ) = − log C(f, e) . C(f ) (3) λ’s in Equation 1 are the weights of different feature functions, learned to maximize development set BLEU scores using a method similar to (Och, 2003). The SMT system is trained with testset-specific training data. This is not cheating. Given a test set, from a large bilingual corpora we select parallel sentence pairs covering n-grams from source sentences. Phrase translation pairs are extracted from the subsampled alignments. This not only reduces the size of the phrase table, but also improves topic relevancy of the extracted phrase pairs. As a results, it improves both the efficiency and the performance of machine translation. 3 Hierarchical System Combination Framework The overall system combination framework is shown in Figure 1. The s"
D07-1029,2001.mtsummit-papers.68,0,0.0247391,"hypothesis among all systems’ top-one outputs based on N-gram language models trained on word stream (word) and word-POS mixed stream(wdpos). i where the mixed stream token T (e) = e when e ≤ N , and T (e) = P OS(e) when e > N . Similar to a class-based LM, this model is less prone to data sparseness problems. 4 Experiments We experiment with different system combination strategies on the NIST 2003 Arabic-English MT evaluation test set. Testset-specific bilingual data are subsampled, which include 260K sentence pairs, 10.8M Arabic words and 13.5M English words. We report case-sensitive BLEU (Papineni et al., 2001) 282 Table 1 shows the improvement by combining phrase tables from multiple MT systems using different combination strategies. We only show the highest and lowest baseline system scores. By combining testset-specific phrase translation tables (Tstcom), we achieved 1.0 BLEU improvement and 0.5 TER reduction. Sentence-level phrase combination and pruning additionally improve the BLEU score by 0.7 point and reduce TER by 0.4 percent. Table 2 shows the improvement with different sentence translation hypothesis selection approaches. The word-based LM is trained with about 1.75G words from newswire"
D07-1029,N07-1029,0,0.559389,"le top-one translation outputs from multiple MT systems, their approach reconstructs a phrase lattice by aligning words from different MT hypotheses. The alignment is based on the surface form of individual words, their stems (after morphology analysis) and part-of-speech (POS) tags. Aligned words are connected via edges. The algorithm finds the best alignment that minimizes the number of crossing edges. Finally the system generates a new translation by searching the lattice based on alignment information, each system’s confidence scores and a language model score. (Matusov et al., 2006) and (Rosti et al., 2007) constructed a confusion network from multiple MT hypotheses, and a consensus translation is selected by redecoding the lattice with arc costs and confidence scores. In this paper, we introduce our hierarchical system combination strategy. This approach allows combination on word, phrase and sentence levels. Similar to glass-box combination, each MT system provides detailed information about the translation process, such as which source word(s) generates which target word(s) in what order. Such information can be combined with existing word and phrase translation tables, and the augmented phra"
D07-1029,2006.amta-papers.25,0,0.0230289,"ram LM score calculated on the mixed stream of word and POS tags of the translation output. We run POS tagging on the translation hypotheses. We keep the word identities of top N frequent words (N =1000 in our experiments), and the remaining words are replaced with their POS tags. As a result, the mixed stream is like a skeleton of the original sentence, as shown in Figure 3. With this model, the optimal translation output E ∗ is selected based on the following formula: E = arg min E X TER 43.11 46.35 42.64 42.32 42.21 Table 1: Translation results with phrase combination and pruning. and TER (Snover et al., 2006) as the MT evaluation metrics. We evaluate the translation quality of different combination strategies: • WdCom: Combine testset-specific word translation model with the baseline model, as described in section 3.1. − log p(ei |ei−1 i−4 ), E ∗ = arg min − log Pwplm (E) sys1 sys4 Tstcom Tstcom+Sentcom Tstcom+Sentcom+Prune BLEUr4n4c 0.5323 0.4742 0.5429 0.5466 0.5505 (11) − log p(T (ei )|T (e)i−1 i−4 ) • PhrCom: Combine and prune phrase translation tables from all systems, as described in section 3.2. This include testset-specific phrase table combination (Tstcom), sentence level phrase combinati"
D07-1029,C00-2122,0,0.0746819,"Missing"
D07-1029,N07-1008,0,\N,Missing
D07-1029,P02-1040,1,\N,Missing
D07-1029,2005.iwslt-1.6,0,\N,Missing
D08-1041,J00-2011,0,0.149533,"Missing"
D08-1041,2006.amta-papers.25,0,0.0588207,"Missing"
D08-1041,P02-1051,0,0.280771,"Missing"
D08-1041,N04-1001,0,\N,Missing
D08-1041,W05-0709,1,\N,Missing
D08-1041,W03-1502,1,\N,Missing
D08-1041,C96-2141,0,\N,Missing
D08-1041,H05-1057,0,\N,Missing
D08-1041,J96-1002,0,\N,Missing
D08-1041,P06-1067,0,\N,Missing
D08-1041,P02-1040,0,\N,Missing
D08-1041,D08-1063,1,\N,Missing
D08-1041,P02-1002,0,\N,Missing
D12-1120,N10-1026,0,0.0183014,"ractable. Just finding the optimal hypothesis for a fixed representation of the training data is intractable for many hypothesis classes. And the d1 metric is intractable to compute from samples of a distribution, although Ben-David et al. propose some tractable bounds (2007; 2010). We view Equation 2 as a high-level goal rather than a computable objective. We leverage prior knowledge to bias the representation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the sou"
D12-1120,W06-1615,0,0.77027,"ystems (Ben-David et al., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011). This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains. There is no need to supply additional labeled examples for each new domain. Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representa"
D12-1120,W09-3821,0,0.0298214,"Missing"
D12-1120,P07-1036,0,0.030437,"d σ, is not a free parameter. It is explicitly minimized in the modified objective function. 1317 The entropy between layer l and the previous layers m measures how unpredictable the previous layers are, given layer l. By biasing the model such that M AX minus the entropy approaches zero, we encourage layer l towards completely different features from previous layers. We call the model with this bias P-HMM+D+E. 4 Efficient Parameter Estimation Several machine learning paradigms have been developed recently for incorporating biases and constraints into parameter estimation (Liang et al., 2009; Chang et al., 2007; Mann and McCallum, 2007). We leverage the Posterior Regularization (PR) framework for our problem because of its flexibility in handling different kinds of biases; we provide a brief overview of the technique here, but see (Ganchev et al., 2010) for full details. 4.1 Overview of PR PR introduces a modified EM algorithm to handle constrained objectives, like Equation 4. The modified E-step estimates a distribution q(Y) that is close to the current estimate of p(Y|x, θ), but also close to the ideal set of distributions that (in expectation) have φ = 0 for each property φ. The M step remains th"
D12-1120,W10-2608,0,0.0877263,"Missing"
D12-1120,P07-1033,0,0.499138,"Missing"
D12-1120,W01-0521,0,0.100299,"Missing"
D12-1120,P09-1056,1,0.892706,"l., 2007). Most systems depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011). This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains. There is no need to supply additional labeled examples for each new domain. Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representations can readily incor"
D12-1120,W10-2604,1,0.921941,"Missing"
D12-1120,W11-0315,1,0.774484,"for a fixed representation of the training data is intractable for many hypothesis classes. And the d1 metric is intractable to compute from samples of a distribution, although Ben-David et al. propose some tractable bounds (2007; 2010). We view Equation 2 as a high-level goal rather than a computable objective. We leverage prior knowledge to bias the representation learner towards attractive regions of the representations space R, and we develop efficient, greedy optimization techniques for learning effective representations. 2.4 and Yates, 2009; Ahuja and Downey, 2010; Turian et al., 2010; Huang et al., 2011); 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007;"
D12-1120,P07-1034,0,0.241732,"; 3) and structured representations that induce multi-dimensional real-valued features (Dhillon et al., 2011; Emami et al., 2003; Morin and Bengio, 2005). Our work falls into the second category, but builds on the previous work by demonstrating how to improve the distributional-similarity clusters with prior knowledge. To our knowledge, we are the first to apply semi-supervised representation learning techniques for structured NLP tasks. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III et al., 2010). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). A few authors have considered domain adaptation with no labeled data from the target domain (Blitzer et al., 2006; Huang et al., 2011) by using features based on distributional similarity. We demonstrate empirically that incorporating biases into this type of representation-learning process can significantly improve results. 3 Biased Representation Learning As before, let US and UT be unlabeled data, and LS be labeled data from the source domain only. Previous work on representation learning wit"
D12-1120,P09-1116,0,0.0331044,"ce new features for a supervised classifier. The objective function for HMM learning in this case is marginal log-likelihood, optimized using the BaumWelch algorithm: L(θ) = Previous Work There is a long tradition of research on representations for NLP, mostly falling into one of three categories: 1) vector space models and dimensionality reduction techniques (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2005; Deerwester et al., 1990; Honkela, 1997) 2) using structured representations to identify clusters based on distributional similarity, and using those clusters as features (Lin and Wu, 2009; Candito and Crabb´e, 2009; Huang 1315 X log X x∈US ∪UT p(x, Y = y|θ) (3) y where x is a sentence, Y is the sequence of latent random variables for the sentence, and y is an instance of the latent sequence. The joint distribution in an HMM factors into observation and transition distributions, typically mixtures of multinomials: p(x, y|θ) = P (y1 )P (x1 |y1 ) Y i≥2 P (yi |yi−1 )P (xi |yi ) Innocent bystanders y1 P(Y) JJ y2 are often the victims y3 y4 y5 y6 KL( pm ||pn ) pm p1 NNS Innocent bystanders ... ... p3 p2 pn Eφentropy (Y, z) RB VBP DT NNS are often the victims Figure 1: Illustration o"
D12-1120,N07-1070,0,0.0727136,"Missing"
D12-1120,A97-1015,0,0.167514,"Missing"
D12-1120,I05-3005,0,0.196113,"Missing"
D12-1120,P10-1040,0,0.457371,"depend on lexical features, which can differ greatly between domains, so that important words in the test data may never be seen in the training data. The connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Recently, several authors have found that learning new features based on distributional similarity can significantly improve domain adaptation (Blitzer et al., 2006; Huang and Yates, 2009; Turian et al., 2010; Dhillon et al., 2011). This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Moreover, since the representation-learning techniques are unsupervised, they can easily be applied to arbitrary new domains. There is no need to supply additional labeled examples for each new domain. Traditional representations still hold one significant advantage over representation-learning, however: because features are hand-crafted, these representations can readily incorporate the linguistic"
D12-1120,W03-0434,0,0.0523083,"Missing"
D15-1254,D14-1154,0,0.359276,"profiles instead of manually annotated, such data is called weakly labeled data. According to the dialect map shown in Fig.3, we group the social media posts into the following 5 dialects according to the author’s country: 1. Egyptian: Egypt 2. Gulf : Saudi Arabia, United Arab Emirate, Qatar, Bahrain, Oman, Yemen 3. Levantine: Syrian, Jordan, Palestinian, Lebanese 4. Iraqi: Iraq 5. Maghrebi: Algeria, Libya, Tunisia, Morocco Figure 3: Arabic dialect map, from (Zaidan and Callison-Burch, 2011). morphological and n-gram information, as seen in previous work ((Zaidan and Callison-Burch, 2011) , (Darwish et al., 2014), (Tillmann et al., 2014) and (Elfardy and Diab, 2013)). However, in this paper we focus on training classifiers with weakly and strongly labeled data, as well as semi-supervised learning methods. So we only choose the geographical and text-based features. Exploration of other features will be reported in another paper. Previous research (Zaidan and Callison-Burch, 2014) indicated that the unigram model obtains the best accuracy in dialect classification. However, (Tillmann et al., 2014) and (Darwish et al., 2014) exploited more sophisticated text features that lead to better accuracy on selec"
D15-1254,elfardy-diab-2012-simplified,0,0.0504937,"em. (Zbib et al., 2012) used crowd sourcing to build LevantineEnglish and Egyptian-English parallel data. Even with small amount of parallel corpora for each dialect, they obtained significant gains (6-7 BLEU pts) over a baseline MSA-English MT system. 2 3 Related Work Previous research on Arabic dialect identification focused on two problems: spoken dialect classification for speech recognition ((Novotney et al., 2011) and (Lei and Hansen, 2011)), and written text dialect classification mostly for machine translation. (Habash and Rambow, 2006), (Habash et al., 2008), (Diab et al., 2010) and (Elfardy and Diab, 2012) developed annotation guidelines and morphology analyzer for Arabic dialect. (Zaidan and Callison-Burch, 2011) created the AOC data set by extracting reader commentary from online Arabic newspaper forums. The selected Arabic sentences are manually labeled with Social Media Arabic Dialect Distribution and Translation The population speaking a dialect does not necessarily reflect its popularity on internet and social media. Many factors, such as a country’s social-economic development status, internet access and government policy, play important roles. To understand the distribution of Arabic di"
D15-1254,P13-2081,0,0.19843,"s called weakly labeled data. According to the dialect map shown in Fig.3, we group the social media posts into the following 5 dialects according to the author’s country: 1. Egyptian: Egypt 2. Gulf : Saudi Arabia, United Arab Emirate, Qatar, Bahrain, Oman, Yemen 3. Levantine: Syrian, Jordan, Palestinian, Lebanese 4. Iraqi: Iraq 5. Maghrebi: Algeria, Libya, Tunisia, Morocco Figure 3: Arabic dialect map, from (Zaidan and Callison-Burch, 2011). morphological and n-gram information, as seen in previous work ((Zaidan and Callison-Burch, 2011) , (Darwish et al., 2014), (Tillmann et al., 2014) and (Elfardy and Diab, 2013)). However, in this paper we focus on training classifiers with weakly and strongly labeled data, as well as semi-supervised learning methods. So we only choose the geographical and text-based features. Exploration of other features will be reported in another paper. Previous research (Zaidan and Callison-Burch, 2014) indicated that the unigram model obtains the best accuracy in dialect classification. However, (Tillmann et al., 2014) and (Darwish et al., 2014) exploited more sophisticated text features that lead to better accuracy on selected test set. In our experiments, we find that the uni"
D15-1254,P06-1086,0,0.0799842,"ures, then translated the normalized input with MSA Arabic-English MT system. (Zbib et al., 2012) used crowd sourcing to build LevantineEnglish and Egyptian-English parallel data. Even with small amount of parallel corpora for each dialect, they obtained significant gains (6-7 BLEU pts) over a baseline MSA-English MT system. 2 3 Related Work Previous research on Arabic dialect identification focused on two problems: spoken dialect classification for speech recognition ((Novotney et al., 2011) and (Lei and Hansen, 2011)), and written text dialect classification mostly for machine translation. (Habash and Rambow, 2006), (Habash et al., 2008), (Diab et al., 2010) and (Elfardy and Diab, 2012) developed annotation guidelines and morphology analyzer for Arabic dialect. (Zaidan and Callison-Burch, 2011) created the AOC data set by extracting reader commentary from online Arabic newspaper forums. The selected Arabic sentences are manually labeled with Social Media Arabic Dialect Distribution and Translation The population speaking a dialect does not necessarily reflect its popularity on internet and social media. Many factors, such as a country’s social-economic development status, internet access and government"
D15-1254,P07-2045,0,0.00685285,"Missing"
D15-1254,W11-2602,0,0.0949752,"Missing"
D15-1254,2010.amta-papers.5,0,0.108427,"Missing"
D15-1254,P11-2007,0,0.621762,"an example: there are big differences between MSA and various dialectal Arabic: MSA is the standardized and literary variety of Arabic used in writing and in most formal speech.1 It is widely used in government proceedings, newspapers and product manuals. Many research and linguistic resources for Arabic natural language processing are based on MSA. For example, most existing Arabic-English bilingual data are MSA-English parallel sentences. The dialect Arabic has more varieties: 5 major dialects are spoken in different regions of the Arab world: Egyptian, Gulf, Iraqi, Levantine and Maghrebi (Zaidan and Callison-Burch, 2011). These dialects differ in morphologies, grammatical cases, vocabularies and verb conjugations. These differences call for dialect-specific processing and modeling when building Arabic automatic speech recognition (ASR) systems or machine translation (MT) systems. Therefore, identification and classification of Arabic text is fundamental for building social media Arabic speech and language processing systems. In order to build better MT systems between Arabic and English, we first analyze the distribution of different Arabic dialects appearing on a very large scale social media platform, as we"
D15-1254,J14-1006,0,0.258729,"anese 4. Iraqi: Iraq 5. Maghrebi: Algeria, Libya, Tunisia, Morocco Figure 3: Arabic dialect map, from (Zaidan and Callison-Burch, 2011). morphological and n-gram information, as seen in previous work ((Zaidan and Callison-Burch, 2011) , (Darwish et al., 2014), (Tillmann et al., 2014) and (Elfardy and Diab, 2013)). However, in this paper we focus on training classifiers with weakly and strongly labeled data, as well as semi-supervised learning methods. So we only choose the geographical and text-based features. Exploration of other features will be reported in another paper. Previous research (Zaidan and Callison-Burch, 2014) indicated that the unigram model obtains the best accuracy in dialect classification. However, (Tillmann et al., 2014) and (Darwish et al., 2014) exploited more sophisticated text features that lead to better accuracy on selected test set. In our experiments, we find that the unigram model does outperform bigram and trigram models, so we stick to the unigram features. 5 5.1 Table 1 shows the number of words for each dialect group. Considering the dialect distribution in the social media platform (shown in Figure 1), we focus on the classification of MSA (msa) and 3 Arabic dialects: Egyptian ("
D15-1254,N12-1006,0,0.0864852,"ata. (Darwish et al., 2014) selected Twitter data and developed models taking consideration of lexical, morphological, and phonological information from different dialects, then classified Egyptian and MSA Arabic tweets. (Cotterell and Callison-Burch, 2014) collected dialect data covering Iraqi and Maghrebi Arabic from Twitter as well. When translating Arabic dialect into English, (Sawaf, 2010) and (Salloum and Habash, 2011) normalized dialect words into MSA equivalents considering character- and morpheme-level features, then translated the normalized input with MSA Arabic-English MT system. (Zbib et al., 2012) used crowd sourcing to build LevantineEnglish and Egyptian-English parallel data. Even with small amount of parallel corpora for each dialect, they obtained significant gains (6-7 BLEU pts) over a baseline MSA-English MT system. 2 3 Related Work Previous research on Arabic dialect identification focused on two problems: spoken dialect classification for speech recognition ((Novotney et al., 2011) and (Lei and Hansen, 2011)), and written text dialect classification mostly for machine translation. (Habash and Rambow, 2006), (Habash et al., 2008), (Diab et al., 2010) and (Elfardy and Diab, 2012)"
D15-1254,P02-1040,0,0.0913197,"Missing"
D15-1254,cotterell-callison-burch-2014-multi,0,\N,Missing
D19-1436,P18-1060,0,0.0137772,"the ability to generate more diverse sentences. We also provide standard deviation of each metric in Table 4, reflecting the stability of each model’s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outperforms policy gradient in the stability of adversarial training. 80 60 40 4.5 Dialogue Generation on WeiboDial 20 0 50 100 150 200 250 Epoch Dialogue evaluation is an open problem and existing works have found that automatic metrics have low correlation to human evaluation (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Thus, we resorted to manual evaluation to assess the generation quality on WeiboDial. We randomly sampled 200 posts from the test set and collected the generated results from all the models. For each pair of responses (one from ARAML and the other from a baseline, given the same input post), five annotators were hired to label which response is better (i.e. win, lose or tie) in terms of grammaticality (whether a response itself is gramFigure 2: PPL-F/PPL-R curves of ARAML, SeqGAN, LeakGAN, MaliGAN and IRL in the training process. The shade area indicates the standard deviation at each data p"
D19-1436,W14-4012,0,0.0815704,"Missing"
D19-1436,P82-1020,0,0.82805,"Missing"
D19-1436,P18-1139,1,0.835699,"ML training into adversarial training paradigm. Experimental results on three text generation tasks show the effectiveness of our method. 2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likelihood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016). Various methods have been proposed to enhance the generation quality by refining the objective function (Li et al., 2016; Mou et al., 2016) or modifying the generation distribution with external information like topic (Xing et al., 2017), sentence type (Ke et al., 2018), emotion (Zhou et al., 2018a) and knowledge (Zhou et al., 2018b). As mentioned above, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016). Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actorcritic (Bahdanau et al., 2017). (Norouzi et al., 2016) proposed an efficient and stable approach called Reward Augmented Maximum Likelihood (RAML), which connects the log-likelihood and expected rewards to incorporate MLE training objective into RL framework. Since some text generation tasks ha"
D19-1436,N16-1014,0,0.0372014,"butions are mainly as follows: • We analyze the fundamental issue of current GANs for text generation from the perspectives of training instability. • We propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML), which incorporates stable RAML training into adversarial training paradigm. Experimental results on three text generation tasks show the effectiveness of our method. 2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likelihood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016). Various methods have been proposed to enhance the generation quality by refining the objective function (Li et al., 2016; Mou et al., 2016) or modifying the generation distribution with external information like topic (Xing et al., 2017), sentence type (Ke et al., 2018), emotion (Zhou et al., 2018a) and knowledge (Zhou et al., 2018b). As mentioned above, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016). Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actorcritic (Bahdanau et al."
D19-1436,D17-1230,0,0.15585,"sues of training GANs on discrete data are more severe than exposure bias (Semeniuta1 et al., 2018; Caccia et al., 2018). One of the fundamental issues when generating discrete text samples with GANs is training instability. Updating the generator with policy gradient always leads to an unstable training process because it’s difficult for the generator to derive positive and stable reward signals from the discriminator even with careful pretraining (Che et al., 2017). As a result, the generator gets lost due to the high variance of reward signals and the training process may finally collapse (Li et al., 2017). In this paper, we propose a novel adversarial training framework called Adversarial Reward Augmented Maximum Likelihood (ARAML) to deal with the instability issue of training GANs for text generation. At each iteration of adversarial training, we first train the discriminator to assign higher rewards to real data than to generated samples. Then, inspired by reward augmented maximum likelihood (RAML) (Norouzi et al., 2016), the generator is updated on the samples acquired from a stationary distribution with maximum likelihood estimation (MLE), weighted by the discriminator’s rewards. This sta"
D19-1436,D16-1230,0,0.0162778,"elp of the MLE training objective and has the ability to generate more diverse sentences. We also provide standard deviation of each metric in Table 4, reflecting the stability of each model’s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outperforms policy gradient in the stability of adversarial training. 80 60 40 4.5 Dialogue Generation on WeiboDial 20 0 50 100 150 200 250 Epoch Dialogue evaluation is an open problem and existing works have found that automatic metrics have low correlation to human evaluation (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Thus, we resorted to manual evaluation to assess the generation quality on WeiboDial. We randomly sampled 200 posts from the test set and collected the generated results from all the models. For each pair of responses (one from ARAML and the other from a baseline, given the same input post), five annotators were hired to label which response is better (i.e. win, lose or tie) in terms of grammaticality (whether a response itself is gramFigure 2: PPL-F/PPL-R curves of ARAML, SeqGAN, LeakGAN, MaliGAN and IRL in the training process. The shade area"
D19-1436,C12-1173,0,0.0538465,"Missing"
D19-1436,C16-1316,0,0.0159788,"ability. • We propose a novel framework called Adversarial Reward Augmented Maximum Likelihood (ARAML), which incorporates stable RAML training into adversarial training paradigm. Experimental results on three text generation tasks show the effectiveness of our method. 2 Related Work Recently, text generation has been widely studied with neural models trained with maximum likelihood estimation (Graves, 2013). However, MLE tends to generate universal text (Li et al., 2016). Various methods have been proposed to enhance the generation quality by refining the objective function (Li et al., 2016; Mou et al., 2016) or modifying the generation distribution with external information like topic (Xing et al., 2017), sentence type (Ke et al., 2018), emotion (Zhou et al., 2018a) and knowledge (Zhou et al., 2018b). As mentioned above, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016). Thus, reinforcement learning has been introduced to text generation tasks such as policy gradient (Ranzato et al., 2016) and actorcritic (Bahdanau et al., 2017). (Norouzi et al., 2016) proposed an efficient and stable approach called Reward Augmented Maximum Likelihood (RAML), which connects t"
D19-1436,D17-1238,0,0.0155499,"ining objective and has the ability to generate more diverse sentences. We also provide standard deviation of each metric in Table 4, reflecting the stability of each model’s performance. Our model ARAML nearly achieves the smallest standard deviation in all the metrics, indicating that our framework outperforms policy gradient in the stability of adversarial training. 80 60 40 4.5 Dialogue Generation on WeiboDial 20 0 50 100 150 200 250 Epoch Dialogue evaluation is an open problem and existing works have found that automatic metrics have low correlation to human evaluation (Liu et al., 2016; Novikova et al., 2017; Chaganty et al., 2018). Thus, we resorted to manual evaluation to assess the generation quality on WeiboDial. We randomly sampled 200 posts from the test set and collected the generated results from all the models. For each pair of responses (one from ARAML and the other from a baseline, given the same input post), five annotators were hired to label which response is better (i.e. win, lose or tie) in terms of grammaticality (whether a response itself is gramFigure 2: PPL-F/PPL-R curves of ARAML, SeqGAN, LeakGAN, MaliGAN and IRL in the training process. The shade area indicates the standard"
D19-1436,D18-1428,0,0.21573,"Although widely used, MLE suffers from the exposure bias problem (Bengio et al., 2015; Ranzato et al., 2016): during test, the model sequentially predicts the next word conditioned on its previous generated words while during training conditioned on ground-truth words. To tackle this ∗ † Equal contribution Corresponding author: Minlie Huang problem, generative adversarial networks (GAN) with reinforcement learning (RL) training approaches have been introduced to text generation tasks (Yu et al., 2017; Che et al., 2017; Lin et al., 2017; Fedus et al., 2018; Guo et al., 2018; Shi et al., 2018; Xu et al., 2018), where the discriminator is trained to distinguish real and generated text samples to provide reward signals for the generator, and the generator is optimized via policy gradient (Yu et al., 2017). However, recent studies have shown that potential issues of training GANs on discrete data are more severe than exposure bias (Semeniuta1 et al., 2018; Caccia et al., 2018). One of the fundamental issues when generating discrete text samples with GANs is training instability. Updating the generator with policy gradient always leads to an unstable training process because it’s difficult for the gene"
E14-1001,W04-3250,0,0.128097,"Missing"
E14-1001,W10-0102,0,0.0295202,"Missing"
E14-1001,D07-1005,0,0.0197481,"osed methods, other researchers have used co-training before for MT (CallisonBurch and Osborne, 2003). They use target strings in multiple languages as different views on translation. However, in our work, we treat the alignment model and language model as different views of LCS data. In addition to co-training, various other semisupervised approaches for MT and word alignment have been proposed, but these have relied on sentence alignments among multiple languages, rather than LCS data. Kay (2000) proposes using multiple target documents as a way of informing subsequent machine translations. Kumar et al. (2007) described a technique for word alignment in a multi-parallel sentence-aligned corpus and showed that this technique can be used to obtain higher quality bilingual word alignments. Other work like (Eisele, 2006) took the issue one step further that they used bilingual translation systems In this paper, we investigate two approaches to using LCS data for machine translation. The first approach focuses exclusively on word alignment, and uses patterns extracted from LCS data to guide the EM training procedure for word alignment over a standard sentence-aligned parallel corpus. We focus on two typ"
E14-1001,P03-1021,0,0.0221405,"y ps = ps ∗ pj new Xl ← Xl ∪ xi 5: LM ← Train-LM(Xlnew ) 6: Extract the tri-gram gram3 from LM 7: For each sentence xi in XLCS : run Algorithm 1: finding tsimilar 8: update tb0 using (tm , sj ) where tm ∈ tsimilar and sj ∈ xi 9: End For 10: Output: word alignment for Xp and LM some of the problems caused by LCS data. To clarify, we use IBM1 model and HMM models in succession for the baseline. We trained the IBM1 model first and used the resulting parameters as the initial parameter values to train HMM model. Parameters for the final MT system are tuned with Minimum Error Rate Training (MERT) (Och, 2003). The tuning set for MERT is the NIST MT06 data set, which includes 1664 sentences. We test the system on NIST MT02 (878 sentences). To evaluate the word alignment results, we manually aligned 250 sentences from NIST MT02 data set. For simplicity, we only have two types of labels for evaluating word alignments: either two words are aligned together or not. (Previous evaluation metrics also consider a third label for ”possible” alignments.) Out of the word-aligned data, we use 100 sentences as a development set and the rest as our testing set. Our MT training corpus contains 2,636,692 sentence"
E14-1001,2005.mtsummit-papers.20,0,0.361726,"on, and thus it contains a diversity of topics that people care about, such as home furnishings, cars, entertainment, etc, that may not show up in standard parallel corpora. Moreover, LCS data is easily accessible from Web communities, such as MITBBS.com, Sina Weibo, Twitter, etc. However, like most unedited natural language text on the Web, LCS data contains symbols like emotions, grammar and spelling mistakes, slang and strongly idiomatic usage, and a variety of other phenomena that are difficult to handle. LCS data with different language pairs may also need special handling. For instance, Sinha and Thakur (2005) focus on words in mixed English and Hindi texts where a single word contains elements from both languages; they propose techniques for translating such words into both pure English and pure Hindi. Our study focuses on ChineseEnglish LCS, where this is rarely a problem, Linguist Code Switching (LCS) is a situation where two or more languages show up in the context of a single conversation. For example, in EnglishChinese code switching, there might be a sentence like “· ‚15© ¨ k ‡meeting (We will have a meeting in 15 minutes)”. Traditional machine translation (MT) systems treat LCS data as nois"
E14-1001,P04-1023,0,0.028742,"tries to maximize the marginal likelihood of the sentence-level aligned pairs. For the HMM alignment model, the forwardbackward algorithm can be used the optimize the posterior probability of the hidden alignment a. which share one or more common pivot languages to build systems which non-parallel corpus is used. Unlike the data in these techniques, LCS data requires no manual alignment effort and is freely available in large quantities. Another line of research has attempted to improve word alignment models by incorporating manually-labeled word alignments in addition to sentence alignments. Callison-Burch et al. (2004) tried to give a higher weight on manually labeled data compared to the automatic alignments. Fraser and Marcu (2006) used a log-linear model with features from IBM models. They alternated the traditional Expectation Maximization algorithm which is applied on a large parallel corpus with a discriminative step aimed at increasing wordalignment quality on a small, manually wordaligned corpus. Ambati et al.(2010) tried to manually correct the alignments which are informative during the unsupervised training and applied them to an active learning model. However, labeled word alignment data is expe"
E14-1001,D08-1102,0,0.10221,"s future work. but for other language pairs, Sinha and Thakur’s techniques may be required as preprocessing steps. Primarily, though, LCS data requires special-purpose algorithms to use it for word alignment, since it contains no explicit alignment labels. 2 Related Work There has been a lot of research on LCS from the theoretical and socio-linguistic communities (Nilep, 2006; De Fina, 2007). Computational research on LCS has studied how to identify the boundaries of an individual language within LCS data, or how to predict when an utterance will switch to another language (Chan et al., 2004; Solorio and Liu, 2008). Manandise and Gdaniec (2011) analyzed the effect on machine translation quality of LCS of Spanish-English and showed that LCS degrades the performance of the syntactic parser. Sinha and Thakur (2005) translate mixed Hindi and English (Hinglish) to pure Hindi and pure English by using two morphological analyzers from both Hindi and English. The difficulty in their problem is that Hindi and English are often mixed into a single word which uses only the English alphabet; approaches based only on the character set cannot tell these words apart from English words. Our current study is for a langu"
E14-1001,C96-2141,0,0.321966,"until they converge to similar predictions on the LCS data. In combination with a larger phrase-based MT system (Koehn et al., 2003), these two training procedures yield an MT system that achieves a BLEU score of 31.79 on an English-to-Chinese translation task, an improvement of 2.64 in BLEU score over a baseline MT system trained on only our parallel corpora. The rest of this paper is organized as follows. The next section presents related work. Section 3 gives an overview of word alignment. Sections 4 2 data to improve the model. IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Vogel et al., 1996) are cascaded to form the baseline model for alignment. These two models have Q a similar formulation L = P (t, a|s) = P (a) j P (tj |saj ) with a different distortion probability P (a). s and t denote the source and target sentences. a is the alignment, and aj is the index of the source language word that generates the target language word at position j. The HMM model assumes the alignments have aQfirst-order Markov dependency, so that P (a) = j P (aj |aj − aj−1 ). IBM Model 1 ignores the word position so Q and uses a uniform distribution, 1 P (a) = j P (aj ) where P (aj ) = |t| , where |t| i"
E14-1001,eisele-2006-parallel,0,0.0318731,"e alignment model and language model as different views of LCS data. In addition to co-training, various other semisupervised approaches for MT and word alignment have been proposed, but these have relied on sentence alignments among multiple languages, rather than LCS data. Kay (2000) proposes using multiple target documents as a way of informing subsequent machine translations. Kumar et al. (2007) described a technique for word alignment in a multi-parallel sentence-aligned corpus and showed that this technique can be used to obtain higher quality bilingual word alignments. Other work like (Eisele, 2006) took the issue one step further that they used bilingual translation systems In this paper, we investigate two approaches to using LCS data for machine translation. The first approach focuses exclusively on word alignment, and uses patterns extracted from LCS data to guide the EM training procedure for word alignment over a standard sentence-aligned parallel corpus. We focus on two types of patterns in the LCS data: first, English words are almost never correct translations for any Chinese word in the same LCS utterance. Second, for sentences that are mostly Chinese but with some English word"
E14-1001,P06-1097,0,0.0274621,"kward algorithm can be used the optimize the posterior probability of the hidden alignment a. which share one or more common pivot languages to build systems which non-parallel corpus is used. Unlike the data in these techniques, LCS data requires no manual alignment effort and is freely available in large quantities. Another line of research has attempted to improve word alignment models by incorporating manually-labeled word alignments in addition to sentence alignments. Callison-Burch et al. (2004) tried to give a higher weight on manually labeled data compared to the automatic alignments. Fraser and Marcu (2006) used a log-linear model with features from IBM models. They alternated the traditional Expectation Maximization algorithm which is applied on a large parallel corpus with a discriminative step aimed at increasing wordalignment quality on a small, manually wordaligned corpus. Ambati et al.(2010) tried to manually correct the alignments which are informative during the unsupervised training and applied them to an active learning model. However, labeled word alignment data is expensive to produce. Our approach is complementary, in that we use mixed data that has no word alignments, but still abl"
E14-1001,N03-1017,0,0.0801789,"translation table of the alignment model, the training procedure finds candidate translations of the English words in the LCS data, and uses those to supplement the language model training data. From the language model, the training procedure identifies Chinese words that complete the Chinese sentence with high probability, and it uses the English word paired with these completion words as additional training points for translation probabilities. These models are trained repeatedly until they converge to similar predictions on the LCS data. In combination with a larger phrase-based MT system (Koehn et al., 2003), these two training procedures yield an MT system that achieves a BLEU score of 31.79 on an English-to-Chinese translation task, an improvement of 2.64 in BLEU score over a baseline MT system trained on only our parallel corpora. The rest of this paper is organized as follows. The next section presents related work. Section 3 gives an overview of word alignment. Sections 4 2 data to improve the model. IBM Model 1 (Brown et al., 1994) and the HMM alignment model (Vogel et al., 1996) are cascaded to form the baseline model for alignment. These two models have Q a similar formulation L = P (t, a"
E14-1001,J93-2003,0,\N,Missing
E14-1001,D08-1076,0,\N,Missing
H05-1055,P02-1051,0,0.163208,"Missing"
H05-1055,W03-0317,0,0.0476982,"Missing"
H05-1055,P04-1024,0,0.0299063,". All these approaches exploit a general model for NE transliteration, where source names from different origins or language families are transliterated into the target language with the same rules or probability distributions, which fails to capture their different 1 Assuming foreign names are already transliterated into Chinese. 435 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 435–442, Vancouver, October 2005. 2005 Association for Computational Linguistics transliteration patterns. Alternatively, (Qu and Grefenstette, 2004) applied language identification of name origins to select language-specific transliterations when back-transliterating Japanese names from English to Japanese. However, they only classified names into three origins: Chinese, Japanese and English, and they used the Unihan database to obtain the mapping between kenji characters and romanji representations. Ideally, to explicitly model these transliteration differences we should construct a transliteration model and a language model for each origin. However, some origins lack enough name translation pairs for reliable model training. In this pap"
H05-1055,2003.mtsummit-papers.53,1,0.894598,"Missing"
H05-1055,W99-0604,0,\N,Missing
H05-1055,W03-1502,1,\N,Missing
H05-1055,W03-1508,0,\N,Missing
H05-1055,J97-3002,0,\N,Missing
H05-1055,P97-1017,0,\N,Missing
H05-1061,J93-2003,0,0.00387009,"e Chinese phrase f has J Chinese characters, f 1 , f 2 ,... f J , and the English candidate phrase e has L English words, e1 , e2 ,..., eL . The transliteration cost between a Chinese query f and an English translation candidate e is calculated as: Ctrl (e, f ) ≈ ∑ log p(ea j |y j ) = ∑∑ log p(ea( i , j ) |yi , j ). j j i where y j is the pinyin of Chinese character f j , y j ,i is the i th letter in y j , and ea and ea j ( j ,i ) Translation Model The translation model measures the semantic equivalence between a Chinese phrase and an English candidate. One widely used model is the IBM model (Brown et al. 1993). The phrase translation probability is computed using the IBM model-1 as: 1 Ptrans ( f |e ) = J L J j 3.3 Frequency-Distance Model The more often a bilingual phrase pair co-occurs, or the closer a bilingual phrase pair is within a snippet, the more likely they are translations of each other. The frequency-distance model measures this correlation. Suppose S is the set of returned snippets for query f , and a single returned snippet is si ∈ S . The source phrase occurs in si as f i , j ( j ≥ 1 since f may occur several times in a snippet). The frequency-distance weight of an English candidate e"
H05-1061,W03-1502,1,0.827904,"and location names, which are phonetically translated and whose written forms resemble their pronunciations. Therefore it is possible to discover these translation pairs through their surface strings. Surface string transliteration does not need a pronunciation lexicon to map words into phoneme sequences; thus it is especially appealing for OOV word translation. For non-Latin languages like Chinese, a romanization script called “pinyin” maps each Chinese character into Latin letter strings. This normalization makes the string alignment possible. We adopt the transliteration model proposed in (Huang, et al. 2003). This model calculates the probabilistic Levinstein distance between a romanized source string and a target string. Unlike the traditional Levinstein distance calculation, the character alignment cost is not binary (0/1); rather it is the logarithm of character alignment probability, which ensures that characters with similar pronunciations (e.g. `p` and `b`) have higher alignment probabilities and lower cost. These probabilities are automatically learned from bilingual name lists using EM. Assume the Chinese phrase f has J Chinese characters, f 1 , f 2 ,... f J , and the English candidate ph"
H05-1061,2003.mtsummit-papers.53,1,\N,Missing
H05-1061,J03-3002,0,\N,Missing
J14-1004,N10-1026,1,0.94083,"r knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representations for domain adaptation (Huang and Yates 2009, 2010) by investigating new language representations—the naive Bayes representation and PL-MRF representation (Huang et al. 2011)—by analyzing results in terms of polysemy, sparsity, and domain divergence; by testing on new data sets including a Chinese POS tagging task; and by providing an empirical comparison with Brown clusters as representations. 3. Learning Representations of Distributional Similarity In this section, we will introduce several representation learning models. 3."
J14-1004,P05-1001,0,0.00903134,"resentation from the task of optimizing a hypothesis. To learn a representation, we can train a statistical language model on unlabeled text, and then use parameters or latent states from the statistical language model to create a representation function. Optimizing a hypothesis then follows the standard learning framework, using the representation from the statistical language model. 90 Huang et al. Computational Linguistics The LMRH is similar to the manifold and cluster assumptions behind other semisupervised approaches to machine learning, such as Alternating Structure Optimization (ASO) (Ando and Zhang 2005) and Structural Correspondence Learning (SCL) (Blitzer, McDonald, and Pereira 2006). All three of these techniques use predictors built on unlabeled data as a way to harness the manifold and cluster assumptions. However, the LMRH is distinct from at least ASO and SCL in important ways. Both ASO and SCL create multiple “synthetic” or “pivot” prediction tasks using unlabeled data, and find transformations of the input feature space that perform well on these tasks. The LMRH, on the other hand, is more specific — it asserts that for language problems, if we optimize word representations on a sing"
J14-1004,C04-1080,0,0.0156283,"like most semisupervised techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs pr"
J14-1004,W04-3224,0,0.221836,"eakly supervised learning, that is, learning when domain-specific labeled training data are scarce. A growing body of theoretical and empirical evidence suggests that traditional, manually crafted features for a variety of NLP tasks limit systems’ performance in this weakly supervised learning for two reasons. First, feature sparsity prevents systems from generalizing accurately, because many words and features are not observed in training. Also because word frequencies are Zipf-distributed, this often means that there is little relevant training data for a substantial fraction of parameters (Bikel 2004b), especially in new domains (Huang and Yates 2009). For example, word-type features form the backbone of most POS-tagging systems, but types like “gene” and “pathway” show up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features “gene” and “pathway” (Blitzer, McDonald, and Pereira 2006; Ben-David et al. 2010). Further, because words are polysemous, word-type features prevent systems from generalizing to situations in which words have di"
J14-1004,P07-1056,0,0.0089954,"ther representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain as well (Daum´e III, Kumar, and Saha 2010). These techniques do not handle open-domain corpora like the Web, where they require expert input to acquire labels for each new sing"
J14-1004,W06-1615,0,0.123652,"Missing"
J14-1004,J92-4003,0,0.705609,"ious Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for rep"
J14-1004,W09-3821,0,0.00854013,"ooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of a word, and hence its sim"
J14-1004,P06-1012,0,0.220756,"results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning when domain-specific labeled training data are scarce. A growing body of theoretical and empirical evidence suggests that traditional, manually crafted features for a variety of NLP tasks limit systems’ performa"
J14-1004,W04-3237,0,0.012546,"ning data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning when domain-specific labeled training data are scarce. A growing body of theoretical and empirical evidence suggests that traditional, manually crafted feat"
J14-1004,P07-1033,0,0.0391516,"Missing"
J14-1004,W10-2608,0,0.0266538,"Missing"
J14-1004,D09-1003,0,0.017399,"ques are not representation learning, and are complementary to our techniques. Our representation-learning approach to domain adaptation is an instance of semi-supervised learning. Of the vast number of semi-supervised approaches to sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki’s (2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text to achieve the current best performance on in-domain chunking, and semi-supervised approaches to improving in-domain SRL with large quantities of unlabeled text ¨ (Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Furstenau and Lapata 2009). Ando and Zhang’s (2005) semi-supervised sequence labeling technique has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and Pereira 2006); our representation-learning approaches outperform it. Unlike most semisupervised techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then"
J14-1004,P07-1088,1,0.808217,"Missing"
J14-1004,D08-1072,0,0.00844888,"l of their chunking and NER tests. We concentrate on probabilistic graphical models with discrete latent states instead. We show that HMMbased and other representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain as well (Daum´e III, Kumar, and"
J14-1004,N09-1068,0,0.0210352,"e concentrate on probabilistic graphical models with discrete latent states instead. We show that HMMbased and other representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain as well (Daum´e III, Kumar, and Saha 2010). These techniq"
J14-1004,E09-1026,0,0.0281669,"arning, and are complementary to our techniques. Our representation-learning approach to domain adaptation is an instance of semi-supervised learning. Of the vast number of semi-supervised approaches to sequence labeling in NLP, the most relevant ones here include Suzuki and Isozaki’s (2008) combination of HMMs and CRFs that uses over a billion words of unlabeled text to achieve the current best performance on in-domain chunking, and semi-supervised approaches to improving in-domain SRL with large quantities of unlabeled text ¨ (Weston, Ratle, and Collobert 2008; Deschacht and Moens 2009; and Furstenau and Lapata 2009). Ando and Zhang’s (2005) semi-supervised sequence labeling technique has been tested on a domain adaptation task for POS tagging (Blitzer, McDonald, and Pereira 2006); our representation-learning approaches outperform it. Unlike most semisupervised techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the"
J14-1004,W01-0521,0,0.0187287,"feature like, “the previous token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that"
J14-1004,P07-1094,0,0.0203683,"ed techniques, we concentrate on a particularly simple task decomposition: unsupervised learning for new representations, followed by standard supervised learning. In addition to our task decomposition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for"
J14-1004,P90-1034,0,0.486328,"never appears in the WSJ portion of the Penn Treebank (Huang and Yates 2010). Our response to the sparsity and polysemy challenges with traditional NLP representations is to seek new representations that allow systems to generalize to previously unseen examples. That is, we seek representations that permit classifiers to have close to the same accuracy on examples from other domains as they do on the domain of the training data. Our approach depends on the well-known distributional hypothesis, which states that a word’s meaning is identified with the contexts in which it appears (Harris 1954; Hindle 1990). Our goal is to develop probabilistic statistical language models that describe the contexts of individual words accurately. We then construct representations, or mappings from word tokens and types to real-valued vectors, from statistical language models. Because statistical language models are designed to model words’ contexts, the features they produce can be used to combat problems with polysemy. And by careful design of the statistical language models, we can limit 86 Huang et al. Computational Linguistics the number of features that they produce, controlling how sparse those features ar"
J14-1004,P09-1056,1,0.690554,"tagger would traditionally use a feature like, “the previous token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supe"
J14-1004,W10-2604,1,0.871307,"Missing"
J14-1004,W11-0315,1,0.580237,"ed, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representations for domain adaptation (Huang and Yates 2009, 2010) by investigating new language representations—the naive Bayes representation and PL-MRF representation (Huang et al. 2011)—by analyzing results in terms of polysemy, sparsity, and domain divergence; by testing on new data sets including a Chinese POS tagging task; and by providing an empirical comparison with Brown clusters as representations. 3. Learning Representations of Distributional Similarity In this section, we will introduce several representation learning models. 3.1 Traditional POS-Tagging Representations As an example of our terminology, we begin by describing a representation used in traditional POS taggers (this representation will later form a baseline for our POS tagging experiments). The instance"
J14-1004,P07-1034,0,0.0259863,"than neural net models on all of their chunking and NER tests. We concentrate on probabilistic graphical models with discrete latent states instead. We show that HMMbased and other representations significantly outperform the more commonly used Brown clustering (Brown et al. 1992) as a representation for domain adaptation settings of sequence-labeling tasks. Most previous work on domain adaptation has focused on the case where some labeled data are available in both the source and target domains (Chan and Ng 2006; Daum´e III and Marcu 2006; Blitzer, Dredze, and Pereira 2007; Daum´e III 2007; Jiang and Zhai 2007a, 2007b; Dredze and Crammer 2008; Finkel and Manning 2009; Dredze, Kulesza, and Crammer 2010). Learning bounds for this domain-adaptation setting are known (Blitzer et al. 2007; Mansour, Mohri, and Rostamizadeh 2009). Approaches to this problem setting have focused on appropriately weighting examples from the source and target domains so that the learning algorithm can balance the greater relevance of the target-domain data with the larger source-domain data set. In some cases, researchers combine this approach with semi-supervised learning to include unlabeled examples from the target domain"
J14-1004,D07-1031,0,0.0115092,"osition being simple, our learned representations are also task-independent, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representation"
J14-1004,P08-1068,0,0.237807,"Missing"
J14-1004,P09-1116,0,0.185604,"t-level lexical cooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of"
J14-1004,J93-2004,0,0.0464831,"Missing"
J14-1004,N10-1004,0,0.00858595,"us token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning when domain-spec"
J14-1004,N04-1043,0,0.431145,"Missing"
J14-1004,P93-1024,0,0.834677,"Missing"
J14-1004,N07-1070,0,0.0143387,"Missing"
J14-1004,W09-1119,0,0.167518,"eaning based on document-level lexical cooccurrence statistics (Salton and McGill 1983; Sahlgren 2006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to captur"
J14-1004,A97-1015,0,0.0543968,", “the previous token is the” to help classify a given token as a noun or adjective. For supervised NLP tasks with sufficient domain-specific training data, these traditional features yield state-of-the-art results. However, NLP systems are increasingly being applied to the Web, scientific domains, personal communications like e-mails and tweets, among many other kinds of linguistic communication. These texts have very different characteristics from traditional training corpora in NLP. Evidence from POS tagging (Blitzer, McDonald, and Pereira 2006; Huang and Yates 2009), parsing (Gildea 2001; Sekine 1997; McClosky 2010), and semantic role labeling (SRL) (Pradhan, Ward, and Martin 2007), among other NLP tasks (Daum´e III and Marcu 2006; Chelba and Acero 2004; Downey, Broadhead, and Etzioni 2007; Chan and Ng 2006; Blitzer, Dredze, and Pereira 2007), shows that the accuracy of supervised NLP systems degrades significantly when tested on domains different from those used for training. Collecting labeled training data for each new target domain is typically prohibitively expensive. In this article, we investigate representations that can be applied to weakly supervised learning, that is, learning"
J14-1004,P07-1096,0,0.0200571,"Missing"
J14-1004,P05-1044,0,0.080707,"t, so we can learn the representation once, and then apply it to any task. One of the best-performing representations that we consider for domain adaptation is based on the HMM (Rabiner 1989). HMMs have of course also been used for supervised, semi-supervised, and unsupervised POS tagging on a single domain (Banko and Moore 2004; Goldwater and Griffiths 2007). Recent efforts on improving unsupervised POS tagging have focused on incorporating prior knowledge into the POS induction model (Grac¸a et al. 2009; Toutanova and Johnson 2007), or on new training techniques like contrastive estimation (Smith and Eisner 2005) for alternative sequence models. Despite the fact that completely connected, standard HMMs perform poorly at the POS induction task (Johnson 2007), we show that they still provide very useful features for a supervised POS tagger. Experiments in information extraction have previously also shown that HMMs provide informative features for this quite different, semantic processing task (Downey, Schoenmackers, and Etzioni 2007; Ahuja and Downey 2010). This article extends our previous work on learning representations for domain adaptation (Huang and Yates 2009, 2010) by investigating new language"
J14-1004,P08-1076,0,0.018922,"Missing"
J14-1004,D09-1058,0,0.0174586,"006; Turney and Pantel 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of a word, and hence its similarity to other words, more precisely. Our experimen"
J14-1004,W00-0726,0,0.112069,"Missing"
J14-1004,I05-3005,0,0.0811765,"Missing"
J14-1004,N09-2062,0,0.0574409,"Missing"
J14-1004,P10-1040,0,0.219549,"Missing"
J14-1004,C96-2212,0,0.159741,"s model. We train our models using standard expectation-maximization (Dempster, Laird, and Rubin 1977) with random initialization of the parameters. Because our factorization of the sentence does not take into account the fact that the trigrams overlap, the resulting statistical language model is mass-deficient. Worse still, it is throwing away information from the dependencies among trigrams which might help make better clustering decisions. Nevertheless, this model closely mirrors many of the clustering algorithms used in previous approaches to representation learning for sequence labeling (Ushioda 1996; Miller, Guinness, and Zamanian 2004; Koo, Carreras, 1 Compare with Dhillon, Foster, and Ungar (2011), who use canonical correlation analysis to find a simultaneous reduction of the left and right context vectors, a significantly more complex undertaking. 95 Computational Linguistics Volume 40, Number 1 and Collins 2008; Lin and Wu 2009; Ratinov and Roth 2009), and therefore serves as an important benchmark. Given a naive Bayes statistical language model, we construct an NB-R representation that produces |S |boolean features Fs (xi ) for each token xi and each possible latent state s ∈ S:  F"
J14-1004,N13-1065,1,0.85938,"Missing"
J14-1004,W09-1208,0,0.0149806,"el 2010); 2) dimensionality reduction techniques for vector space models (Deerwester et al. 1990; Ritter and Kohonen 1989; Honkela 1997; Kaski 1998; Sahlgren 2001, 2005; Blei, Ng, and Jordan 2003; V¨ayrynen and Honkela 2004, 2005; V¨ayrynen, Honkela, and Lindqvist 2007); 3) using clusters that are induced from distributional similarity (Brown et al. 1992; Pereira, Tishby, and Lee 1993; Martin, Liermann, and Ney 1998) as non-sparse features (Miller, Guinness, and Zamanian 2004; Ratinov and Roth 2009; Lin and Wu 2009; Candito and Crabbe 2009; Koo, Carreras, and Collins 2008; Suzuki et al. 2009; Zhao et al. 2009); and, recently, 4) neural network statistical language models (Bengio 2008; Bengio et al. 2003; Morin and Bengio 2005; Mnih, Yuecheng, and Hinton 2009; Mnih and Hinton 2007, 2009) as representations (Weston, Ratle, and Collobert 2008; Collobert and Weston 2008; Bengio et al. 2009). Our work is a form of distributional clustering for representations, but where previous work has used bigram and trigram statistics to form clusters, we build sophisticated models that attempt to capture the context of a word, and hence its similarity to other words, more precisely. Our experiments show that the ne"
J14-1004,J04-4004,0,\N,Missing
J14-1004,D09-1098,0,\N,Missing
K16-1031,P15-1061,0,0.0135941,"others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014)"
K16-1031,D11-1033,0,0.762012,"the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language model"
K16-1031,W15-3003,0,0.201493,"nually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language models (Mikolov et al., 2010). To train the in"
K16-1031,D12-1025,0,0.0209149,"phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wh"
K16-1031,P13-2119,0,0.586118,"main). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural network language models (Mikolov et al.,"
K16-1031,W09-0432,0,0.032288,"used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the in-domain data according to some criterion. The state-of-th"
K16-1031,P15-2030,0,0.0196367,"t. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson and Zhang, 2015b) extends their “one-hot” CNN in (Johnson and Zhang, 2015a) to take region embeddings trained on unlabeled dat"
K16-1031,W07-0717,0,0.0479909,"e and randomly select the same number of sentences from the general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data. Together with the labeled data, these word embed315 and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to"
K16-1031,D10-1044,0,0.135851,"Missing"
K16-1031,P13-1126,1,0.925672,"Missing"
K16-1031,2013.mtsummit-papers.23,1,0.779631,"general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data. Together with the labeled data, these word embed315 and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it"
K16-1031,2014.amta-researchers.10,1,0.823006,"Missing"
K16-1031,D13-1109,0,0.0494064,"Missing"
K16-1031,P15-1017,0,0.0161462,"a is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned fo"
K16-1031,N15-1011,0,0.463172,"there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent"
K16-1031,P11-2071,0,0.0602114,"Missing"
K16-1031,P15-1026,0,0.0312173,"odel to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson and Zhang, 2015b) extends their “one-hot” CNN in (Johnson"
K16-1031,P14-1062,0,0.357216,"augment the original model with semi-supervised convolutional neural networks for domain classification. Convolutional neural networks (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual paralle"
K16-1031,D14-1181,0,0.00450541,"tem. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes"
K16-1031,D14-1162,0,0.080122,"entence is represented as a sequence of d-dimensional vectors, which is the input to a convolution network that generates feature vectors for each text segment. The segment vectors and one-hot vectors are fed into another convolution layer, which outputs the classification labels. The second network is trained with the labeled indomain/out-domain data. Therefore, Equation 1 is replaced with: One-hot CNN When applying CNN to NLP tasks, the first layer of the network takes word embeddings as input. Word embeddings can be pre-trained using tools such as word2vec (Mikolov et al., 2013) or GloV e (Pennington et al., 2014), in which case a table lookup is enough. Alternatively, these vectors can be learned from scratch as a step in the network training process. When there are enough in-domain data, training in-domain word embeddings is meaningful. However, when the in-domain data are limited, the word embeddings learned from these data are unreliable. In this case, the input sentence x can be represented with one-hot vectors where each vector’s length is the vocabulary size, value 1 at index i indicates word i appears in the sentence, and 0 indicates its absence. A CNN with one-hot vector input is called “one-h"
K16-1031,W04-3250,0,0.234449,"lection by recurrent neural network LM, with the RNNLM Toolkit (Duh et al., 2013) 4. comblm: Data selection by the combined LM using ngram & rnnlm (equal weight) (Duh et al., 2013). All systems are trained with a standard phrasebased SMT system with standard settings, i.e., GIZA++ alignment, phrase table Kneser-Ney smoothing, hierarchical reordering models, target side 4-gram language model, “gigaword” 5-gram language model for systems with English as the target language, etc. 5.3 Experimental results We evaluated the system using BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. Table 2 summarizes the results and numbers of the selected sentences for each task. First, we can see that all the data selection methods improved the performance over the baseline “alldata” with much less 6 The code and scripts for the three baselines are available at http://cl.naist.jp/ kevinduh/a/acl2013/. 7 For small amounts of data, Witten-Bell smoothing had performed better than Kneser-Ney smoothing in our experiments. 319 alldata ngram rnnlm comblm ohcnn sscnn zh2en #sent BLEU 12.2M 22.9 300K 25.3** 300K 25.6** 400K 25.7"
K16-1031,2008.iwslt-papers.6,0,0.0537453,"on, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the in-domain data according to s"
K16-1031,E12-1055,0,0.0452885,"tences from the general-domain training data as the negative sample to form the training sample for training the CNN classification model. This is a typical supervised learning setting. To compensate the limit of in-domain data size, we use word2vec (Mikolov et al., 2013) to learn the word embedding from a large amount of general-domain data. Together with the labeled data, these word embed315 and out-domain data, respectively. Data weighting approaches weight each data item according to its relevance to the in-domain data. Mixture model adaptation (Foster and Kuhn, 2007; Foster et al., 2010; Sennrich, 2012; Foster et al., 2013) assumes that the general-domain data can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in"
K16-1031,J07-1003,0,0.030816,"as been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general-domain data, then it is used to translate in-domain monolingual data. The resulting target sentences or bilingual sentence pairs are then used as additional training data for language model or translation model training. Some early data selection approaches (Zhao et al., 2004; L¨u et al., 2007; Moore and Lewis, 2010) use in-domain monolingual data to select monolingual or bilingual data that are similar to the in-domain data"
K16-1031,P15-2058,0,0.124317,"ication. Convolutional neural networks (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domain monolingual data. An MT system is first trained on bilingual general"
K16-1031,D07-1036,0,0.150423,"Missing"
K16-1031,I08-2088,0,0.15319,"r performance by adapting the SMT system to the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram mode"
K16-1031,D09-1074,0,0.113093,"ta can be clustered to several sub-corpora, with some parts that are not too far from test domain. It combines sub-models trained on different sub-corpus data sets linearly or log-linearly with different weights. Vector space model adaptation (Chen et al., 2013) has the same assumption, and it weights each phrase pair based on vector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reaso"
K16-1031,N15-1091,0,0.0554186,"Missing"
K16-1031,C14-1220,0,0.0638379,"with semi-supervised convolutional neural networks for domain classification. Convolutional neural networks (LeCun and Bengio, 1998) are feed-forward neural networks that exploit the internal structure of data through convolution layers; each computation unit processes a small region of the input data. CNN has been very successful on image classification. When applying it to text input, the convolution layers process small regions of a document, i.e., a sequence of sentences or words. CNN has been gaining attention, and is now used in many text classification tasks (Kalchbrenner et al., 2014; Zeng et al., 2014; Johnson and Zhang, 2015b; Yin and Sch¨utze, 2015; Wang et al., 2015). 2 2.1 Related Work SMT adaptation techniques Domain adaptation to SMT systems has recently received considerable attention. Based on the availability of in-domain bilingual or monolingual training data, there are several adaptation scenarios. Different domain adaptation techniques, including self-training, data selection, data weighting, etc., have been developed for different scenarios. Self-training (Ueffing and Ney, 2007; Schwenk, 2008; Bertoldi and Federico, 2009) uses generaldomain bilingual parallel data and in-domai"
K16-1031,P10-2041,0,0.530225,"ting the SMT system to the test domain (in-domain). However, manually creating training data to match the test domain is not a preferred solution, because 1) sometimes the test domain is not known when training the model, and it could change from sentence to sentence; 2) even if the test domain is pre-determined, the resources required and slow turnaround in data collection process will still delay the system development process. Therefore, training data selection is widely used for domain adaptation in statistical machine translation (Zhao et al., 2004; L¨u et al., 2007; Yasuda et al., 2008; Moore and Lewis, 2010; Axelrod et al., 2011; Duh et al., 2013; Axelrod et al., 2015). Data selection techniques select monolingual or bilingual data that are similar to the indomain seed data based on some criteria, which are incorporated into the training data. The most successful data selection approaches (Moore and Lewis, 2010; Axelrod et al., 2011) train n-gram language models on in-domain text to select similar sentences from the large general-domain corpora according to the cross entropy. Furthermore, (Duh et al., 2013) obtained some gains by extending these approaches from n-gram models to recurrent neural"
K16-1031,P13-1140,0,0.0152568,"ector space model (VSM). (Chen et al., 2014) improved the VSM adaptation by extending it to distributed VSM and grouped VSM. Instance weighting adopts a rich set of features to compute weights for each instance in the training data; it can be applied to sentence pairs (Matsoukas et al., 2009) or phrase pairs (Foster et al., 2010). If in-domain comparable data are available, (Daume III and Jagarlamudi, 2011; Irvine et al., 2013) propose mining translations from the comparable data to translate out-of-vocabulary (OOV) words and capture new senses for the new test domains. (Dou and Knight, 2012; Zhang and Zong, 2013) learn bilingual lexical or phrase tables from in-domain monolingual data with a decipherment method, then incorporate them into the SMT system. All the above approaches assume that either there is an in-domain (mono-lingual, parallel, or comparable) data set with a reasonable size available, or that some sub-corpora are closer to the test domain than others. There is no previous work considering the scenario where only a tiny amount of in-domain data is available: this is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the"
K16-1031,P15-2060,0,0.0241725,"s is the scenario we address in this paper. 2.2 layers allow the model to find such local indicators, wherever they are in the text. Recently, CNNs have shown promising results on many text classification tasks, such as sentiment analysis (Kalchbrenner et al., 2014; Kim, 2014), topic and sentiment classification (Johnson and Zhang, 2015a; Johnson and Zhang, 2015b), paraphrase identification (Yin and Sch¨utze, 2015), entity relation type classification (Zeng et al., 2014; dos Santos et al., 2015), short-text classification (Wang et al., 2015), event extraction and detection (Chen et al., 2015; Nguyen and Grishman, 2015), question understanding and answering (Dong et al., 2015), and box-office prediction based on reviews (Bitvai and Cohn, 2015). Within the CNN architecture, people also use word embeddings for text classification. (Kalchbrenner et al., 2014) proposes a CNN framework with multiple convolution layers, with latent, dense and low-dimensional word embeddings as inputs. (Kim, 2014) defines a one-layer CNN architecture with comparable performance to (Kalchbrenner et al., 2014). The word embeddings input to the CNN can be pre-trained, and treated as fixed input, or tuned for a specific task. (Johnson"
K16-1031,C04-1059,0,0.201474,"Missing"
K16-1031,P02-1040,0,0.0970612,"Bell 7 smoothing (Axelrod et al., 2011) 3. rnnlm: Data selection by recurrent neural network LM, with the RNNLM Toolkit (Duh et al., 2013) 4. comblm: Data selection by the combined LM using ngram & rnnlm (equal weight) (Duh et al., 2013). All systems are trained with a standard phrasebased SMT system with standard settings, i.e., GIZA++ alignment, phrase table Kneser-Ney smoothing, hierarchical reordering models, target side 4-gram language model, “gigaword” 5-gram language model for systems with English as the target language, etc. 5.3 Experimental results We evaluated the system using BLEU (Papineni et al., 2002) score on the test set. Following (Koehn, 2004), we use the bootstrap resampling test to do significance testing. Table 2 summarizes the results and numbers of the selected sentences for each task. First, we can see that all the data selection methods improved the performance over the baseline “alldata” with much less 6 The code and scripts for the three baselines are available at http://cl.naist.jp/ kevinduh/a/acl2013/. 7 For small amounts of data, Witten-Bell smoothing had performed better than Kneser-Ney smoothing in our experiments. 319 alldata ngram rnnlm comblm ohcnn sscnn zh2en #sent BL"
P09-1056,P05-1001,0,0.039438,"a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007;"
P09-1056,P07-1094,0,0.0207273,"the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram,"
P09-1056,C04-1080,0,0.11927,"pervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the p"
P09-1056,D07-1031,0,0.0188999,"elation are of the appropriate type (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by u"
P09-1056,J93-2004,0,0.0419685,"Missing"
P09-1056,W06-1615,0,0.944029,"ed examples from which to estimate parameters. The negative effects of data sparsity have been well-documented in the NLP literature. The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001). Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daum´e III and Marcu, 2006; Chelba and Acero, 2004). We investigate the use of distributional representations, which model the probability distribution of a word’s context, as techniques for finding smoothed representations of word sequences. That is, we use the distributional representations to share information across unannotated examples of the same word type. We then compute features of the distributional representations, and provide them as input to our supervised sequence labelers. Our technique is particularly well-suited to handling data sparsity because it is possible to improve per"
P09-1056,W04-3237,0,0.259132,"negative effects of data sparsity have been well-documented in the NLP literature. The performance of state-of-the-art, supervised NLP systems like part-of-speech (POS) taggers degrades significantly on words that do not appear in the training data, or out-of-vocabulary (OOV) words (Lafferty et al., 2001). Performance also degrades when the domain of the test set differs from the domain of the training set, in part because the test set includes more OOV words and words that appear only a few times in the training set (henceforth, rare words) (Blitzer et al., 2006; Daum´e III and Marcu, 2006; Chelba and Acero, 2004). We investigate the use of distributional representations, which model the probability distribution of a word’s context, as techniques for finding smoothed representations of word sequences. That is, we use the distributional representations to share information across unannotated examples of the same word type. We then compute features of the distributional representations, and provide them as input to our supervised sequence labelers. Our technique is particularly well-suited to handling data sparsity because it is possible to improve performance on rare words by supplementing the training"
P09-1056,P96-1041,0,0.0174739,"unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially higher n-gram sequences to obtain accurate probability estimates in the face of data sparsity. Our task differs in that we are primarily concerned with the case where even the unigram model (single word) is rarely or never observed in the labeled training data. Sparsity for low-order contexts has recently spurred interest in using latent variables to represent distributions over contexts in language models. While n-gram models have traditionally dominated in language modeling, two recent efforts develop latent-variab"
P09-1056,N03-1028,0,0.211713,"Missing"
P09-1056,P07-1033,0,0.216518,"Missing"
P09-1056,P05-1044,0,0.36931,"Language Model Representation To take smoothing one step further, we present a technique that aggregates context distributions both for similar context words xi−1 = v and v 0 , and for similar words xi = w and w0 . Latent variable language models (LVLMs) can be used to produce just such a distributional representation. We use Hidden Markov Models (HMMs) as the main example in the discussion and as the LVLMs in our experiments, but the smoothing technique can be generalized to other forms of LVLMs, such as factorial HMMs and latent variable maximum entropy models (Ghahramani and Jordan, 1997; Smith and Eisner, 2005). An HMM is a generative probabilistic model that generates each word xi in the corpus conditioned on a latent variable Yi . Each Yi in the model takes on integral values from 1 to S, and each one is generated by the latent variable for the preceding word, Yi−1 . The distribution for a corpus x = (x1 , . . . , xN ) given a set of state vectors y = (y1 , . . . , yN ) is given by: 2.2 LSA Model One drawback of the multinomial representation is that it does not handle sparsity well enough, because the multinomial distributions themselves are so high-dimensional. For example, the two phrases “red"
P09-1056,W00-0726,0,0.384244,"Missing"
P09-1056,P07-1088,0,0.160039,"utperforms the baseline system on all chunks. No peak in performance is reached, so further improvements are possible with more unlabeled data. Thus smoothing is optimizing performance for the case where unlabeled data is plentiful and labeled data is scarce, as we would hope. 4 Related Work To our knowledge, only one previous system — the R EALM system for sparse information extrac501 tion — has used HMMs as a feature representation for other applications. R EALM uses an HMM trained on a large corpus to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). We extend and generalize this smoothing technique and apply it to common NLP applications involving supervised sequence-labeling, and we provide an in-depth empirical analysis of its performance. Several researchers have previously studied methods for using unlabeled data for tagging and chunking, either alone or as a supplement to labeled data. Ando and Zhang develop a semisupervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Sm"
P09-1056,C04-1004,0,0.0172326,"Recent projects in semisupervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. Unlike these systems, our efforts are aimed at using unlabeled data to find distributional representations that work well on rare terms, making the supervised systems more applicable to other domains and decreasing their sample complexity. HMMs have been used many times for POS tagging and chunking, in supervised, semisupervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007; Zhou, 2004). We take a novel perspective on the use of HMMs by using them to compute features of each token in the data that represent the distribution over that token’s contexts. Our technique lets the HMM find parameters that maximize cross-entropy, and then uses labeled data to learn the best mapping from the HMM categories to the POS categories. Smoothing in NLP usually refers to the problem of smoothing n-gram models. Sophisticated smoothing techniques like modified Kneser-Ney and Katz smoothing (Chen and Goodman, 1996) smooth together the predictions of unigram, bigram, trigram, and potentially hig"
P09-1105,P06-1002,0,0.0536995,"Missing"
P09-1105,N06-1013,0,0.135406,"Missing"
P09-1105,H05-1009,0,0.0609126,"Missing"
P09-1105,W08-0321,0,0.0193194,"a translated sentence is. (Gandrabur and Foster, 2003) used neural-net to improve the confidence estimate for text predictions in a machine-assisted translation tool. (Ueffing et al., 2003) presented several word-level confidence measures for machine translation based on word posterior probabilities. (Blatz et al., 2004) conducted extensive study incorporating various sentence-level and word-level features thru multilayer perceptron and naive Bayes algorithms for sentence and word confidence estimation. (Quirk, 2004) trained a sentence level confidence measure using a human annotated corpus. (Bach et al., 2008) used the sentence-pair confidence scores estimated with source and target language models to weight phrase translation pairs. However, there has been little research focusing on confi7 Conclusion In this paper we presented two alignment confidence measures for word alignment. The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se938 # phrase pairs Baseline ALF 939911 618179 TER 43.53 43.11 Average BLEU (TER-BLEU)/2 50.51 -3.49 50.24 -3.56 TER 53.14 51.75 BLEU 40.60 42.05 Tail (TER-BLEU)/2 6.27 4.85 Table 7: Improved Arabic-English Newswi"
P09-1105,C04-1046,0,0.060985,"e tail documents range from 0.6 to 1.4. On the whole test set the difference is smaller, 0.07 for the newswire translation and 0.58 for the web-blog translation. 6 Related Work In the machine translation area, most research on confidence measure focus on the confidence of MT output: how accurate a translated sentence is. (Gandrabur and Foster, 2003) used neural-net to improve the confidence estimate for text predictions in a machine-assisted translation tool. (Ueffing et al., 2003) presented several word-level confidence measures for machine translation based on word posterior probabilities. (Blatz et al., 2004) conducted extensive study incorporating various sentence-level and word-level features thru multilayer perceptron and naive Bayes algorithms for sentence and word confidence estimation. (Quirk, 2004) trained a sentence level confidence measure using a human annotated corpus. (Bach et al., 2008) used the sentence-pair confidence scores estimated with source and target language models to weight phrase translation pairs. However, there has been little research focusing on confi7 Conclusion In this paper we presented two alignment confidence measures for word alignment. The first is the sentence"
P09-1105,P06-1067,0,0.0865714,"the remaining sorted links. For each link {aij : c(aij ) &lt; H}, (a) Find the closest anchor link amn 2 , (b) Define the context window width w = |m − i |+ |n − j|. 1 H is selected to maximize the F-score on an alignment devset. 2 When two equally close alignment links have the same confidence score), we randomly select one of the tied links as the anchor link. 936 Figure 4: Alignment link filtering based on context-independent link confidence. Baseline +ALF Precision 72.66 78.14 Recall 66.17 64.36 F-score 69.26 70.59 and web-blog (WB). The MT system is a phrasebased SMT system as described in (Al-Onaizan and Papineni, 2006). The training data are bilingual sentence pairs with word alignment, from which we obtained phrase translation pairs. We extract phrase translation tables from the baseline MaxEnt word alignment as well as the alignment with confidence-based link filtering, then translate the test set with each phrase translation table. We measure the translation quality with automatic metrics including BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). The higher the BLEU score is, or the lower the TER score is, the better the translation quality is. We combine the two metrics into (TER-BLEU)/2 and"
P09-1105,J07-3002,0,0.141879,"Missing"
P09-1105,W03-0413,0,0.0582402,"Missing"
P09-1105,H05-1012,0,0.640631,"lignment confidence measure and alignment link confidence measure. Based on these measures, we improve the alignment quality by selecting high confidence sentence alignments and alignment links from multiple word alignments of the same sentence pair. Additionally, we remove low confidence alignment links from the word alignment of a bilingual training corpus, which increases the alignment F-score, improves Chinese-English and Arabic-English translation quality and significantly reduces the phrase translation table size. 1 Figure 1: An example of inaccurate translation and word alignment. and (Ittycheriah and Roukos, 2005)), which also introduce many word alignment errors. The example in Figure 1 shows the word alignment of the given Chinese and English sentence pair, where the English words following each Chinese word is its literal translation. We find untranslated Chinese and English words (marked with underlines). These spurious words cause significant word alignment errors (as shown with dash lines), which in turn directly affect the quality of phrase translation tables or translation rules that are learned based on word alignment. In this paper we introduce a confidence measure for word alignment, which i"
P09-1105,J03-1002,0,0.0664215,"ision, recall and F-score of individual alignments and the combined alignment. F-content and F-function are the F-scores for content words and function words, respectively. The link selection algorithm improves the recall over the best aligner (the ME alignment) by 7 points (from 65.4 to 72.5) while decreasing the precision by 4.4 points (from 73.6 to 69.2). Overall it improves the F-score by 1.5 points (from 69.3 to 70.8), 1.8 point improvement for content words and 1.0 point for function words. It also significantly outperforms the traditionally used heuristics, ”intersection-union-refine” (Och and Ney, 2003) by 6 points. 4 Improved MaxEnt Aligner with Confidence-based Link Filtering In addition to the alignment combination, we also improve the performance of the MaxEnt aligner through confidence-based alignment link filtering. Here we select the MaxEnt aligner because it has 935 HMM BM ME Link-Select Intersection-Union-Refine Precision 62.65 72.76 72.66 69.19 63.34 Recall 48.57 54.82 66.17 72.49 66.07 F-score 54.72 62.53 69.26 70.81 64.68 F-content 62.10 68.64 72.52 74.31 70.15 F-function 34.39 43.93 61.41 60.26 49.72 Table 2: Link Selection and Combination Results the highest F-measure among the"
P09-1105,2001.mtsummit-papers.68,0,0.0127371,"nt link confidence. Baseline +ALF Precision 72.66 78.14 Recall 66.17 64.36 F-score 69.26 70.59 and web-blog (WB). The MT system is a phrasebased SMT system as described in (Al-Onaizan and Papineni, 2006). The training data are bilingual sentence pairs with word alignment, from which we obtained phrase translation pairs. We extract phrase translation tables from the baseline MaxEnt word alignment as well as the alignment with confidence-based link filtering, then translate the test set with each phrase translation table. We measure the translation quality with automatic metrics including BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). The higher the BLEU score is, or the lower the TER score is, the better the translation quality is. We combine the two metrics into (TER-BLEU)/2 and try to minimize it. In addition to the whole test set’s scores, we also measure the scores of the ”tail” documents, whose (TER-BLEU)/2 scores are at the bottom 10 percentile (for A-E translation) and 20 percentile (for C-E translation) and are considered the most difficult documents to translate. Table 3: Confidence-based Alignment Link Filtering on C-E Alignment Baseline +ALF Precision 84.43 88.29 Recall 83.64 83.1"
P09-1105,quirk-2004-training,0,0.0454762,"a, most research on confidence measure focus on the confidence of MT output: how accurate a translated sentence is. (Gandrabur and Foster, 2003) used neural-net to improve the confidence estimate for text predictions in a machine-assisted translation tool. (Ueffing et al., 2003) presented several word-level confidence measures for machine translation based on word posterior probabilities. (Blatz et al., 2004) conducted extensive study incorporating various sentence-level and word-level features thru multilayer perceptron and naive Bayes algorithms for sentence and word confidence estimation. (Quirk, 2004) trained a sentence level confidence measure using a human annotated corpus. (Bach et al., 2008) used the sentence-pair confidence scores estimated with source and target language models to weight phrase translation pairs. However, there has been little research focusing on confi7 Conclusion In this paper we presented two alignment confidence measures for word alignment. The first is the sentence alignment confidence measure, based on which the best whole sentence alignment is se938 # phrase pairs Baseline ALF 939911 618179 TER 43.53 43.11 Average BLEU (TER-BLEU)/2 50.51 -3.49 50.24 -3.56 TER"
P09-1105,2006.amta-papers.25,0,0.024233,"F Precision 72.66 78.14 Recall 66.17 64.36 F-score 69.26 70.59 and web-blog (WB). The MT system is a phrasebased SMT system as described in (Al-Onaizan and Papineni, 2006). The training data are bilingual sentence pairs with word alignment, from which we obtained phrase translation pairs. We extract phrase translation tables from the baseline MaxEnt word alignment as well as the alignment with confidence-based link filtering, then translate the test set with each phrase translation table. We measure the translation quality with automatic metrics including BLEU (Papineni et al., 2001) and TER (Snover et al., 2006). The higher the BLEU score is, or the lower the TER score is, the better the translation quality is. We combine the two metrics into (TER-BLEU)/2 and try to minimize it. In addition to the whole test set’s scores, we also measure the scores of the ”tail” documents, whose (TER-BLEU)/2 scores are at the bottom 10 percentile (for A-E translation) and 20 percentile (for C-E translation) and are considered the most difficult documents to translate. Table 3: Confidence-based Alignment Link Filtering on C-E Alignment Baseline +ALF Precision 84.43 88.29 Recall 83.64 83.14 F-score 84.04 85.64 Table 4:"
P09-1105,2003.mtsummit-papers.52,0,0.0575108,"Missing"
P09-1105,C96-2141,0,0.373113,"r translation rules from large amount of bilingual data with word alignment. The quality of the parallel data and the word alignment have significant impacts on the learned translation models and ultimately the quality of translation output. Due to the high cost of commissioned translation, many parallel sentences are automatically extracted from comparable corpora, which inevitably introduce many ”noises”, i.e., inaccurate or non-literal translations. Given the huge amount of bilingual training data, word alignments are automatically generated using various algorithms ((Brown et al., 1994), (Vogel et al., 1996) 932 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 932–940, c Suntec, Singapore, 2-7 August 2009. 2009 ACL and AFNLP Aligner HMM BM MaxEnt the improved alignments also lead to better MT quality. The paper is organized as follows: In section 2 we introduce the sentence and alignment link confidence measures. In section 3 we demonstrate two approaches to improve alignment accuracy through alignment combination. In section 4 we show how to improve a MaxEnt word alignment quality by removing low confidence alignment links, which also leads to improved tra"
P09-1105,H05-1023,0,0.0145662,"eriah and Roukos, 2005). We evaluate per sentence alignment Fscores by comparing the system output with a reference alignment. For each sentence pair, we also calculate the sentence alignment confidence score − log C(A|S, T ). We compute the correlation coefficients between the alignment confidence measure and the alignment F-scores. The results in Figure 2 shows strong correlation between the confidence measure and the alignment F-score, with the correlation coefficients equals to -0.69. Such strong correlation is also observed on an HMM alignment (Ge, 2004) and a Block Model (BM) alignment (Zhao et al., 2005) with varying alignment accuracies, as seen in Table1. j=1 X 0 Sentence Alignment Selection Based on Confidence Measure The strong correlation between the sentence alignment confidence measure and the alignment F(4) j=1 i=1 933 the higher confidence the link has. Similarly, the target-to-source link posterior probability is defined as: qt2s (aij |T, S) = p(si |tj ) PI i0 =1 p(si0 |tj ) . (10) Intuitively, the above link confidence definition compares the lexical translation probability of the aligned word pair with the translation probabilities of all the target words given the source word. If"
P09-1105,J93-2003,0,\N,Missing
P09-1105,P02-1040,0,\N,Missing
P10-1099,D09-1002,0,0.0239089,"Missing"
P10-1099,P09-1004,0,0.095878,"e data for previously unseen predicates with semantic role information. This task differs from ours in that it focuses on previously unseen predicates, which may or may not be part of text from a new domain. Their techniques also result in relatively lower performance (F1 between 15 and 25), although their tests are on a more difficult and very different corpus. Weston et al. (2008) use deep learning techniques based on semi-supervised embeddings to improve an SRL system, though their tests are on in-domain data. Unsupervised SRL systems (Swier and Stevenson, 2004; Grenager and Manning, 2006; Abend et al., 2009) can naturally be ported to new domains with little trouble, but their accuracy thus far falls short of state-ofthe-art supervised and semi-supervised systems. 8 Conclusion and Future Work We have presented novel representation-learning techniques for building an open-domain SRL system. By incorporating learned features from HMMs and Span-HMMs trained on unlabeled text, our SRL system is able to correctly identify predicates in out-of-domain text with an F1 of 93.5, and it can identify and classify arguments to predicates with an F1 of 73.8, outperforming comparable state-of-the-art systems. O"
P10-1099,E09-1026,0,0.0282279,"Missing"
P10-1099,J02-3001,0,0.092328,"models, we can turn the word span models into representations by using the state value for a span as a feature in our supervised SRL system. Unlike path features, the features from our models of word spans consist of a single latent state value rather than a concatenation of state values, and as a consequence they tend to be much less sparse in the training data. tween a predicate and a potential argument. In standard SRL systems, these path features usually consist of a sequence of constituent parse nodes representing the shortest path through the parse tree between a word and the predicate (Gildea and Jurafsky, 2002). We substitute paths that do not depend on parse trees. We use four types of paths: word paths, POS paths, chunk paths, and HMM state paths. Given an input sentence labeled with POS tags, and chunks, we construct path features for a token wi by concatenating words (or tags or chunk labels) between wi and the predicate. For example, in the sentence “The HIV infection rate is expected to peak in 2010,” the word path between “rate” and predicate “peak” would be “is expected to”, and the POS path would be “VBZ VBD TO.” Since word, POS, and chunk paths are all subject to data sparsity for argument"
P10-1099,W01-0521,0,0.0320864,"ken that is part of an argument with a class label, such as Arg0 or ArgM. Following argument classification, multi-word arguments may have different classification labels for each token. We post-process the labels by changing them to match the label of the first token. We use CRFs as our models for both tasks (Cohn and Blunsom, 2005). Most previous approaches to SRL have relied heavily on parsers, and especially constituency parsers. Indeed, when SRL systems use gold standard parses, they tend to perform extremely well (Toutanova et al., 2008). However, as several previous studies have noted (Gildea, 2001; Pradhan et al., 2007), using parsers can cause problems for open-domain SRL. The parsers themselves may not port well to new domains, or the features they generate for SRL may not be stable across domains, and therefore may cause sparse data problems on new domains. Our first step is therefore to build an SRL system that relies on partial parsing, as was done in CoNLL 2004 (Carreras and M`arquez, 2004). We then gradually add in lesssparse alternatives for the syntactic features that previous systems derive from parse trees. 5.1 Incorporating HMM-based Representations As a first step towards"
P10-1099,W06-1601,0,0.0183157,"ues to automatically annotate data for previously unseen predicates with semantic role information. This task differs from ours in that it focuses on previously unseen predicates, which may or may not be part of text from a new domain. Their techniques also result in relatively lower performance (F1 between 15 and 25), although their tests are on a more difficult and very different corpus. Weston et al. (2008) use deep learning techniques based on semi-supervised embeddings to improve an SRL system, though their tests are on in-domain data. Unsupervised SRL systems (Swier and Stevenson, 2004; Grenager and Manning, 2006; Abend et al., 2009) can naturally be ported to new domains with little trouble, but their accuracy thus far falls short of state-ofthe-art supervised and semi-supervised systems. 8 Conclusion and Future Work We have presented novel representation-learning techniques for building an open-domain SRL system. By incorporating learned features from HMMs and Span-HMMs trained on unlabeled text, our SRL system is able to correctly identify predicates in out-of-domain text with an F1 of 93.5, and it can identify and classify arguments to predicates with an F1 of 73.8, outperforming comparable state-"
P10-1099,W06-1615,0,0.258417,"Span-HMM-Base10 paths achieved far better success in this respect. Importantly, the improvement is mostly due to features that are seen often in training, rather than features that were seen just once or twice. Thus Span975 Predicate picked passed come sat Span B-Arg the things up through the barbed wire down from Sundays over his second rock from at to in The disparity in performance between indomain and out-of-domain tests is by no means restricted to SRL. Past research in a variety of NLP tasks has shown that parsers (Gildea, 2001), chunkers (Huang and Yates, 2009), part-of-speech taggers (Blitzer et al., 2006), named-entity taggers (Downey et al., 2007a), and word sense disambiguation systems (Escudero et al., 2000) all suffer from a similar drop-off in performance on out-of-domain tests. Numerous domain adaptation techniques have been developed to address this problem, including self-training (McClosky et al., 2006) and instance weighting (Bacchiani et al., 2006) for parser adaptation and structural correspondence learning for POS tagging (Blitzer et al., 2006). Of these techniques, structural correspondence learning is closest to our technique in that it is a form of representation learning, but"
P10-1099,P09-1056,1,0.854038,"y of domains. Recent theoretical and empirical evidence suggests that the fault for poor performance on out-ofdomain tests lies with the representations, or sets of features, traditionally used in supervised NLP. Building on recent efforts in domain adaptation, we develop unsupervised techniques for learning new representations of text. Using latent-variable language models, we learn representations of texts that provide novel kinds of features to our supervised learning algorithms. Similar representations have proven useful in domain-adaptation for part-of-speech tagging and phrase chunking (Huang and Yates, 2009). We demonstrate how to learn representations that are effective for SRL. Experiments on out-of-domain test sets show that our learned representations can dramatically improve out-of-domain performance, and narrow the gap between in-domain and out-of-domain performance by half. The next section provides background information on learning representations for NLP tasks using latent-variable language models. Section 3 presents our experimental setup for testing opendomain SRL. Sections 4, 5, 6 describe our SRL system: first, how we identify predicates in opendomain text, then how our baseline tec"
P10-1099,W04-2412,0,0.083457,"Missing"
P10-1099,W05-0620,0,0.217225,"Missing"
P10-1099,W05-0622,0,0.014061,". During argument identification, the system must label each token with labels that indicate either the beginning or interior of an argument (B-Arg or I-Arg), or a label that indicates the token is not part of an argument (O-Arg). During argument classification, the system labels each token that is part of an argument with a class label, such as Arg0 or ArgM. Following argument classification, multi-word arguments may have different classification labels for each token. We post-process the labels by changing them to match the label of the first token. We use CRFs as our models for both tasks (Cohn and Blunsom, 2005). Most previous approaches to SRL have relied heavily on parsers, and especially constituency parsers. Indeed, when SRL systems use gold standard parses, they tend to perform extremely well (Toutanova et al., 2008). However, as several previous studies have noted (Gildea, 2001; Pradhan et al., 2007), using parsers can cause problems for open-domain SRL. The parsers themselves may not port well to new domains, or the features they generate for SRL may not be stable across domains, and therefore may cause sparse data problems on new domains. Our first step is therefore to build an SRL system tha"
P10-1099,P06-1043,0,0.0251736,"rom Sundays over his second rock from at to in The disparity in performance between indomain and out-of-domain tests is by no means restricted to SRL. Past research in a variety of NLP tasks has shown that parsers (Gildea, 2001), chunkers (Huang and Yates, 2009), part-of-speech taggers (Blitzer et al., 2006), named-entity taggers (Downey et al., 2007a), and word sense disambiguation systems (Escudero et al., 2000) all suffer from a similar drop-off in performance on out-of-domain tests. Numerous domain adaptation techniques have been developed to address this problem, including self-training (McClosky et al., 2006) and instance weighting (Bacchiani et al., 2006) for parser adaptation and structural correspondence learning for POS tagging (Blitzer et al., 2006). Of these techniques, structural correspondence learning is closest to our technique in that it is a form of representation learning, but it does not learn features for word spans. None of these techniques have been successfully applied to SRL. Table 7: Example spans labeled with the same Span-HMM state. The examples are taken from sentences where the Span-HMM-Base10 model correctly identified the argument on the right, but the Baseline+HMM+Paths"
P10-1099,J05-1004,0,0.0159855,"rize tokens according to distributional similarity. And second, it would be difficult to tell two domains apart based on their HMM labels, since the same HMM state can generate similar words from a variety of domains. In what follows, we adapt these representationlearning concepts to open-domain SRL. 3 Experimental Setup We test our open-domain semantic role labeling system using data from the CoNLL 2005 shared task (Carreras and M`arquez, 2005). We use the standard training set, consisting of sections 02-21 of the Wall Street Journal (WSJ) portion of the Penn Treebank, labeled with PropBank (Palmer et al., 2005) annotations for predicates and arguments. We perform our tests on the Brown corpus (Kucera and Francis, 1967) test data from CoNLL 2005, consisting of 3 sections (ck01-ck03) of propbanked Brown corpus data. This test set consists of 426 sentences containing 7,159 tokens, 804 propositions, and 2,177 arguments. While the 969 training data contains newswire text, the test sentences are drawn from the domain of “general fiction,” and contain an entirely different style (or styles) of English. The data also includes a second test set of in-domain text (section 23 of the Treebank), which we refer t"
P10-1099,D09-1003,0,0.248298,"Missing"
P10-1099,W05-0634,0,0.0634188,"Missing"
P10-1099,P07-1088,0,0.280272,"cess in this respect. Importantly, the improvement is mostly due to features that are seen often in training, rather than features that were seen just once or twice. Thus Span975 Predicate picked passed come sat Span B-Arg the things up through the barbed wire down from Sundays over his second rock from at to in The disparity in performance between indomain and out-of-domain tests is by no means restricted to SRL. Past research in a variety of NLP tasks has shown that parsers (Gildea, 2001), chunkers (Huang and Yates, 2009), part-of-speech taggers (Blitzer et al., 2006), named-entity taggers (Downey et al., 2007a), and word sense disambiguation systems (Escudero et al., 2000) all suffer from a similar drop-off in performance on out-of-domain tests. Numerous domain adaptation techniques have been developed to address this problem, including self-training (McClosky et al., 2006) and instance weighting (Bacchiani et al., 2006) for parser adaptation and structural correspondence learning for POS tagging (Blitzer et al., 2006). Of these techniques, structural correspondence learning is closest to our technique in that it is a form of representation learning, but it does not learn features for word spans."
P10-1099,N07-1070,0,0.0123332,"rt of an argument with a class label, such as Arg0 or ArgM. Following argument classification, multi-word arguments may have different classification labels for each token. We post-process the labels by changing them to match the label of the first token. We use CRFs as our models for both tasks (Cohn and Blunsom, 2005). Most previous approaches to SRL have relied heavily on parsers, and especially constituency parsers. Indeed, when SRL systems use gold standard parses, they tend to perform extremely well (Toutanova et al., 2008). However, as several previous studies have noted (Gildea, 2001; Pradhan et al., 2007), using parsers can cause problems for open-domain SRL. The parsers themselves may not port well to new domains, or the features they generate for SRL may not be stable across domains, and therefore may cause sparse data problems on new domains. Our first step is therefore to build an SRL system that relies on partial parsing, as was done in CoNLL 2004 (Carreras and M`arquez, 2004). We then gradually add in lesssparse alternatives for the syntactic features that previous systems derive from parse trees. 5.1 Incorporating HMM-based Representations As a first step towards an open-domain represen"
P10-1099,W00-1322,0,0.0294305,"Missing"
P10-1099,J08-2005,0,0.0726922,"Missing"
P10-1099,J08-2002,0,0.0901635,"Missing"
P10-1099,W04-3213,0,\N,Missing
P11-1022,2007.iwslt-1.4,1,0.943922,"on correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. 1 Introduction State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs. In particular, statistical machine translation systems (Koehn et al., 2007; Bach et al., 2007; Shen et al., 2008) have advanced to a state that the translation quality for certain language pairs (e.g. SpanishEnglish, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users. However, a remaining open question is how to predict confidence scores for machine translated words and sentences. An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whole sentence. Such in"
P11-1022,2009.mtsummit-papers.1,1,0.813646,"Missing"
P11-1022,C04-1046,0,0.81907,"y of machine translation in many areas. For example, a post-editor would like to quickly identify which sentences might be incorrectly translated and in need of correction. Other areas, such as cross-lingual question-answering, information extraction and retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) in"
P11-1022,N09-1025,0,0.0223261,"To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg minwt+1 ||wt+1 − wt || (2) s.t. score(x, y) ≥ score(x, y 0 ) + L(y, y 0 ) where L(y, y 0 ) is a measure of the loss of using y 0 instead of the true label y. In this problem L(y, y 0 ) is 0-1 loss function. More specifically, for each instance xi in the training data at a time t we find the label with the highest score: y 0 = arg max score(xi , y) (3) y the weight vector is updated as follow wt+1 = wt + τ (f (xi , y) − f (xi , y 0 )) (4) τ can be interpreted as a step size; when τ is a large numbe"
P11-1022,W09-0431,0,0.0168581,"Missing"
P11-1022,2008.amta-srw.3,0,0.00817602,"Such information is vital ∗ Work done during an internship at IBM T.J. Watson 211 Research Center to realize the utility of machine translation in many areas. For example, a post-editor would like to quickly identify which sentences might be incorrectly translated and in need of correction. Other areas, such as cross-lingual question-answering, information extraction and retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target"
P11-1022,P09-1105,1,0.816815,"ource alignment context feature: We anchor the target word and derive context features surrounding its source word. For example, in Figure 2a and 2b we have an alignment between “tshyr” and “refers” The source contexts “tshyr” with a window of one word are “ayda” to the left and “aly” to the right. ( DT Source & Target Dependency (b) Structures Child-Father agreement WPP: 1.0 0.67 1.0 1.0 1.0 0.67 … IBM Model-1 feature but also the surrounding alignment context. The key intuition is that collocation is a reliable indicator for judging if a target word is generated by a particular source word (Huang, 2009). Moreover, the IBM Model-1 feature was already used in several steps of a translation system such as word alignment, phrase extraction and scoring. Also the impact of this feature alone might fade away when the MT system is scaled up. We obtain word-to-word alignments by applying IBM Model-1 to bilingual phrase pairs that generated the MT output. The IBM Model-1 assumes one target word can only be aligned to one source word. Therefore, given a target word we can always identify which source word it is aligned to. IN if source-POS = “VBP” and target-context = “to” otherwise 214 3.3 Source and"
P11-1022,H05-1012,0,0.0147024,"Experiments The SMT engine is a phrase-based system similar to the description in (Tillmann, 2006), where various features are combined within a log-linear framework. These features include source-to-target phrase translation score, source-to-target and target-to-source wordto-word translation scores, language model score, distortion model scores and word count. The training data for these features are 7M Arabic-English sentence pairs, mostly newswire and UN corpora released by LDC. The parallel sentences have word alignment automatically generated with HMM and MaxEnt word aligner (Ge, 2004; Ittycheriah and Roukos, 2005). Bilingual phrase translations are extracted from these word-aligned parallel corpora. The language model is a 5-gram model trained on roughly 3.5 billion English words. Our training data contains 72k sentences ArabicEnglish machine translation with human corrections which include of 2.2M words in newswire and weblog domains. We have a development set of 2,707 sentences, 80K words (dev); an unseen test set of 2,707 sentences, 79K words (test). Feature selection and parameter tuning has been done on the development set in which we experimented values of C, n and iterations in range of [0.5:10]"
P11-1022,P07-2045,0,0.0043906,"n F-score. The Pearson correlation between the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. 1 Introduction State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs. In particular, statistical machine translation systems (Koehn et al., 2007; Bach et al., 2007; Shen et al., 2008) have advanced to a state that the translation quality for certain language pairs (e.g. SpanishEnglish, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users. However, a remaining open question is how to predict confidence scores for machine translated words and sentences. An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whol"
P11-1022,W08-0409,0,0.0226703,"Missing"
P11-1022,de-marneffe-etal-2006-generating,0,0.00667893,"Missing"
P11-1022,H05-1124,0,0.0100386,"T output. 2.2 Word-level model In our problem, a training instance is a word from MT output, and its label when the MT sentence is aligned with the human correction. Given a training instance x, y is the true label of x; f stands for its feature vector f (x, y); and w is feature weight vector. We define a feature-rich classifier score(x, y) as follow score(x, y) = w.f (x, y) (1) To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg minwt+1 ||wt+1 − wt || (2) s.t. score(x, y) ≥ score(x, y 0 ) + L(y, y 0 ) where L(y, y 0 ) is a measure of the loss of usi"
P11-1022,P02-1040,0,0.108439,"ed many training examples and to train discriminatively we need to search through all possible translations of each training example. Another issue of previous work was that they are all trained with BLEU/TER score computing against Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211–219, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics the translation references which is different from predicting the human-targeted translation edit rate (HTER) which is crucial in post-editing applications (Snover et al., 2006; Papineni et al., 2002). Finally, the backtranslation approach faces a serious issue when forward and backward translation models are symmetric. In this case, back-translation will not be very informative to indicate forward translation quality. In this paper, we predict error types of each word in the MT output with a confidence score, extend it to the sentence level, then apply it to n-best list reranking task to improve MT quality, and finally design a visualization prototype. We try to answer the following questions: • Can we use a rich feature set such as sourceside information, alignment context, and dependenc"
P11-1022,quirk-2004-training,0,0.415982,"extraction and retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2"
P11-1022,2007.mtsummit-papers.54,0,0.0694223,"d retrieval, can also benefit from the confidence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it tr"
P11-1022,P08-1066,0,0.185309,"een the proposed confidence measure and the human-targeted translation edit rate (HTER) is 0.6. Improvements between 0.4 and 0.9 TER reduction are obtained with the n-best list reranking task using the proposed confidence measure. Also, we present a visualization prototype of MT errors at the word and sentence levels with the objective to improve post-editor productivity. 1 Introduction State-of-the-art Machine Translation (MT) systems are making progress to generate more usable translation outputs. In particular, statistical machine translation systems (Koehn et al., 2007; Bach et al., 2007; Shen et al., 2008) have advanced to a state that the translation quality for certain language pairs (e.g. SpanishEnglish, French-English, Iraqi-English) in certain domains (e.g. broadcasting news, force-protection, travel) is acceptable to users. However, a remaining open question is how to predict confidence scores for machine translated words and sentences. An MT system typically returns the best translation candidate from its search space, but still has no reliable way to inform users which word is likely to be correctly translated and how confident it is about the whole sentence. Such information is vital ∗"
P11-1022,2006.amta-papers.25,0,0.0620368,"n many features we need many training examples and to train discriminatively we need to search through all possible translations of each training example. Another issue of previous work was that they are all trained with BLEU/TER score computing against Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 211–219, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics the translation references which is different from predicting the human-targeted translation edit rate (HTER) which is crucial in post-editing applications (Snover et al., 2006; Papineni et al., 2002). Finally, the backtranslation approach faces a serious issue when forward and backward translation models are symmetric. In this case, back-translation will not be very informative to indicate forward translation quality. In this paper, we predict error types of each word in the MT output with a confidence score, extend it to the sentence level, then apply it to n-best list reranking task to improve MT quality, and finally design a visualization prototype. We try to answer the following questions: • Can we use a rich feature set such as sourceside information, alignmen"
P11-1022,P10-1063,0,0.261006,"put. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the source language to see whether t"
P11-1022,2009.mtsummit-papers.16,0,0.05431,"dence scores of MT output. Finally, even MT systems can leverage such information to do n-best list reranking, discriminative phrase table and rule filtering, and constraint decoding (Hildebrand and Vogel, 2008). Numerous attempts have been made to tackle the confidence estimation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the sou"
P11-1022,W06-3602,0,0.0208021,", prepositional modifier, and prepositional complement, are having child-father agreements. Children agreement: In the child-father agreement feature we look up in the dependency tree, however, we also can look down to the dependency tree with a similar motivation. Essentially, given an alignment between a source word si and a target word tj , how many children of si and tj are aligned together? For example, “tshyr” and “refers” have 2 aligned children which are “ayda-also” and “aly-to” as shown in Figure 3c. 4 Experiments The SMT engine is a phrase-based system similar to the description in (Tillmann, 2006), where various features are combined within a log-linear framework. These features include source-to-target phrase translation score, source-to-target and target-to-source wordto-word translation scores, language model score, distortion model scores and word count. The training data for these features are 7M Arabic-English sentence pairs, mostly newswire and UN corpora released by LDC. The parallel sentences have word alignment automatically generated with HMM and MaxEnt word aligner (Ge, 2004; Ittycheriah and Roukos, 2005). Bilingual phrase translations are extracted from these word-aligned"
P11-1022,J07-1003,0,0.60481,"rk of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the source language to see whether the output of backward translation matches the original source sentence. However, previous studies had a few shortcomings. First, source-side features were not extensively investigated. Blatz et al.(2004) only investigated source ngram frequency statistics and source lan"
P11-1022,D07-1080,0,0.0226156,"x, y) = w.f (x, y) (1) To obtain the label, we choose the class with the highest score as the predicted label for that data instance. To learn optimized weights, we use the Margin Infused Relaxed Algorithm or MIRA (Crammer and Singer, 2003; McDonald et al., 2005) which is an online learner closely related to both the support vector machine and perceptron learning framework. MIRA has been shown to provide state-of-the-art performance for sequential labelling task (Rozenfeld et al., 2006), and is also able to provide an efficient mechanism to train and optimize MT systems with lots of features (Watanabe et al., 2007; Chiang et al., 2009). In general, weights are updated at each step time t according to the following rule: wt+1 = arg minwt+1 ||wt+1 − wt || (2) s.t. score(x, y) ≥ score(x, y 0 ) + L(y, y 0 ) where L(y, y 0 ) is a measure of the loss of using y 0 instead of the true label y. In this problem L(y, y 0 ) is 0-1 loss function. More specifically, for each instance xi in the training data at a time t we find the label with the highest score: y 0 = arg max score(xi , y) (3) y the weight vector is updated as follow wt+1 = wt + τ (f (xi , y) − f (xi , y 0 )) (4) τ can be interpreted as a step size; w"
P11-1022,P10-1062,0,0.663538,"ation problem. The work of Blatz et al. (2004) is perhaps the best known study of sentence and word level features and their impact on translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009). Soricut and Echihabi (2010) developed regression models which are used to predict the expected BLEU score of a given translation hypothesis. Improvement also can be obtained by using target part-of-speech and null dependency link in a MaxEnt classifier (Xiong et al., 2010). Ueffing and Ney (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. From the usability point of view, back-translation is a tool to help users to assess the accuracy level of MT output (Bach et al., 2007). Literally, it translates backward the MT output into the source language to see whether the output of backward translation matches the original source sentence. However, previous studies had a few shortcomings. First, source-side features were not extensively investigated. Blatz et al.(2004) only investigated source ngram frequency s"
P13-2069,H05-1012,0,0.0810006,"Missing"
P13-2069,P08-1066,0,0.0901454,"Missing"
P13-2069,P06-1067,0,0.0620788,"Missing"
P13-2069,N04-4026,0,0.198554,"Missing"
P13-2069,D07-1077,0,0.0553185,"Missing"
P13-2069,C10-1126,0,0.101745,"Missing"
P13-2069,C04-1073,0,0.212374,"Missing"
P13-2069,P06-1077,0,\N,Missing
P13-2069,P01-1067,0,\N,Missing
P13-2069,P05-1033,0,\N,Missing
P13-2069,N03-1017,0,\N,Missing
P13-2069,P05-1066,0,\N,Missing
P13-2069,W06-3119,0,\N,Missing
P13-2069,W06-3602,0,\N,Missing
P13-2069,D11-1125,0,\N,Missing
P14-1081,C04-1046,0,0.317297,"y measurement for MT post editing since the reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whe"
P14-1081,W12-3102,0,0.110492,"Missing"
P14-1081,W12-3110,0,0.0124939,"e ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time to analyze ∗ This work was done when the author was with IBM Research. 861 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 861–870, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics source side of the MT training corpus, which, combined with features from translation output, achieved significantly superior performance in the MT QE evaluation. Felice and Specia (2012) investigates the impact of a large set of linguistically inspired features on quality estimation accuracy, which are not able to outperform the shallower features based on word statistics. Gonz´alezRubio et al. (2013) proposed a principled method for performing regression for quality estimation using dimensionality reduction techniques based on partial least squares regression. Given the feature redundancy in MT QE, their approach is able to improve prediction accuracy while significantly reducing the size of the feature sets. it can not be adapted to provide accurate estimates on the outputs"
P14-1081,D10-1044,0,0.0622853,"Missing"
P14-1081,D11-1125,0,0.0213707,"e pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-of-the-art translation performance in various Arabic-English translation evaluations (NIST MT2008, GALE and BOLT projects). 4 4.1 Features for MT QE The features for quality estimation should reflect the complexity of the source sentence and the decoding process. Therefore we conduct syntactic analysis on the source sentences, extract features from the decoding process and select the following 26 features: • 17 decoding features, including phrase translation probabilities (source-to-target and target-to-source), word translation probabilities (also"
P14-1081,H05-1012,1,0.697822,"62 Figure 1: Adaptive QE for document-specific MT system. score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with we"
P14-1081,P11-1022,1,0.890138,"-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time to analyze ∗ This work was done when the author"
P14-1081,N07-1008,1,0.754885,"f the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether the decoder incorporates 17 features with weights estimated by PRO (Hopkins and May, 2011) in the decoding process, and achieves state-"
P14-1081,C96-2141,0,0.0501762,"of the source sentence to the 862 Figure 1: Adaptive QE for document-specific MT system. score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall system built pipeline. 3.1 MT Decoder The MT decoder (Ittycheriah and Roukos, 2007) employed in our study extracts various features (source words, morphemes and POS tags, target words and POS tags, etc.) with their weights trained in a maximum entropy framework. These features are combined with other features used in a typical phrase-based translation system. Altogether th"
P14-1081,D09-1074,0,0.0224513,"utomatically segmented into sentences, which are also called segments. Thus in the rest of the paper we will use sentences and segments interchangeably. Our parallel corpora includes tens of millions of sentence pairs covering a wide range of topics. Building a general MT system using all the parallel data not only produces a huge translation model (unless with very aggressive pruning), the performance on the given input document is suboptimal due to the unwanted dominance of out-of-domain data. Past research suggests using weighted sentences or corpora for domain adaptation (Lu et al., 2007; Matsoukas et al., 2009; Foster et al., 2010). Here we adopt the same strategy, building a documentspecific translation model for each input document. The document-specific system is built based on sub-sampling: from the parallel corpora we select sentence pairs that are the most similar to the sentences from the input document, then build the MT system with the sub-sampled sentence pairs. The similarity is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the sta"
P14-1081,P10-1062,0,0.155701,"from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time"
P14-1081,quirk-2004-training,0,0.0256409,"is defined as the number of n-grams that appear in both source sentences, divided by the input sentence’s length, with higher weights assigned to longer n-grams. From the extracted sentence pairs, we utilize the standard pipeline in SMT system building: word alignRelated Work There has been a long history of study in confidence estimation of machine translation. The work of (Blatz et al., 2004) is among the best known study of sentence and word level features for translation error prediction. Along this line of research, improvements can be obtained by incorporating more features as shown in (Quirk, 2004; Sanchis et al., 2007; Raybaud et al., 2009; Specia et al., 2009b). Soricut and Echihabi (2010b) proposed various regression models to predict the expected BLEU score of a given sentence translation hypothesis. Ueffing and Hey (2007) introduced word posterior probabilities (WPP) features and applied them in the n-best list reranking. Target part-of-speech and null dependency link are exploited in a MaxEnt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Calliso"
P14-1081,E12-3001,0,0.0259277,"Missing"
P14-1081,2006.amta-papers.25,0,0.0852742,"nt classifier to improve the MT quality estimation (Xiong et al., 2010). Quality estimation focusing on MT post-editing has been an active research topic, especially after the WMT 2012 (Callison-Burch et al., 2012) and WMT2013 (Bojar et al., 2013) workshops with the “Quality Estimation” shared task. Bic¸ici et al. (2013) proposes a number of features measuring the similarity of the source sentence to the 862 Figure 1: Adaptive QE for document-specific MT system. score or translation error rate of the translated sentences or documents based on a set of features. In this work, we adopt HTER in (Snover et al., 2006) as our prediction output. HTER measures the percentage of insertions, deletions, substitutions and shifts needed to correct the MT outputs. In the rest of the paper, we use TER and HTER interchangably. In this section we will first introduce the set of features, and then discuss MT QE problem from classification and regression point of views. ment (HMM (Vogel et al., 1996) and MaxEnt (Ittycheriah and Roukos, 2005) alignment models, phrase pair extraction, MT model training (Ittycheriah and Roukos, 2007) and LM model training. The top region within the dashed line in Figure 1 shows the overall"
P14-1081,P10-1063,0,0.25206,"on of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output unless they spend time to analyze ∗ This work was"
P14-1081,2009.mtsummit-papers.16,0,0.171072,"reference is obtained from human correction of the MT output. Document-specific MT model is an MT model that is specifically built for the given input document. It is demonstrated in (Roukos et al., 2012) that document-specific MT models significantly improve the translation quality. However, this raises two issues for quality estimation. First, existing approaches to MT quality estimation rely on lexical and syntactical features defined over parallel sentence pairs, which includes source sentences, MT outputs and references, and translation models (Blatz et al., 2004; Ueffing and Ney, 2007; Specia et al., 2009a; Xiong et al., 2010; Soricut and Echihabi, 2010a; Bach et al., 2011). Therefore, when the MT quality estimation model is trained, Introduction Machine translation (MT) systems suffer from an inconsistent and unstable translation quality. Depending on the difficulty of the input sentences (sentence length, OOV words, complex sentence structures and the coverage of the MT system’s training data), some translation outputs can be perfect, while others are ungrammatical, missing important words or even totally garbled. As a result, users do not know whether they can trust the translation output u"
P14-1081,D07-1036,0,\N,Missing
P14-1081,W13-2201,0,\N,Missing
W03-1502,J93-2003,0,\N,Missing
W03-1502,C96-2141,1,\N,Missing
W03-1502,W98-1005,0,\N,Missing
W03-1502,P00-1004,1,\N,Missing
W03-1502,J00-2004,0,\N,Missing
W03-1502,P02-1051,0,\N,Missing
W03-1502,A97-1029,0,\N,Missing
W10-2604,P05-1001,0,0.0160404,"d up the process and make domain adaptation much more accessible. Finally, there are cases where small amounts of labeled data are available for new domains; models that combine our representation learning approach with instance weighting and other forms of supervised domain adaptation may take better advantage of these cases. Several researchers have previously studied methods for using unlabeled data for sequence labeling, either alone or as a supplement to labeled data. Ando and Zhang develop a semi-supervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semi-supervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. HMMs have been used many times for POS tagging in supervised, semi-supervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007). The R EALM system for sparse information extraction has also used unsupervised HMMs to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). Sch¨utze (1994) has presented an algorithm that cat"
W10-2604,C04-1080,0,0.0639902,"r advantage of these cases. Several researchers have previously studied methods for using unlabeled data for sequence labeling, either alone or as a supplement to labeled data. Ando and Zhang develop a semi-supervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semi-supervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. HMMs have been used many times for POS tagging in supervised, semi-supervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007). The R EALM system for sparse information extraction has also used unsupervised HMMs to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). Sch¨utze (1994) has presented an algorithm that categorizes word tokens in context instead of word types for tagging words. We take a novel perspective on the use of unsupervised latent-variable models by using them to compute features of each token that represent the distribution over that token’s contexts. These features prove to be highly useful f"
W10-2604,P07-1094,0,0.0260299,"ses. Several researchers have previously studied methods for using unlabeled data for sequence labeling, either alone or as a supplement to labeled data. Ando and Zhang develop a semi-supervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semi-supervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. HMMs have been used many times for POS tagging in supervised, semi-supervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007). The R EALM system for sparse information extraction has also used unsupervised HMMs to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). Sch¨utze (1994) has presented an algorithm that categorizes word tokens in context instead of word types for tagging words. We take a novel perspective on the use of unsupervised latent-variable models by using them to compute features of each token that represent the distribution over that token’s contexts. These features prove to be highly useful for supervised sequence labelers"
W10-2604,P09-1056,1,0.590699,"Missing"
W10-2604,D07-1031,0,0.0375738,", there are cases where small amounts of labeled data are available for new domains; models that combine our representation learning approach with instance weighting and other forms of supervised domain adaptation may take better advantage of these cases. Several researchers have previously studied methods for using unlabeled data for sequence labeling, either alone or as a supplement to labeled data. Ando and Zhang develop a semi-supervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semi-supervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. HMMs have been used many times for POS tagging in supervised, semi-supervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007). The R EALM system for sparse information extraction has also used unsupervised HMMs to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). Sch¨utze (1994) has presented an algorithm that categorizes word tokens in context instead of word types for tagging"
W10-2604,J93-2004,0,0.0345588,"Missing"
W10-2604,N06-1020,0,0.0115479,"l to find a different local maximum in parameter space during training (and thus a different model of the observation sequence), we initialize the parameters randomly. Suppose there are L independent layers in an IHMM model for corpus x = (x1 , . . . , xN ), and l ), where l = 1...L and each layer is (y1l ,y2l ,...yN each y can have K states. The distribution of the corpus and one hidden layer l is Y l P (xi |yil )P (yil |yi−1 P (x, yl ) = ) • T EST-CRF: Our baseline model, trained and tested on the test data. This is our upper bound. • S ELF -CRF: Following the self-training paradigm (e.g., (McClosky et al., 2006b; McClosky et al., 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. We perform only one iteration of retraining, although in general multiple iterations are possible, usually with diminishing marginal returns. 4 Multi-dimensional Representations From a linguistic perspective, words are multidimensional objects. For instance, the word “we” in “We like doing domain adaptation research” is a pronoun, a subject, first person, and plural, among other things. Each of these properties"
W10-2604,W06-1615,0,0.746036,"n performance, they are pitfalls for out-of-domain tests for two reasons: first, the vocabulary can differ greatly between domains, so that important words in the test data may never be seen in the training data. And second, the connection between words and labels may also change across domains. For instance, “signaling” appears only as a present participle (VBG) in WSJ text (as in, “signaling that ...”), but predominantly as a noun (as in “signaling pathway”) in biomedical text. Representation learning is a promising new approach to discovering useful features that are stable across domains. Blitzer et al. (2006) and our previous work (2009) demonstrate novel, unsupervised representation learning techniques that produce new features for domain adaptation of a POS tagger. This framework is attractive for several reasons: experimentally, learned features can yield significant improvements over standard supervised models on out-of-domain tests. Since the representation learning techniques are unsupervised, they can be applied to arbitrary new domains to yield the best set of features for learning on WSJ text and predicting on the new domain. There is no need to supply additional labeled examples for each"
W10-2604,P06-1043,0,0.0316601,"l to find a different local maximum in parameter space during training (and thus a different model of the observation sequence), we initialize the parameters randomly. Suppose there are L independent layers in an IHMM model for corpus x = (x1 , . . . , xN ), and l ), where l = 1...L and each layer is (y1l ,y2l ,...yN each y can have K states. The distribution of the corpus and one hidden layer l is Y l P (xi |yil )P (yil |yi−1 P (x, yl ) = ) • T EST-CRF: Our baseline model, trained and tested on the test data. This is our upper bound. • S ELF -CRF: Following the self-training paradigm (e.g., (McClosky et al., 2006b; McClosky et al., 2006a)), we train our baseline first on the training set, then apply it to the test set, then retrain it on the training set plus the automatically labeled test set. We perform only one iteration of retraining, although in general multiple iterations are possible, usually with diminishing marginal returns. 4 Multi-dimensional Representations From a linguistic perspective, words are multidimensional objects. For instance, the word “we” in “We like doing domain adaptation research” is a pronoun, a subject, first person, and plural, among other things. Each of these properties"
W10-2604,W04-3237,0,0.0277117,"Missing"
W10-2604,P07-1033,0,0.265231,"Missing"
W10-2604,P07-1088,0,0.0210585,"ly supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semi-supervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. HMMs have been used many times for POS tagging in supervised, semi-supervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007). The R EALM system for sparse information extraction has also used unsupervised HMMs to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). Sch¨utze (1994) has presented an algorithm that categorizes word tokens in context instead of word types for tagging words. We take a novel perspective on the use of unsupervised latent-variable models by using them to compute features of each token that represent the distribution over that token’s contexts. These features prove to be highly useful for supervised sequence labelers in out-of-domain tests. Acknowledgments We wish to thank the anonymous reviewers for their helpful comments and suggestions. In the deep learning (Bengio, 2009) paradigm, researchers have investigated multi-layer l"
W10-2604,P05-1044,0,0.0415722,"re available for new domains; models that combine our representation learning approach with instance weighting and other forms of supervised domain adaptation may take better advantage of these cases. Several researchers have previously studied methods for using unlabeled data for sequence labeling, either alone or as a supplement to labeled data. Ando and Zhang develop a semi-supervised chunker that outperforms purely supervised approaches on the CoNLL 2000 dataset (Ando and Zhang, 2005). Recent projects in semi-supervised (Toutanova and Johnson, 2007) and unsupervised (Biemann et al., 2007; Smith and Eisner, 2005) tagging also show significant progress. HMMs have been used many times for POS tagging in supervised, semi-supervised, and in unsupervised settings (Banko and Moore, 2004; Goldwater and Griffiths, 2007; Johnson, 2007). The R EALM system for sparse information extraction has also used unsupervised HMMs to help determine whether the arguments of a candidate relation are of the appropriate type (Downey et al., 2007). Sch¨utze (1994) has presented an algorithm that categorizes word tokens in context instead of word types for tagging words. We take a novel perspective on the use of unsupervised la"
W10-2604,W01-0521,0,0.0288544,"Missing"
W10-2604,E95-1020,0,\N,Missing
W11-0315,N10-1026,1,0.738063,"007); 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in POS tagging in a domain adaptation setting. We also analyze the novel PLMRF representation on an IE task, and several representations along the key dimensions of sparsity and polysemy. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Z"
W11-0315,W06-1615,0,0.213371,"ing. Because word frequencies are Zipf distributed, this often means that there is little relevant training data for a substantial fraction of parameters (Bikel, 2004), especially in new domains (Huang and Yates, 2009). For example, word-type features form the backbone of most POS-tagging systems, but types like “gene” and “pathway” show up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features “gene” and “pathway” (BenDavid et al., 2009; Blitzer et al., 2006). Further, because words are polysemous, wordtype features prevent systems from generalizing to situations in which words have different meanings. For instance, the word type “signaling” appears primarily as a present participle (VBG) in Wall Street Journal (WSJ) text, as in, “Interest rates rose, signaling that . . . ” (Marcus et al., 1993). In biomedical text, however, “signaling” appears primarily in the phrase “signaling pathway,” where it is considered a noun (NN) (PennBioIE, 2005); this phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates, 2010a). Our response to"
W11-0315,W09-3821,0,0.107368,"radition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (Deerwester et al., 1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005; Blei et al., 2003; V¨ayrynen et al., 2007); 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and dem"
W11-0315,W10-2608,0,0.131246,"Missing"
W11-0315,P07-1033,0,0.13096,"Missing"
W11-0315,P07-1088,1,0.937663,"that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in POS tagging in a domain adaptation setting. We also analyze the novel PLMRF representation on an IE task, and several representations along the key dimensions of sparsity and polysemy. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III"
W11-0315,D08-1072,0,0.0482695,"Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in POS tagging in a domain adaptation setting. We also analyze the novel PLMRF representation on an IE task, and several representations along the key dimensions of sparsity and polysemy. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III and Marcu, 2006; Finkel and Manning, 2009; Dredze et al., 2010; Dredze and Crammer, 2008). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). Daum´e III et al. (2010) use semisupervised learning to incorporate labeled and unlabeled data from the target domain. In contrast, we investigate a domain adaptation setting where no labeled data is available for the target domain. 3 Representations A representation is a set of features that describe instances for a classifier. Formally, let X be an instance set, and let Z be the set of labels for a classification task. A representation is a function R : X → Y for some suitable feature space Y (such as Rd ). We refer to"
W11-0315,P05-2004,0,0.0196016,"d a host of semantic categories (agency, animate vs. inanimate, physical vs. abstract, etc.), to name a few (Sag et al., 2003). Our model seeks to capture a multidimensional representation of words by creating a separate layer of latent variables for each dimension. The values of the M layers of latent variables for a single word can be used as M distinct features in our representation. The I-HMM attempts to model the same intuition, but unlike a lattice model the IHMM layers are entirely independent, and as a result there is no mechanism to enforce that the layers model different dimensions. Duh (2005) previously used a 2-layer lattice for tagging and chunking, but in a supervised setting rather than for representation learning. Let Cliq(x, y) represent the set of all maximal cliques in the graph of the MRF model for x and y. 1 Percy Liang’s implementation is available at http://metaoptimize.com/projects/wordreprs/. Turian et al. also tested a run with 3200 clusters in their experiments, which we have been training for months, but which has not finished in time for publication. 128 Expressing the lattice model in log-linear form, we can write the marginal probability P (x) of a given senten"
W11-0315,N09-1068,0,0.0378917,"this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in POS tagging in a domain adaptation setting. We also analyze the novel PLMRF representation on an IE task, and several representations along the key dimensions of sparsity and polysemy. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III and Marcu, 2006; Finkel and Manning, 2009; Dredze et al., 2010; Dredze and Crammer, 2008). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). Daum´e III et al. (2010) use semisupervised learning to incorporate labeled and unlabeled data from the target domain. In contrast, we investigate a domain adaptation setting where no labeled data is available for the target domain. 3 Representations A representation is a set of features that describe instances for a classifier. Formally, let X be an instance set, and let Z be the set of labels for a classification task. A representation is a function R : X → Y for some sui"
W11-0315,P90-1034,0,0.52823,"al., 1993). In biomedical text, however, “signaling” appears primarily in the phrase “signaling pathway,” where it is considered a noun (NN) (PennBioIE, 2005); this phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates, 2010a). Our response to these problems with traditional NLP representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples. Our approach depends on the well-known distributional hypothesis, which states that a word’s meaning is identified with the contexts in which it appears (Harris, 1954; Hindle, 1990). Our goal is to develop probabilistic lan125 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 125–134, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics guage models that describe the contexts of individual words accurately. We then construct representations, or mappings from word tokens and types to real-valued vectors, from these language models. Since the language models are designed to model words’ contexts, the features they produce can be used to combat problems with polysemy. And by careful design of the la"
W11-0315,P09-1056,1,0.922428,"vely expensive. We investigate representations that can be applied when domain-specific labeled training data is scarce. An increasing body of theoretical and empirical evidence suggests that traditional, manually-crafted features limit systems’ performance in this setting for two reasons. First, feature sparsity prevents systems from generalizing accurately to words and features not seen during training. Because word frequencies are Zipf distributed, this often means that there is little relevant training data for a substantial fraction of parameters (Bikel, 2004), especially in new domains (Huang and Yates, 2009). For example, word-type features form the backbone of most POS-tagging systems, but types like “gene” and “pathway” show up frequently in biomedical literature, and rarely in newswire text. Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features “gene” and “pathway” (BenDavid et al., 2009; Blitzer et al., 2006). Further, because words are polysemous, wordtype features prevent systems from generalizing to situations in which words have different meanings. For instance, the word type “signaling” appears pr"
W11-0315,W10-2604,1,0.937463,"avid et al., 2009; Blitzer et al., 2006). Further, because words are polysemous, wordtype features prevent systems from generalizing to situations in which words have different meanings. For instance, the word type “signaling” appears primarily as a present participle (VBG) in Wall Street Journal (WSJ) text, as in, “Interest rates rose, signaling that . . . ” (Marcus et al., 1993). In biomedical text, however, “signaling” appears primarily in the phrase “signaling pathway,” where it is considered a noun (NN) (PennBioIE, 2005); this phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates, 2010a). Our response to these problems with traditional NLP representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples. Our approach depends on the well-known distributional hypothesis, which states that a word’s meaning is identified with the contexts in which it appears (Harris, 1954; Hindle, 1990). Our goal is to develop probabilistic lan125 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 125–134, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics gu"
W11-0315,P10-1099,1,0.932055,"avid et al., 2009; Blitzer et al., 2006). Further, because words are polysemous, wordtype features prevent systems from generalizing to situations in which words have different meanings. For instance, the word type “signaling” appears primarily as a present participle (VBG) in Wall Street Journal (WSJ) text, as in, “Interest rates rose, signaling that . . . ” (Marcus et al., 1993). In biomedical text, however, “signaling” appears primarily in the phrase “signaling pathway,” where it is considered a noun (NN) (PennBioIE, 2005); this phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates, 2010a). Our response to these problems with traditional NLP representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples. Our approach depends on the well-known distributional hypothesis, which states that a word’s meaning is identified with the contexts in which it appears (Harris, 1954; Hindle, 1990). Our goal is to develop probabilistic lan125 Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 125–134, c Portland, Oregon, USA, 23–24 June 2011. 2011 Association for Computational Linguistics gu"
W11-0315,P07-1034,0,0.10884,"owney, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in POS tagging in a domain adaptation setting. We also analyze the novel PLMRF representation on an IE task, and several representations along the key dimensions of sparsity and polysemy. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains (Daum´e III, 2007; Jiang and Zhai, 2007; Daum´e III and Marcu, 2006; Finkel and Manning, 2009; Dredze et al., 2010; Dredze and Crammer, 2008). Learning bounds are known (Blitzer et al., 2007; Mansour et al., 2009). Daum´e III et al. (2010) use semisupervised learning to incorporate labeled and unlabeled data from the target domain. In contrast, we investigate a domain adaptation setting where no labeled data is available for the target domain. 3 Representations A representation is a set of features that describe instances for a classifier. Formally, let X be an instance set, and let Z be the set of labels for a classification task."
W11-0315,P08-1068,0,0.0824034,"n representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (Deerwester et al., 1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005; Blei et al., 2003; V¨ayrynen et al., 2007); 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it o"
W11-0315,P09-1116,0,0.0611585,"There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (Deerwester et al., 1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005; Blei et al., 2003; V¨ayrynen et al., 2007); 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation"
W11-0315,J93-2004,0,0.0399743,"requently in biomedical literature, and rarely in newswire text. Thus, a classifier trained on newswire data and tested on biomedical data will have seen few training examples related to sentences with features “gene” and “pathway” (BenDavid et al., 2009; Blitzer et al., 2006). Further, because words are polysemous, wordtype features prevent systems from generalizing to situations in which words have different meanings. For instance, the word type “signaling” appears primarily as a present participle (VBG) in Wall Street Journal (WSJ) text, as in, “Interest rates rose, signaling that . . . ” (Marcus et al., 1993). In biomedical text, however, “signaling” appears primarily in the phrase “signaling pathway,” where it is considered a noun (NN) (PennBioIE, 2005); this phrase never appears in the WSJ portion of the Penn Treebank (Huang and Yates, 2010a). Our response to these problems with traditional NLP representations is to seek new representations that allow systems to generalize more accurately to previously unseen examples. Our approach depends on the well-known distributional hypothesis, which states that a word’s meaning is identified with the contexts in which it appears (Harris, 1954; Hindle, 199"
W11-0315,D09-1098,0,0.0688344,"Missing"
W11-0315,P93-1024,0,0.685976,"r representations on each of them. Section 7 concludes. 2 Previous Work There is a long tradition of NLP research on representations, mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (Deerwester et al., 1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005; Blei et al., 2003; V¨ayrynen et al., 2007); 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Latt"
W11-0315,P07-1096,0,0.0925221,"Missing"
W11-0315,P05-1044,0,0.061538,"h of min(M ,N ) for the unpruned model where N (x), the neighborhood of x, indicates a set of perturbed variations of the original sentence x. Contrastive estimation seeks to move probability mass away from the perturbed neighborhood sentences and onto the original sentence. We use a neighborhood function that includes all sentences which can be obtained from the original sentence by swapping the order of a consecutive pair of words. Training uses gradient descent over this non-convex objective function with a standard software package (Liu and Nocedal, 1989) and converges to a local maximum (Smith and Eisner, 2005). For tractability, we modify the training procedure to train the PL-MRF one layer at a time. Let θi represent the set of parameters relating to features of layer i, and let θ¬i represent all other parameters. We fix θ¬0 = 0, and optimize θ0 using contrastive estimation. After convergence, we fix θ¬1 , and optimize θ1 , and so on. We use a convergence threshold of 10−6 , and each layer typically converges in under 100 iterations. 5 Domain Adaptation for a POS Tagger We evaluate the representations described above on a POS tagging task in a domain adaptation setting. 5.1 Experimental Setup We u"
W11-0315,P10-1040,0,0.384193,"l., 2003; V¨ayrynen et al., 2007); 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previous state-of-the-art in POS tagging in a domain adaptation setting. We also analyze the novel PLMRF representation on an IE task, and several representations along the key dimensions of sparsity and polysemy. Most previous work on domain adaptation has focused on the case where some labeled data is available in both the source and target domains"
W11-0315,W09-1208,0,0.0217666,"mostly falling into one of four categories: 1) vector space models of meaning based on document-level lexical cooccurrence statistics (Salton and McGill, 1983; Turney and Pantel, 2010; Sahlgren, 2006); 2) dimensionality reduction techniques for vector space models (Deerwester et al., 1990; Honkela, 1997; Kaski, 1998; Sahlgren, 2005; Blei et al., 2003; V¨ayrynen et al., 2007); 3) using clusters that are induced from distributional similarity (Brown et al., 1992; Pereira et al., 1993; Mar126 tin et al., 1998) as non-sparse features (Lin and Wu, 2009; Candito and Crabbe, 2009; Koo et al., 2008; Zhao et al., 2009); 4) and recently, language models (Bengio, 2008; Mnih and Hinton, 2009) as representations (Weston et al., 2008; Collobert and Weston, 2008; Bengio et al., 2009), some of which have already yielded state of the art performance on domain adaptation tasks (Huang and Yates, 2009; Huang and Yates, 2010a; Huang and Yates, 2010b; Turian et al., 2010) and IE (Ahuja and Downey, 2010; Downey et al., 2007b). In contrast to this previous work, we develop a novel Partial Lattice MRF language model that incorporates a factorial representation of latent states, and demonstrate that it outperforms the previ"
W11-0315,J04-4004,0,\N,Missing
W11-0315,J92-4003,0,\N,Missing
W12-2014,W05-1203,0,0.188619,"tensive pattern-building step. A widely used approach to measuring text similarity between two text strings is to convert each text string into a word vector and then use the angle between these two vectors as a similarity metric. For example, Content Vector Analysis (CVA) has been successfully utilized to detect off-topic essays (Higgins et al., 2006) and to provide content-related features for essay scoring (Attali and Burstein, 2004). For this group of methods, measuring the semantics similarity between two terms is a key question. A number of metrics have been proposed, including metrics (Courley and Mihalcea, 2005) derived from WordNet, a semantics knowledge database (Fellbaum, 1998), and metrics related to terms’ co-occurrence in corpora or on the Web (Turney, 2001). The third group of methods treats content scoring as a Text Categorization (TC) task, which treats the responses being scored on different score levels as different categories. Therefore, a large amount of previous TC research, such as the many machine learning approaches proposed for the TC task, can be utilized. For example, Furnkranz et al. (1998) compared the performance of applying two machine learning methods on a web-page categoriza"
W12-2014,P09-1056,1,0.826643,"thods instead of the simple vectordistance computation used in CVA. Due to short response-time in the speaking test being considered, the ordinary vector analysis may face a problem that the obtained vectors are too short to be reliably used. In addition, using other non-CVA machine learning methods can enable us to try other types of linguistic features. To address the feature sparsity issue, a smoothing method, which converts word-based text features into features based on other entities with a much smaller vocabulary size, is used. We use a Hidden Markov Model (HMM) based smoothing method (Huang and Yates, 2009), which induces classes, corresponding to hidden states in the HMM model, from the observed word strings. This smoothing method can use contextual information of the word sequences due to the nature of HMM. Then, we convert word-entity vectors to the vectors based on the induced classes. TF-IDF (term 1 Most specific ancestor node 124 frequency and inverse document frequency) weighting is applied on the new class vectors. Finally, the processed class vectors are used as input features (smoothed) to a machine learning method. In this research, after comparing several widely used machine learning"
W12-2014,E09-1065,0,0.0216102,"ollowing three groups. The first group relies on obtaining and matching patterns associated with the correct answers (Leacock and Chodorow, 2003; Sukkarieh and Blackmore, 2009). The second group of methods, also mostly used 122 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 122–126, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics for content-scoring, is to rely on a variety of text similarity measurements to compare a response with either pre-defined correct answers or a group of responses rated with a high score (Mohler and Mihalcea, 2009). Compared to the first group, such methods can bypass a labor intensive pattern-building step. A widely used approach to measuring text similarity between two text strings is to convert each text string into a word vector and then use the angle between these two vectors as a similarity metric. For example, Content Vector Analysis (CVA) has been successfully utilized to detect off-topic essays (Higgins et al., 2006) and to provide content-related features for essay scoring (Attali and Burstein, 2004). For this group of methods, measuring the semantics similarity between two terms is a key ques"
W12-2014,P94-1019,0,\N,Missing
