2019.rocling-1.33,P18-1097,0,0.0580022,"Missing"
2020.lrec-1.198,D15-1166,0,0.0606404,"Missing"
2020.lrec-1.198,P02-1040,0,0.115054,"ogue generation, learning for evaluation 1. Introduction Assessment of open domain dialogue systems usually rely on human, and it is hard to have a consistent evaluation. Due to the lack of automatic metrics, there is very limited reproducibility of dialogue system evaluation (Fokkens et al., 2013). At the same time, human evaluation methodologies are also very diverse. Papers report novel generation methods for dialogue systems, but pay little attention on datasets and the evaluation process. Traditional automatic evaluation measures on NLP applications, such as BLEU for machine translation (Papineni et al., 2002), requires human references as ground truth. Recent research on natural language generation makes Chatbot more interesting, however, it is hard to evaluate the quality of the generated dialogue since it is hard to provide human references. Although there are many research on dialogue evaluations (Shawar and Atwell, 2007), most automatic measure metrics cannot reflect the quality of a Chatbot. Currently, the Chatbot evaluation can only rely on human judge, no matter it is for research such as a shared task or for application development (Chen et al., 2019). There are two main reasons on why it"
2020.lrec-1.198,N19-4011,0,0.0285217,"all, it is hard to evaluate natural language of most natural language processing tasks, such as machine translation and summarization. It required predefined reference for the evaluation and golden reference usually do not exist. The second reason is more complicate, since any known automatic evaluation tool can be incorporated into the generation process; the performance will be no different among these systems that incorporated a known automatic evaluation tool. For example, there are some End-to-End NLG systems that can learn sentence planning and surface realization from nonaligned data (Sedoc et al., 2019). These systems are based on parallel datasets, without the need of human references. However, if an evaluation is fully automatic, then it can be incorporated into a generation system. Thus, it can always generate sentences with higher evaluation value. The evaluation is also based on human judgment. However, it is very hard to develop new models without knowing the feedback during the system developing process. Therefore, we have to compare our own models by ourselves. We used a model A versus model B voting system (A/B test) to acquire the comparison between two models. We show two generate"
2020.lrec-1.198,W07-0313,0,0.145251,"uman evaluation methodologies are also very diverse. Papers report novel generation methods for dialogue systems, but pay little attention on datasets and the evaluation process. Traditional automatic evaluation measures on NLP applications, such as BLEU for machine translation (Papineni et al., 2002), requires human references as ground truth. Recent research on natural language generation makes Chatbot more interesting, however, it is hard to evaluate the quality of the generated dialogue since it is hard to provide human references. Although there are many research on dialogue evaluations (Shawar and Atwell, 2007), most automatic measure metrics cannot reflect the quality of a Chatbot. Currently, the Chatbot evaluation can only rely on human judge, no matter it is for research such as a shared task or for application development (Chen et al., 2019). There are two main reasons on why it is hard to do the automatic evaluation. First at all, it is hard to evaluate natural language of most natural language processing tasks, such as machine translation and summarization. It required predefined reference for the evaluation and golden reference usually do not exist. The second reason is more complicate, since"
2020.nlptea-1.12,2020.acl-main.81,0,0.0372484,"Missing"
2020.nlptea-1.12,N19-1423,0,0.037636,"organizers ask professional teachers to label the errors in learners’ sentences. There are four types of label in the sentences: Redundant (R), Selection (S), Disorder (W), and Missing (M). The goal of the task is to build a system that can predict whether a sentence is wrong and correct it. In previous years, we participated in the NLPTEA CGED (Wu et al., 1 2018) and shows that such a system can be precision oriented or recall oriented for different users. Since the emerging of deep learning, we find that sequence-to-sequence models have good effect on grammar correction, and the BERT model (Devlin et al., 2019; Xu et al., 2019) is the best sequence-to-sequence pre-training language model using a large number of data sets. The pretrained model is trained with mask language model (MLM) to enhance the strength of the model. In Run1 of 2020, we use BERT as the first level of our identification. We fine-tuning the BERT model with the Lang-81 corpus and all the data from NLPTEA since 2016 to 2020, so that the model can be used to predict correct and incorrect sentences, and reproduce the wrong sentences. The error types are determined by CRF. In Run2 is not used to determine the wrong and correct sentenc"
2020.nlptea-1.12,W18-3707,0,0.0631319,"Missing"
2020.nlptea-1.12,W15-4401,0,0.0281707,"positive rate 0.0163 on detection, while Run2 gives a more balanced performance. 1 Introduction Learning Chinese is very popular for foreigners, but it is difficult for them to write correct sentence. Grammatical error detection is a big challenge for the Chinese learners as a second language. Learning Chinese sentences will rely too much on the teacher to correct the wrong sentences. It is not easy for learners to get timely feedback. Therefore, how to use existing technology to detect and correct the grammatical errors that learners make has become a hot topic. Since 2014 (Yu et al.,2014) (Lee et al. 2015) (Lee et al. 2016) (Rao et al., 2017) (Rao et al., 2018), the NLP-TEA workshop provides a series Chinese Grammar Error Detection (CGED) shared tasks to promote the research on grammar error diagnosis. The organizers ask professional teachers to label the errors in learners’ sentences. There are four types of label in the sentences: Redundant (R), Selection (S), Disorder (W), and Missing (M). The goal of the task is to build a system that can predict whether a sentence is wrong and correct it. In previous years, we participated in the NLPTEA CGED (Wu et al., 1 2018) and shows that such a system"
2020.nlptea-1.12,W18-3708,0,0.0495268,"Missing"
2020.nlptea-1.12,W18-3706,0,0.039671,"Missing"
2020.nlptea-1.12,2020.acl-main.82,0,0.045625,"Missing"
C02-2013,bredenkamp-etal-2000-looking,0,0.0561434,"Missing"
C02-2013,C92-2082,0,0.036896,"(QA), knowledge management and organization memory (KM/OM), information retrieval, machine translation (Guarino 1998), and grammar checking systems (Bredenkamp 2000). With the help of domain ontology, software systems can perform better in understanding natural language. However, building domain ontology is laborious and time consuming. Previous works suggest that ontology acquisition is an iterative process which includes keyword collection as well as structure reorganization. The ontology will be revised, refined, and filled in detail during iteration. (Noy and McGuinness 2001) For example (Hearst 1992), in order to find a hyponym of a keyword, the human editor must observe sentences containing this keyword and its related hyponyms. The editor then deduces rules for finding more hyponyms of this keyword. As such cycle iterates, the editor refines the rules to obtain better quality pairs of keyword-hyponyms. In this work we try to speed up the above labor-intensive approach by designing acquisition rules that can be applied recursively. Wen-Lian HSU Institute of Information Science Academia Sinica Nankang, Taipei, Taiwan, R.O.C. hsu@iis.sinica.edu.tw A human editor only has to verify the resu"
C02-2013,W99-0621,0,0.0187516,"ot） Nc+VC 中國（Nc）建設（VC） 銀行（Nc Root） Nc+Nc+Na 中國（Nc）國際（Nc） 商業（Na）銀行（Nc Root） Nc+Nc+VC 中國（Nc）國際（Nc） 開 發 （ VC ） 銀 行 （ Nc Root） Table 4. Attribute extraction rules of an Nc noun Extraction rule Extraction Example target Nc（root）+Na Na 中央研究院（Nc）院長 （Na） Nc（root）+Nc Nc 中央研究院（Nc）停車場 （Nc） 中央研究院（Nc）語言所 Nc （ root ） Nc+Nc +Nc+Nc （Nc）語音實驗室（Nc） 中央研究院（Nc）的(DE) Nc （ root ） Na +DE+Na 出版品（Na） 4. Discussion Li and Thompson (1981) describe Mandarin Chinese as a Topic-prominent language in which the subject or the object is not as obvious as in other languages. Therefore, the highly precise shallow parsing result (Munoz et al. 1999) on NN and SV pairs in English is probably not applicable to Chinese. 4.1 The Experiment of Extraction Rate To test the qualitative and quantitative performance of SOAT, we design two experiments. We construct three domain ontology prototypes for three different domains and corpora. Table 5 shows the result in which the frequently asked questions (FAQs) for stocks are taken from test sentences of the financial QA system. The university and bank corpora are University Bank 4.2 Results from Different Corpora We select three different corpora from different information resources in the “network”"
C02-2013,O02-1002,1,0.81846,"Missing"
I17-4022,W02-0109,0,0.022844,"Missing"
I17-4022,W13-4205,1,0.876319,"Missing"
I17-4022,O14-3002,1,0.890694,"Missing"
O03-1012,C02-1012,0,0.0539335,"Missing"
O03-1012,M98-1018,0,0.0768205,"Missing"
O03-1012,M98-1016,0,0.115914,"Missing"
O03-1012,J96-1002,0,0.0159547,"Missing"
O03-1012,M98-1004,0,\N,Missing
O03-1012,M98-1012,0,\N,Missing
O03-1012,M98-1014,0,\N,Missing
O03-1012,M98-1021,0,\N,Missing
O03-1012,M98-1017,0,\N,Missing
O03-1012,M98-1009,0,\N,Missing
O04-1032,A97-1029,0,0.080643,"Missing"
O04-1032,W98-1118,0,0.0432005,"Missing"
O04-1032,W98-1120,0,0.06186,"Missing"
O04-1032,W02-2024,0,0.0575095,"Missing"
O04-1032,W03-0419,0,0.0519798,"Missing"
O04-1032,C96-1079,0,0.104601,"Missing"
O04-1032,O04-2004,1,0.884319,"Missing"
O04-1032,N03-1002,0,\N,Missing
O04-1032,M98-1028,0,\N,Missing
O04-2004,M98-1018,0,0.0853448,"Missing"
O04-2004,M98-1028,0,0.0611186,"Missing"
O04-2004,M98-1025,0,0.272439,"Missing"
O04-2004,M98-1026,0,0.164703,"Missing"
O04-2004,M98-1017,0,0.0753656,"Missing"
O04-2004,M98-1021,0,0.0552534,"Missing"
O04-2004,M98-1009,0,0.0725893,"Missing"
O04-2004,C02-1012,0,0.123883,"Missing"
O04-2004,M98-1016,0,0.0397731,"Missing"
O04-2004,M98-1004,0,\N,Missing
O04-2004,M98-1012,0,\N,Missing
O04-2004,M98-1014,0,\N,Missing
O04-2004,J96-1002,0,\N,Missing
O05-1017,W03-1705,0,0.0705084,"Missing"
O05-1017,C02-1131,0,0.0226017,"Missing"
O05-1017,W95-0107,0,0.181464,"Missing"
O05-1017,W96-0202,0,0.0855969,"Missing"
O05-1017,W04-1114,0,0.0517487,"Missing"
O05-1017,W00-1212,0,0.0514526,"Missing"
O05-1017,W00-1320,0,\N,Missing
O05-1017,W00-1211,0,\N,Missing
O05-1017,A88-1019,0,\N,Missing
O05-1017,J96-1002,0,\N,Missing
O06-1023,J96-1002,0,0.0227461,"Missing"
O06-1023,J93-4006,0,0.952711,"Missing"
O06-1023,J00-4006,0,0.00642051,"Missing"
O06-1023,A97-2014,0,\N,Missing
O08-6004,W02-1103,0,0.0225928,"Missing"
O08-6004,N07-2032,0,0.0300785,"Missing"
O08-6004,P07-2013,0,0.0417889,"Missing"
O09-2007,P00-1032,0,0.421126,"Missing"
O09-2007,W09-3412,1,0.891098,"Missing"
O09-2007,P09-2007,1,0.844674,"Missing"
O09-2007,P08-2024,0,0.0247937,"Missing"
O10-1008,J96-1002,0,0.0729206,"Missing"
O10-3002,J96-1002,0,0.0993701,"Missing"
O10-4003,P08-2024,0,0.0409205,"Missing"
O10-4003,P09-2007,1,0.762932,"Missing"
O10-4003,W09-3412,1,0.878514,"Missing"
O10-4003,P00-1032,0,0.0994424,"Missing"
O10-4003,P02-1006,0,\N,Missing
O11-2008,P02-1034,0,0.0245521,"Missing"
O11-2008,H05-1079,0,0.078217,"Missing"
O11-2008,W07-1404,0,0.0500105,"Missing"
O11-2008,H05-1047,0,0.0266536,"Missing"
O11-2008,P01-1052,0,0.0780495,"Missing"
O11-2008,J07-2002,0,0.0169946,"Missing"
O11-2008,P08-1028,0,0.0896925,"Missing"
O11-2008,W07-1407,0,0.0606611,"Missing"
O11-2008,D09-1010,0,0.0553597,"Missing"
O11-2008,1999.mtsummit-1.85,0,0.0366864,"Missing"
O11-2008,1994.amta-1.25,0,0.0935513,"Missing"
O11-2008,O10-3002,1,0.892931,"Missing"
O11-2008,E06-1015,0,0.0809333,"Missing"
O11-2008,P03-1056,0,0.0323071,"Missing"
O11-2008,P02-1040,0,\N,Missing
O11-2008,W06-1610,0,\N,Missing
O11-2008,P09-3004,0,\N,Missing
O11-2008,O10-1008,1,\N,Missing
O12-1034,W07-1407,0,0.0548604,"Missing"
O12-1034,U06-1019,0,0.040657,"Missing"
O12-1034,P02-1040,0,0.0878291,"Missing"
O12-1034,P09-3004,0,0.0288624,"Missing"
O12-1034,J03-1002,0,0.00619326,"Missing"
O13-1012,O12-1034,1,0.885826,"Missing"
O13-1012,O10-3002,1,0.894365,"Missing"
O13-1012,P02-1040,0,\N,Missing
O13-1012,W06-1610,0,\N,Missing
O13-1012,O02-2003,0,\N,Missing
O13-1012,J03-1002,0,\N,Missing
O13-1012,O10-1008,1,\N,Missing
O13-5001,O02-2003,0,0.0735128,"Missing"
O13-5001,O10-3002,1,0.893251,"Missing"
O13-5001,J03-1002,0,0.00474233,"Missing"
O13-5001,P02-1040,0,0.0866394,"Missing"
O13-5001,W06-1610,0,0.049015,"Missing"
O13-5001,O10-1008,1,\N,Missing
O14-3002,W06-1650,0,0.349699,"Missing"
O14-3002,P11-2088,0,0.037514,"Missing"
O15-1021,J90-2002,0,0.612492,"Missing"
O15-1021,O01-3004,0,0.0774158,"Missing"
O15-1021,J96-1002,0,0.131867,"Missing"
O15-1021,P96-1041,0,0.282209,"Missing"
O16-1011,J90-2002,0,0.755847,"Missing"
O16-1011,O01-3004,0,0.158396,"Missing"
O16-1011,P96-1041,0,0.247562,"Missing"
O16-1011,Y15-2030,0,0.0476606,"Missing"
P09-2007,W09-3412,1,0.349789,"Missing"
P09-2007,P08-2024,1,0.394469,"submitted queries is an important factor, and, in other cases, incorrect words were more commonly used. 5 Facilitating Test Item Authoring Incorrect character correction is a very popular type of test in Taiwan. There are simple test items for young children, and there are very challenging test items for the competitions among adults. Finding an attractive incorrect character to replace a correct character to form a test item is a key step in authoring test items. We have been trying to build a software environment for assisting the authoring of test items for incorrect character correction (Liu and Lin, 2008, Liu et al., 2009). It should be easy to find a lexicon that contains pronunciation information about Chinese characters. In contrast, it might not be easy to find visually similar Chinese characters with computational methods. We expanded the original Cangjie codes (OCC), and employed the expanded Cangjie codes (ECC) to find visually similar characters (Liu and Lin, 2008). With a lexicon, we can find characters that can be pronounced in a particular way. However, this is not enough for our goal. We observed that there were different symptoms when people used incorrect characters that are rel"
W03-1118,C92-2082,0,0.00648027,"is to compile the concepts within documents in a training set and use these concepts to understand documents in a testing set. However, building rigorous domain ontology is laborious and time-consuming. Previous works suggest that ontology acquisition is an iterative process, which includes keyword collection and structure reorganization. The ontology is revised, refined, and accumulated by a human editor at each iteration (Noy and McGuinness, 2001). For example, in order to find a hyponym of a keyword, the human editor must observe sentences containing this keyword and its related hyponyms (Hearst, 1992). The editor then deduces rules for finding more hyponyms of this keyword. At each iteration the editor refines the rules to obtain better quality pairs of keyword-hyponyms. To speed up the above labor-intensive approach, semiautomatic approaches have been designed in which a human editor only has to verify the results of the acquisition (Maedche and Staab, 2000). A knowledge representation framework, Information Map (InfoMap) in our previous work (Hsu et al., 2001), has been designed to integrate various linguistic, common-sense and domain knowledge. InfoMap is designed to perform natural lan"
W03-1118,C02-1089,1,\N,Missing
W03-1118,C02-2013,1,\N,Missing
W03-1118,Y96-1018,0,\N,Missing
W03-1118,O98-4003,1,\N,Missing
W09-3412,P08-2024,1,0.368853,"lly examined all of the n-grams in the initial list, and removed such n-grams from the list. In addition to considering the frequencies of ngrams formed by the basic Cangjie codes to determine the list of components, we also took advantage of radicals that are used to categorize Chinese characters in typical printed dictionaries. Radicals that are standalone Chinese words were included in the list of components. After selecting the list of basic components with the above procedure, we encoded the words in Elist with these basic components. We inherited the 12 ways reported in a previous work (Liu and Lin, 2008) to decompose Chinese characters. There are other methods for decomposing Chinese characters into components. Juang et al. (2005) and their team at the Sinica Academia propose 13 different ways for decomposing characters. At the same time when we annotated individual characters with their ECCs, we may revise the list of basic components. If a character that actually contained an intuitively “common” part and that part had not been included in the list of basic component, we would add this part into the list to make it a basic component and revised the ECC for all characters accordingly. The ju"
W10-4107,J93-2003,0,0.0390042,"Missing"
W10-4107,P96-1041,0,0.0211056,"Language model Since there is no large corpus of student essays, we used a news corpus to train the language model. The size of the news corpus is 1.5 GB, which consists of 1,278,787 news articles published between 1998 and 2001. The n-gram language model was adopted to calculate the probability of a sentence p(S). The general n-gram formula is: p ( S ) = p ( wn |wnn−−1N +1 ) (1) Where N was set to two for bigram and N was set to one for unigram. The Maximum Likelihood Estimation (MLE) was used to train the n-gram model. We adopted the interpolated Kneser-Ney smoothing method as suggested by Chen & Goodman (1996). As following: pint erpolate ( w |wi −1 ) = λpbigram ( w |wi −1 ) + (1 − λ ) punigram ( w) (2) To determine whether a replacement is good or not, our system use the modified perplexity: Perplexity = 2 − log( p ( S )) / N (3) Where N is the length of a sentence and p(S) is the bigram probability of a sentence after smoothing. 3.3 Dictionary and test set We used a free online dictionary provided by Taiwan’s Ministry of Education, MOE (2007). We filtered out one character words and used the remaining 139,976 words which were more than one character as our lexicon in the following experiments. Th"
W10-4107,O09-2007,1,0.838226,"pendently constructed confusion sets, and a fixed language model to reconstruct the systems. We performed tests on the same test set. 3 3.1 Data in Experiments Confusion sets Confusion sets are a collection of sets for each individual Chinese character. A confusion set of a certain character consists of phonologically or logographically similar characters. For example, the confusion set of “辦” might consist of the following characters with the same pronunciation“半伴扮姅拌絆瓣＂ or with similar forms“辨瓣辮辯避僻辣梓辭鋅辟滓辛宰癖 莘辜薜薛闢”. In this study, we use the confusion sets used by Liu, Tien, Lai, Chuang, & Wu (2009). The similar Cangjie (SC1 and SC2) sets of similar forms, and both the same-sound-same-tone (SSST) and same-sound-different-tone (SSDT) sets for similar pronunciation were used in the experiments. There were 5401 confusion sets for each of the 5401 high frequency characters. The size of each confusion set was one to twenty characters. The characters in each confusion set were ranked according to Google search results. 3.2 Language model Since there is no large corpus of student essays, we used a news corpus to train the language model. The size of the news corpus is 1.5 GB, which consists of"
W10-4107,W09-3412,1,\N,Missing
W10-4107,P09-2007,1,\N,Missing
W12-6339,P08-1109,0,0.0864563,"Missing"
W12-6339,O05-1017,1,\N,Missing
W12-6339,J96-1002,0,\N,Missing
W12-6339,W10-4143,0,\N,Missing
W12-6339,W10-4145,0,\N,Missing
W13-4205,W06-1650,0,0.0573239,", Japan, 14 October 2013. 1.1 of the unhelpful. These ideas can be viewed as features for a NLP classifier. However, some of them are hard to implement and require clear definition. Related Works Early works on opinion mining focused on the polarity of opinion, positive or negative, this kind of opinion mining was also called sentiment analysis. Another kind of opinion mining focused on finding the detail information of a product from reviews; such approach was a kind of information extraction (Hu & Liu, 2004). Recent researches focus on assessing the review quality before mining the opinion. Kim et al. (2006) explored the use of some semantic features for review helpfulness ranking. They found that some important features of review, including Length, Unigrams, and Stars might provide the basis for assessing helpfulness of reviews. Siersdorfer et al. (2010) presented a system that could automatically structure and filter comments for YouTube videos by analyzing dependencies between comments, views, comment ratings and topic categories. The method used the SentiWordNet thesaurus, a lexical WordNet-based resource containing sentiment Moghaddam annotations. et al. (2011) proposed Matrix Factorization"
W13-4205,P11-2088,0,0.0152623,"used the SentiWordNet thesaurus, a lexical WordNet-based resource containing sentiment Moghaddam annotations. et al. (2011) proposed Matrix Factorization Model and Tensor Factorization Model for the prediction of the quality of online reviews, and evaluated the models by using a real life database from Epinions.com. Lu (2010) exploited contextual information about authors’ identities and social networks for improving review quality prediction. The method provided a generic framework for incorporating social context information by adding regularization constraints to the text-based predictor. Xiong and Litman (2011) investigated the utility of incorporating additionally specialized features tailored to peer-review helpfulness. They found that structural features, review unigrams and meta-data combination were useful in modeling the helpfulness of both peer reviews and product reviews. 2 2.1 Helpfulness Pros and Cons Product Usage Information Detail Good Writing Style Background Knowledge of Product Personal Information About Reviewer Comparisons Lay-Man's Terms Conciseness Lengthy Use of Ratings Authenticity Honesty Miscellaneous Unbiased Accuracy Relevancy Thoroughness Unhelpfulness Times Mentioned 36 3"
W13-4406,P00-1032,0,0.81353,"the correct characters of detected errors. Spelling check must be done within a context, say a sentence or a long phrase with a certain meaning, and cannot be done within one word (Mays et al., 1991). However, spelling check in Chinese is very different from that in English or other alphabetic languages. There are no word delimiters between words and the length of each word is very short. There are several previous studies addressing the Chinese spelling check problem. Chang (1995) has proposed a bi-gram language model to substitute the confusing character for error detection and correction. Zhang et al. (2000) have presented an approximate word-matching algorithm to detect and correct Chinese spelling errors us35 Proceedings of the Seventh SIGHAN Workshop on Chinese Language Processing (SIGHAN-7), pages 35–42, Nagoya, Japan, 14 October 2013. uation. Section 4 proposes the evaluation metrics for both subtasks. Section 5 presents the results of participants’ approaches for performance comparison. Section 6 elaborates on the semantic and pragmatic aspects of automatic correction of Chinese text. Finally, we conclude this paper with the findings and future research direction in the Section 7. 2 <DOC Ni"
W15-4402,W12-6339,1,0.748536,"al., 2014). In the NLP-TEA-2 CGED shared task data set, there are four types of errors in the leaners’ sentences: Redundant, Selection, Disorder, and Missing. The research goal is to build a system that can detect the errors, identify the type of the error, and point out the position of the error in the sentence. 2 可是 C O 有 Vt O 一點 DET O 冷 Vi O 了 T R Methodology Our system is based on the conditional random field (CRF) (Lafferty, 2001). CRF has been used in many natural language processing applications, such as named entity recognition, word segmentation, information extraction, and parsing (Wu and Hsieh, 2012). For different task, it requires different feature set and different labeled training data. The CRF can be regarded as a sequential labeling tagger. Given a sequence data X, the CRF can generate the corresponding label sequence Y, based on the trained model. Each label Y is taken from a specific tag set, 你 N 的 T 過年 呢 T O R Vi O O Figure 1: A snapshot of our CRF sequential labeling working file 1 2 http://ckipsvr.iis.sinica.edu.tw/ http://crfpp.sourceforge.net/index.html 7 Proceedings of The 2nd Workshop on Natural Language Processing Techniques for Educational Applications, pages 7–14, c Beij"
W16-4909,W15-4402,1,0.885857,"Missing"
W16-4909,W11-1422,0,0.0457139,"Missing"
W16-4909,W15-4401,0,0.0344905,"arners to get instant feedback when they are writing an essay in a computer aided language learning environment (Shiue and Chen, 2016). In order to develop a grammar error detection system with the statistical natural language processing technology, developers need a large learner corpus (Chang et al., 2012). However, currently there is no publicly available large leaner corpus in Chinese essay writing. That puts off the research in this field. The NLP-TEA workshop has been holding a Chinese Grammar Error Detection (CGED) shared task in the workshop for two years since 2014 (Yu et al., 2014) (Lee et al. 2015). They provides a set of learner corpus and a clear definition on 4 major Grammar error types in the foreign learner corpus. The shared tasks stimulated the research and drew many participants. The goal of the shared task is to develop a system that can detect the four types of grammar errors in learner corpus. Comparing to the task definition of CGED in 2014 and 2015, the major difference in this year is the sentences might contain multiple errors. And the organizers provide two data sets: one is in traditional Chinese, the TOCFL dataset; the other is in simplified Chinese, the HSK dataset. F"
W16-4909,L16-1033,0,0.0272658,"he CRF model. Our system presents the best detection accuracy and Identification accuracy on the TOCFL dataset, which is in traditional Chinese. The same system also works well on the simplified Chinese HSK dataset. 1 Introduction Chinese essay writing is hard for foreign learners, not only on the aspect of learning pictograph Chinese characters but also on that of learning Chinese grammar that has no strong syntax rules. An automatic grammar error detection system might help the learners to get instant feedback when they are writing an essay in a computer aided language learning environment (Shiue and Chen, 2016). In order to develop a grammar error detection system with the statistical natural language processing technology, developers need a large learner corpus (Chang et al., 2012). However, currently there is no publicly available large leaner corpus in Chinese essay writing. That puts off the research in this field. The NLP-TEA workshop has been holding a Chinese Grammar Error Detection (CGED) shared task in the workshop for two years since 2014 (Yu et al., 2014) (Lee et al. 2015). They provides a set of learner corpus and a clear definition on 4 major Grammar error types in the foreign learner c"
W16-4909,W12-6339,1,0.855357,"Missing"
W18-3718,S13-2046,0,0.600854,"ined as features for a supervised learning model. Features range from word level n-gram overlap to deeper semantic similarity measures based on dictionary and distributional methods. The short-text grading in SemEval Semantic Textual Similarity (STS) task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) drew the attention of many researchers and provided an evaluation platform. Since then, several systems have been proposed for short answer grading based on the semantic similarity with given reference answers (Mohler and Mihalcea, 2009; Mohler et al., 2011; Heilman and Madnani, 2013; Ramachandran et al., 2015). (Sultan et al., 2016) presented a simple short answer grading system for short answer in English. Given a question and its reference answers, a system measures the correctness of a student answer by calculating the similarity with the correct answers. Comparing to the field in English, there are very little research projects on short answer grading in Chinese, and there is no publicly available corpus for short answering grading in Chinese. In this paper we report how we build a system and how to test it with a translated corpus from two publicly available English"
W18-3718,W03-1726,0,0.236665,"Missing"
W18-3718,E09-1065,0,0.778898,"rning. A large set of similarity measures is defined as features for a supervised learning model. Features range from word level n-gram overlap to deeper semantic similarity measures based on dictionary and distributional methods. The short-text grading in SemEval Semantic Textual Similarity (STS) task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) drew the attention of many researchers and provided an evaluation platform. Since then, several systems have been proposed for short answer grading based on the semantic similarity with given reference answers (Mohler and Mihalcea, 2009; Mohler et al., 2011; Heilman and Madnani, 2013; Ramachandran et al., 2015). (Sultan et al., 2016) presented a simple short answer grading system for short answer in English. Given a question and its reference answers, a system measures the correctness of a student answer by calculating the similarity with the correct answers. Comparing to the field in English, there are very little research projects on short answer grading in Chinese, and there is no publicly available corpus for short answering grading in Chinese. In this paper we report how we build a system and how to test it with a trans"
W18-3718,W16-4904,0,0.0403806,"Missing"
W18-3718,P11-1076,0,0.44038,"arity measures is defined as features for a supervised learning model. Features range from word level n-gram overlap to deeper semantic similarity measures based on dictionary and distributional methods. The short-text grading in SemEval Semantic Textual Similarity (STS) task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) drew the attention of many researchers and provided an evaluation platform. Since then, several systems have been proposed for short answer grading based on the semantic similarity with given reference answers (Mohler and Mihalcea, 2009; Mohler et al., 2011; Heilman and Madnani, 2013; Ramachandran et al., 2015). (Sultan et al., 2016) presented a simple short answer grading system for short answer in English. Given a question and its reference answers, a system measures the correctness of a student answer by calculating the similarity with the correct answers. Comparing to the field in English, there are very little research projects on short answer grading in Chinese, and there is no publicly available corpus for short answering grading in Chinese. In this paper we report how we build a system and how to test it with a translated corpus from two"
W18-3718,W15-0612,0,0.11284,"rvised learning model. Features range from word level n-gram overlap to deeper semantic similarity measures based on dictionary and distributional methods. The short-text grading in SemEval Semantic Textual Similarity (STS) task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) drew the attention of many researchers and provided an evaluation platform. Since then, several systems have been proposed for short answer grading based on the semantic similarity with given reference answers (Mohler and Mihalcea, 2009; Mohler et al., 2011; Heilman and Madnani, 2013; Ramachandran et al., 2015). (Sultan et al., 2016) presented a simple short answer grading system for short answer in English. Given a question and its reference answers, a system measures the correctness of a student answer by calculating the similarity with the correct answers. Comparing to the field in English, there are very little research projects on short answer grading in Chinese, and there is no publicly available corpus for short answering grading in Chinese. In this paper we report how we build a system and how to test it with a translated corpus from two publicly available English corpus. The system first ex"
W18-3718,N16-1123,0,0.0557417,"es range from word level n-gram overlap to deeper semantic similarity measures based on dictionary and distributional methods. The short-text grading in SemEval Semantic Textual Similarity (STS) task (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015) drew the attention of many researchers and provided an evaluation platform. Since then, several systems have been proposed for short answer grading based on the semantic similarity with given reference answers (Mohler and Mihalcea, 2009; Mohler et al., 2011; Heilman and Madnani, 2013; Ramachandran et al., 2015). (Sultan et al., 2016) presented a simple short answer grading system for short answer in English. Given a question and its reference answers, a system measures the correctness of a student answer by calculating the similarity with the correct answers. Comparing to the field in English, there are very little research projects on short answer grading in Chinese, and there is no publicly available corpus for short answering grading in Chinese. In this paper we report how we build a system and how to test it with a translated corpus from two publicly available English corpus. The system first extracts the text similar"
W18-3729,P09-2007,1,0.749605,"ences: Redundant, Selection, Disorder, and Missing. The research goal is to build a system that can detect the errors, identify the type of the error, and point out the position of the error in the sentence (Yu et al., 2014). This year, the 2. Previous Works 2.1 Pattern-Based Approach The pattern matching approach is an old approach, which has been used in many previous works (Wu et al., 2010; Chen et al., 2011). The pattern contains frequent error terms, in which a character is replace by a similar one. This is based on an assumption that students often make mistake among similar characters (Liu et al., 2009). The advantage of pattern matching is stable, the many drawback is it cost a lot to collect the patterns. The system is based on the previous work, the error pattern from a native student essay corpus in traditional Chinese. Before testing 199 Proceedings of the 5th Workshop on Natural Language Processing Techniques for Educational Applications, pages 199–202 c Melbourne, Australia, July 19, 2018. 2018 Association for Computational Linguistics the system, the test data is transformed into traditional by MS-Word 2010. 一點 DET O 冷 Vi O 2.2 Sequential Labelling Approach 了 T R 你 N O 的 T R 過年 Vi O"
W18-3729,D14-1179,0,0.0117296,"Missing"
W18-3729,W16-4909,1,0.752346,"Missing"
