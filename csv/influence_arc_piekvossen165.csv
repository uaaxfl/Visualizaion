2016.gwc-1.43,W97-0802,0,0.757793,"verbs, ODWN has 154 tops. The reduction of the tops is due to the additional relations that were created in Cornetto to provide more structure to the verb hierarchy. In Cornetto, there are only two top nodes for the verb hierarchy. Open Dutch WordNet currently contains a limited amount of monosemous adjectives. We hope to be able to map the polysemous adjective synsets to PWN synsets by translating the Dutch glosses and by making use of the synset relations in Cornetto and Princeton WordNet. Because Dutch is very close to German, another possibility is to map the Cornetto synsets to GermaNet (Hamp and Feldweg, 1997) and make use of the rich set of synset relations that it provides. Finally, the current format of the resource is XML. We would also like to make the resource available in RDF (Klyne and Carroll, 2006). 6 Conclusion We described Open Dutch WordNet, which is derived from the Cornetto database, Princeton WordNet and various external resources. We exploited existing equivalence relations between Cornetto synsets and WordNet synsets in order to replace WordNet synonyms by Dutch synonyms. In addition, the inter-language links in various external resources such as Wiktionary and Omegawiki were used"
2016.gwc-1.43,W14-0102,0,0.0119328,"s not have a direct ESR (no ESR or one of EQ HAS HYPERONYM) to a WordNet Synset but the parent of the Cornetto synset does have an ESR to a WordNet synset. In that case a new synset (not represented in WordNet) is created as a hyponym of the target of the ESR of the hyperonym. Finally, the ESRs are used to insert Cornetto synset relations into Open Dutch WordNet that do not originate from Van Dale but were created manually in one of the projects. 3.3 External resources Using various external open source resources such as Wiktionary (Foundation, 2014b), Omegawiki 4 , and Google (Google, 2014), Oliver (2014) translated both monosemous and polysemous lemmas into Dutch for the part of speeches noun, verb, and adjective. For the monosemous lemmas, the English lemmas are simply translated into Dutch. For the polysemous lemmas, the gloss overlap between examples in an external resource and the possible WordNet synsets for a lemma are used to determine the correct synset for a lemma. We used a similar procedure to add synonyms from Wikipedia (Wikipedia, 2014; Foundation, 2014a). 3.4 Adjectives extended We created a mapping for two kinds of adjectives: monosemous adjectives, that have only one sense in"
2016.gwc-1.43,W14-5815,0,0.0640658,"Missing"
2016.gwc-1.43,tiedemann-2012-parallel,0,0.0122617,"edure to add synonyms from Wikipedia (Wikipedia, 2014; Foundation, 2014a). 3.4 Adjectives extended We created a mapping for two kinds of adjectives: monosemous adjectives, that have only one sense in WordNet, and ‘slightly polysemous adjectives’ that have exactly one adjectival sense and one nominal sense. Adjectives of the latter kind are typically nationalities (Cameroonian), religious denominations (Buddhist), and words like purebred. To create the mapping, we translated the English word forms using Google Translate and Bing Translate. We also use the word alignments from the OPUS project (Tiedemann, 2012). These resources provide us with Dutch candidate word forms that should correspond to the original WordNet synonyms in synsets. We then checked for each word form how many senses are associated with them in RBN. If there is only one (and the word is indeed an adjective), we conclude that this Dutch sense corresponds with the original WordNet synset. One problem with the translation-based approach is that Dutch adjectives are sometimes in4 http://www.omegawiki.org/ flected with the suffix -e. For example, the English ontological is automatically translated by Google to ontologische. In RBN, al"
2016.gwc-1.51,kipper-etal-2006-extending,0,0.112706,"O (Segers et al., 2015) is a newly developed domain ontology to enhance the extraction and linking of dynamic and static events and their implications in text. Explicit modeling of event implications allows for extracting sequences of states and changes over time regardless of if this information was directly expressed in text, or inferred by a reasoner. Figure 1 shows such a chain of expressions for dynamic (hire, starts at, fire, leave) and static events (works for, employs, is CEO) and their implied situations. Lexicons that define implications of events, e.g. VerbNet (Kipper et al., 2000; Kipper et al., 2006), are rare and usually focus on the meaning of verbs in isolation. However, lexical structures do not make explicit how the meaning of a verb needs to be combined with other event components, such as the participants and the temporal properties for the purpose of semantic parsing. We therefore follow an ontological approach to interpret situations on the basis of text interpretation of all the event components to make the implications explicit. Though some research on deductive reasoning over Frame annotated text (e.g. (Scheffczyk et al., 2006)) and defining pre and post situations of predicat"
2016.gwc-1.51,P98-1013,0,0.535899,"the car 600 dollar 600 dollar the car during situation the car hasValue 600 dollar Figure 3: Non-formal transcription of the mappings, assertions and instantiation for the ESO class FinancialTransaction from ESO roles (65) to FrameNet Frame Elements (131). The properties in this table pertain to those properties that are used in the situation rule assertions. 3 Predicate Matrix The PredicateMatrix (PM)(de Lacalle et al., 2014a; de Lacalle et al., 2014b) is an automatic extension of SemLink (Palmer, 2009) that merges several models of predicates such as VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). The PM also contains for each predicate features of the ontologies integrated in the Multilingual Central Repository (GonzalezAgirre et al., 2012) like SUMO (Niles and Pease, ´ 2001), Top Ontology (Alvez et al., 2008) or WordNet domains (Bentivogli et al., 2004). The mappings between such knowledge bases allow to take advantage from their individual strengths. For example, the coverage of PropBank or the semantic relations among events and participants of FrameNet. The semantic interoperability offered by the PM allows to translate"
2016.gwc-1.51,J05-1004,0,0.145031,"he car during situation the car hasValue 600 dollar Figure 3: Non-formal transcription of the mappings, assertions and instantiation for the ESO class FinancialTransaction from ESO roles (65) to FrameNet Frame Elements (131). The properties in this table pertain to those properties that are used in the situation rule assertions. 3 Predicate Matrix The PredicateMatrix (PM)(de Lacalle et al., 2014a; de Lacalle et al., 2014b) is an automatic extension of SemLink (Palmer, 2009) that merges several models of predicates such as VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). The PM also contains for each predicate features of the ontologies integrated in the Multilingual Central Repository (GonzalezAgirre et al., 2012) like SUMO (Niles and Pease, ´ 2001), Top Ontology (Alvez et al., 2008) or WordNet domains (Bentivogli et al., 2004). The mappings between such knowledge bases allow to take advantage from their individual strengths. For example, the coverage of PropBank or the semantic relations among events and participants of FrameNet. The semantic interoperability offered by the PM allows to translate the output of a SRL analysis to"
2016.gwc-1.51,W04-2214,0,0.537594,"re used in the situation rule assertions. 3 Predicate Matrix The PredicateMatrix (PM)(de Lacalle et al., 2014a; de Lacalle et al., 2014b) is an automatic extension of SemLink (Palmer, 2009) that merges several models of predicates such as VerbNet (Kipper et al., 2000), FrameNet (Baker et al., 1998), PropBank (Palmer et al., 2005) and WordNet (Fellbaum, 1998). The PM also contains for each predicate features of the ontologies integrated in the Multilingual Central Repository (GonzalezAgirre et al., 2012) like SUMO (Niles and Pease, ´ 2001), Top Ontology (Alvez et al., 2008) or WordNet domains (Bentivogli et al., 2004). The mappings between such knowledge bases allow to take advantage from their individual strengths. For example, the coverage of PropBank or the semantic relations among events and participants of FrameNet. The semantic interoperability offered by the PM allows to translate the output of a SRL analysis to Component Event classes – DynamicEvent classes – StaticEvent classes SUMO class mappings FrameNet Frame mappings Situation rules Situation rule assertions – Pre situation rule assertions – Post situation rule assertions During situation rule assertions Properties – Unary properties – Binary"
2016.gwc-1.51,W14-0150,1,0.882019,"Missing"
2016.gwc-1.51,lopez-de-lacalle-etal-2014-predicate,1,0.908889,"Missing"
2016.gwc-1.51,gonzalez-agirre-etal-2012-multilingual,1,0.882799,"Missing"
2016.gwc-1.51,C98-1013,0,\N,Missing
2016.gwc-1.59,W04-2214,0,0.0601204,"adhere to some version of the Princeton WordNet also means that the fund of concepts is biased towards an AngloSaxon worldview and is not open to concepts from other languages and cultures. It could be argued that these problems would go away if a single multilingual database was developed instead. This would, in theory, solve problems of incompatible formats and coordination. In practice, however there is no single group that has expertise in all the world’s languages. Further, much experimentation is done in the different projects; adding new relations (Vossen, 1998), adding richer domains (Bentivogli et al., 2004), adding new parts-of-speech (Seah and Bond, 2014) and so forth. This would be harder to do in one monolithic project. As time passes, and PWN now celebrates its 25th anniversary, the need for implementing the GWG becomes more urgent. The GWG should be a platform for achieving linguistic and conceptual interoperability across wordnets and all related machinery. It should allow researchers to study the universals and idiosyncracies in lexicalisation across languages, to address fundamental questions about what is a word and what is a concept (Fellbaum and Vossen, 2010; Vossen and Fellbaum, 2011"
2016.gwc-1.59,2016.gwc-1.9,1,0.711705,"ike license (CC BY SA). In order to keep compatibility across the grid, any projects in the Global Wordnet Grid must have a license compatible with CC BY SA (such as the original wordnet license, CC BY, MIT and many others), and the entire grid will be released under this license. The individual projects, starting with PWN, are the foundations upon which the GWG is built, the CILI links them and the platform ties them together, allows for versioning and adaptation through the community. 4 The Collaborative ILI (CILI) The Collaborative ILI is an extension of the ILI defined in EuroWordNet (see Bond et al., 2016, for more details). As a base for the CILI we take the synsets currently in Princeton Wordnet 3.0, the de facto ILI for the Open Multilingual Wordnet. This shows its central position in the current wordnet community. Each synset in PWN 3.0 gives rise to a concept in the CILI. The CILI is just a collection of concepts to which all wordnets are linked. It does not duplicate the relations between these concepts as represented in any wordnet and it does not have any lexicalizations. Concepts and concept identifiers in the CILI are permanent. They will never be removed or changed. However, new con"
2016.gwc-1.59,P15-4013,1,0.881799,"Missing"
2016.gwc-1.59,2016.gwc-1.43,1,0.805029,"Missing"
2016.gwc-1.59,Q14-1018,0,0.021166,"Missing"
2016.gwc-1.59,vossen-etal-2014-newsreader,1,0.789611,"Missing"
2018.gwc-1.25,E09-1005,0,0.0379359,"Missing"
2018.gwc-1.25,P14-1023,0,0.0115362,"dNet and how they can be acquired. We evaluate two methods of extracting event coreference relations using WordNet relations against a manual annotation of 38 documents within the same topical domain of gun violence. We conclude that precision is reasonable but recall is lower because the WordNet hierarchy does not sufficiently capture the required coherence and perspective relations. 1 Introduction Synonyms from the same synset (Fellbaum, 1998) are assumed to be exchangeable in contexts. Similarly, word embeddings are based on sharing of contexts represented as vectors (Mikolov et al., 2013; Baroni et al., 2014). Both synsets and word embeddings capture some variation in language, but they do not fully capture variation in reference and coreference. Reference relations are different in that they cross local (sentence) contexts. We typically tell stories in discourse in which entities or events play different roles and reflect different phases in relation to the same incident (the topic of the story). Furthermore, authors may frame these entities and events differently either within the same story or across different stories. We can thus consider a story as a larger topical context within which co-ref"
2018.gwc-1.25,P10-1143,0,0.431858,"Missing"
2018.gwc-1.25,W13-1203,0,0.263923,"ave nominal or entity coreference for future work. We discuss two methods for obtaining event ReferenceNet data from text collections using semantic parsing: 1) text-to-data and 2) data-to-text. Text-to-data involves semantic text parsing without knowing the referents a priori and without knowing which texts report on the same incident. It therefore relies on high-quality cross-document event coreference resolution and it is computationally very expensive as it requires comparing all event mentions with each other (within and across documents). Automatic event coreference is a difficult task (Hovy et al., 2013) and made little progress over the years. To compare different approaches on the ECB+ dataset (Cybulska and Vossen, 2014), Yang et al. (2015) reimplemented stateof-the-art algorithms proposed by Bejan and Harabagiu (2010) and Chen and Ji (2009), as well as their own approach. They report 58.7 CoNLL-F1 (Luo et al., 2014) on ECB+ for their own approach, compared to 53.6 CoNLL-F1 for (Bejan and Harabagiu, 2010) and 55.2 CoNLL-F1 for (Chen and Ji, 2009). They obtained their results however only after boosting event detection from an original 65F to 95F by training a separate event detection system"
2018.gwc-1.25,W08-2123,0,0.0305693,"Missing"
2018.gwc-1.25,kingsbury-palmer-2002-treebank,0,0.483569,":5 gun:1 gunman:1 address:1 deal:1 handle:1 speak:1 pull:4 return:3 turn:3 use:2 mother:1 shoot:5 shooting:4 shot:2 shoot:26 shot:7 shooting:4 hit:3 charge:1 shoot:2 charge:1 hit:3 shoot:3 strike:2 send:7 post:4 message:1 message:1 send:1 treat:2 wound:3 hurt:3 death:7 die:4 die:9 death:7 run:1 kill:12 house:2 live:2 life:1 life:8 live:5 house:2 lose:4 loss:1 put:4 place:1 place:1 put:1 say:52 take:21 involve:10 need:9 come:8 get:8 tell:3 ask:2 bring:2 carry:2 want:2 conduct:1 involve:10 come:8 get:8 take:4 need:3 bring:2 want:2 carry:2 take:2 ask:2 take:2 need:1 which is trained on PropBank (Kingsbury and Palmer, 2002), and NomBank (Meyers et al., 2004). Improving the recall for lexical coverage therefore primarily requires improving the coverage of these resources. 8 Conclusions and future work In this paper, we present ReferenceNet: a network of referential relations between synsets that is complementary to WordNet and word embeddings. ReferenceNet consists of ReferenceSets that group synsets and words that make reference to similar entities and events within similar topical contexts. Typically, ReferenceSets reflect different local contexts and perspectives within a shared topical context as opposed to s"
2018.gwc-1.25,2016.gwc-1.9,1,0.79896,"ways of framing persons and events within these topics and according to some topical schema. ReferenceSets thus will reflect which synonyms from which synsets are used how frequently to make reference within the same topical context. Figure 1 shows two examples of a ReferenceSet for the two texts in the introduction that report on the same incident and thus the same topical context of gun-violence. We see separate ReferenceSets for the shooter and for the shooting. Each ReferenceSet consists of a list of synset-ref elements.2 The synset-ref element has attributes for the CILI identifier iid (Bond et al., 2016; Vossen et al., 2016b), the language specific WordNet synset, and the corefcount attribute to express how often this entity was mentioned in the text. Each synset-ref contains a list of surface-form elements with the surface form and its observed token frequency of making reference. We can see that the words span different synsets and also different parts-of-speech tags. The first ReferenceSet exhibits the perspective of the shooter and the suspect before the trial. We abstracted from the actual names of the people through a separate element and counter proper-name. The second ReferenceSet sh"
2018.gwc-1.25,W09-3208,0,0.324206,"ut knowing the referents a priori and without knowing which texts report on the same incident. It therefore relies on high-quality cross-document event coreference resolution and it is computationally very expensive as it requires comparing all event mentions with each other (within and across documents). Automatic event coreference is a difficult task (Hovy et al., 2013) and made little progress over the years. To compare different approaches on the ECB+ dataset (Cybulska and Vossen, 2014), Yang et al. (2015) reimplemented stateof-the-art algorithms proposed by Bejan and Harabagiu (2010) and Chen and Ji (2009), as well as their own approach. They report 58.7 CoNLL-F1 (Luo et al., 2014) on ECB+ for their own approach, compared to 53.6 CoNLL-F1 for (Bejan and Harabagiu, 2010) and 55.2 CoNLL-F1 for (Chen and Ji, 2009). They obtained their results however only after boosting event detection from an original 65F to 95F by training a separate event detection system on part of the ECB+ data. Without such nearly perfect event detection, their results are much lower. All three approaches are clustering approaches over the dataset using event mentions as input. Likewise, they can only recover coreference rel"
2018.gwc-1.25,cybulska-vossen-2014-using,1,0.93474,"rom text collections using semantic parsing: 1) text-to-data and 2) data-to-text. Text-to-data involves semantic text parsing without knowing the referents a priori and without knowing which texts report on the same incident. It therefore relies on high-quality cross-document event coreference resolution and it is computationally very expensive as it requires comparing all event mentions with each other (within and across documents). Automatic event coreference is a difficult task (Hovy et al., 2013) and made little progress over the years. To compare different approaches on the ECB+ dataset (Cybulska and Vossen, 2014), Yang et al. (2015) reimplemented stateof-the-art algorithms proposed by Bejan and Harabagiu (2010) and Chen and Ji (2009), as well as their own approach. They report 58.7 CoNLL-F1 (Luo et al., 2014) on ECB+ for their own approach, compared to 53.6 CoNLL-F1 for (Bejan and Harabagiu, 2010) and 55.2 CoNLL-F1 for (Chen and Ji, 2009). They obtained their results however only after boosting event detection from an original 65F to 95F by training a separate event detection system on part of the ECB+ data. Without such nearly perfect event detection, their results are much lower. All three approache"
2018.gwc-1.25,P14-2006,0,0.0280946,"e same incident. It therefore relies on high-quality cross-document event coreference resolution and it is computationally very expensive as it requires comparing all event mentions with each other (within and across documents). Automatic event coreference is a difficult task (Hovy et al., 2013) and made little progress over the years. To compare different approaches on the ECB+ dataset (Cybulska and Vossen, 2014), Yang et al. (2015) reimplemented stateof-the-art algorithms proposed by Bejan and Harabagiu (2010) and Chen and Ji (2009), as well as their own approach. They report 58.7 CoNLL-F1 (Luo et al., 2014) on ECB+ for their own approach, compared to 53.6 CoNLL-F1 for (Bejan and Harabagiu, 2010) and 55.2 CoNLL-F1 for (Chen and Ji, 2009). They obtained their results however only after boosting event detection from an original 65F to 95F by training a separate event detection system on part of the ECB+ data. Without such nearly perfect event detection, their results are much lower. All three approaches are clustering approaches over the dataset using event mentions as input. Likewise, they can only recover coreference relations between mentions that match local structural 1 2 3 4 5 6 7 8 9 10 11 1"
2018.gwc-1.25,W04-2705,0,0.400916,"1 speak:1 pull:4 return:3 turn:3 use:2 mother:1 shoot:5 shooting:4 shot:2 shoot:26 shot:7 shooting:4 hit:3 charge:1 shoot:2 charge:1 hit:3 shoot:3 strike:2 send:7 post:4 message:1 message:1 send:1 treat:2 wound:3 hurt:3 death:7 die:4 die:9 death:7 run:1 kill:12 house:2 live:2 life:1 life:8 live:5 house:2 lose:4 loss:1 put:4 place:1 place:1 put:1 say:52 take:21 involve:10 need:9 come:8 get:8 tell:3 ask:2 bring:2 carry:2 want:2 conduct:1 involve:10 come:8 get:8 take:4 need:3 bring:2 want:2 carry:2 take:2 ask:2 take:2 need:1 which is trained on PropBank (Kingsbury and Palmer, 2002), and NomBank (Meyers et al., 2004). Improving the recall for lexical coverage therefore primarily requires improving the coverage of these resources. 8 Conclusions and future work In this paper, we present ReferenceNet: a network of referential relations between synsets that is complementary to WordNet and word embeddings. ReferenceNet consists of ReferenceSets that group synsets and words that make reference to similar entities and events within similar topical contexts. Typically, ReferenceSets reflect different local contexts and perspectives within a shared topical context as opposed to synsets and word embeddings that cap"
2018.gwc-1.25,strapparava-valitutti-2004-wordnet,0,0.441368,"Missing"
2018.gwc-1.25,P10-4014,0,\N,Missing
2019.gwc-1.12,I17-1001,0,0.0535863,"Missing"
2019.gwc-1.12,W18-5449,0,0.130126,"r the property. Whether this evidence is sufficient for a distributional model to represent the property is an open question. 4 A Dataset of Concepts and Properties This section describes the design of our dataset. We first outline the experiments we envision, because they provide the motivation of some of the key properties of our dataset. To conduct experiments on whether the predictions introduced in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we propose"
2019.gwc-1.12,W15-0107,0,0.113611,"es of a property. In addition to proposing a dataset design, we offer specific hypotheses based on a variety of observations from different fields about information that is likely or unlikely to be expressed in English natural language corpora. Rather than making claims based on entire categories of semantic properties, we base our predictions on underlying factors involved in the relations between concepts and properties. By testing these hypotheses, we hope to go beyond insights from experimental approaches comparing the information captured in embeddings to semantic feature norm sets (e.g. Fagarasan et al. (2015), Herbelot and Vecchi (2015), Tsvetkov et al. (2015), Derby et al. (2018), Sommerauer and Fokkens (2018)). Finally, we hope that comparing the relations captured by our dataset to traditional, taxonomic categories represented in WordNet may yield insights about the relation between properties of concepts and categorization. This could be extended to other languages to enable crosslinguistic comparisons. Acknowledgments This research is funded by the PhD in the Humanities Grant provided by the Netherlands Organization of Scientific Research (Nederlandse Organisatie voor Wetenschappelijk Onderzo"
2019.gwc-1.12,W16-2507,0,0.0141945,"tic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish them from other classes. Therefore, the distribution of positive and negative examples of properties is crucial to ensure that the vec"
2019.gwc-1.12,N16-2002,0,0.0129859,"ov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish them from other classes. Therefore, the distribution of positive and negative examples of properties is crucial to ensure that the vector dimensions discovere"
2019.gwc-1.12,D15-1003,0,0.0993304,"tion to proposing a dataset design, we offer specific hypotheses based on a variety of observations from different fields about information that is likely or unlikely to be expressed in English natural language corpora. Rather than making claims based on entire categories of semantic properties, we base our predictions on underlying factors involved in the relations between concepts and properties. By testing these hypotheses, we hope to go beyond insights from experimental approaches comparing the information captured in embeddings to semantic feature norm sets (e.g. Fagarasan et al. (2015), Herbelot and Vecchi (2015), Tsvetkov et al. (2015), Derby et al. (2018), Sommerauer and Fokkens (2018)). Finally, we hope that comparing the relations captured by our dataset to traditional, taxonomic categories represented in WordNet may yield insights about the relation between properties of concepts and categorization. This could be extended to other languages to enable crosslinguistic comparisons. Acknowledgments This research is funded by the PhD in the Humanities Grant provided by the Netherlands Organization of Scientific Research (Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO) PGW.17.041 awarded"
2019.gwc-1.12,W14-1618,0,0.015348,"in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish t"
2019.gwc-1.12,Q15-1016,0,0.0234751,"y we are confident about (i.e. we do not include concepts returned by a search for a category containing ‘mixed’ examples). This results in a selection of candidate concepts which are very difficult to separate into positive and negative examples based on general similarity. We collect the 200 nearest neighbors of this approximate property representation. We exclude negative examples further away from the centroid than the furthest positive example by manual inspection. The embedding model used in this step is the skip-gram model with negative sampling (using recommended settings according to Levy et al. (2015)), trained on the full Wikipedia corpus (dump from August 2018). 4.4 Sampling for the Crowd The strategies outlined above result in rather large numbers of candidates not all of which are useful (e.g. the distributional model returns non-standard spelling variants and words other than nouns). We reduce and clean the resulting sets (1) by means of preprocessing and (2) sampling based on characteristics with potential impact on how well distributional data can represent information. The characteristics we consider are (1) different types of ambiguity, (2) psycholinguistic factors such as concret"
2019.gwc-1.12,W16-2503,0,0.0135416,"uch as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particular class and distinguish them from other classes. Therefore, the distribution of positive and negative examples of properties is cruc"
2019.gwc-1.12,N13-1090,0,0.0780938,"predictions introduced in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn to distinguish vectors of words clearly associated with a property from vectors of words which are clearly not associated with the property. Any supervised classification approach relies on finding regularities which are shared among all or most examples of a particul"
2019.gwc-1.12,W18-5430,1,0.911963,"is an open question. 4 A Dataset of Concepts and Properties This section describes the design of our dataset. We first outline the experiments we envision, because they provide the motivation of some of the key properties of our dataset. To conduct experiments on whether the predictions introduced in Section 3 hold, we plan to use approaches suggested in the field of investigating neural network representations, such as diagnostic classification (Belinkov et al., 2017; Hupkes et al., 2018; Derby et al., 2018). In particular, we plan to extend the experiments presented in (accessed 2019/09/30) Sommerauer and Fokkens (2018), which try to investigate whether dimensions of embedding representations can capture semantic properties. While this seems to be implied by the method of inferring the missing word in an analogy pair by means of vector subtraction and addition (Mikolov et al., 2013; Levy and Goldberg, 2014), analogy calculation methods have been heavily criticized, calling this notion into question (Linzen, 2016; Gladkova and Drozd, 2016; Gladkova et al., 2016). To shed light on this, we proposed an experimental set-up in which we tested whether a supervised machine learning system could successfully learn t"
2019.gwc-1.12,speer-havasi-2012-representing,0,0.30742,"Missing"
2019.gwc-1.12,D15-1243,0,0.0916102,"design, we offer specific hypotheses based on a variety of observations from different fields about information that is likely or unlikely to be expressed in English natural language corpora. Rather than making claims based on entire categories of semantic properties, we base our predictions on underlying factors involved in the relations between concepts and properties. By testing these hypotheses, we hope to go beyond insights from experimental approaches comparing the information captured in embeddings to semantic feature norm sets (e.g. Fagarasan et al. (2015), Herbelot and Vecchi (2015), Tsvetkov et al. (2015), Derby et al. (2018), Sommerauer and Fokkens (2018)). Finally, we hope that comparing the relations captured by our dataset to traditional, taxonomic categories represented in WordNet may yield insights about the relation between properties of concepts and categorization. This could be extended to other languages to enable crosslinguistic comparisons. Acknowledgments This research is funded by the PhD in the Humanities Grant provided by the Netherlands Organization of Scientific Research (Nederlandse Organisatie voor Wetenschappelijk Onderzoek, NWO) PGW.17.041 awarded to Pia Sommerauer and NW"
2020.coling-main.422,J08-4004,0,0.4425,"ions. It includes more fine-grained semantic judgments and intentionally ambiguous words. We can thus expect even more disagreement than already observed in Herbelot and Vecchi (2016). Despite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement. Traditionally, annotations by a few annotators who worked on the same units are evaluated in terms of Kappa scores (usually Cohens’s kappa) and tasks with varying workers annotating the same units (usually crowd tasks) in terms of Krippendorff’s alpha (Artstein and Poesio, 2008). The CrowdTruth framework suggested in Aroyo and Welty (2014) and Aroyo and Welty (2015) offers a more fine-grained view by distinguishing the levels of workers, units and labels, rather than reducing the entire task to a single score. The goal is to distinguish meaningful disagreements (i.e. agreements by reliable annotators) from noise (i.e. disagreement or agreement by generally unreliable annotators). The framework provides scores for workers, annotation units (clear units receive a high score, units triggering disagreement between reliable annotators a low score), labels and associations"
2020.coling-main.422,Q19-1004,0,0.065104,"es and can thus provide highly relevant semantic information. Existing evaluation and label extraction methods, however, still heavily rely on agreement between annotators, which implies a single correct interpretation. Finished datasets rarely provide indications about difficulty and ambiguity on the level of annotated units. The explanatory power of NLP experiments that aim to evaluate or analyze models depends on the informativeness of the data. This is particularly relevant for experiments which specifically aim to understand models better, such as the tradition of diagnostic experiments (Belinkov and Glass, 2019).Traditional error analyses could also benefit substantially from test sets which contain information about phenomena with a likely impact on model performance. Furthermore, knowing whether model-errors are similar to human disagreements can yield important insights about models. For instance, an analysis of natural language inference models shows that classifiers do not necessarily capture the same type of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 4798 Proceedings of the 28th Internation"
2020.coling-main.422,W15-1510,0,0.0305681,"n the quality and informativeness of the underlying data (Hupkes et al., 2018). In this paper, we present an approach to crowd-annotation for a diagnostic dataset which attempts to tackle these limitations. The dataset is meant to test which semantic properties are captured by distributional word representations. The task is designed to trigger fine-grained semantic judgements of potentially ambiguous examples. The behavior of ambiguous words in distributional semantic models is not well understood and thus particularly interesting (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015). We investigate to what extent existing and new quality metrics indicate annotation accuracy on the one hand and ambiguity and difficulty of annotation units on the other hand. We evaluate our task from three perspectives: (1) comparison against an expert-annotated gold standard, (2) a task-specific coherence metric independent of agreement and (3) evaluation in terms of inter-annotator agreement metrics compared to predefined expectations about agreement and disagreement. In particular, we aim to investigate (1) how we can exploit the strengths and weaknesses of various suggested metrics to"
2020.coling-main.422,N19-1224,0,0.329377,"r for cases where we expect worker agreement. The remainder of this paper is structured as follows: After reviewing related work (Section 2), we introduce the use-case of a diagnostic dataset (Section 3) and describe the annotation task (Section 4). We present our expert-annotated gold standard in Section 5 and different quality metrics in Section 6. The results of our experiments are described in Section 7, followed by a discussion and conclusion. 2 Related work Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019). Pavlick and Kwiatkowski (2019) demonstrate that the fundamental task of Natural Language Inferencing contains large proportions of instances with multiple valid interpretations and argue that this phenomenon is central to the task rather than an aspect which can be disregarded. Herbelot and Vecchi (2016) show that even experts disagree on a difficult semantic annotation task and that interpretations are likely to vary due to differences in conceptualizations, which are in them"
2020.coling-main.422,P03-1068,0,0.232296,"Missing"
2020.coling-main.422,Q19-1043,0,0.396066,"ermore, knowing whether model-errors are similar to human disagreements can yield important insights about models. For instance, an analysis of natural language inference models shows that classifiers do not necessarily capture the same type of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 4798 Proceedings of the 28th International Conference on Computational Linguistics, pages 4798–4809 Barcelona, Spain (Online), December 8-13, 2020 ambiguity and uncertainty as reflected in the annotations (Pavlick and Kwiatkowski, 2019). Error analysis often require manual annotation and tend to focus on small and not representative subsections of test sets (Wu et al., 2019). We argue that the behavior of human annotators can provide rich information which should be exploited, rather than reduced to single labels. Information about (dis)agreement is a by-product of the original annotation effort and thus comes for free. It can form the basis of an error analysis or, in the case of our data, should be used to draw informative conclusions from diagnostic experiments. Such experiments crucially depend on the quality and informa"
2020.coling-main.422,N19-1176,0,0.0285436,"s follows: After reviewing related work (Section 2), we introduce the use-case of a diagnostic dataset (Section 3) and describe the annotation task (Section 4). We present our expert-annotated gold standard in Section 5 and different quality metrics in Section 6. The results of our experiments are described in Section 7, followed by a discussion and conclusion. 2 Related work Recent annotation studies recognize that ambiguity, vagueness and varying degrees of difficulty are inherent to semantic phenomena (Dumitrache et al., 2019; Aroyo and Welty, 2015; Erk et al., 2003; Kairam and Heer, 2016; Poesio et al., 2019; Pavlick and Kwiatkowski, 2019). Pavlick and Kwiatkowski (2019) demonstrate that the fundamental task of Natural Language Inferencing contains large proportions of instances with multiple valid interpretations and argue that this phenomenon is central to the task rather than an aspect which can be disregarded. Herbelot and Vecchi (2016) show that even experts disagree on a difficult semantic annotation task and that interpretations are likely to vary due to differences in conceptualizations, which are in themselves justified and cannot simply be disregarded as ‘mistakes’. Information about am"
2020.coling-main.422,N16-3012,0,0.0268404,"ple of things which are light’. I know that having (an/an) blade is necessary for many things (a/an) razor does or is used for. You can find (a/an) t-shirt which is white. White is one of many possible colors (a/an) t-shirt usually has. The range of colors is almost unlimited. I think (a/an) wine glass can is made of plastic, but this is rare or uncommon. I think it is impossible for (a/an) corpse to be alive. typical of property affording activity variability open rare impossible Table 1: Examples of statements expressing semantic relations. . We used the freely available Lingoturk software (Pusse et al., 2016) to set up an annotation environment and distributed the task via the recruitment platform Prolific.3 Peer et al. (2017) show that the annotation quality of annotators recruited via Prolific is higher than for Amazon Mechanical Turk workers. The platform encourages fair payment and asks researchers to pay participants based on the time they estimate for a task rather than per annotated item. We split the dataset into batches of around 70 descriptions. A worker who is proficient in English would need about 10 minutes per batch. While some statements may be difficult to judge and therefore take"
2020.coling-main.422,W18-5430,1,0.911144,"m diagnostic experiments. Such experiments crucially depend on the quality and informativeness of the underlying data (Hupkes et al., 2018). In this paper, we present an approach to crowd-annotation for a diagnostic dataset which attempts to tackle these limitations. The dataset is meant to test which semantic properties are captured by distributional word representations. The task is designed to trigger fine-grained semantic judgements of potentially ambiguous examples. The behavior of ambiguous words in distributional semantic models is not well understood and thus particularly interesting (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019; Del Tredici and Bel, 2015). We investigate to what extent existing and new quality metrics indicate annotation accuracy on the one hand and ambiguity and difficulty of annotation units on the other hand. We evaluate our task from three perspectives: (1) comparison against an expert-annotated gold standard, (2) a task-specific coherence metric independent of agreement and (3) evaluation in terms of inter-annotator agreement metrics compared to predefined expectations about agreement and disagreement. In particular, we aim to investigate (1) how we can exploit the st"
2020.coling-main.422,2019.gwc-1.12,1,0.917628,"Approaches which aim to capture information about semantics (such as embedding analysis (Sommerauer and Fokkens, 2018; Yaghoobzadeh et al., 2019)), however, are much more complex as ambiguity, vagueness and differences in required knowledge are by no means marginal phenomena and cannot simply be disregarded. Furthermore, the role of ambiguity in the behavior of word embeddings is not fully understood yet (Del Tredici and Bel, 2015; Yaghoobzadeh et al., 2019). We have designed an annotation task to analyze how different aspects of word meaning are represented in distributional representations (Sommerauer et al., 2019). In this paper, we investigate how we can measure the quality of the annotations and capture valid disagreement, which is crucial information for the diagnostic experiments we want to conduct (Sommerauer, 2020). The task is similar to that of Herbelot and Vecchi (2016), but uses basic yes-no questions so that it is suitable for crowd-annotations. It includes more fine-grained semantic judgments and intentionally ambiguous words. We can thus expect even more disagreement than already observed in Herbelot and Vecchi (2016). Despite the central nature of phenomena triggering disagreement in anno"
2020.coling-main.422,2020.acl-srw.18,1,0.707159,"in required knowledge are by no means marginal phenomena and cannot simply be disregarded. Furthermore, the role of ambiguity in the behavior of word embeddings is not fully understood yet (Del Tredici and Bel, 2015; Yaghoobzadeh et al., 2019). We have designed an annotation task to analyze how different aspects of word meaning are represented in distributional representations (Sommerauer et al., 2019). In this paper, we investigate how we can measure the quality of the annotations and capture valid disagreement, which is crucial information for the diagnostic experiments we want to conduct (Sommerauer, 2020). The task is similar to that of Herbelot and Vecchi (2016), but uses basic yes-no questions so that it is suitable for crowd-annotations. It includes more fine-grained semantic judgments and intentionally ambiguous words. We can thus expect even more disagreement than already observed in Herbelot and Vecchi (2016). Despite the central nature of phenomena triggering disagreement in annotation tasks, we are not aware of evaluation methods that do not mainly rely on agreement. Traditionally, annotations by a few annotators who worked on the same units are evaluated in terms of Kappa scores (usua"
2020.coling-main.422,P19-1073,0,0.0252741,"guage inference models shows that classifiers do not necessarily capture the same type of This work is licensed under a Creative Commons Attribution 4.0 International License. creativecommons.org/licenses/by/4.0/. License details: http:// 4798 Proceedings of the 28th International Conference on Computational Linguistics, pages 4798–4809 Barcelona, Spain (Online), December 8-13, 2020 ambiguity and uncertainty as reflected in the annotations (Pavlick and Kwiatkowski, 2019). Error analysis often require manual annotation and tend to focus on small and not representative subsections of test sets (Wu et al., 2019). We argue that the behavior of human annotators can provide rich information which should be exploited, rather than reduced to single labels. Information about (dis)agreement is a by-product of the original annotation effort and thus comes for free. It can form the basis of an error analysis or, in the case of our data, should be used to draw informative conclusions from diagnostic experiments. Such experiments crucially depend on the quality and informativeness of the underlying data (Hupkes et al., 2018). In this paper, we present an approach to crowd-annotation for a diagnostic dataset whi"
2020.coling-main.422,P19-1574,0,0.0260658,"Missing"
2020.framenet-1.5,L16-1141,0,0.0222278,"Type Save Frame Element Save Reference Annotation 4.1. Resources In this subsection, we introduce the resources used in the annotation tool, i.e., the lexicon and the data. lexicon We make use of the canonical version 1.7 of FrameNet (Fillmore and Baker, 2010; Ruppenhofer et al., 2006). All annotations make use of a Resource Description Framework (RDF) of FrameNet, for which the two most common resources are Framester (Gangemi et al., 2016) Save Frame Annotation 2 https://getbootstrap.com/docs/3.4/css/ https://jquery.com/ 4 https://nodejs.org/en/ 3 Figure 4: Annotation workflow 35 and PreMOn (Corcoglioniti et al., 2016). We chose to use PreMOn since the project was more active.5 data acquisition We have developed a data architecture (Vossen et al., 2020) to obtain and represent the data according to the data model as presented in Subsection 3.2., for which we primarily make use of Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014). preprocessing spaCy6 is used for sentence splitting, tokenization, and part of speech tagging, for which models in English, Dutch, and Italian are used. The preprocessing is stored in the NLP Annotation Format (NAF) (Fokkens et al., 2014), a stand-off, multilayered annotation schema for r"
2020.framenet-1.5,W11-1506,1,0.764453,"presidential election, with structured information about the location, time, and participants of the incident. 2. each incident to be tagged with one or more event types, e.g., election. This makes it possible to generalize over incidents of the same type to learn which frames are typical. 3. different texts that make reference to the same incident, possibly written in multiple languages with varying document creation times and from different sources, which provides us with insights into cross-lingual differences, source perspectives and the impact of historical distance to the incident time (Cybulska and Vossen, 2011). 4. an environment for efficient and consistent FrameNet and reference annotation to (given) structured data. This makes it possible to consider the framing of the incident throughout all texts that make reference to it as a discourse unit. In this paper, we introduce an annotation tool in which both structured data about an incident and many reference texts describing that one incident are simultaneously presented to the user. This interface enables both FrameNet-based annoWe construct narratives to describe events in the world around us. The language that we use in those narratives forms a"
2020.framenet-1.5,W16-4011,0,0.0530062,"Missing"
2020.framenet-1.5,burchardt-etal-2006-salto,0,0.0880518,"ic set of frame elements that apply to the syntactic realization of the phrases dominated by the frame. We refer to Example (1). (1) FrameNet annotation tools Kidnapping [P ERPETRATOR Two men] kidnapped [V ICTIM the children] [T IME yesterday]. In this example, ‘kidnapped’ evokes Kidnapping, which consists of several frame elements. ‘Two men’ expresses 1 We chose to not use the OntoLex (McCrae et al., 2017) relationship ontolex:reference since it might lead to confusion in distinguishing between conceptual and referential relationships. 32 straint rules in order to speed up annotation. Salto (Burchardt et al., 2006) is a multi-level annotation tool, which can be used to annotate FrameNet information. The annotation starts with a syntactic analysis of a sentence. After determining the target word and labeling it with a frame, the constituents can be tagged with a frame element by means of drag and drop functionality. All four described annotation tools provide the functionality to annotate FrameNet information. All of them start with a syntactic analysis of the sentence and annotate FrameNet information on top of that analysis. They differ in what syntactic information is used and how this drives the anno"
2020.framenet-1.5,2020.framenet-1.3,1,0.862518,"Missing"
2020.framenet-1.5,S10-1008,0,0.0421944,", meaning that these frame elements are analyzed within sentence boundaries. When their absence is assumed to be bounded by sentenceexternal words or phrases, this information is not further specified. The downside of this approach is that we do not gain insight into the way that these core frame elements are linguistically encoded in the full discourse or if they are encoded at all. Certain approaches address this problem by going beyond the predicate scope in annotating unexpressed core frame elements. For instance, in SemEval2010 Task 10: Linking Events and Their Participants in Discourse (Ruppenhofer et al., 2010), unexpressed core frame elements were annotated outside of the scope of the predicate in order to gain insight into the referents of these unexpressed roles. A small number of texts from a work of Arthur Conan Doyle were annotated. There were three Variation of framing in FrameNet Within FrameNet, variation in framing can be observed by measuring the degree to which different subframes stand in a similar frame-to-frame relation to a superframe. See a classic example below. (2) Change of leadership a. Commerce sell [T IME Yesterday,] [S ELLER John] sold [B UYER Mary] [G OODS a book]. b. Commer"
2020.framenet-1.5,L18-1480,1,0.733809,"Missing"
2020.framenet-1.5,2020.lrec-1.387,1,0.878936,"as already observed by Fillmore, evidenced by the following quote: “since FrameNet has been working mainly on single sentences and has done nothing (yet) on connections within whole texts, the FrameNet database has nothing direct to offer.” (Andor, 2010, p.168) We aim at combining FrameNet annotations with referential annotations in order to analyze framing variation in texts describing an event. For this we need to extend FrameNet annotations to the discourse level. Following the data-to-text method described in Vossen et al. (2018), we make use of the data acquisition platform described in Vossen et al. (2020) to enable this type of research, for which we require: 1. a referential representation of an incident, i.e., an event instance such as the 2012 Slovenian presidential election, with structured information about the location, time, and participants of the incident. 2. each incident to be tagged with one or more event types, e.g., election. This makes it possible to generalize over incidents of the same type to learn which frames are typical. 3. different texts that make reference to the same incident, possibly written in multiple languages with varying document creation times and from differen"
2020.framenet-1.5,L16-1601,0,0.0543084,"Missing"
2020.framenet-1.5,L16-1688,0,0.0232612,"components of the real-world event. English FrameNet (Ruppenhofer et al., 2006) brought conceptual framing research to a computational setting. The English lexicon made it possible to gain insight into the relationship between lexical items and the semantic frames that they evoke. English FrameNet has also motivated researchers to create FrameNets in other languages such as in Japanese (Ohara et al., 2004), German (Burchardt et al., 2009), Swedish (Heppin and Gronostaj, 2012), Brazilian Portuguese (Laviola et al., 2017), Spanish (Subirats and Sato, 2003), French (Djemaa et al., 2016), Hebrew (Hayoun and Elhadad, 2016), and Latvian (Gruzitis et al., 2018). Multiple annotation efforts resulted in many corpora and also served as training, development, and test data to train 31 tations as well as referential linking to the incident that the reference texts make reference to. The analysis of conceptual and referential framing enriches research into variation in framing beyond the level of sentences and across different types of reference texts and languages. This paper is structured as follows. In Section 2., we introduce English FrameNet and the related work on frame annotation, followed by a discussion on com"
2020.framenet-1.5,heppin-gronostaj-2012-rocky,0,0.0213697,"ntial information. To perform such an analysis, we need both semantic resources to describe this conceptual information, and information about the components of the real-world event. English FrameNet (Ruppenhofer et al., 2006) brought conceptual framing research to a computational setting. The English lexicon made it possible to gain insight into the relationship between lexical items and the semantic frames that they evoke. English FrameNet has also motivated researchers to create FrameNets in other languages such as in Japanese (Ohara et al., 2004), German (Burchardt et al., 2009), Swedish (Heppin and Gronostaj, 2012), Brazilian Portuguese (Laviola et al., 2017), Spanish (Subirats and Sato, 2003), French (Djemaa et al., 2016), Hebrew (Hayoun and Elhadad, 2016), and Latvian (Gruzitis et al., 2018). Multiple annotation efforts resulted in many corpora and also served as training, development, and test data to train 31 tations as well as referential linking to the incident that the reference texts make reference to. The analysis of conceptual and referential framing enriches research into variation in framing beyond the level of sentences and across different types of reference texts and languages. This paper"
2020.lrec-1.387,cybulska-vossen-2010-event,1,0.716027,"compare specific reference texts grounded to the same incident for framing differences, i.e. which frames are used from the potential bag-of-frames and which are not. The former will help us to disambiguate expressions for the frames that they evoke, while the latter will learn the vocabulary for framing and show how much variation there is to frame the same incident across sources. For example, occurrences of the word fire in texts that refer to murder incidents are more likely to evoke the fn:Shoot projectiles frame than the fn:Firing frame for employment relations. Similarly as observed by Cybulska and Vossen (2010), an incident such as the fall of Srebrenica can be described as a violent conflict with shootings and transport of women and children, focusing on the reporting, or as deportation and genocide, focusing on the intention and the responsibility. There are two crucial questions to address for this to work: 1) at what level of abstraction do we need to aggregate text such that we maximize the volume of text that still exhibits coherent typical frames? 2) at what level of granularity do we need to aggregate text such that we maximize the volume of text and still obtain coherent temporal sequences"
2020.lrec-1.387,W13-1202,1,0.873194,"Missing"
2020.lrec-1.387,fokkens-etal-2017-grasp,1,0.819802,"Missing"
2020.lrec-1.387,E17-1045,0,0.102837,"Missing"
2020.lrec-1.387,C16-1112,1,0.843176,"erent ways, which often reflects different perspectives. Although there are many corpora capturing language, hardly any of these also represent the actual situations that texts refer to, let alone provide indications of which texts refer to the same situation. Event coreference corpora could serve this purpose as they are annotated for mentions of the same event. However, available event coreference corpora are very small and they exhibit hardly any ambiguity, i.e. there typically is one referent for each expression, nor variation, i.e. there are only one or few expressions for each referent (Ilievski et al., 2016; Postma et al., 2016). Not having sufficient texts that refer to the same or similar situations, or not knowing which texts do, makes it difficult to investigate the different ways in which people make reference. It also hampers the development of systems to automatically resolve (cross-document) coreference and to understand and develop technology that detects how events are framed. Imagine you want to create a text corpus that represents the language used to describe murders. How to proceed? You can use public corpora such as the Gigaword corpus (Napoles et al., 2012) and search for texts u"
2020.lrec-1.387,W16-6004,1,0.899937,"Missing"
2020.lrec-1.387,W15-4507,1,0.872197,"Missing"
2020.lrec-1.387,L18-1480,1,0.577993,"search for texts using keywords. How many murders will you find and will you find all murders? Referring to events as murder is actually already subjective and may miss situations that some people describe differently. Even if you get a substantial amount of texts about murders, we still do not know which texts make reference to the same murder. Such referential grounding is however a prerequisite to study differences in framing these events. The project Framing Situations in the Dutch Language1 tries to tackle this problem using the data-to-text method 1 http://dutchframenet.nl described in Vossen et al. (2018), which compiles massive text data (so-called reference texts) in different languages that is referentially grounded to specific event instances represented as so-called microworlds. We not only ground these texts but also automatically disambiguate mentions of these events in texts following a one-sense-per-event-type principle. Furthermore, we automatically derive the typical vocabulary and FrameNet frames (Baker et al., 2003) for different event types. We believe that inferring typical expressions and frames is an efficient and comprehensive way to enrich text collections with framing inter"
2020.lrec-1.387,W12-3018,0,0.0812971,"Missing"
2020.lrec-1.387,W16-5706,0,0.041195,"Missing"
2020.lrec-1.611,C16-1324,0,0.0459725,"Missing"
2020.lrec-1.611,W18-2501,0,0.0126978,"ocurator.org/es Sentences 23,467 Tokens 528,727 Table 1: The Vaccination Corpus. To ensure future accessibility of the web documents, we made use of their archived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool eHOST,6 which also provides options to calculate pairwise inter annotator agreement (IAA) in terms of F-Score, which is the weighted harmonic mean of precision and"
2020.lrec-1.611,J17-1004,0,0.0528841,"Missing"
2020.lrec-1.611,P17-1044,0,0.0266272,"sibility of the web documents, we made use of their archived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool eHOST,6 which also provides options to calculate pairwise inter annotator agreement (IAA) in terms of F-Score, which is the weighted harmonic mean of precision and recall calculated as 2*((precison*recall)/precision+recall). The IAA scores presented in this pap"
2020.lrec-1.611,P14-5010,0,0.00275018,"he ControCurator dataset.3 The data was filtered using a set of keywords (e.g. vaccin, inoculation) to make sure we only included relevant documents. 3 http://controcurator.org/es Sentences 23,467 Tokens 528,727 Table 1: The Vaccination Corpus. To ensure future accessibility of the web documents, we made use of their archived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool"
2020.lrec-1.611,L18-1524,0,0.0491706,"Missing"
2020.lrec-1.611,J05-1004,0,0.252949,"hived versions in the Internet Archive (http://archive.org). We took the most recent snapshot in the Archive at the time of collecting, and retrieved the meta data and texts from this snapshot. All texts in the corpus have been automatically preprocessed with Stanford CoreNLP (Manning et al., 2014)4 for tokenization, sentence splitting, part-of-speech tagging, lemmatization and dependency parsing. In addition, we used the AllenNLP tools (Gardner et al., 2018)5 for semantic role labeling, which is a reimplementation of a deep BiLSTM model (He et al., 2017) and uses the PropBank representation (Palmer et al., 2005) of predicate-argument structures in sentences. The preprocessing information has not been used to support the manual annotation process, but we it will be used in our future research to extract perspectives. Manual annotations were performed with the open source annotation tool eHOST,6 which also provides options to calculate pairwise inter annotator agreement (IAA) in terms of F-Score, which is the weighted harmonic mean of precision and recall calculated as 2*((precison*recall)/precision+recall). The IAA scores presented in this paper have been calculated with eHost, with lenient span match"
2020.lrec-1.611,C10-2117,0,0.0973323,"Missing"
2020.lrec-1.611,W06-0305,0,0.167196,"Missing"
2020.lrec-1.611,prasad-etal-2008-penn,0,0.308652,"Missing"
2020.lrec-1.611,W08-1203,0,0.106262,"Missing"
2020.lrec-1.611,sauri-etal-2014-newsome,0,0.016614,"per articles. Their schema focuses on two functional components of private states, i.e. the experiencers holding attitudes and the opinion targets towards which the attitudes are expressed. For this task we concentrate on the identification of opinion expressions and targets leaving the identification of holders for later work. Additionally, we do not annotate opinions on all topics, but only those on persons, groups of persons and institutions (called Person+ for the remainder of this section). This choice is motivated by the fact that the identification of opinion targets is difficult (see (Sauri et al., 2014)) which we aim to resolve by predefining possible opinion targets in the text. Besides, many different stakeholders participate in a debate like the vaccination debate and their attitude is not only expressed by giving opinions about the topic of vaccination, but also by criticizing and praising each other. As with most annotations of subjective content (Reidsma and op den Akker, 2008), our guidelines are not specified in extreme detail and the coding relies on the often subjective interpretation of the annotators. There are no fixed rules about how particular words should be annotated and sen"
2020.lrec-1.611,J17-3005,0,0.261612,"Missing"
2020.lrec-1.611,P10-1059,0,0.0539595,"Missing"
2020.lrec-1.611,W18-5207,1,0.848424,"be interpreted in different ways; (iii) it should also be stressed that claim-like statements that are not directly related to the topic need to be marked, as they are relevant; and (iv) the guidelines should be more restrictive. For example, the analysis of the errors related to attributability lead to the conclusion that claims should be attributable to a source and that the source should express a high level of commitment to the claim. A general conclusion is that agreeing on what a claim is still remains a difficult endeavor for human annotators. For more details about the annotation see Torsi and Morante (2018). 6. For this task a subset of the corpus is annotated. We selected 210 texts about children’s vaccinations thus excluding texts about, for example, the vaccination of animals and travellers. The documents were treated as follows: • Person+ entities were annotated to create a fixed set of possible opinion targets. 26,996 person+ entities were identified in 210 documents. Table 7 shows the most frequent entities. • The 210 documents were annotated with opinions by 2 different annotators. They found that only 168 of the documents indeed contained opinions. These 168 documents contain 23,000 Pers"
2020.lrec-1.611,L16-1187,1,0.702653,"Missing"
2020.lrec-1.680,P18-2005,0,0.0604696,"Missing"
2020.lrec-1.680,P18-3010,0,0.0204747,"Missing"
2020.lrec-1.680,P18-1246,0,0.0988842,"Missing"
2020.lrec-1.680,D16-1034,0,0.0641556,"Missing"
2020.lrec-1.680,P18-1197,0,0.0391306,"Missing"
2020.lrec-1.680,P18-1031,0,0.0492424,"Missing"
2020.lrec-1.680,W18-3026,0,0.0278988,"Missing"
2020.lrec-1.680,P17-2014,0,0.0241643,"Missing"
2020.lrec-1.680,S18-1112,0,0.196409,"Missing"
2020.lrec-1.680,W18-0515,0,0.0590939,"Missing"
2020.nl4xai-1.12,J95-4001,0,0.629975,"Missing"
2021.latechclfl-1.3,agerri-etal-2014-ixa,0,0.0292591,"han on the text; this may have to do with a more homogeneous language in the notes, but also with differences in sequence lengths, which are long on average in the text’s testset and short in the notes30 . The length of text sequences coupled with our working at the paragraph level may also explain the higher unstability of results of BERTje and mBERT, as these were trained on the sentence level. the notes also accommodates for GPE labels being concentrated in three letters (from 1744, 1750 and 1752). Data selection is summarized in Table 3. The data are tokenized with the IXA-pipe tokenizer (Agerri et al., 2014). We do not segment sentences with the tokenizer, but take paragraphs and separate notes as text units, splitting sequences longer than 256 subword units for each language model. This is motivated practically by the tokenizer being to greedy (the letters abound with abbreviations), but we also believe that working at the paragraph level may be beneficial as it provides more context for NER. Summary statistics are provided in Table 4. For both standard and semantic-oriented experiments, rare labels are mapped to lexically-related labels: REL and RELpart to RELderiv , LOCpart to LOCderiv , ORGpa"
2021.latechclfl-1.3,benikova-etal-2014-nosta,0,0.284131,"that notes and historical text are complementary for NER, as all pretrained models, multilingual and monolingual, benefit from their combination. On the more semantic-oriented NER task, we find that monolingual and multilingual models perform competitively, with RobBERT and mBERT performing best. This confirms the importance of the semantic nature of a task as a factor in choosing between monolingual and multilingual models. 2 Annotations The labelset consists of five base labels—LOC, ORG, PER, REL and SHP—for the entity types locations, organisations, persons, religions and ships. Following (Benikova et al., 2014), this labelset is extended with secondary labels of the form Xderiv or Xpart for expressions derived from entity names by grammatical derivation (as with for instance, the location Banda and the derived Bandanezen3 ) or composition (as with Java and Javakoffie4 ). The labelset is extended with a GPE label (geopolitical entities) to distinguish metonymical use of location names. 2.1 Data selection The data selected for annotations consist of letters spread out through the General Letters corpus5 . In editing these letters, Huygens ING transcribed parts of the letters, and summarized other part"
2021.latechclfl-1.3,2020.lrec-1.562,0,0.0324984,"disambiguate difficult cases, the annotators could rely on the indices of persons, locations and ships accompanying each volume of the corpus, as well as on a glossarium22 . Annotations were performed for the most part on raw text23 with the help of a gazetteer compiled from the indices; a few documents were preannotated with either string matching from the gazetteer or with a preliminary NER system. Agreement was measured halfway through the annotation process, on three documents (two with historical text and one with editorial notes). We measure inter-annotator agreement with F score: like Brandsen et al. (2020) and Deleger et al. (2012), we question the use of Cohen’s kappa for NER, as it is unclear how chance agreement should be defined for NER. Table 2 provides F scores for the text and notes and details cases of agreement and disagreement between both annotators. To this end, we first pair up annotations with a same span to isolate cases of agreement and cases of label disagreement. We then attempt to pair remaining annotations, and count annotations that overlap with annotations of the other annotator; those that do are cases of span disagreement, while the remaining cases are mentions that only"
2021.latechclfl-1.3,2020.acl-main.747,0,0.463205,"torical texts. Sophie Arnoult Vrije Universiteit Amsterdam s.i.arnoult@ vu.nl Lodewijk Petram Huygens ING lodewijk.petram@ huygens.knaw.nl Abstract BERT has given birth to a myriad of variants, differing by their training procedure, model size or language. Resource-rich languages in particular are spoilt for choice. Dutch for instance has two main monolingual models: the BERT-based BERTje (de Vries et al., 2019) and RobBERT (Delobelle et al., 2020), itself based on RoBERTa (Liu et al., 2019), a revision of BERT’s training procedure. But multilingual models like mBERT (Devlin, 2018) and XLM-R (Conneau et al., 2020) are also applicable. These models, which are trained on 104, respectively 100 languages at once, perform well on crosslingual transfer (Pires et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Pires et al. (2019) notably show that mBERT learns generic representations over different input languages and scripts. This makes multilingual models particularly interesting for historical texts, not only because these contain noncontemporary language, but also because they exhibit language variation due to language change over long periods of time or unstandardized spelling. But what when histo"
2021.latechclfl-1.3,P19-1493,0,0.0197854,"iven birth to a myriad of variants, differing by their training procedure, model size or language. Resource-rich languages in particular are spoilt for choice. Dutch for instance has two main monolingual models: the BERT-based BERTje (de Vries et al., 2019) and RobBERT (Delobelle et al., 2020), itself based on RoBERTa (Liu et al., 2019), a revision of BERT’s training procedure. But multilingual models like mBERT (Devlin, 2018) and XLM-R (Conneau et al., 2020) are also applicable. These models, which are trained on 104, respectively 100 languages at once, perform well on crosslingual transfer (Pires et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Pires et al. (2019) notably show that mBERT learns generic representations over different input languages and scripts. This makes multilingual models particularly interesting for historical texts, not only because these contain noncontemporary language, but also because they exhibit language variation due to language change over long periods of time or unstandardized spelling. But what when historical texts are accompanied by modern notes? How do monolingual and multilingual models compare on the historical and editorial parts of such texts, and on"
2021.latechclfl-1.3,2020.findings-emnlp.292,0,0.214218,"over long periods of time or unstandardized spelling. But what when historical texts are accompanied by modern notes? How do monolingual and multilingual models compare on the historical and editorial parts of such texts, and on their combination? If we only consider modern notes, it is not given that monolingual models outperform multilingual models on their training language. Notwithstanding possible differences in genre, the nature of the linguistic task at hand is an important factor in determining whether a monolingual or multilingual model is more appropriate. de Vries et al. (2019) and Delobelle et al. (2020) show for Dutch that mBERT is competitive with monolingual models on POS tagging (on Universal Dependencies POS), while Dutch models perform better at semantic tasks like Semantic Role Labelling or language-specific tasks like agreement resolution. For Named Entity Recognition (NER), results appear mixed, as both BERTje and RobBERT Pretrained language models like BERT have advanced the state of the art for many NLP tasks. For resource-rich languages, one has the choice between a number of language-specific models, while multilingual models are also worth considering. These models are well know"
2021.latechclfl-1.3,N19-1423,0,0.0378975,"slingual abilities of multilingual models while showing that all language models can leverage mixed-variant data. In particular, language models successfully incorporate notes for the prediction of entities in historical texts. We also find that multilingual models outperform monolingual models on our data, but that this superiority is linked to the task at hand: multilingual models lose their advantage when confronted with more semantical tasks. 1 Piek Vossen Vrije Universiteit Amsterdam p.t.j.m.vossen@ vu.nl Introduction Pretrained language models (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2019) have recently advanced the state-of-the-art in many NLP tasks, providing deep contextual language representations and ease of deployment. BERT (Devlin et al., 2019) has proven particularly successful, combining the attention-based contextual representations of Transformers (Vaswani et al., 2017) with a simple fine-tuning procedure, allowing to deploy quickly to any sequence or document classification task. 21 Proceedings of LaTeCH-CLfL 2021, pages 21–30 Punta Cana, Dominican Republic (Online), November 11, 2021. perform in-between mBERT and a version optimized by Wu and Dredze (2019). How sem"
2021.latechclfl-1.3,C18-2002,0,0.023804,"Missing"
2021.latechclfl-1.3,D18-2012,0,0.0159195,"nhabitants 22 http://resources.huygens.knaw.nl/ vocglossarium/VocGlossarium/zoekvoc 23 Extracted from TEI post-OCR files. Annotations on this text were later ported to the Text-Fabric release of the corpus. 24 Corrections were performed by one of the authors, after annotations were gathered. 24 F1 entities agreeing disagreeing - label - span - entity text notes total 88.5 92.1 90.8 492/484 432 910/877 823 1402/1361 1255 10 39/39 11/3 24 24/25 39/5 34 63/64 50/8 equivalent to BERTbase , with 12 Transformer layers of size H = 768 and 12 attention heads. The tokenizer is based on Sentence-Piece (Kudo and Richardson, 2018), and has a vocabulary size of 30k. Unlike BERT, BERTje is trained on a Sentence-Order Prediction objective next to Masked Language Modelling (MLM). RobBERT (Delobelle et al., 2020) is a Dutch model trained on 39GB of data from the OSCAR corpus (Ortiz Suárez et al., 2019). The model is structurally equivalent to BERTbase while following the training procedure of RoBERTa (Liu et al., 2019), with a dynamic MLM objective, and a tokenizer based on byte-level BPE following (Radford et al., 2019), with a vocabulary size of 40k (we consider RobBERT v2). Table 2: Inter-annotator agreement. Pairs of co"
2021.latechclfl-1.3,2021.ccl-1.108,0,0.0661377,"Missing"
2021.latechclfl-1.3,D19-1077,0,0.019515,"ad of variants, differing by their training procedure, model size or language. Resource-rich languages in particular are spoilt for choice. Dutch for instance has two main monolingual models: the BERT-based BERTje (de Vries et al., 2019) and RobBERT (Delobelle et al., 2020), itself based on RoBERTa (Liu et al., 2019), a revision of BERT’s training procedure. But multilingual models like mBERT (Devlin, 2018) and XLM-R (Conneau et al., 2020) are also applicable. These models, which are trained on 104, respectively 100 languages at once, perform well on crosslingual transfer (Pires et al., 2019; Wu and Dredze, 2019; Conneau et al., 2020). Pires et al. (2019) notably show that mBERT learns generic representations over different input languages and scripts. This makes multilingual models particularly interesting for historical texts, not only because these contain noncontemporary language, but also because they exhibit language variation due to language change over long periods of time or unstandardized spelling. But what when historical texts are accompanied by modern notes? How do monolingual and multilingual models compare on the historical and editorial parts of such texts, and on their combination? I"
bosma-vossen-2010-bootstrapping,E09-1005,0,\N,Missing
bosma-vossen-2010-bootstrapping,C92-3150,0,\N,Missing
bosma-vossen-2010-bootstrapping,P90-1034,0,\N,Missing
bosma-vossen-2010-bootstrapping,P98-2127,0,\N,Missing
bosma-vossen-2010-bootstrapping,C98-2122,0,\N,Missing
C16-1112,S10-1013,1,0.895678,"Missing"
C16-1112,W13-2322,0,0.0111547,"ng (EL) and Word Sense Disambiguation (WSD) (Hulpus¸ et al., 2015; Moro et al., 2014), combined event and entity coreference (EvC and EnC) (Lee et al., 2012) and resolving WSD and Semantic Role Labeling (SRL) together (Che and Liu, 2010). Although some task combinations are well-supported by multi-task datasets, such as CoNLL 2011 and 2012 for joint coreference (Pradhan et al., 2011; Pradhan et al., 2012), and Moro and Navigli (2015) for WSD and EL, still many multi-task systems have to be evaluated on separate datasets. Notable efforts to create multi-task annotated corpora are the AMR Bank (Banarescu et al., 2013) and the MEANTIME corpus (Minard et al., 2016a). Properties of existing datasets have been examined for individual tasks. For WSD, the correct sense of a lemma is shown to often coincide with the most frequent sense (Preiss, 2006) or the predominant sense (McCarthy et al., 2004). In the case of McCarthy et al. (2004), the predominant sense is deliberately adapted with respect to the topic of the text. Our work differs from McCarthy et al. (2004) because they do not consider the temporal dimension. As a response to sense-skewed datasets, Vossen et al. (2013) created a balanced sense corpus in t"
C16-1112,P13-1141,0,0.0607999,"uity in the current datasets for the tasks of EvC and EnC, respectively. Motivated by these findings, Guha et al. (2015) created a new dataset (QuizBowl), while Cybulska and Vossen (2014) extended the existing dataset ECB to ECB+, both efforts resulting in notably greater ambiguity and temporal diversity. As far as we are aware, no existing disambiguation dataset has included the temporal dependency of ambiguity, variance, or dominance. The problem of overfitting to a limited set of test data has been of central interest to the body of 1182 work focusing on domain adaptation (Daume III, 2007; Carpuat et al., 2013; Jiang and Zhai, 2007). By evaluating on a different domain than the training one, these efforts have provided valuable insights into system performance. However, to our knowledge, this research has also not addressed the temporal aspect of the task. We therefore propose to take this a step further and examine system performance with respect to a set of metrics, applicable over disambiguation tasks, thus setting the stage for creation of metric-aware datasets. We expect that these metrics show reduced complexity within well-defined temporal and topical boundaries and increased complexity acro"
C16-1112,C10-1019,0,0.0301522,"language system is still used for many different situations within a changing world. While this works for humans, this is not yet solved for machines. 3 Related Work The three problems enumerated in Section 1 have been addressed to some extent in past work. Several approaches have attempted to resolve pairs of disambiguation tasks jointly. Examples include: combined Entity Linking (EL) and Word Sense Disambiguation (WSD) (Hulpus¸ et al., 2015; Moro et al., 2014), combined event and entity coreference (EvC and EnC) (Lee et al., 2012) and resolving WSD and Semantic Role Labeling (SRL) together (Che and Liu, 2010). Although some task combinations are well-supported by multi-task datasets, such as CoNLL 2011 and 2012 for joint coreference (Pradhan et al., 2011; Pradhan et al., 2012), and Moro and Navigli (2015) for WSD and EL, still many multi-task systems have to be evaluated on separate datasets. Notable efforts to create multi-task annotated corpora are the AMR Bank (Banarescu et al., 2013) and the MEANTIME corpus (Minard et al., 2016a). Properties of existing datasets have been examined for individual tasks. For WSD, the correct sense of a lemma is shown to often coincide with the most frequent sens"
C16-1112,cybulska-vossen-2014-using,1,0.880588,". (2004) because they do not consider the temporal dimension. As a response to sense-skewed datasets, Vossen et al. (2013) created a balanced sense corpus in the DutchSemCor project in which each sense gets an equal number of examples. Similarly, Van Erp et al. (2016) conclude that EL datasets contain very little referential ambiguity. Evaluation is focused on well-known entities, i.e. entities with high PageRank (Page et al., 1999) values. Additionally, the authors observe a considerable overlap of entities across datasets, even for pairs of datasets that represent entirely different topics. Cybulska and Vossen (2014) and Guha et al. (2015) both stress the low ambiguity in the current datasets for the tasks of EvC and EnC, respectively. Motivated by these findings, Guha et al. (2015) created a new dataset (QuizBowl), while Cybulska and Vossen (2014) extended the existing dataset ECB to ECB+, both efforts resulting in notably greater ambiguity and temporal diversity. As far as we are aware, no existing disambiguation dataset has included the temporal dependency of ambiguity, variance, or dominance. The problem of overfitting to a limited set of test data has been of central interest to the body of 1182 work"
C16-1112,P07-1033,0,0.338402,"Missing"
C16-1112,N15-1117,0,0.118096,"nsider the temporal dimension. As a response to sense-skewed datasets, Vossen et al. (2013) created a balanced sense corpus in the DutchSemCor project in which each sense gets an equal number of examples. Similarly, Van Erp et al. (2016) conclude that EL datasets contain very little referential ambiguity. Evaluation is focused on well-known entities, i.e. entities with high PageRank (Page et al., 1999) values. Additionally, the authors observe a considerable overlap of entities across datasets, even for pairs of datasets that represent entirely different topics. Cybulska and Vossen (2014) and Guha et al. (2015) both stress the low ambiguity in the current datasets for the tasks of EvC and EnC, respectively. Motivated by these findings, Guha et al. (2015) created a new dataset (QuizBowl), while Cybulska and Vossen (2014) extended the existing dataset ECB to ECB+, both efforts resulting in notably greater ambiguity and temporal diversity. As far as we are aware, no existing disambiguation dataset has included the temporal dependency of ambiguity, variance, or dominance. The problem of overfitting to a limited set of test data has been of central interest to the body of 1182 work focusing on domain ada"
C16-1112,D11-1072,0,0.194922,"Missing"
C16-1112,P15-2079,0,0.014572,"be found at https://github.com/cltl/ SemanticOverfitting. 9 While computing RORA and RORV, we ignore cases with resource ambiguity and variance of 1. 8 1187 variance per concept. This is an indication of strong semantic overfitting of the data to a small selection that is not representative for the full potential of expressions and meanings. Furthermore, we observe that this representativeness is relatively constant across concept datasets, which in part can be explained by the fact that the WSD and SRL datasets mainly stem from the same time period (Figure 2), and even from the same corpus (Hovy and Søgaard, 2015). One could argue that the data is correctly representing the natural complexity of a specific time period and genre but it does not challenge systems to be able to shift from one situation to another. We also note a temporal discrepancy between the concept- and instance-based datasets, with the instance-based systems being evaluated on more recent data. Figure 2: DTR values of the datasets To understand further the time-bound interaction in our datasets, we study them together with timebound resources. While our lexical resources and instance knowledge sources contain very little temporal inf"
C16-1112,P07-1034,0,0.0415544,"tasets for the tasks of EvC and EnC, respectively. Motivated by these findings, Guha et al. (2015) created a new dataset (QuizBowl), while Cybulska and Vossen (2014) extended the existing dataset ECB to ECB+, both efforts resulting in notably greater ambiguity and temporal diversity. As far as we are aware, no existing disambiguation dataset has included the temporal dependency of ambiguity, variance, or dominance. The problem of overfitting to a limited set of test data has been of central interest to the body of 1182 work focusing on domain adaptation (Daume III, 2007; Carpuat et al., 2013; Jiang and Zhai, 2007). By evaluating on a different domain than the training one, these efforts have provided valuable insights into system performance. However, to our knowledge, this research has also not addressed the temporal aspect of the task. We therefore propose to take this a step further and examine system performance with respect to a set of metrics, applicable over disambiguation tasks, thus setting the stage for creation of metric-aware datasets. We expect that these metrics show reduced complexity within well-defined temporal and topical boundaries and increased complexity across these boundaries. Mo"
C16-1112,kingsbury-palmer-2002-treebank,0,0.278784,"Missing"
C16-1112,D12-1045,0,0.181884,"encouraged to focus on the temporal aspect of the task but in reality the same language system is still used for many different situations within a changing world. While this works for humans, this is not yet solved for machines. 3 Related Work The three problems enumerated in Section 1 have been addressed to some extent in past work. Several approaches have attempted to resolve pairs of disambiguation tasks jointly. Examples include: combined Entity Linking (EL) and Word Sense Disambiguation (WSD) (Hulpus¸ et al., 2015; Moro et al., 2014), combined event and entity coreference (EvC and EnC) (Lee et al., 2012) and resolving WSD and Semantic Role Labeling (SRL) together (Che and Liu, 2010). Although some task combinations are well-supported by multi-task datasets, such as CoNLL 2011 and 2012 for joint coreference (Pradhan et al., 2011; Pradhan et al., 2012), and Moro and Navigli (2015) for WSD and EL, still many multi-task systems have to be evaluated on separate datasets. Notable efforts to create multi-task annotated corpora are the AMR Bank (Banarescu et al., 2013) and the MEANTIME corpus (Minard et al., 2016a). Properties of existing datasets have been examined for individual tasks. For WSD, the"
C16-1112,P04-1036,0,0.0602445,"supported by multi-task datasets, such as CoNLL 2011 and 2012 for joint coreference (Pradhan et al., 2011; Pradhan et al., 2012), and Moro and Navigli (2015) for WSD and EL, still many multi-task systems have to be evaluated on separate datasets. Notable efforts to create multi-task annotated corpora are the AMR Bank (Banarescu et al., 2013) and the MEANTIME corpus (Minard et al., 2016a). Properties of existing datasets have been examined for individual tasks. For WSD, the correct sense of a lemma is shown to often coincide with the most frequent sense (Preiss, 2006) or the predominant sense (McCarthy et al., 2004). In the case of McCarthy et al. (2004), the predominant sense is deliberately adapted with respect to the topic of the text. Our work differs from McCarthy et al. (2004) because they do not consider the temporal dimension. As a response to sense-skewed datasets, Vossen et al. (2013) created a balanced sense corpus in the DutchSemCor project in which each sense gets an equal number of examples. Similarly, Van Erp et al. (2016) conclude that EL datasets contain very little referential ambiguity. Evaluation is focused on well-known entities, i.e. entities with high PageRank (Page et al., 1999) v"
C16-1112,L16-1699,0,0.0605941,"Missing"
C16-1112,S15-2049,0,0.0128585,"in Section 1 have been addressed to some extent in past work. Several approaches have attempted to resolve pairs of disambiguation tasks jointly. Examples include: combined Entity Linking (EL) and Word Sense Disambiguation (WSD) (Hulpus¸ et al., 2015; Moro et al., 2014), combined event and entity coreference (EvC and EnC) (Lee et al., 2012) and resolving WSD and Semantic Role Labeling (SRL) together (Che and Liu, 2010). Although some task combinations are well-supported by multi-task datasets, such as CoNLL 2011 and 2012 for joint coreference (Pradhan et al., 2011; Pradhan et al., 2012), and Moro and Navigli (2015) for WSD and EL, still many multi-task systems have to be evaluated on separate datasets. Notable efforts to create multi-task annotated corpora are the AMR Bank (Banarescu et al., 2013) and the MEANTIME corpus (Minard et al., 2016a). Properties of existing datasets have been examined for individual tasks. For WSD, the correct sense of a lemma is shown to often coincide with the most frequent sense (Preiss, 2006) or the predominant sense (McCarthy et al., 2004). In the case of McCarthy et al. (2004), the predominant sense is deliberately adapted with respect to the topic of the text. Our work"
C16-1112,Q14-1019,0,0.0308211,"ull complexity of the disambiguation task. Consequently, systems are not encouraged to focus on the temporal aspect of the task but in reality the same language system is still used for many different situations within a changing world. While this works for humans, this is not yet solved for machines. 3 Related Work The three problems enumerated in Section 1 have been addressed to some extent in past work. Several approaches have attempted to resolve pairs of disambiguation tasks jointly. Examples include: combined Entity Linking (EL) and Word Sense Disambiguation (WSD) (Hulpus¸ et al., 2015; Moro et al., 2014), combined event and entity coreference (EvC and EnC) (Lee et al., 2012) and resolving WSD and Semantic Role Labeling (SRL) together (Che and Liu, 2010). Although some task combinations are well-supported by multi-task datasets, such as CoNLL 2011 and 2012 for joint coreference (Pradhan et al., 2011; Pradhan et al., 2012), and Moro and Navigli (2015) for WSD and EL, still many multi-task systems have to be evaluated on separate datasets. Notable efforts to create multi-task annotated corpora are the AMR Bank (Banarescu et al., 2013) and the MEANTIME corpus (Minard et al., 2016a). Properties of"
C16-1112,S13-2040,0,0.0307608,"Missing"
C16-1112,S01-1005,0,0.0973633,"Missing"
C16-1112,W16-6004,1,0.793029,"Missing"
C16-1112,S07-1016,0,0.133685,"Missing"
C16-1112,W11-1901,0,0.0683011,"Missing"
C16-1112,W12-4501,0,0.0274835,"e three problems enumerated in Section 1 have been addressed to some extent in past work. Several approaches have attempted to resolve pairs of disambiguation tasks jointly. Examples include: combined Entity Linking (EL) and Word Sense Disambiguation (WSD) (Hulpus¸ et al., 2015; Moro et al., 2014), combined event and entity coreference (EvC and EnC) (Lee et al., 2012) and resolving WSD and Semantic Role Labeling (SRL) together (Che and Liu, 2010). Although some task combinations are well-supported by multi-task datasets, such as CoNLL 2011 and 2012 for joint coreference (Pradhan et al., 2011; Pradhan et al., 2012), and Moro and Navigli (2015) for WSD and EL, still many multi-task systems have to be evaluated on separate datasets. Notable efforts to create multi-task annotated corpora are the AMR Bank (Banarescu et al., 2013) and the MEANTIME corpus (Minard et al., 2016a). Properties of existing datasets have been examined for individual tasks. For WSD, the correct sense of a lemma is shown to often coincide with the most frequent sense (Preiss, 2006) or the predominant sense (McCarthy et al., 2004). In the case of McCarthy et al. (2004), the predominant sense is deliberately adapted with respect to the"
C16-1112,W04-0811,0,0.0881131,"Missing"
C16-1112,L16-1693,1,0.773868,"Missing"
C16-1112,R13-1092,1,0.864982,"Missing"
C16-1330,C08-1003,0,0.067061,"Missing"
C16-1330,S10-1013,1,0.892453,"Missing"
C16-1330,P16-1143,0,0.020526,"Missing"
C16-1330,S07-1060,0,0.0667712,"Missing"
C16-1330,P13-1141,0,0.0429253,"Missing"
C16-1330,P07-1033,0,0.0760377,"Missing"
C16-1330,S13-2042,0,0.0227443,"Missing"
C16-1330,P07-1034,0,0.0631641,") apply a model that considers the words of a document generated coherently to the topic distribution of that document. The most likely sense for each instance of a word is predicted from this topic distribution considering the words in the context. Finally, Lau et al. (2014; 2012) focused on the detection of novel senses, which might be considered as an extreme case of unbalanced acquisition with respect to training and evaluation distributions. Mismatches between the training and test data have been of central interest to research on domain adaptation (Daume III, 2007; Carpuat et al., 2013; Jiang and Zhai, 2007). The 2010 edition of the SemEval series (SemEval-2010) proposed a task called “All-words Word Sense Disambiguation on a Specific Domain” (Agirre et al., 2010). The aim was to analyze to what extent WSD systems are sensitive to specific domains when there is no annotated data available for that domain. In the same direction, another goal was to investigate how a general domain WSD system should be adapted to perform properly when the sense distribution is unknown and different to the sense distribution of traditional corpora used for training machine learning models. This evaluation showed tha"
C16-1330,H05-1053,0,0.205685,"ty in the graph (Calvo and Gelbukh, 2015), hence also favoring it in the sense assignment phase of a WSD system. Overall, less attention has been paid to the less represented and less frequent senses, despite the fact that these provide the biggest room for improvement (Postma et al., 2016). Although the MFS in the training data often coincides with the MFS of the task, this is not always the case, specially in cross-domain or genre scenarios. A system able to detect the predominant sense of a lemma within a target document would obtain a vast increase in accuracy. McCarthy et al. (2004b) and Koeling et al. (2005) provided the first proof of concepts in order to demonstrate this. They collected for 3497 each target word k nearest neighbors using distributional similarity, where the similarity between each sense of the target word and the nearest neighbors is determined by WordNet similarity measures. The sense with the highest similarity is chosen as the predominant sense. Building upon the same idea, Chan and Ng (2005) apply two algorithms, a confusion matrix algorithm and an Expectation-maximizationbased algorithm, to determine the sense distribution of the test data. This information is fed into the"
C16-1330,Y15-2026,0,0.0300773,"ation on a Specific Domain” (Agirre et al., 2010). The aim was to analyze to what extent WSD systems are sensitive to specific domains when there is no annotated data available for that domain. In the same direction, another goal was to investigate how a general domain WSD system should be adapted to perform properly when the sense distribution is unknown and different to the sense distribution of traditional corpora used for training machine learning models. This evaluation showed that the most successful approaches included knowledge from the specific domain in different forms. For example, Kouno et al. (2015) present a framework for WSD where an unsupervised approach is applied to abstract the features across different domains and then feeds these features into a Support Vector Machine. A semi-supervised framework is introduced by Agirre and Lopez de Lacalle (2008). Several domains are used to establish cross-domain experiments, where two supervised machine learning WSD systems (k-NN and SVM) are applied on one domain and evaluated on a different one. Singular Value Decomposition (SVD) is employed to find correlations between terms, alleviate the scarcity of data, and extract examples from unlabel"
C16-1330,E12-1060,0,0.0293024,"utional similarity, where the similarity between each sense of the target word and the nearest neighbors is determined by WordNet similarity measures. The sense with the highest similarity is chosen as the predominant sense. Building upon the same idea, Chan and Ng (2005) apply two algorithms, a confusion matrix algorithm and an Expectation-maximizationbased algorithm, to determine the sense distribution of the test data. This information is fed into the systems to improve the sense assignment. Similar approaches have been used to tackle the task of Word Sense Induction. The work presented in Lau et al. (2012) introduces a Topic Modeling approach based on LDA (Blei et al., 2003) and HDP (Teh et al., 2012) for deriving clusters that can be identified for different senses of a word. As an intermediate outcome, the authors also provide the expected predominant senses in the target corpus. Similarly, BoydGraber and Blei (2007) apply a model that considers the words of a document generated coherently to the topic distribution of that document. The most likely sense for each instance of a word is predicted from this topic distribution considering the words in the context. Finally, Lau et al. (2014; 2012)"
C16-1330,P14-1025,0,0.0134264,"ed in Lau et al. (2012) introduces a Topic Modeling approach based on LDA (Blei et al., 2003) and HDP (Teh et al., 2012) for deriving clusters that can be identified for different senses of a word. As an intermediate outcome, the authors also provide the expected predominant senses in the target corpus. Similarly, BoydGraber and Blei (2007) apply a model that considers the words of a document generated coherently to the topic distribution of that document. The most likely sense for each instance of a word is predicted from this topic distribution considering the words in the context. Finally, Lau et al. (2014; 2012) focused on the detection of novel senses, which might be considered as an extreme case of unbalanced acquisition with respect to training and evaluation distributions. Mismatches between the training and test data have been of central interest to research on domain adaptation (Daume III, 2007; Carpuat et al., 2013; Jiang and Zhai, 2007). The 2010 edition of the SemEval series (SemEval-2010) proposed a task called “All-words Word Sense Disambiguation on a Specific Domain” (Agirre et al., 2010). The aim was to analyze to what extent WSD systems are sensitive to specific domains when ther"
C16-1330,P04-1036,0,0.473583,"ing the training data according to the sense distribution of the test data boosts the results for the LFS cases while maintaining the high performance for the MFS instances. Following this assumption, our experiments presented in this paper reach an overall accuracy of 86.8 as an upper ceiling. This points towards the conclusion that the evaluation data sets have so-called “long-tail details” that need to be modeled to obtain a high-performance in addition to the properties of the head of the distribution. We therefore conclude that the distributional effect is more complex than suggested in (McCarthy et al., 2004a), who focus only on the predominant sense, but also has a larger potential if acquisition is guided by strategies to match meta properties. The paper is structured as follows: in Section 2 we discuss related work, following by a description of the resources and the evaluation framework (Section 3). The experiments and the results are presented in Section 4. Finally, we discuss the results in Section 5 and conclude the paper in Section 6. All the training data, system output files and scripts required to fully reproduce the experiments presented in this paper have been made publicly available"
C16-1330,J07-4005,0,0.167817,"on” English all-words subtask (from now on sem2013-aw) (Navigli et al., 2013) was 72.28 (Weissenborn et al., 2015)) out of competition and 64.7 in competition (Guti´errez et al., 2013), exceeding the naive Most Frequent Sense (MFS) baseline by only 10 points at most. Supervised machine learning systems have been successful for WSD and it is commonly believed that the problem of the low performance can be solved by providing more (manually annotated) training data. However, in addition to the sparseness of training data, another aspect of the problem is the Zipfian distribution of word senses (McCarthy et al., 2007). In both training data and test data, the same single sense of a word tends to heavily dominate, making the MFS not only a baseline that is difficult to beat (Kilgarriff, 2004), but also being used as a fall-back option by many systems. Given the Zipfian distribution of word senses, it comes as no surprise that WSD systems have a bias towards assigning the MFS (Preiss, 2006). Building upon this observation, Postma et al. (2016) analyzed systems participating in previous Senseval and SemEval competitions with respect to their performance on the MFS and on all other senses, which are called the"
C16-1330,H93-1061,0,0.0602107,"ons between terms, alleviate the scarcity of data, and extract examples from unlabeled data. The authors show an improvement on the cross-domain setting when including the SVD technique to add training data of the target domain. The importance of the MFS bias in training data was already highlighted in previous work. For instance, Agirre and Martinez (2004) try to overcome the problem of this skewness by automatically acquiring examples from the Internet using an heuristic based on monosemous words. Improvement is achieved on the Senseval-2 task for nouns with less than 10 examples in SemCor (Miller et al., 1993). Finally, researchers combined the task of WSD with Entity Linking to improve the performance on the disambiguation part. For example, Weissenborn et al. (2015) jointly disambiguate nouns and entities by exploiting the links between them in BabelNet (Navigli and Ponzetto, 2012). In addition, the Babelfy system (Moro et al., 2014) goes one step further by also making use of interlingual relations. Despite these efforts to either acquire more data or adapt the distributions, none of these systems gained an improvement big enough to consider the problem as solved. Our work differs from these app"
C16-1330,C14-3003,0,0.0142349,"girre and Martinez (2004) try to overcome the problem of this skewness by automatically acquiring examples from the Internet using an heuristic based on monosemous words. Improvement is achieved on the Senseval-2 task for nouns with less than 10 examples in SemCor (Miller et al., 1993). Finally, researchers combined the task of WSD with Entity Linking to improve the performance on the disambiguation part. For example, Weissenborn et al. (2015) jointly disambiguate nouns and entities by exploiting the links between them in BabelNet (Navigli and Ponzetto, 2012). In addition, the Babelfy system (Moro et al., 2014) goes one step further by also making use of interlingual relations. Despite these efforts to either acquire more data or adapt the distributions, none of these systems gained an improvement big enough to consider the problem as solved. Our work differs from these approaches mainly in that we analyze more precisely the contribution of the volume, nature, and distribution of the training data in relation to the properties of the test data. The experiments are designed to isolate each phenomenon and be able to extract meaningful conclusions. For example, whereas Agirre and Martinez (2004) acquir"
C16-1330,S13-2040,0,0.19971,"Missing"
C16-1330,L16-1268,1,0.834951,"enon and also show Zipfian characteristics (McCarthy et al., 2007; Kilgarriff, 2004). This means that, given any document or collection of documents, one sense of a lemma is usually overrepresented, while the other senses are barely used. The MFS consequently plays a critical role in the task of WSD and establishes a challenge for systems. In evaluation, the MFS is usually used as a hard to beat baseline and it is therefore also used as a fallback strategy by systems. In general, WSD systems perform well on the MFS instances, whereas their performance drops dramatically for the LFS instances (Postma et al., 2016). This is not surprising considering the Zipfian distribution of senses which has a big impact on supervised approaches. Given the dominance of MFS examples in the training data, this results in better sense representations for these cases. This unbalance contributes to the system bias towards the MFS. For unsupervised approaches, in particular graph-based approaches, the MFS of a lemma also appears to have a higher connectivity in the graph (Calvo and Gelbukh, 2015), hence also favoring it in the sense assignment phase of a WSD system. Overall, less attention has been paid to the less represe"
C16-1330,P15-1058,0,0.139985,"n including the SVD technique to add training data of the target domain. The importance of the MFS bias in training data was already highlighted in previous work. For instance, Agirre and Martinez (2004) try to overcome the problem of this skewness by automatically acquiring examples from the Internet using an heuristic based on monosemous words. Improvement is achieved on the Senseval-2 task for nouns with less than 10 examples in SemCor (Miller et al., 1993). Finally, researchers combined the task of WSD with Entity Linking to improve the performance on the disambiguation part. For example, Weissenborn et al. (2015) jointly disambiguate nouns and entities by exploiting the links between them in BabelNet (Navigli and Ponzetto, 2012). In addition, the Babelfy system (Moro et al., 2014) goes one step further by also making use of interlingual relations. Despite these efforts to either acquire more data or adapt the distributions, none of these systems gained an improvement big enough to consider the problem as solved. Our work differs from these approaches mainly in that we analyze more precisely the contribution of the volume, nature, and distribution of the training data in relation to the properties of t"
C16-1330,P10-4014,0,0.158766,"h questions: 1. Volume: What is the influence of using more training data without distinguishing between MFS and LFS cases? 2. LFS: What is the influence of only adding more LFS training examples? 3. Provenance: What is the effect of training using manually annotated data versus automatically annotated data? 4. Balancing: What is the effect of mimicking the perfect target distribution in the training data? By providing evidence for each research question, we firstly demonstrate that more data has a small impact on the performance of a state-of-the-art supervised system (It Makes Sense (IMS), (Zhong and Ng, 2010)). Interestingly enough, the type of data has a bigger impact, more specifically, adding silver data with a better fit to the test set with respect to time and genre appears to be better than adding more manually annotated data. Our biggest contribution is however that a perfect balance of data has a major impact on the performance. Balancing the training data according to the sense distribution of the test data boosts the results for the LFS cases while maintaining the high performance for the MFS instances. Following this assumption, our experiments presented in this paper reach an overall a"
C18-1030,P06-1017,0,0.0597689,"tion. Yuan et al. (2016) argue that the averaging procedure is suboptimal because of two reasons. First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. Second, averaging reduces the representation of occurrences of each sense to a single vector and therefore ignores sense prior. For this reason, they propose to use label propagation for inference as an alternative to averaging. Label propagation (Zhu and Ghahramani, 2002) is a classic semi-supervised algorithm that has been employed in WSD (Niu et al., 2005) and other NLP tasks (Chen et al., 2006; Zhou, 2011). The procedure involves predicting senses for not only the target cases but also for unannotated words queried from a corpus. It represents both the target cases and unannotated words as points in a vector space and iteratively propagates classification labels from the target classes to the words. In this way, it can be used to construct non-spherical clusters and to give more influence to frequent senses. Overall algorithm. The overall disambiguation procedure that we implemented proceeds as follows: 1. Monosemous: First, the WSD algorithm checks whether the target lemma is mono"
C18-1030,P15-1033,0,0.0279521,"iated with its meaning. Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences. Each operation is described below. Constructing Language Models. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (Sutskever et al., 2014; Dyer et al., 2015; He et al., 2017, among others). Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short- and long-range dependencies. In Yuan et al. (2016), the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes. Given a sentence s = (w1 , w2 , . . . , wn ), they replace word wk (1 ≤ k ≤ n) by a special token $. The model takes this new sentence as input and produces a context vector"
C18-1030,eisele-chen-2010-multiun,0,0.0268953,"Missing"
C18-1030,P17-1044,0,0.0279777,"ing. Broadly speaking, the disambiguation is done by: 1) constructing a language model from a large unannotated dataset; 2) extracting sense embeddings from this model using a much smaller annotated dataset; 3) relying on the sense embeddings to make predictions on the lemmas in unseen sentences. Each operation is described below. Constructing Language Models. Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a celebrated recurrent neural network architecture that has proven to be effective in many natural language processing tasks (Sutskever et al., 2014; Dyer et al., 2015; He et al., 2017, among others). Different from previous architectures, LSTM is equipped with trainable gates that control the flow of information, allowing the neural networks to learn both short- and long-range dependencies. In Yuan et al. (2016), the first operation consists of constructing an LSTM language model to capture the meaning of words in context. They use an LSTM network with a single hidden layer of h nodes. Given a sentence s = (w1 , w2 , . . . , wn ), they replace word wk (1 ≤ k ≤ n) by a special token $. The model takes this new sentence as input and produces a context vector c of dimensional"
C18-1030,P16-1085,0,0.563679,"npopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation. All code and trained models are made freely available. 1 Introduction Word Sense Disambiguation (WSD) is a long-established task in the NLP community (see Navigli (2009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998). Many approaches have been proposed – the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010), SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch¨utze, 2017), and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015). In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016). These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by Yuan et al. (2016), in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small"
C18-1030,P15-1001,0,0.110856,"Missing"
C18-1030,K16-1006,0,0.325248,"goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998). Many approaches have been proposed – the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010), SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch¨utze, 2017), and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015). In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016). These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by Yuan et al. (2016), in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by Yuan et al. (2016) outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. This is unfortunate becaus"
C18-1030,H93-1061,0,0.41168,"Missing"
C18-1030,Q14-1019,0,0.57721,"Sense (IMS) (Zhong and Ng, 2010; Taghipour and Ng, 2015). This system uses an SVM to train classifiers for each lemma using only annotated data as training evidence. In contrast, graph-based WSD systems do not use (un)annotated data but rely on the synset relations. The system UKB (Agirre et al., 2014) represents WordNet as a graph where the synsets are the nodes and the relations are the edges. After the node weights have been initialized using the Personalized Page Rank algorithm, they are updated depending on context information. Then, the synset with the highest weight is chosen. Babelfy (Moro et al., 2014) and the system by Weissenborn et al. (2015) both represent the whole input document as a graph with synset relations as edges and jointly disambiguate nouns and verbs. In the case of Babelfy, a densest-subgraph heuristic is used to compute the high-coherence semantic interpretations of the text. Instead, in Weissenborn et al. (2015) a set of complementary objectives, which include sense probabilities and type classification, are combined together to perform WSD. A number of systems make use of both unannotated data and synset relations. Both Tripodi and Pelillo (2017) and Camacho-Collados et"
C18-1030,S13-2040,0,0.100737,"Missing"
C18-1030,P05-1049,0,0.0716969,"the similarity function. Label Propagation. Yuan et al. (2016) argue that the averaging procedure is suboptimal because of two reasons. First, the distribution of occurrences of senses is unknown whereas averaging is only suitable for spherical clusters. Second, averaging reduces the representation of occurrences of each sense to a single vector and therefore ignores sense prior. For this reason, they propose to use label propagation for inference as an alternative to averaging. Label propagation (Zhu and Ghahramani, 2002) is a classic semi-supervised algorithm that has been employed in WSD (Niu et al., 2005) and other NLP tasks (Chen et al., 2006; Zhou, 2011). The procedure involves predicting senses for not only the target cases but also for unannotated words queried from a corpus. It represents both the target cases and unannotated words as points in a vector space and iteratively propagates classification labels from the target classes to the words. In this way, it can be used to construct non-spherical clusters and to give more influence to frequent senses. Overall algorithm. The overall disambiguation procedure that we implemented proceeds as follows: 1. Monosemous: First, the WSD algorithm"
C18-1030,S01-1005,0,0.110144,"Missing"
C18-1030,C16-1330,1,0.855124,"-frequent-sense instances. The original paper only analyses the performance on the whole test sets. We extend this analysis by looking at the performance for disambiguating the most frequent-sense (MFS) and less-frequent-sense (LFS) instances. The first type of instances are the ones for which the correct link is the most-frequent sense, whereas the second subset consists of the remaining ones. This analysis is important because it is well-known that the simple strategy of always choosing the MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (Postma et al., 2016). Table 2 shows that the method by Yuan et al. (2016) does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them). On semeval13, the recall on LFS is already relatively high using only SemCor (0.33), and reaches 0.38 when using both SemCor and OMSTI. For comparison, the default system IMS (Zhong and Ng, 2010) trained on SemCor only obtains an R lfs of 0.15 on semeval13 (Postma et al., 2016) and only reaches 0.33"
C18-1030,E17-1010,0,0.482619,"009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998). Many approaches have been proposed – the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010), SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch¨utze, 2017), and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015). In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016). These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by Yuan et al. (2016), in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by Yuan et al. (2016) outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. Th"
C18-1030,D17-1120,0,0.47717,"009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998). Many approaches have been proposed – the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010), SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch¨utze, 2017), and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015). In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016). These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by Yuan et al. (2016), in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the results obtained by Yuan et al. (2016) outperform the previous state-of-the-art, neither the used datasets nor the constructed models are available to the community. Th"
C18-1030,D17-1035,0,0.0497114,"Missing"
C18-1030,J17-3004,0,0.0277892,"Missing"
C18-1030,K15-1037,0,0.268836,"the LSTM models, we used the English Gigaword Fifth Edition (Linguistic Data Consortium (LDC) catalog number LDC2011T07). The corpus consists of 1.8 billion tokens in 4.1 million documents, originated from four major news agencies. We leave the study of bigger corpora for future work. For the training of the sense embeddings, we use the same two corpora used by Yuan et al. (2016): 1. SemCor (Miller et al., 1993) is a corpus containing approximately 240,000 sense annotated words. The tagged documents originate from the Brown corpus (Francis and Kucera, 1979) and cover various genres. 2. OMSTI (Taghipour and Ng, 2015) contains one million sense annotations automatically tagged by exploiting the English-Chinese part of the parallel MultiUN corpus (Eisele and Chen, 2010). A list of English translations were manually created for each WordNet sense. If the Chinese translation of an English word matches one of the manually curated translations for a WordNet sense, that sense is selected. Implementation. We used the BeautifulSoup HTML parser to extract plain text from the Gigaword corpus. Then, we used the English models3 of Spacy 1.8.2 for sentence boundary detection and tokenization. The LSTM model is implemen"
C18-1030,J17-1002,0,0.0535512,"ghest weight is chosen. Babelfy (Moro et al., 2014) and the system by Weissenborn et al. (2015) both represent the whole input document as a graph with synset relations as edges and jointly disambiguate nouns and verbs. In the case of Babelfy, a densest-subgraph heuristic is used to compute the high-coherence semantic interpretations of the text. Instead, in Weissenborn et al. (2015) a set of complementary objectives, which include sense probabilities and type classification, are combined together to perform WSD. A number of systems make use of both unannotated data and synset relations. Both Tripodi and Pelillo (2017) and Camacho-Collados et al. (2016) make use of statistical information from unannotated data to weigh the relevance of nodes in a graph, which is then used to perform WSD. Rothe and Sch¨utze (2017) use word embeddings as a starting point and then rely on the formal constraints in a lexical resource to create synset embeddings. Recently, there has been a surge in WSD approaches that use unannotated data but do not consider synset relations. One example is provided by Iacobacci et al. (2016), who investigated the role of word embeddings as features in a WSD system. Four methods (concatenation,"
C18-1030,P15-1058,0,0.250075,"on. All code and trained models are made freely available. 1 Introduction Word Sense Disambiguation (WSD) is a long-established task in the NLP community (see Navigli (2009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from a lexical database like WordNet (Fellbaum, 1998). Many approaches have been proposed – the more popular ones include the usage of Support Vector Machine (SVM) (Zhong and Ng, 2010), SVM combined with unsupervised trained embeddings (Iacobacci et al., 2016; Rothe and Sch¨utze, 2017), and graph-based approaches (Agirre et al., 2014; Weissenborn et al., 2015). In recent years, there has been a surge in interest in using Long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) to perform WSD (Raganato et al., 2017b; Melamud et al., 2016). These approaches are characterized by their high performance, simplicity and their ability to extract a lot of information from raw text. Among the best-performing ones is the approach by Yuan et al. (2016), in which an LSTM language model trained on a corpus with 100 billion tokens was coupled with small sense-annotated datasets to achieve state-of-the-art performance in all-words WSD. Even though the res"
C18-1030,C16-1130,0,0.117985,",m.c.postma,piek.vossen}@vu.nl, jacopo@cs.vu.nl Abstract LSTM-based language models have been shown effective in Word Sense Disambiguation (WSD). In particular, the technique proposed by Yuan et al. (2016) returned state-of-the-art performance in several benchmarks, but neither the training data nor the source code was released. This paper presents the results of a reproduction study and analysis of this technique using only openly available datasets (GigaWord, SemCor, OMSTI) and software (TensorFlow). Our study showed that similar results can be obtained with much less data than hinted at by Yuan et al. (2016). Detailed analyses shed light on the strengths and weaknesses of this method. First, adding more unannotated training data is useful, but is subject to diminishing returns. Second, the model can correctly identify both popular and unpopular meanings. Finally, the limited sense coverage in the annotated datasets is a major limitation. All code and trained models are made freely available. 1 Introduction Word Sense Disambiguation (WSD) is a long-established task in the NLP community (see Navigli (2009) for a survey) which goal is to annotate lemmas in text with the most appropriate meaning from"
C18-1030,P10-4014,0,0.416957,"he MFS is a strong baseline in WSD, thus there is a tendency for WSD systems to overfit towards the MFS (Postma et al., 2016). Table 2 shows that the method by Yuan et al. (2016) does not overfit towards the MFS to the same extent as other supervised systems since the recall on LFS instances is still quite high 0.41 (a lower recall on LFS instances than on MFS ones is expected due to the reduced training data for them). On semeval13, the recall on LFS is already relatively high using only SemCor (0.33), and reaches 0.38 when using both SemCor and OMSTI. For comparison, the default system IMS (Zhong and Ng, 2010) trained on SemCor only obtains an R lfs of 0.15 on semeval13 (Postma et al., 2016) and only reaches 0.33 with a large amount of annotated data. Finally, our implementation of the label propagation does seem to slightly overfit towards the MFS. When we compare the results of the averaging technique using SemCor and OMSTI versus when we use label propagation, we notice an increase in the MFS recall (from 0.85 to 0.91), whereas the LFS recall drops from 0.40 to 0.32. Meaning coverage in annotated datasets. The WSD procedure depends on an annotated corpus to compose its sense representations, mak"
C18-1056,D11-1072,0,0.653197,"Missing"
C18-1056,C16-1112,1,0.935787,"of systems. The past years featured a plethora of EL systems: DBpedia Spotlight (Daiber et al., 2013), WAT (Piccinno and Ferragina, 2014), AGDISTIS MAG (Moussallem et al., 2017), to name a few. These systems propose various probabilistic algorithms for graph optimization or machine learning, in order to perform disambiguation, i.e., to pick the correct entity candidate for a surface form in a given context. The reported accuracy scores are fairly high, which gives an impression that the task of EL is both well-understood and fairly solved by existing systems. At the same time, several papers (Ilievski et al., 2016; Van Erp et al., 2016; Esquivel et al., 2017; Ilievski et al., 2017) have argued that state-of-the-art EL systems base their performance on frequent ‘head’ cases, while performance drops significantly when moving towards the rare ‘long tail’ entities. This statement seems intuitively obvious, but no previous work has quantified what the ‘head’ and ‘tail’ of EL entails. In fact, the lack of definition of head and tail in this task prevents the (in)validation of the hypothesis that interpreting some (classes of) cases is more challenging for systems than others. This, in turn, means that we are"
C18-1056,P11-1115,0,0.0623192,"Missing"
C18-1056,L16-1268,1,0.869424,"Missing"
C18-1056,L16-1693,1,0.896351,"Missing"
C18-1147,P16-1054,0,0.111699,"Missing"
C18-1147,W14-3348,0,0.367397,"Missing"
C18-1147,P15-2017,0,0.311606,"Missing"
C18-1147,P14-2074,1,0.840351,"example, van Miltenburg et al. (2016) provide a thorough overview of the uses of negations in human-generated image descriptions. Even though this is a low-frequent (or long-tail) phenomenon, studying a subset of the image descriptions informs us about the human image description process, and the cognitive requirements to produce a description containing a negation. It remains to be seen whether image description systems could produce similar descriptions. 5.2 Limitations and human validation Earlier work has shown that automated evaluation metrics do not correlate well with human judgments (Elliott and Keller, 2014; Kilickaya et al., 2017). For this reason, we should not blindly trust evaluation metrics in their assessment of system performance. Still, this paper only includes automatic, intrinsic metrics. This is by design: we want to gain insight into the descriptions, not to evaluate their quality. While you cannot evaluate a system using only automated metrics, they do tell us something about how a system behaves. Future researchers could try to improve the diversity metrics while maintaining or improving the quality of the descriptions (ideally measured by human judgments). At that point, we should"
C18-1147,D15-1021,0,0.0264482,"ed in future work. For reasons of space, we were also not able to cover metrics based on the frequency distribution of words in the training and validation data. We already mentioned Shetty et al.’s (2017) use of frequency ratios in the introduction. Their approach could be extended (perhaps also using log-likelihood (Rayson and Garside, 2000)) to produce a ranking of words that are over- or underused by a particular system. Overused words could be further analyzed by computing a ‘local precision’ metric, measuring how often a generated word is also used in at least one reference description. Ferraro et al. (2015) present other metrics in their survey of datasets for vision and language research, including: Yngve and Frazier measurements of syntactic complexity (Yngve, 1960; Frazier, 1985). Ferraro et al. (2015) found that the MS COCO and Flickr30K datasets have the most complex sentences, compared to other vision & language datasets. It is still an open question whether machine-generated descriptions are of equal complexity and, if not, what are the differences. Abstract-to-concrete ratio The authors also compare the proportion of abstract words that each corpus contains. They count abstract words by"
C18-1147,J84-3009,0,0.430681,"Missing"
C18-1147,N16-1014,0,0.144142,"Missing"
C18-1147,P02-1040,0,0.106323,"riptions generated for the MS COCO validation set. All these systems are listed in Table 1. With the exception of the two GAN-based systems (Dai et al., 2017; Shetty et al., 2017), the other systems are based on a conditioned recurrent neural network, trained using a Maximum Likelihood (MLE) objective. 2.2 Results Table 1 presents the results for the metrics discussed above. We discuss each of them in turn. Average sentence length. We observe that all models produce shorter sentences than humans, on average, perhaps also conveying less information. It also means that the BLEU brevity penalty (Papineni et al., 2002) and Meteor length penalty (Denkowski and Lavie, 2014) are affecting the metric scores. However, producing shorter sentences does not necessarily mean producing worse descriptions. Standard deviation of sentence length. We observe that the GAN-based systems vary more than most other systems, but the systems by Liu et al. (2017) and Vinyals et al. (2017) have more variation than other MLE-based systems. Humans vary much more than any model in the length of their descriptions. Number of types. The model by Liu et al. (2017) produces the fewest distinct word types (598), which severely limits the"
C18-1147,L16-1268,1,0.893335,"Missing"
C18-1147,W00-0901,0,0.0892584,"n ratio corresponds to the number of prepositional phrases per description. Types-1 refers to the number of PP types of depth 1. 1737 5 5.1 Discussion and Future Research Other metrics In addition to the metrics proposed in this paper, there are other options that could be explored in future work. For reasons of space, we were also not able to cover metrics based on the frequency distribution of words in the training and validation data. We already mentioned Shetty et al.’s (2017) use of frequency ratios in the introduction. Their approach could be extended (perhaps also using log-likelihood (Rayson and Garside, 2000)) to produce a ranking of words that are over- or underused by a particular system. Overused words could be further analyzed by computing a ‘local precision’ metric, measuring how often a generated word is also used in at least one reference description. Ferraro et al. (2015) present other metrics in their survey of datasets for vision and language research, including: Yngve and Frazier measurements of syntactic complexity (Yngve, 1960; Frazier, 1985). Ferraro et al. (2015) found that the MS COCO and Flickr30K datasets have the most complex sentences, compared to other vision & language datase"
C18-1147,W16-3207,1,0.779117,"Missing"
C18-1191,W12-3808,0,0.105199,"e approaches to the annotation of focus and different definitions of the corresponding task. Blanco and Moldovan (2012) introduce the concept of granularity of focus and add fine-grained foci on top of the course-grained foci in PB - FOC; whereas coarse-grained focus includes all words belonging to a semantic role, fine-grained focus comprises fewer words within the semantic role (e.g. We didn’t get [an offer for more than $40]FOCUS ), allowing for more specific implicit positive interpretations (“we got something, but not an offer for more than $40” versus “we got an offer for $40 or less”). Anand and Martell (2012) evaluated the annotations of PB - FOC, arguing that positive interpretations resulting from scalar implicatures and neg-raising predicates should be separated from those (indirectly) resulting from focus, and reannotated the corpus by using an alternative annotation approach that relies on relevant questions under discussion (QUDs). Another criticism on PB - FOC was raised by Blanco and Sarabi (2016), who point out that the guidelines required the annotators to choose one semantic role as the focus, prioritizing “the one that yields the 2254 most meaningful implicit [positive] information” in"
C18-1191,P11-1059,0,0.167156,"ent on our test set and the ablation tests on feature combinations against the baseline. In Section 5, we redefine the task as a classification task and apply an error analysis to understand system performance against the baseline and the role of the different features in the classification task. Our error analysis leads to a discussion reported in Section 6 and plans for future work. Section 7 summarizes our conclusions. 2 Related Work The task of scoring implicit positive meanings from negated statements was preceded by the task of detecting the focus of negation. This task was pioneered by Blanco and Moldovan (2011), who argued that the scope and focus of negation are crucial for a correct interpretation of negated statements. Following Huddleston and Pullum (2002), they defined scope as “the part of the meaning that is negated” and focus as “the part of the scope that is most prominently or explicitly negated.” More specifically, according to Blanco and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al.,"
C18-1191,N12-1050,0,0.0227298,"s using a set of features derived from gold syntactic annotation and semantic role labels. Whereas both CLaCs NegFocus and FOC - DET use only information from within the sentence, Zou et al. (2015) argue that contextual discourse information plays a critical role in negation focus identification and propose a word-topic graph model that uses this contextual information from both lexical and topical perspectives. They report an accuracy of 69.39 on PB - FOC. PB - FOC has given rise to various alternative approaches to the annotation of focus and different definitions of the corresponding task. Blanco and Moldovan (2012) introduce the concept of granularity of focus and add fine-grained foci on top of the course-grained foci in PB - FOC; whereas coarse-grained focus includes all words belonging to a semantic role, fine-grained focus comprises fewer words within the semantic role (e.g. We didn’t get [an offer for more than $40]FOCUS ), allowing for more specific implicit positive interpretations (“we got something, but not an offer for more than $40” versus “we got an offer for $40 or less”). Anand and Martell (2012) evaluated the annotations of PB - FOC, arguing that positive interpretations resulting from sc"
C18-1191,N16-1169,0,0.0802587,"ecting implicit positive meaning from negated statements. Such a system could support a range of Natural Language Processing (NLP) technologies that require deep understanding of language, such as Recognizing Textual Entailment (RTE) and Question Answering (QA). The research that we reproduce This work is licenced under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/ Licence details: http:// 2253 Proceedings of the 27th International Conference on Computational Linguistics, pages 2253–2264 Santa Fe, New Mexico, USA, August 20-26, 2018. is that of Blanco and Sarabi (2016), who propose an interesting methodology to automatically generate positive interpretations from negated statements and score them according to their likelihood. We reflect on the definition of the task and the contribution of the different features to the performance of their system. Our findings show that the features they propose are not able to improve upon our baseline because of class imbalance. Based on an error analysis that we perform on the output of our baseline, we discuss future lines of research that take levels of informativeness into account, which we expect to be also applicab"
C18-1191,N06-2015,0,0.0102531,"their likelihood (see Section 3 for more details). Similar to the distinction between fine-grained and coarse-grained foci, Sarabi and Blanco (2016) propose to extract and score more fine-grained positive interpretations by manipulating syntactic dependencies instead of semantic roles. Sanders and Blanco (2016) further extend this approach of generating and scoring implicit positive interpretations by applying it to modal constructions. 3 Dataset and Task The dataset created by Blanco and Sarabi (2016) contains 1,888 positive interpretations generated from 600 negated verbs in OntoNotes 5.0 (Hovy et al., 2006). These positive interpretations are automatically generated by first converting the negated statement into its positive counterpart by (1) removing the negation mark, (2) removing auxiliaries, expanding contractions and fixing third-person singular and past tense, and (3) rewriting negatively-oriented polarity-sensitive items (e.g. anyone becomes someone). From this positive counterpart, positive interpretations are generated by rewriting each semantic role or the (originally negated) verb. For example, ARG0-ARG4 are rewritten as someone / some people / something, and ARGM-TMP is rewritten as"
C18-1191,S12-1035,1,0.826675,"d.” More specifically, according to Blanco and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al., 2005). Candidates for focus annotation were the verb itself or any of its semantic roles and annotators were instructed to choose only one. The PB - FOC corpus was used in the first edition of the *SEM Shared Task, which was dedicated to resolving the scope (Task 1) and focus (Task 2) of negation (Morante and Blanco, 2012). Only one team (Rosenberg and Bergler, 2012) participated in the Focus Detection task. Their system, called CLaCs NegFocus, is rule-based and consists of three components: the identification of explicit negation cues (not, nor and never), the detection of the syntactic scope of negation, and the detection of the focus of negation using a set of four syntactic heuristics (e.g. “if an adverb directly precedes the negated verb and is connected through an advmod dependency relation to the negated verb, this adverb is annotated as the focus”). They report an F-measure of 0.584. Blanco and Moldovan"
C18-1191,J05-1004,0,0.0411398,"Moldovan (2011), who argued that the scope and focus of negation are crucial for a correct interpretation of negated statements. Following Huddleston and Pullum (2002), they defined scope as “the part of the meaning that is negated” and focus as “the part of the scope that is most prominently or explicitly negated.” More specifically, according to Blanco and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al., 2005). Candidates for focus annotation were the verb itself or any of its semantic roles and annotators were instructed to choose only one. The PB - FOC corpus was used in the first edition of the *SEM Shared Task, which was dedicated to resolving the scope (Task 1) and focus (Task 2) of negation (Morante and Blanco, 2012). Only one team (Rosenberg and Bergler, 2012) participated in the Focus Detection task. Their system, called CLaCs NegFocus, is rule-based and consists of three components: the identification of explicit negation cues (not, nor and never), the detection of the syntactic scope of n"
C18-1191,W11-1901,0,0.0519411,"Missing"
C18-1191,S12-1039,0,0.0193194,"and Moldovan (2011), the focus of negation gives rise to implicit positive meaning. To capture this information, they created the PB - FOC corpus, which contains annotations for the focus of negation in the 3,993 verbal negations in PropBank (Palmer et al., 2005). Candidates for focus annotation were the verb itself or any of its semantic roles and annotators were instructed to choose only one. The PB - FOC corpus was used in the first edition of the *SEM Shared Task, which was dedicated to resolving the scope (Task 1) and focus (Task 2) of negation (Morante and Blanco, 2012). Only one team (Rosenberg and Bergler, 2012) participated in the Focus Detection task. Their system, called CLaCs NegFocus, is rule-based and consists of three components: the identification of explicit negation cues (not, nor and never), the detection of the syntactic scope of negation, and the detection of the focus of negation using a set of four syntactic heuristics (e.g. “if an adverb directly precedes the negated verb and is connected through an advmod dependency relation to the negated verb, this adverb is annotated as the focus”). They report an F-measure of 0.584. Blanco and Moldovan (2013) report an F-measure of 0.641 with the"
C18-1191,D16-1118,0,0.0135375,"aningful implicit [positive] information” in case of multiple candidates, but that it is not specified what “most meaningful” means. Therefore, they designed a new annotation task where several positive interpretations per negation (automatically generated by manipulating semantic roles) are scored according to their likelihood (see Section 3 for more details). Similar to the distinction between fine-grained and coarse-grained foci, Sarabi and Blanco (2016) propose to extract and score more fine-grained positive interpretations by manipulating syntactic dependencies instead of semantic roles. Sanders and Blanco (2016) further extend this approach of generating and scoring implicit positive interpretations by applying it to modal constructions. 3 Dataset and Task The dataset created by Blanco and Sarabi (2016) contains 1,888 positive interpretations generated from 600 negated verbs in OntoNotes 5.0 (Hovy et al., 2006). These positive interpretations are automatically generated by first converting the negated statement into its positive counterpart by (1) removing the negation mark, (2) removing auxiliaries, expanding contractions and fixing third-person singular and past tense, and (3) rewriting negatively-"
C18-1191,D16-1119,0,0.0142649,"rabi (2016), who point out that the guidelines required the annotators to choose one semantic role as the focus, prioritizing “the one that yields the 2254 most meaningful implicit [positive] information” in case of multiple candidates, but that it is not specified what “most meaningful” means. Therefore, they designed a new annotation task where several positive interpretations per negation (automatically generated by manipulating semantic roles) are scored according to their likelihood (see Section 3 for more details). Similar to the distinction between fine-grained and coarse-grained foci, Sarabi and Blanco (2016) propose to extract and score more fine-grained positive interpretations by manipulating syntactic dependencies instead of semantic roles. Sanders and Blanco (2016) further extend this approach of generating and scoring implicit positive interpretations by applying it to modal constructions. 3 Dataset and Task The dataset created by Blanco and Sarabi (2016) contains 1,888 positive interpretations generated from 600 negated verbs in OntoNotes 5.0 (Hovy et al., 2006). These positive interpretations are automatically generated by first converting the negated statement into its positive counterpar"
C18-1191,D15-1187,0,0.0139022,"of the focus of negation using a set of four syntactic heuristics (e.g. “if an adverb directly precedes the negated verb and is connected through an advmod dependency relation to the negated verb, this adverb is annotated as the focus”). They report an F-measure of 0.584. Blanco and Moldovan (2013) report an F-measure of 0.641 with their system FOC - DET, which is trained with bagging over standard C4.5 decision trees using a set of features derived from gold syntactic annotation and semantic role labels. Whereas both CLaCs NegFocus and FOC - DET use only information from within the sentence, Zou et al. (2015) argue that contextual discourse information plays a critical role in negation focus identification and propose a word-topic graph model that uses this contextual information from both lexical and topical perspectives. They report an accuracy of 69.39 on PB - FOC. PB - FOC has given rise to various alternative approaches to the annotation of focus and different definitions of the corresponding task. Blanco and Moldovan (2012) introduce the concept of granularity of focus and add fine-grained foci on top of the course-grained foci in PB - FOC; whereas coarse-grained focus includes all words bel"
cuadros-etal-2010-integrating,vossen-etal-2008-kyoto,1,\N,Missing
cuadros-etal-2010-integrating,P00-1064,0,\N,Missing
cuadros-etal-2010-integrating,E09-1005,0,\N,Missing
cuadros-etal-2010-integrating,R09-1039,1,\N,Missing
cybulska-vossen-2010-event,bosma-vossen-2010-bootstrapping,1,\N,Missing
cybulska-vossen-2010-event,vossen-etal-2008-kyoto,1,\N,Missing
cybulska-vossen-2010-event,E09-1005,0,\N,Missing
cybulska-vossen-2010-event,pustejovsky-etal-2006-towards,0,\N,Missing
cybulska-vossen-2010-event,vossen-etal-2008-integrating,1,\N,Missing
cybulska-vossen-2014-using,M95-1005,0,\N,Missing
cybulska-vossen-2014-using,D12-1045,0,\N,Missing
cybulska-vossen-2014-using,H05-1004,0,\N,Missing
cybulska-vossen-2014-using,W11-1901,0,\N,Missing
cybulska-vossen-2014-using,I11-1012,0,\N,Missing
cybulska-vossen-2014-using,P10-1143,0,\N,Missing
cybulska-vossen-2014-using,bartalesi-lenzi-etal-2012-cat,0,\N,Missing
cybulska-vossen-2014-using,bejan-harabagiu-2008-linguistic,0,\N,Missing
elkateb-etal-2006-building,P00-1026,0,\N,Missing
fokkens-etal-2014-biographynet,N07-4013,0,\N,Missing
fokkens-etal-2014-biographynet,W13-1202,1,\N,Missing
gorog-vossen-2010-computer,mihalcea-2002-bootstrapping,0,\N,Missing
gorog-vossen-2010-computer,W04-2405,0,\N,Missing
gorog-vossen-2010-computer,N06-2015,0,\N,Missing
gorog-vossen-2010-computer,oostdijk-etal-2008-coi,0,\N,Missing
gorog-vossen-2010-computer,vossen-etal-2008-integrating,1,\N,Missing
L16-1187,P11-1059,0,0.0216625,"ent to annotate the event, as a representative of the whole proposition, as the target of factuality. This is because factuality cues can target specific relations within a proposition. To clarify, consider the following example, taken from FactBank: 1182 12 13 TimeBank/FactBank – APW19980227.476-S1 GEN AUTHOR denotes a non-explicit generic source. We call this phenomenon perspective scope, referring to those specific propositional relations associated with an event (or entity) that are affected by a perspective cue. It is strongly related to the scope and focus of negation as investigated by Blanco and Moldovan (2011), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and T"
L16-1187,N15-1146,0,0.0316723,"of factuality. In the factuality layer, each event identified in the event layer is to be annotated as the target of a factuality relation. In the opinion layer, annotators have no clear pre-defined targets; instead, they need to look for cues and understand the text in more detail. The first thing to look for are attributional cues, since they are fairly easy to recognize and some of them will already have been identified in the attribution layer. An example of such an attributional cue is support in our example sentence repeated below, which expresses a positive attitude of Mbeki. Following Deng and Wiebe (2015), we aim to identify the specific entities and events that are the target of the opinion. In this case, there are two targets: the entity denoted by Mugabe and the event expressed by elections. 10. Investors and Western diplomats have saide1 they might interprete2 {Mbeki’s}SENT- SOURCE {supporte3 }SENT- CUE for {Mugabe}SENT- TARGET or the {electionse4 }SENT- TARGET as a sign that Africa is not intent on revitalizinge5 its economies through good governmente6 and expanded international tradee7 . The other two types of cues are also present in our example. An example of a factual opinion cue is e"
L16-1187,W09-3012,0,0.198309,"events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the notion of perspectives lies at the semantic-pragmatic interface"
L16-1187,pareti-2012-database,0,0.1945,"ortion, vaccinations, etc.), and interpretative frames on events and their causes (e.g. conspiracy theories on 9/11). Textual data always provide specific perspectives of the author and quoted sources on the information they contain. Mining information from texts thus implies dealing with these perspectives. In the last decade, different aspects of linguistic encoding of perspectives have been targeted as separated phenomena through different annotation initiatives, each with its own approaches and goals. Targeted aspects of perspectives include, for example, attribution (Prasad et al., 2007; Pareti, 2012), factuality (Saur´ı and Pustejovsky, 2009; Diab et al., 2009), and opinion (Wiebe et al., 2005). Coordinating initiatives such as the Unified Linguistic Annotation project1 have tried to technically combine such annotations into a unique annotation model, but they lack an overarching framework for the various layers of annotation from different resources. Furthermore, annotation initiatives such as those proposed by Prasad et al. (2007) and Pareti (2012) have attempted to tackle the annotation of perspectives in a unified approach, but with different levels of success.2 In our approach, the n"
L16-1187,S15-1009,0,0.0145949,"uting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting noun phrases (e.g. the col"
L16-1187,pustejovsky-etal-2010-iso,0,0.0225563,"d approach. Section 4 describes the four layers that we have currently defined for the an1177 notation of perspectives: events, attribution, factuality, and opinion. Finally, we conclude and summarize our work and give an outlook on future work in Section 5. 2. Related Work In our annotations, events play an important role because we consider them to be the basic semantic elements that may give rise to or be involved in perspectives. A wellknown specification language for events is TimeML (Pustejovsky et al., 2003a), which has been consolidated as an international cross-language ISO standard (Pustejovsky et al., 2010) and has been used as the annotation language for the TempEval shared task series (Verhagen et al., 2009). Its reference corpus is TimeBank (Pustejovsky et al., 2003b). TimeML defines an event as something that can be said to obtain or hold true, to happen or to occur. TimeML adopts a surface-based annotation of texts and morpho-syntactic information plays a key role for detecting all possible mentions of an event. According to TimeML, both demonstrations and taken (place) in Example 1 are to be annotated as valid event mentions. 1. Several pro-Iraq demonstrations have taken place in the last"
L16-1187,P15-5003,0,0.0165306,"e represented as: PARTY −−−−−−−−−→ MARY OrganizedBy Note that both Examples 3 and 4 mention the O RGA NIZED B Y relation between Mary and his birthday party; however, whereas in the former it functions merely as additional information on the target, in the latter it is the target of the perspective. The schematic representation in Figure 1 illustrates the differences between these targets. Not all attitude dimensions can take the same types of targets. For example, opinion can target entities, events or propositional relations, but factuality can only target events or propositional relations (Rambow and Wiebe, 2015). Whereas the source and target are usually expressed by a single linguistic unit (e.g. an NP or a clause), the attitude may be expressed either by a single linguistic cue or a combination of cues.6 For example, the commitment of a source towards the factual nature of an event or proposition may be expressed by a combination of polarity (e.g. not, never) and modality cues (e.g. could, maybe). In turn, one cue can express multiple attitude dimensions. For example, a verb like hope expresses positive sentiment and uncertainty towards the target at the same time. 5 If the author of a document is"
L16-1187,P10-1059,0,0.0384465,"), but to our knowledge its annotation has not been investigated before in the context of factuality (or sentiment). We believe that perspective scope is an important and innovative aspect of our annotation scheme, and our formal model GRaSP allows for the representation of separate factuality assignments for the event and its relations. In the near future, we will work out the details with respect to its annotation. 4.4. Opinion Layer The final annotation layer that we have included in our scheme is that of opinion or sentiment. As our annotations are largely based on Wiebe et al. (2005) and Toprak et al. (2010), the three main elements are defined as follows: • Source: The entity that has a positive or negative attitude towards some target. • Cue: A linguistic cue that, possibly in combination with other cues, expresses the positive or negative attitude of the source towards the target. We regard cues as belonging to one of the following categories: – Attributional cue: contributes a source while directly expressing the positive or negative attitude of the source towards the embedded target; – Indirect cue: signals the positive or negative attitude of the source by the choice of words; – Factual opi"
L16-1187,W15-1304,0,0.0122515,"merely helps constituting the description of the single ‘real’ event in this sentence expressed by demonstrations. In other words, only one event in this sentence should be put on a timeline. In this framework, syntax is only used to decide on the span of the event. The two main corpora for factuality (or belief) are FactBank (Saur´ı and Pustejovsky, 2009) and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). Although both corpora address the same phenomenon (i.e. the commitment of a source towards the truth of some event/proposition), the annotations are quite different (Werner et al., 2015; Prabhakaran et al., 2015). The LU Corpus so far has only addressed the problem from the perspective of the speaker/writer, in contrast to FactBank, which has fully annotated nested sources. Furthermore, the LU corpus ignores negation (the polarity axis of factuality in FactBank) and does not distinguish between POSSI BLE and PROBABLE (the certainty axis of factuality in FactBank). Finally, there is a subtle difference in the targets. Whereas FactBank has assigned factuality values to events, the LU corpus has assigned belief tags to the head words of propositions, disregarding event-denoting"
L16-1233,P98-1013,0,0.346679,"ecification of the concepts, but these axioms do not always provide the information relevant and specific for our domain. Also, SUMO needs a so´ phisticated reasoning system to be productive (Alvez et al., 2015). Furthermore, such ontologies need to be integrated with semantic parsing systems that deal with expressions on natural language. We therefore decided to develop a new ontology for modelling events and their implications that is tailored to a semantic parsing system for text. The Predicate Matrix (L´opez de Lacalle et al., 2014) integrates predicate and role information from FrameNet (Baker et al., 1998), VerbNet (Kipper et al., 2000), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and WordNet (Fellbaum, 1998). This resource is used to assign role and predicate annotations at sentence level. All classes and roles in ESO are fed back into the Predicate Matrix. As such the ontology provides an additional layer of annotations in text that allow for inferencing over events and implications. The remainder of this paper is organized as follows. Section 2. presents the ontological meta model and the content of ESO. Section 3. describes the Predicate Matrix and the integration with ESO"
L16-1233,W04-2214,0,0.142993,"Missing"
L16-1233,gonzalez-agirre-etal-2012-multilingual,1,0.890287,"Missing"
L16-1233,kipper-etal-2006-extending,0,0.0104722,"sent the implied situations of each event. Modeling of event implications allows for extracting sequences of states and changes over time regardless of this information being directly expressed in text, or inferred by a reasoner. The model targets interpretations of situations rather than the semantics of predicates per se. Events are interpreted as situations using RDF, taking all event components into account. Hence, the ontology and the linked resources need to be considered from the perspective of this interpretation model. Lexicons that define implications of events, for example VerbNet (Kipper et al., 2006), are rare and usually focus on the meaning of verbs in isolation. However, lexical structures do not make explicit how the meaning of a verb needs to be combined with other event components, such as the participants and the temporal properties for the purpose of semantic parsing. We therefore follow an ontological approach to interpret situations on the basis of the event components to make these implications explicit. Though some research on deductive reasoning over Frame annotated text (Scheffczyk et al., 2006) and defining pre and post situations of predicates exist (Im and Pustejovsky, 20"
L16-1233,lopez-de-lacalle-etal-2014-predicate,1,0.929586,"Missing"
L16-1233,L16-1423,1,0.854358,"Missing"
L16-1233,W04-2705,0,0.101321,"and specific for our domain. Also, SUMO needs a so´ phisticated reasoning system to be productive (Alvez et al., 2015). Furthermore, such ontologies need to be integrated with semantic parsing systems that deal with expressions on natural language. We therefore decided to develop a new ontology for modelling events and their implications that is tailored to a semantic parsing system for text. The Predicate Matrix (L´opez de Lacalle et al., 2014) integrates predicate and role information from FrameNet (Baker et al., 1998), VerbNet (Kipper et al., 2000), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and WordNet (Fellbaum, 1998). This resource is used to assign role and predicate annotations at sentence level. All classes and roles in ESO are fed back into the Predicate Matrix. As such the ontology provides an additional layer of annotations in text that allow for inferencing over events and implications. The remainder of this paper is organized as follows. Section 2. presents the ontological meta model and the content of ESO. Section 3. describes the Predicate Matrix and the integration with ESO. In Section 4. we provide an overview of the Predicate Matrix and ESO in our document collect"
L16-1233,L16-1699,1,0.881072,"Missing"
L16-1233,J05-1004,0,0.100043,"ovide the information relevant and specific for our domain. Also, SUMO needs a so´ phisticated reasoning system to be productive (Alvez et al., 2015). Furthermore, such ontologies need to be integrated with semantic parsing systems that deal with expressions on natural language. We therefore decided to develop a new ontology for modelling events and their implications that is tailored to a semantic parsing system for text. The Predicate Matrix (L´opez de Lacalle et al., 2014) integrates predicate and role information from FrameNet (Baker et al., 1998), VerbNet (Kipper et al., 2000), PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004) and WordNet (Fellbaum, 1998). This resource is used to assign role and predicate annotations at sentence level. All classes and roles in ESO are fed back into the Predicate Matrix. As such the ontology provides an additional layer of annotations in text that allow for inferencing over events and implications. The remainder of this paper is organized as follows. Section 2. presents the ontological meta model and the content of ESO. Section 3. describes the Predicate Matrix and the integration with ESO. In Section 4. we provide an overview of the Predicate Matrix"
L16-1233,2016.gwc-1.51,1,0.502091,"ESO assertions are correct. Keywords: Ontology, Semantic Role Labeling, Text Mining, Semantic Web 1. Introduction In this paper, we present the Event and Implied Situation Ontology (ESO) Version 2, that is matched with the Predicate Matrix (PM). Both resources rely on Semantic Role Labeling (SRL) descriptions and are used to detect and abstract over events, their participants and event implications in a large document collection about ten years of global automotive industries, thus favoring the construction of large event-centric knowledge graphs (Rospocher et al., to appear). ESO Version 2 (Segers et al., 2016) is a newly developed domain ontology to enhance the extraction and linking of dynamic and static events and their implications in text. Such a chain of changes and states and their implied situations is presented in Figure 1. Here, the boxes represent various event expressions about John’s employment while the ovals represent the implied situations of each event. Modeling of event implications allows for extracting sequences of states and changes over time regardless of this information being directly expressed in text, or inferred by a reasoner. The model targets interpretations of situation"
L16-1268,S10-1013,1,0.943639,"Missing"
L16-1268,J14-1003,1,0.901281,"Missing"
L16-1268,D14-1110,0,0.091557,"Missing"
L16-1268,H93-1061,0,0.83636,"Missing"
L16-1268,S13-2040,0,0.221915,"Missing"
L16-1268,S01-1005,0,0.0195329,"Missing"
L16-1268,W97-0108,0,0.195731,"s. One of the reasons why IMS is performing so well is that it partly overcomes the knowledge bottleneck problem by making use of parallel data from two different languages and thus generating more training data for the LFS. Other supervised approaches focus on improving the performance of the mapping function by reducing the number of possible classes. The rationale behind this approach is to reduce the knowledge bottleneck problem by combining the training data from related senses. Good results have been reported for these approaches on WordNet Domains, Supersenses, and Base Level Concepts (Peh and Ng, 1997; Izquierdo et al., 2007). In this paper, we propose an approach to modify the output from a WSD system using a MFS classifier. We do not attempt to overcome the knowledge bottleneck problem, but we try to correct the systems for their MFS bias by reducing the mapping function to MFS and LFS. 3. System Description The starting point for our system is the output from a WSD system. We report the results for the UKB and the IMS systems. A feature set containing mostly static features, focusing predominantly on frequency and domain properties of lemmas, is combined with the WSD output and fed into"
L16-1268,S07-1016,0,0.0785243,"Missing"
L16-1268,P15-1173,0,0.0826637,"Missing"
L16-1268,W04-0811,0,0.0750197,"Missing"
L16-1268,steinberger-etal-2012-jrc,0,0.0299295,"B+C IMS IMS+C 65.9 66.1 60.6 59.4 65.9 66.1 60.6 59.4 25.3 36.3 20.9 31.5 sval2015 relation between the system sense entropy and the sense entropy in Semcor. In addition, we use mostly static features, which are the same in training, development, and test for a particular lemma. These features include TF-IDF, part of speech, number of senses and system sense entropy, as well as WordNet domains, and the WordNet Supersense. Finally, we use one feature that is dependent on the corpus used in training, development, and test. This feature makes use of the domain classifier JRC EuroVoc Indexer JEX (Steinberger et al., 2012). We compare the domain distribution of Semcor to the domain distribution of the instances of a lemma. Finally, the output from the MFS classifier and a WSD system are combined to obtain the final sense assignment. The algorithm is visualized in Algorithm 1. UKB UKB+C IMS IMS+C 68.5 69.5 67.1 64.8 67.1 68.1 65.8 63.6 20.8 27.3 17.3 23.5 Table 2: In this Table, the WSD results are presented for the competitions sval2013 and sval2015, respectively. Three measures are used to show the performance of UKB and IMS WSD systems with the MFS classifier (+C) and without. Precision (Pwsd) and Recall (Rws"
L16-1268,P10-4014,0,0.127711,"e initialized using the knowledge from the graph. Next, the node weights are updated with respect to the knowledge found in the local context of a target word, resulting in context-dependent PageRank. In general, unsupervised approaches do not suffer greatly from the knowledge bottleneck problem. However, recent work has shown that they also have a strong bias towards the MFS (Calvo and Gelbukh, 2015). Supervised approaches attempt to maximize the performance of the mapping function by training word and sense experts using (mostly) sense-labeled training data. The It makes sense (IMS) system (Zhong and Ng, 2010) is one of the best performing supervised approaches, which makes use of linear support vector machines with mostly local contextual features. The biggest challenge for supervised approaches is the reliance on manually sense-tagged training data, which is expensive and time-consuming to create, especially for high polysemous words. One of the reasons why IMS is performing so well is that it partly overcomes the knowledge bottleneck problem by making use of parallel data from two different languages and thus generating more training data for the LFS. Other supervised approaches focus on improvi"
L16-1268,S15-2049,0,\N,Missing
L18-1178,W14-2907,0,0.0577576,"Missing"
L18-1178,W13-2322,0,0.0318646,"Missing"
L18-1178,bejan-harabagiu-2008-linguistic,0,0.0598379,"Missing"
L18-1178,calzolari-etal-2012-lre,0,0.0293628,"ts, after which we focus on a subset: PropBank/NomBank (PB/NB), FactBank (FB) and TempEval-3 (TE3). We analyse their interoperability at the level of text in Section 5 and at the level of annotations in Section 6. Finally, Section 7 concludes with our lessons learned and proposes some best-practice guidelines. 2 Related Work There is a range of initiatives collecting and indexing metadata of language resources at the corpus-level to support researchers in finding the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Reposito"
L18-1178,W11-0402,0,0.0274906,"d (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoen, 2003), ISOcat (Windhouwer and Wright, 2012) and its successor CCR,3 make it possible to overcome the heterogeneity of annotation schemes by acting as an interlingua that allows mapping annotations from one scheme to another, thus addressing conceptual interoperability (Chiarcos, 2012a). However, far from all corpora that we use today follow the principles mentioned above. This may be because they were created in a time where these standards simply did not yet exist. For more recently cre"
L18-1178,cybulska-vossen-2014-using,1,0.834358,"ata is not yet publicly available, but the LDC kindly provided us with a list of training data filenames. 2010) which was built to encode event structures with relations like SUBEVENT or REASON, and intra- and crossdocument event coreference. ECB 1.0 consists of 482 documents from Google News clustered into 43 topics. A first extension to ECB was released by Lee et al. (2012), who revised and completed the original annotations and added entity coreference relations following the OntoNotes annotation guidelines for coreference (Pradhan et al., 2007). We included a second extension called ECB+ (Cybulska and Vossen, 2014), which contains another corpus consisting of 502 documents, completely (re)annotated according to new guidelines. The corpus is annotated with event classes (based on TimeML), locations and times (based on ACE and TimeML), and intra- and cross-document coreference. 3.4 Other Annotation Standards The full-text annotations in FrameNet (ICSI Berkeley, 2017) capture the frame semantic structures as defined in its lexical database (Fillmore et al., 2003). The documents come from different sources, including PropBank, the AQUAINT Program7 and the Lexical Understanding (LU) Annotation Corpus (Diab e"
L18-1178,W09-3012,0,0.0322581,"2014), which contains another corpus consisting of 502 documents, completely (re)annotated according to new guidelines. The corpus is annotated with event classes (based on TimeML), locations and times (based on ACE and TimeML), and intra- and cross-document coreference. 3.4 Other Annotation Standards The full-text annotations in FrameNet (ICSI Berkeley, 2017) capture the frame semantic structures as defined in its lexical database (Fillmore et al., 2003). The documents come from different sources, including PropBank, the AQUAINT Program7 and the Lexical Understanding (LU) Annotation Corpus (Diab et al., 2009). The latter contains annotations of dialog acts, event coreference, event relations and entity relations, but is best known for its annotations of committed belief, i.e. the strength of the author’s beliefs and the degree of commitment to their utterance (similar to FactBank). The EventStatus Corpus (Huang et al., 2017) annotated approximately 3,000 English and 1,500 Spanish news articles with temporal and aspectual properties of major societal events, that is, whether an event has already happened, is currently happening or may happen in the future. Its English documents were sourced from En"
L18-1178,doddington-etal-2004-automatic,0,0.217079,"Missing"
L18-1178,N06-2015,0,0.142213,"Missing"
L18-1178,D16-1005,0,0.0450954,"Missing"
L18-1178,W07-1501,0,0.0291678,"the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoen, 2003), ISOcat (Windhouwer and Wright, 2012) and its successor CCR,3 make it possible to overcome the heterogeneity of annotation schemes by acting as an interlingua that allows mapping annotations from one scheme to another, thus addressing conceptual interoperability (Chiarcos, 2012a). However, far from all corpora that we use today follow the principles mentioned above. This may be"
L18-1178,hinrichs-krauwer-2014-clarin,0,0.0171782,"their interoperability at the level of text in Section 5 and at the level of annotations in Section 6. Finally, Section 7 concludes with our lessons learned and proposes some best-practice guidelines. 2 Related Work There is a range of initiatives collecting and indexing metadata of language resources at the corpus-level to support researchers in finding the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoen, 2003), ISOcat (Windhouwer and Wrig"
L18-1178,D12-1045,0,0.0195649,"datasets of the TempEval shared tasks, from which we selected the TempEval3 dataset (UzZaman et al., 2013). The TimeML specifications were followed to annotate events in EventCorefBank (ECB) (Bejan and Harabagiu, 6 At the moment, the ERE data is not yet publicly available, but the LDC kindly provided us with a list of training data filenames. 2010) which was built to encode event structures with relations like SUBEVENT or REASON, and intra- and crossdocument event coreference. ECB 1.0 consists of 482 documents from Google News clustered into 43 topics. A first extension to ECB was released by Lee et al. (2012), who revised and completed the original annotations and added entity coreference relations following the OntoNotes annotation guidelines for coreference (Pradhan et al., 2007). We included a second extension called ECB+ (Cybulska and Vossen, 2014), which contains another corpus consisting of 502 documents, completely (re)annotated according to new guidelines. The corpus is annotated with event classes (based on TimeML), locations and times (based on ACE and TimeML), and intra- and cross-document coreference. 3.4 Other Annotation Standards The full-text annotations in FrameNet (ICSI Berkeley,"
L18-1178,P14-5010,0,0.00299599,"representing a token and information about its document id, sentence id, token id, token text, lemma and POS. If possible, we used the gold sentence splitting, tokenization, lemmatization and POS tagging. If that was not available (e.g. TE3 only has POS tags for most but not all events, PB/NB only has lemmas for events), we used the Stanford CoreNLP pipeline to retrieve the POS tags and the lemma of all the tokens in the datasets. 12 http://www.nltk.org/howto/propbank.html https://github.com/propbank/propbank-release 14 http://timeml.org/timeMLdocs/TimeML1.2.1.xsd 13 We used Stanford CoreNLP (Manning et al., 2014) in order to split the documents into sentences. 1105 PB/NB FB TE-3 Verb 114,574 6,377 5,835 Noun 109,793 2,498 2,451 Adjective 250 202 Preposition 45 10 Number 46 - Adverb 21 - Particle 6 - Determiner 2 - Oth/Unknown 1 2,632 Table 2: Distribution of POS tags across annotated single-token events 6.2 Conceptual Interoperability The event annotations in both FB and TE3 are based on the TimeML 1.2.1 Annotation Guidelines (Saur´ı et al., 2006), which define an event as a situation that happens or occurs. The guidelines further specify in which cases an event should or should not be annotated as a"
L18-1178,J93-2004,0,0.0609057,"Missing"
L18-1178,W04-2705,0,0.199006,"Missing"
L18-1178,W16-5706,0,0.0364542,"Missing"
L18-1178,J05-1004,0,0.236,"Missing"
L18-1178,W14-3004,0,0.0142939,"w of) that actually clearly illustrate the extent of the problems. Some studies indirectly discuss conceptual interoperability by comparing annotation schemes. For example, Aguilar et al. (2014) compare the Events, Entities and Relations represented in ACE, ERE, TAC-KBP Slot-filling, and FrameNet. Werner et al. (2015) compare the factuality/committed belief annotations in FactBank and the Language Understanding (LU) corpus. The differences between the representations of semantic propositions in PropBank, VerbNet and FactBank have been extensively described and even leveraged to build SemLink (Palmer et al., 2014). Close to our work is that of Pustejovsky et al. (2005), who discuss the issues involved in creating a Unified Linguistic Annotation (ULA) by merging the annotation schemes of PropBank, NomBank, TimeBank, the Discourse Treebank and Coreference Annotation. However, their work remains on theoretical ground by limiting their discussion to overlapping and conflicting annotations in example sentences. Our approach is unique in the sense that we provide empirical evidence by discussing the overlap of the actual annotations for the complete resources when aligning them on the same texts, as well as"
L18-1178,piperidis-2012-meta,0,0.0287955,"mpEval-3 (TE3). We analyse their interoperability at the level of text in Section 5 and at the level of annotations in Section 6. Finally, Section 7 concludes with our lessons learned and proposes some best-practice guidelines. 2 Related Work There is a range of initiatives collecting and indexing metadata of language resources at the corpus-level to support researchers in finding the right one for their task or application. These include OLAC (Simons and Bird, 2003), Language Grid (Ishida, 2006), the LRE Map (Calzolari et al., 2012), the ELRA Universal Catalog,1 the LDC Catalog,2 META-SHARE (Piperidis, 2012), CLARIN (Krauwer and Hinrichs, 2014) and Linghub (McCrae and Cimiano, 2015). The last decades have also seen various metamodel proposals for representing annotations that facilitate structural interoperability, most of which are also translatable to each other. These include GATE (Cunningham, 2002), UIMA (Ferrucci and Lally, 2004), LAF/GrAF (Ide and Romary, 2004; Ide and Suderman, 2007), NIF (Hellmann et al., 2013), NAF/GAF (Fokkens et al., 2014) and PAULA/POWLA (Chiarcos and Erjavec, 2011; Chiarcos, 2012b). Repositories of linguistic annotation terminology, such as GOLD (Farrar and Langendoe"
L18-1178,W05-0302,0,0.259717,": a proposition is formed by a predicate with its arguments; events are expressed by predicates describing situations that happen/occur (Saur´ı et al., 2006); predicates can be of “propositional” type (representing an event, state, etc.) (Meyers, 2007). There is, however, little agreement on the degree of meaning overlap and relatedness. For the sake of clarity, we will use in this article the term event to refer to all three of the overlapping and interrelated notions. Many existing event corpora contain annotations of different aspects of events that are often applied to the same documents (Pustejovsky et al., 2005), providing an interesting use case for analysing interoperability. Furthermore, event annotations involve a wide range of properties and phenomena which makes it ultimately rewarding to achieve interoperability and combine these annotations. The contributions of this paper are the following: • a comprehensive overview of interoperability issues 1101 across event corpora that result from differences in document naming conventions, textual content and structural/conceptual representations of annotations; • a method for aligning diverse language resource corpora to identify divergent and overlap"
L18-1178,W15-0812,0,0.0194146,"contain event annotations, but a selection of its documents was used in TimeBank, which does (Section 3.3). ACE was followed by Light ERE, which was designed as a lighter-weight version of ACE with the goal of making annotation easier and more consistent. Modifications to ACE for Light ERE included a reduced inventory of entity and relation types, a slightly modified and reduced event ontology, and the addition of event coreference. In turn, Light ERE has transitioned to the more complex Rich ERE, with the latter enabling a more comprehensive treatment of phenomena such as event coreference (Song et al., 2015).6 3.3 TimeML TimeML (Pustejovsky et al., 2003a) is a specification language for events and temporal expressions, designed to capture their attributes, to link them (event time-stamping) and to determine the temporal order between events. It has been applied in several corpora, including the AQUAINT TimeML Corpus (Brandeis University, 2008) and TimeBank 1.2 (Pustejovsky et al., 2006). The documents in TimeBank come from PropBank and the ACE-2 corpus. In turn, data from TimeBank and AQUAINT TimeML was used to build FactBank 1.0 (Saur´ı and Pustejovsky, 2009), adding a representation of factuali"
L18-1178,strassel-etal-2008-linguistic,0,0.0435113,"n, 145K of P2.5 data and 200K of Web data taken from other sources. PropBank’s representation of semantic roles is also used in the Abstract Meaning Representation (AMR) corpus (Knight et al., 2014), which represents the semantics of English sentences as single rooted, directed graphs with the aim of abstracting away from syntactic idiosyncrasies. It uses a variety of sources for its data, including WSJ news. 3.2 Automatic Content Extraction (ACE) & Entities, Relations and Events (ERE) The key content extraction tasks of the Automatic Content Extraction (ACE) program (Doddington et al., 2004; Strassel et al., 2008), which ran between 1999 and 2008, were defined as the automatic detection and characterization of real-world Entities, Relations, and Events. However, the program mostly focused on entities and relations between them. Event annotations are available only in the ACE 2005 Multilingual Training Corpus (Walker et al., 2006), where annotators tagged the extent, trigger, polarity, tense, genericity, modality, participants and attributes for a constrained set of event (sub)types. This data has been reused in several other corpora, including OntoNotes and Datasets for Generic Relation Extraction (reA"
L18-1178,S13-2001,0,0.239601,"es, to link them (event time-stamping) and to determine the temporal order between events. It has been applied in several corpora, including the AQUAINT TimeML Corpus (Brandeis University, 2008) and TimeBank 1.2 (Pustejovsky et al., 2006). The documents in TimeBank come from PropBank and the ACE-2 corpus. In turn, data from TimeBank and AQUAINT TimeML was used to build FactBank 1.0 (Saur´ı and Pustejovsky, 2009), adding a representation of factuality interpretation to the event annotations, and the evaluation datasets of the TempEval shared tasks, from which we selected the TempEval3 dataset (UzZaman et al., 2013). The TimeML specifications were followed to annotate events in EventCorefBank (ECB) (Bejan and Harabagiu, 6 At the moment, the ERE data is not yet publicly available, but the LDC kindly provided us with a list of training data filenames. 2010) which was built to encode event structures with relations like SUBEVENT or REASON, and intra- and crossdocument event coreference. ECB 1.0 consists of 482 documents from Google News clustered into 43 topics. A first extension to ECB was released by Lee et al. (2012), who revised and completed the original annotations and added entity coreference relatio"
L18-1178,W15-1304,0,0.0208891,"ot yet exist. For more recently created corpora, however, there is presumably a plethora of reasons. We hypothesize that one of them is that whereas working groups such as the Open Linguistics Working Group (OWLG)4 actively promote resource interoperability, there seem to be few examples (that we know of) that actually clearly illustrate the extent of the problems. Some studies indirectly discuss conceptual interoperability by comparing annotation schemes. For example, Aguilar et al. (2014) compare the Events, Entities and Relations represented in ACE, ERE, TAC-KBP Slot-filling, and FrameNet. Werner et al. (2015) compare the factuality/committed belief annotations in FactBank and the Language Understanding (LU) corpus. The differences between the representations of semantic propositions in PropBank, VerbNet and FactBank have been extensively described and even leveraged to build SemLink (Palmer et al., 2014). Close to our work is that of Pustejovsky et al. (2005), who discuss the issues involved in creating a Unified Linguistic Annotation (ULA) by merging the annotation schemes of PropBank, NomBank, TimeBank, the Discourse Treebank and Coreference Annotation. However, their work remains on theoretical"
L18-1480,P10-1143,0,0.127948,"led lemma baseline to establish coreference relations1 scores already very high in 1 all occurrences of the same word, e.g. “murder”, mention a this dataset and is difficult to beat by state-of-the-art systems. From the perspective of a real-world situation and the many different ways in which events can be described and framed in language, these datasets are far too sparse and do not reflect true ambiguity and variation. Partly due to this lack of data and variation, automatic event coreference detection has made little progress over the years, especially across documents (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016; Vossen and Cybulska, 2016). All data listed in Table 1 are created according to the T2D approach: a selection of text is made and interpreted by annotators who add an annotation layer. Creating data following a T2D approach is expensive and labor-intense, as all mentions of events need to be cross-checked against all other mentions across documents for coreference relations. With the size of the data, the effort increases exponentially. 2.2. State of text-to-data guidelines Besides meta-level choices on what needs to be"
L18-1480,W09-3208,0,0.0363136,"ted that the so-called lemma baseline to establish coreference relations1 scores already very high in 1 all occurrences of the same word, e.g. “murder”, mention a this dataset and is difficult to beat by state-of-the-art systems. From the perspective of a real-world situation and the many different ways in which events can be described and framed in language, these datasets are far too sparse and do not reflect true ambiguity and variation. Partly due to this lack of data and variation, automatic event coreference detection has made little progress over the years, especially across documents (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016; Vossen and Cybulska, 2016). All data listed in Table 1 are created according to the T2D approach: a selection of text is made and interpreted by annotators who add an annotation layer. Creating data following a T2D approach is expensive and labor-intense, as all mentions of events need to be cross-checked against all other mentions across documents for coreference relations. With the size of the data, the effort increases exponentially. 2.2. State of text-to-data guidelines Besides meta-level c"
L18-1480,cybulska-vossen-2014-using,1,0.128179,"PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004), and FrameNet (Baker et al., 2003). In these corpora, syntactic structures are taken as the starting point and all predicates mark mentions of events. Similarity of events follows from the assigned event type, the meaning of the word or frame, and from having similar argument structures. These corpora, however, lack a notion of event reference and are not very well-suited for studying the different ways we describe the same or similar events. For the latter purpose, specific event coreference corpora have been created: ECB+ (Cybulska and Vossen, 2014), RED (O’Gorman et al., 2016), among others. Event coreference annotations have been created using what we call a text-to-data (T2D) approach. In the T2D approach, annotators start from the text and first decide what phrases are labeled as event mentions after which different event mentions are related to each other through an event coreference relation. The coreference relations establish event identity across event mentions a posteriori by chaining event mentions that share coreference relations. Due to the complexity and labor-intensity of this T2D approach, only a limited amount of referen"
L18-1480,P16-1145,0,0.0216019,"Missing"
L18-1480,W13-1203,0,0.600783,"rence annotations. This annotation process resulted in the Gun Violence Corpus, whose development process and outcome are described in this paper. Keywords: event coreference, text corpora, structured data 1. Introduction Events and entities are central to referential semantics. Semantic parsing of news articles not only concerns detecting the meaning of words and their relations, but especially establishing the referential relations to the outside world. For entities, it is straightforward what this referential world is. However, compared to entities, events are less tangible (Guarino, 1999; Hovy et al., 2013) for various reasons: 1. we use a small vocabulary to name events, which results in large referential ambiguity 2. events are more open to interpretation and framing, which leads to more variation in making reference 3. events are less persistent in time than entities 4. each event has many idiosyncratic properties, e.g. unique participants playing different roles in a unique spatio-temporal context, making generalization harder. Due to these properties, textual data on events is more fragmented than textual data on entities. In news, events are mentioned during a very short period of time and"
L18-1480,C16-1112,1,0.885726,"notated (1.8 sentences per article on average in ECB+). The Rich Entities, Relations and Events corpus (Song et al., 2015) and the Richer Event Description corpus (O’Gorman et al., 2016) are two recent initiatives to manually create similar annotations for all sentences in articles and also partially across documents, but the number of documents covered is small. We analysed the referential annotations in a number of these datasets, revealing that they, despite efforts such as the creation of ECB+, hardly reflect referential ambiguity and show very little variation (Cybulska and Vossen, 2014; Ilievski et al., 2016). For example, ECB with 482 documents contains 8 news articles on one specific murder, but since there are no other murders in the dataset, searching for the word “murder” results in almost all mentions of that specific incident with high accuracy: one-form-onereferent and one-referent-one-form. Cybulska and Vossen (2014) demonstrated that the so-called lemma baseline to establish coreference relations1 scores already very high in 1 all occurrences of the same word, e.g. “murder”, mention a this dataset and is difficult to beat by state-of-the-art systems. From the perspective of a real-world"
L18-1480,D12-1045,0,0.189487,"oth withinand cross-document relations: less than four thousand documents and less than forty thousand mentions in total (10 mentions per document on average). The ratios between mentions and clusters vary considerably, which is due to the different ways in which the datasets have been compiled: either subsets of the sentences and/or event types were annotated or all mentions in a full article. Cross-document data is more sparse than within-document data, as can be seen in Table 1. ECB+ (Cybulska and Vossen, 2014) therefore extended the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008; Lee et al., 2012) from 482 articles to 982 articles by including more events of the same type. This slightly increased the referential ambiguity and variation, but, nevertheless, only a few sentences per article were annotated (1.8 sentences per article on average in ECB+). The Rich Entities, Relations and Events corpus (Song et al., 2015) and the Richer Event Description corpus (O’Gorman et al., 2016) are two recent initiatives to manually create similar annotations for all sentences in articles and also partially across documents, but the number of documents covered is small. We analysed the referential anno"
L18-1480,bartalesi-lenzi-etal-2012-cat,0,0.0670306,"Missing"
L18-1480,liu-etal-2014-supervised,0,0.0147239,"elations1 scores already very high in 1 all occurrences of the same word, e.g. “murder”, mention a this dataset and is difficult to beat by state-of-the-art systems. From the perspective of a real-world situation and the many different ways in which events can be described and framed in language, these datasets are far too sparse and do not reflect true ambiguity and variation. Partly due to this lack of data and variation, automatic event coreference detection has made little progress over the years, especially across documents (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016; Vossen and Cybulska, 2016). All data listed in Table 1 are created according to the T2D approach: a selection of text is made and interpreted by annotators who add an annotation layer. Creating data following a T2D approach is expensive and labor-intense, as all mentions of events need to be cross-checked against all other mentions across documents for coreference relations. With the size of the data, the effort increases exponentially. 2.2. State of text-to-data guidelines Besides meta-level choices on what needs to be annotated, guidelines and annotation"
L18-1480,L16-1631,0,0.0162004,"1 all occurrences of the same word, e.g. “murder”, mention a this dataset and is difficult to beat by state-of-the-art systems. From the perspective of a real-world situation and the many different ways in which events can be described and framed in language, these datasets are far too sparse and do not reflect true ambiguity and variation. Partly due to this lack of data and variation, automatic event coreference detection has made little progress over the years, especially across documents (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016; Vossen and Cybulska, 2016). All data listed in Table 1 are created according to the T2D approach: a selection of text is made and interpreted by annotators who add an annotation layer. Creating data following a T2D approach is expensive and labor-intense, as all mentions of events need to be cross-checked against all other mentions across documents for coreference relations. With the size of the data, the effort increases exponentially. 2.2. State of text-to-data guidelines Besides meta-level choices on what needs to be annotated, guidelines and annotations tend to differ in criteria for dec"
L18-1480,D16-1038,0,0.182225,"lready very high in 1 all occurrences of the same word, e.g. “murder”, mention a this dataset and is difficult to beat by state-of-the-art systems. From the perspective of a real-world situation and the many different ways in which events can be described and framed in language, these datasets are far too sparse and do not reflect true ambiguity and variation. Partly due to this lack of data and variation, automatic event coreference detection has made little progress over the years, especially across documents (Chen and Ji, 2009; Bejan and Harabagiu, 2010; Lee et al., 2012; Liu et al., 2014; Peng et al., 2016; Lu and Ng, 2016; Vossen and Cybulska, 2016). All data listed in Table 1 are created according to the T2D approach: a selection of text is made and interpreted by annotators who add an annotation layer. Creating data following a T2D approach is expensive and labor-intense, as all mentions of events need to be cross-checked against all other mentions across documents for coreference relations. With the size of the data, the effort increases exponentially. 2.2. State of text-to-data guidelines Besides meta-level choices on what needs to be annotated, guidelines and annotations tend to differ in"
L18-1480,2018.gwc-1.25,1,0.816465,"dentifying the WordNet synsets kill.v.01 (cause to die; put to death, usually intentionally or knowingly) and killing.n.02 (the act of terminating a life). However, this 15 The corpus can be downloaded at: https://github. com/cltl/GunViolenceCorpus 3040 is not the case for all expressions in the GVC. For example, expressions like mourn and autopsy that refer to the event type Death show that manual and automatic annotators can not fully rely on resources to detect all event types correctly, but that additional reasoning is needed. We analyze the referential potential of this corpus further in Vossen et al. (2018). 5. Conclusions We discussed the problems in collecting large scale and high-quality text corpora for event extraction, and specifically with respect to identity and reference. We concluded that most data has been created through a text-todata method, which faces the obstacles of data size and scalability. To circumvent these, we propose a scalable data-to-text method to create far more data with high quality, ambiguity, and variation. Following this method, we created the Gun Violence Corpus, whose development is reported in this paper. We present the specification and the guidelines of the"
L18-1480,bejan-harabagiu-2008-linguistic,0,0.0230463,"and mentions is small for both withinand cross-document relations: less than four thousand documents and less than forty thousand mentions in total (10 mentions per document on average). The ratios between mentions and clusters vary considerably, which is due to the different ways in which the datasets have been compiled: either subsets of the sentences and/or event types were annotated or all mentions in a full article. Cross-document data is more sparse than within-document data, as can be seen in Table 1. ECB+ (Cybulska and Vossen, 2014) therefore extended the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008; Lee et al., 2012) from 482 articles to 982 articles by including more events of the same type. This slightly increased the referential ambiguity and variation, but, nevertheless, only a few sentences per article were annotated (1.8 sentences per article on average in ECB+). The Rich Entities, Relations and Events corpus (Song et al., 2015) and the Richer Event Description corpus (O’Gorman et al., 2016) are two recent initiatives to manually create similar annotations for all sentences in articles and also partially across documents, but the number of documents covered is small. We analysed t"
L18-1480,S16-1193,0,0.0251147,"lis events are annotated or also irrealis events; aspectual verbs (“begin”, “stop”, “continue”, “happen”, “take place”) are sometimes seen as events and sometimes not; adjectival or adverbial modifiers (“fatal accident”) are not marked, etc. Such choices are based on a priori criteria regardless of the types of events annotated and they tend to vary depending on the specific task for which the data were annotated e.g. semantic role detection (Kingsbury and Palmer, 2002), detecting temporal and causal event relations (Boguraev et al., 2007; Pustejovsky and Verhagen, 2009; Bethard et al., 2015; Caselli and Morante, 2016), or event coreference relations (Hovy et al., 2013). Besides the differences in guidelines, annotators following guidelines may also have different interpretations, which may lead to relatively low inter-annotator-agreement and conservative annotation strategies. Due to the complexity of the task, annotators may for example stay on the safe side and create identity relations only when the same word is used, hence eliminating variation. Such difficulties in defining events, event relations, and event coreference have led to the creation of the KBP2015 dataset (Mitamura et al., 2015) in which a"
L18-1480,W16-1701,0,0.0403626,"Missing"
L18-1480,kingsbury-palmer-2002-treebank,0,0.499332,"time. We thus do not find a typical Zipfian distribution for events, with a few events that dominate the news (the head) and a long tail of low-frequent events, but a more even low-frequent distribution. Given this fragmented distribution, it is not surprising that NLP tasks on event detection, event relation detection, and event coreference are difficult, which is reflected by relatively low inter-annotator-agreements and small amounts of data with event annotations. Most corpora with event annotations do not consider how they relate to the same or similar events in the world, e.g. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004), and FrameNet (Baker et al., 2003). In these corpora, syntactic structures are taken as the starting point and all predicates mark mentions of events. Similarity of events follows from the assigned event type, the meaning of the word or frame, and from having similar argument structures. These corpora, however, lack a notion of event reference and are not very well-suited for studying the different ways we describe the same or similar events. For the latter purpose, specific event coreference corpora have been created: ECB+ (Cybulska and Vossen, 2014), RED (O’Go"
L18-1480,W04-2705,0,0.098541,"pfian distribution for events, with a few events that dominate the news (the head) and a long tail of low-frequent events, but a more even low-frequent distribution. Given this fragmented distribution, it is not surprising that NLP tasks on event detection, event relation detection, and event coreference are difficult, which is reflected by relatively low inter-annotator-agreements and small amounts of data with event annotations. Most corpora with event annotations do not consider how they relate to the same or similar events in the world, e.g. PropBank (Kingsbury and Palmer, 2002), NomBank (Meyers et al., 2004), and FrameNet (Baker et al., 2003). In these corpora, syntactic structures are taken as the starting point and all predicates mark mentions of events. Similarity of events follows from the assigned event type, the meaning of the word or frame, and from having similar argument structures. These corpora, however, lack a notion of event reference and are not very well-suited for studying the different ways we describe the same or similar events. For the latter purpose, specific event coreference corpora have been created: ECB+ (Cybulska and Vossen, 2014), RED (O’Gorman et al., 2016), among other"
L18-1480,L16-1699,0,0.0446357,"Missing"
L18-1480,W15-0809,0,0.0311537,"2015; Caselli and Morante, 2016), or event coreference relations (Hovy et al., 2013). Besides the differences in guidelines, annotators following guidelines may also have different interpretations, which may lead to relatively low inter-annotator-agreement and conservative annotation strategies. Due to the complexity of the task, annotators may for example stay on the safe side and create identity relations only when the same word is used, hence eliminating variation. Such difficulties in defining events, event relations, and event coreference have led to the creation of the KBP2015 dataset (Mitamura et al., 2015) in which a weaker definition of an event has been applied, so-called Event Nuggets, to ease the annotation and the task for establishing coreference relations. In the KBP2015 dataset, “attack”, “shooting”, and “murder” do not represent separate event instances, but are considered single unique event and hence are coreferential 3035 Table 1: Event coreference corpora for English created by a text-to-data method Name Reference ACE2005 KBP2015 OntoNotes IC EECB ECB+ MEANTIME EER RED Total GVC (Peng et al., 2016) (Mitamura et al., 2015) (Pradhan et al., 2007) (Hovy et al., 2013) (Lee et al., 2012"
L18-1480,W16-5706,0,0.066561,"Missing"
L18-1480,S07-1016,0,0.121451,"Missing"
L18-1480,W09-2418,0,0.015044,"(“measles is a deadly disease”) are excluded; only realis events are annotated or also irrealis events; aspectual verbs (“begin”, “stop”, “continue”, “happen”, “take place”) are sometimes seen as events and sometimes not; adjectival or adverbial modifiers (“fatal accident”) are not marked, etc. Such choices are based on a priori criteria regardless of the types of events annotated and they tend to vary depending on the specific task for which the data were annotated e.g. semantic role detection (Kingsbury and Palmer, 2002), detecting temporal and causal event relations (Boguraev et al., 2007; Pustejovsky and Verhagen, 2009; Bethard et al., 2015; Caselli and Morante, 2016), or event coreference relations (Hovy et al., 2013). Besides the differences in guidelines, annotators following guidelines may also have different interpretations, which may lead to relatively low inter-annotator-agreement and conservative annotation strategies. Due to the complexity of the task, annotators may for example stay on the safe side and create identity relations only when the same word is used, hence eliminating variation. Such difficulties in defining events, event relations, and event coreference have led to the creation of the"
L18-1480,W15-0812,0,0.056173,"ences and/or event types were annotated or all mentions in a full article. Cross-document data is more sparse than within-document data, as can be seen in Table 1. ECB+ (Cybulska and Vossen, 2014) therefore extended the Event Coreference Bank (ECB) (Bejan and Harabagiu, 2008; Lee et al., 2012) from 482 articles to 982 articles by including more events of the same type. This slightly increased the referential ambiguity and variation, but, nevertheless, only a few sentences per article were annotated (1.8 sentences per article on average in ECB+). The Rich Entities, Relations and Events corpus (Song et al., 2015) and the Richer Event Description corpus (O’Gorman et al., 2016) are two recent initiatives to manually create similar annotations for all sentences in articles and also partially across documents, but the number of documents covered is small. We analysed the referential annotations in a number of these datasets, revealing that they, despite efforts such as the creation of ECB+, hardly reflect referential ambiguity and show very little variation (Cybulska and Vossen, 2014; Ilievski et al., 2016). For example, ECB with 482 documents contains 8 news articles on one specific murder, but since the"
L18-1725,bartalesi-lenzi-etal-2012-cat,0,0.0253447,"were created of which 2244 circumstantial ones and 193 subevents. On average, 7 Subevents are currently not modeled in CEO, but they were annotated for future experiments and evaluations. every ECB+/CEO article contains about 7 new coreference sets and about 5 different circumstantial relations. Instances Coreference sets CEO relations - of which Circumstantial - of which subEvent ECB+ 3323 3323 - ECB+/CEO 3038 3448 2437 2244 193 Table 1: Overview of the annotations made for ECB+/CEO in contrast with ECB+ for the topics annotated For the annotation, we used the CAT annotation tool (Bartalesi Lenzi et al., 2012) which outputs the annotations in XML. In terms of annotation effort, a single article took about 30 minutes to annotate on average. The corpus and the annotation guidelines are publicly available at https: //github.com/newsreader/eso-and-ceo. Inter Annotator Agreement For the calculation of the Inter Annotator Agreement (IAA), we selected 25 articles from five different topics in ECB+/CEO covering variation in article length and complexity. The evaluation was carried out on the CEO links. Agreement was calculated on the existence, or identification, of CEO links. CEO links are created between"
L18-1725,W09-1206,0,0.087529,"Missing"
L18-1725,W17-2712,0,0.030511,"two Abstract Objects (called Arg1 and Arg2), corresponding to discourse units, rather than event mentions. The contingency relation is annotated either in presence of an explicit connective, i.e. a lexical item, between the two abstract objects, or implicitly, by adjacency in discourse. In our approach, contingency relations are one of the possible values which express circumstantial relations, and, most importantly, they are independent of the presence of connectives or adjacency in discourse, but grounded on (shared) properties of events. A related resource is the Rich Event Ontology (REO) (Brown et al., 2017), that provides an independent semantic backbone to different lexical resources such as FrameNet and VerbNet. REO will have explicit causal relations between event classes as well as predefined pre- and post conditions. However, these relations are more strictly defined and on class level. On the other hand, CEO maintains a looser definition in terms of causality, and takes into account the roles affected by the event and the circumstantial relation. A resource such as CEO is envisioned to be of added value for several NLP tasks such as script mining, question answering, information extraction"
L18-1725,L18-1051,1,0.889849,"Missing"
L18-1725,chambers-jurafsky-2010-database,0,0.336561,"perties. The implication is however not necessary. Previous work on the encoding of semantic relations between event pairs has focused on specific subsets of circumstantial relations. For instance, one example is the encoding of the entailment relations in WordNet (Fellbaum, 1998). With respect to the WordNet approach, we abstract from various event types (i.e. lexical items) and do not depend on relations defined at a synset level, by formalizing event knowledge and relations in an ontology. We also provide more details on the property involved. Another related approach are narrative chains (Chambers and Jurafsky, 2010), that provide chains of various event mentions. However, the relation between these mentions is not specified explicitly but based on co-occurrence of participants and a basic precedence relation. Manual inspection of these chains revealed that dissimilar relations are implied within these chains, varying from temporal ordering, to episodic, up to causal. The Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) annotates contingency relations, of which causal relations are a subclass. In PDTB, the focus of the annotation is between two Abstract Objects (called Arg1 and Arg2), corresponding to"
L18-1725,cybulska-vossen-2014-using,1,0.768256,"ication of instances of the calamity classes in CEO, we used Chamber’s narrative chains (Chambers and Jurafsky, 2010). This selection was made manually, based on at least three calamity events per event chain. We also manually selected FrameNet frames that capture calamity events and we used the SUMO ontology as a backbone for modeling our initial list of verbs 3 Contents of CEO The ECB+/CEO Corpus In addition to the CEO, we developed a corpus of annotated circumstantial event relations. For this, we build upon an existing corpus, specifically annotated for event coreference: the ECB+ Corpus (Cybulska and Vossen, 2014). ECB+ consists of 984 news articles divided over 42 topics. From these topics, we manually selected 22 topics (508 articles) that cover calamities such as earthquakes, murders, hijacks and arson. In ECB+, only the most relevant event mentions are manually annotated. For ECB+/CEO, we automatically extended the set of annotated event mentions by applying a state-of-the art machine learning based system 6 . Two linguistically trained annotators were hired for the selection of relevant calamity events and the annotation of circumstantial relations. More specifically, the annotation procedure cons"
L18-1725,L16-1423,0,0.041351,"Missing"
L18-1725,L16-1233,1,0.772607,"is envisioned to be of added value for several NLP tasks such as script mining, question answering, information extraction, and textual entailment, among others. Furthermore, the explicitly defined relations between events can be of help in reconstructing Figure 1: The ESO assertions for the class eso:Damaging storylines (Van den Akker et al., 2010; Vossen et al., 2015) and improve the coherence of existing narrative chain models (Chambers and Jurafsky, 2010). 3. The Circumstantial Event Ontology CEO builds upon an existing event ontology called the Event and Implied Situation Ontology (ESO) (Segers et al., 2016). ESO is designed to run over the output of Semantic Role Labeling systems by making explicit both the ontological type of the predicative element and the situation that holds before, during and after the predicate. Each so called pre-, post- and during situation consists of a set of properties and roles that define what holds true. For instance, as can be seen in Figure 1, the pre- and post-situations of the event class “eso:Damaging” define: • that something is in a “relatively plus (+)” state (presituation); • that this something is in a “relatively less (-)” state, i.e. it underwent a loss"
L18-1725,W17-2706,1,0.142835,"cting” event may, but not necessarily, lead to “ceo:Injuring” or “ceo:Damaging”, which is based on the shared property of some object being damaged. Modeling these relations provides a means to track chains of logically related events and their shared participants within and across documents. Semantic circumstantial relations define possible explanatory sequences of events, but not the actual explanatory sequences. Episodic relations, on the other hand, define circumstantial relations that are dependent on the actual occurrences of events in the world. The Circumstantial Event Ontology (CEO) (Segers et al., 2017) 1 , described in this paper, models such semantic relations, based on shared properties of the event classes with the aim to support the detection of episodic circumstantial relations in texts. Modeling these semantic relations in an ontology will allow us to 1.) abstract over the different lexical realizations of the same concept (i.e. at an event mention level); and 2.) facilitate reasoning between event classes and enrich the extraction of information for event knowledge and event sequences. The remainder of this paper is organized as follows: in section 2., we describe related work; in se"
L18-1725,W15-4507,1,0.837072,"hese relations are more strictly defined and on class level. On the other hand, CEO maintains a looser definition in terms of causality, and takes into account the roles affected by the event and the circumstantial relation. A resource such as CEO is envisioned to be of added value for several NLP tasks such as script mining, question answering, information extraction, and textual entailment, among others. Furthermore, the explicitly defined relations between events can be of help in reconstructing Figure 1: The ESO assertions for the class eso:Damaging storylines (Van den Akker et al., 2010; Vossen et al., 2015) and improve the coherence of existing narrative chain models (Chambers and Jurafsky, 2010). 3. The Circumstantial Event Ontology CEO builds upon an existing event ontology called the Event and Implied Situation Ontology (ESO) (Segers et al., 2016). ESO is designed to run over the output of Semantic Role Labeling systems by making explicit both the ontological type of the predicative element and the situation that holds before, during and after the predicate. Each so called pre-, post- and during situation consists of a set of properties and roles that define what holds true. For instance, as"
laparra-etal-2012-mapping,cuadros-etal-2010-integrating,1,\N,Missing
laparra-etal-2012-mapping,alvez-etal-2008-complete,1,\N,Missing
laparra-etal-2012-mapping,W11-0129,0,\N,Missing
maks-etal-2008-adjectives,peters-peters-2000-treatment,0,\N,Missing
maks-etal-2014-generating,E09-1046,0,\N,Missing
maks-etal-2014-generating,E09-1077,0,\N,Missing
maks-etal-2014-generating,oostdijk-etal-2008-coi,0,\N,Missing
maks-etal-2014-generating,C12-2036,0,\N,Missing
maks-etal-2014-generating,vossen-etal-2008-integrating,1,\N,Missing
maks-etal-2014-generating,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
maks-etal-2014-generating,roventini-etal-2000-italwordnet,0,\N,Missing
maks-vossen-2010-annotation,kamps-etal-2004-using,0,\N,Missing
maks-vossen-2010-annotation,N06-1026,0,\N,Missing
maks-vossen-2010-annotation,W08-1207,0,\N,Missing
maks-vossen-2010-annotation,H05-1044,0,\N,Missing
maks-vossen-2010-annotation,P06-1134,0,\N,Missing
maks-vossen-2010-annotation,C04-1200,0,\N,Missing
maks-vossen-2010-annotation,J04-3002,0,\N,Missing
maks-vossen-2010-annotation,P97-1023,0,\N,Missing
maks-vossen-2010-annotation,vossen-etal-2008-integrating,1,\N,Missing
maks-vossen-2010-annotation,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
maks-vossen-2012-building,maks-vossen-2010-annotation,1,\N,Missing
maks-vossen-2012-building,kamps-etal-2004-using,0,\N,Missing
maks-vossen-2012-building,N06-1026,0,\N,Missing
maks-vossen-2012-building,W08-1207,0,\N,Missing
maks-vossen-2012-building,H05-1044,0,\N,Missing
maks-vossen-2012-building,N09-1002,0,\N,Missing
maks-vossen-2012-building,N09-1001,0,\N,Missing
maks-vossen-2012-building,W03-1017,0,\N,Missing
maks-vossen-2012-building,P06-1134,0,\N,Missing
maks-vossen-2012-building,D07-1115,0,\N,Missing
maks-vossen-2012-building,W11-1702,1,\N,Missing
maks-vossen-2012-building,P97-1023,0,\N,Missing
maks-vossen-2012-building,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
P13-1166,O97-1002,0,0.00955661,":// search.cpan.org/dist/WordNet-Similarity/. 1693 measure Spearman ρ Kendall τ ranking min max min max variation path based similarity path 0.70 0.78 0.55 0.62 1-8 wup 0.70 0.79 0.53 0.61 1-6 lch 0.70 0.78 0.55 0.62 1-7 path based information content res 0.65 0.75 0.26 0.57 4-11 lin 0.49 0.73 0.36 0.53 6-10 jcn 0.46 0.73 0.32 0.55 5, 7-11 path based relatedness hso 0.73 0.80 0.36 0.41 1-3,5-10 dictionary and corpus based relatedness vpairs 0.40 0.70 0.26 0.50 7-11 vector 0.48 0.92 0.33 0.76 1,2,4,6-11 lesk 0.66 0.83 -0.02 0.61 1-8,11,12 Leacock and Chodorow (1998) (lch), Resnik (1995) (res), Jiang and Conrath (1997) (jcn), Lin (1998) (lin), Banerjee and Pedersen (2003) (lesk), Hirst and St-Onge (1998) (hso) and Patwardhan and Pedersen (2006) (vector and vpairs) respectively. Consequently, settings and properties were changed systematically and shared with Pedersen who attempted to produce the new results with his own implementations. First, we made sure that the script implemented by Fokkens could produce the same WordNet similarity scores for each individual word pair as those used to calculate the ranking on the mc-set by Pedersen (2010). Finally, the gold standard and exact implementation of the Spear"
P13-1166,J06-1003,0,0.0172015,"es the difference in rank as its basis to calculate a correlation, where Kendall τ uses the number of items with the correct rank. The low Kendall τ for lesk is the result of three pairs receiving a score that is too high. Other pairs that get a relatively accurate score are pushed one place down in rank. Because only items that receive the exact same rank help to increase τ , such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ . We included τ , because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results"
P13-1166,W08-2206,0,0.013078,"at receive the exact same rank help to increase τ , such a shift can result in a drastic drop in the coefficient. In our opinion, Spearman ρ is therefore preferable over Kendall τ . We included τ , because many authors do not mention the ranking coefficient they use (cf. Budanitsky and Hirst (2006), Resnik (1995)) and both ρ and τ are com1695 monly used coefficients. Except for WordNet, which Budanitsky and Hirst (2006) hold accountable for minor variations in a footnote, the influential categories we investigated in this paper, to our knowledge, have not yet been addressed in the literature. Cramer (2008) points out that results from WordNet-Human similarity correlations lead to scattered results reporting variations similar to ours, but she compares studies using different measures, data and experimental setup. This study shows that even if the main properties are kept stable, results vary enough to change the identity of the measure that yields the best performance. Table 1 reveals a wide variation in ranking relative to alternative approaches. Results in Table 2 show that it is common for the ranking of a score to change due to variations that are not at the core of the method. This study s"
P13-1166,P05-1045,0,0.0057573,"Missing"
P13-1166,W06-2501,1,0.861219,"n the methodology, such as data sets, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by R"
P13-1166,N10-1047,1,0.4317,"s, evaluation metrics and type of cross-validation can influence the conclusions of an experiment, as we also find in our second use case. However, they focus on the problem of evaluation and recommendations on how to achieve consistent reproducible results. Our contribution is to investigate how much results vary. We cannot control how fellow researchers carry out their evaluation, but if we have an idea of the variations that typically occur within a system, we can better compare approaches for which not all details are known. 3 WordNet Similarity Measures Patwardhan and Pedersen (2006) and Pedersen (2010) present studies where the output of a variety of WordNet similarity and relatedness measures are compared. They rank Miller and Charles (1991)’s set (henceforth “mc-set”) of 30 word pairs according to their semantic relatedness with several WordNet similarity measures. Each measure ranks the mc-set of word pairs and these outputs are compared to Miller and Charles (1991)’s gold standard based on human rankings using the Spearman’s Correlation Coefficient (Spearman, 1904, ρ). Pedersen (2010) also ranks the original set of 65 word pairs ranked by humans in an experiment by Rubenstein and Gooden"
P13-1166,P94-1019,0,0.0317356,"us research, address the following questions: 1) Which properties have an impact on the performance of WordNet similarity measures? 2) How much does the performance of individual measures vary? 3) How do commonly used measures compare when the variation of their performance are taken into account? 3.2 Methodology and first observations The questions above were addressed in two stages. In the first stage, Fokkens, who was not involved in the first replication attempt implemented a script to calculate similarity measures using WordNet::Similarity. This included similarity measures introduced by Wu and Palmer (1994) (wup), 3 Obtained from http://talisker.d.umn.edu/ cgi-bin/similarity/similarity.cgi, WordNet::Similarity version 2.05. This web interface has now moved to http://maraca.d.umn.edu 4 WordNet::Similarity were obtained http:// search.cpan.org/dist/WordNet-Similarity/. 1693 measure Spearman ρ Kendall τ ranking min max min max variation path based similarity path 0.70 0.78 0.55 0.62 1-8 wup 0.70 0.79 0.53 0.61 1-6 lch 0.70 0.78 0.55 0.62 1-7 path based information content res 0.65 0.75 0.26 0.57 4-11 lin 0.49 0.73 0.36 0.53 6-10 jcn 0.46 0.73 0.32 0.55 5, 7-11 path based relatedness hso 0.73 0.80 0"
P13-1166,J08-3010,1,\N,Missing
P13-1166,J04-4004,0,\N,Missing
P13-1166,J03-4003,0,\N,Missing
R13-1021,R11-1036,0,0.0364343,"Missing"
R13-1021,D12-1045,0,0.115918,"establishing coreference relations; obviously together with other coreference indicators such as lemma repetition, anaphora, synonymy and disjunction. Once semantic distance and granularity agreement is calculated for every component of an event pair, the separate scores are combined into a single score for an event pair indicating the likelihood of real world coreference as a whole. Through empirical testing, we determine thresholds for establishing optimal coreference relations across events and their components. 4 Experiments For the experiments we used the stand-off annotation of events (Lee et al. 2012) on top of the EventCorefBank (ECB) corpus 2, annotated with cross - document coreference between event mentions. The corpus contains 482 texts from Google News (selected based on inclusion of keywords such as commercial transaction, attack, death or sports) and grouped into 43 topics. To measure the influence of time, location and participants on event coreference resolution, we first extract the set of events from the evaluation data. The ECB texts were processed by means of tools developed within the KYOTO project 3 . First, the corpus was lemmatized and tagged with PoS and syntactic inform"
R13-1021,M95-1005,0,\N,Missing
R13-1021,N01-1008,0,\N,Missing
R13-1021,H05-1004,0,\N,Missing
R13-1021,W11-1901,0,\N,Missing
R13-1021,P05-1020,0,\N,Missing
R13-1021,I11-1012,0,\N,Missing
R13-1021,N06-1025,0,\N,Missing
R13-1021,P02-1014,0,\N,Missing
R13-1021,P10-1143,0,\N,Missing
R13-1021,W13-1203,0,\N,Missing
R13-1021,W11-0143,0,\N,Missing
R13-1021,cybulska-vossen-2010-event,1,\N,Missing
R13-1021,S12-1006,0,\N,Missing
R13-1021,bejan-harabagiu-2008-linguistic,0,\N,Missing
R13-1021,W11-1506,1,\N,Missing
R13-1054,W12-3708,0,0.0174762,"f the different types of ratings on the performance of two widely used sentiment-analysis techniques. Finally, we conclude with a discussion of our findings. 2 Related Work There is a large body of work concerning sentiment analysis of customer reviews (Liu, 2012). Most of these studies regard sentiment analysis as a classification problem and apply supervised learning methods where the positive and negative classes are determined by reviewer ratings. Studies propose additional annotations only when focusing on novel information which is not reflected in the user ratings (Toprak et al., 2010, Ando and Ishizaki, 2012). The issue of a possible mismatch between reviewer ratings and review text is usually not addressed. Much attention is paid to the customer’s (or reader’s) perspective in studies in the area of business and social science. Mahony et al. (2010) and Ghose et al. (2012) study product reviews in relation to customer behavior. Their aim is to identify reviews which are considered 416 helpful to customers and to know what kind of reviews affect sales. Their work is similar to ours because of the focus on the effect of the review text on the customer/reader, but they also include other types of info"
R13-1054,D08-1083,0,0.0331482,"ew’s sentiment than reviewer ratings. 4 Implications for sentiment analysis We investigated how automated sentiment analysis methods perform with the different sets of annotations by applying two widely used approaches to document-level sentiment classification. Classifier accuracy is measured against the three sets of ratings (R1, R2 and REV) we described in the previous section. 4.1 The lexicon-based approach The first method is a lexicon-based approach which starts from a text which is lemmatized with the Dutch Alpino-parser1.The approach is similar to the “vote-flip-algorithm” proposed by Choi and Cardie (2008). The intuition about this algorithm is simple: for each review the number of matched positive and negative words from the sentiment lexicon are counted. If polar words are preceded by a negator, their polarity is flipped; if polar words are preceded by an intensifier, their polarity is doubled. We then assign the majority polarity to the review. In the case of a tie (being zero or higher than zero), we assign neutral polarity. The sentiment lexicon used in this approach is an automatically derived general language sentiment lexicon obtained by WordNet propagation (Maks and Vossen, 2011). 4.2"
R13-1054,P10-1059,0,0.0257114,"we study the effect of the different types of ratings on the performance of two widely used sentiment-analysis techniques. Finally, we conclude with a discussion of our findings. 2 Related Work There is a large body of work concerning sentiment analysis of customer reviews (Liu, 2012). Most of these studies regard sentiment analysis as a classification problem and apply supervised learning methods where the positive and negative classes are determined by reviewer ratings. Studies propose additional annotations only when focusing on novel information which is not reflected in the user ratings (Toprak et al., 2010, Ando and Ishizaki, 2012). The issue of a possible mismatch between reviewer ratings and review text is usually not addressed. Much attention is paid to the customer’s (or reader’s) perspective in studies in the area of business and social science. Mahony et al. (2010) and Ghose et al. (2012) study product reviews in relation to customer behavior. Their aim is to identify reviews which are considered 416 helpful to customers and to know what kind of reviews affect sales. Their work is similar to ours because of the focus on the effect of the review text on the customer/reader, but they also i"
R13-1054,P12-2018,0,0.0388279,"l language sentiment lexicon obtained by WordNet propagation (Maks and Vossen, 2011). 4.2 The machine-learning approach The second method is a machine learning approach that also starts from a text that is lemmatized by the Dutch Alpino-parser. After lemmatization the text is transformed to a word-vector representation by applying Weka’s StringToWord Vector with frequency representation (instead of binary). We used Weka’s NaiveBayesMultinominal (NBM) classifier to classify the reviews. The NBM was chosen because our review texts are rather short (with an average of 68 words) and, according to Wang and Manning (2012), NBM classifiers perform well on short snippets of 1 http://www.let.rug.nl/ vannoord/alp/Alpino/ 418 text. Results reported are average of ten-foldcross-validation-accuracies using R1, R2 and REV ratings as training and test data. 4.3 Results on different types of ratings Results are evaluated against the whole set of 1,172 reviews (cf. table 2 ‘all’). As many approaches to sentiment analysis do not use the class of weak sentiment (Liu, 2012), we also evaluated against a subset of strong negative (ratings 1 to 3) and strong positive (ratings 8 to 10) reviews (cf. table 2, ‘strong’). Table (2)"
R13-1092,E09-1005,0,0.104876,"Vector Machines, which belongs to the family of linear separators (Cortes and Vapnik, 1995). This technique was extensively used in automatic classification tasks applying WSD systems and showed excellent performance in very high dimensional and sparse feature spaces, which is typically the case for WSD. In the project, we used the library SVMLight5 . In this case the features were a bag-of-words around the target words. We also carried out a filtering process similar to the one mentioned above. The third system (DSC-UKB) was an unsupervised Machine Learning system based on the UKB algorithm (Agirre and Soroa, 2009). This algorithm implements a so-called Personalized Page Rank algorithm similar to the one used by Google. It considers Wordnet as a graph where each synset is a node in the graph and the relation between the synsets are seen as edges between the nodes. Disambiguation is performed through the ranking of the candidate nodes following the Personalized Page Rank algorithm. We used different sets of relations to build the graph: relations of the Dutch WordNet, English Wordnet, equivalence relations from Dutch synsets to English synsets, WordNet Domain relations and co-ocurrence relations extracte"
R13-1092,P94-1020,0,0.076365,"uential tagging usually results in an all-words corpus that contains annotations for all content words in texts. Targeted tagging usually results in a lexical sample corpus, a selection of target word occurrences with different contexts annotated with senses. The most famous example of an all-words corpus is SemCor (Miller et al., 1993), which was created through sequential tagging of parts of the Brown corpus (186 texts have all-words annotation, while in 166 texts only the verbs are annotated). An example of a lexicalsample corpus is the so-called line-hard-serve cor2 See also the interest (Bruce and Wiebe, 1994) corpus Only Senseval-1 used a different lexical database. Senseval2&3 used WordNet1.7 and subsequent competitions used other versions of WordNet (Fellbaum, 1998). 3 711 2. Use this lexical sample corpus to train a WSD system that automatically annotates the remainder of a very large and diverse corpus. This corpus represents a large variety of contexts (criterion 2), while the WSD does not suffer from over-fitting for the MFS or for contexts and properties of the training corpus. Likewise, the system can detect rare senses equally well as frequent senses. senting the corpus rather than repres"
R13-1092,J07-4005,0,0.0245743,"th state-of-the-art results for English. We think that future research is needed to find out whether the drop in results is due to context diversity or other facts. Finally, the sense-probabilities were tested against an all-words corpus. Again, the results are compatible with state-of-the-art results for English. As such, we can expect that the senseprobabilities derived from DutchSemCor will also provide as strong a baseline as the MFS from SemCor is now for English. Last but not least, SoNaR provides many opportunities to differentiate these distributions over different domains and genres (McCarthy et al., 2007). 10 References Eneko Agirre and Philip Edmonds. 2006. Word sense disambiguation : algorithms and applications. Text, speech and language technology. Springer, Dordrecht, NE. Conclusion In this paper, we presented a classification of different sense-annotated corpora and described their (dis-)advantages. We proposed a method for meeting three different requirements for sense-tagged corpora. From a manually annotated seed corpus, we automatically extended the representative annotations through WSD, where we used high-confidence results and active learning for low-performing words. A small propo"
R13-1092,H93-1061,0,0.774205,"ins and usages of Dutch. The results of the project can be downloaded freely from the project website. 1 Credits The DutchSemCor project was an NWO Humanities medium investment subsidy project with a subsidy period of September 2009 - August 2012. We would like to thank NWO for making it possible to carry out the project. 2 Introduction Word Sense Disambiguation (WSD) research in the last decade demonstrated a number of important insights (Agirre and Edmonds, 2006): 1. evaluation results are strongly dependent on the corpus and the lexicons used, 2. the most-frequentsense derived from SemCor (Miller et al., 1993) is 1 http://www2.let.vu.nl/oz/cltl/dutchsemcor/ 710 Proceedings of Recent Advances in Natural Language Processing, pages 710–718, Hissar, Bulgaria, 7-13 September 2013. pus (Mooney, 1996)2 , which contains 4,000 instances of the noun line (six meanings), 4,000 instances of the verb serve (four meanings), and 4,000 instances of the adjective hard (three meanings). Another lexical-sample corpus is DSO which has annotations only for the most frequent and ambiguous nouns (121) and verbs (70) in parts of the Brown corpus and a selection of Wall Street Journal articles, but is comparable in size to"
R13-1092,W96-0208,0,0.133357,"Missing"
R13-1092,P96-1006,0,0.111912,"lements a K-nearest neighbor algorithm (Aha et al., 1991). TiMBL has been widely used in NLP tasks. In the project, we used three different types of features. From the local context, we selected the word forms, lemmas and part-of-speech tags. The global context was modelled through bag-of-words contained in the same sentence as the target word. Finally, the system made use of information on SoNaR text type and of the token identifier to which the example belonged. Some filtering for the bag-of-words was performed in order to ensure the quality of the word predictors following the approach in (Ng and Lee, 1996). The second system (DSC-SVM) uses a supervised Machine Learning approach based on Support Vector Machines, which belongs to the family of linear separators (Cortes and Vapnik, 1995). This technique was extensively used in automatic classification tasks applying WSD systems and showed excellent performance in very high dimensional and sparse feature spaces, which is typically the case for WSD. In the project, we used the library SVMLight5 . In this case the features were a bag-of-words around the target words. We also carried out a filtering process similar to the one mentioned above. The thir"
R13-1092,oostdijk-etal-2008-coi,0,0.0750985,"Missing"
S07-1001,P00-1064,0,0.0127234,"ets the occurrence identifier, the sense tag (if in training), and the list of features that apply to the occurrence. 5 http://ixa2.si.ehu.es/semeval-clir/ 6 http://en.wikipedia.org/wiki/ Information retrieval 4 Allocation. Using topic-specific synset similarity measures, they create predictions for each word in each document using only word frequency information. The disambiguation process took aprox. 12 hours on a cluster of 48 machines (dual Xeons with 4GB of RAM). Note that contrary to the specifications, this team returned WordNet 2.1 senses, so we had to map automatically to 1.6 senses (Daude et al., 2000). UNIBA This team uses a a knowledge-based WSD system that attempts to disambiguate all words in a text by exploiting WordNet relations. The main assumption is that a specific strategy for each Part-Of-Speech (POS) is better than a single strategy. Nouns are disambiguated basically using hypernymy links. Verbs are disambiguated according to the nouns surrounding them, and adjectives and adverbs use glosses. ORGANIZERS In addition to the regular participants, and out of the competition, the organizers run a regular supervised WSD system trained on Semcor. The system is based on a single k-NN cl"
S07-1001,P97-1010,0,\N,Missing
S07-1001,D07-1007,0,\N,Missing
S07-1001,P07-1005,0,\N,Missing
S07-1001,W99-0624,0,\N,Missing
S10-1013,J07-4005,0,0.597125,"0.505 ±0.026 0.350 R verbs 0.450 ±0.034 0.454 ±0.034 0.291 ±0.025 0.403 ±0.033 0.293 R 0.529 ±0.021 0.521 ±0.018 0.496 ±0.019 0.462 ±0.020 0.294 R nouns 0.530 ±0.024 0.522 ±0.023 0.507 ±0.020 0.472 ±0.024 0.308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Re"
S10-1013,W04-0807,0,0.0284195,"omain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The"
S10-1013,E09-1005,1,0.474766,".308 R verbs 0.528 ±0.038 0.519 ±0.035 0.468 ±0.037 0.437 ±0.035 0.257 Table 3: Overall results for the domain WSD datasets, ordered by recall. This is the only group using hand-tagged data from the target domain. Their best run ranked 1st. with two variants. In the first (IIITH1), the vertices of the graph are initialized following the ranking scores obtained from predominant senses as in (McCarthy et al., 2007). In the second (IIITH2), the graph is initialized with keyness values as in IIITTH: They presented a personalized PageRank algorithm over a graph constructed from WordNet similar to (Agirre and Soroa, 2009), 77 CFILT-2 CFILT-1 IIITH1-d.l.ppr.05 IIITH2-d.l.ppr.05 BLC20SCBG BLC20SC CFILT-3 Treematch Treematch-2 Kyoto-2 Treematch-3 RACAI-MFS UCF-WS HIT-CIR-DMFS UCF-WS-domain IIITH2-d.r.l.baseline.05 IIITH1-d.l.baseline.05 RACAI-2MFS-BOW IIITH1-d.l.ppv.05 IIITH2-d.r.l.ppv.05 UCF-WS-domain.noPropers Kyoto-1 BLC20BG NLEL-WSD-PDB RACAI-Lexical-Chains MFS NLEL-WSD Rel. Sem. Trees Rel. Sem. Trees-2 Rel. Cliques 0.3 0.35 0.4 0.45 0.5 0.55 Figure 1: Plot for all the systems which participated in English domain WSD. Each point correspond to one system (denoted in axis Y) according each recall and confidence"
S10-1013,S07-1016,0,0.0570391,"English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to the participants included"
S10-1013,S07-1097,0,0.0246659,"wledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most Frequent Sense from SemCor; 2) Conceptual Density ; 3) Supervised Domain Relative Entropy classifier based on WordNet Domains; 4) Supervised Bayesian classifier based on WordNet Domains probabilities; and 5) Unsupervised Knownet-20 classifiers. The best run ranked 24th. UMCC-DLSI (Relevant): The team submitted three different runs using a knowledge-based system. The first two runs use domain vectors and the third is based on cliques, which measure how much a concept is correlated to the sentence by obtaining Relevant S"
S10-1013,W08-2114,0,0.0667055,"e was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods using fuzzy-Borda voting. A similar system was proposed in SemEval-2007 task-7 (Buscaldi and Rosso, 2007). In this case, the component method used where the following ones: 1) Most"
S10-1013,W04-0811,0,0.196041,"anguages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dutch English Italian The data made available to th"
S10-1013,E09-1045,0,0.0508574,"Missing"
S10-1013,vossen-etal-2008-kyoto,1,0.763195,"ses in specific domains, the context of the senses might change, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. The main goal of this task is to provide a multilingual testbed to evaluate WSD systems when faced with full-texts from a specific domain. All datasets and related information are publicly available from the task websites1 . This task was designed in the context of Kyoto (Vossen et al., 2008)2 , an Asian-European project that develops a community platform for modeling knowledge and finding facts across languages and cultures. The platform operates as a Wiki system with an ontological support that social communities can use to agree on the meaning of terms in specific domains of their interest. Kyoto focuses on the environmental domain because it poses interesting challenges for information sharing, but the techniques and platforms are Domain portability and adaptation of NLP components and Word Sense Disambiguation systems present new challenges. The difficulties found by supervis"
S10-1013,S01-1004,0,0.00916615,"the environment domain for WSD in four languages (Chinese, Dutch, English, Italian). 11 teams participated, with supervised and knowledge-based systems, mainly in the English dataset. The results show that in all languages the participants where able to beat the most frequent sense heuristic as estimated from general corpora. The most successful approaches used some sort of supervision in the form of hand-tagged examples from the domain. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in previous Senseval and SemEval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Snyder and Palmer, 2004; Pradhan et al., 2007). Spe1 http://xmlgroup.iit.cnr.it/SemEval2010/ and http://semeval2.fbk.eu/ 2 http://www.kyoto-project.eu/ 75 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 75–80, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics independent of the application domain. The paper is structured as follows. We first present the preparation of the data. Section 3 reviews participant systems and Section 4 the results. Finally, Section 5 presents the conclusions. 2 Chinese Dut"
S10-1013,H05-1053,0,0.486241,". Note that this method of estimating statistical significance might be more strict than other pairwise methods. We also include the results of two baselines. The random baseline was calculated analytically. The first sense baseline for each language was taken from each wordnet. The first sense baseline in English and Chinese corresponds to the most frequent sense, as estimated from out-of-domain corpora. In Dutch and Italian, it followed the intuitions of the lexicographer. Note that we don’t have the most frequent sense baseline from the domain texts, which would surely show higher results (Koeling et al., 2005). thesauri from bilingual parallel corpora. The system ranked 14. UCFWS: This knowledge-based WSD system was based on an algorithm originally described in (Schwartz and Gomez, 2008), in which selectors are acquired from the Web via searching with local context of a given word. The sense is chosen based on the similarity or relatedness between the senses of the target word and various types of selectors. In some runs they include predominant senses(McCarthy et al., 2007). The best run ranked 13th. NLEL-WSD(-PDB): The system used for the participation is based on an ensemble of different methods"
S10-1013,N09-1004,0,\N,Missing
S10-1013,W00-0901,0,\N,Missing
S10-1093,E09-1005,1,0.892281,"sources. Our participation in Semeval task #17 focused on producing running systems for all languages in the task, and we attained good results in all except Chinese. Due to the pressure of the time-constraints in the competition, the system is still under development, and we expect results to improve in the near future. 1 2 We will present in turn UKB, the Tybots, and the lexical knowledge-bases used. 2.1 UKB UKB is a knowledge-based unsupervised WSD system which exploits the structure of an underlying Language Knowledge Base (LKB) and finds the most relevant concepts given an input context (Agirre and Soroa, 2009). UKB starts by taking the LKB as a graph of concepts G = (V, E) with a set of vertices V derived from LKB concepts and a set of edges E representing relations among them. Giving an input context, UKB applies the so called Personalized PageRank (Haveliwala, 2002) over it to obtain the most representative senses for the context. PageRank (Brin and Page, 1998) is a method for scoring the vertices V of a graph according to each node’s structural importance. The algorithm can be viewed as random walk process that postulate the existence of a particle that randomly traverses the graph, but at any t"
S10-1093,S10-1013,1,0.884699,"Missing"
S10-1093,bosma-vossen-2010-bootstrapping,1,0.72995,"et 3.0 with gloss relations (Fellbaum, 1998). Dutch: The Dutch LKB is part of the Cornetto database version 1.3 (Vossen et al., 2008). The Cornetto database can be obtained from the Dutch/Flanders Taalunie3 . Cornetto comprises taxonomic relations and equivalence rela2 #rels. Table 1: Wordnets and their sizes (entries, synsets, relations and links to WN30g). Tybots (Term Yielding Robots) are text mining software that mine domain terms from corpus (e.g. web pages), organizing them in a hierarchical structure, connecting them to wordnets and ontologies to create a semantic model for the domain (Bosma and Vossen, 2010). The software is freely available using Subversion 2 . Tybots try to establish a view on the terminology of the domain which is as complete as possible, discovering relations between terms and ranking terms by domain relevance. Preceding term extraction, we perform tokenization, part-of-speech tagging and lemmatization, which is stored in Kyoto Annotation Format (KAF) (Bosma et al., 2009). Tybots work through KAF documents, acquire domain relevant terms based on the syntactic features, gather cooccurrence statistics to decide which terms are significant in the domain and produce a thesaurus w"
S10-1093,P98-2127,0,0.0122903,"y et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Finally, we used up to 50 related words for each target word. As in run1, we used the monolingual graphs for the LKBs in each language. Table 2: Overall results of our runs, including precision (P) and recall (R), overall and for each PoS. We include the First Sense (1sense) and random baselines, as well as the best run, as provided by the organizers. 3.2 Run2: UKB using related words Run1: UKB using context The first run is an application of the UKB tool in the standard setting, as described in (Agirre and Soroa, 2009). Given the input text, we split it in sentences, and we disambiguate eac"
S10-1093,J07-4005,0,0.0212144,"ea is to first obtain a list of related words for each of the target words, as collected from a domain corpus. On a second step each target word is disambiguated using the N most related words as context (see below). For instance, in order to disambiguate the word environment, we would not take into account the context of occurrence (as in Section 3.2), but we would use the list of most related words in the thesaurus (e.g. “biodiversity, agriculture, ecosystem, nature, life, climate, . . .”). Using UKB over these contexts we obtain the most predominant sense for each target word in the domain(McCarthy et al., 2007), which is used to label all occurrences of the target word in the test dataset. In order to build the thesaurus with the lists of related words, we used Tybots (c.f. section 2.2), one for each corpus of the evaluation dataset, i.e. Chinese, Dutch, English, and Italian. We used the background documents provided by the organizers, which we processed using the linguistic processors of the project to obtain the documents in KAF. We used the Tybots with the following settings. We discarded co-occurring words with frequencies below 105 . Distributional similarity was computed using (Lin, 1998). Fin"
S10-1093,C98-2122,0,\N,Missing
S15-2058,W09-2420,1,0.898155,"Missing"
S15-2058,E12-1060,0,0.0909961,"and the EE 6 We have used the Python library GenSim for this purpose http://radimrehurek.com/gensim/ 7 This process can be quite time consuming (there are a total of 15 entries in DBpedia for HumanGene, but there are 1.65 million entries for Person) 347 corpus, which usually is a large collection of NLPprocessed documents. For each lemma in these documents, we extract all the sentences containing this lemma. If there are at least 100 sentences, we feed the sentences for this specific lemma into the predominant sense algorithm. The predominant sense algorithm we use is based on topic modeling (Lau et al., 2012; Lau et al., 2014). The algorithm first tries to induce senses using a Hierarchical Dirichlet Process and then tries to determine the sense ranking of all senses of a lemma according to the documents. The output of this step is a list of sense confidences for each lemma for which we had enough training data. 8 2.2 Route 2: it–makes–sense WSD system Our idea is to start from the output of a state-of-theart WSD system, and combine it with the predominant sense information automatically gathered with our approach, in order to obtain an overall WSD approach specific to our target domain. We selec"
S15-2058,P14-1025,0,0.132878,"ve used the Python library GenSim for this purpose http://radimrehurek.com/gensim/ 7 This process can be quite time consuming (there are a total of 15 entries in DBpedia for HumanGene, but there are 1.65 million entries for Person) 347 corpus, which usually is a large collection of NLPprocessed documents. For each lemma in these documents, we extract all the sentences containing this lemma. If there are at least 100 sentences, we feed the sentences for this specific lemma into the predominant sense algorithm. The predominant sense algorithm we use is based on topic modeling (Lau et al., 2012; Lau et al., 2014). The algorithm first tries to induce senses using a Hierarchical Dirichlet Process and then tries to determine the sense ranking of all senses of a lemma according to the documents. The output of this step is a list of sense confidences for each lemma for which we had enough training data. 8 2.2 Route 2: it–makes–sense WSD system Our idea is to start from the output of a state-of-theart WSD system, and combine it with the predominant sense information automatically gathered with our approach, in order to obtain an overall WSD approach specific to our target domain. We selected the it–makes–se"
S15-2058,S13-2040,0,0.0475309,"Missing"
S15-2058,S01-1005,0,0.0375979,"in Natural Language Processing. Many different approaches have been proposed throughout the years to tackle this task from different perspectives. In addition, competitions have been organized to compare the performance of these approaches. Our hypothesis is that, in general, the context is not being modelled properly by the systems, which usually consider very narrow contexts and do not pay any attention to the background information or information that is not explicitly included in the text. We conducted an in-depth error analysis of previous all-words tasks (Senseval–2 : English all words (Palmer et al., 2001), Senseval– 3 : English all words (Snyder and Palmer, 2004), Semeval–2007 : all words task 17 (Pradhan et al., 2007), Semeval–2010 : all words task 17 (Agirre et al., 2010), Semeval–2013 : all words task 12 (Navigli et al., 2013)) in order to gain better insight as to sva l3 20 Introduction sva l2 1 100Accuracy when sense is MFS versus when it is not Figure 1: The average accuracy of all systems per competition is shown. Figure 1 shows the average accuracy of all the systems per competition. We clearly observe the trend that systems perform well when the sense is the most frequent sense, but n"
S15-2058,S07-1016,0,0.0305445,"Missing"
S15-2058,W04-0811,0,0.0521054,"es have been proposed throughout the years to tackle this task from different perspectives. In addition, competitions have been organized to compare the performance of these approaches. Our hypothesis is that, in general, the context is not being modelled properly by the systems, which usually consider very narrow contexts and do not pay any attention to the background information or information that is not explicitly included in the text. We conducted an in-depth error analysis of previous all-words tasks (Senseval–2 : English all words (Palmer et al., 2001), Senseval– 3 : English all words (Snyder and Palmer, 2004), Semeval–2007 : all words task 17 (Pradhan et al., 2007), Semeval–2010 : all words task 17 (Agirre et al., 2010), Semeval–2013 : all words task 12 (Navigli et al., 2013)) in order to gain better insight as to sva l3 20 Introduction sva l2 1 100Accuracy when sense is MFS versus when it is not Figure 1: The average accuracy of all systems per competition is shown. Figure 1 shows the average accuracy of all the systems per competition. We clearly observe the trend that systems perform well when the sense is the most frequent sense, but not in other cases. Furthermore, when the sense is not the m"
S15-2058,P10-4014,0,0.0227628,"thm first tries to induce senses using a Hierarchical Dirichlet Process and then tries to determine the sense ranking of all senses of a lemma according to the documents. The output of this step is a list of sense confidences for each lemma for which we had enough training data. 8 2.2 Route 2: it–makes–sense WSD system Our idea is to start from the output of a state-of-theart WSD system, and combine it with the predominant sense information automatically gathered with our approach, in order to obtain an overall WSD approach specific to our target domain. We selected the it–makes–sense system (Zhong and Ng, 2010) that has proved to be one of the best performing WSD systems in general domains. Similarly, we have created our own wrapper around the it–makes– sense system that allows the use of NAF format as input/output for this tool9 . Following our purpose, we did not only select the most likely sense in each case according to the WSD engine, but we stored all the possible senses for each lemma along with the probability returned by it–makes–sense. 2.3 Voting For each token in the test data, we first check if we have predominant sense output for this lemma. In addition, we check if the sense ranking is"
S15-2058,S10-1013,1,\N,Missing
S15-2058,S15-2049,0,\N,Missing
S15-2133,P09-1068,0,0.0302034,"ent levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output"
S15-2133,chambers-jurafsky-2010-database,0,0.0132799,"ation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system,"
S15-2133,I11-1012,0,0.0308787,"nteractions between the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have"
S15-2133,R13-1021,1,0.8536,"the participants involved in an event individually; • Event relations: in an entity based representation, event mentions with more than one entity as their participants will be repeated in the final representation (both at in-document at crossdocument levels); such a representation can be further used to explore and discover additional event relations2 ; • Event coreference: we assume that two event mentions (either in the same document or in different documents) are coreferential if they share the same participant set (i.e., entities) and occur at the same time and place (Chen et al., 2011; Cybulska and Vossen, 2013); • Temporal relations: temporal relation processing can benefit from an entity driven approach as sequences of events sharing the same entities (i.e., co-participant events) can be assumed to stand in precedence relation (Chambers and Jurafsky, 2009; Chambers and Jurafsky, 2010). 2.1 The SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 W"
S15-2133,lopez-de-lacalle-etal-2014-predicate,0,0.0274096,"Missing"
S15-2133,S10-1063,0,0.124598,"SPINOZA VU System The NWR pipeline which forms the basis of the SPINOZA VU system consists of 15 modules carrying out various NLP tasks and outputs the results in NLP Annotation Format (Fokkens et al., 2014), a layered standoff representation format. Two versions of the system have been developed, namely: 2 We are referring to a broader set of relations that we labeled as “bridging relations” among events which involve coparticipation, primary and secondary causal relations, temporal relations, and entailment relations. 788 • SPINOZA VU 1 uses the output of a state of the art system, TIPSem (Llorens et al., 2010), for event detection and temporal relations; • SPINOZA VU 2 is entirely based on data from the NWR pipeline including the temporal (TLINKs) and causal relation (CLINKs) layers. The final output is based on a dedicated rulebased module, the TimeLine (TML) module. We will describe in the following paragraphs how each subtask has been tackled with respect to each version of the system. Entity identification Entity identification relies on the entity detection and disambiguation layer (NERD) of the NWR pipeline. Each detected entity is associated with a URI (a unique identifier), either from DBpe"
S15-2133,S13-2001,0,0.0340615,"extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build structured event indexes from large volumes of"
S15-2133,S07-1014,0,0.171512,"towards a more complex task such as storyline extraction. 1 Introduction This paper reports on a system (SPINOZA VU) for timeline extraction developed at the CLTL Lab of the VU Amsterdam in the context of the SemEval 2015 Task 4: Cross Document TimeLines. In this task, a timeline is defined as a set of chronologically anchored and ordered events extracted from a corpus spanning over a (large) period of time with respect to a target entity. Cross-document timeline extraction benefits from previous works and evaluation campaigns in Temporal Processing, such as the TempEval evaluation campaigns (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013) and aims at promoting research in temporal processing by tackling the following issues: cross-document and cross-temporal event detection and ordering; event coreference (indocument and cross-document); and entity-based temporal processing. The SPINOZA VU system is based on the NewsReader (NWR) NLP pipeline (Agerri et al., 2013; Beloki et al., 2014), which has been developed within the context of the NWR project1 and provides multi-layer annotations over raw texts from tokenization up to temporal relations. The goal of the NWR project is to build"
S15-2133,S10-1010,1,\N,Missing
S18-1009,P16-1145,0,0.024206,"ith respect to ambiguity, reference, and variation, and that is representative for the long tail as well, needs to fit certain constraints. 2.2 Reading Comprehension & Question Answering In several recent tasks, systems are asked to answer entity-based questions, typically by point3 https://github.com/cltl/ LongTailQATask 71 ing to the correct segment or coreference chain in text, or by composing an answer by abstracting over multiple paragraphs/text pieces. These tasks are based on Wikipedia (SQuAD (Rajpurkar et al., 2016), WikiQA (Yang et al., 2015), QASent (Wang et al., 2007), WIKIREADING (Hewlett et al., 2016)) or on annotated individual documents (MARCO (Nguyen et al., 2016), CNN and DailyMail datasets (Hermann et al., 2015)). Weston et al. (2015) outlined 20 skill sets, such as causality, resolving time and location, and reasoning over world knowledge, that are needed to build an intelligent QA system. These have been partially captured by the datasets MCTest (Richardson et al., 2013) and QuizBowl (Iyyer et al., 2014)), as well as the SemEval task on Answer Selection in Community Question Answering (Nakov et al., 2015, 2016).4 However, all these datasets avoid representing real-world referential"
S18-1009,C16-1112,1,0.89229,"Missing"
S18-1009,D14-1070,0,0.0721595,"Missing"
S18-1009,S18-1109,0,0.0606904,"Missing"
S18-1009,H05-1004,0,0.149665,"ween the system and the gold standard, resulting in a value for the customary metrics of Precision, Recall, and F1 per question. The scores per subtask are then averaged over all questions to compute a single documentlevel evaluation score. The mention-level evaluation is a crossdocument event coreference evaluation. Mentionlevel evaluation is only done for questions with the event types killing or injuring. We apply the customary metrics to score the event coreference: BCUB (Bagga and Baldwin, 1998), BLANC (Recasens and Hovy, 2011), entity-based CEAF (CEAF E) and mention-based CEAF (CEAF M) (Luo, 2005), and MUC (Vilain et al., 1995). The final F1-score is the average of the F1-scores of the individual metrics. The set of mentions to annotate should conform to the schema defined in the task annotation guidelines.11 Data Partitioning We divided the overall task data into two partitions: trial and test data. In practice, we separated these two data partitions by reserving one year of news documents (2017) from our task for the trial data, while using all the other data as test data. The trial data stems from the gun violence domain, whereas the test data also contains data from the fire incide"
S18-1009,S18-1010,0,0.0292707,"Table 3 shows that it is similar to the test data with respect to the core properties, meaning that the trial data can be used as training data. 6 Evaluation This Section describes the evaluation criteria in this task and the baselines we compare against. 6.1 Baselines Criteria Evaluation is performed on three levels: incidentlevel, document-level, and mention-level. The incident-level evaluation compares the numeric answer provided by the system to the gold 11 Link to the guidelines: https://goo.gl/8JpwCE. The code of the baselines can be found here: https: //goo.gl/MwSqBj. 12 75 Team ID-DE (Mirza et al., 2018) created KOI (Knowledge of Incidents), a system that builds a knowledge graph of incidents, given news articles as input. The required steps include: 1. Document preprocessing using various semantic NLP tasks such as Word Sense Disambiguation, Named-Entity Recognition, Temporal expression recognition, and Semantic Role Labeling. 2. Incident extraction and document clustering based on the output of step 1. 3. Ontology construction to capture the knowledge model from incidents and documents which makes it possible to run SPARQL queries on the ontology to answer the questions. to the same inciden"
S18-1009,S15-2047,0,0.0126869,"rehension & Question Answering In several recent tasks, systems are asked to answer entity-based questions, typically by point3 https://github.com/cltl/ LongTailQATask 71 ing to the correct segment or coreference chain in text, or by composing an answer by abstracting over multiple paragraphs/text pieces. These tasks are based on Wikipedia (SQuAD (Rajpurkar et al., 2016), WikiQA (Yang et al., 2015), QASent (Wang et al., 2007), WIKIREADING (Hewlett et al., 2016)) or on annotated individual documents (MARCO (Nguyen et al., 2016), CNN and DailyMail datasets (Hermann et al., 2015)). Weston et al. (2015) outlined 20 skill sets, such as causality, resolving time and location, and reasoning over world knowledge, that are needed to build an intelligent QA system. These have been partially captured by the datasets MCTest (Richardson et al., 2013) and QuizBowl (Iyyer et al., 2014)), as well as the SemEval task on Answer Selection in Community Question Answering (Nakov et al., 2015, 2016).4 However, all these datasets avoid representing real-world referential ambiguity to its full extent by mainly asking questions that require knowledge about popular Wikipedia entities and/or text understanding of"
S18-1009,P98-1013,0,0.173819,"he per-question scores allow us to detect and remove the “easy” ones, and keep those that: 1. have a high number of answer incidents (only applicable to S2 and S3) 2. have a high number of confusion incidents 3. have a high average number of answer and confusion documents, i.e. news sources describing the answer and the confusion incidents correspondingly 4. have a Question Template Questions in each subtask consist of an event type and two event properties. Event type We consider four event types in this task described through their representation in WordNet (Fellbaum, 1998) and FrameNet (F. Baker et al., 1998). Each question is constrained by exactly one event type. event type description meanings killing at least one person is killed wn30:killing.n.02 wn30:kill.v.01 fn17:Killing injuring at least one person is injured wn30:injure.v.01 wn30:injured.a.01 fn17:Cause harm fn17:Experiencebodily harm the event of something burning wn30:fire.n.01 fn17:Fire burning terminated employment wn30:displace.v.03 fn17:Firing fire burning job firing Question Creation Table 2: Description of the event types. The meanings column lists meanings that best describe the event type. It contains both FrameNet 1.7 frames ("
S18-1009,W16-6004,1,0.875842,"Missing"
S18-1009,D13-1020,0,0.0326457,"t, or by composing an answer by abstracting over multiple paragraphs/text pieces. These tasks are based on Wikipedia (SQuAD (Rajpurkar et al., 2016), WikiQA (Yang et al., 2015), QASent (Wang et al., 2007), WIKIREADING (Hewlett et al., 2016)) or on annotated individual documents (MARCO (Nguyen et al., 2016), CNN and DailyMail datasets (Hermann et al., 2015)). Weston et al. (2015) outlined 20 skill sets, such as causality, resolving time and location, and reasoning over world knowledge, that are needed to build an intelligent QA system. These have been partially captured by the datasets MCTest (Richardson et al., 2013) and QuizBowl (Iyyer et al., 2014)), as well as the SemEval task on Answer Selection in Community Question Answering (Nakov et al., 2015, 2016).4 However, all these datasets avoid representing real-world referential ambiguity to its full extent by mainly asking questions that require knowledge about popular Wikipedia entities and/or text understanding of a single document.5 Unlike existing work, our task deliberately addresses the referential ambiguity of the world beyond Wikipedia, by asking questions about long-tail events described in multiple documents. By doing so, we require deep process"
S18-1009,M95-1005,0,0.600243,"he gold standard, resulting in a value for the customary metrics of Precision, Recall, and F1 per question. The scores per subtask are then averaged over all questions to compute a single documentlevel evaluation score. The mention-level evaluation is a crossdocument event coreference evaluation. Mentionlevel evaluation is only done for questions with the event types killing or injuring. We apply the customary metrics to score the event coreference: BCUB (Bagga and Baldwin, 1998), BLANC (Recasens and Hovy, 2011), entity-based CEAF (CEAF E) and mention-based CEAF (CEAF M) (Luo, 2005), and MUC (Vilain et al., 1995). The final F1-score is the average of the F1-scores of the individual metrics. The set of mentions to annotate should conform to the schema defined in the task annotation guidelines.11 Data Partitioning We divided the overall task data into two partitions: trial and test data. In practice, we separated these two data partitions by reserving one year of news documents (2017) from our task for the trial data, while using all the other data as test data. The trial data stems from the gun violence domain, whereas the test data also contains data from the fire incidents and business domain. A subs"
S18-1009,S18-1108,1,0.67282,"hat there are also 10 corresponding incidents. No baseline was implemented for subtask 3. Mention annotation baseline We annotate mentions of events of type killing and injuring, when these surface forms or their synonyms in WordNet are found as tokens in a document. We assume that all mentions of the same event type within a document are coreferential, whereas all mentions found in different documents are not. 7 8 Results Participants R In this Section, we describe the systems that took part in SemEval-2018 task 5. We refer to the individual system papers for further information. NewsReader (Vossen, 2018) consists of three steps: 1. the event mentions in the input documents are represented as Event-Centric Knowledge Graphs (ECKGs). 2. the ECKGs of all documents are compared to each other to decide which documents refer to the same incident, resulting in an incident-document index. 3. the constraints of each question (its event type, time, participant names, and location) are matched with the stored ECKGs, resulting in a number of incidents and source documents for each question. NAI-SEA (Liu and Li, 2018) consists of three components: 1. extraction of basic information on time, location, and p"
S18-1009,D07-1003,0,0.0090562,"ion, a task that is challenging with respect to ambiguity, reference, and variation, and that is representative for the long tail as well, needs to fit certain constraints. 2.2 Reading Comprehension & Question Answering In several recent tasks, systems are asked to answer entity-based questions, typically by point3 https://github.com/cltl/ LongTailQATask 71 ing to the correct segment or coreference chain in text, or by composing an answer by abstracting over multiple paragraphs/text pieces. These tasks are based on Wikipedia (SQuAD (Rajpurkar et al., 2016), WikiQA (Yang et al., 2015), QASent (Wang et al., 2007), WIKIREADING (Hewlett et al., 2016)) or on annotated individual documents (MARCO (Nguyen et al., 2016), CNN and DailyMail datasets (Hermann et al., 2015)). Weston et al. (2015) outlined 20 skill sets, such as causality, resolving time and location, and reasoning over world knowledge, that are needed to build an intelligent QA system. These have been partially captured by the datasets MCTest (Richardson et al., 2013) and QuizBowl (Iyyer et al., 2014)), as well as the SemEval task on Answer Selection in Community Question Answering (Nakov et al., 2015, 2016).4 However, all these datasets avoid"
S18-1009,D15-1237,0,0.0451039,"Missing"
S18-1108,E09-1005,0,0.0392139,"e following properties for each event: subclass relations with WordNet synsets and FrameNet frames, denotedBy pointers to the offset positions in the original texts, the words or labels used to mention the event, PropBank roles filled by DBpedia URIs, or unresolved phrases that are not entities and finally, the date to which the events are anchored. In this output of NewsReader, we did not apply any event coreference and we represent each mention as a separate event instance or ECKG. The WordNet synsets and FrameNet frames are associated through the WSD modules in NewsReader. We used the UKB (Agirre and Soroa, 2009) and IMS ((Zhong and Ng, 2010) to score the WordNet synsets for each predicate. Next, we take the highest scoring synsets and use the Predicate Matrix (Carreras et al., 2014) to obtain the associated FrameNet frames. The interpretation of the predicates as events for the task is thus derived from the SRL output in combination with the WSD output and the Predicate Matrix association. We call the above output of NewsReader the raw-ECKGs. In the next sections, we describe how we post-process these to derive so-called task-ECKGs with only the information relevant for the task. We finally reason ov"
S18-1108,E14-2003,0,0.0230314,"Missing"
S18-1108,P10-4014,0,0.0147889,"ent: subclass relations with WordNet synsets and FrameNet frames, denotedBy pointers to the offset positions in the original texts, the words or labels used to mention the event, PropBank roles filled by DBpedia URIs, or unresolved phrases that are not entities and finally, the date to which the events are anchored. In this output of NewsReader, we did not apply any event coreference and we represent each mention as a separate event instance or ECKG. The WordNet synsets and FrameNet frames are associated through the WSD modules in NewsReader. We used the UKB (Agirre and Soroa, 2009) and IMS ((Zhong and Ng, 2010) to score the WordNet synsets for each predicate. Next, we take the highest scoring synsets and use the Predicate Matrix (Carreras et al., 2014) to obtain the associated FrameNet frames. The interpretation of the predicates as events for the task is thus derived from the SRL output in combination with the WSD output and the Predicate Matrix association. We call the above output of NewsReader the raw-ECKGs. In the next sections, we describe how we post-process these to derive so-called task-ECKGs with only the information relevant for the task. We finally reason over these taskECKGs to answer t"
S18-1108,W13-1202,1,0.890321,"Missing"
S18-1108,kingsbury-palmer-2002-treebank,0,0.224653,"ble in the Github repository, including the scripts to extract the latter from the former. 2 The NewsReader system NewsReader processes text by applying a wide range of NLP modules, among which named entity recognition, classification and disambiguation (NERCD), semantic role labeling (SRL), word sense disambiguation (WSD), and temporal expressions detection and normalization (TIMEX). The NLP modules store their output as separate layers in the Natural language processing Annotation Format (NAF) (Fokkens et al., 2014). For example, events are detected by the SRL system as PropBank predicates (Kingsbury and Palmer, 2002), while FrameNet frames (Baker, 2008) and Wordnet synsets (Fellbaum, 1998) are attached to these predicates on the basis of the WSD output. Similarly, NERCD will annotate the text with entities and entity classes and it will annotate some of them with DBpedia URIs. From these entities, we derive participant names and locations for the predicates in the SRL output, while TIMEX anchors these predicates to dates. In a second step, NewsReader derives so-called Event-Centric-Knowledge-Graphs (ECKGs) by combining the results of the NLP module. The ECKGs follow the Simple Event Model (SEM) (Van Hage"
S18-1108,S18-1009,1,0.743145,"Missing"
S18-1154,S18-1117,0,0.0975625,"n aiming for high performance, we explore which kind of semantic knowledge is best captured by different methods. The results indicate that WordNet glosses on different levels of the hierarchy capture many attributes relevant for this task. In combination with exploiting word embedding similarities, this source of information yielded our best results. Our best performing system ranked 5th out of 13 final ranks. Our analysis yields insights into the different kinds of attributes represented by different sources of knowledge. 1 Introduction SemEval Task 10 “Capturing Discriminative Attributes” (Krebs et al., 2018) provides participants with triples of words consisting of two concepts and an attribute. The task is to determine whether the attribute is a distinguishing property of the first concept compared to the second concept. This is the case in triple shrimp, spinach, pink, for instance, because shrimp can be pink whereas spinach is usually of a different color. When the first concept does not have a semantic relation with the attribute or both the concepts have the same semantic relation with it, the attribute is considered not to be discriminative. In general, Task 10 can be understood as detectin"
S18-1154,P14-1023,0,0.237153,"Missing"
S18-1154,P14-2050,0,0.216153,"g knowledge and reasoning about attributes we are aware of is an exploratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offs"
S18-1154,W14-1618,0,0.398221,"g knowledge and reasoning about attributes we are aware of is an exploratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offs"
S18-1154,W16-2503,0,0.0629117,"ratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman - man + king. The result should be"
S18-1154,P14-1113,0,0.0212918,"ns contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman - man + king. The result should be closest to the fourth component (queen). Thus, the f"
S18-1154,D17-1193,0,0.0324207,"Missing"
S18-1154,N13-1090,0,0.690436,"evel of concreteness, but might also be distinguished on a more abstract level (e.g. herbs, root, green v.s. apse, nightgown, royal) we exploit the entire WordNet hierarchy. In both of our systems, the second component exploits information encoded in distributional vector representations of words. Word vectors have not only been shown to capture information about semantic similarity and relatedness but, beyond that, seem to encode information about individual components of word meaning that are necessary to solve analogy tasks such as in the famous example man is to woman as king is to queen (Mikolov et al., 2013b). This indicates that the dimensions of the distributional vector representations encode information about specific attributes of words. We experiment with two approaches: a basic approach comparing cosine similarities and an exploratory approach that deducts word vectors from one another to detect meaning differences. Best performance was obtained by the system using cosine similarity. The second approach perThis paper presents the two systems submitted by the meaning space team in Task 10 of the SemEval competition 2018 entitled Capturing discriminative attributes. The systems consist of c"
S18-1154,S16-1202,0,0.0181551,"question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman - man + king. The result should be closest to the fourth component (queen). Thus, the first component for"
S18-1154,N16-2002,0,0.0132031,"about attributes we are aware of is an exploratory study examining what knowledge in definitions contributes to question-answering tasks (Clark et al., 2008). Vector representations of word meaning based on the distribution of words in large corpora do not yield explicit information about specific relations, but implicitly encode all kinds of associations between concepts. In contrast to manually constructed resources, their coverage is much larger. More specifically, they have been shown to encode information relevant in solving analogy tasks (Mikolov et al., 2013a; Levy and Goldberg, 2014b; Gladkova et al., 2016; G´abor et al., 2017; Linzen, 2016) and inferring semantic hierarchies (Fu et al., 2014; Pocostales, 2016). This indicates that the dimensions of distributional representations encode information about attributes of concepts (Levy and Goldberg, 2014b, p.177). For instance, in order to find the fourth component in the analogy man is to woman as king is to queen, a model has to detect the relation holding between the pairs in the analogy. In this example, the relations are formed by the two features of royalty and gender. One way of solving this is to use the vector offsets resulting from woman"
S18-1154,W05-1003,0,0.251487,"epts they apply to and thus appear in proximity to them. In a comparative set-up such as in this task, the attribute should be closer to the concept it applies to. In our second system, we attempt to exploit the operations used for solving analogies in order to determine whether an attribute distinguishes two concepts. forms lower in isolation, but performance is comparable to the first system in combination with the WordNet component. The main insights gained from our experiments are the following. First, despite the limited coverage of information on attributes in WordNet (as pointed out by Poesio and Almuhareb (2005)), the contribution of the WordNet component to the overall results indicates that definitions yield a valuable source of knowledge with respect to discriminative attributes. Second, we analyze how individual systems perform across different types of attributes. Our analysis shows that similarity performs best on general descriptive properties and WordNet definitions help most for finding specific properties. These observations indicate that more sophisticated methods of combining these components could lead to superior results in future work. The remainder of this paper is structured as follo"
S18-1154,P04-1048,0,0.060259,"ore from a cognitive (e.g. McRae et al. (2005)) and computational (e.g. Poesio and Almuhareb (2005)) perspective, this task is, to our knowledge, the first task aiming at detecting discriminative features. We use WordNet (Fellbaum, 1998) as a source of explicitly represented knowledge. Whereas the WordNet structure contains a vast amount of information about lexical relations (hyponymy, synonymy, meronymy), its definitions constitute a resource of world knowledge. WordNet definitions have been used successfully in approaches to word sense disambiguation (Lesk, 1986) and inferring verb frames (Green et al., 2004). The only 3 System description Each of our systems1 consists of a WordNet component and a component exploiting word embed1 Code can be found at https://github.com/ cltl/meaning_space 941 ding vectors. If the WordNet component is unable to classify an example, it is passed on to the word embedding component. After presenting the WordNet component, we describe the two embedding-based systems. The word vectors used in all approaches are taken from the Word2Vec Google News model (Mikolov et al., 2013a).2 The systems are developed using training and validation data and evaluated using test data.3"
segers-vossen-2010-facilitating,bosma-vossen-2010-bootstrapping,1,\N,Missing
segers-vossen-2010-facilitating,vossen-etal-2008-kyoto,1,\N,Missing
segers-vossen-2010-facilitating,W03-1901,0,\N,Missing
van-erp-etal-2014-discovering,vossen-etal-2014-newsreader,1,\N,Missing
van-erp-etal-2014-discovering,agerri-etal-2014-ixa,0,\N,Missing
van-erp-etal-2014-discovering,cybulska-vossen-2014-using,1,\N,Missing
van-erp-etal-2014-discovering,cybulska-vossen-2010-event,1,\N,Missing
van-son-etal-2014-hope,miltsakaki-etal-2004-penn,0,\N,Missing
van-son-etal-2014-hope,N07-2036,0,\N,Missing
van-son-etal-2014-hope,bartalesi-lenzi-etal-2012-cat,0,\N,Missing
vossen-etal-2008-integrating,magnini-cavaglia-2000-integrating,0,\N,Missing
vossen-etal-2008-integrating,fillmore-etal-2004-framenet,0,\N,Missing
vossen-etal-2008-kyoto,W02-1304,1,\N,Missing
vossen-etal-2008-kyoto,W01-0703,1,\N,Missing
vossen-etal-2008-kyoto,magnini-cavaglia-2000-integrating,0,\N,Missing
vossen-etal-2008-kyoto,atserias-etal-2004-towards,1,\N,Missing
vossen-etal-2008-kyoto,soria-etal-2006-moving,1,\N,Missing
vossen-etal-2008-kyoto,chou-huang-2006-hantology,1,\N,Missing
vossen-etal-2008-kyoto,W06-1003,1,\N,Missing
vossen-etal-2012-dutchsemcor,gorog-vossen-2010-computer,1,\N,Missing
vossen-etal-2012-dutchsemcor,S10-1053,0,\N,Missing
vossen-etal-2012-dutchsemcor,N06-1016,0,\N,Missing
vossen-etal-2012-dutchsemcor,D07-1082,0,\N,Missing
vossen-etal-2012-dutchsemcor,S10-1013,1,\N,Missing
vossen-etal-2012-dutchsemcor,E09-1005,0,\N,Missing
vossen-etal-2012-dutchsemcor,H93-1061,0,\N,Missing
vossen-etal-2012-dutchsemcor,J07-4005,0,\N,Missing
vossen-etal-2012-dutchsemcor,oostdijk-etal-2008-coi,0,\N,Missing
vossen-etal-2012-dutchsemcor,P04-1075,0,\N,Missing
vossen-etal-2012-dutchsemcor,P96-1006,0,\N,Missing
vossen-etal-2012-dutchsemcor,W04-0827,1,\N,Missing
vossen-etal-2012-dutchsemcor,vossen-etal-2008-integrating,1,\N,Missing
vossen-etal-2014-newsreader,P98-1013,0,\N,Missing
vossen-etal-2014-newsreader,C98-1013,0,\N,Missing
vossen-etal-2014-newsreader,P10-1143,0,\N,Missing
vossen-etal-2014-newsreader,cattoni-etal-2012-knowledgestore,1,\N,Missing
vossen-etal-2014-newsreader,kipper-etal-2006-extending,0,\N,Missing
vossen-etal-2014-newsreader,W13-1202,1,\N,Missing
W02-1304,W02-1304,1,0.0511956,"Missing"
W02-1304,W00-1702,1,0.907606,"Missing"
W02-1304,W01-0703,1,0.824628,"Missing"
W02-1304,W99-0603,0,0.0765567,"Missing"
W02-1304,W00-1322,1,0.899983,"Missing"
W02-1304,J98-1001,0,0.0564481,"Missing"
W02-1304,J98-1006,0,0.0986902,"Missing"
W02-1304,magnini-cavaglia-2000-integrating,1,0.726135,"Missing"
W02-1304,W00-1326,1,0.886817,"Missing"
W02-1304,P98-2247,0,0.048344,"Missing"
W02-1304,S01-1029,1,0.792942,"Missing"
W02-1304,W97-0201,0,0.436936,"Missing"
W02-1304,S01-1017,1,\N,Missing
W02-1304,W00-1325,0,\N,Missing
W02-1304,P00-1064,0,\N,Missing
W02-1304,W00-0706,1,\N,Missing
W02-1304,P95-1026,0,\N,Missing
W02-1304,W02-0801,1,\N,Missing
W02-1304,C98-2242,0,\N,Missing
W09-2420,C08-1003,1,0.83081,"Missing"
W09-2420,W06-1615,0,0.185914,"Missing"
W09-2420,P07-1007,0,0.123157,"Missing"
W09-2420,W04-3237,0,0.0779774,"Missing"
W09-2420,P07-1033,0,0.148042,"Missing"
W09-2420,W00-1322,0,0.0374768,"Missing"
W09-2420,S01-1004,0,0.385256,"The difficulties found by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Langu"
W09-2420,H05-1053,0,0.676819,"ated words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambiguation. Supervised Word Sense Disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (Escudero et al., 2000; Mart´ınez and Agirre, 2000), and domain adaptation techniques have been proposed as a solution to this problem with mixed results. Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005). This kind of dataset contains hand-labeled examples for a handful of selected target words. As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured. Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007): supervised systems attain results on the high 80’s and beat the most frequent baseline by a large margin for lexical-sample datasets, but results on the all-wo"
W09-2420,W04-0807,0,0.189564,"ound by supervised systems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, includi"
W09-2420,H93-1061,0,0.37412,"cific domain. Good results in this setting would show that supervised domain adaptation is working, and that generic WSD systems can be supplemented with hand-tagged examples from the target domain. There is an additional setting, where a generic WSD system is supplemented with untagged examples from the domain. Good results in this setting would show that semi-supervised domain adaptation works, and that generic WSD systems can be supplemented with untagged examples from the target domain in order to improve their results. Most of current all-words generic supervised WSD systems take SemCor (Miller et al., 1993) as their source corpus, i.e. they are trained on SemCor examples and then applied to new examples. SemCor is the largest publicly available annotated corpus. It’s mainly a subset of the Brown Corpus, plus the novel The Red Badge of Courage. The Brown corpus is balanced, yet not from the general domain, as it comprises 500 documents drawn from different domains, each approximately 2000 words long. Although the Brown corpus is balanced, SemCor is not, as the documents were not chosen at random. 4 State-of-the-art in WSD for specific domains Initial work on domain adaptation for WSD systems show"
W09-2420,P96-1006,0,0.698695,"fer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambiguation. Supervised Word Sense Disambiguation systems trained on general corpora are known to perform worse when applied to specific domains (Escudero et al., 2000; Mart´ınez and Agirre, 2000), and domain adaptation techniques have been proposed as a solution to this problem with mixed results. Current research on applying WSD to specific domains has been evaluated on three available lexicalsample datasets (Ng and Lee, 1996; Weeber et al., 2001; Koeling et al., 2005). This kind of dataset contains hand-labeled examples for a handful of selected target words. As the systems are evaluated on a few words, the actual performance of the systems over complete texts can not be measured. Differences in behavior of WSD systems when applied to lexical-sample and all-words datasets have been observed on previous Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007): supervised systems attain results on the high 80’s and beat the most frequent baseline by a large margin for lexica"
W09-2420,S07-1016,0,0.278613,"ems to adapt might change the way we assess the strengths and weaknesses of supervised and knowledgebased WSD systems. Unfortunately, all existing evaluation datasets for specific domains are lexical-sample corpora. With this paper we want to motivate the creation of an allwords test dataset for WSD on the environment domain in several languages, and present the overall design of this SemEval task. 1 Introduction Word Sense Disambiguation (WSD) competitions have focused on general domain texts, as attested in the last Senseval and Semeval competitions (Kilgarriff, 2001; Mihalcea et al., 2004; Pradhan et al., 2007). Specific domains pose fresh challenges to WSD systems: the context in which the senses occur might change, distributions and predominant senses vary, some words tend to occur in fewer senses in specific domains, and new senses and terms might be involved. Both supervised and knowledge-based systems are affected by these issues: while the first suffer from different context and sense priors, the later suffer from lack of coverage of domain-related words and information. 123 Domain adaptation of supervised techniques is a hot issue in Natural Language Processing, including Word Sense Disambigu"
W09-2420,rose-etal-2002-reuters,0,0.0411856,"d a total of 192,800 occurrences of these words were tagged with WordNet 1.5 senses, more than 1,000 instances per word in average. The examples from BC comprise 78,080 occurrences of word senses, and examples from WSJ consist on 114,794 occurrences. In domain adaptation experiments, the Brown Corpus examples play the role of general corpora, and the examples from the WSJ play the role of domain-specific examples. Koeling et al. (2005) present a corpus were the examples are drawn from the balanced B NC corpus (Leech, 1992) and the S PORTS and F INANCES sections of the newswire Reuters corpus (Rose et al., 2002), comprising around 300 examples (roughly 100 from each of those corpora) for each of the 41 nouns. The nouns were selected because they were 124 salient in either the S PORTS or F INANCES domains, or because they had senses linked to those domains. The occurrences were hand-tagged with the senses from WordNet version 1.7.1 (Fellbaum, 1998). In domain adaptation experiments the B NC examples play the role of general corpora, and the F INANCES and S PORTS examples the role of two specific domain corpora. Finally, a dataset for biomedicine was developed by Weeber et al. (2001), and has been used"
W09-2420,D08-1105,0,0.0322595,"Missing"
W09-2420,W04-0811,0,\N,Missing
W09-2420,vossen-etal-2008-kyoto,1,\N,Missing
W09-2420,E09-1006,1,\N,Missing
W09-2420,E09-1005,1,\N,Missing
W09-2420,E09-1045,0,\N,Missing
W09-2420,W00-0901,0,\N,Missing
W09-2420,W00-1326,1,\N,Missing
W09-2420,J07-4005,0,\N,Missing
W09-2420,W08-2114,0,\N,Missing
W09-2420,S07-1097,0,\N,Missing
W10-3301,alvez-etal-2008-complete,1,\N,Missing
W10-3301,E09-1005,1,\N,Missing
W11-1506,E09-1005,0,0.0141873,"ifically designed to extract events from text. This pipeline-architecture of lin4 For more information about the KYOTO - project (www.kyoto-project.eu) see Vossen et al (2008a). 5 The New Web Portal is part of the National Semantic Web 2.0 (FinnONTO 2.0) project. 6 http://digi.lib.helsinki.fi/sanomalehti/secure/main.html 40 Semantic Tagging of Historical Events Kyoto Annotation Format is described in Bosma et al (2009). http://www.let.rug.nl/vannoord/alp/Alpino/ 9 For word sense disambiguation the UKB system (http://ixa2.si.ehu.es/ukb/) was used. For more information the reader is referred to Agirre & Soroa (2009). 10 For more information see Vossen et al (2008b). 11 For more information see KYOTO deliverable 5.4 at http://www.kyoto-project.eu/. 12 See tools at http://www.kyoto-project.eu/. 13 The development set contains one Wikipedia entry, two educational texts and two newspaper articles written a few years after the Srebrenica massacre happened. 8 locations, time markers, participants and actions there were no Wordnet synsets automatically assigned. No WN-concepts were found for geographical names as Srebrenica or Zagreb. Also person and organization names (Mladic, Dutchbat III, NIOD) and dates wou"
W11-1506,bosma-vossen-2010-bootstrapping,1,0.872182,"Missing"
W11-1506,cybulska-vossen-2010-event,1,0.667204,"Missing"
W11-1506,vossen-etal-2008-kyoto,1,\N,Missing
W11-1702,D08-1083,0,0.0174009,"hat can be modelled in a lexicon. Consider the following example: Ex. (1) … Damilola’s killers were boasting about his murder... This sentence expresses a positive sentiment of the killers towards the fact they murdered Damilola and it expresses the negative attitude on behalf of the speaker/writer who has negative opinion of the the murderers of Damilola. Both attitudes are part of the semantic profile of the verb and should be modelled in a subjectivity lexicon. As opinion mining and sentiment analysis applications tend to utilize more and more the composition of sentences (Moilanen (2007), Choi and Cardie (2008), Jia et al. (2009)) and to use the value and properties of the verbs expressed by its dependency trees, there is a need for specialized lexicons where this information can be found. For the analysis of more complex opinionated text like news, political documents, and (online) debates the identification of the attitude holder and topic are of crucial importance. Applications that exploit the relations between the verb meaning and its arguments can better determine sentiment at sentencelevel and trace emotions and opninions to their holders. Our model seeks to combine the insights from a rather"
W11-1702,esuli-sebastiani-2006-sentiwordnet,0,0.120689,"exploited by tools for deeper sentiment analysis and rich opinion mining, the model is validated by an annotation study of 580 verb lexical units (cf. section 4). 2 Related Work Polarity and subjectivity lexicons are valuable resources for sentiment analysis and opinion mining. For English, a couple of smaller and larger lexicons are available. Widely used in sentiment analysis are automatically derived or manually built polarity lexicons. These lexicons are lists of words (for example, Hatzivassiloglou and McKeown (1997), Kamps et al. (2004), Kim and Hovy (2004) or word senses (for example, Esuli and Sebastiani (2006), Wiebe and Mihalcea (2006), Su and Markert, (2008)) annotated for negative or positive polarity. As they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf. example 1) which carry both negative and positive polarity dependening on who is the attitude holder. Strapparava and Valitutti (2004) developed Wordnet-Affect, an affective extension of Wordnet. It describes ‘direct’ affective words, i.e. words which denote emotions. Synsets are classified into categories like emotion, cognitive state, trait, behaviou"
W11-1702,P97-1023,0,0.347842,"pening are conveyed in the text. As we wish to provide a model for a lexicon that is operational and can be exploited by tools for deeper sentiment analysis and rich opinion mining, the model is validated by an annotation study of 580 verb lexical units (cf. section 4). 2 Related Work Polarity and subjectivity lexicons are valuable resources for sentiment analysis and opinion mining. For English, a couple of smaller and larger lexicons are available. Widely used in sentiment analysis are automatically derived or manually built polarity lexicons. These lexicons are lists of words (for example, Hatzivassiloglou and McKeown (1997), Kamps et al. (2004), Kim and Hovy (2004) or word senses (for example, Esuli and Sebastiani (2006), Wiebe and Mihalcea (2006), Su and Markert, (2008)) annotated for negative or positive polarity. As they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf. example 1) which carry both negative and positive polarity dependening on who is the attitude holder. Strapparava and Valitutti (2004) developed Wordnet-Affect, an affective extension of Wordnet. It describes ‘direct’ affective words, i.e. words which den"
W11-1702,kamps-etal-2004-using,0,0.237082,"Missing"
W11-1702,C04-1200,0,0.0459346,"odel for a lexicon that is operational and can be exploited by tools for deeper sentiment analysis and rich opinion mining, the model is validated by an annotation study of 580 verb lexical units (cf. section 4). 2 Related Work Polarity and subjectivity lexicons are valuable resources for sentiment analysis and opinion mining. For English, a couple of smaller and larger lexicons are available. Widely used in sentiment analysis are automatically derived or manually built polarity lexicons. These lexicons are lists of words (for example, Hatzivassiloglou and McKeown (1997), Kamps et al. (2004), Kim and Hovy (2004) or word senses (for example, Esuli and Sebastiani (2006), Wiebe and Mihalcea (2006), Su and Markert, (2008)) annotated for negative or positive polarity. As they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf. example 1) which carry both negative and positive polarity dependening on who is the attitude holder. Strapparava and Valitutti (2004) developed Wordnet-Affect, an affective extension of Wordnet. It describes ‘direct’ affective words, i.e. words which denote emotions. Synsets are classified into"
W11-1702,W06-0301,0,0.105749,"negative attitude. Their model seems to focus on the cognitive aspects of emotion whereas we aim to also model the linguistic aspects by including specifically the attitude of the Speaker/Writer in our model. Moreover, our description is not at the level of the synset but at lexical unit level which enables us to differentiate gradations of the strength of emotions within the synsets. This enables us to relate the attitudes directly to the syntactic-semantic patterns of the lexical unit. Also Framenet (Ruppenhofer et al. (2010)) is used as a resource in opinion mining and sentiment analysis (Kim and Hovy (2006)). Framenet (FN) is an online lexical resource for English that contains more than 11,600 lexical units. The aim is to classify words into categories (frames) which give for each lexical unit the range of semantic and syntactic combinatory possibilities. The semantic roles range from general ones like Agent, Patient and Theme to specific ones such as Speaker, Message and Addressee for Verbs of Communication. FN includes frames such as Communication, Judgment, Opinion, Emotion_Directed and semantic roles such as Judge, Experiencer, Communicator which are highly relevant for opinion mining and s"
W11-1702,strapparava-valitutti-2004-wordnet,0,0.0673015,"alysis are automatically derived or manually built polarity lexicons. These lexicons are lists of words (for example, Hatzivassiloglou and McKeown (1997), Kamps et al. (2004), Kim and Hovy (2004) or word senses (for example, Esuli and Sebastiani (2006), Wiebe and Mihalcea (2006), Su and Markert, (2008)) annotated for negative or positive polarity. As they attribute single polarity values (positive, negative, neutral) to words they are not able to account for more complex cases like boast (cf. example 1) which carry both negative and positive polarity dependening on who is the attitude holder. Strapparava and Valitutti (2004) developed Wordnet-Affect, an affective extension of Wordnet. It describes ‘direct’ affective words, i.e. words which denote emotions. Synsets are classified into categories like emotion, cognitive state, trait, behaviour, attitude and feeling. The resource is further developed (Valittutti and Strapparava, 2010) by adding the descriptions of ‘indirect’ affective words according to a specific appraisal model of emotions (OCC). An indirect affective word indirectly refers to emotion categories and can refer to different possible emotions according to the subjects (actor, actee and observer) sema"
W11-1702,P06-1134,0,0.624511,"Missing"
W11-1702,E06-1027,0,\N,Missing
W11-1702,W08-1207,0,\N,Missing
W11-1702,R09-1048,0,\N,Missing
W13-1202,P10-1143,0,0.0191435,"on of the conflict, which enables use cases that require the analysis of the conflict itself. 5 The GAF Annotation Framework This section explains the basic idea behind GAF by using texts on earthquakes in Indonesia. GAF provides a general model for event representation (including textual and extra-textual mentions) as well as exact representation of linguistic annotation or output of NLP tools. Simply put, GAF is the combination of textual analyses and formal semantic representations in RDF. 5.1 A SEM for earthquakes We selected newspaper texts on the January 2009 West Papua earthquakes from Bejan and Harabagiu (2010) to illustrate GAF. This choice was made because the topic “earthquake” illustrates the advantage of sharing URIs across domains. Gao and Hunter (2011) propose a Linked Data model to capture major geological events such as earthquakes, volcano activity and tsunamis. They combine information from different seismological databases with the intention to provide more complete information 15 to experts which may help to predict the occurrence of such events. The information can also be used in text interpretation. We can verify whether interpretations by NLP tools correspond to the data and relatio"
W13-1202,P11-1098,0,0.0122162,"Missing"
W13-1202,C96-1079,0,0.0462667,"ents, temporal relations, etc.) are represented explicitly in the semantic layer. The remainder of this paper is structured as follows. In Section 2, we present related work and explain the motivation behind our approach. Section 3 describes the in-text annotation approach. Our semantic annotation layer is presented in Section 4. Sections 5-7 present GAF through a use case on earthquakes in Indonesia. This is followed by our conclusions and future work in section 8. 2 Motivation and Background Annotation of events and of relations between them has a long tradition in NLP. The MUC conferences (Grishman and Sundheim, 1996) in the 90s did not explicitly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic"
W13-1202,P12-2045,0,0.0756526,"Missing"
W13-1202,J05-1004,0,0.020106,"tly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic corpora, such as Propbank (Palmer et al., 2005) and the Framenet corpus (Baker et al., 2003) have been built according to linguistic principles. The annotations aim at properly representing verb structures within a sentence context, focusing on verb arguments, semantic roles and other elements. In ACE 2004 (Linguistic Data Consortium, 2004b), event detection and linking is included as a pilot task for the first time, inspired by annotation schemes developed for named entities. They distinguish between event mentions and the trigger event, which is the mention that most clearly expresses its occurrence (Linguistic Data Consortium, 2004a). T"
W13-1202,pustejovsky-etal-2010-iso,0,0.0205156,"between instances and instance mentions avoiding the problem of determining a trigger event. Additionally, it facilitates the integration of information from extra-textual sources and information that can be inferred from texts, but is not explicitly mentioned. Sections 5 to 7 will explain how we can achieve this with GAF. 3 The TERENCE annotation format The TERENCE Annotation Format (TAF) is defined within the TERENCE Project1 with the goal to include event mentions, temporal expressions and participant mentions in a single annotation protocol (Moens et al., 2011). TAF is based on ISOTimeML (Pustejovsky et al., 2010), but introduces several adaptations in order to fit the domain of children’s stories for which it was originally developed. The format has been used to annotate around 30 children stories in Italian and 10 in English. We selected TAF as the basis for our in-text annotation for three reasons. First, it incorporates the (in our opinion crucial) distinction between instances and instance mentions. Second, it adapts some consolidated paradigms for linguistic annotation such as TimeML for events and temporal expressions and ACE for participants and participant mentions (Linguistic Data Consortium,"
W13-1202,setzer-gaizauskas-2000-annotating,0,0.0651309,"4. Sections 5-7 present GAF through a use case on earthquakes in Indonesia. This is followed by our conclusions and future work in section 8. 2 Motivation and Background Annotation of events and of relations between them has a long tradition in NLP. The MUC conferences (Grishman and Sundheim, 1996) in the 90s did not explicitly annotate events and coreference relations, but the templates used for evaluating the information extraction tasks indirectly can be seen as annotation of events represented in newswires. Such events are not ordered in time or further related to each other. In response, Setzer and Gaizauskas (2000) describe an annotation framework to create coherent temporal orderings of events represented in documents using closure rules. They suggest that reasoning with text independent models, such as a calendar, helps annotating textual representations. More recently, generic corpora, such as Propbank (Palmer et al., 2005) and the Framenet corpus (Baker et al., 2003) have been built according to linguistic principles. The annotations aim at properly representing verb structures within a sentence context, focusing on verb arguments, semantic roles and other elements. In ACE 2004 (Linguistic Data Cons"
W13-1202,W11-0116,0,\N,Missing
W13-1202,bartalesi-lenzi-etal-2012-cat,1,\N,Missing
W14-0118,P13-1166,1,0.8471,"Missing"
W14-0118,I05-1067,0,0.0263661,"ly proficient in English, were asked to translate the datasets. They were asked not to use multi-word expressions. They were asked to take into account the relatedness within a word pair for disambiguation. In addition, they were allowed to use so-called replacement words to overcome slang or if words were culturally dependent. They then asked 5 participants to rate the Spanish word pairs. A sixth person evaluated the translation. Because of the fact that the Pearson correlation with the original datasets was 0.86, only one translator translated the datasets into Arabic and Romanian. Finally, Gurevych (2005) translated the datasets into German. However, no instructions, as to how it was done, were provided. 3 Dutch gold standard We would like to see whether the similarity intuitions of Dutch speakers are the same as the English speakers. We also want to known if the Dutch wordnet Cornetto, which was built independently of the English WordNet, would perform in the same way as the English WordNet using the same similarity measures and against a comparable gold standard. For that, we need to create a Dutch gold standard. We opted to translate the gold standards by Rubenstein & Goodenough (65 word pa"
W14-0118,D09-1124,0,0.218031,"rder co-occurence Point-wise mutual information for which the Google n-gram corpus was used. They then compared the output from the similarity measures to the language specific gold standards and to the original scores collected by Rubenstein & Goodenough. The difference between these correlations was relatively small, which is why they claim that it is possible to use the original scores from the English gold standard in other languages. Besides Joubarne and Inkpen (2011), other studies have made an effort to translate the original datasets by Rubenstein & Goodenough and by Miller & Charles. Hassan and Mihalcea (2009) translated these datasets into Spanish, Arabic, and Romanian. For Spanish, native speakers, who were highly proficient in English, were asked to translate the datasets. They were asked not to use multi-word expressions. They were asked to take into account the relatedness within a word pair for disambiguation. In addition, they were allowed to use so-called replacement words to overcome slang or if words were culturally dependent. They then asked 5 participants to rate the Spanish word pairs. A sixth person evaluated the translation. Because of the fact that the Pearson correlation with the o"
W14-0118,O97-1002,0,0.068043,"therefore the intuitions of English and Dutch native speakers, appear to be highly compatible. We also show that our package generates similar results for English as reported earlier and good results for Dutch. To the contrary of what we expected, some measures even perform better in Dutch than English. 1 Introduction Various methods have been proposed in the past for measuring similarity between words using Princeton WordNet (Fellbaum, 1998). Some of these methods (path (Rada et al., 1989), lch (Leacock and Chodorow, 1998), wup (Wu and Palmer, 1994), res (Resnik, 1995), lin (Lin, 1998), jcn (Jiang and Conrath, 1997), among others) were Piek Vossen VU University Amsterdam Amsterdam, Netherlands piek.vossen@vu.nl implemented in the WordNet::Similarity package (Pedersen et al., 2004). WordNet::Similarity 1 has become an important instrument for measuring similarity between any set of words in a language but also for testing the performance of wordnet as a database of synonymy and semantic relations. The toolkit was used to evaluate the different measures against a gold standard of English words created by Rubenstein and Goodenough (1965) and Miller and Charles (1991). The evaluation results tell us somethin"
W14-0118,N10-1047,0,0.0502241,"Missing"
W14-0118,vossen-etal-2012-dutchsemcor,1,0.890566,"Missing"
W14-0118,H93-1061,0,0.701433,"Missing"
W14-0118,W06-2501,0,0.0188741,"r. Words and synsets that have other relations than synonymy and hyponymy respectively, e.g. part-whole or causal relations, are most likely not similar but strongly related. This difference is dubbed the ‘tennisphenomenon’ in Fellbaum (1998) : where tennis ball, player, racket and game are closely related but all very different things. Since WordNet dominantly consists of synonymy and hyponymy relations, it more naturally reflects similarity than relatedness. Since the first release of WordNet, researchers have tried to use it to simulate similarity. Except for the lesk (Lesk, 1986), vector (Patwardhan and Pedersen, 2006), and vector pairs (Patwardhan and Pedersen, 2006) algorithms, these measures are all based on synonymy and hyponymy. Another approach to measure similarity across different languages is described by Joubarne and Inkpen (2011). The aim of their paper is to show that it might be possible to use the scores from the English gold standards in other languages, hence making it unnecessary to create gold standards with human-assigned judgements in every single language. In order to show this, they used an existing gold standard for German, which is a translation of the gold standard by Rubenstein & G"
W14-0118,P94-1019,0,\N,Missing
W14-0118,oostdijk-etal-2008-coi,0,\N,Missing
W15-0801,P10-1143,0,0.777161,"or more information see Cybulska and Vossen (2014a)). An event template can be filled at different levels of information such as the entire document, a paragraph or a sentence. The approach investigated in this study operates at the sentence level which means that event templates are filled only with information available in the sentence in which an event mention occurs (for a report on experiments with a two step approach first considering document and subsequently sentence templates, see Cybulska and Vossen (2015)). Figure 1 considers an excerpt from topic one, text seven of the ECB corpus (Bejan and Harabagiu, 2010). Table 1 shows the distribution of Proceedings of the 3rd Workshop on EVENTS at the NAACL-HLT 2015, pages 1–10, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics Event Slot Action Time Location Human Participant Non-human Participant Sentence Template 1 entered N/A Promises actress N/A Sentence Template 2 headed on Tuesday to a Malibu treatment facility actress N/A Table 1: Sentence templates ECB topic1, text 7, sentences 1 and 2. The “American Pie” actress has entered Promises for undisclosed reasons. The actress, 33, reportedly headed to a Malibu treatment fac"
W15-0801,R13-1021,1,0.879676,"Missing"
W15-0801,cybulska-vossen-2014-using,1,0.919002,"ted to but will probably not fully corefer with a “lower level” event of shorter duration and with single participants involved (e.g. A Russian soldier has shot dead a Ukrainian naval officer). We experiment with an “event template” approach to event coreference resolution. The way in which event information can be semantically categorized is used in an event template to shape comparison of information about two event descriptions. Coreference between mentions of two events is determined through compatibility of slots of a pair of event templates. For the experiments, we use the ECB+ dataset (Cybulska and Vossen, 2014b). The five slots in our event template correspond to different elements of event information as annotated in the ECB+. The considered event slots are: 1) event action that is the event trigger (following the ACE (LDC, 2005) terminology) and four kinds of event arguments: 2) time, 3) location, 4) human and 5) non-human participant slots (for more information see Cybulska and Vossen (2014a)). An event template can be filled at different levels of information such as the entire document, a paragraph or a sentence. The approach investigated in this study operates at the sentence level which mean"
W15-0801,W11-0116,0,0.178732,"rtedly headed to a Malibu treatment facility on Tuesday. Figure 1: Topic 1, text 7, ECB (Bejan and Harabagiu, 2010). event information over the five event slots (as annotated in the ECB+) in the two example sentences. In the event template approach different kinds of event information are contrasted per slot of the template (Table 3). We determine the grain-size within slots of the event template. The idea is to represent the grain size of the event action as well as of the entities involved with it by means of granularity features. To capture granularity we employ durations of event actions (Gusev et al., 2011) and granularity levels of event participants, time and locations. To determine granularity levels, a new granularity ontology consisting of 15 semantic classes is used. The 15 predefined semantic classes represent different granularity levels, which are defined over 434 hypernyms in WordNet, covering 11979 WordNet synsets. We make the granularity ontology available for research. This work sheds light on the task of crossdocument resolution of coreference between mentions of events in text. This study explores the actual task of resolution of coreference between two event descriptions, without"
W15-0801,S12-1006,0,0.0692753,"Missing"
W15-0801,W97-1311,0,0.734497,"Missing"
W15-0801,D12-1045,0,0.510713,"Missing"
W15-0801,H05-1004,0,0.853972,"Missing"
W15-0801,W11-0143,0,0.0254041,"8” second,”second 1,sec 1” min,”quarter 4” hr,”hours 2” day,”day of the week 1” week,”week 3,calendar week 1” month,”Gregorian calendar month 1” season,”season 1” year,”year 1” thousands years,”Bronze Age 1” street,”government building 1” city,”city district 1” country,”Upper Egypt 1” continent,”East Africa 1” Figure 2: Example entries from the granularity ontology file. tions. Change in granularity was considered as a special case of abstraction in which elements, which are indistinguishable in a particular context, are collapsed. Mani focused on grain-size shifts amongst polysemous events. Mulkar-Mehta et al. (2011b) describe event granularity as the concept of breaking down a higherlevel event into smaller parts, fine-grained events such that each smaller granule plays a part in the higher level whole. Relation types that can exist between the objects at coarse and fine granularity are part-whole relationships amongst entities and events, and causal relationships. Based on annotation of granularity relations in text, the authors conclude that part-whole and causal relations are a good indication of shifts in granularity. In this study we focus on the notion of granularity in event descriptions. We pres"
W15-0801,W11-1901,0,0.0971466,"Missing"
W15-0801,M95-1005,0,0.269575,"Missing"
W15-0814,P98-1013,0,0.371456,"ressions, where the elements in the layers point to spans of terms. In the next examples, we show in NAF entities, a SR structure with a predicate and several of its roles, and a time expression for an English text. Each of the elements has a span element pointing to term identifiers that mark words and phrases in the text. We see in the first structure that the expression United States is detected as a named entity of the type LOCATION 111 and is disambiguated to a DBpedia entry.3 The SR element consists of a predicate and roles, where the predicate has references to various FrameNet frames (Baker et al., 1998) and WordNet synsets (Fellbaum, 1998) along with the predicate information included in the Predicate Matrix (Lacalle et al., 2014). The roles have a PropBank role (Palmer et al., 2005) and possibly one or more FrameNet elements.4 Finally, the time expression Monday has been normalised by reference to a particular date. <entity id=&quot;e3&quot; type=&quot;LOCATION&quot;> <!--United States--> <span><target id=&quot;t28&quot;/><target id=&quot;t29&quot;/></span> <externalReferences> <externalRef confidence=&quot;0.94&quot; reference=&quot;http://dbpedia.org/resource/United_States reftype=&quot;en&quot; resource=&quot;spotlight_v1&quot;/> </externalReferences> </entity>"
W15-0814,P10-1143,0,0.0188894,"tools, as shown by user evaluations (Hellmann et al., 2013). Because linguistic annotations are linked to strings it is furthermore not practical for representing hierarchical structures. (Fokkens et al., 2014) presents a more detailed discussion of the formal representations of linguistic annotations. Besides the formal representation of NLP output, our work relates to the representation of events and cross-document and cross-lingual event coreference. Cross-document event coreference so far has been addressed as a task, in which event markables are related to each other as coreference sets (Bejan and Harabagiu, 2010; Lee et al., 2012). For instance, the ECB corpus represents events and coreference relations using inline annotations in text and crossdocument identifiers with offset references. Representation and evaluation of cross-document eventcoreference is often done using scorers that use the CONLL-2011 format for expressing coreference (Pradhan et al., 2011). This format also exploits a simple token representation and identifiers. To the best of our knowledge, nobody really addressed the semantic representation of events as instances, exploiting interoperable semantic representations of event instan"
W15-0814,W13-1202,1,0.897189,"Missing"
W15-0814,lopez-de-lacalle-etal-2014-predicate,1,0.895255,"Missing"
W15-0814,D12-1045,0,0.0211208,"luations (Hellmann et al., 2013). Because linguistic annotations are linked to strings it is furthermore not practical for representing hierarchical structures. (Fokkens et al., 2014) presents a more detailed discussion of the formal representations of linguistic annotations. Besides the formal representation of NLP output, our work relates to the representation of events and cross-document and cross-lingual event coreference. Cross-document event coreference so far has been addressed as a task, in which event markables are related to each other as coreference sets (Bejan and Harabagiu, 2010; Lee et al., 2012). For instance, the ECB corpus represents events and coreference relations using inline annotations in text and crossdocument identifiers with offset references. Representation and evaluation of cross-document eventcoreference is often done using scorers that use the CONLL-2011 format for expressing coreference (Pradhan et al., 2011). This format also exploits a simple token representation and identifiers. To the best of our knowledge, nobody really addressed the semantic representation of events as instances, exploiting interoperable semantic representations of event instances and entity inst"
W15-0814,J05-1004,0,0.0331553,"expression for an English text. Each of the elements has a span element pointing to term identifiers that mark words and phrases in the text. We see in the first structure that the expression United States is detected as a named entity of the type LOCATION 111 and is disambiguated to a DBpedia entry.3 The SR element consists of a predicate and roles, where the predicate has references to various FrameNet frames (Baker et al., 1998) and WordNet synsets (Fellbaum, 1998) along with the predicate information included in the Predicate Matrix (Lacalle et al., 2014). The roles have a PropBank role (Palmer et al., 2005) and possibly one or more FrameNet elements.4 Finally, the time expression Monday has been normalised by reference to a particular date. <entity id=&quot;e3&quot; type=&quot;LOCATION&quot;> <!--United States--> <span><target id=&quot;t28&quot;/><target id=&quot;t29&quot;/></span> <externalReferences> <externalRef confidence=&quot;0.94&quot; reference=&quot;http://dbpedia.org/resource/United_States reftype=&quot;en&quot; resource=&quot;spotlight_v1&quot;/> </externalReferences> </entity> <predicate id=&quot;pr5&quot;> <!--flying--> <externalReferences> <externalRef reference=&quot;fn:Bringing&quot;, &quot;fn:Motion&quot;, &quot;fn:Operate_vehicle&quot;, &quot;fn:Ride_vehicle&quot;, &quot;fn:Self_motion&quot;, &quot;wn:ili-30-014518"
W15-0814,W11-1901,0,0.0568404,"Missing"
W15-4507,P98-1013,0,0.0199579,"lines The timeline extraction is obtained from an NLP pipeline that has been developed in the NewsReader project4 . The pipeline applies a cascade of modules, ranging from tokenization up to temporal and causal relation extraction, to documents (mention level). Next, it generates a semantic representation of the content in SEM (instance level). The NLP modules generate representations of entities mentioned in the text with possible links to DBpedia URIs, time expressions normalized to dates and a semantic role representation with events and participants linked to FrameNet frames and elements (Baker et al., 1998). Furthermore, coreference relations are created to bind participants and events to instances within each document. The NLP modules interpret mentions in the text, i.e. at single document level. However, given a set of documents or a corpus, these mention based representations are combined resolving cross-document coreference for entities and events, anchoring events to time and aggregating event-participant relations and generating an instance level representation. Details about this process can be found in (Agerri et al., 2014). The timeline representation anchors events either to a time anc"
W15-4507,P09-1068,0,0.221452,"t which distinguishes these works concerns the type of datasets, i.e., fictitious or news documents, used or referred to for the storyline generation or modelization. Although such differences are less relevant for the development of models, they are important for the development of systems. Furthermore, the task of storyline extraction is multidisciplinary, concerning different fields such as Multi Document Summarization, Temporal Processing, Topic Detection and Tracking. What follows is a selection of previous works which we consider more strictly related to our work. Chambers and Jurafsky (Chambers and Jurafsky, 2009) extended previous work on the identification of “event narrative chains”, i.e., sets of partially ordered events that involve the same shared participant. They propose an unsupervised method to learn narrative schemas, i.e. coherent sequences of events whose arguments are filled with participants’ semantic roles. The approach can be applied to all text types. The validity of the extracted narrative schemas (event and associated participants) have been evaluated against FrameNet and on a narrative cloze task: a variation of the cloze task defined by (Taylor, 1953). The narrative schema propose"
W15-4507,D13-1127,0,0.0291879,"must be based, at least, on two aspects: informativeness and interest. A good storyline is a storyline which interest the user, provides all relevant and necessary information with respect to a target entity, and it is coherent. We envisage two types of evaluation: direct and indirect. Direct evaluation necessarily needs human interaction. This can be achieved in two methods: using experts and using crowdsourcing techniques. Experts can evaluate the data provided with the storylines with respect to a set of reference documents and check the informativeness and coherence parameters. Following (Xu et al., 2013), two types of questions can be addressed at the microlevel and at the macro-level of knowledge. Both evaluation types address the quality of the generated storylines. The former addresses the efficiency of the storylines in retrieving the information while the latter addresses the quality of the storylines with respect to a certain topic (e.g. the commercial “war” between Boeing and Airbus). Concerning metrics, micro-knowledge can be measured by the time the users need to gather the information, while the macro-knowledge can be measured as the text proportion, i.e. how many sentences of the s"
W15-4507,D13-1068,0,0.0431174,"orpus and then validate if the identified events correlate with the climax events of the storylines. Indirect evaluation can be based on a crossdocument Summarization tasks. The ideal situation is the one in which the storyline contains the most salient and related events. These sets of data can be used either to recover the sentences in a collection of documents and generate an extractive summary (story) or used to produce an abstractive summary. Summarization measures such as ROUGE can then be used to evaluate the quality of summaries and, indirectly, of the storylines (Nguyen et al., 2014; Huang and Huang, 2013; Erkan and Radev, 2004). pants, e.g. Airbus and Airbus 380 are not considered as standing in a co-participation relation by our system because they have different URIs. In other cases, we see more or less the opposite: a storyline reporting on journeys by Boeing is interrupted by a plane crash from Airbus due to overgenerated bridging relations. What is the optimal combination of features still needs to be determined empirically. For this we need a data set, which we will discuss in the next subsection. 4.3 Benchmarking and evaluation In this phase we are not yet able to provide an extensive"
W15-4507,P98-2127,0,0.205584,"Missing"
W15-4507,P09-1025,0,0.00822154,"Missing"
W15-4507,C98-1013,0,\N,Missing
W15-4507,S15-2132,0,\N,Missing
W15-4507,C98-2122,0,\N,Missing
W15-4507,C14-1114,0,\N,Missing
W16-2819,bartalesi-lenzi-etal-2012-cat,0,0.0180456,". 169 sentences (n) 150 Round 1 139 91 100 Round 2 83 42 47 50 42 43 32 14 0 1 Task 1: Pilot annotation 2 3 4 annotators (n) 5 Figure 2: Distribution of annotations. This section reports on a pilot annotation experiment targeted at the first subtask. Five expert annotators were asked to identify those sentences in the editorial article that were COMMENTED UPON in the comment. A set of eight editorial articles (152 unique sentences, including titles) and a total of 62 comments were provided. In total, this came down to 1,186 sentences to be annotated. We used the Content Annotation Tool (CAT) (Lenzi et al., 2012) for the annotations. The experiment was performed in two rounds. First, simple instructions were given to the annotators to explore the data and task. For the second round, the instructions were refined by adding two simple rules: exclude titles (they are part of the meta-data), and include cases where a proposition is simply ‘mentioned’ rather than functioning as part of the argumentation. For example, the fact that the closing of Sweet Briar College is repeated in the comment below without its factual status beA deeper analysis of the annotated data and the annotation distribution shows the"
W16-2819,W10-0214,0,0.0249339,"itly mentioned or at least supposed opponent, as for instance in the rebutting of possible objections.” Therefore, these (implicit) interactions between participants should be given a central role when performing argument mining. In recent years, several studies have addressed the annotation and automatic classification of agreement and disagreement in online debates. The main difference between them is the annotation unit they have targeted, i.e. the textual units that are in (dis)agreement. Some studies focused on global (dis)agreement, i.e. the overall stance towards the main debate topic (Somasundaran and Wiebe, 2010). Other studies focused on local (dis)agreement, comparing pairs of posts (Walker 1 http://argmining2016.arg.tech/index. php/home/call-for-papers/unshared-task 160 Proceedings of the 3rd Workshop on Argument Mining, pages 160–165, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics ernment to reduce teen pregnancies?) and article title describing the author’s stance (e.g. Publicly Funded Birth Control Is Crucial); and ii) Discussions (i.e. collections of comments from different users) about these editorial articles (Variant D). The remainder of this paper is st"
W16-2819,L16-1187,1,0.846655,"Missing"
W16-2819,walker-etal-2012-corpus,0,0.0611907,"Missing"
W16-2819,W14-2617,0,0.0324304,"Missing"
W16-2819,W12-3710,0,0.0423338,"Missing"
W16-2819,andreas-etal-2012-annotating,0,\N,Missing
W16-5708,P15-2137,0,0.0160943,"ifficult challenges for the selection and extraction of relevant information. Relevant information can be missed in this vast amount of data, leading to inconsistencies, fragmented reports, or gaps in the extraction and representation of complex stories. Different solutions have been proposed to deal with this problem ranging from the generation of multi-document extractive summaries (Barzilay et al., 1999), to clustering of news with respect to a topic (Swan and Allan, 2000), to the generation of timelines to monitor relevant events in a topic (Shahaf and Guestrin, 2010; Nguyen et al., 2014; Bauer and Teufel, 2015). In this work, we want to expand on a different approach to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have i"
W16-5708,P14-2082,0,0.0389167,"limiting inferences. As such, no temporal relation should be annotated on the basis of world knowledge only. 1 https://goo.gl/iWUCFr Furthermore, the set of temporal values has been limited to 8 values (BEFORE, AFTER, OVERLAP, BEFORE OVERLAP, BEGINS ON, ENDS ON, SIMULTANEOUS, INCLUDES). We also annotate temporal relations between events and the Document Creation Time (DCT). The DCT represents a special temporal anchor for actions which expresses a broad temporal dimension (e.g. Present, Past, or Future with respect the time the author created and published the text). Following the proposal in Cassidy et al. (2014), we also annotate transitive closure relations between pairs of events to develop highly connected event graph. This means that in case of two pairs of events A BEFORE B and B BEFORE C, we explicitly mark the transitive closure relation A BEFORE C. Finally, we extend the TLINK tag with the attribute contextualModality, from the RED scheme. It has 4 values: ACTUAL, UNCERTAIN, HYPOTHETICAL, and GENERIC. The attribute allows to represent claims of different sources concerning the reality or certainty of a temporal relation. The assignment of the contextual modality values is connected to the fac"
W16-5708,P09-1068,0,0.0230816,"Missing"
W16-5708,cybulska-vossen-2014-using,1,0.831487,"oints (settings); • the anchoring of events to time and their ordering (a timeline); • bridging relations: a set of relations between events with explanatory and predictive 68 value(s). The proposed annotation scheme aims at grounding these concepts to linguistic elements in document collections. The scheme has been developed to maximize compatibility with existing annotation efforts on event and temporal processing, such as the Richer Event Description (RED) 1 , THYME (Styler IV et al., 2014), and TimeML (Pustejovsky et al., 2003a), and event coreference, such as the Event CorefBank+ (ECB+) (Cybulska and Vossen, 2014b). 2.1 STaR: The Storyline Annotation and Representation Scheme The Storyline Annotation and Representation Scheme (StaR) builds on and extends the ECB+ annotation scheme (Cybulska and Vossen, 2014a). The ECB+ scheme addresses event coreference both at the in- and cross-document levels. Event action coreference is specified as two action mentions which occur/hold true: i.) at the same time; ii.) in the same location; and iii.) with the same actors/participants. Thus, ECB+ data provides access to the first basic elements of the storyline model, i.e., events, participants (actors), locations, a"
W16-5708,D13-1068,0,0.0123716,"h to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have identified four main contributions (Shahaf et al., 2013; Huang and Huang, 2013; Hu et al., 2014; Laparra et al., 2015) in this area proposing methods to generate storyline datasets. Although each contribution proposes its own definition of storyline, based on the sharing of participants, time and location, one of the commonalities of these works consists of the use of interactions and connections between crossdocument topic threads or events which give rise to timelines, i.e. a basic temporal ordering. Storylines also differ from Narrative Schemas (Chambers and Jurafsky, 2009). Narrative Schemas qualify as sets of partially ordered events with no distinction in relevanc"
W16-5708,W15-4508,0,0.0149152,"evant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have identified four main contributions (Shahaf et al., 2013; Huang and Huang, 2013; Hu et al., 2014; Laparra et al., 2015) in this area proposing methods to generate storyline datasets. Although each contribution proposes its own definition of storyline, based on the sharing of participants, time and location, one of the commonalities of these works consists of the use of interactions and connections between crossdocument topic threads or events which give rise to timelines, i.e. a basic temporal ordering. Storylines also differ from Narrative Schemas (Chambers and Jurafsky, 2009). Narrative Schemas qualify as sets of partially ordered events with no distinction in relevance or salience 67 Proceedings of 2nd Work"
W16-5708,C14-1114,0,0.0193278,"ng every day posing difficult challenges for the selection and extraction of relevant information. Relevant information can be missed in this vast amount of data, leading to inconsistencies, fragmented reports, or gaps in the extraction and representation of complex stories. Different solutions have been proposed to deal with this problem ranging from the generation of multi-document extractive summaries (Barzilay et al., 1999), to clustering of news with respect to a topic (Swan and Allan, 2000), to the generation of timelines to monitor relevant events in a topic (Shahaf and Guestrin, 2010; Nguyen et al., 2014; Bauer and Teufel, 2015). In this work, we want to expand on a different approach to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolut"
W16-5708,P16-1028,0,0.0125321,"lience 67 Proceedings of 2nd Workshop on Computing News Storylines, pages 67–72, c Austin, TX, November 5, 2016. 2016 Association for Computational Linguistics of their elements and, most importantly, with no explanatory power of the ways events are connected together, except for precedence relations. A Narrative Schema looks like an un-prioritized set of events which share some participants, thus leading to the development of entity-centric timelines. Furthermore, the use of entity driven relations (e.g. co-participation) to generate the schemas often result in non-coherent chains of events (Peng and Roth, 2016). The remainder of this paper will be structured as follows: in Section 2 we will present the main aspects of the storyline model described in (Vossen et al., 2015) and show how these elements have been used to develop a proposal to annotate storylines. Section 3 will report on the preliminary application of the annotation scheme to a corpus presenting insights on the data and interaction between different layers of annotation ranging from event coreference to storyline. Finally, conclusions and future work will be reported in Section 4. 2 Annotating Storylines: A Proposal The model described"
W16-5708,W15-4507,1,0.934657,"ion and representation of complex stories. Different solutions have been proposed to deal with this problem ranging from the generation of multi-document extractive summaries (Barzilay et al., 1999), to clustering of news with respect to a topic (Swan and Allan, 2000), to the generation of timelines to monitor relevant events in a topic (Shahaf and Guestrin, 2010; Nguyen et al., 2014; Bauer and Teufel, 2015). In this work, we want to expand on a different approach to select, organize, and represent relevant information from collections of documents clustered around a specific topic. Following Vossen et al. (2015), we adopt the storyline model as a representational device to structure the information, and Previous work in storyline generation is limited and in most cases what is labelled as a storyline is a timeline. The main difference is that storylines and narrative structures exhibit some causal and explanatory relation between events and some tension towards a resolution, or climax. We have identified four main contributions (Shahaf et al., 2013; Huang and Huang, 2013; Hu et al., 2014; Laparra et al., 2015) in this area proposing methods to generate storyline datasets. Although each contribution p"
W16-6004,P13-1141,0,0.115335,"Missing"
W16-6004,cybulska-vossen-2014-using,1,0.881377,"Missing"
W16-6004,P07-1033,0,0.324222,"Missing"
W16-6004,N15-1117,0,0.149355,"Missing"
W16-6004,P16-1145,0,0.0810449,"Missing"
W16-6004,C16-1112,1,0.792678,"Missing"
W16-6004,P07-1034,0,0.259866,"Missing"
W16-6004,L16-1693,1,0.504551,"Missing"
W16-6004,R13-1092,1,0.831274,"Missing"
W16-6004,D07-1003,0,0.0242037,"Missing"
W16-6004,D15-1237,0,0.0915749,"Missing"
W17-2706,cybulska-vossen-2014-using,1,0.876168,"damaging-undergoer hasDamage damage hasNegativeEffectOn true damage activity 3 Evaluation Figure 3: Explicit chaining of event classes (left) and their shared properties in the pre, post and during situation (right). The CEO will be evaluated against a benchmark corpus to determine precision and recall for both the classes and the semantic circumstantial relations. For this, we plug the CEO into an existing NLP pipeline for text annotation and analysis (Vossen et al., 2016) For this, we are cur3 4 40 https://iptc.org/ https://www.w3.org/2004/02/skos/ rently annotating part of the ECB+ corpus (Cybulska and Vossen, 2014). We selected 24 topics that describe a calamity event. In our annotation, we only use the existing event mention annotations and add new mentions if they realize an event calamity class. In addition to this, the annotators define co-reference sets among event mentions and the semantic circumstantial relations. As such, we can evaluate what events are captured by our ontology and what relations can be successfully reconstructed. For the annotation, we use the CAT annotation tool (Bartalesi Lenzi et al., 2012). Additionally, we are designing a QuestionAnswering task, where systems will have to"
W17-2706,L16-1423,0,0.264153,"Missing"
W17-2706,L16-1233,1,0.576854,"Missing"
W17-2706,bartalesi-lenzi-etal-2012-cat,0,0.309905,"Missing"
W17-2706,W09-1206,0,0.393581,"Missing"
W17-2706,chambers-jurafsky-2010-database,0,0.626859,", C implies D or D is implied by C. The implication is however not necessary. Previous work on the encoding of semantic relations between event pairs has focused on specific subsets of circumstantial relations. For instance, one example is the encoding of the entailment relations in WordNet (Fellbaum, 1998). With respect to the WordNet approach in this work, we abstract from various event types (i.e. lexical items) and do not depend on relations defined at a synset level by formalizing event knowledge and relations in an ontology. Another related approach are narrative chains as described in (Chambers and Jurafsky, 2010) that provide chains of various event mentions. However, the relation between these mentions is not specified explicitly but based on cooccurrence of participants and a basic precedence relation. Manual inspection of these chains revealed that dissimilar relations are implied within these chains, varying from temporal ordering, to episodic, up to causal. The Penn Discourse TreeBank (PDTB) (Prasad et al., 2007) annotates contingency relations, of which causal relations are a subclass. In PDTB, the focus of the annotation is between two Abstract Objects (called Arg1 and Arg2), corresponding to d"
W17-2706,W15-4507,1,\N,Missing
W17-2711,P13-4006,0,0.0295079,"tional Similarity (Jurgens et al., 2012). In particular, we extracted words pairs from the test set Phase1 Answers corresponding to class-8 (CAUSEPURPOSE), retaining only word pairs in the categories Cause:Effect, Cause:Compensatory Action, Action/Activity, and Prevention, where both words express events. This initial set of seed elements has been further extended by looking for “cause”, “enablement”, and “entails” relations in SUMO (Niles and Pease, 2001, 2003) and in WordNet (Miller, 1995). This resulted in a list of 1,609 unique seed pairs. PPMI has been computed using the DISSECT Toolkit (Dinu et al., 2013), and pair frequencies have been extracted from Google bigrams(Brants and Franz, 2006). Rather than identifying a unique threshold for eligible pairs, we looked for a range of PPMI values. Classification 0.744 0.638 Table 2: Inter-annotator agreement: Dice coefficient at token level. One of the most interesting observations on the PLOT LINK analysis is that the agreement may vary according to the type of seminal event. For instance, the highest agreement has been observed for T19: a shooting accident :Dice 0.723 for relation identification, and 0.728 for relation classification. The lowest agr"
W17-2711,J86-2003,0,0.0315621,"annotation framework. 2.1 Basic Components: Events and Temporal Expressions A storyline relation can be best described as a loose causal and temporal relation between a pair of event mentions, where one event mention explains/justifies the occurrence of the other event mention in the pair (more details are reported in Events and temporal expressions are the basic components of the annotation scheme for the ESC v0.9 dataset. The term “event” is used as a cover term to refer to any situations that can happen, occur, or hold. The use of the term event is a synonym to “eventuality” introduced by Bach (1986), covering both dynamic and static situations (i.e. events and states). The annotation of events in NLP is a topic that got a lot of interest and on which yet no consensus has been reached. In this work, we adopted a definition of events that is provided in the ECB+ Annotation Guidelines (Cybulska and Vossen, 2014a), which is compatible with definitions in ACE (Linguistic Data Consortium, 2005) and TimeML. In particular, an event is any punctual, durational, or stative situation which happens or holds, and which results from a combination of four components such as: 1) an action component refe"
W17-2711,W13-1202,1,0.897705,"Missing"
W17-2711,bartalesi-lenzi-etal-2012-cat,0,0.021003,"nging to the class ACTION ASPECTUAL, which functions as lexical morphosyntactic markers of the the internal temporal structure of a situation, and ACTION CAUSATIVE as meta-level event mentions, and thus they are excluded. 6 Event annotation is directly inherited from ECB+, where only sentences containing relevant mentions of the topic were annotated. 7 Note that this can be extended further using the crossdocument event coreference chains of ECB+ 81 3 The annotation of the ESC v0.9 corpus has been conducted by 2 experts following a multistep process and using the web-based tool CAT (Bartalesi Lenzi et al., 2012). In the first phase, both annotators went through a training phase to familiarize with the task, and were allowed to discuss and compare their annotations, especially for the PLOT LINK task. This phase led to a revision of the annotation guidelines, by introducing more specific rules to select event pairs. In the second phase, the inter-annotator agreement was calculated on a subset of the ESC v0.9 dataset. In particular, given that the basic components, i.e. event mentions, temporal expressions, and event co-referential chains, are directly inherited from the ECB+ corpus, the agreement was c"
W17-2711,bethard-etal-2008-building,0,0.0387349,"fically designed to capture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Miltsakaki et al., 2004; Bethard et al., 2008; Mirza and Tonelli, 2014; Dunietz et al., 2015), but they differ in three ways: 1) they include the standard causal relations, i.e. cause, enablement, and prevention, but also additional event-event relations such as contingency, sub-event, entailment, and co-participation relations; 2) they are often not explicitly marked in the text through a relational structure; and 3) they are more specific than all events that stand in a temporal relation as they add explanatory information. 4. The earthquake killed 14 and left hundred trapped in collapsed buildings. earthquake rising action killed eart"
W17-2711,D10-1008,0,0.0531712,"lation. In our work both implicit and explicit relations are annotated, allowing the annotation at both intra- and inter-sentential levels. In addition to this, the availability of within- and cross-document event co-reference chains allows the extension of the annotated data across documents, providing access to a larger, “global” level of analysis. cal order, not only a temporal one. However, this does not hold anymore when cross-sentence relations are taken into account. 4 Related Work Frameworks and models for understanding narratives have mainly focused on fictional texts (Lehnert, 1981; Goyal et al., 2010; Mani, 2012) Modern day news reports still reflect narrative structures but they have proven difficult for automatic tools (Rospocher et al., 2016). To the best of our knowledge, previous work on StoryLine Extraction is limited, if we exclude the contribution by Caselli and Vossen (2016). However, there are several related works in NLP dealing with related tasks. The extraction of causal relations is the nearest task. One of the most prominent work is represented by the Penn Discourse Treebank (PDTB) (Miltsakaki et al., 2004), where explicit and implicit causal relations are annotated between"
W17-2711,W16-5708,1,0.90869,"holds, but we are looking for explanations of “why” events happened, according to the information that we are given in the document of analysis. Thus, in example 4, the relation between the events “earthquake” and “trapped” is obtained by answering the question “why were people trapped?” and not by means of transitive relation between the pairs earthquake rising action collapsed and collapsed rising action trapped. Explanatory Relation Annotation (PLOT LINKs) The annotation of explanatory relations between event pairs is encoded in the PLOT LINK tag, following a previous proposal described in Caselli and Vossen (2016). PLOT LINKs are specifically designed to capture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Mil"
W17-2711,S12-1047,0,0.0329063,"ion of event pairs in relations that mimic the textual order of presentation; 2) PPMI1: selection of event pairs using Positive Pointwise Mutual Information (PPMI) obtained from a set of selected seed pairs and the manually annotated pairs from the development set; 3) PPMI-CONTAINS: selection of the event pairs using PPMI as in the PPMI1 model but restricting the sets of events to those which share the same temporal anchors, i.e. have a TLINK of type contains. The seed pairs for the PPMI based models have been extracted from the SemEval 2012 Task-2: Measuring Degrees of Relational Similarity (Jurgens et al., 2012). In particular, we extracted words pairs from the test set Phase1 Answers corresponding to class-8 (CAUSEPURPOSE), retaining only word pairs in the categories Cause:Effect, Cause:Compensatory Action, Action/Activity, and Prevention, where both words express events. This initial set of seed elements has been further extended by looking for “cause”, “enablement”, and “entails” relations in SUMO (Niles and Pease, 2001, 2003) and in WordNet (Miller, 1995). This resulted in a list of 1,609 unique seed pairs. PPMI has been computed using the DISSECT Toolkit (Dinu et al., 2013), and pair frequencies"
W17-2711,P14-2082,0,0.0211046,"to annotate temporal expressions, 2) re-introducing the type attribute as part of the temporal expression tag; 3) re-introducing the attribute value for temporal expressions’ normalization. We also allow 3 All examples are taken from the ECB+ Annotation Guidelines or the ECB+ annotated data 79 is also a strategy to avoid the complexity of ordering relations between events. Most of the current solutions are not optimal, as they give the annotators too much freedom in the the selection of the event pairs (e.g. TimeML), or force the annotators to mark all possible relations (e.g. TimeBankDense (Cassidy et al., 2014)), or limit the annotations to the presence of explicit linguistic evidence (e.g. RED). The temporal values in ESC are derived from the RED guidelines. We apply two sets of TLINK values according to the type of anchoring relation annotated: four values apply for relations between events and DCTs (namely before, after, overlap, and contains), while only one value (contains) applies to relations between events and temporal expressions. Annotators are also instructed on the directionality of the TLINK, which should always go from the temporal expression, or DCT, to the target event. 2.3 PLOT LINK"
W17-2711,cybulska-vossen-2014-using,1,0.891471,"ally. Current NLP systems can identify complex information but they lack a method to connect it in a unitary and coherent message. Steps in this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the"
W17-2711,miltsakaki-etal-2004-penn,0,0.346239,"16). PLOT LINKs are specifically designed to capture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Miltsakaki et al., 2004; Bethard et al., 2008; Mirza and Tonelli, 2014; Dunietz et al., 2015), but they differ in three ways: 1) they include the standard causal relations, i.e. cause, enablement, and prevention, but also additional event-event relations such as contingency, sub-event, entailment, and co-participation relations; 2) they are often not explicitly marked in the text through a relational structure; and 3) they are more specific than all events that stand in a temporal relation as they add explanatory information. 4. The earthquake killed 14 and left hundred trapped in collapsed buildings. earthquake ris"
W17-2711,C14-1198,0,0.226389,"pture the semantics of plot structures. PLOT LINK annotation is conducted in two steps: first, annotators have to identify all eligible relations between event pairs, and then they have to classify each relation as belonging to one of the two classes: rising action, events which are circumstantial to, cause or enable another event, or falling action, which explicitly mark speculations and consequences, i.e. events which are the (anticipated) outcome or the effect of another event. PLOT LINKs are related to causal and temporal relation annotation (Miltsakaki et al., 2004; Bethard et al., 2008; Mirza and Tonelli, 2014; Dunietz et al., 2015), but they differ in three ways: 1) they include the standard causal relations, i.e. cause, enablement, and prevention, but also additional event-event relations such as contingency, sub-event, entailment, and co-participation relations; 2) they are often not explicitly marked in the text through a relational structure; and 3) they are more specific than all events that stand in a temporal relation as they add explanatory information. 4. The earthquake killed 14 and left hundred trapped in collapsed buildings. earthquake rising action killed earthquake rising action trap"
W17-2711,C16-1007,0,0.38978,"ack a method to connect it in a unitary and coherent message. Steps in this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the ESC data. Finally, conclusions and future work are reported in Sect"
W17-2711,N16-1098,0,0.34704,"this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the ESC data. Finally, conclusions and future work are reported in Section 5. The annotated data, the evaluation scripts, and the baselines mo"
W17-2711,W16-1007,0,0.539168,"this direction have been conducted but are very limited and do not cover 77 Proceedings of the Events and Stories in the News Workshop, pages 77–86, c Vancouver, Canada, August 4, 2017. 2017 Association for Computational Linguistics are compliant with other initiatives for event annotation: temporal processing (TimeML (Pustejovsky et al., 2003a) and Richer Event Description (RED) (O’Gorman et al., 2016)), event coreference (Event Coreference Bank+ (ECB+) (Cybulska and Vossen, 2014b)), and causal relations (Causal-TimeBank (Mirza and Tonelli, 2016), BECauSE (Dunietz et al., 2015), ROCStories (Mostafazadeh et al., 2016b) among others). The remainder of the paper is structured as follows: Section 2 will explain the annotation scheme, describe the annotation layers of the Event StoryLines Corpus (ESC) v0.9, and report on agreement measures. Section 3 will describe experiments related to the development of baselines for the StoryLine Extraction task. In Section 4 a review of previous annotation initiatives is given, showing differences and commonalities between them and the ESC data. Finally, conclusions and future work are reported in Section 5. The annotated data, the evaluation scripts, and the baselines mo"
W17-2711,W16-5706,0,0.0761097,"Missing"
W17-2711,W11-0419,0,0.0256782,"Temporal Anchoring of Events (TLINKs) Temporal information plays an essential role for StoryLine Extraction. At the same time, the annotation of temporal relations is by no means a trivial task. Two types of temporal relations can be identified: 1) ordering relations, which involve elements of the same ontological type, e.g. pairs of events or temporal expressions; and 2) anchoring relations, which involve cross-type element relations, e.g. pairs of event and related temporal expression. Although both types of temporal relations are useful, they have different informational status. Following Pustejovsky and Stubbs (2011), we assume that the informational level of a temporal relation can be expressed as a function of the information contained in each temporal link and their closure. Under this assumption, anchoring relations expressing when an event mention occurred or its duration, are more informative than ordering relations. The former allow us to put event mentions on a specific point (or interval) on an imaginary timeline and, as a consequence, also gives us the ordering relations between event mentions. The ESC Annotation Scheme expresses temporal relations using the TimeML TLINK tag and restricts them t"
W17-3503,W16-6615,0,0.0854802,"Missing"
W17-3503,W16-3210,1,0.897799,"Missing"
W17-3503,P16-1227,0,0.0306585,"of crowd workers with the subjects of the images has a noticeable influence on description specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To wha"
W17-3503,P14-2135,0,0.0310482,"e looking wood trailer is parked in a street in front of stores. c. An unusual looking vehicle parked in front of some stores. This example illustrates two strategies the crowd may use to provide descriptions for unfamiliar objects: (1) signal the unfamiliarity of the object using adjectives like strange and unusual looking. This is similar to the finding by Miyazaki and Shimizu (2016) that the Japanese crowd made frequent use of terms like foreign and overseas for the Western images from MS COCO. (2) use a more general cover term, like vehicle. Such terms may have a higher visual dispersion (Kiela et al., 2014), but they provide a safe back-off strategy. 27 Sports We found that unfamiliarity with different kinds of sports leads to the misclassification of those sports. We focus on three sports: American Football, Rugby, and Soccer. Looking at images for these sports, we compared how the three different groups referred to them. We found that the German and Dutch groups patterned together, deviating from the American crowd workers. As expected, the Dutch and German workers make the most mistakes categorizing American Football. For all seven pictures of American Football, there is at least one Dutch an"
W17-3503,P16-1168,0,0.463929,"ceable influence on description specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To what extent do speakers of different languages differ in their des"
W17-3503,N16-1021,0,0.0555208,"e subjects of the images has a noticeable influence on description specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To what extent do speakers of d"
W17-3503,W16-2346,1,0.903232,"Missing"
W17-3503,W16-3207,1,0.71946,"Missing"
W17-3503,P17-2066,0,0.135224,"ion specificity. 1 1. Task design effects: There are many possible approaches to collecting descriptions of images. Previous research has re-used the Flickr8K (Hodosh et al., 2013) template and methodology. Baltaretu and Castro Ferreira (2016) showed that task design may influence the form of crowd-sourced descriptions. Introduction Vision and language researchers have started to collect image description corpora for languages other than English, e.g. Chinese (Li et al., 2016), German (Elliott et al., 2016; Hitschler et al., 2016; Rajendran et al., 2016), Japanese (Miyazaki and Shimizu, 2016; Yoshikawa et al., 2017), French (Rajendran et al., 2016), and Turkish (Unal et al., 2016). The main aim of those efforts is to develop image description systems for non-English languages and to explore the related problems of cross-lingual image description (Elliott et al., 2015; Miyazaki and Shimizu, 2016) and machine translation in a visual context (Specia et al., 2016; Hitschler et al., 2016). We view these new corpora as sociological data that is in itself worth studying. Our research stems from the following question: To what extent do speakers of different languages differ in their descriptions of the same 2."
W17-3503,Q14-1006,0,0.706336,"claims about the last three factors based on the workers’ language and geolocations. We believe that studying differences between languages shows us which phenomena are robust across languages and thus important to consider when implementing and deploying models. Also, differences between languages can inform us about the feasibility of approaches to image description in different languages by translating existing English data (Li et al., 2016; Yoshikawa et al., 2017). Our analysis combines quantitative and qualitative studies of a trilingual corpus of described images. We use the Flickr30K (Young et al., 2014) for English, Multi30K for German (Elliott et al., 2016), and a new corpus of Dutch descriptions (Section 3). We build on earlier work that studies the semantic and pragmatic properties of English descriptions (van Miltenburg, 2016; van Miltenburg et al., 2016). Those works study ethnicity marking, negation marking, and unwarranted inferences about the roles of people. The main finding of our analysis is that all of these properties are stable across Dutch, US English, and German (Section 4). We also show how differences in background knowledge can affect description specificity (Section 5). W"
W17-4207,W13-1202,1,0.894307,"Missing"
W17-4207,kingsbury-palmer-2002-treebank,0,0.112657,"l., 2012) visualizes news stories as theme rivers. It also has a network visualization of actor-actor relations. This can be used when the corpus consists of e-mails, to show who writes about what to whom. In StoryTeller, the relations are not based on metadata but are relations in the domain of discourse extracted from text. 3 The following dimensions from the NewsReader data are used for the visualization. Event refers to the SEM-RDF ID: the instance identifier. The actors in the news article, which are described using role labels that come from different event ontologies, such as PropBank (Kingsbury and Palmer, 2002), FrameNet (Baker et al., 1998), and ESO (Segers et al., 2015). A climax score indicating the relevance of the event (normalized between 0 and 100) for a story. The climax score is a normalized score based on the number of mentions of an event and the prominence of each mention, where early mentions count as more prominent. A group label that uniquely identifies the event-group to which the event belongs. In NewsReader, groups are formed by connecting events by topic overlap of the articles in which they are mentioned and by sharing the same actors. Each group also has a groupScore which indic"
W17-4207,W15-4507,1,0.921873,"k (GAF, (Fokkens et al., 2013)) as an extension to SEM through gaf:denotedBy relations between instance representations in SEM and their mentions represented as offsets to different places in the texts. Likewise, information that is the same across mentions in different news articles gets deduplicated and information that is different gets aggregated. For each piece of information, the system stores the source and the perspective of the source on the information. The result is a complex multidimensional data set in which events and their relations are defined according to multiple properties (Vossen et al., 2015). The Storyteller application exploits these dimensions to present events within a context that explains them, approximating a story. ical data and emotions in Dutch 17th century theater plays in Section 5. Section 2 explains how our work differs from others. Section 6 concludes with future plans. 2 Related work Interactive graphics have been used for before to analyze high dimensional data (Buja et al., 1996; Martin and Ward, 1995; Buja et al., 1991), but fast, web-based and highly interactive visualizations with filtering are a fairly new development. With the advent of the open source libra"
W18-6550,P16-1054,0,0.0374173,"Missing"
W18-6550,W14-3348,0,0.0247587,"cription systems: Flickr30K and MS COCO (Young et al., 2014; Lin et al., 2014). Both of these data sets contain images with multiple crowd-sourced descriptions per image. These datasets are typically used to train data-driven natural language generation systems to automatically learn to associate visual features with natural language descriptions (Bernardi et al., 2016). Following the training phase, image description systems are evaluated by comparing their output with human generated descriptions for the same image (using textual similarity metrics like BLEU or METEOR, Papineni et al. 2002; Denkowski and Lavie 2014). The standard for what the image descriptions should look like is implicit in the corpus. The only point at which any explicit guidelines are provided is during the crowd-sourcing task, where annotators are given general instructions about what their description should look like. Here are the Flickr30K instructions (the MS COCO These guidelines leave much of the task open for interpretation by the annotator. For example, it is unclear how the descriptions will be used, or what the target audience is (as pointed out by van Miltenburg et al. 2017). Thus, the underspecified nature of the task in"
W18-6550,L18-1525,0,0.0389422,"Missing"
W18-6550,W17-3503,1,0.906527,"Missing"
W18-6550,C18-1147,1,0.888614,"Missing"
W18-6550,W16-3207,1,0.907843,"Missing"
W18-6550,P02-1040,0,0.10116,"evaluate automatic description systems: Flickr30K and MS COCO (Young et al., 2014; Lin et al., 2014). Both of these data sets contain images with multiple crowd-sourced descriptions per image. These datasets are typically used to train data-driven natural language generation systems to automatically learn to associate visual features with natural language descriptions (Bernardi et al., 2016). Following the training phase, image description systems are evaluated by comparing their output with human generated descriptions for the same image (using textual similarity metrics like BLEU or METEOR, Papineni et al. 2002; Denkowski and Lavie 2014). The standard for what the image descriptions should look like is implicit in the corpus. The only point at which any explicit guidelines are provided is during the crowd-sourcing task, where annotators are given general instructions about what their description should look like. Here are the Flickr30K instructions (the MS COCO These guidelines leave much of the task open for interpretation by the annotator. For example, it is unclear how the descriptions will be used, or what the target audience is (as pointed out by van Miltenburg et al. 2017). Thus, the underspec"
W18-6550,Q14-1006,0,0.168407,"Although this gives us a rich and diverse collection of data to work with, it also introduces uncertainty about how the world should be described. This paper shows the extent of this uncertainty in the PEOPLE domain. We present a taxonomy of different ways to talk about other people. This taxonomy serves as a reference point to think about how other people should be described, and can be used to classify and compute statistics about labels applied to people. 1 Introduction There are currently two major data sets used to train and evaluate automatic description systems: Flickr30K and MS COCO (Young et al., 2014; Lin et al., 2014). Both of these data sets contain images with multiple crowd-sourced descriptions per image. These datasets are typically used to train data-driven natural language generation systems to automatically learn to associate visual features with natural language descriptions (Bernardi et al., 2016). Following the training phase, image description systems are evaluated by comparing their output with human generated descriptions for the same image (using textual similarity metrics like BLEU or METEOR, Papineni et al. 2002; Denkowski and Lavie 2014). The standard for what the image"
W91-0211,J90-1003,0,0.0190531,"of the possible scope of lexical semantics would thus be one which tries to chart out the systematic, generalizable aspects of word meanings, and of the relations between words, drawing on readily accessible sources of lexical knowledge, such as machine readable dictionaries, encyclopedias, and representative corpora, coupled with the kind of analytic apparatus that is needed to fruitfully explore such sources, for instance custom-built parsers to cope with dictionary definitions (Vossen 1990), statistical programs to deal with the distributional properties of lexical items in large corpora (Church & Hanks 1990) etc. At the same time this kind of massive data-acquisition should be made sensitive to the borders between perceptual experience, lexical knowledge and expert knowledge. 1 Introduction Many psychologists are quite content to declare that intelligence is what you measure by means of an intelligence test. In a somewhat similar way one could say that lexical knowledge is the kind of knowledge that can be expressed in words. An advantage of this position is that you can turn it round: knowledge that cannot be expressed in words is not lexical knowledge. A further advantage is that it automatical"
W91-0211,C88-2153,0,0.0533796,"Missing"
W99-0512,C96-1005,0,0.121423,"Missing"
