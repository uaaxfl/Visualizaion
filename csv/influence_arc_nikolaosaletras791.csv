2020.aacl-main.80,N19-1056,0,0.0335297,"Missing"
2020.aacl-main.80,N19-1423,0,0.0798186,"Missing"
2020.aacl-main.80,N16-1122,0,0.394807,"endation (Alazzawi et al., 2012; Yuan et al., 2013; Preot¸iuc-Pietro and Cohn, 2013; Gao et al., 2015). Predicting the type of a POI is inherently different to predicting the POI type from comments or reviews. The role of the latter is to provide opinions or descriptions of the places, rather than the activities and feelings of the user posting the text (McKenzie et al., 2015), as illustrated in Table 1. This is also different, albeit related, to the popular task of geolocation prediction (Cheng et al., 2010; Eisenstein et al., 2010; Han et al., 2012; Roller et al., 2012; Rahimi et al., 2015; Dredze et al., 2016), as this aims to infer the exact geographical location of a post using language variation and geographical cues rather than inferring the place’s type. Our task aims to uncover the geographic agnostic features associated with POIs of different types. Our contributions are as follows: (1) We provide the first study of POI type prediction in computational linguistics; (2) A large data set made out of 804 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages"
2020.aacl-main.80,D10-1124,0,0.650817,"Missing"
2020.aacl-main.80,2020.acl-main.403,1,0.643972,"alysis We first analyze our data set to understand the relationship between location type, language and posting time. 3.1 Linguistic Analysis We analyze the linguistic features specific to each category by ranking unigrams that appear in at least 5 different locations, such that these are representative of the larger POI category rather than a few specific places. Features are normalized to sum up to unit for each tweet, then we compute the (Pearson) χ2 coefficient independently between its distribution across posts and the binary category label of the post similar to the approach followed by Maronikolakis et al. (2020) and Preot¸iuc-Pietro et al. (2019). Table 2 presents the top unigram features for each category. We note that most top unigrams specific of a category naturally refer to types of places (e.g. ‘campus’, ‘beach’, ‘mall’, ‘airport’) that are part of that category. All categories also contain words that refer to activities that the poster of the tweet is performing or observing while at a location (e.g. ‘camp’ and ‘football’ for College, ‘concert’ and ‘show’ for Arts & Entertainment, ‘party’ for Nightlife Spot, ‘landed’ for Travel & Transport, ‘hike’ for Greater Outdoors). Nightlife Spot and Food"
2020.aacl-main.80,D14-1162,0,0.0826003,"Missing"
2020.aacl-main.80,P19-1495,1,0.728814,"Missing"
2020.aacl-main.80,C12-1064,0,0.0255142,"such as POI visualisation (McKenzie et al., 2015) and recommendation (Alazzawi et al., 2012; Yuan et al., 2013; Preot¸iuc-Pietro and Cohn, 2013; Gao et al., 2015). Predicting the type of a POI is inherently different to predicting the POI type from comments or reviews. The role of the latter is to provide opinions or descriptions of the places, rather than the activities and feelings of the user posting the text (McKenzie et al., 2015), as illustrated in Table 1. This is also different, albeit related, to the popular task of geolocation prediction (Cheng et al., 2010; Eisenstein et al., 2010; Han et al., 2012; Roller et al., 2012; Rahimi et al., 2015; Dredze et al., 2016), as this aims to infer the exact geographical location of a post using language variation and geographical cues rather than inferring the place’s type. Our task aims to uncover the geographic agnostic features associated with POIs of different types. Our contributions are as follows: (1) We provide the first study of POI type prediction in computational linguistics; (2) A large data set made out of 804 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th Inter"
2020.aacl-main.80,N15-1153,0,0.0236014,"al., 2015) and recommendation (Alazzawi et al., 2012; Yuan et al., 2013; Preot¸iuc-Pietro and Cohn, 2013; Gao et al., 2015). Predicting the type of a POI is inherently different to predicting the POI type from comments or reviews. The role of the latter is to provide opinions or descriptions of the places, rather than the activities and feelings of the user posting the text (McKenzie et al., 2015), as illustrated in Table 1. This is also different, albeit related, to the popular task of geolocation prediction (Cheng et al., 2010; Eisenstein et al., 2010; Han et al., 2012; Roller et al., 2012; Rahimi et al., 2015; Dredze et al., 2016), as this aims to infer the exact geographical location of a post using language variation and geographical cues rather than inferring the place’s type. Our task aims to uncover the geographic agnostic features associated with POIs of different types. Our contributions are as follows: (1) We provide the first study of POI type prediction in computational linguistics; (2) A large data set made out of 804 Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Langu"
2020.aacl-main.80,D17-2010,0,0.0160226,"adjectives indicative of the user’s surroundings (e.g. ‘beautiful’ in Greater Outdoors places). Finally, we also uncover words indicative of the time the user is at a place, such as ‘tonight’ for Arts & Entertainment, ‘sunset’ for the Greater Outdoors and ‘night’ for Nightlife Spots and Arts & Entertainment. 2.5 3.2 2.4 Data Split Text Processing We lower-case text and replace all URLs and mentions of users with placeholders. We preserve emoticons and punctuation and replace tokens that appear in less than five tweets with an ‘unknown’ token. We tokenize text using a Twitter-aware tokenizer (Schwartz et al., 2017). 3 Analysis We first analyze our data set to understand the relationship between location type, language and posting time. 3.1 Linguistic Analysis We analyze the linguistic features specific to each category by ranking unigrams that appear in at least 5 different locations, such that these are representative of the larger POI category rather than a few specific places. Features are normalized to sum up to unit for each tweet, then we compute the (Pearson) χ2 coefficient independently between its distribution across posts and the binary category label of the post similar to the approach follow"
2020.aacl-main.80,P19-1272,0,0.276468,"Missing"
2020.acl-main.403,D11-1120,0,0.0283938,"staff. While both pastiches and staff writers aim to present similar content with similar style to the original authors, the texts lack the humorous component specific of parodies. A large body of related NLP work has explored the inference of user characteristics. Past research studied predicting the type of a Twitter account, most frequently between individual or organizational, using linguistic features (De Choudhury et al., 2012; McCorriston et al., 2015; Mac Kim et al., 2017). A broad literature has been devoted to predicting personal traits from language use on Twitter, such as gender (Burger et al., 2011), age (Nguyen et al., 2011), geolocation (Cheng et al., 2010), political preference (Volkova et al., 2014; Preot¸iuc-Pietro et al., 2017), income (Preot¸iuc-Pietro et al., 2015; Aletras and Chamberlain, 2018), impact (Lampos et al., 2014), socio-economic status (Lampos et al., 2016), race (Preot¸iuc-Pietro and Ungar, 2018) or personality (Schwartz et al., 2013; Preot¸iuc-Pietro et al., 2016). 3 Task & Data We define parody detection in social media as a binary classification task performed at the social media post level. Given a post T , defined as a sequence of tokens T = {t1 , ..., tn }, the"
2020.acl-main.403,N19-1423,0,0.0212245,"sing AWD-LSTM (Merity et al., 2018) as the base text encoder pretrained on the Wikitext 103 data set and we fine-tune it on our own parody classification task. For this purpose, after the AWS-LSTM layers, we add a fully-connected layer with a ReLU activation function followed by an output layer with a sigmoid activation function. Before each of these two additional layers, we perform batch normalization. 4377 4.4 BERT and RoBERTa Bidirectional Encoder Representations from Transformers (BERT) is a language model based on transformer networks (Vaswani et al., 2017) pre-trained on large corpora (Devlin et al., 2019). The model makes use of multiple multi-head attention layers to learn bidirectional embeddings for input tokens. It is trained for masked language modelling, where a fraction of the input tokens in a given sequence are masked and the task is to predict a masked word given its context. BERT uses wordpieces which are passed through an embedding layer and get summed together with positional and segment embeddings. The former introduce positional information to the attention layers, while the latter contain information about the location of a segment. Similar to ULMFit, we fine-tune the BERT-base"
2020.acl-main.403,W12-0411,0,0.0136196,"on to other NLP Tasks The pretense aspect of parody relates our task to a few other NLP tasks. In authorship attribution, the goal is to predict the author of a given text (Stamatatos, 2009; Juola et al., 2008; Koppel et al., 2009). However, there is no intent for the authors to imitate the style of others and most differences between authors are in the topics they write about, which we aim to limit by focusing on political parody. Further, in our setups, no tweets from an author are in both training and testing to limit the impact of terms specific to a particular person. Pastiche detection (Dinu et al., 2012) aims to distinguish between an original text and a text written by someone aiming to imitate the style of the original author with the goal of impersonating. Most similar in experimental setup to our task, Preot¸iuc-Pietro and Devlin Marier (2019) aim to distinguish between tweets published from the same account by different types of users: politicians or their staff. While both pastiches and staff writers aim to present similar content with similar style to the original authors, the texts lack the humorous component specific of parodies. A large body of related NLP work has explored the infe"
2020.acl-main.403,P11-2102,0,0.0325144,"will never be on the table for any trade negotiations. Were investing more than ever before - and when we leave the EU, we will introduce an Australian style, points-based immigration system so the NHS can plan for the future. People seem to be ignoring the many advantages of selling off the NHS, like the fact that hospitals will be far more spacious once poor people can’t afford to use them. Table 1: Two examples of Twitter accounts of politicians and their corresponding parody account with a sample tweet from each. sense, irony is treated in NLP in a similar way as sarcasm (Gonz´alez-Ib´an˜ ez et al., 2011; Khattri et al., 2015; Joshi et al., 2017). In addition to the words in the utterance, further using the user and pragmatic context is known to be informative for irony or sarcasm detection in NLP (Bamman and Smith, 2015; Wallace, 2015). For instance, Oprea and Magdy (2019) make use of user embeddings for textual sarcasm detection. In the design of our data splits, we aim to limit the contribution of this aspects from the results. Relation to other NLP Tasks The pretense aspect of parody relates our task to a few other NLP tasks. In authorship attribution, the goal is to predict the author of"
2020.acl-main.403,P18-1031,0,0.0175915,"iter and Schmidhuber, 1997) with a self-attention mechanism (BiLSTM-Att; Zhou et al. (2016)). Tokens ti in a given tweet T = {t1 , ..., tn } are mapped to embeddings and passed through a bidirectional LSTM. A single tweet representation (h) is computed as the sum of the resulting contexP tualized vector representations ( i ai hi ) where ai is the self-attention score in timestep i. The tweet representation (h) is subsequently passed to the output layer using a sigmoid activation function. 4.3 ULMFit The Universal Language Model Fine-tuning (ULMFit) is a method for efficient transfer learning (Howard and Ruder, 2018). The key intuition is to train a text encoder on a language modelling task (i.e. predicting the next token in a sequence) where data is abundant, then fine-tune it on a target task where data is more limited. During fine-tuning, ULMFit uses gradual layer unfreezing to avoid catastrophic forgetting. We experiment with using AWD-LSTM (Merity et al., 2018) as the base text encoder pretrained on the Wikitext 103 data set and we fine-tune it on our own parody classification task. For this purpose, after the AWS-LSTM layers, we add a fully-connected layer with a ReLU activation function followed by"
2020.acl-main.403,D19-3020,1,0.826068,"ional social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet;4 and many users were fooled by a Donald Trump parody tweet about ‘Dow Joans’.5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019), sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018). Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016); (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997); (iii) network science, to identify the adoption and diffusion mechanisms of parody (Vosoughi et al., 2018). 2 Related"
2020.acl-main.403,W15-2905,0,0.0253556,"the table for any trade negotiations. Were investing more than ever before - and when we leave the EU, we will introduce an Australian style, points-based immigration system so the NHS can plan for the future. People seem to be ignoring the many advantages of selling off the NHS, like the fact that hospitals will be far more spacious once poor people can’t afford to use them. Table 1: Two examples of Twitter accounts of politicians and their corresponding parody account with a sample tweet from each. sense, irony is treated in NLP in a similar way as sarcasm (Gonz´alez-Ib´an˜ ez et al., 2011; Khattri et al., 2015; Joshi et al., 2017). In addition to the words in the utterance, further using the user and pragmatic context is known to be informative for irony or sarcasm detection in NLP (Bamman and Smith, 2015; Wallace, 2015). For instance, Oprea and Magdy (2019) make use of user embeddings for textual sarcasm detection. In the design of our data splits, we aim to limit the contribution of this aspects from the results. Relation to other NLP Tasks The pretense aspect of parody relates our task to a few other NLP tasks. In authorship attribution, the goal is to predict the author of a given text (Stamata"
2020.acl-main.403,E14-1043,1,0.890442,"Missing"
2020.acl-main.403,P13-1098,0,0.0850658,"Missing"
2020.acl-main.403,2021.ccl-1.108,0,0.0772189,"Missing"
2020.acl-main.403,P17-2075,0,0.0233415,"n Marier (2019) aim to distinguish between tweets published from the same account by different types of users: politicians or their staff. While both pastiches and staff writers aim to present similar content with similar style to the original authors, the texts lack the humorous component specific of parodies. A large body of related NLP work has explored the inference of user characteristics. Past research studied predicting the type of a Twitter account, most frequently between individual or organizational, using linguistic features (De Choudhury et al., 2012; McCorriston et al., 2015; Mac Kim et al., 2017). A broad literature has been devoted to predicting personal traits from language use on Twitter, such as gender (Burger et al., 2011), age (Nguyen et al., 2011), geolocation (Cheng et al., 2010), political preference (Volkova et al., 2014; Preot¸iuc-Pietro et al., 2017), income (Preot¸iuc-Pietro et al., 2015; Aletras and Chamberlain, 2018), impact (Lampos et al., 2014), socio-economic status (Lampos et al., 2016), race (Preot¸iuc-Pietro and Ungar, 2018) or personality (Schwartz et al., 2013; Preot¸iuc-Pietro et al., 2016). 3 Task & Data We define parody detection in social media as a binary c"
2020.acl-main.403,N19-1069,0,0.0308889,"Parody is considered an integral part of Twitter (Vis, 2013) and previous studies on parody in social media focused on analysing how these accounts contribute to topical discussions (Highfield, 2016) and the relationship between identity, impersonation and authenticity (Page, 2014). Public relation studies showed that parody accounts impact organisations during crises while they can become a threat to their reputation (Wan et al., 2015). Satire Most related to parody, satire has been tangentially studied as one of several prediction targets in NLP in the context of identifying disinformation (McHardy et al., 2019; de Morais et al., 2019). (Rashkin et al., 2017) compare the language of real news with that of satire, hoaxes, and propaganda to identify linguistic features of unreliable text. They demonstrate how stylistic characteristics can help to decide the text’s veracity. The study of parody is therefore relevant to this topic, as satire and parodies are classified by some as a type of disinformation with ‘no intention to cause harm but has potential to fool’ (Wardle and Derakhshan, 2018). Irony and Sarcasm There is a rich body of work in NLP on identifying irony and sarcasm as a classification task"
2020.acl-main.403,W11-1515,0,0.0320037,"Missing"
2020.acl-main.403,P19-1275,0,0.0246644,"tages of selling off the NHS, like the fact that hospitals will be far more spacious once poor people can’t afford to use them. Table 1: Two examples of Twitter accounts of politicians and their corresponding parody account with a sample tweet from each. sense, irony is treated in NLP in a similar way as sarcasm (Gonz´alez-Ib´an˜ ez et al., 2011; Khattri et al., 2015; Joshi et al., 2017). In addition to the words in the utterance, further using the user and pragmatic context is known to be informative for irony or sarcasm detection in NLP (Bamman and Smith, 2015; Wallace, 2015). For instance, Oprea and Magdy (2019) make use of user embeddings for textual sarcasm detection. In the design of our data splits, we aim to limit the contribution of this aspects from the results. Relation to other NLP Tasks The pretense aspect of parody relates our task to a few other NLP tasks. In authorship attribution, the goal is to predict the author of a given text (Stamatatos, 2009; Juola et al., 2008; Koppel et al., 2009). However, there is no intent for the authors to imitate the style of others and most differences between authors are in the topics they write about, which we aim to limit by focusing on political parod"
2020.acl-main.403,D14-1162,0,0.0821219,"Missing"
2020.acl-main.403,P19-1274,0,0.0308767,"Missing"
2020.acl-main.403,P19-1495,1,0.72309,"Missing"
2020.acl-main.403,P17-1068,0,0.0828179,"Missing"
2020.acl-main.403,C18-1130,0,0.0360063,"Missing"
2020.acl-main.403,D17-1317,0,0.022483,"(Vis, 2013) and previous studies on parody in social media focused on analysing how these accounts contribute to topical discussions (Highfield, 2016) and the relationship between identity, impersonation and authenticity (Page, 2014). Public relation studies showed that parody accounts impact organisations during crises while they can become a threat to their reputation (Wan et al., 2015). Satire Most related to parody, satire has been tangentially studied as one of several prediction targets in NLP in the context of identifying disinformation (McHardy et al., 2019; de Morais et al., 2019). (Rashkin et al., 2017) compare the language of real news with that of satire, hoaxes, and propaganda to identify linguistic features of unreliable text. They demonstrate how stylistic characteristics can help to decide the text’s veracity. The study of parody is therefore relevant to this topic, as satire and parodies are classified by some as a type of disinformation with ‘no intention to cause harm but has potential to fool’ (Wardle and Derakhshan, 2018). Irony and Sarcasm There is a rich body of work in NLP on identifying irony and sarcasm as a classification task (Wallace, 2015; Joshi et al., 2017). Van Hee et"
2020.acl-main.403,D17-2010,0,0.0873776,"the data based on the location of the politicians. We group the accounts in three groups of locations: US, UK and the rest of the world (RoW). We obtain three different splits, where each group makes up the test set and the other two groups make up the train and development set (see Table 4). 3.4 Text Preprocessing We preprocess text by lower-casing, replacing all URLs and anonymizing all mentions of usernames with placeholder token. We preserve emoticons and punctuation marks and replace tokens that appear in less than five tweets with a special ‘unknown’ token. We tokenize text using DLATK (Schwartz et al., 2017), a Twitter-aware tokenizer. BiLSTM-Att The first neural model is a bidirectional LongShort Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) with a self-attention mechanism (BiLSTM-Att; Zhou et al. (2016)). Tokens ti in a given tweet T = {t1 , ..., tn } are mapped to embeddings and passed through a bidirectional LSTM. A single tweet representation (h) is computed as the sum of the resulting contexP tualized vector representations ( i ai hi ) where ai is the self-attention score in timestep i. The tweet representation (h) is subsequently passed to the output layer using a sigmoid a"
2020.acl-main.403,P16-2034,0,0.0128367,"he other two groups make up the train and development set (see Table 4). 3.4 Text Preprocessing We preprocess text by lower-casing, replacing all URLs and anonymizing all mentions of usernames with placeholder token. We preserve emoticons and punctuation marks and replace tokens that appear in less than five tweets with a special ‘unknown’ token. We tokenize text using DLATK (Schwartz et al., 2017), a Twitter-aware tokenizer. BiLSTM-Att The first neural model is a bidirectional LongShort Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) with a self-attention mechanism (BiLSTM-Att; Zhou et al. (2016)). Tokens ti in a given tweet T = {t1 , ..., tn } are mapped to embeddings and passed through a bidirectional LSTM. A single tweet representation (h) is computed as the sum of the resulting contexP tualized vector representations ( i ai hi ) where ai is the self-attention score in timestep i. The tweet representation (h) is subsequently passed to the output layer using a sigmoid activation function. 4.3 ULMFit The Universal Language Model Fine-tuning (ULMFit) is a method for efficient transfer learning (Howard and Ruder, 2018). The key intuition is to train a text encoder on a language modelli"
2020.acl-main.403,S18-1005,0,0.0587638,"Missing"
2020.acl-main.403,W14-2508,0,0.0294434,"ifying it are important to applications in computational social science and beyond. Parody tweets can often be misinterpreted as facts even though Twitter only allows parody accounts if they are explicitly marked as parody3 and the poster does not have the intention to mislead. For example, the Speaker of the US House of Representatives, Nancy Pelosi, falsely cited a Michael Flynn parody tweet;4 and many users were fooled by a Donald Trump parody tweet about ‘Dow Joans’.5 Thus, accurate parody classification methods can be useful in downstream NLP applications such as automatic fact checking (Vlachos and Riedel, 2014) and rumour verification (Karmakharm et al., 2019), sentiment analysis (Pang et al., 2008) or nowcasting voting intention (Tumasjan et al., 2010; Lampos et al., 2013; Tsakalidis et al., 2018). Beyond NLP, parody detection can be used in: (i) political communication, to study and understand the effects of political parody in the public speech on a large scale (Hariman, 2008; Highfield, 2016); (ii) linguistics, to identify characteristics of figurative language (Rose, 1993; Kreuz and Roberts, 1993; Rossen-Knill and Henry, 1997); (iii) network science, to identify the adoption and diffusion mecha"
2020.acl-main.403,P14-1018,0,0.0576436,"Missing"
2020.coling-main.157,P19-1458,0,0.0390757,"Missing"
2020.coling-main.157,D19-1070,0,0.027242,"(2) it allows a direct comparison with existing methods. We also use the data for distant supervision2 collected by Preotiuc-Pietro et al. (2019). This extra ‘noisy’ data source contains 18,218 complaint tweets collected by querying Twitter API with certain complaint related hashtags (e.g. #badbusiness, #badcustomerservice, etc.) and the same amount of noncomplaint tweets that were sampled randomly. 3 Transformer-based Models Transformer architectures trained on language modeling have been recently adapted to downstream tasks demonstrating state-of-the-art performance (Weller and Seppi, 2019; Gupta and Durrett, 2019; Maronikolakis et al., 2020). In this paper, we adapt and subsequently combine transformers with external linguistic information for complaint prediction. BERT, ALBERT and RoBERTa Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) learns language representations by jointly conditioning on both left and right contexts using transformers. It is trained on masked language modeling where some of the tokens are randomly masked with the aim to predict them using only the context. We further experiment with ALBERT (Lan et al., 2019) and RoBERTa (Liu et al., 2019). A"
2020.coling-main.157,P18-1031,0,0.018183,"evious approaches for complaint identification by Preotiuc-Pietro et al. (2019) and a transfer learning method: (1) Logistic Regression with bag-of-words trained using the original and distantly supervised complaint data (LR-BOW + Dist. Supervision); (2) A Long-Short Term Memory (LSTM) network (Hochreiter and Schmidhuber, 1997) that takes as input a tweet, maps its words to embeddings and subsequently passes them through the LSTM to obtain a contextualized representation which is finally fed to the output layer; (3) Adapting the pre-trained Universal Language Model Fine-tuning (ULMFiT) model (Howard and Ruder, 2018) for complaint prediction. ULMFiT uses a AWD-LSTM (Merity et al., 2017) encoder for language modeling. Hyper-parameters We use BERT, ALBERT and RoBERTa Base uncased models; fine-tuning them with learning rate l = 1e-5, l ∈ {1e-4, 1e-5, 2e-5, 1e-6}. We use the Base cased pre-trained XLNet tuning the learning rate over the same range as for BERT models. For ULMFiT, we use AWD-LSTM trained on Wikitext-103. We simplify the default fine-tuning by only unfreezing the last 1 layer, the last 2 layers and all layers with learning rates l1 = 1e−4 , l2 = 1e−4 and l3 = 1e-3 respectively. For M-BERT, 2.64"
2020.coling-main.157,2020.acl-main.403,1,0.305134,"mparison with existing methods. We also use the data for distant supervision2 collected by Preotiuc-Pietro et al. (2019). This extra ‘noisy’ data source contains 18,218 complaint tweets collected by querying Twitter API with certain complaint related hashtags (e.g. #badbusiness, #badcustomerservice, etc.) and the same amount of noncomplaint tweets that were sampled randomly. 3 Transformer-based Models Transformer architectures trained on language modeling have been recently adapted to downstream tasks demonstrating state-of-the-art performance (Weller and Seppi, 2019; Gupta and Durrett, 2019; Maronikolakis et al., 2020). In this paper, we adapt and subsequently combine transformers with external linguistic information for complaint prediction. BERT, ALBERT and RoBERTa Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) learns language representations by jointly conditioning on both left and right contexts using transformers. It is trained on masked language modeling where some of the tokens are randomly masked with the aim to predict them using only the context. We further experiment with ALBERT (Lan et al., 2019) and RoBERTa (Liu et al., 2019). ALBERT uses two parameter-redu"
2020.coling-main.157,P15-1169,1,0.82711,"Missing"
2020.coling-main.157,P19-1495,1,0.641371,"e a battery of neural models underpinned by transformer networks which we subsequently combine with linguistic information. Experiments on a publicly available data set of complaints demonstrate that our models outperform previous state-of-the-art methods by a large margin achieving a macro F1 up to 87. 1 Introduction Complaining is a basic speech act, usually triggered by a discrepancy between reality and expectations towards an entity or event (Olshtain and Weinbach, 1985; Cohen and Olshtain, 1993; Kowalski, 1996). Social media has become a popular platform for expressing complaints online (Preotiuc-Pietro et al., 2019) where customers can directly address companies regarding issues with services and products. Complaint detection aims to identify a breach of expectations in a given text snippet. However, the use of implicit and ironic expressions and accompaniment of other speech acts such as suggestions, criticism, warnings and threats (Pawar et al., 2015) make it a challenging task. Identifying and classifying complaints automatically is important for: (a) improving customer service chatbots (Coussement and Van den Poel, 2008; Lailiyah et al., 2017; Yang et al., 2019a); (b) linguists to analyze complaint c"
2020.coling-main.157,P16-1148,0,0.0994282,"classification task. Then we fine-tune them using the smaller original complaint data set. 2 1766 M-BERT To combine our model with external linguistic information, we adapt the Multimodal BERT (M-BERT) (Rahman et al., 2019) model structure that has been introduced for multimodal modeling (text, image, speech). Instead of cross-modal interactions, we inject extra linguistic information as alternative views of the data into the pre-trained BERT model. We use (a) Emotion, a 9 dimensional vector obtained by quantifying six basic emotions of Ekman (1992) for each tweet using a predictive model by Volkova and Bachrach (2016); (b) Topics, a 200 dimensional vector representing word frequencies in word clusters designed to identify semantic themes in tweets by Preotiuc-Pietro et al. (2015; 2015). To inject external linguistic information to M-BERT,3 we first project the linguistic information into vectors with similar size to the BERT CLS embeddings. Then we concatenate word representations obtained from BERT and the linguistic information (Emotion, Topics or Emotion+Topics) to generate combined embeddings. During concatenation, an Attention Gating Mechanism called Multimodal Shifting Gate (Wang et al., 2019) is app"
2020.coling-main.157,D19-1372,0,0.0285341,"publicly available; and (2) it allows a direct comparison with existing methods. We also use the data for distant supervision2 collected by Preotiuc-Pietro et al. (2019). This extra ‘noisy’ data source contains 18,218 complaint tweets collected by querying Twitter API with certain complaint related hashtags (e.g. #badbusiness, #badcustomerservice, etc.) and the same amount of noncomplaint tweets that were sampled randomly. 3 Transformer-based Models Transformer architectures trained on language modeling have been recently adapted to downstream tasks demonstrating state-of-the-art performance (Weller and Seppi, 2019; Gupta and Durrett, 2019; Maronikolakis et al., 2020). In this paper, we adapt and subsequently combine transformers with external linguistic information for complaint prediction. BERT, ALBERT and RoBERTa Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018) learns language representations by jointly conditioning on both left and right contexts using transformers. It is trained on masked language modeling where some of the tokens are randomly masked with the aim to predict them using only the context. We further experiment with ALBERT (Lan et al., 2019) and RoBE"
2020.coling-main.157,N19-2008,0,0.173395,"pressing complaints online (Preotiuc-Pietro et al., 2019) where customers can directly address companies regarding issues with services and products. Complaint detection aims to identify a breach of expectations in a given text snippet. However, the use of implicit and ironic expressions and accompaniment of other speech acts such as suggestions, criticism, warnings and threats (Pawar et al., 2015) make it a challenging task. Identifying and classifying complaints automatically is important for: (a) improving customer service chatbots (Coussement and Van den Poel, 2008; Lailiyah et al., 2017; Yang et al., 2019a); (b) linguists to analyze complaint characteristics on large scale (V´asquez, 2011; Kakolaki and Shahrokhi, 2016); and (c) psychologists to understand the behavior of humans that express complaints (Sparks and Browning, 2010). Previous work has focused on binary classification between complaints and non-complaints in various domains (Preotiuc-Pietro et al., 2019; Jin et al., 2013; Coussement and Van den Poel, 2008). Furthermore, some studies have performed more fine-grained complaint classification. For instance, complaints directed to public authorities have been categorized based on their"
2020.eamt-1.16,C04-1046,0,0.505771,"slation errors in a sentence while minimising the need for direct human annotation. For that purpose, we use transfer-learning to leverage large scale noisy annotations and small sets of high-quality human annotated translation errors to train QE models. Experiments on four language pairs and translations obtained by statistical and neural models show consistent gains over strong baselines. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) is the task of predicting the overall quality of an automatically generated translation e.g., on either word, sentence or document level (Blatz et al., 2004; Ueffing and Ney, 2007). In opposition to automatic metrics and manual evaluation which rely on gold standard reference translations, QE models can produce quality estimates on unseen data, c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. and at runtime. QE has already proven its usefulness in many applications such as improving productivity in post-editing of MT, and recent neuralbased approaches to QE have been shown to provide promising performance in predicting quality of neural MT output (Fonseca et al., 2019)."
2020.eamt-1.16,P13-1004,1,0.801381,"scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., word, phrase, or sentence). Then, Kepler et al. (2019) used a predictorestimator architecture similar to P OSTECH alongside v"
2020.eamt-1.16,P15-1174,0,0.0165917,"standard labels. We also experimented with two approaches for finetuning: (1) unfreezing all the layers at the same time; and (2) a gradual unfreezing approach proposed by (Howard and Ruder, 2018). We use Adam (Kingma and Ba, 2014) with default parameters, and a batch size of 100. For the Hybrid model, we optimise the L2 regularisation penalty. Table 2 reports on the optimal values determined by hyper-parameters optimisation. 5 Results Tables 3 and 4 show respectively the average absolute Pearson’s r correlation co-efficient and the Root Mean Square Error (the official metrics for this task (Graham, 2015)) between actual and predicted MQM error proportions in six combinations of MT models (PBMT, NMT) and language pairs (EN-DE, EN-LV, DE-EN and EN-CS). First, we observe that the baseline model (LRQEfeat) performs fairly well on predicting the proportion of errors, especially for the EN-DE and EN-CS PBMT. However, it is not robust across language pairs and types of translation systems. 4 We have also tested a Support Vector Regression with a radial basis function kernel, but it yielded lower performance. 5 We did not observe noticeable differences in performance using smaller or larger size in e"
2020.eamt-1.16,P18-1031,0,0.171417,"ween annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., wor"
2020.eamt-1.16,C18-1266,1,0.91762,"Missing"
2020.eamt-1.16,W19-5406,0,0.039002,"Missing"
2020.eamt-1.16,W17-4763,0,0.0444526,"(TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., word, phrase, or sentence). Then, Kepler et al. (2019) used a predictorestimator architecture similar to P OSTECH alongside very large scale pre-trained representations from BERT (Devlin et al., 2018) and XLM (Lample and Conneau, 2019), and ensembling techniques, to win the QE tasks at WMT’19 (Fonseca et al., 2019)"
2020.eamt-1.16,quirk-2004-training,0,0.143311,"e introduce a new task of predicting the proportion of actual translation errors using transfer-learning for QE1 , by leveraging large scale noisy HTER annotations and smaller but of higher quality expert MQM annotations; (2) we show that our simple yet effective approach using transfer-learning yields better performance at predicting the proportion of actual errors in MT, compared to models trained directly on expert-annotated MQM or HTER-only data; (3) we report experiments on four language pairs and both statistical and neural MT systems. 2 Related Work Quality labels for sentence-level QE Quirk (2004) introduced the use of manually created 1 https://github.com/sheffieldnlp/tlqe quality labels for evaluating MT systems. With a rather small dataset (approximately 350 sentences), they reported better results than those obtained with a much larger set of instances annotated automatically. Similarly, Specia et al. (2009) proposed the use of a (1-4) Likert scale representing a translator’s perception on quality with regard to the degree of difficulty to fix a translation. However, sentence-level quality annotations appear to be subjective while agreement between annotators is generally low (Spec"
2020.eamt-1.16,N16-1069,1,0.821075,"013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learning at different levels of granularity (e.g., word, phrase, or sentence). Then, Kepler et al. (2019) used a predictorestimator architecture similar to P OSTECH alongside very large scale pre-trai"
2020.eamt-1.16,2006.amta-papers.25,0,0.321078,"MT models themselves, such as drastic degradation of their performance on out-of-domain data. As an alternative, QE models are often trained under weak supervision, using training instances labelled from noisy or limited sources (e.g. data labelled with automatic metrics for MT). Here, we focus on sentence-level QE, where given a pair of sentences (the source and its translation), the aim is to train supervised Machine Learning (ML) models that can predict a quality label as a numerical value. The most widely used label for sentence-level QE is the Human-mediated Translation Edit Rate (HTER) (Snover et al., 2006), which represents the post-editing effort. HTER consists of the minimum number of edits a human language expert is required to make in order to fix the translation errors in a sentence, taking values between 0 and 1. The main limitation of HTER is that it does not represent an actual translation error rate, but its noisy approximation. The noise stems mostly from errors in the heuristics used to automatically align the machine translation and its post-edited version, but also from the fact that some edits represent preferential choices of humans, rather than errors. To overcome such limitatio"
2020.eamt-1.16,2009.eamt-1.5,1,0.770558,"rformance at predicting the proportion of actual errors in MT, compared to models trained directly on expert-annotated MQM or HTER-only data; (3) we report experiments on four language pairs and both statistical and neural MT systems. 2 Related Work Quality labels for sentence-level QE Quirk (2004) introduced the use of manually created 1 https://github.com/sheffieldnlp/tlqe quality labels for evaluating MT systems. With a rather small dataset (approximately 350 sentences), they reported better results than those obtained with a much larger set of instances annotated automatically. Similarly, Specia et al. (2009) proposed the use of a (1-4) Likert scale representing a translator’s perception on quality with regard to the degree of difficulty to fix a translation. However, sentence-level quality annotations appear to be subjective while agreement between annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach wher"
2020.eamt-1.16,P15-4020,1,0.844269,"mall learning rate following (Howard and Ruder, 2018). Hybrid Finally, we hypothesise that linguistic information (e.g., number of tokens in the source/target sentence, language model probability of source/target sentence, etc.) might be complementary to the source-target representations obtained by our BiRNN-MQMT L +FT model. For that purpose, we first extract a representation of the source and translated sentence by removing the BiRNN-MQMT L +FT output layer and then we concatenate it with the widely used 17 blackbox sentence-level QE features extracted with the open-source QuEst++ toolkit (Specia et al., 2015). The joint neural and linguistic information of the source and target sentences is fed into a linear regression2 model using a L2 regularisation penalty. 4 4.1 Experimental Setup Data For our experiments, we use the freely available QT21 dataset3 (Specia et al., 2017) used in the QE shared task (Bojar et al., 2017; Specia et al., 2018). This dataset contains both post-edited (HTER) and error-annotated (MQM) data in four language pairs: English into German, Latvian and Czech, and German into English; and phrase-based statistical (PBMT) and neural (NMT) translation models. The annotation for er"
2020.eamt-1.16,W18-6451,1,0.907321,"Missing"
2020.eamt-1.16,2011.eamt-1.12,1,0.763893,"004) introduced the use of manually created 1 https://github.com/sheffieldnlp/tlqe quality labels for evaluating MT systems. With a rather small dataset (approximately 350 sentences), they reported better results than those obtained with a much larger set of instances annotated automatically. Similarly, Specia et al. (2009) proposed the use of a (1-4) Likert scale representing a translator’s perception on quality with regard to the degree of difficulty to fix a translation. However, sentence-level quality annotations appear to be subjective while agreement between annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL fo"
2020.eamt-1.16,J07-1003,0,0.0660234,"sentence while minimising the need for direct human annotation. For that purpose, we use transfer-learning to leverage large scale noisy annotations and small sets of high-quality human annotated translation errors to train QE models. Experiments on four language pairs and translations obtained by statistical and neural models show consistent gains over strong baselines. 1 Introduction Quality Estimation (QE) for Machine Translation (MT) is the task of predicting the overall quality of an automatically generated translation e.g., on either word, sentence or document level (Blatz et al., 2004; Ueffing and Ney, 2007). In opposition to automatic metrics and manual evaluation which rely on gold standard reference translations, QE models can produce quality estimates on unseen data, c 2020 The authors. This article is licensed under a Creative Commons 3.0 licence, no derivative works, attribution, CCBY-ND. and at runtime. QE has already proven its usefulness in many applications such as improving productivity in post-editing of MT, and recent neuralbased approaches to QE have been shown to provide promising performance in predicting quality of neural MT output (Fonseca et al., 2019). QE models are trained un"
2020.eamt-1.16,D16-1163,0,0.0285634,"appear to be subjective while agreement between annotators is generally low (Specia, 2011). More recently, sentence-level QE models are most typically trained on HTER scores (Bojar et al., 2013; Bojar et al., 2014; Bojar et al., 2015; Bojar et al., 2016; Bojar et al., 2017; Specia et al., 2018; Fonseca et al., 2019). Transfer-learning for QE Transfer-learning (TL) is a machine learning approach where models trained on a source task are adapted to a related target task (Pan et al., 2010; Yosinski et al., 2014). Transfer-learning methods have been widely used in NLP, e.g., machine translation (Zoph et al., 2016) and text classification (Howard and Ruder, 2018). Previous work on TL for QE focused on adapting models for labels produced by different annotators (Cohn and Specia, 2013; Shah and Specia, 2016) which is different to this work. More recent work on TL techniques for QE explore pre-trained word representations. This was first done by P OSTECH (Kim et al., 2017), best performing neural-based architecture in the QE shared task at WMT’17 (Bojar et al., 2017). P OSTECH re-purposes a recurrent neural network encoder pre-trained on large parallel corpora, to predict HTER scores using multi-task learn"
2020.emnlp-main.607,P19-1424,1,0.907824,"Missing"
2020.emnlp-main.607,P19-1636,1,0.874433,"Missing"
2020.emnlp-main.607,N19-1423,0,0.462082,"ls in LMTC, but without adequately tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they did not consider other confounding factors, such as using deeper neural networks at the same time, or alternative encodings of the hierarchy. Chalkidis et al. (2019b) also considered few and zero-shot learning, but"
2020.emnlp-main.607,W19-5032,1,0.844058,"o = tanh(Wo dl + bo )   pl = sigmoid (u3l )&gt; dlo (8) (9) DC - BIGRU - LWAN: The stack of GCN layers in GC BIGRU - LWAN (Eq. 5–6) can be turned into a plain two-layer Multi-Layer Perceptron (MLP), unaware of the label hierarchy, by setting Np,l = Nc,l = ∅. We call DC - BIGRU - LWAN the resulting (deeper than C - BIGRU - LWAN) variant of GC - BIGRU - LWAN. We use it as an ablation method to evaluate the impact of the GCN layers on performance. DN - BIGRU - LWAN: As an alternative approach to exploit the label hierarchy, we used a recent improvement of NODE 2 VEC (Grover and Leskovec, 2016) by Kotitsas et al. (2019) to obtain alternative hierarchy-aware label representations. NODE 2 VEC is similar to WORD 2 VEC (Mikolov et al., 2013), but pre-trains node embeddings instead of word embeddings, replacing WORD 2 VEC’s text windows by random walks on a graph (here the label hierarchy). 4 In a variant of DC - BIGRU - LWAN, dubbed DN - BIGRU - LWAN , we simply replace the initial centroid ul label representations of DC - BIGRU LWAN in Eq. 5 and 7 by the label representations gl generated by the NODE 2 VEC extension. DNC - BIGRU - LWAN: In another version of DC called DNC - BIGRU - LWAN, we replace the initial"
2020.emnlp-main.607,2021.ccl-1.108,0,0.0940112,"Missing"
2020.emnlp-main.607,D18-1211,1,0.90059,"Missing"
2020.emnlp-main.607,N18-1100,0,0.468757,"n MIMIC - III, only leaf nodes can be used, causing the label assignments to be much sparser (GAP: 0.27). In AMAZON 13 K, documents are tagged with leaf nodes, but it is assumed that all the parent nodes are also assigned, leading to dense label assignments (GAP: 0.86). Introduction Large-scale Multi-label Text Classification (LMTC) is the task of assigning a subset of labels from a large predefined set (typically thousands) to a given document. LMTC has a wide range of applications in Natural Language Processing (NLP), such as associating medical records with diagnostic and procedure labels (Mullenbach et al., 2018; Rios and Kavuluru, 2018), legislation with relevant legal concepts (Mencia and F¨urnkranzand, 2007; Chalkidis et al., 2019b), and products with categories (Lewis et al., 2004; Partalas et al., 2015). Apart from the large label space, LMTC datasets often have skewed label distributions (e.g., some labels have few or no training examples) and a label hierarchy with different labelling guidelines (e.g., they may require documents to be tagged only with leaf nodes, or they may allow both leaf and other nodes to be used). The latter affects graph-aware annotation proximity (GAP), i.e., the proxim"
2020.emnlp-main.607,D14-1162,0,0.087906,"Missing"
2020.emnlp-main.607,N18-1202,0,0.393403,"ely tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they did not consider other confounding factors, such as using deeper neural networks at the same time, or alternative encodings of the hierarchy. Chalkidis et al. (2019b) also considered few and zero-shot learning, but ignoring the label hierarchy."
2020.emnlp-main.607,D18-1352,0,0.267613,"nodes can be used, causing the label assignments to be much sparser (GAP: 0.27). In AMAZON 13 K, documents are tagged with leaf nodes, but it is assumed that all the parent nodes are also assigned, leading to dense label assignments (GAP: 0.86). Introduction Large-scale Multi-label Text Classification (LMTC) is the task of assigning a subset of labels from a large predefined set (typically thousands) to a given document. LMTC has a wide range of applications in Natural Language Processing (NLP), such as associating medical records with diagnostic and procedure labels (Mullenbach et al., 2018; Rios and Kavuluru, 2018), legislation with relevant legal concepts (Mencia and F¨urnkranzand, 2007; Chalkidis et al., 2019b), and products with categories (Lewis et al., 2004; Partalas et al., 2015). Apart from the large label space, LMTC datasets often have skewed label distributions (e.g., some labels have few or no training examples) and a label hierarchy with different labelling guidelines (e.g., they may require documents to be tagged only with leaf nodes, or they may allow both leaf and other nodes to be used). The latter affects graph-aware annotation proximity (GAP), i.e., the proximity of the gold labels in"
2020.emnlp-main.607,2020.tacl-1.54,0,0.018382,"esults in XMTC. Nonetheless, previous work has not thoroughly compared PLT-based methods to neural models in LMTC. In particular, only You et al. (2018) have compared PLT methods to neural models in LMTC, but without adequately tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they did not consider othe"
2020.emnlp-main.607,N19-5004,0,0.0216006,"also achieving top results in XMTC. Nonetheless, previous work has not thoroughly compared PLT-based methods to neural models in LMTC. In particular, only You et al. (2018) have compared PLT methods to neural models in LMTC, but without adequately tuning their parameters, nor considering few and zero-shot labels. More recently, You et al. (2019) introduced ATTENTION - XML, a new method primarily intended for XMTC, which combines PLTs with LWAN classifiers. Similarly to the rest of PLTbased methods, it has not been evaluated in LMTC. 2.2 The new paradigm of transfer learning Transfer learning (Ruder et al., 2019; Rogers et al., 2020), which has recently achieved state-of-the-art results in several NLP tasks, has only been considered in legal LMTC by Chalkidis et al. (2019b), who experimented with BERT (Devlin et al., 2019) and ELMO (Peters et al., 2018). Other BERT variants, e.g. ROBERTA (Liu et al., 2019), or BERT-based models have not been explored in LMTC so far. 2.3 Few and zero-shot learning in LMTC Finally, few and zero-shot learning in LMTC is mostly understudied. Rios and Kavuluru (2018) investigated the effect of encoding the hierarchy in these settings, with promising results. However, they"
2020.findings-emnlp.261,W19-1909,0,0.0384325,"Missing"
2020.findings-emnlp.261,D19-1371,0,0.206451,"ang et al., 2018), SQUAD (Rajpurkar et al., 2016), and RACE (Lai et al., 2017). Typically, transfer learning with language models requires a computationally heavy step where the language model is pre-trained on a large corpus and a less expensive step where the model is finetuned for downstream tasks. When using BERT, the first step can be omitted as the pre-trained models are publicly available. Being pre-trained on generic corpora (e.g., Wikipedia, Children’s Books, etc.) BERT has been reported to under-perform in specialised domains, such as biomedical or scientific text (Lee et al., 2019; Beltagy et al., 2019). To overcome this limitation there are two possible strategies; either further pre-train (FP) BERT on domain specific corpora, or pre-train BERT from scratch (SC) on domain specific corpora. Consequently, to employ BERT in specialised domains one may consider three alternative strategies before fine-tuning for the downstream task (Figure 1): (a) use BERT out of the box, (b) further pre-train (FP) BERT on domain-specific corpora, and (c) pre-train BERT from scratch (SC) on domain specific corpora with a new vocabulary of sub-word units. In this paper, we systematically explore strategies (a)–("
2020.findings-emnlp.261,W19-2209,1,0.887592,"Missing"
2020.findings-emnlp.261,D17-1082,0,0.0218727,"hree alternatives when employing BERT for NLP tasks in specialised domains: (a) use BERT out of the box, (b) further pre-train BERT (FP), and (c) pre-train BERT from scratch (SC). All strategies have a final fine-tuning step. Introduction Pre-trained language models based on Transformers (Vaswani et al., 2017), such as BERT (Devlin et al., 2019) and its variants (Liu et al., 2019; Yang et al., 2019; Lan et al., 2019), have achieved state-of-the-art results in several downstream NLP tasks on generic benchmark datasets, such as GLUE (Wang et al., 2018), SQUAD (Rajpurkar et al., 2016), and RACE (Lai et al., 2017). Typically, transfer learning with language models requires a computationally heavy step where the language model is pre-trained on a large corpus and a less expensive step where the model is finetuned for downstream tasks. When using BERT, the first step can be omitted as the pre-trained models are publicly available. Being pre-trained on generic corpora (e.g., Wikipedia, Children’s Books, etc.) BERT has been reported to under-perform in specialised domains, such as biomedical or scientific text (Lee et al., 2019; Beltagy et al., 2019). To overcome this limitation there are two possible stra"
2020.findings-emnlp.261,W18-5446,0,0.072335,"Missing"
2020.findings-emnlp.261,N16-1174,0,0.0685726,"Missing"
2020.tacl-1.35,D16-1025,0,0.021127,"the advent of neural models, Machine Translation (MT) systems have made substantial progress, reportedly achieving near-human quality for high-resource language pairs (Hassan et al., 2018; Barrault et al., 2019). However, translation quality is not consistent across language pairs, domains, and datasets. This is problematic for low-resource scenarios, where there is not enough training data and translation quality significantly lags behind. Additionally, neural MT (NMT) systems can be deceptive to the end user as they can generate fluent translations that differ in meaning from the original (Bentivogli et al., 2016; Castilho et al., 2017). 539 Transactions of the Association for Computational Linguistics, vol. 8, pp. 539–555, 2020. https://doi.org/10.1162/tacl a 00330 Action Editor: Stefan Riezler. Submission batch: 1/2020; Revision batch: 4/2020; Published 9/2020. c 2020 Association for Computational Linguistics. Distributed under a CC-BY 4.0 license. probability mass to predictions that are far from the training data (Gal and Ghahramani, 2016). To overcome such deficiencies, we propose ways to exploit output distributions beyond the top-1 prediction by exploring uncertainty quantification methods for"
2020.tacl-1.35,2020.eamt-1.16,1,0.787809,"/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding to the best epoch as identified by the metric of refe"
2020.tacl-1.35,C04-1046,0,0.353281,"Missing"
2020.tacl-1.35,W17-4755,0,0.0539918,"Missing"
2020.tacl-1.35,W16-2302,0,0.0478511,"Missing"
2020.tacl-1.35,D19-1308,0,0.0163557,"experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and model probabilities (orange), and Met"
2020.tacl-1.35,W14-3348,0,0.0203514,"ity outputs on NMT training with back translations. Second, we measure lexical variation between the MT outputs generated for the same source segment when running inference with dropout. We posit that differences between likely MT hypotheses may also capture uncertainty and potential ambiguity and complexity of the original sentence. We compute an average similarity score (sim) between the set H of translation hypotheses: |H ||H| 1 XX D-Lex-Sim = sim(hi , hj ) C i=1 j =1 where hi , hj ∈ H, i 6= j and C = 2−1 |H|(|H |− 1) is the number of pairwise comparisons for |H| hypotheses. We use Meteor (Denkowski and Lavie, 2014) to compute similarity scores. 3.3 Attention Attention weights represent the strength of connection between source and target tokens, which may be indicative of translation quality (Rikters and Fishel, 2017). One way to measure it is to compute the entropy of the attention distribution: I Att-Ent = − J 1 XX αji log αji I i=1 j =1 where α represents attention weights, I is the number of target tokens and J is the number of source tokens. This mechanism can be applied to any NMT model with encoder-decoder attention. We focus on attention in Transformer models, as it is currently the most widely"
2020.tacl-1.35,N19-1423,0,0.0129677,"ration (Guo et al., 2017). On the other hand, due to the small amount of training data the model can overfit, resulting in inferior results both in terms of translation quality and correlation. It is noteworthy, however, that supervised QE system suffers a larger drop in performance than unsupervised indicators, as its 6 We note that PredEst models are systematically and significantly outperformed by BERT-BiRNN. This is not surprising, as large-scale pretrained representations have been shown to boost model performance for QE (Kepler et al., 2019a) and other natural language processing tasks (Devlin et al., 2019). 7 Models for these languages were trained using Transformer-Big architecture from Vaswani et al. (2017). Low-resource Mid-resource High-resource Method Si-En Ne-En Et-En Ro-En En-De En-Zh I TP Softmax-Ent (-) Sent-Std (-) 0.399 0.457 0.418 0.482 0.528 0.472 0.486 0.421 0.471 0.647 0.613 0.595 0.208 0.147 0.264 0.257 0.251 0.301 II D-TP D-Var (-) D-Combo (-) D-Lex-Sim 0.460 0.307 0.286 0.513 0.558 0.299 0.418 0.600 0.642 0.356 0.475 0.612 0.693 0.332 0.383 0.669 0.259 0.164 0.189 0.172 0.321 0.232 0.225 0.313 III AW : Ent-Min (-) AW : Ent-Avg (-) AW : best head/layer (-) 0.097 0.10 0.255 0.26"
2020.tacl-1.35,P18-1069,0,0.0928735,"ed. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defi"
2020.tacl-1.35,C16-1133,0,0.0483943,"Missing"
2020.tacl-1.35,C16-1294,0,0.0197172,"me of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 201"
2020.tacl-1.35,N15-1124,0,0.135368,"translation quality can be extracted from multihead attention. To evaluate our approach in challenging settings, we collect a new dataset for QE with 6 language pairs representing NMT training in high, medium, and low-resource scenarios. To reduce the chance of overfitting to particular domains, our dataset is constructed from Wikipedia documents. We annotate 10K segments per language pair. By contrast to the vast majority of work on QE that uses semi-automatic metrics based on post-editing distance as gold standard, we perform quality labeling based on the Direct Assessment (DA) methodology (Graham et al., 2015b), which has been widely used for popular MT evaluation campaigns in the recent years. At the same time, the collected data differs from the existing datasets annotated with DA judgments for the well known WMT Metrics task1 in two important ways: We provide enough data to train supervised QE models and access to the NMT systems used to generate the translations, thus allowing for further exploration of the glass-box unsupervised approach to QE for NMT introduced in this paper. Our main contributions can be summarized as follows: (i) A new, large-scale dataset for sentencelevel2 QE annotated w"
2020.tacl-1.35,W13-2305,0,0.432265,"tion of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that closely preserves the semantics of the source sentence; and 91–100, a perfect translation. Each segment was evaluated indep"
2020.tacl-1.35,W19-6721,0,0.0372157,"Missing"
2020.tacl-1.35,D19-1632,1,0.899651,"Missing"
2020.tacl-1.35,P19-3020,0,0.0842974,"Missing"
2020.tacl-1.35,W17-4763,0,0.168703,"London 5 Facebook AI 1 {m.fomicheva,f.blain,n.aletras,l.specia}@sheffield.ac.uk 2 {ssun32}@jhu.edu 3 {lisa.yankovskaya,fishel}@ut.ee 5 {fguzman,vishrav}@fb.com Abstract Thus, it is crucial to have a feedback mechanism to inform users about the trustworthiness of a given MT output. Quality estimation (QE) aims to predict the quality of the output provided by an MT system at test time when no gold-standard human translation is available. State-of-the-art (SOTA) QE models require large amounts of parallel data for pretraining and in-domain translations annotated with quality labels for training (Kim et al., 2017a; Fonseca et al., 2019). However, such large collections of data are only available for a small set of languages in limited domains. Current work on QE typically treats the MT system as a black box. In this paper we propose an alternative glass-box approach to QE that allows us to address the task as an unsupervised problem. We posit that encoder-decoder NMT models (Sutskever et al., 2014; Bahdanau et al., 2015; Vaswani et al., 2017) offer a rich source of information for directly estimating translation quality: (a) the output probability distribution from the NMT system (i.e., the probabilit"
2020.tacl-1.35,K18-1056,0,0.0255158,"del ensembling). For this smallscale experiment we focus on Et-En. For each system variant we translated 400 sentences from the test partition of our dataset and collected the DA accordingly. As baseline, we use a standard Transformer model with beam search decoding. All system variants are trained using Fairseq implementation (Ott et al., 2019) for 30 epochs, with the best checkpoint chosen according to the validation loss. First, we consider three system variants with differences in architecture or training: RNN-based NMT (Bahdanau et al., 2015; Luong et al., 2015), Mixture of Experts (MoE, He et al., 2018; Shen et al., 2019; Cho et al., 2019), and model ensemble (Garmash and Monz, 2016). Shen et al. (2019) use the MoE framework to capture the inherent uncertainty of the MT task where the same input sentence can have multiple Method r DA TP-Beam 0.482 58.88 TP-Sampling TP-Diverse beam 0.533 0.424 42.02 55.12 TP-RNN TP-Ensemble TP-MoE 0.502 0.538 0.449 43.63 61.19 51.20 D-TP 0.526 58.88 Table 4: Pearson correlation (r ) between sequence-level output probabilities (TP) and average DA for translations generated by different NMT systems. Figure 3: Pearson correlation between translation quality and"
2020.tacl-1.35,2005.mtsummit-papers.11,0,0.0830211,"nese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conveys the overall meaning of the source but contains typos or grammatical errors; 70–90, a translation that clos"
2020.tacl-1.35,C18-1266,1,0.840227,"he DA judgments are available at https://github. com/facebookresearch/mlqe. 545 5 Experiments and Results Below we analyze how our unsupervised QE indicators correlate with human judgments. 5.1 Settings Benchmark Supervised QE Systems We compare the performance of the proposed unsupervised QE indicators against the best performing supervised approaches with available open-source implementation, namely, the Predictor-Estimator (PredEst) architecture (Kim et al., 2017b) provided by OpenKiwi toolkit (Kepler et al., 2019b), and an improved version of the BiRNN model provided by DeepQuest toolkit (Ive et al., 2018), which we refer to as BERT-BiRNN (Blain et al., 2020). PredEst. We trained PredEst models (see §2) using the same parameters as in the default configurations provided by Kepler et al. (2019b). Predictor models were trained for 6 epochs on the same training and development data as the NMT systems, while the Estimator models were trained for 10 epochs on the training and development sets of our dataset (see §4). Unlike Kepler et al. (2019b), the Estimator was not trained using multitask learning, as our dataset currently does not contain any word-level annotation. We use the model corresponding"
2020.tacl-1.35,1983.tc-1.13,0,0.671274,"Missing"
2020.tacl-1.35,D15-1166,0,0.133746,"Missing"
2020.tacl-1.35,J82-2005,0,0.698459,"Missing"
2020.tacl-1.35,N19-4009,0,0.237923,"nd the empirical frequencies of the predicted labels, or by assessing generalization of uncertainty under domain shift (see §6). Only a few studies have analyzed calibration in NMT and they came to contradictory conclusions. Kumar and Sarawagi (2019) measure calibration error by comparing model probabilities and the percentage of times NMT output matches reference translation, and conclude that NMT probabilities are poorly calibrated. However, the calibration error metrics they use are designed for binary classification tasks and cannot be easily transferred to NMT (Kuleshov and Liang, 2015). Ott et al. (2019) analyze uncertainty in NMT by comparing predictive probability distributions with the empirical distribution observed in human translation data. They conclude that NMT models ministic NMT (§3.1) or (ii) using uncertainty quantification (§3.2), and (iii) attention weights (§3.3). are well calibrated. However, this approach is limited by the fact that there are many possible correct translations for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides"
2020.tacl-1.35,W18-6450,0,0.0135015,"he current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WMT translation tasks, as well as for assessing the performance of 541 reference-based automatic MT evaluation metrics at the WMT Metrics Task (Bojar et al., 2016, 2017; Ma et al., 2018, 2019). Existing datasets with sentence-level DA judgments from the WMT Metrics Task could in principle be used for benchmarking QE systems. However, they contain only a few hundred segments per language pair and thus hardly allow for training supervised systems, as illustrated by the weak correlation results for QE on DA judgments based on the Metrics Task data recently reported by Fonseca et al. (2019). Furthermore, for each language pair the data contains translations from a number of MT systems often using different architectures, and these MT systems are not readily available, making it"
2020.tacl-1.35,W18-6301,0,0.1208,"e sampled documents and then translated them into English using the MT models described below. For German and Chinese we selected 20K sentences from the top 100 documents in English Wikipedia. To ensure sufficient representation of high- and low-quality translations for high-resource language pairs, we selected the sentences with minimal lexical overlap with respect to the NMT training data. NMT systems For medium- and high-resource language pairs we trained the MT models based on the standard Transformer architecture (Vaswani et al., 2017) and followed the implementation details described in Ott et al. (2018b). We used publicly available MT datasets such as Paracrawl (Espl`a et al., 2019) and Europarl (Koehn, 2005). 4 DA Judgments We followed the FLORES setup (Guzm´an et al., 2019), which presents a form of DA (Graham et al., 2013). The annotators are asked to rate each sentence from 0–100 according to the perceived translation quality. Specifically, the 0–10 range represents an incorrect translation; 11–29, a translation with few correct keywords, but the overall meaning is different from the source; 30–50, a translation with major mistakes; 51–69, a translation which is understandable and conve"
2020.tacl-1.35,W19-5302,0,0.0298412,"Missing"
2020.tacl-1.35,W12-3116,0,0.0450527,"Missing"
2020.tacl-1.35,2021.eacl-main.115,1,0.859327,"Missing"
2020.tacl-1.35,W12-3114,0,0.0324498,"system. Existing work on glass-box QE is limited to features extracted from statistical MT, such as language model probabilities or number of hypotheses in the n-best list (Blatz et al., 2004; Specia et al., 2013). The few approaches for unsupervised QE are also inspired by the work on statistical MT 2 While the paper covers QE at sentence level, the extension of our unsupervised metrics to word-level QE would be straightforward and we leave it for future work. 1 h t t p : / /www.statmt.org/wmt19/metricstask.html. 540 and perform significantly worse than supervised approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based"
2020.tacl-1.35,D15-1182,0,0.0718096,"Missing"
2020.tacl-1.35,W19-8671,0,0.0592008,"Missing"
2020.tacl-1.35,2006.amta-papers.25,0,0.114,"nterpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this metric for assessing the performance of QE systems has been shown to be questionable (Graham et al., 2016). The current practice in MT evaluation is the so-called Direct Assessment (DA) of MT quality (Graham et al., 2015b), where raters evaluate the MT on a continuous 1–100 scale. This method has been shown to improve the reproducibility of manual evaluation and to provide a more reliable gold standard for automatic evaluation metrics (Graham et al., 2015a). DA methodology is currently used for manual evaluation of MT quality at the WM"
2020.tacl-1.35,W19-4808,0,0.0153618,"l information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the re"
2020.tacl-1.35,P13-4014,1,0.932381,"Missing"
2020.tacl-1.35,P19-1580,0,0.162993,"models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challenging to extract information from attention weights in Transformer (see §5). To the best of our knowledge, our work is the first on glass-box unsupervised QE for NMT that performs competitively with respect to the SOTA supervised systems. QE Datasets The performance of QE systems has been typically assessed using the semi-automatic Human-mediated Translation Edit Rate (Snover et al., 2006) metric as gold standard. However, the reliability of this met"
2020.tacl-1.35,2009.eamt-1.5,1,0.860291,"t unsupervised QE indicators obtained from well-calibrated NMT model probabilities rival strong supervised SOTA models in terms of correlation with human judgments. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require"
2020.tacl-1.35,W18-6465,0,0.0275911,"s. 2 Related Work QE QE is typically addressed as a supervised machine learning task where the goal is to predict MT quality in the absence of reference translation. Traditional feature-based approaches relied on manually designed features, extracted from the MT system (glass-box features) or obtained from the source and translated sentences, as well as external resources, such as monolingual or parallel corpora (black-box features) (Specia et al., 2009). Currently, the best performing approaches to QE use NNs to learn useful representations for source and target sentences (Kim et al., 2017b; Wang et al., 2018; Kepler et al., 2019a). A notable example is the Predictor-Estimator (PredEst) model (Kim et al., 2017b), which consists of an encoder-decoder RNN (predictor) trained on parallel data for a word prediction task and a unidirectional RNN (estimator) that produces quality estimates leveraging the context representations generated by the predictor. Despite achieving strong performances, neural-based approaches are resource-heavy and require a significant amount of in-domain labeled data for training. They do not use any internal information from the MT system. Existing work on glass-box QE is lim"
2020.tacl-1.35,D19-1073,0,0.314906,"ns for a given sentence and only one human translation is available in practice. Although the goal of this paper is to devise an unsupervised solution for the QE task, the analysis presented here provides new insights into calibration in NMT. Different from existing work, we study the relation between model probabilities and human judgments of translation correctness. Uncertainty quantification methods have been successfully applied to various practical tasks, for example, neural semantic parsing (Dong et al., 2018), hate speech classification (Miok et al., 2019), or back-translation for NMT (Wang et al., 2019). Wang et al. (2019), whose work is the closest to our work, explore a small set of uncertaintybased metrics to minimize the weight of erroneous synthetic sentence pairs for back translation in NMT. However, improved NMT training with weighted synthetic data does not necessarily imply better prediction of MT quality. In fact, metrics that Wang et al. (2019) report to perform the best for back-translation do not perform well for QE (see §3.2). 3.1 Exploiting the Softmax Distribution We start by defining a simple QE measure based on sequence-level translation probability normalized by length: TP"
2020.tacl-1.35,W18-6466,1,0.857973,"d approaches (Popovi´c, 2012; Moreau and Vogel, 2012; Etchegoyhen et al., 2018). For example, Etchegoyhen et al. (2018) use lexical translation probabilities from word alignment models and language model probabilities. Their unsupervised approach averages these features to produce the final score. However, it is largely outperformed by the neural-based supervised QE systems (Specia et al., 2018). The only works that explore internal information from neural models as an indicator of translation quality rely on the entropy of attention weights in RNN-based NMT systems (Rikters and Fishel, 2017; Yankovskaya et al., 2018). However, attention-based indicators perform competitively only when combined with other QE features in a supervised framework. Furthermore, this approach is not directly applicable to the SOTA Transformer model that uses multihead attention mechanism. Recent work on attention interpretability showed that attention weights in Transformer networks might not be readily interpretable (Vashishth et al., 2019; Vig and Belinkov, 2019). Voita et al. (2019) show that different attention heads of Transformer have different functions and some of them are more important than others. This makes it challe"
2020.tacl-1.35,W18-6451,1,\N,Missing
2020.tacl-1.35,W19-5301,1,\N,Missing
2020.tacl-1.35,W19-5401,1,\N,Missing
2021.acl-long.40,W16-1601,0,0.0504951,"Missing"
2021.acl-long.40,W17-5221,0,0.0584357,"Missing"
2021.acl-long.40,2020.emnlp-main.263,0,0.38653,"and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018). Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017). Chen and Ji (2020) propose learning a variational word mask to improve model interpretability. Finally, extracting a short snippet from the original input text (rationale) and using it to make a prediction has been recently proposed (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Chalkidis et al., 2021). Nguyen (2018) and Atanasova et al. (2020) compare explanations produced by different approaches, showing that in most cases gradientbased approaches outperform sparse linear metamodels. 2.2 Attention as Explanation robust explanations. In contrast to Jain and Wallace (2019), Wiegreffe and Pinter (2019) and Vashishth et al. (2019) demonstrate that attention weights can in certain cases provide robust explanations. Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations. They test this through manipulating the attention mechanism by penalising words a priori known to be relevant to the t"
2021.acl-long.40,D18-1508,0,0.023394,"classification tasks. Focusing also on recurrent-encoders, Mohankumar et al. (2020) introduce a modification to recurrent encoders to reduce repetitive information across different words in the input to improve faithfulness of explanations. To the best of our knowledge, no previous work has attempted to improve the faithfulness of attention-based explanations across different encoders for text classification by inducing taskspecific information to the attention weights. Attention weights have been extensively used to interpret model predictions in NLP; i.e. (Cho et al., 2014; Xu et al., 2015; Barbieri et al., 2018; Ghaeini et al., 2018). However, the hypothesis that attention should be used as explanation had not been explicitly studied until recently. Jain and Wallace (2019) first explored the effectiveness of attention explanations. They show that adversary attention distributions can yield equivalent predictions with the original attention distribution, suggesting that attention weights do not offer 478 3 Neural Text Classification Models In a typical neural model with attention for text classification; one-hot-encoded tokens xi P R|V| are first mapped to embeddings ei P Rd , where i P r1, ..., ts d"
2021.acl-long.40,P19-1284,0,0.0406006,"Missing"
2021.acl-long.40,2020.emnlp-main.347,0,0.0403183,"tworks can be obtained by identifying which parts of the input are important for a given prediction. One way is to use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018). Another way is to calculate the difference in a model’s prediction between keeping ˇ and omitting an input token (Robnik-Sikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018). Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017). Chen and Ji (2020) propose learning a variational word mask to improve model interpretability. Finally, extracting a short snippet from the original input text (rationale) and using it to make a prediction has been recently proposed (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Chalkidis et al., 2021). Nguyen (2018) and Atanasova et al. (2020) compare explanations produced by different approaches, showing that in most cases gradientbased approaches outperform sparse linear metamodels. 2.2 Attention as Explanation robust explanations. In contrast to Jain and Wallace (201"
2021.acl-long.40,D17-1047,0,0.0238674,"y that makes them opaque and hard to interpret by humans which usually treat 1 Code is available at: https://github.com/ GChrysostomou/tasc.git them as black boxes (Zhang et al., 2018; Linzen et al., 2019). Attention mechanisms (Bahdanau et al., 2015) produce a probability distribution over the input to compute a vector representation of the entire token sequence as the weighted sum of its constituent vectors. A common practice is to provide explanations for a given prediction and qualitative model analysis by assigning importance to input tokens using scores provided by attention mechanisms (Chen et al., 2017; Wang et al., 2016; Jain et al., 2020; Sun and Lu, 2020) as a mean towards model interpretability (Lipton, 2016; Miller, 2019). A faithful explanation is one that accurately represents the true reasoning behind a model’s prediction (Jacovi and Goldberg, 2020). A series of recent studies illustrate that explanations obtained by attention weights do not always provide faithful explanations (Serrano and Smith, 2019) while different text encoders can affect attention interpretability, e.g. results can differ when using a recurrent or non-recurrent encoder (Wiegreffe and Pinter, 2019). A limitatio"
2021.acl-long.40,N19-1423,0,0.215974,"original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attentionbased explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. Finally, we demonstrate that TaSc consistently provides more faithful attentionbased explanations compared to three widelyused interpretability techniques.1 1 Introduction Natural Language Processing (NLP) approaches for text classification are often underpinned by large neural network models (Cho et al., 2014; Devlin et al., 2019). Despite the high accuracy and efficiency of these models in dealing with large amounts of data, an important problem is their increased complexity that makes them opaque and hard to interpret by humans which usually treat 1 Code is available at: https://github.com/ GChrysostomou/tasc.git them as black boxes (Zhang et al., 2018; Linzen et al., 2019). Attention mechanisms (Bahdanau et al., 2015) produce a probability distribution over the input to compute a vector representation of the entire token sequence as the weighted sum of its constituent vectors. A common practice is to provide explana"
2021.acl-long.40,D18-1537,0,0.0298684,"Missing"
2021.acl-long.40,2020.lrec-1.220,0,0.591815,"provide robust explanations. Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations. They test this through manipulating the attention mechanism by penalising words a priori known to be relevant to the task, showing that the predictive performance remain relatively unaffected. Sen et al. (2020) assess the plausibility of attention weights by correlating them with manually annotated explanation heat-maps, where plausibility refers to how convincing an explanation is to humans (Jacovi and Goldberg, 2020). However, Jacovi and Goldberg (2020) and Grimsley et al. (2020) suggest caution with interpreting the results of these experiments as they do not test the faithfulness of explanations (e.g. an explanation can be non-plausible but faithful or vice-versa). Serrano and Smith (2019) test the faithfulness of attention-based explanations by removing tokens to observe how fast a decision flip happens. Results show that gradient attention-based rankings (i.e. combining an attention weight with its gradient) better predict word importance for model predictions, compared to just using the attention weights. Tutek and Snajder (2020) propose a method to improve the f"
2021.acl-long.40,2020.acl-main.386,0,0.173705,"ence, University of Sheffield United Kingdom {gchrysostomou1, n.aletras}@sheffield.ac.uk Abstract Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attentionbased explanations across two attention mechanisms, five encoders and five text classification datasets without sacrificing predictive performance. Finally, we demonstrate that TaSc"
2021.acl-long.40,N19-1357,0,0.352119,"ness of Attention-based Explanations with Task-specific Information for Text Classification George Chrysostomou Nikolaos Aletras Department of Computer Science, University of Sheffield United Kingdom {gchrysostomou1, n.aletras}@sheffield.ac.uk Abstract Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attentionbased explanations acr"
2021.acl-long.40,2020.acl-main.409,0,0.0447028,"nterpret by humans which usually treat 1 Code is available at: https://github.com/ GChrysostomou/tasc.git them as black boxes (Zhang et al., 2018; Linzen et al., 2019). Attention mechanisms (Bahdanau et al., 2015) produce a probability distribution over the input to compute a vector representation of the entire token sequence as the weighted sum of its constituent vectors. A common practice is to provide explanations for a given prediction and qualitative model analysis by assigning importance to input tokens using scores provided by attention mechanisms (Chen et al., 2017; Wang et al., 2016; Jain et al., 2020; Sun and Lu, 2020) as a mean towards model interpretability (Lipton, 2016; Miller, 2019). A faithful explanation is one that accurately represents the true reasoning behind a model’s prediction (Jacovi and Goldberg, 2020). A series of recent studies illustrate that explanations obtained by attention weights do not always provide faithful explanations (Serrano and Smith, 2019) while different text encoders can affect attention interpretability, e.g. results can differ when using a recurrent or non-recurrent encoder (Wiegreffe and Pinter, 2019). A limitation of attention as an indicator of inpu"
2021.acl-long.40,D16-1011,0,0.0605853,"Missing"
2021.acl-long.40,N16-1082,0,0.118333,"ased explanations with TaSc consistently outperform explanations obtained from two gradient-based and a word-erasure explanation approaches (§7). 2 2.1 Related Work Model Interpretability Explanations for neural networks can be obtained by identifying which parts of the input are important for a given prediction. One way is to use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018). Another way is to calculate the difference in a model’s prediction between keeping ˇ and omitting an input token (Robnik-Sikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018). Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017). Chen and Ji (2020) propose learning a variational word mask to improve model interpretability. Finally, extracting a short snippet from the original input text (rationale) and using it to make a prediction has been recently proposed (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Chalkidis et al., 2021). Nguyen (2018) and Atanasova et al. (2020) compare expla"
2021.acl-long.40,P11-1015,0,0.0392414,"et al., 2020; Atanasova et al., 2020). Convolutional TaSc (Conv-TaSc) 4 We also tried max and mean-pooling or using the ui directly instead of si in early experimentation resulting in lower results. 5 We only apply Conv-TaSc over Lin-TaSc to keep the mechanism relatively lightweight. Note that Feat-TaSc learns an extra matrix of equal size to the embedding matrix. 6 See CNN configurations in Appendix A. Attention-based Importance Metrics 6 Experiments and Results 6.1 Data We use five datasets for text classification following Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al., 2011); (iii) ADR 7 Note that Jacovi and Goldberg (2020) argue that a human evaluation is not an appropriate method to test faithfulness. 8 Serrano and Smith (2019) show that gradient-based attention ranking metrics (∇α, α∇α) are better in providing faithful explanations compared to just using attention (α). 480 Dataset Av. |W | |V| SST ADR IMDB AG MIMIC 20 22 185 34 2,180 13,686 6,716 12,147 14,573 16,277 Data Splits Train/Dev/Test 6,920 / 872 / 1,821 14,452 / 2,551 / 4,251 17,212 / 4,304 / 4,363 60,895 / 7,145 / 3,960 4,654 / 822 / 1,369 Table 1: Dataset statistics including average words per inst"
2021.acl-long.40,2020.acl-main.387,0,0.109791,"r vice-versa). Serrano and Smith (2019) test the faithfulness of attention-based explanations by removing tokens to observe how fast a decision flip happens. Results show that gradient attention-based rankings (i.e. combining an attention weight with its gradient) better predict word importance for model predictions, compared to just using the attention weights. Tutek and Snajder (2020) propose a method to improve the faithfulness of attention explanations when using recurrent encoders by introducing a word-level objective to sequence classification tasks. Focusing also on recurrent-encoders, Mohankumar et al. (2020) introduce a modification to recurrent encoders to reduce repetitive information across different words in the input to improve faithfulness of explanations. To the best of our knowledge, no previous work has attempted to improve the faithfulness of attention-based explanations across different encoders for text classification by inducing taskspecific information to the attention weights. Attention weights have been extensively used to interpret model predictions in NLP; i.e. (Cho et al., 2014; Xu et al., 2015; Barbieri et al., 2018; Ghaeini et al., 2018). However, the hypothesis that attentio"
2021.acl-long.40,N18-1097,0,0.193691,"pared to using vanilla attention in a set of standard interpretability benchmarks, without sacrificing predictive performance (§6); • We demonstrate that attention-based explanations with TaSc consistently outperform explanations obtained from two gradient-based and a word-erasure explanation approaches (§7). 2 2.1 Related Work Model Interpretability Explanations for neural networks can be obtained by identifying which parts of the input are important for a given prediction. One way is to use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018). Another way is to calculate the difference in a model’s prediction between keeping ˇ and omitting an input token (Robnik-Sikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018). Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017). Chen and Ji (2020) propose learning a variational word mask to improve model interpretability. Finally, extracting a short snippet from the original input text (rationale) and using it to make a prediction has been recently proposed ("
2021.acl-long.40,D14-1162,0,0.0883275,"Missing"
2021.acl-long.40,2020.acl-main.432,0,0.15663,"tionale) and using it to make a prediction has been recently proposed (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Chalkidis et al., 2021). Nguyen (2018) and Atanasova et al. (2020) compare explanations produced by different approaches, showing that in most cases gradientbased approaches outperform sparse linear metamodels. 2.2 Attention as Explanation robust explanations. In contrast to Jain and Wallace (2019), Wiegreffe and Pinter (2019) and Vashishth et al. (2019) demonstrate that attention weights can in certain cases provide robust explanations. Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations. They test this through manipulating the attention mechanism by penalising words a priori known to be relevant to the task, showing that the predictive performance remain relatively unaffected. Sen et al. (2020) assess the plausibility of attention weights by correlating them with manually annotated explanation heat-maps, where plausibility refers to how convincing an explanation is to humans (Jacovi and Goldberg, 2020). However, Jacovi and Goldberg (2020) and Grimsley et al. (2020) suggest caution with interp"
2021.acl-long.40,N16-3020,0,0.34012,"and faithful attention-based explanations compared to using vanilla attention in a set of standard interpretability benchmarks, without sacrificing predictive performance (§6); • We demonstrate that attention-based explanations with TaSc consistently outperform explanations obtained from two gradient-based and a word-erasure explanation approaches (§7). 2 2.1 Related Work Model Interpretability Explanations for neural networks can be obtained by identifying which parts of the input are important for a given prediction. One way is to use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017; Nguyen, 2018). Another way is to calculate the difference in a model’s prediction between keeping ˇ and omitting an input token (Robnik-Sikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018). Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017). Chen and Ji (2020) propose learning a variational word mask to improve model interpretability. Finally, extracting a short snippet from the original input text (rationale) and using it to make a"
2021.acl-long.40,2020.acl-main.419,0,0.0273887,"t cases gradientbased approaches outperform sparse linear metamodels. 2.2 Attention as Explanation robust explanations. In contrast to Jain and Wallace (2019), Wiegreffe and Pinter (2019) and Vashishth et al. (2019) demonstrate that attention weights can in certain cases provide robust explanations. Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations. They test this through manipulating the attention mechanism by penalising words a priori known to be relevant to the task, showing that the predictive performance remain relatively unaffected. Sen et al. (2020) assess the plausibility of attention weights by correlating them with manually annotated explanation heat-maps, where plausibility refers to how convincing an explanation is to humans (Jacovi and Goldberg, 2020). However, Jacovi and Goldberg (2020) and Grimsley et al. (2020) suggest caution with interpreting the results of these experiments as they do not test the faithfulness of explanations (e.g. an explanation can be non-plausible but faithful or vice-versa). Serrano and Smith (2019) test the faithfulness of attention-based explanations by removing tokens to observe how fast a decision fli"
2021.acl-long.40,P19-1282,0,0.611979,"Explanations with Task-specific Information for Text Classification George Chrysostomou Nikolaos Aletras Department of Computer Science, University of Sheffield United Kingdom {gchrysostomou1, n.aletras}@sheffield.ac.uk Abstract Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attentionbased explanations across two attention mechani"
2021.acl-long.40,D13-1170,0,0.00485773,"Vashishth et al., 2019; Grimsley et al., 2020; Atanasova et al., 2020). Convolutional TaSc (Conv-TaSc) 4 We also tried max and mean-pooling or using the ui directly instead of si in early experimentation resulting in lower results. 5 We only apply Conv-TaSc over Lin-TaSc to keep the mechanism relatively lightweight. Note that Feat-TaSc learns an extra matrix of equal size to the embedding matrix. 6 See CNN configurations in Appendix A. Attention-based Importance Metrics 6 Experiments and Results 6.1 Data We use five datasets for text classification following Jain and Wallace (2019): (i) SST (Socher et al., 2013); (ii) IMDB (Maas et al., 2011); (iii) ADR 7 Note that Jacovi and Goldberg (2020) argue that a human evaluation is not an appropriate method to test faithfulness. 8 Serrano and Smith (2019) show that gradient-based attention ranking metrics (∇α, α∇α) are better in providing faithful explanations compared to just using attention (α). 480 Dataset Av. |W | |V| SST ADR IMDB AG MIMIC 20 22 185 34 2,180 13,686 6,716 12,147 14,573 16,277 Data Splits Train/Dev/Test 6,920 / 872 / 1,821 14,452 / 2,551 / 4,251 17,212 / 4,304 / 4,363 60,895 / 7,145 / 3,960 4,654 / 822 / 1,369 Table 1: Dataset statistics i"
2021.acl-long.40,2020.acl-main.312,0,0.0371267,"which usually treat 1 Code is available at: https://github.com/ GChrysostomou/tasc.git them as black boxes (Zhang et al., 2018; Linzen et al., 2019). Attention mechanisms (Bahdanau et al., 2015) produce a probability distribution over the input to compute a vector representation of the entire token sequence as the weighted sum of its constituent vectors. A common practice is to provide explanations for a given prediction and qualitative model analysis by assigning importance to input tokens using scores provided by attention mechanisms (Chen et al., 2017; Wang et al., 2016; Jain et al., 2020; Sun and Lu, 2020) as a mean towards model interpretability (Lipton, 2016; Miller, 2019). A faithful explanation is one that accurately represents the true reasoning behind a model’s prediction (Jacovi and Goldberg, 2020). A series of recent studies illustrate that explanations obtained by attention weights do not always provide faithful explanations (Serrano and Smith, 2019) while different text encoders can affect attention interpretability, e.g. results can differ when using a recurrent or non-recurrent encoder (Wiegreffe and Pinter, 2019). A limitation of attention as an indicator of input importance is tha"
2021.acl-long.40,2020.blackboxnlp-1.10,0,0.407726,"nce in a model’s prediction between keeping ˇ and omitting an input token (Robnik-Sikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018). Input importance is also measured using the gradients computed with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Arras et al., 2016; Sundararajan et al., 2017). Chen and Ji (2020) propose learning a variational word mask to improve model interpretability. Finally, extracting a short snippet from the original input text (rationale) and using it to make a prediction has been recently proposed (Lei et al., 2016; Bastings et al., 2019; Treviso and Martins, 2020; Jain et al., 2020; Chalkidis et al., 2021). Nguyen (2018) and Atanasova et al. (2020) compare explanations produced by different approaches, showing that in most cases gradientbased approaches outperform sparse linear metamodels. 2.2 Attention as Explanation robust explanations. In contrast to Jain and Wallace (2019), Wiegreffe and Pinter (2019) and Vashishth et al. (2019) demonstrate that attention weights can in certain cases provide robust explanations. Pruthi et al. (2020) also investigate the ability of attention weights to provide plausible explanations. They test this through manipula"
2021.acl-long.40,2020.repl4nlp-1.17,0,0.492964,"aithful explanation is one that accurately represents the true reasoning behind a model’s prediction (Jacovi and Goldberg, 2020). A series of recent studies illustrate that explanations obtained by attention weights do not always provide faithful explanations (Serrano and Smith, 2019) while different text encoders can affect attention interpretability, e.g. results can differ when using a recurrent or non-recurrent encoder (Wiegreffe and Pinter, 2019). A limitation of attention as an indicator of input importance is that it refers to the word in context due to information mixing in the model (Tutek and Snajder, 2020). Motivated by this, we aim to improve the effectiveness of neural models in providing more faithful attention-based explanations for text classification, by introducing noncontextualised information in the model. Our contributions are as follows: • We introduce three Task-Scaling (TaSc) mechanisms (§4), a family of encoder-independent components that learn task-specific noncontextualised importance scores for each word in the vocabulary to scale the original attention weights which can be easily ported to any neural architecture; 477 Proceedings of the 59th Annual Meeting of the Association f"
2021.acl-long.40,D16-1058,0,0.02805,"paque and hard to interpret by humans which usually treat 1 Code is available at: https://github.com/ GChrysostomou/tasc.git them as black boxes (Zhang et al., 2018; Linzen et al., 2019). Attention mechanisms (Bahdanau et al., 2015) produce a probability distribution over the input to compute a vector representation of the entire token sequence as the weighted sum of its constituent vectors. A common practice is to provide explanations for a given prediction and qualitative model analysis by assigning importance to input tokens using scores provided by attention mechanisms (Chen et al., 2017; Wang et al., 2016; Jain et al., 2020; Sun and Lu, 2020) as a mean towards model interpretability (Lipton, 2016; Miller, 2019). A faithful explanation is one that accurately represents the true reasoning behind a model’s prediction (Jacovi and Goldberg, 2020). A series of recent studies illustrate that explanations obtained by attention weights do not always provide faithful explanations (Serrano and Smith, 2019) while different text encoders can affect attention interpretability, e.g. results can differ when using a recurrent or non-recurrent encoder (Wiegreffe and Pinter, 2019). A limitation of attention as a"
2021.acl-long.40,D19-1002,0,0.370968,"ecific Information for Text Classification George Chrysostomou Nikolaos Aletras Department of Computer Science, University of Sheffield United Kingdom {gchrysostomou1, n.aletras}@sheffield.ac.uk Abstract Neural network architectures in natural language processing often use attention mechanisms to produce probability distributions over input token representations. Attention has empirically been demonstrated to improve performance in various tasks, while its weights have been extensively used as explanations for model predictions. Recent studies (Jain and Wallace, 2019; Serrano and Smith, 2019; Wiegreffe and Pinter, 2019) have showed that it cannot generally be considered as a faithful explanation (Jacovi and Goldberg, 2020) across encoders and tasks. In this paper, we seek to improve the faithfulness of attention-based explanations for text classification. We achieve this by proposing a new family of Task-Scaling (TaSc) mechanisms that learn task-specific non-contextualised information to scale the original attention weights. Evaluation tests for explanation faithfulness, show that the three proposed variants of TaSc improve attentionbased explanations across two attention mechanisms, five encoders and five t"
2021.acl-long.40,2020.emnlp-demos.6,0,0.0998332,"Missing"
2021.acl-long.40,N16-1174,0,0.0444454,"ialised and updated partially at each training iteration, because naturally each input sequence contains only a small subset of the vocabulary words. We then obtain a task-scaled embedding ˆ ei for a token i in the input by multiplying the original token embedding with its word type weight ui : Attention scores (ai ) are computed by passing the representations (hi ) obtained from the encoder to the attention mechanism which usually consists of a similarity function φ followed by softmax: exppφphi , qqq k“1 exppφpq, hk qq ai “ řt (2) where q P RN is a trainable self-attention vector similar to Yang et al. (2016). Following Jain and Wallace (2019), we consider two self-attention similarity functions: (i) Additive Attention (Tanh; Bahdanau et al. (2015)): φphi , qq “ qT tanhpW hi q Linear TaSc (Lin-TaSc) (3) where W is a trainable model parameter; and (ii) Scaled Dot-Product (Dot; Vaswani et al. (2017)): ˆ ei “ ui ei (6) Attention indicates how well inputs around a position i correspond to the output (Bahdanau et al., 2015). For example, in a bidirectional recurrent The intuition is that the embedding vector ei was trained on general corpora and is a noncontextualised “generic” representation of input"
2021.acl-short.60,N19-1423,0,0.0292784,"mposition which models interactions between visual (from CNN) and textual (from RNN) representations. Those systems exhibit rather low performance compared to those obtained on standard VQA, demonstrating that the corpus requires external knowledge to be solved correctly. Recent work has introduced methods to incorporate visual information to create Vision+Language BERT models through joint multimodal embeddings (Chen et al., 2020; Su et al., 2019; Lu et al., 2019). First, image and text are embedded into the same space, and then Transformer networks are applied as in the standard BERT model (Devlin et al., 2019). Our work is most similar to that of Shah et al. (2019) since the same preprocessing pipeline is used. However, our system does not use a memory network, and instead relies on on a BERT-based model (UNITER, see section 3) to model the relationship between question, facts, and image with self-attention layers. 3 Methodology To answer KVQA with Neural models, we first take the V+L BERT model UNITER (Chen et al., 2020) with the highest score on the commonsense VQA task, VCR (Zellers et al., 2019). In order to allow UNITER to accept external KG facts, we cast these facts to a textual form ‘Entity"
2021.acl-short.60,D15-1075,0,0.113647,"Missing"
2021.acl-short.60,K19-1063,0,0.0517608,"Missing"
2021.acl-short.60,W19-4828,0,0.0566364,"Missing"
2021.acl-short.60,2020.conll-1.45,0,0.0786985,"Missing"
2021.acl-short.60,2020.acl-main.469,0,0.0993039,"Missing"
2021.acl-short.60,P19-1487,0,0.0397214,"Missing"
2021.acl-short.60,P18-1238,0,0.0689464,"Missing"
2021.acl-short.60,2020.acl-main.357,0,0.0383506,"Missing"
2021.emnlp-main.249,2020.emnlp-main.20,0,0.05992,"Missing"
2021.emnlp-main.249,P16-1162,0,0.142392,"Missing"
2021.emnlp-main.249,N19-1423,0,0.0627522,"Missing"
2021.emnlp-main.249,2020.tacl-1.5,0,0.0347362,"Missing"
2021.emnlp-main.249,D16-1264,0,0.0494649,"ve not been explored. Motivated by ral language processing (NLP) for self-supervised this, we propose five frustratingly simple pretrainlearning of text representations. MLM trains a ing tasks, showing that they result into models that model (typically a neural network) to predict a parperform competitively to MLM when pretrained for ticular token that has been replaced with a [MASK] the same duration (e.g. five days) and fine-tuned in placeholder given its surrounding context. Devlin downstream tasks in G LUE (Wang et al., 2019) and et al. (2019) first proposed MLM with an additional SQ UAD (Rajpurkar et al., 2016) benchmarks. next sentence prediction (NSP) task (i.e. predicting Contributions: (1) To the best of our knowledge, whether two segments appear consecutively in the this study is the first to investigate whether linguisoriginal text) to train B ERT. tically and non-linguistically intuitive tasks can ∗ Work was done while at the University of Sheffield. 1 Our code is publicly available here: https://github. effectively be used for pretraining (§2). (2) We com/gucci-j/light-transformer-emnlp2021 empirically demonstrate that our proposed objec3116 Proceedings of the 2021 Conference on Empirical Me"
2021.emnlp-main.51,I17-2052,0,0.0285663,"ties compared to its neighbors in the training set (Eq. 1, 2), suggesting that it may lie near the model’s decision boundary. To this end, our acquisition function selects the top b examples from the pool that have the highest score sxp (cf. line 8), that form the acquired batch Q. 3 3.1 Experimental Setup Tasks & Datasets We conduct experiments on sentiment analysis, topic classification, natural language inference and paraphrase detection tasks. We provide details for the datasets in Table 1. We follow Yuan et al. (2020) and use IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), PUBMED (Dernoncourt and Lee, 2017) and AGNEWS from Zhang et al. (2015) where we also acquired DBPEDIA. We experiment with tasks requiring pairs of input sequences, using QQP and QNLI from GLUE (Wang et al., 2019). To evaluate robustness on out-of-distribution (OOD) data, we follow Hendrycks et al. (2020) and use SST-2 as OOD dataset for IMDB and vice versa. We finally use TWITTERPPDB (Lan et al., 2017) as OOD data for QQP as in Desai and Durrett (2020). 3.2 Baselines uncertainty sampling, by computing gradient embeddings gx for every candidate data point x in Dpool and then using clustering to select a batch. Each gx is comput"
2021.emnlp-main.51,2020.emnlp-main.21,0,0.0203091,"paraphrase detection tasks. We provide details for the datasets in Table 1. We follow Yuan et al. (2020) and use IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), PUBMED (Dernoncourt and Lee, 2017) and AGNEWS from Zhang et al. (2015) where we also acquired DBPEDIA. We experiment with tasks requiring pairs of input sequences, using QQP and QNLI from GLUE (Wang et al., 2019). To evaluate robustness on out-of-distribution (OOD) data, we follow Hendrycks et al. (2020) and use SST-2 as OOD dataset for IMDB and vice versa. We finally use TWITTERPPDB (Lan et al., 2017) as OOD data for QQP as in Desai and Durrett (2020). 3.2 Baselines uncertainty sampling, by computing gradient embeddings gx for every candidate data point x in Dpool and then using clustering to select a batch. Each gx is computed as the gradient of the crossentropy loss with respect to the parameters of the model’s last layer, aiming to be the component that incorporates uncertainty in the acquisition function 7 . We also evaluate a recently introduced coldstart acquisition function called A LPS (Yuan et al., 2020) that uses the masked language model (MLM) loss of B ERT as a proxy for model uncertainty in the downstream classification task."
2021.emnlp-main.51,N19-1423,0,0.103054,"ing Loop Assuming a multi-class classification problem with C classes, labeled data for training Dlab and a pool of unlabeled data Dpool , we perform AL for T iterations. At each iteration, we train a model on Dlab and then use our proposed acquisition function, C AL (Algorithm 1), to acquire a batch Q consisting of b examples from Dpool . The acquired examples are then labeled4 , they are removed from the pool Dpool and added to the labeled dataset Dlab , which will serve as the training set for training a model in the next AL iteration. In our experiments, we use a pretrained B ERT model M (Devlin et al., 2019), which we fine-tune at each AL iteration using the current Dlab . We begin the AL loop by training a model M using an initial labeled dataset Dlab 5 . 4 We simulate AL, so we already have the labels of the examples of Dpool (but still treat it as an unlabeled dataset). 5 We acquire the first examples that form the initial training set Dlab by applying random stratified sampling (i.e. keeping the initial label distribution). Find Nearest Neighbors for Unlabeled Candidates The first step of our contrastive acquisition function (cf. line 2) is to find examples that are similar in the model featu"
2021.emnlp-main.51,2020.emnlp-main.638,0,0.171095,"d sentences Q and tokens from the rest of the data pool U . Following Yuan et al. (2020), we compute D IV.-I as the Jaccard similarity between the set of tokens from the sampled sentences Q, VQ , and the set of tokens from the unsampled sentences UQ, VQ0 , |VQ ∩VQ0 | J (VQ , VQ0 ) = |VQ ∪VQ0 |. A high D IV.-I value indicates high diversity because the sampled and unsampled sentences have many tokens in common. Diversity in feature space (D IV.-F) We next evaluate diversity in the (model) feature space, using the [CLS] representations of a trained B ERT model 11 . Following Zhdanov (2019) and Ein-Dor et al. (2020), we compute D IV.-F of a set Q as  −1 1 P min d(Φ(x ), Φ(x )) , where Φ(xi ) i j |U | xi ∈U xj ∈Q denotes the [CLS] output token of example xi obtained by the model which was trained using L, and d(Φ(xi ), Φ(xj )) denotes the Euclidean distance between xi and xj in the feature space. Uncertainty (U NC .) To measure uncertainty, we use the model Mf trained on the entire training dataset (Figure 2 - Full supervision). As in Yuan et al. (2020), we use the logits from the fully trained model to estimate the uncertainty of an example, as it is a reliable estimate due to its high performance afte"
2021.emnlp-main.51,D17-1063,0,0.0225474,", might be beneficial. Especially, if similar behavior appears in other NLP tasks too. Another interesting future direction for C AL, related to interpretability, would be to evaluate whether acquiring contrastive examples for the task (Kaushik et al., 2020; Gardner et al., 2020) is more beneficial than contrastive examples for the model, as we do in C AL. Hybrid There are several existing approaches that combine representative and uncertainty sampling. Such approaches include active learning algorithms that use meta-learning (Baram et al., 2004; Hsu and Lin, 2015) and reinforcement learning (Fang et al., 2017; Liu et al., 2018), aiming to learn a policy for switching between a diversitybased or an uncertainty-based criterion at each iteration. Recently, Ash et al. (2020) propose Batch Active learning by Diverse Gradient Embeddings (BADGE) and Yuan et al. (2020) propose Active Learning by Processing Surprisal (A LPS), a coldstart acquisition function specific for pretrained Acknowledgments language models. Both methods construct representations for the unlabeled data based on uncertainty, KM and NA are supported by Amazon through the and then use them for clustering; hence combining Alexa Fellowshi"
2021.emnlp-main.51,P04-1075,0,0.136048,"l, both approaches have core limitations that may lead to acquiring redundant data points. Algorithms based on uncertainty may end up choosing uncertain yet uninformative repetitive data, while diversity-based methods may tend to select diverse yet easy examples for the model (Roy and McCallum, 2001). The two approaches are orthogonal to each other, since uncertainty sampling is usually based on the model’s output, while diversity exploits information from the input (i.e. feature) space. Hybrid data acquisition functions that combine uncertainty and diversity sampling have also been proposed (Shen et al., 2004; Zhu et al., 2008; Ducoffe and Precioso, 2018; Ash et al., 2020; Yuan et al., 2020; Ru et al., 2020). Active learning (AL) is a machine learning paradigm for efficiently acquiring data for annotation from a (typically large) pool of unlabeled data (Lewis and Catlett, 1994; Cohn et al., 1996; Settles, 2009). Its goal is to concentrate the human labeling effort on the most informative data points In this work, we aim to leverage characteristics that will benefit model performance the most and from hybrid data acquisition. We hypothesize that thus reducing data annotation cost. data points that"
2021.emnlp-main.51,W17-2630,0,0.0261208,"e predictive entropy. Houlsby et al. (2011) 657 propose Bayesian Active Learning by Disagreement (BALD), a method that chooses data points that maximize the mutual information between predictions and model’s posterior probabilities. Gal et al. (2017) applied BALD for deep neural models using Monte Carlo dropout (Gal and Ghahramani, 2016) to acquire multiple uncertainty estimates for each candidate example. Least confidence, entropy and BALD acquisition functions have been applied in a variety of text classification and sequence labeling tasks, showing to substantially improve data efficiency (Shen et al., 2017; Siddhant and Lipton, 2018; Lowell and Lipton, 2019; Kirsch et al., 2019; Shelmanov et al., 2021; Margatina et al., 2021). Diversity Sampling On the other hand, diversity or representative sampling is based on selecting batches of unlabeled examples that are representative of the unlabeled pool, based on the intuition that a representative set of examples once labeled, can act as a surrogate for the full data available. In the context of deep learning, Geifman and El-Yaniv (2017) and Sener and Savarese (2018) select representative examples based on core-set construction, a fundamental problem"
2021.emnlp-main.51,D18-1318,0,0.0145939,"y. Houlsby et al. (2011) 657 propose Bayesian Active Learning by Disagreement (BALD), a method that chooses data points that maximize the mutual information between predictions and model’s posterior probabilities. Gal et al. (2017) applied BALD for deep neural models using Monte Carlo dropout (Gal and Ghahramani, 2016) to acquire multiple uncertainty estimates for each candidate example. Least confidence, entropy and BALD acquisition functions have been applied in a variety of text classification and sequence labeling tasks, showing to substantially improve data efficiency (Shen et al., 2017; Siddhant and Lipton, 2018; Lowell and Lipton, 2019; Kirsch et al., 2019; Shelmanov et al., 2021; Margatina et al., 2021). Diversity Sampling On the other hand, diversity or representative sampling is based on selecting batches of unlabeled examples that are representative of the unlabeled pool, based on the intuition that a representative set of examples once labeled, can act as a surrogate for the full data available. In the context of deep learning, Geifman and El-Yaniv (2017) and Sener and Savarese (2018) select representative examples based on core-set construction, a fundamental problem in computational geometry."
2021.emnlp-main.51,D13-1170,0,0.00365068,"e in model predicted probabilities compared to its neighbors in the training set (Eq. 1, 2), suggesting that it may lie near the model’s decision boundary. To this end, our acquisition function selects the top b examples from the pool that have the highest score sxp (cf. line 8), that form the acquired batch Q. 3 3.1 Experimental Setup Tasks & Datasets We conduct experiments on sentiment analysis, topic classification, natural language inference and paraphrase detection tasks. We provide details for the datasets in Table 1. We follow Yuan et al. (2020) and use IMDB (Maas et al., 2011), SST-2 (Socher et al., 2013), PUBMED (Dernoncourt and Lee, 2017) and AGNEWS from Zhang et al. (2015) where we also acquired DBPEDIA. We experiment with tasks requiring pairs of input sequences, using QQP and QNLI from GLUE (Wang et al., 2019). To evaluate robustness on out-of-distribution (OOD) data, we follow Hendrycks et al. (2020) and use SST-2 as OOD dataset for IMDB and vice versa. We finally use TWITTERPPDB (Lan et al., 2017) as OOD data for QQP as in Desai and Durrett (2020). 3.2 Baselines uncertainty sampling, by computing gradient embeddings gx for every candidate data point x in Dpool and then using clustering"
2021.emnlp-main.51,C08-1143,0,0.0542557,"have core limitations that may lead to acquiring redundant data points. Algorithms based on uncertainty may end up choosing uncertain yet uninformative repetitive data, while diversity-based methods may tend to select diverse yet easy examples for the model (Roy and McCallum, 2001). The two approaches are orthogonal to each other, since uncertainty sampling is usually based on the model’s output, while diversity exploits information from the input (i.e. feature) space. Hybrid data acquisition functions that combine uncertainty and diversity sampling have also been proposed (Shen et al., 2004; Zhu et al., 2008; Ducoffe and Precioso, 2018; Ash et al., 2020; Yuan et al., 2020; Ru et al., 2020). Active learning (AL) is a machine learning paradigm for efficiently acquiring data for annotation from a (typically large) pool of unlabeled data (Lewis and Catlett, 1994; Cohn et al., 1996; Settles, 2009). Its goal is to concentrate the human labeling effort on the most informative data points In this work, we aim to leverage characteristics that will benefit model performance the most and from hybrid data acquisition. We hypothesize that thus reducing data annotation cost. data points that are close in the m"
2021.emnlp-main.51,2020.emnlp-main.746,0,0.0834467,"Missing"
2021.emnlp-main.51,2020.emnlp-main.637,0,0.117787,"points. Algorithms based on uncertainty may end up choosing uncertain yet uninformative repetitive data, while diversity-based methods may tend to select diverse yet easy examples for the model (Roy and McCallum, 2001). The two approaches are orthogonal to each other, since uncertainty sampling is usually based on the model’s output, while diversity exploits information from the input (i.e. feature) space. Hybrid data acquisition functions that combine uncertainty and diversity sampling have also been proposed (Shen et al., 2004; Zhu et al., 2008; Ducoffe and Precioso, 2018; Ash et al., 2020; Yuan et al., 2020; Ru et al., 2020). Active learning (AL) is a machine learning paradigm for efficiently acquiring data for annotation from a (typically large) pool of unlabeled data (Lewis and Catlett, 1994; Cohn et al., 1996; Settles, 2009). Its goal is to concentrate the human labeling effort on the most informative data points In this work, we aim to leverage characteristics that will benefit model performance the most and from hybrid data acquisition. We hypothesize that thus reducing data annotation cost. data points that are close in the model feature space The most widely used approaches to acquiring ("
2021.emnlp-main.614,P19-1239,0,0.145727,"his is the first study to combine textual and visual features to classify POI types (e.g. arts & entertainment, nightlife spot) from social media messages, regardless of its geographic location. 2.3 Social Media Analysis using Text and Images The combination of text and images of social media posts has been largely used for different appliPOIs have been studied to classify functional re- cations such as sentiment analysis, (Nguyen and gions (e.g. residential, business, and transportation Shirai, 2015; Chambers et al., 2015), sarcasm deareas) and to analyze activity patterns using so- tection (Cai et al., 2019) and text-image relation cial media check-in data and geo-referenced im- classification (Vempala and Preo¸tiuc-Pietro, 2019; ages (Zhi et al., 2016; Liu et al., 2020a; Zhou Kruk et al., 2019). Moon et al. (2018b) propose a et al., 2020a; Zhang et al., 2020). Zhou et al. model for recognizing named entities from short (2020a) presents a model for classifying POI func- social media texts using image and text. Cai et al. tion types (e.g. bank, entertainment, culture) using (2019) use a hierarchical fusion model to integrate POI names and a list of results produced by search- image and text contex"
2021.emnlp-main.614,D15-1007,0,0.0130505,"information (hour, and day of the week) of a Twitter’s post. To the best of our knowledge, this is the first study to combine textual and visual features to classify POI types (e.g. arts & entertainment, nightlife spot) from social media messages, regardless of its geographic location. 2.3 Social Media Analysis using Text and Images The combination of text and images of social media posts has been largely used for different appliPOIs have been studied to classify functional re- cations such as sentiment analysis, (Nguyen and gions (e.g. residential, business, and transportation Shirai, 2015; Chambers et al., 2015), sarcasm deareas) and to analyze activity patterns using so- tection (Cai et al., 2019) and text-image relation cial media check-in data and geo-referenced im- classification (Vempala and Preo¸tiuc-Pietro, 2019; ages (Zhi et al., 2016; Liu et al., 2020a; Zhou Kruk et al., 2019). Moon et al. (2018b) propose a et al., 2020a; Zhang et al., 2020). Zhou et al. model for recognizing named entities from short (2020a) presents a model for classifying POI func- social media texts using image and text. Cai et al. tion types (e.g. bank, entertainment, culture) using (2019) use a hierarchical fusion mode"
2021.emnlp-main.614,D19-1061,0,0.110298,"on (Vempala and Preo¸tiuc-Pietro, 2019; ages (Zhi et al., 2016; Liu et al., 2020a; Zhou Kruk et al., 2019). Moon et al. (2018b) propose a et al., 2020a; Zhang et al., 2020). Zhou et al. model for recognizing named entities from short (2020a) presents a model for classifying POI func- social media texts using image and text. Cai et al. tion types (e.g. bank, entertainment, culture) using (2019) use a hierarchical fusion model to integrate POI names and a list of results produced by search- image and text context with an attention-based fuing for the POI name in a web search engine. Zhang sion. Chinnappa et al. (2019) examine the posseset al. (2020) makes use of social media check-ins sion relationships from text-image pairs in social and street-level images to compare the different media posts. Wang et al. (2020) use texts and imactivity patterns of visitors and locals, and uncover ages for predicting the keyphrases (i.e. representainconspicuous but interesting places for them in a tive terms) for a post by aligning and capturing the city. A framework for extracting emotions (e.g. joy, cross-modal interactions via cross-attention. Prehappiness) from photos taken at various locations vious text-image class"
2021.emnlp-main.614,N19-1423,0,0.0612061,"is ‘letters’, which concerns images that contain embedded text. Finally, the category Great Outdoors includes object tags such as ‘cloud’, ‘hill’, and ‘grass’, words that describe the landscape of this type of place. 4.1 ht = tanh(W t f t + bt ) h = tanh(W f + b ) Table 2: Most common objects for each POI category. 4 MM-Gate Multimodal POI Type Prediction Text and Image Representation Given a text-image post P = (xt , xv ), xt ∈ Rdt , xv ∈ Rdv , we first compute text and image encoding vectors f t , f v respectively. Text We use Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) to obtain the text feature representations f t by extracting the ‘classification’ [CLS] token. Image For encoding the images, we use Xception (Chollet, 2017) pre-trained on ImageNet (Deng et al., 2009).5 We extract convolutional feature maps for each image and we apply average pooling to obtain the image representation f v . (3) v (4) where W t ∈ Rdt , W v ∈ Rdv and W z ∈ Rdt +dv are learnable parameters, tanh is the activation function and ht , hv ∈ R are projections of f t and f v . [; ] denotes concatenation and σ is the sigmoid activation function. h is a weighted combination of the textu"
2021.emnlp-main.614,N16-1122,0,0.0248959,": (i) a modality gate to control the amount of information needed from the text and image; (ii) a cross-attention mechanism to learn crossmodal interactions. Our model significantly outperforms the best state-of-the-art method proposed by Sánchez Villegas et al. (2020); • We provide an in-depth analysis to uncover the limitations of our model and uncover cross-modal characteristics of POI types. 2 Related Work 2.2 POI Type Prediction POI type prediction is related to geolocation prediction of social media posts that has been widely studied in NLP (Eisenstein et al., 2010; Roller et al., 2012; Dredze et al., 2016). However, while geolocation prediction aims to infer the exact geographical location of a post using language variation and geographical cues, POI type prediction is focused on identifying the characteristics associated with each type of place, regardless of its geographic location. Previous work on POI type prediction from social media content has used Twitter posts (text and posting time), to identify the POI type from where a post was sent from (Liu et al., 2012; Sánchez Villegas et al., 2020). Liu et al. (2012) incorporate text, temporal features (posting hour) and user history informatio"
2021.emnlp-main.614,D10-1124,0,0.142358,"Missing"
2021.emnlp-main.614,2020.emnlp-main.62,0,0.0298926,"e captioning and visual question answering (Zhou To compare the effect of the ‘average’ image (see et al., 2020b; Lu et al., 2019). On the other hand, Section 3) on the performance of the models, we text-image relationships in social media data for train MM-Gate, MM-XAtt, and MM-Gated-XAtt inferring the type of location from which a mes- on tweets that are originally accompanied by an sage was sent are more diverse, highlighting the image excluding all text-only tweets; and we test particular challenges for modeling text and images on all tweets as in our original setting (text-only together (Hessel and Lee, 2020). tweets are paired with the ‘average’ image). The Our proposed MM-Gated-XAtt model achieves results are shown in Table 5. MM-Gated-XAtt is 47.21 F1 which significantly (t-test, p &lt; 0.05) im- consistently the best performing model, followed proves over BERT, the best performing model in by MM-Gate. However, their performance is inSánchez Villegas et al. (2020) and consistently out- ferior than when models are trained on all tweets performs all other image-only and multimodal ap- using the ‘average’ image as in the original setting. proaches. This confirms our main hypothesis that This suggests"
2021.emnlp-main.614,2020.coling-main.157,1,0.721964,"tion about the locations (e.g. finer subcategories of a type of place and how POI types are related to one another). 8 Conclusion and Future Work Post (a) miso creamed kale with mushrooms &lt;mention> Post (b) celebrate the fruits of #fermentation’s labor at #bostonfermentationfestival! next sun 10-4 &lt;mention> True: Nightlife Spot Ours: Food True: Shop & Service Ours: Food Figure 5: Example of misclassifications made by our MM-Gated-XAtt model. media such as analyzing political ads (Sánchez Villegas et al., 2021), parody (Maronikolakis et al., 2020) and complaints (Preo¸tiuc-Pietro et al., 2019; Jin and Aletras, 2020, 2021). Acknowledgments We would like to thank Mali Jin, Panayiotis Karachristou and all reviewers for their valuable feedback. DSV is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by the UK Research and Innovation grant EP/S023062/1. NA is supported by a Leverhulme Trust Research Project Grant. Ethical Statement This paper presents the first study on multimodal Our work complies with Twitter data policy for rePOI type classification using text and images from search,7 and has received approval from the Ethics social medi"
2021.emnlp-main.614,2021.naacl-main.180,1,0.842393,"Missing"
2021.emnlp-main.614,2021.eacl-main.4,0,0.0369953,"text-image pairs in low-noise settings without hurting the model performance. The benefit of controlling the flow of information through a gating mechanism, on the other hand, strongly improves model robustness. We observe that the text-only model (BERT) achieves 43.67 F1 which is substantially higher than the performance of image-only models (e.g. the best performing EfficientNet model obtains 24.72 F1). This suggests that text encapsulates more relevant information for this task than images on their own, similar to other studies in multimodal computational social science (Wang et al., 2020; Ma et al., 2021). Models that simply concatenate text and image vectors have close performance to BERT (44.0 for Concat-BERT+Xception) or lower (41.56 for Concat-BERT+EfficientNet). This suggests that assigning equal importance to text and image information can deteriorate performance. It also shows that modeling cross-modal interactions is necessary to boost performance of POI type classification models. Surprisingly, we observe that the pre-trained multimodal LXMERT fails to improve over BERT (40.17 F1) while its performance is lower than simpler concatenative fusion models. We speculate that this is becaus"
2021.emnlp-main.614,2020.acl-main.403,1,0.819,"category of POIs, the model might need access to deeper contextual information about the locations (e.g. finer subcategories of a type of place and how POI types are related to one another). 8 Conclusion and Future Work Post (a) miso creamed kale with mushrooms &lt;mention> Post (b) celebrate the fruits of #fermentation’s labor at #bostonfermentationfestival! next sun 10-4 &lt;mention> True: Nightlife Spot Ours: Food True: Shop & Service Ours: Food Figure 5: Example of misclassifications made by our MM-Gated-XAtt model. media such as analyzing political ads (Sánchez Villegas et al., 2021), parody (Maronikolakis et al., 2020) and complaints (Preo¸tiuc-Pietro et al., 2019; Jin and Aletras, 2020, 2021). Acknowledgments We would like to thank Mali Jin, Panayiotis Karachristou and all reviewers for their valuable feedback. DSV is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by the UK Research and Innovation grant EP/S023062/1. NA is supported by a Leverhulme Trust Research Project Grant. Ethical Statement This paper presents the first study on multimodal Our work complies with Twitter data policy for rePOI type classification using text and image"
2021.emnlp-main.614,D19-1469,0,0.342725,"Conference on Empirical Methods in Natural Language Processing, pages 7785–7797 c November 7–11, 2021. 2021 Association for Computational Linguistics use in social media from different locations, by inferring the type of a POI of a given social media post using only text and posting time, ignoring the visual context (Sánchez Villegas et al., 2020). However, communication and interactions in social media are naturally shaped by the variety of available modalities and their semiotic relationships (i.e. how meaning is created and communicated) with one another (Georgakopoulou and Spilioti, 2015; Kruk et al., 2019; Vempala and Preo¸tiuc-Pietro, 2019). In this paper, we propose POI type prediction using multimodal content available at posting time by taking into account textual and visual information. Our contributions are as follows: • We enrich a publicly available data set of social media posts and POI types with images; • We propose a multimodal model that combines text and images in two levels using: (i) a modality gate to control the amount of information needed from the text and image; (ii) a cross-attention mechanism to learn crossmodal interactions. Our model significantly outperforms the best"
2021.emnlp-main.614,P18-1186,0,0.140855,"a Analysis using Text and Images The combination of text and images of social media posts has been largely used for different appliPOIs have been studied to classify functional re- cations such as sentiment analysis, (Nguyen and gions (e.g. residential, business, and transportation Shirai, 2015; Chambers et al., 2015), sarcasm deareas) and to analyze activity patterns using so- tection (Cai et al., 2019) and text-image relation cial media check-in data and geo-referenced im- classification (Vempala and Preo¸tiuc-Pietro, 2019; ages (Zhi et al., 2016; Liu et al., 2020a; Zhou Kruk et al., 2019). Moon et al. (2018b) propose a et al., 2020a; Zhang et al., 2020). Zhou et al. model for recognizing named entities from short (2020a) presents a model for classifying POI func- social media texts using image and text. Cai et al. tion types (e.g. bank, entertainment, culture) using (2019) use a hierarchical fusion model to integrate POI names and a list of results produced by search- image and text context with an attention-based fuing for the POI name in a web search engine. Zhang sion. Chinnappa et al. (2019) examine the posseset al. (2020) makes use of social media check-ins sion relationships from text-imag"
2021.emnlp-main.614,N18-1078,0,0.155818,"a Analysis using Text and Images The combination of text and images of social media posts has been largely used for different appliPOIs have been studied to classify functional re- cations such as sentiment analysis, (Nguyen and gions (e.g. residential, business, and transportation Shirai, 2015; Chambers et al., 2015), sarcasm deareas) and to analyze activity patterns using so- tection (Cai et al., 2019) and text-image relation cial media check-in data and geo-referenced im- classification (Vempala and Preo¸tiuc-Pietro, 2019; ages (Zhi et al., 2016; Liu et al., 2020a; Zhou Kruk et al., 2019). Moon et al. (2018b) propose a et al., 2020a; Zhang et al., 2020). Zhou et al. model for recognizing named entities from short (2020a) presents a model for classifying POI func- social media texts using image and text. Cai et al. tion types (e.g. bank, entertainment, culture) using (2019) use a hierarchical fusion model to integrate POI names and a list of results produced by search- image and text context with an attention-based fuing for the POI name in a web search engine. Zhang sion. Chinnappa et al. (2019) examine the posseset al. (2020) makes use of social media check-ins sion relationships from text-imag"
2021.emnlp-main.614,P15-1131,0,0.0638316,"Missing"
2021.emnlp-main.614,P19-1495,1,0.848802,"Missing"
2021.emnlp-main.614,D12-1137,0,0.0155505,"s in two levels using: (i) a modality gate to control the amount of information needed from the text and image; (ii) a cross-attention mechanism to learn crossmodal interactions. Our model significantly outperforms the best state-of-the-art method proposed by Sánchez Villegas et al. (2020); • We provide an in-depth analysis to uncover the limitations of our model and uncover cross-modal characteristics of POI types. 2 Related Work 2.2 POI Type Prediction POI type prediction is related to geolocation prediction of social media posts that has been widely studied in NLP (Eisenstein et al., 2010; Roller et al., 2012; Dredze et al., 2016). However, while geolocation prediction aims to infer the exact geographical location of a post using language variation and geographical cues, POI type prediction is focused on identifying the characteristics associated with each type of place, regardless of its geographic location. Previous work on POI type prediction from social media content has used Twitter posts (text and posting time), to identify the POI type from where a post was sent from (Liu et al., 2012; Sánchez Villegas et al., 2020). Liu et al. (2012) incorporate text, temporal features (posting hour) and u"
2021.emnlp-main.614,2021.findings-acl.321,1,0.695228,"2020). To correctly classify the category of POIs, the model might need access to deeper contextual information about the locations (e.g. finer subcategories of a type of place and how POI types are related to one another). 8 Conclusion and Future Work Post (a) miso creamed kale with mushrooms &lt;mention> Post (b) celebrate the fruits of #fermentation’s labor at #bostonfermentationfestival! next sun 10-4 &lt;mention> True: Nightlife Spot Ours: Food True: Shop & Service Ours: Food Figure 5: Example of misclassifications made by our MM-Gated-XAtt model. media such as analyzing political ads (Sánchez Villegas et al., 2021), parody (Maronikolakis et al., 2020) and complaints (Preo¸tiuc-Pietro et al., 2019; Jin and Aletras, 2020, 2021). Acknowledgments We would like to thank Mali Jin, Panayiotis Karachristou and all reviewers for their valuable feedback. DSV is supported by the Centre for Doctoral Training in Speech and Language Technologies (SLT) and their Applications funded by the UK Research and Innovation grant EP/S023062/1. NA is supported by a Leverhulme Trust Research Project Grant. Ethical Statement This paper presents the first study on multimodal Our work complies with Twitter data policy for rePOI typ"
2021.emnlp-main.614,2020.aacl-main.80,1,0.17365,"posts consisting of image-text pairs, shared from two different places or Point-of-Interests (POIs). Users share content that is relevant to their experience in the location. For example, the text imagine all the people sharing all the world which is accompanied by a photograph of the Imagine Mosaic in Central Park; and the text Next stop: NYC along with a picture of descriptive items that people carry at an airport such as luggage, a camera and a takeaway coffee cup. Developing computational methods to infer the type of a POI from social media posts (Liu et al., 1 Introduction 2012; Sánchez Villegas et al., 2020) is useful for A place is typically described as a physical space complementing studies in computational social sciinfused with human meaning and experiences that ence including sociolinguistics, geosemiotics, and facilitate communication (Tuan, 1977). The mul- cultural geography (Kress et al., 1996; Scollon and timodal content of social media posts (e.g. text, Scollon, 2003; Al Zydjaly, 2014), and has applicaimages, emojis) generated by users from specific tions in geosocial networking technologies such as places such as restaurants, shops, and parks, con- recommendation and visualization sys"
2021.emnlp-main.614,D19-1514,0,0.328412,"tanh is the activation function and ht , hv ∈ R are projections of f t and f v . [; ] denotes concatenation and σ is the sigmoid activation function. h is a weighted combination of the textual and visual information ht and hv respectively. We fine-tune the entire model by adding a classification layer with a softmax activation function for POI type prediction 4.3 MM-XAtt The MM-Gate model does not capture interactions between text and image that might be beneficial for learning semiotic relationships. To model crossmodal interactions, we adapt the cross-attention mechanism (Tsai et al., 2019; Tan and Bansal, 2019) to combine text and image information for multimodal POI type prediction (MM-XAtt). Cross-attention consists of two attention layers, one from textual f t to visual features f v and one from visual to textual features. We first linearly project the text and visual representations to obtain the same dimensionality (dproj ). Then, we compute T) √ the scaled dot attention (a = sof tmax (Q(K) V) dproj with the projected textual vector as query (Q), and the projected image vector as the key (K) and values (V ), and vice versa. The multimodal representation h is the sum of the resulting attention l"
2021.emnlp-main.614,N19-1268,0,0.0613122,"Missing"
2021.emnlp-main.614,P19-1656,0,0.0278928,"rnable parameters, tanh is the activation function and ht , hv ∈ R are projections of f t and f v . [; ] denotes concatenation and σ is the sigmoid activation function. h is a weighted combination of the textual and visual information ht and hv respectively. We fine-tune the entire model by adding a classification layer with a softmax activation function for POI type prediction 4.3 MM-XAtt The MM-Gate model does not capture interactions between text and image that might be beneficial for learning semiotic relationships. To model crossmodal interactions, we adapt the cross-attention mechanism (Tsai et al., 2019; Tan and Bansal, 2019) to combine text and image information for multimodal POI type prediction (MM-XAtt). Cross-attention consists of two attention layers, one from textual f t to visual features f v and one from visual to textual features. We first linearly project the text and visual representations to obtain the same dimensionality (dproj ). Then, we compute T) √ the scaled dot attention (a = sof tmax (Q(K) V) dproj with the projected textual vector as query (Q), and the projected image vector as the key (K) and values (V ), and vice versa. The multimodal representation h is the sum of th"
2021.emnlp-main.614,P19-1272,0,0.0264745,"Missing"
2021.emnlp-main.614,2020.emnlp-main.268,0,0.0726594,"Missing"
2021.emnlp-main.645,2021.acl-long.40,1,0.819627,"ttention to attend to uninformative tokens. Pasinput tokens in context and not individually due cual et al. (2021) and Brunner et al. (2020) argue to information mixing (Tutek and Snajder, 2020; that this might be due to significant information Pascual et al., 2021). As such, we hypothesize that mixing in higher layers of the model, with recent we can improve the ability of a pretrained LM in studies showing improvements in the faithfulness providing faithful explanations, by showing to the of attention-based explanations by addressing this model alternative distributions of input importance (Chrysostomou and Aletras, 2021; Tutek and Sna(i.e. word salience). We assume that by introducing jder, 2020). the salience distribution via an auxiliary objective Atanasova et al. (2020) evaluate faithfulness (Ross et al., 2017b), we can reduce information of explanations (Jacovi and Goldberg, 2020) by mixing by “shifting” the model’s attention to other removing important tokens and observing difinformative tokens. In a similar direction to ours, ferences in prediction, showing that generally Xu et al. (2020) showed that by computing attention gradient-based approaches for transformers protogether with salience information"
2021.emnlp-main.645,W19-4828,0,0.0213009,"transformer-based (Vaswani et al., 2017) language models (LMs) such as B ERT (Devlin et al., 2019), have achieved state-of-the-art results in various language understanding tasks (Wang et al., 2019b,a). Despite their success, their highly complex nature consisting of millions of parameters, makes them difficult to interpret (Jain et al., 2020). This has motivated new research on understanding and explaining their predictions. Previous work has explored whether LMs encode syntactic knowledge by studying their multi1 Code: https://github.com/GChrysostomou/ saloss. head attention distributions (Clark et al., 2019; Htut et al., 2019; Voita et al., 2019). Recent studies have evaluated the faithfulness of explanations2 for predictions made by these models (Vashishth et al., 2019; Atanasova et al., 2020; Jain et al., 2020). In general, LMs can provide faithful explanations, particularly using attention (Jain et al., 2020), but still fall behind other simpler architectures (Atanasova et al., 2020) possibly due to increased information mixing and higher contextualization in the model (Brunner et al., 2020; Pascual et al., 2021; Tutek and Snajder, 2020). Recent studies have attempted to improve the explainab"
2021.emnlp-main.645,N19-1423,0,0.192808,"lose to salient information extracted a priori using TextRank. Experiments for explanation faithfulness across five datasets, show that models trained with S A L OSS consistently provide more faithful explanations across four different feature attribution methods compared to vanilla BERT. Using the rationales extracted from vanilla BERT and S A L OSS models to train inherently faithful classifiers, we further show that the latter result in higher predictive performance in downstream tasks.1 1 Introduction Pretrained transformer-based (Vaswani et al., 2017) language models (LMs) such as B ERT (Devlin et al., 2019), have achieved state-of-the-art results in various language understanding tasks (Wang et al., 2019b,a). Despite their success, their highly complex nature consisting of millions of parameters, makes them difficult to interpret (Jain et al., 2020). This has motivated new research on understanding and explaining their predictions. Previous work has explored whether LMs encode syntactic knowledge by studying their multi1 Code: https://github.com/GChrysostomou/ saloss. head attention distributions (Clark et al., 2019; Htut et al., 2019; Voita et al., 2019). Recent studies have evaluated the faith"
2021.emnlp-main.645,W14-3010,0,0.0298887,"Missing"
2021.emnlp-main.645,2020.lrec-1.220,0,0.0352779,"Missing"
2021.emnlp-main.645,N16-1082,0,0.029794,"g, 2020). 8189 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8189–8200 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Related Work Model Explainability Explanations can be obtained by computing importance scores for input tokens to identify which parts of the input contributed the most towards a model’s prediction (i.e. feature attribution). A common approach to attributing input importance is by measuring differences in a model’s prediction between keeping and omitting an input token (Robnik-Šikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018a). Input importance can also be obtained by calculating the gradients of a prediction with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Sundararajan et al., 2017; Bastings and Filippova, 2020). We can also use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017). Finally, recent studies propose using feature attribution to extract a fraction of the input as a rationale and then use it to train a classifier (Jain et al., 2020; Treviso and Martins, 2020). that attention-based feature attributions, in general, ou"
2021.emnlp-main.645,P19-1631,0,0.101154,"ain et al., 2020). In general, LMs can provide faithful explanations, particularly using attention (Jain et al., 2020), but still fall behind other simpler architectures (Atanasova et al., 2020) possibly due to increased information mixing and higher contextualization in the model (Brunner et al., 2020; Pascual et al., 2021; Tutek and Snajder, 2020). Recent studies have attempted to improve the explainability of non transformer-based models, by guiding them through an auxiliary objective towards informative input importance distributions (e.g. human or adversarial priors) (Ross et al., 2017a; Liu and Avci, 2019; Moradi et al., 2021). In a similar direction, we propose Salient Loss (S A L OSS), an auxiliary objective that allows the multi-head attention of the model to learn from salient information (i.e. token importance) during training to reduce the effects of information mixing (Pascual et al., 2021). We compute a priori token importance scores (Xu et al., 2020) using T EXT R ANK (Mihalcea and Tarau, 2004) (i.e. an unsupervised graph-based method) and penalize the model when the attention distribution deviates from the salience distribution. Our contributions are as follows: • We demonstrate that"
2021.emnlp-main.645,2021.ccl-1.108,0,0.0811716,"Missing"
2021.emnlp-main.645,W04-3252,0,0.180937,"xplainability of non transformer-based models, by guiding them through an auxiliary objective towards informative input importance distributions (e.g. human or adversarial priors) (Ross et al., 2017a; Liu and Avci, 2019; Moradi et al., 2021). In a similar direction, we propose Salient Loss (S A L OSS), an auxiliary objective that allows the multi-head attention of the model to learn from salient information (i.e. token importance) during training to reduce the effects of information mixing (Pascual et al., 2021). We compute a priori token importance scores (Xu et al., 2020) using T EXT R ANK (Mihalcea and Tarau, 2004) (i.e. an unsupervised graph-based method) and penalize the model when the attention distribution deviates from the salience distribution. Our contributions are as follows: • We demonstrate that models trained with S A L OSS generate more faithful explanations in an input erasure evaluation. • We finally show that rationales extracted from S A L OSS models result in higher predictive performance in downstream tasks when used as the only input for training inherently faithful classifiers. 2 A faithful explanation represents the true reasons behind a model’s prediction (Jacovi and Goldberg, 2020"
2021.emnlp-main.645,2020.acl-main.387,0,0.0189505,"f studies introduced adversarial auxiliary objectives to influence attentionbased explanations during training (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Ross et al., 2017b; Liu and Avci, 2019). These objectives have typically been used as a tool for evaluating explanation faithfulness generated by attention (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithfulness of explanations generated by non-transformer based models (Ross et al., 2017b; Liu and Avci, 2019; Moradi et al., 2021; Mohankumar et al., 2020; Tutek and Snajder, 2020). The auxiliary objectives guide the model using human annotated importance scores (Liu and Avci, 2019), or allow for selective input gradient penalization (Ross et al., 2017b). Such studies illustrate the effectiveness of auxiliary objectives for improving the faithfulness of model explanations suggesting that we can also improve explanation faithfulness in transformers using appropriate prior information. Faithfulness of Pretrained LM Explanations Brunner et al. (2020) criticize the ability of atten3 Improving Explanation Faithfulness tion in providing faithful expl"
2021.emnlp-main.645,2021.eacl-main.243,0,0.161009,"n general, LMs can provide faithful explanations, particularly using attention (Jain et al., 2020), but still fall behind other simpler architectures (Atanasova et al., 2020) possibly due to increased information mixing and higher contextualization in the model (Brunner et al., 2020; Pascual et al., 2021; Tutek and Snajder, 2020). Recent studies have attempted to improve the explainability of non transformer-based models, by guiding them through an auxiliary objective towards informative input importance distributions (e.g. human or adversarial priors) (Ross et al., 2017a; Liu and Avci, 2019; Moradi et al., 2021). In a similar direction, we propose Salient Loss (S A L OSS), an auxiliary objective that allows the multi-head attention of the model to learn from salient information (i.e. token importance) during training to reduce the effects of information mixing (Pascual et al., 2021). We compute a priori token importance scores (Xu et al., 2020) using T EXT R ANK (Mihalcea and Tarau, 2004) (i.e. an unsupervised graph-based method) and penalize the model when the attention distribution deviates from the salience distribution. Our contributions are as follows: • We demonstrate that models trained with S"
2021.emnlp-main.645,N18-1097,0,0.384647,"ceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8189–8200 c November 7–11, 2021. 2021 Association for Computational Linguistics 2 Related Work Model Explainability Explanations can be obtained by computing importance scores for input tokens to identify which parts of the input contributed the most towards a model’s prediction (i.e. feature attribution). A common approach to attributing input importance is by measuring differences in a model’s prediction between keeping and omitting an input token (Robnik-Šikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018a). Input importance can also be obtained by calculating the gradients of a prediction with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Sundararajan et al., 2017; Bastings and Filippova, 2020). We can also use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017). Finally, recent studies propose using feature attribution to extract a fraction of the input as a rationale and then use it to train a classifier (Jain et al., 2020; Treviso and Martins, 2020). that attention-based feature attributions, in general, outperform gradie"
2021.emnlp-main.645,2021.eacl-main.9,0,0.126811,"ulti1 Code: https://github.com/GChrysostomou/ saloss. head attention distributions (Clark et al., 2019; Htut et al., 2019; Voita et al., 2019). Recent studies have evaluated the faithfulness of explanations2 for predictions made by these models (Vashishth et al., 2019; Atanasova et al., 2020; Jain et al., 2020). In general, LMs can provide faithful explanations, particularly using attention (Jain et al., 2020), but still fall behind other simpler architectures (Atanasova et al., 2020) possibly due to increased information mixing and higher contextualization in the model (Brunner et al., 2020; Pascual et al., 2021; Tutek and Snajder, 2020). Recent studies have attempted to improve the explainability of non transformer-based models, by guiding them through an auxiliary objective towards informative input importance distributions (e.g. human or adversarial priors) (Ross et al., 2017a; Liu and Avci, 2019; Moradi et al., 2021). In a similar direction, we propose Salient Loss (S A L OSS), an auxiliary objective that allows the multi-head attention of the model to learn from salient information (i.e. token importance) during training to reduce the effects of information mixing (Pascual et al., 2021). We comp"
2021.emnlp-main.645,2020.acl-main.432,0,0.0327527,"fraction of the input as a rationale and then use it to train a classifier (Jain et al., 2020; Treviso and Martins, 2020). that attention-based feature attributions, in general, outperform gradient-based ones. A different branch of studies introduced adversarial auxiliary objectives to influence attentionbased explanations during training (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Ross et al., 2017b; Liu and Avci, 2019). These objectives have typically been used as a tool for evaluating explanation faithfulness generated by attention (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithfulness of explanations generated by non-transformer based models (Ross et al., 2017b; Liu and Avci, 2019; Moradi et al., 2021; Mohankumar et al., 2020; Tutek and Snajder, 2020). The auxiliary objectives guide the model using human annotated importance scores (Liu and Avci, 2019), or allow for selective input gradient penalization (Ross et al., 2017b). Such studies illustrate the effectiveness of auxiliary objectives for improving the faithfulness of model explanations suggesting that we can also improve explan"
2021.emnlp-main.645,P19-1282,0,0.0779444,"erasure experiments. 4 α ∈ Rt ; σ ∈ Rt , where t is the sequence length. 3 S A L OSS .91 (.00) .93 (.00) .80 (.02) .76 (.00) .57 (.03) Models Similar to Jain et al. (2020) we use: BERT (Devlin et al., 2019) for (SST, AG, SEMEVAL); S CI BERT (Beltagy et al., 2019) for E V.I NF.; ROBERTA (Liu et al., 2019) for M.RC. Evaluating Explanation Faithfulness We evaluate the faithfulness5 of model explanations using two standard approaches: • Input Erasure: We first compute the average fraction of tokens required to be removed (in decreasing importance) to cause a change in prediction (decision flip) (Serrano and Smith, 2019; Nguyen, 2018b). • FRESH: We also compute the predictive performance of a classifier trained on rationales extracted with feature attribution metrics (see §4) using FRESH (Jain et al., 2020). We extract rationales by; (1) selecting the top-k most important tokens (T OP K) and (2) selecting the span of length k that has the highest overall importance (C ONTIGUOUS). where Lc is the Cross-Entropy Loss for a downstream text classification task and λ a regularization coefficient for the proposed S A L OSS (Lsal ) which can be tuned in a development set. Lsal is defined as the KL divergence between"
2021.emnlp-main.645,D15-1300,0,0.031985,"Missing"
2021.emnlp-main.645,D13-1170,0,0.0053804,"y objective which allows the model to learn attending to more informative input tokens jointly with the task. S A L OSS penalizes the model when the attention distribution (α) deviates from the word salience distribution (σ).4 For α we compute the average attention scores of the CLS token from the last layer (Jain et al., 2020). The joint objective for adapting a LM to a downstream classification task with S A L OSS is: L = Lc + λLsal DATASET SST AG E V.I NF M.RC SEMEVAL Experimental Setup Datasets We consider five natural language understanding tasks (see dataset statistics in Appx. A): SST (Socher et al., 2013); AGNews (AG) (Corso et al., 2005); Evidence Inference (E V.I NF.) (Lehman et al., 2019); MultiRC (M.RC) (Khashabi et al., 2018) and Semeval 2017 Task 4 Subtask A (S EMEVAL) (Nakov et al., 2013). We also considered the use of T FIDF and χ2 scores observing comparable but lower performance in early experimentation. We hypothesize that TextRank performs well due to its effectiveness in improving performance in text summarization (Xu et al., 2020). See also Appx. H for T FIDF and χ2 results on input erasure experiments. 4 α ∈ Rt ; σ ∈ Rt , where t is the sequence length. 3 S A L OSS .91 (.00) .93"
2021.emnlp-main.645,2020.blackboxnlp-1.10,0,0.0193696,"ping and omitting an input token (Robnik-Šikonja and Kononenko, 2008; Li et al., 2016b; Nguyen, 2018a). Input importance can also be obtained by calculating the gradients of a prediction with respect to the input (Kindermans et al., 2016; Li et al., 2016a; Sundararajan et al., 2017; Bastings and Filippova, 2020). We can also use sparse linear meta-models that are easier to interpret (Ribeiro et al., 2016; Lundberg and Lee, 2017). Finally, recent studies propose using feature attribution to extract a fraction of the input as a rationale and then use it to train a classifier (Jain et al., 2020; Treviso and Martins, 2020). that attention-based feature attributions, in general, outperform gradient-based ones. A different branch of studies introduced adversarial auxiliary objectives to influence attentionbased explanations during training (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Ross et al., 2017b; Liu and Avci, 2019). These objectives have typically been used as a tool for evaluating explanation faithfulness generated by attention (Kennedy et al., 2020; Wiegreffe and Pinter, 2019; Pruthi et al., 2020; Ghorbani et al., 2019) while others used auxiliary objectives to improve the faithfulness of explanat"
2021.emnlp-main.645,2020.repl4nlp-1.17,0,0.0856068,"thub.com/GChrysostomou/ saloss. head attention distributions (Clark et al., 2019; Htut et al., 2019; Voita et al., 2019). Recent studies have evaluated the faithfulness of explanations2 for predictions made by these models (Vashishth et al., 2019; Atanasova et al., 2020; Jain et al., 2020). In general, LMs can provide faithful explanations, particularly using attention (Jain et al., 2020), but still fall behind other simpler architectures (Atanasova et al., 2020) possibly due to increased information mixing and higher contextualization in the model (Brunner et al., 2020; Pascual et al., 2021; Tutek and Snajder, 2020). Recent studies have attempted to improve the explainability of non transformer-based models, by guiding them through an auxiliary objective towards informative input importance distributions (e.g. human or adversarial priors) (Ross et al., 2017a; Liu and Avci, 2019; Moradi et al., 2021). In a similar direction, we propose Salient Loss (S A L OSS), an auxiliary objective that allows the multi-head attention of the model to learn from salient information (i.e. token importance) during training to reduce the effects of information mixing (Pascual et al., 2021). We compute a priori token importa"
2021.emnlp-main.645,P19-1580,0,0.0296744,"7) language models (LMs) such as B ERT (Devlin et al., 2019), have achieved state-of-the-art results in various language understanding tasks (Wang et al., 2019b,a). Despite their success, their highly complex nature consisting of millions of parameters, makes them difficult to interpret (Jain et al., 2020). This has motivated new research on understanding and explaining their predictions. Previous work has explored whether LMs encode syntactic knowledge by studying their multi1 Code: https://github.com/GChrysostomou/ saloss. head attention distributions (Clark et al., 2019; Htut et al., 2019; Voita et al., 2019). Recent studies have evaluated the faithfulness of explanations2 for predictions made by these models (Vashishth et al., 2019; Atanasova et al., 2020; Jain et al., 2020). In general, LMs can provide faithful explanations, particularly using attention (Jain et al., 2020), but still fall behind other simpler architectures (Atanasova et al., 2020) possibly due to increased information mixing and higher contextualization in the model (Brunner et al., 2020; Pascual et al., 2021; Tutek and Snajder, 2020). Recent studies have attempted to improve the explainability of non transformer-based models, b"
2021.emnlp-main.722,N19-1259,0,0.0889988,"r target ‘service’: The/O food/O is/O good/O but/O the/O service/O is/O extremely/B slow/I. TOWE Extraction Results: {(food, good), (service, extremely slow)} Table 1: Identifying target-oriented opinion words in a sentence. Underlined words are opinion targets. Spans tagged B and I are considered as opinion words. Learning effective word representations is a critical step towards tackling TOWE. Traditional work (Zhuang et al., 2006a; Hu and Liu, 2004a; Qiu et al., 2011) has used hand-crafted features to represent words which do not often generalize easily. More recent work (Liu et al., 2015; Fan et al., 2019b; Wu et al., 2020a; Veyseh et al., 2020) has explored neural networks to learn word representations automatically. Previous neural-based methods (Liu et al., 2015; Fan et al., 2019b) has used word embeddings (Collobert and Weston, 2008; Mikolov et al., 2013; Pen1 Introduction nington et al., 2014) to represent the input. HowTarget-oriented opinion words extraction (TOWE) ever, TOWE is a complex task that requires a model (Fan et al., 2019b) is a fine-grained task of target- to know the relative position of each word to the aspect in text. Words that are relatively closer to oriented sentiment"
2021.emnlp-main.722,P82-1020,0,0.491087,"Missing"
2021.emnlp-main.722,P19-1356,0,0.0158433,"ingly, BiLSTM(G) outperforms the current state-of-the-art ONG by +0.62 Avg.F1 despite its simple architecture, demonstrating the importance to first experiment with simpler methods before designing more complex structures. suggest that BiLSTM(G) has an inductive bias appropriate for the TOWE task and the performance mostly depends on the quality of the input representation. We observe that when using BERT embeddings, there is a minimal performance difference between using GCNs or not. We attribute this to the expressiveness of BERT embeddings and its ability to capture syntactic dependencies (Jawahar et al., 2019). Overall results suggest that our proposed method outperforms SOTA consistently across datasets. 3.5 Ablation Study Comparison of Text+GCN Encoders: Adding We perform ablation experiments on the two best a GCN over any text encoder generally improves performing models, BiLSTM+GCN(G) and BiLperformance. This happens because the GCN STM+GCN(B), to study the contribution of their provides additional syntactic information that is different components. The results are shown in Tahelpful for representation learning. We find that ble 4. On BiLSTM+GCN(G), as we consecutively BiLSTM+GCN(G) achieves fe"
2021.emnlp-main.722,D15-1168,0,0.0226161,"Missing"
2021.emnlp-main.722,2021.ccl-1.108,0,0.0388095,"Missing"
2021.emnlp-main.722,D14-1162,0,0.0962691,"ctic information achieves only minor gains. This empirically highlights that BiLSTMbased methods have an inductive bias appropriate for the TOWE task, making a GCN less important. 2 Methodology Given sentence s = {w1 , . . . , wn } with aspect wt ∈ s, our approach consists of a text encoder that takes as input a combination of words, partof-speech and position information for TOWE. We further explore enhancing text encoding by incorporating information from a syntactic parse of the sentence through a GCN encoder. 2.1 Input Representation Word Embeddings: We experiment with Glove word vectors (Pennington et al., 2014) as well as BERT-based representations, extracted from the last layer of a BERT base model (Devlin et al., 2018) fine-tuned on TOWE. Position Embeddings (POSN): We compute the relative distance di from wi to wt (i.e., di = i − t), and lookup their embedding in a randomly initialized position embedding table. Par-of-Speech Tag Embeddings (POST): We assign part-of-speech tags to each word token using the Stanford parser,2 and lookup their embedding in a randomly initialized POST embedding table. Combined Input: We consider two types of input representations: 1. Glove Input (G): Constructed from"
2021.emnlp-main.722,S16-1002,0,0.0498549,"Missing"
2021.emnlp-main.722,S15-2082,0,0.0723587,"Missing"
2021.emnlp-main.722,S14-2004,0,0.044261,"Missing"
2021.emnlp-main.722,J11-1002,0,0.0746169,"rvice is extremely slow. True Labels for target ‘food’: The/O food/O is/O good/B but/O the/O service/O is/O extremely/O slow/O. True Labels for target ‘service’: The/O food/O is/O good/O but/O the/O service/O is/O extremely/B slow/I. TOWE Extraction Results: {(food, good), (service, extremely slow)} Table 1: Identifying target-oriented opinion words in a sentence. Underlined words are opinion targets. Spans tagged B and I are considered as opinion words. Learning effective word representations is a critical step towards tackling TOWE. Traditional work (Zhuang et al., 2006a; Hu and Liu, 2004a; Qiu et al., 2011) has used hand-crafted features to represent words which do not often generalize easily. More recent work (Liu et al., 2015; Fan et al., 2019b; Wu et al., 2020a; Veyseh et al., 2020) has explored neural networks to learn word representations automatically. Previous neural-based methods (Liu et al., 2015; Fan et al., 2019b) has used word embeddings (Collobert and Weston, 2008; Mikolov et al., 2013; Pen1 Introduction nington et al., 2014) to represent the input. HowTarget-oriented opinion words extraction (TOWE) ever, TOWE is a complex task that requires a model (Fan et al., 2019b) is a fine-gra"
2021.emnlp-main.722,2020.emnlp-main.719,0,0.509133,"good/O but/O the/O service/O is/O extremely/B slow/I. TOWE Extraction Results: {(food, good), (service, extremely slow)} Table 1: Identifying target-oriented opinion words in a sentence. Underlined words are opinion targets. Spans tagged B and I are considered as opinion words. Learning effective word representations is a critical step towards tackling TOWE. Traditional work (Zhuang et al., 2006a; Hu and Liu, 2004a; Qiu et al., 2011) has used hand-crafted features to represent words which do not often generalize easily. More recent work (Liu et al., 2015; Fan et al., 2019b; Wu et al., 2020a; Veyseh et al., 2020) has explored neural networks to learn word representations automatically. Previous neural-based methods (Liu et al., 2015; Fan et al., 2019b) has used word embeddings (Collobert and Weston, 2008; Mikolov et al., 2013; Pen1 Introduction nington et al., 2014) to represent the input. HowTarget-oriented opinion words extraction (TOWE) ever, TOWE is a complex task that requires a model (Fan et al., 2019b) is a fine-grained task of target- to know the relative position of each word to the aspect in text. Words that are relatively closer to oriented sentiment analysis (Liu, 2012) aiming to the targe"
2021.emnlp-main.722,C14-1220,0,0.0537322,"low”, Fan et al. (2019b) employ Long Short-Term TOWE attempts to identify the opinion words Memory (LSTM) networks (Hochreiter and “good” and “extremely slow” corresponding respec- Schmidhuber, 1997) to encode the target position tively to the targets “food” and “service”. TOWE information in word embeddings. Wu et al. (2020a) is usually treated as a sequence labeling problem transfer latent opinion knowledge into a Bidirecusing the BIO tagging scheme (Ramshaw and Mar- tional LSTM (BiLSTM) network that leverages cus, 1999) to distinguish the Beginning, Inside and word and position embeddings (Zeng et al., 2014). Outside of a span of opinion words. Table 1 shows Recently, Veyseh et al. (2020) have proposed ONG, an example of applying the BIO tagging scheme a method that combines BERT (Bidirectional Enfor TOWE. coder Representations from Transformers) (De9174 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 9174–9179 c November 7–11, 2021. 2021 Association for Computational Linguistics vlin et al., 2018), position embeddings, Ordered Neurons LSTM (ON-LSTM) (Shen et al., 2018),1 and a graph convolutional network (GCN) (Kipf and Welling, 2016) to introduce sy"
2021.findings-acl.314,Q18-1041,0,0.0481384,"tions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et al., 2019) using a dataset constructed from published and publicly available records of past cases of the Supreme People’s Court of China, a debate ensued about the ethical limits of legal NLP. More specifically, Leins et al. (2020) queried in a systematic way whether papers such as that of Chen et al. (2019) should be published. Leins et al. (2020) invoked a number of arguments including considerations to do with the construction of the dataset (Bender and Friedman, 2018), and so-called ‘dual use’ arguments (Radford et al., 2019), i.e. the possibility of using a system developed for some purpose for another, potentially harmful, purpose. Following a rich discussion, Leins et al. (2020) asked whether it is ethically permissible that legal NLP should be used at all to predict items such as prison terms. We believe that the kind of ethical query put forth by Leins et al. (2020) is vital for the future of legal NLP and computational law in general. However, we also contend that it is essential that a more general discussion should be conducted about the pertinent"
2021.findings-acl.314,P19-1424,1,0.925733,"law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community. 1 Introduction Developing computational methods for analyzing legal text is an emerging area in natural language processing (NLP) with various applications such as legal topic classification (Nallapati and Manning, 2008), court opinion generation (Ye et al., 2018) and legal judgment prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019). Legal NLP holds the promise of improving access to justice and offers to legal scholars the tools that allow for an empirical analysis of law on a large scale (Katz, 2012; Zhong et al., 2020). The development and use of legal text processing technologies also raise a series of ethical questions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et al., 2019) using a dataset constructed from published and publicly available records of past cases of the Supreme People’s Court of China, a debate ensued abou"
2021.findings-acl.314,D19-1667,0,0.136043,"g, 2008), court opinion generation (Ye et al., 2018) and legal judgment prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019). Legal NLP holds the promise of improving access to justice and offers to legal scholars the tools that allow for an empirical analysis of law on a large scale (Katz, 2012; Zhong et al., 2020). The development and use of legal text processing technologies also raise a series of ethical questions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et al., 2019) using a dataset constructed from published and publicly available records of past cases of the Supreme People’s Court of China, a debate ensued about the ethical limits of legal NLP. More specifically, Leins et al. (2020) queried in a systematic way whether papers such as that of Chen et al. (2019) should be published. Leins et al. (2020) invoked a number of arguments including considerations to do with the construction of the dataset (Bender and Friedman, 2018), and so-called ‘dual use’ arguments (Radford et al., 2019), i.e. the possibility of using a system developed for some purpose for an"
2021.findings-acl.314,2020.acl-main.261,0,0.763704,"Missing"
2021.findings-acl.314,D17-1289,0,0.0875194,"in research related to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community. 1 Introduction Developing computational methods for analyzing legal text is an emerging area in natural language processing (NLP) with various applications such as legal topic classification (Nallapati and Manning, 2008), court opinion generation (Ye et al., 2018) and legal judgment prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019). Legal NLP holds the promise of improving access to justice and offers to legal scholars the tools that allow for an empirical analysis of law on a large scale (Katz, 2012; Zhong et al., 2020). The development and use of legal text processing technologies also raise a series of ethical questions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et al., 2019) using a dataset constructed from published and publicly available records of past cases of the Supreme"
2021.findings-acl.314,D08-1046,0,0.0460725,"ce of a wide diversity of legal and ethical norms domestically but even more so internationally and (c) the threat of moralism in research related to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community. 1 Introduction Developing computational methods for analyzing legal text is an emerging area in natural language processing (NLP) with various applications such as legal topic classification (Nallapati and Manning, 2008), court opinion generation (Ye et al., 2018) and legal judgment prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019). Legal NLP holds the promise of improving access to justice and offers to legal scholars the tools that allow for an empirical analysis of law on a large scale (Katz, 2012; Zhong et al., 2020). The development and use of legal text processing technologies also raise a series of ethical questions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et"
2021.findings-acl.314,N18-1168,0,0.0218986,"ically but even more so internationally and (c) the threat of moralism in research related to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community. 1 Introduction Developing computational methods for analyzing legal text is an emerging area in natural language processing (NLP) with various applications such as legal topic classification (Nallapati and Manning, 2008), court opinion generation (Ye et al., 2018) and legal judgment prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019). Legal NLP holds the promise of improving access to justice and offers to legal scholars the tools that allow for an empirical analysis of law on a large scale (Katz, 2012; Zhong et al., 2020). The development and use of legal text processing technologies also raise a series of ethical questions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et al., 2019) using a dataset constructed from"
2021.findings-acl.314,2020.acl-main.466,0,0.0181578,"nt debate in the legal NLP research community. 1 Introduction Developing computational methods for analyzing legal text is an emerging area in natural language processing (NLP) with various applications such as legal topic classification (Nallapati and Manning, 2008), court opinion generation (Ye et al., 2018) and legal judgment prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019). Legal NLP holds the promise of improving access to justice and offers to legal scholars the tools that allow for an empirical analysis of law on a large scale (Katz, 2012; Zhong et al., 2020). The development and use of legal text processing technologies also raise a series of ethical questions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et al., 2019) using a dataset constructed from published and publicly available records of past cases of the Supreme People’s Court of China, a debate ensued about the ethical limits of legal NLP. More specifically, Leins et al. (2020) queried in a systematic way whether papers such as that of Chen et al. (2019) should be published. Leins et al. (2020)"
2021.findings-acl.314,D18-1390,0,0.12488,"ed to computational law. For each of these three parameters we provide specific recommendations for the legal NLP community. Our discussion is structured around the study of a real-life scenario that has prompted recent debate in the legal NLP research community. 1 Introduction Developing computational methods for analyzing legal text is an emerging area in natural language processing (NLP) with various applications such as legal topic classification (Nallapati and Manning, 2008), court opinion generation (Ye et al., 2018) and legal judgment prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chalkidis et al., 2019). Legal NLP holds the promise of improving access to justice and offers to legal scholars the tools that allow for an empirical analysis of law on a large scale (Katz, 2012; Zhong et al., 2020). The development and use of legal text processing technologies also raise a series of ethical questions, on which we focus in this paper. For example, following the publication at EMNLP 2019 of a paper on automatic prison term prediction (Chen et al., 2019) using a dataset constructed from published and publicly available records of past cases of the Supreme People’s Court of Ch"
2021.findings-acl.321,N19-1216,0,0.011083,"iment analysis for ideology detection (i.e. conservative, liberal) in speeches from the U.S. Congress. Kulkarni et al. (2018) propose a multi-view model that incorporates textual and network information to predict the ideology of news articles. Johnson and Goldwasser 3670 (2018) investigate the relationship between political ideology and language to represent morality by analyzing political slogans in tweets posted by politicians. Maronikolakis et al. (2020) present a study of political parody on Twitter focusing on the linguistic differences between tweets shared by real and parody accounts. Baly et al. (2019) estimate the trustworthiness and political ideology (left/right bias) of news sources as a multi-task problem. Stefanov et al. (2020) develop methods to predict the overall political leaning (left, center or right) of online media and popular Twitter users. Political ideology and communicative intents have also been studied in computer vision. Political images have been analyzed to infer the persuasive intents using various features such as facial display types, body poses, and scene context (Joo et al., 2014; Huang and Kovashka, 2016; Joo and Steinert-Threlkeld, 2018; Bai et al., 2020; Chen"
2021.findings-acl.321,W18-6212,0,0.0317513,"Missing"
2021.findings-acl.321,N19-1423,0,0.215196,"hs posted in social media by political candidates and their relationship to voter support. 2.3 Computational Analysis of Online Ads Hussain et al. (2017) propose the task of ad understanding using vision and language. The aim is to predict the topical category, sentiment and rhetoric of an ad (i.e. what the message is about). The latter task has been approached as a visual question-answering task by ranking human generated statements that explain the intent of the ad in computer vision (Ye and Kovashka, 2018; Ahuja et al., 2018). More recently in NLP, Kalra et al. (2020) propose a BERT-based (Devlin et al., 2019) model for this task using the text and visual descriptions of the ad (Johnson et al., 2016). Thomas and Kovashka (2018) study the persuasive cues of faces across ad categories (e.g. beauty, clothing). Zhang et al. (2018) explore the relationship between the text of an ad and the visual content to analyze the semantics across modalities. Ye et al. (2018) integrates audio and visual modalities to predict the climax of an advertisement (i.e. stress levels) using sentiment annotations. 3 Tasks & Data We aim to analyze the political ideology of ads consisting of image and text, and the type of the"
2021.findings-acl.321,P05-1045,0,0.00820758,"k 1: Conservative (C)/ Liberal (L), and Task 2: Political Party (PP)/ThirdParty (TP). Avg. Tokens (Train/Dev/Test) Task IT D IT+D T1 17.1/16.5/17.1 38.3/39.9/36.9 55.4/56.4/54.0 T2 16.2/17.6/19.2 36.7/38.9/37.2 52.9/56.5/56.4 Table 4: Average number of tokens in image text (IT), densecaps (D) and both (IT+D) for sponsor ad ideology (T1) and type (T2) prediction. 3.5 Data Preprocessing Text We normalize the text from the image (IT) and the densecap (D) by lower-casing, and replacing all URLs and person names with a placeholder token. To identify the person names we use the Stanford NER Tagger (Finkel et al., 2005). Also, we replace tokens that appear in less than five ads with an ‘unknown’ token. We tokenize the text using the NLTK tokenizer (Bird et al., 2009). Table 4 shows the average number of tokens in IT and D for each data split. Image Each image is resized to (300 × 300) pixels represented by red, green and blue color values. Each color channel is an integer in the range [0, 255]. The pixel values of all images are dived by 255 to normalize them in the range [0, 1]. BERT We also test three models proposed by Kalra et al. (2020) for generic ad classification demonstrating state-of-the-art perfor"
2021.findings-acl.321,P14-1105,0,0.0308995,"itical communication and identify challenges for understanding the role of these platforms in political elections, highlighting the lack of transparency (Caplan and Boyd, 2016). Fowler et al. (2020b) explore differences between television and online ads, and demonstrate that there is a greater number of candidates advertising online than on television. 2.2 Political Ideology Prediction Inferring the political ideology of various types of text including news articles, political speeches and social media has been vastly studied in NLP (Lin et al., 2008; Gerrish and Blei, 2011; Sim et al., 2013; Iyyer et al., 2014; Preot¸iuc-Pietro et al., 2017; Kulkarni et al., 2018; Stefanov et al., 2020). Bhatia and P (2018) exploit topic-specific sentiment analysis for ideology detection (i.e. conservative, liberal) in speeches from the U.S. Congress. Kulkarni et al. (2018) propose a multi-view model that incorporates textual and network information to predict the ideology of news articles. Johnson and Goldwasser 3670 (2018) investigate the relationship between political ideology and language to represent morality by analyzing political slogans in tweets posted by politicians. Maronikolakis et al. (2020) present a"
2021.findings-acl.321,P18-1067,0,0.0374116,"Missing"
2021.findings-acl.321,2020.acl-main.674,0,0.416861,"z-Bacha, 2006). It is guided by ideology and morals (Scammell and Langer, 2006; Kumar and Pathak, 2012), and often expresses more negativity (Haselmayer, 2019; Iyengar and Prior, 1999; Lau et al., 1999) compared to the aesthetic nature of commercial advertising. Table 1 shows examples of online political ads across different political parties and sponsor types. While the closely related online commercial advertising domain has recently been explored in natural language processing (NLP) for predicting the category (e.g. politics, cars, electronics) and sentiment of an ad (Hussain et al., 2017; Kalra et al., 2020), online political advertising has yet to be explored. Large-scale studies of online political advertising have so far focused on understanding targeting strategies rather than developing predictive models for analyzing its content (Edelson et al., 2019; Medina Serrano et al., 2020). Automatically analyzing political ads is important in political science for researching the characteristics of online campaigns (e.g. voter targeting, sponsors, non-party campaigns, privacy, and misinformation) on a large scale (Scammell and Langer, 2006; Johansson and Holtz-Bacha, 2019). Moreover, identifying ads"
2021.findings-acl.321,D18-1388,0,0.0122675,"derstanding the role of these platforms in political elections, highlighting the lack of transparency (Caplan and Boyd, 2016). Fowler et al. (2020b) explore differences between television and online ads, and demonstrate that there is a greater number of candidates advertising online than on television. 2.2 Political Ideology Prediction Inferring the political ideology of various types of text including news articles, political speeches and social media has been vastly studied in NLP (Lin et al., 2008; Gerrish and Blei, 2011; Sim et al., 2013; Iyyer et al., 2014; Preot¸iuc-Pietro et al., 2017; Kulkarni et al., 2018; Stefanov et al., 2020). Bhatia and P (2018) exploit topic-specific sentiment analysis for ideology detection (i.e. conservative, liberal) in speeches from the U.S. Congress. Kulkarni et al. (2018) propose a multi-view model that incorporates textual and network information to predict the ideology of news articles. Johnson and Goldwasser 3670 (2018) investigate the relationship between political ideology and language to represent morality by analyzing political slogans in tweets posted by politicians. Maronikolakis et al. (2020) present a study of political parody on Twitter focusing on the l"
2021.findings-acl.321,2020.wnut-1.3,0,0.0604768,"Missing"
2021.findings-acl.321,2020.acl-main.403,1,0.585023,"; Sim et al., 2013; Iyyer et al., 2014; Preot¸iuc-Pietro et al., 2017; Kulkarni et al., 2018; Stefanov et al., 2020). Bhatia and P (2018) exploit topic-specific sentiment analysis for ideology detection (i.e. conservative, liberal) in speeches from the U.S. Congress. Kulkarni et al. (2018) propose a multi-view model that incorporates textual and network information to predict the ideology of news articles. Johnson and Goldwasser 3670 (2018) investigate the relationship between political ideology and language to represent morality by analyzing political slogans in tweets posted by politicians. Maronikolakis et al. (2020) present a study of political parody on Twitter focusing on the linguistic differences between tweets shared by real and parody accounts. Baly et al. (2019) estimate the trustworthiness and political ideology (left/right bias) of news sources as a multi-task problem. Stefanov et al. (2020) develop methods to predict the overall political leaning (left, center or right) of online media and popular Twitter users. Political ideology and communicative intents have also been studied in computer vision. Political images have been analyzed to infer the persuasive intents using various features such a"
2021.findings-acl.321,P17-1068,0,0.0619608,"Missing"
2021.findings-acl.321,W12-3713,0,0.0354062,"registered) and Third-Party (i.e. other organizations) following Fowler et al. (2020b); 3. Experiments with text-based and multimodal (text and images) models (§4) for political ideology prediction and sponsor type classification reaching up to 75.76 and 87.36 macro F1 in each task respectively (§6); 4. Analysis of textual and visual features of online political ads (§7) and error analysis to understand model limitations. 2 2.1 Related Work Political Communication and Advertising Previous work on analyzing political advertising has covered television and online ads (Kaid and Postelnicu, 2005; Reschke and Anand, 2012; West, 2017; Fowler et al., 2020b). Ridout et al. (2010) analyze a series of YouTube videos posted during the 2008 presidential campaign to understand its influence on election results as well as the actors and formats compared to traditional television ads. Anstead et al. (2018) study how online platforms such as Facebook are being used for political communication and identify challenges for understanding the role of these platforms in political elections, highlighting the lack of transparency (Caplan and Boyd, 2016). Fowler et al. (2020b) explore differences between television and online ad"
2021.findings-acl.321,D13-1010,0,0.0254935,"being used for political communication and identify challenges for understanding the role of these platforms in political elections, highlighting the lack of transparency (Caplan and Boyd, 2016). Fowler et al. (2020b) explore differences between television and online ads, and demonstrate that there is a greater number of candidates advertising online than on television. 2.2 Political Ideology Prediction Inferring the political ideology of various types of text including news articles, political speeches and social media has been vastly studied in NLP (Lin et al., 2008; Gerrish and Blei, 2011; Sim et al., 2013; Iyyer et al., 2014; Preot¸iuc-Pietro et al., 2017; Kulkarni et al., 2018; Stefanov et al., 2020). Bhatia and P (2018) exploit topic-specific sentiment analysis for ideology detection (i.e. conservative, liberal) in speeches from the U.S. Congress. Kulkarni et al. (2018) propose a multi-view model that incorporates textual and network information to predict the ideology of news articles. Johnson and Goldwasser 3670 (2018) investigate the relationship between political ideology and language to represent morality by analyzing political slogans in tweets posted by politicians. Maronikolakis et a"
2021.findings-acl.321,2020.acl-main.50,0,0.0425003,"these platforms in political elections, highlighting the lack of transparency (Caplan and Boyd, 2016). Fowler et al. (2020b) explore differences between television and online ads, and demonstrate that there is a greater number of candidates advertising online than on television. 2.2 Political Ideology Prediction Inferring the political ideology of various types of text including news articles, political speeches and social media has been vastly studied in NLP (Lin et al., 2008; Gerrish and Blei, 2011; Sim et al., 2013; Iyyer et al., 2014; Preot¸iuc-Pietro et al., 2017; Kulkarni et al., 2018; Stefanov et al., 2020). Bhatia and P (2018) exploit topic-specific sentiment analysis for ideology detection (i.e. conservative, liberal) in speeches from the U.S. Congress. Kulkarni et al. (2018) propose a multi-view model that incorporates textual and network information to predict the ideology of news articles. Johnson and Goldwasser 3670 (2018) investigate the relationship between political ideology and language to represent morality by analyzing political slogans in tweets posted by politicians. Maronikolakis et al. (2020) present a study of political parody on Twitter focusing on the linguistic differences be"
2021.findings-acl.452,2020.acl-main.747,0,0.0742845,"Missing"
2021.findings-acl.452,W13-2305,0,0.0345617,"acted from Wikipedia translated to and from English for a total of six language pairs: English–German (En-De),3 English–Chinese (En-Zh), Romanian–English (Ro-En), Estonian– English (Et-En), Sinhala–English (Si-En) and Nepali–English (Ne-En). Each translation was produced with a SoTA Transformer-based NMT model and manually annotated for quality using Language Sentences Estonian Romanian Sinhala Nepalese English 25,176 372,690 139,406 85,343 1,563,519 Table 2: Number of sentences extracted from Wikipedia for data augmentation. an annotation scheme inspired by the Direct Assessment methodology (Graham et al., 2013). The scores are produced on a continuous scale indicating perceived translation quality in 0-100. For each language pair, this dataset contains partitions for training (7K), dev (1K), and test (1K). Distilled dataset. Monolingual data for data augmentation was sampled from Wikipedia following the procedure described in Fomicheva et al. (2020) to preserve the domain of the MLQE dataset. Specifically, we sampled documents from Wikipedia for English, Estonian, Romanian, Sinhalese and Nepalese and selected the top 100 documents containing the largest number of sentences that are: (i) in the inten"
2021.findings-acl.452,C18-1266,1,0.903172,"Missing"
2021.findings-acl.452,2020.findings-emnlp.372,0,0.042971,"nd prohibits deployment on client machines with limited resources. Making models based on pre-trained representations smaller and more usable in practice is an active area of research. One approach is Knowledge Distillation (KD), aiming to extract knowledge from a top-performing large model (the teacher) into a smaller (in terms of memory print, computational power and prediction latency) yet wellperforming model (the student) (Hinton et al., 2015; Gou et al., 2020). KD techniques have been used to make BERT and similar models smaller. For example, DistilBERT (Sanh et al., 2019) and TinyBERT (Jiao et al., 2020) follow the same general architecture as the teacher BERT, but with a reduced number of layers. However, these student models are also based on Transformers and, as such, they still have too large memory and disk footprints. For instance, the number of parameters in the multilingual DistilBERT-based TransQuest model for QE (Ranasinghe et al., 2020) is 135M. In this paper, we propose to distill the QE model directly, where the student architecture can be completely different from that of the teacher. Namely, we distill a large and powerful QE model based on XLM-Roberta into a small RNN-based mo"
2021.findings-acl.452,P19-3020,0,0.036983,"Missing"
2021.findings-acl.452,W17-4763,0,0.0289451,"on 2. We used a random subset of 100K sentences from Wikipedia to train the student model for each of the language pairs except for EtEn where the total amount of collected in-domain monolingual data is 25K. Models. As teachers, we use pre-trained models from TransQuest (TQTEACHER ), one of the winning submissions in the WMT2020 QE Shared Task, which we fine-tuned on the MLQE dataset. For noise filtering, we train five teacher models with random initialisation. As students, we use BiRNN models from DeepQuest (Ive et al., 2018). We also compare our results against the PredictorEstimator model (Kim et al., 2017; Kepler et al., 2019), the baseline at the WMT2020 QE Shared Task, and TransQuest models using multilingual DistilBERT.6 4 Results Table 3 shows the Pearson correlation with human judgments on the test partition of the MLQE dataset for different models and specifies the type of data used for training.7 The correlation for the student models (BiRNNSTUDENT∗ ) does not reach the performance of TQTEACHER . Smaller models may lack representation power for modeling cross-lingual tasks such as QE. Also, distillation for regression is more challenging, as discussed in Section 2. However, training on"
2021.findings-acl.452,N19-4009,0,0.0285837,"he length between 50 and 150 characters. Table 2 shows the total amount of sentences in the monolingual Wikipedia dataset collected for data augmentation. To test the impact of data domain on the performance of the student QE models, we also collect out-of-domain data for the Et-En language pair. The out-of-domain data is sampled from Common Crawl. We use the version of Common Crawl distributed by the WMT2018 News Translation Task4 . The total amount of sentences in this dataset is 100,779,314. To translate the data, we used the same MT models that generated the test data, built with fairseq (Ott et al., 2019) and made available by the WMT2020 QE Shared Task organisers.5 Sentences that were part of the training data for the MT models or part of the MLQE dataset were excluded. We generate quality predictions for the remaining sentences using the teacher models, as 2 Here we use ensemble only as a way of estimating the error in the predictions and leave distillation based on ensemble predictions to future work. 3 We skip this language pair as the performance of the teacher model for it is too weak. 4 http://www.statmt.org/wmt18/translati on-task.html 5 https://github.com/facebookresearch/ mlqe/tree/m"
2021.findings-acl.452,2020.coling-main.445,0,0.0826241,"nt, computational power and prediction latency) yet wellperforming model (the student) (Hinton et al., 2015; Gou et al., 2020). KD techniques have been used to make BERT and similar models smaller. For example, DistilBERT (Sanh et al., 2019) and TinyBERT (Jiao et al., 2020) follow the same general architecture as the teacher BERT, but with a reduced number of layers. However, these student models are also based on Transformers and, as such, they still have too large memory and disk footprints. For instance, the number of parameters in the multilingual DistilBERT-based TransQuest model for QE (Ranasinghe et al., 2020) is 135M. In this paper, we propose to distill the QE model directly, where the student architecture can be completely different from that of the teacher. Namely, we distill a large and powerful QE model based on XLM-Roberta into a small RNN-based model. Existing work along these lines has applied KD mainly to classification tasks (Tang et al., 2019; Sun et al., 2019). We instead explore this approach in the context of regression. In contrast to classification, where KD provides useful information on the output distribution of incorrect classes, for regression the teacher predictions are point"
2021.findings-acl.452,D19-1441,0,0.0645368,"Missing"
2021.naacl-main.180,2020.peoples-1.14,0,0.0249858,"l categories (Forster and Entrup, 2017; Merson and Mary, 2017) or responsible departments (Laksana and Purwarianti, 2014; Gunawan et al., 2018; Tjandra et al., 2015). Furthermore, other complaint related categorizations are based on product hazards and risks (Bhat and Culotta, 2017), service failure (Jin et al., 2013) and escalation likelihood (Yang et al., 2019). 2.3 Emotion Detection Most related to complaint severity is emotion detection and its intensity which have been extensively studied in NLP (Danisman and Alpkocak, 2008; Volkova and Bachrach, 2016; Zhang et al., 2018). More recently, Alejo et al. (2020) explored cross-lingual transfer approaches to predict emotion intensity in Twitter. Similarly, Akhtar et al. (2020) evaluated a series of feature-based machine learning models for both emotion and sentiment intensity prediction in social and news media. 3 Task & Data We define complaint severity prediction as a multiclass classification task. Given a text snippet T , 2.2 Complaint Analysis defined as a sequence of tokens T = {t1 , ..., tn }, Most of the existing studies on complaint classifi- the aim is to classify T as one of the four predefined severity labels. cation in NLP have explored d"
2021.naacl-main.180,J08-4004,0,0.0238279,"Missing"
2021.naacl-main.180,2020.coling-main.157,1,0.622198,"hat the complainer is willing to undertake and their purpose (Olshtain and Weinbach, 1985; Trosborg, 2011; Kakolaki and Shahrokhi, 2016). Complaining purposes might include the expression of dissatisfaction, to find solutions (e.g. ask for reparations) or both. Furthermore, a complaint can be categorized as implicit (i.e. without mentioning who is responsible) or explicit (i.e. accusing someone for doing something). Recent work on modeling complaints in natural language processing (NLP) has focused on distinguishing complaints from non-complaints in social media (Preotiuc-Pietro et al., 2019; Jin and Aletras, 2020), however there is no previous study into more fine-grained complaint categories. Table 1 shows examples of social media posts expressing complaints grouped into four severity classes according to Trosborg (2011): (a) no explicit reproach; (b) disapproval; (c) accusation; and (d) blame. Identifying and analyzing the severity of complaints is important for: (a) improving customer service by recognizing the level of dissatisfaction and understanding complainers’ needs (Van Noort and Willemsen, 2012); (b) linguists to study the speech act of complaints in different levels of granularity on large"
2021.naacl-main.180,2021.ccl-1.108,0,0.0525013,"Missing"
2021.naacl-main.180,2020.acl-main.403,1,0.692112,"Missing"
2021.naacl-main.180,D14-1162,0,0.0864842,"Aletras (2020) in the context of binary complaint classification. 4.1 Baselines Majority Class We use Majority Class as the first baseline, where we calculate scores by labeling all the tweets with the majority class. LR-BOW We use a linear baseline, Logistic Regression with standard bag-of-words (LR-BOW) and L2 regularization. BiGRU-Att We also use a neural baseline trained from scratch; a bidirectional Gated Recurrent Unit 2266 (GRU) network (Cho et al., 2014) with a selfattention mechanism (BiGRU-Att; (Tian et al., 2018)). Given a Twitter post T , a token ti is mapped to a GloVe embedding (Pennington et al., 2014). We then apply dropout to the output of GloVe embedding layer and pass it to a bidirectional GRU with self-attention layer. Finally, the contextualized token representations are passed to an output layer using a softmax activation function for multi-class classification. a Multimodal Shifting Gate (Wang et al., 2019), where an attention gating mechanism is applied to control the influence of each representation. Finally, we apply layer normalization and dropout after the Multimodal Shifting Gate and pass the output to RoBERTa. We add an output layer to MRoBERTa for classification similar to t"
2021.naacl-main.180,P19-1495,1,0.702356,"y, the amount of face-threat that the complainer is willing to undertake and their purpose (Olshtain and Weinbach, 1985; Trosborg, 2011; Kakolaki and Shahrokhi, 2016). Complaining purposes might include the expression of dissatisfaction, to find solutions (e.g. ask for reparations) or both. Furthermore, a complaint can be categorized as implicit (i.e. without mentioning who is responsible) or explicit (i.e. accusing someone for doing something). Recent work on modeling complaints in natural language processing (NLP) has focused on distinguishing complaints from non-complaints in social media (Preotiuc-Pietro et al., 2019; Jin and Aletras, 2020), however there is no previous study into more fine-grained complaint categories. Table 1 shows examples of social media posts expressing complaints grouped into four severity classes according to Trosborg (2011): (a) no explicit reproach; (b) disapproval; (c) accusation; and (d) blame. Identifying and analyzing the severity of complaints is important for: (a) improving customer service by recognizing the level of dissatisfaction and understanding complainers’ needs (Van Noort and Willemsen, 2012); (b) linguists to study the speech act of complaints in different levels"
2021.naacl-main.180,P15-1169,1,0.820091,"Missing"
2021.naacl-main.180,2020.acl-main.394,0,0.564934,"ana, 1997) for using severity categories to improve binary complaint prediction (i.e. complaint or non-complaint). MTL enables two or more tasks to be learned jointly by sharing information and parameters of a model. We explore whether or not the severity level of a complaint helps in complaint identification. We use the same data set as Preotiuc-Pietro et al. (2019), where each tweet is annotated as a complaint or not and our severity level annotations.7 8.1 Predictive models We first adapt three multi-task learning models based on bidirectional recurrent neural networks recently proposed by Rajamanickam et al. (2020) for jointly modeling abusive language detection and emotion detection. We also adapt our MRoBERTaEmo model in a multi-task setting using two variants. We use the severity complaint prediction as an auxiliary task and the binary complaint prediction as the main task to train different MTL models. All models are trained on the two tasks and updated at the same time with a joint loss: L = (1 − α)Lcom + αLsev where Lcom and Lsev are the losses of complaint identification and severity level classification tasks respectively. α is a parameter to control the importance of each loss. MTL-Double Encod"
2021.naacl-main.180,D17-2010,0,0.0311267,"Missing"
2021.naacl-main.180,P16-1148,0,0.106692,"so, previous work has classified complaints into detailed topical categories (Forster and Entrup, 2017; Merson and Mary, 2017) or responsible departments (Laksana and Purwarianti, 2014; Gunawan et al., 2018; Tjandra et al., 2015). Furthermore, other complaint related categorizations are based on product hazards and risks (Bhat and Culotta, 2017), service failure (Jin et al., 2013) and escalation likelihood (Yang et al., 2019). 2.3 Emotion Detection Most related to complaint severity is emotion detection and its intensity which have been extensively studied in NLP (Danisman and Alpkocak, 2008; Volkova and Bachrach, 2016; Zhang et al., 2018). More recently, Alejo et al. (2020) explored cross-lingual transfer approaches to predict emotion intensity in Twitter. Similarly, Akhtar et al. (2020) evaluated a series of feature-based machine learning models for both emotion and sentiment intensity prediction in social and news media. 3 Task & Data We define complaint severity prediction as a multiclass classification task. Given a text snippet T , 2.2 Complaint Analysis defined as a sequence of tokens T = {t1 , ..., tn }, Most of the existing studies on complaint classifi- the aim is to classify T as one of the four"
2021.naacl-main.180,N19-2008,0,0.0281994,"tter. More recent, Jin and Aletras (2020) explored a battery of transformerbased architectures combined with sentiment and topic information for complaint identification in social media. Also, previous work has classified complaints into detailed topical categories (Forster and Entrup, 2017; Merson and Mary, 2017) or responsible departments (Laksana and Purwarianti, 2014; Gunawan et al., 2018; Tjandra et al., 2015). Furthermore, other complaint related categorizations are based on product hazards and risks (Bhat and Culotta, 2017), service failure (Jin et al., 2013) and escalation likelihood (Yang et al., 2019). 2.3 Emotion Detection Most related to complaint severity is emotion detection and its intensity which have been extensively studied in NLP (Danisman and Alpkocak, 2008; Volkova and Bachrach, 2016; Zhang et al., 2018). More recently, Alejo et al. (2020) explored cross-lingual transfer approaches to predict emotion intensity in Twitter. Similarly, Akhtar et al. (2020) evaluated a series of feature-based machine learning models for both emotion and sentiment intensity prediction in social and news media. 3 Task & Data We define complaint severity prediction as a multiclass classification task."
2021.naacl-main.22,P19-1424,1,0.498034,"ionales it selects, as opposed to inferring explaand there is a large scope for further research. nations from the model’s decisions in a post-hoc manner (Ribeiro et al., 2016; Alvarez-Melis and 1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may"
2021.naacl-main.22,C18-1041,0,0.0188322,"all the ECHR articles in each case, which is not true. In reality, the Court considers only alleged violations of particular articles, argued by applicants. Establishing which articles are allegedly violated is an important preliminary task when preparing an EC t HR application. Instead of oversimplifying the overall judgment prediction task, we focus on the preliminary task and use it as a test-bed for generating paragraph-level rationales in a multi-label text classification task for the first time. Legal judgment prediction has also been studied in Chinese criminal cases (Luo et al., 2017; Hu et al., 2018; Zhong et al., 2018). Similarly to the literature on legal judgment prediction for ECtHR cases, the aforementioned approaches ignore the crucial aspect of justifying the models’ predictions. Given the gravity that legal outcomes have for individuals, explainability is essential to increase the trust of both legal professionals and laypersons on system decisions and promote the use of supportive tools (Barfield, 2020). To the best of our knowledge, our work is the first step towards this direction for the legal domain, but is also applicable in other domains (e.g., biomedical), where justifica"
2021.naacl-main.22,2020.findings-emnlp.7,0,0.0127884,"s are sensitive to specific language, e.g., they misuse (are easily fooled by) references to health issues and medical examinations as support for Article 3 alleged violations, or references to appeals in higher courts as support for Article 5, even when there is no concrete evidence.11 Manually inspecting the predicted rationales, we did not identify bias on demographics. Although such spurious features may be buried in the contextualized paragraph encodings (Pi[ CLS ] ). In general, de-biasing models could benefit rationale extraction and we aim to investigate this direction in future work (Huang et al., 2020). Plausibility: Plausibility refers to how convincing the interpretation is to humans (Jacovi and Goldberg, 2020). While the legal expert annotated all relevant facts with respect to allegations, according to his manual review, allegations can also be justified by sub-selections (parts) of rationales. Thus, although a method may fail to extract all the available rationales, the provided (incomplete) set of rationales may still be a convincing explanation. To properly estimate plausibility across methods, one has to perform a subjective human evaluation which we did not conduct due to lack of r"
2021.naacl-main.22,W19-4828,0,0.0416749,"Missing"
2021.naacl-main.22,N19-1357,0,0.0163759,"al., 2016; Alvarez-Melis and 1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle"
2021.naacl-main.22,2020.acl-main.409,0,0.0538525,"aint through a minimax game, where two players, one using the predicted binary mask and another using the complement of this mask, aim to correctly classify the text. If the first player fails to outperform the second, the model is penalized. Chang et al. (2019) use a Generative Adversarial Network (GAN) (Goodfellow et al., 2014), where a generator producing factual rationales competes with a generator producing counterfactual rationales to trick a discriminator. The GAN was not designed to perform classification. Given a text and a label it produces a rationale supporting (or not) the label. Jain et al. (2020) decoupled the model’s predictor from the rationale extractor to produce inherently faithful explanations, ensuring that the predictor considers only the rationales and not other parts of the text. Faithfulness refers to how accurately an explanation reflects the true reasoning of a model (Lipton, 2018; Jacovi and Goldberg, 2020). All the aforementioned work conceives rationales as selections of words, targeting binary classification tasks even when this is inappropriate. For instance, DeYoung et al. (2020) and Jain et al. (2020) over-simplified the task of the multipassage reading comprehensi"
2021.naacl-main.22,D17-1289,0,0.0863497,"model’s decisions in a post-hoc manner (Ribeiro et al., 2016; Alvarez-Melis and 1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems tha"
2021.naacl-main.22,D19-1445,0,0.0239455,"and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanation (Goodman and 226 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 226–241 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: A de"
2021.naacl-main.22,D16-1011,0,0.0585929,"Missing"
2021.naacl-main.22,2020.tacl-1.54,0,0.0172295,"rom a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanation (Goodman and 226 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 226–241 June 6–11, 2021. ©2021 Association for Computational Linguistics Figure 1: A depiction of the ECtHR p"
2021.naacl-main.22,D19-1002,0,0.0161259,"2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanation (Goodman and 226 Proceedings of the 2021 Conference o"
2021.naacl-main.22,N16-1174,0,0.0877746,"Missing"
2021.naacl-main.22,D19-1420,0,0.194451,"case study on European Court of Human Rights Cases Ilias Chalkidis † ‡ Manos Fergadiotis † ‡ Dimitrios Tsarapatsanis ?  ‡† Nikolaos Aletras Ion Androutsopoulos Prodromos Malakasiotis † ‡ † EY AI Centre of Excellence in Document Intelligence, NCSR “Demokritos” ‡ Department of Informatics, Athens University of Economics and Business  Computer Science Department, University of Sheffield ? Law School, University of York Abstract justification for their decisions, similar to those of humans, (Zaidan et al., 2007; Lei et al., 2016; Interpretability or explainability is an emergChang et al., 2019; Yu et al., 2019) by requiring the ing research field in NLP. From a user-centric models to satisfy additional constraints. point of view, the goal is to build models that provide proper justification for their deciHere we follow a user-centric approach to ratiosions, similar to those of humans, by requirnale extraction, where the model learns to select ing the models to satisfy additional constraints. a subset of the input that justifies its decision. To To this end, we introduce a new application this end, we introduce a new application on leon legal text where, contrary to mainstream gal text where, contrar"
2021.naacl-main.22,N07-1033,0,0.192157,"Missing"
2021.naacl-main.22,D16-1076,0,0.0493649,"Missing"
2021.naacl-main.22,2020.acl-main.466,0,0.0184695,"iction task, the goal is to predict the court’s decision; this is much more difficult and vastly relies on case law (precedent cases). Although the new task (alleged violation prediction) is simpler than legal judgment prediction, models that address it (and their rationales) can still be useful in the judicial process (Fig. 1). For example, they can help applicants (plaintiffs) identify alleged violations that are supported by the facts of a case. They can help judges identify more quickly facts that support the alleged violations, contributing towards more informed judicial decision making (Zhong et al., 2020). They can also help legal experts identify previous cases related to particular allegations, helping analyze case law (Katz, 2012). Our contributions are the following: • We introduce rationale extraction for alleged violation prediction in ECtHR cases, a more tractable task compared to legal judgment prediction. This is a multi-label classification task that requires paragraph-level rationales, unlike previous work on word-level rationales for binary classification. that continuity is not beneficial and requisite in paragraph-level rationale-extraction, while comprehensiveness needs to be re"
2021.naacl-main.22,D18-1390,0,0.0977148,"1 Introduction Jaakkola, 2017; Murdoch et al., 2018). Model interpretability (or explainability) is an Legal judgment prediction has been studied in emerging field of research in NLP (Lipton, 2018; the past for cases ruled by the European Court of Jacovi and Goldberg, 2020). From a model-centric Human Rights (Aletras et al., 2016; Medvedeva point of view, the main focus is to demystify a et al., 2018; Chalkidis et al., 2019) and for Chinese model’s inner workings, for example targeting self- criminal court cases (Luo et al., 2017; Hu et al., attention mechanisms (Jain and Wallace, 2019; 2018; Zhong et al., 2018), but there is no precedent Wiegreffe and Pinter, 2019), and more recently of work investigating the justification of the models’ Transformer-based language models (Clark et al., decisions. Similarly to other domains (e.g., finan2019; Kovaleva et al., 2019; Rogers et al., 2020). cial, biomedical), explainability is a key feature in From a user-centric point of view, the main fo- the legal domain, which may potentially improve cus is to build models that learn to provide proper the trustworthiness of systems that abide by the Correspondence to: ihalk.aueb.gr principle of the right to explanatio"
D19-3020,S17-2082,0,0.0594781,"Missing"
D19-3020,N16-1138,0,0.0326165,"limited number of production quality tools are currently available to support them in the individual steps of this process, with much of the technology still in research and experimental phases. In more detail, existing projects and tools are mostly focused on images/video forensics and verification (e.g. InVID (Teyssou et al., 2017), REVEAL3 ), crowdsourced verification (e.g. CheckDesk4 , Veri.ly5 ), fact-checking claims made by politicians (e.g. Politifact6 , FactCheck.org7 , FullFact8 ), citizen journalism (e.g. Citizen Desk), repositories of checked facts/rumours/websites (e.g. Emergent (Ferreira and Vlachos, 2016), • Hoaxy (Shao et al., 2016) is a recent open-source tool focused on visualising and searching over claims and fact checks. Such sophisticated visualisations are out of scope of our system, but relevant open-source visualisation tools, e.g. from Hoaxy, could be integrated in the future. • CrossCheck10 was a collaborative rumour checking project led by First Draft and Google News Lab, during the French elections. Its output was a useful dataset of false or unverified rumours. • Meedan’s Check4 is an open-source breaking news verification platform, which however does not support continuously up"
D19-3020,C18-1288,0,0.0162241,"the model using the available annotated rumours dataset prepared by the data pro11 Rumour Classification Model https://pytorch.org 117 Figure 1: Data flow diagram of the rumour classification service. denying, questioning and commenting. It is split into training, development and test set as in the RumourEval2017 challenge with 272, 25 and 28 rumours respectively. The dataset has a majority class baseline of 0.429. Evaluation is performed by comparing against several state-of-the-art approaches – NileTMRG (Enayet and El-Beltagy, 2017), Branch LSTM (Zubiaga et al., 2018b), Multi-task Learning (Kochkina et al., 2018), vanilla LSTM (without inner-attention), LSTM with soft attention (Bahdanau et al., 2014) and our Inner-Attention algorithm. The algorithms achieved F-1 scores of 0.539, 0.558, 0.491, 0.528, 0.496, 0.616 and accuracy of 0.571, 0.571, 0.5, 0.537, 0.5, and 0.607 respectively. We expect the results should improve as more data is added to the training set through the Rumour Annotation Service. Further analysis and discussion of the algorithm and its evaluation can be found in Aker et al. (2019). Figure 2: Network diagram of the rumour veracity model. 5 The Rumour Annotation Service part provides"
E14-1043,P13-1004,1,0.7237,"the various modelling approaches for the underlying inference task, the impact score (S) prediction of Twitter users based on a set of their actions. 5.1 Learning functions for regression We formulate this problem as a regression task, i.e. we infer a real numbered value based on a set of observed features. As a simple baseline, we apply Ridge Regression (RR) (Hoerl and Kennard, 1970), a reguralised version of the ordinary least squares. Most importantly, we focus on nonlinear methods for the impact score prediction task given the multimodality of the feature space. Recently, it was shown by Cohn and Specia (2013) that Support Vector Machines for Regression (SVR) (Vapnik, 1998; Smola and Sch¨olkopf, 2004), commonly considered the state-of-the-art for NLP regression tasks, can be outperformed by Gaussian Processes (GPs), a kernelised, probabilistic approach to learning (Rasmussen and Williams, 2006). Their setting is close to ours, in that they had few (17) features and were also aiming to predict a complex continuous phenomenon (human post-editing time). The initial stages of our experimental process confirmed that GPs performed better than SVR; thus, 407 we based our modelling around them, including R"
E14-1043,P13-1098,1,0.559762,"Missing"
E14-4005,N09-1003,0,0.128596,"Missing"
E14-4005,W13-0102,1,0.87551,"equencies. The matrix Z consists of V rows and columns representing the V vocabulary words. Element zij is the log of the number of documents that contains the words i and j normalised by the document frequency, DF, of the word j. Mimno et al. (2011) introduced that metric to measure topic coherence. We adapted it to estimate topic similarity by aggregating the co-document frequency of the words between two topics (Doc-Co-occ). Reference Corpus Semantic Space Topic words can also be represented as vectors in a semantic space constructed from an external source. We adapt the method proposed by Aletras and Stevenson (2013) for measuring topic coherence using distributional semantics1 . 2.5 Top-N Features A semantic space is constructed considering only the top n most frequent words in Wikipedia (excluding stop words) as context features. Each topic word is represented as a vector of n features weighted by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1989) between the topic word and each context feature, PMI(wi , wj )γ . γ is a variable for assigning more importance to higher PMI values. In our experiments, we set γ = 3 and found that the best performance is obtained for n = 5000. Similari"
E14-4005,D11-1024,0,0.262278,"Missing"
E14-4005,N10-1012,0,0.0267255,"titles weighted by their relevance to the keyword. For each topic, the centroid is computed from the keyword vectors. Similarity between topics is computed as the cosine similarity of the ESA centroid vectors. Topic Word Space Alternatively, we consider only the top-10 topic words from the two topics as context features to generate topic word vectors. Then, topic similarity is computed as the pairwise cosine similarity of the topic word vectors (RCSCos-TWS). 2.6 Feature Combination Using SVR Word Association Topic similarity can also be computed by applying word association measures directly. Newman et al. (2010) measure topic coherence as the average PMI between the topic words. This approach can be adapted to measure We also evaluate the performance of a support vector regression system (SVR) (Vapnik, 1998) with a linear kernel using a combination of approaches described above as features2 . The system is trained and tested using 10-fold cross validation. 1 Wikipedia is used as a reference corpus to count word co-occurrences and frequencies using a context window of ±10 words centred on a topic word. 2 With the exception of JSD, features based on the topics’ word probability distributions were not u"
E14-4005,P89-1010,0,0.568287,"document frequency of the words between two topics (Doc-Co-occ). Reference Corpus Semantic Space Topic words can also be represented as vectors in a semantic space constructed from an external source. We adapt the method proposed by Aletras and Stevenson (2013) for measuring topic coherence using distributional semantics1 . 2.5 Top-N Features A semantic space is constructed considering only the top n most frequent words in Wikipedia (excluding stop words) as context features. Each topic word is represented as a vector of n features weighted by computing the Pointwise Mutual Information (PMI) (Church and Hanks, 1989) between the topic word and each context feature, PMI(wi , wj )γ . γ is a variable for assigning more importance to higher PMI values. In our experiments, we set γ = 3 and found that the best performance is obtained for n = 5000. Similarity between two topics is defined as the average cosine similarity of the topic word vectors (RCS-Cos-N). Knowledge-based Methods UKB (Agirre et al., 2009) is used to generate a probability distribution over WordNet synsets for each word in the vocabulary V of the topic model using the Personalized PageRank algorithm. The similarity between two topic words is c"
E14-4005,D09-1026,0,0.0174585,"hef.ac.uk Abstract topic models, such as the Correlated Topic Model (CTM) (Blei and Lafferty, 2006), overcome this limitation and identify correlations between topics. Approaches to identifying similar topics for a range of tasks have been described in the literature but they have been restricted to using information from the word probability distribution to compare topics and have not been directly evaluated. Word distributions have been compared using a variety of measures such as KL-divergence (Li and McCallum, 2006; Wang et al., 2009; Newman et al., 2009), cosine measure (He et al., 2009; Ramage et al., 2009) and the average Log Odds Ratio (Chaney and Blei, 2012). Kim and Oh (2011) also applied the cosine measure and KL-Divergence which were compared with four other measures: Jaccard’s Coefficient, Kendall’s τ coefficient, Discount Cumulative Gain and Jensen Shannon Divergence (JSD). This paper compares a wider range of approaches to measuring topic similarity than previous work. In addition these measures are evaluated directly by comparing them against human judgements. Previous approaches to the problem of measuring similarity between automatically generated topics have been based on comparison"
E14-4005,D12-1087,0,0.0886818,"0.47 0.58 0.61 0.51 0.49 0.51 0.42 0.42 0.47 0.59 0.70 0.61 0.62 0.64 0.57 0.60 0.58 0.42 0.55 0.62 0.41 0.54 0.64 0.54 0.36 0.58 0.57 0.49 0.31 0.43 0.26 0.43 0.34 0.41 0.67 0.40 0.64 0.41 0.70 0.43 0.62 0.42 0.61 0.71 0.62 0.60 0.65 0.66 0.68 0.64 0.67 0.63 0.64 Table 1: Results for various approaches to topic similarity. All correlations are significant p < 0.001. Underlined scores denote best performance of a single feature. Bold denotes best overall performance. rics with human judgements increase with the number of topics for both corpora. This result is consistent with the findings of Stevens et al. (2012) that topic model coherence increases with the number of topics. Fewer topics makes the task of identifying similar topics more difficult because it is likely that they will contain some terms that do not relate to the topic’s main subject. Correlations in CTM are more stable for different number of topics because of the nature of the model, the pairs have been generated using the topic graph which by definition contains correlated topics. ate reliable estimates of tf.idf or co-document frequency. ESA, one of the knowledge-based methods (Section 2.5), performs well and is comparable to (or in"
E14-4005,D08-1038,0,0.0174424,"ons. It seems intuitively plausible that some automatically generated topics will be similar while others are dis-similar. For example, a topic about basketball (team game james season player nba play knicks coach league) is more similar to a topic about football (world cup team soccer africa player south game match goal) than one about the global finance (fed financial banks federal reserve bank bernanke rule crisis credit). Methods for automatically determining the similarity between topics have several potential applications, such as analysis of corpora to determine topics being discussed (Hall et al., 2008) or within topic browsers to decide which topics should be shown together (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). Latent Dirichlet Allocation (LDA) (Blei et al., 2003) is a popular type of topic model but cannot capture such correlations unless the semantic similarity between topics is measured. Other 2 Measuring Topic Similarity We compare measures based on word probability distributions (Section 2.1), distributional semantic methods (Sections 2.2-2.4), knowledge-based approaches (Section 2.5) and their combination (Section 2.6). 2.1 Topic Word Probability Di"
E14-4005,J90-1003,0,\N,Missing
E17-2111,N13-1016,1,0.897975,"ikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a common scale, and the optimal modality (text vs. image) for"
E17-2111,P14-2103,1,0.88136,"ethods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of candidate labels by relevance to the topic. Textual labels have been sourced from in a number of different ways, including noun chunks from a reference corpus (Mei et al., 2007), Wikipedia ar701 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 701–706, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ticle titles (Lau et al., 2011; Aletras and Stevenson, 2014; Bhatia et al., 2016), or short text summaries (Cano Basave et al., 2014; Wan and Wang, 2016). Images are often selected from Wikipedia or the web based on querying with topic words (Aletras and Stevenson, 2013; Aletras and Mittal, 2017). Recent work on topic labelling has shown that text or image embeddings can improve candidate label generation and ranking (Bhatia et al., 2016; Aletras and Mittal, 2017). Bhatia et al. (2016) use word2vec (Mikolov et al., 2013) and doc2vec (Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most rele"
E17-2111,C16-1091,1,0.294226,"ut Sorodoc1 , Jey Han Lau1,2 , Nikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a common scale, and the"
E17-2111,P16-1110,0,0.0188527,"rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems. 1 Introduction LDA-style topic models (Blei et al., 2003) are a popular approach to document clustering, with the “topics” (in the form of multinominal distributions over words) and topic allocations per document (in the form of a multinomial distribution over the topics) providing a powerful document collection visualisation, gisting and navigational aid (Griffiths et al., 2007; Newman et al., 2010a; Chaney and Blei, 2012; Sievert and Shirley, 2014; Poursabzi-Sangdeh et al., 2016). Given its internal structure, an obvious way of presenting a topic t is as a ranked list of the highest-probability terms wi based on Pr(wi |t), often simply based on a fixed “cardinality” (i.e. number of topic words) such as 10. However, this has a number of disadvantages: (a) there is a cognitive load in forming an impression of what concept the topic represents from its topic words (Ale2 Related work Topic labelling methods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of candidate labels by relevance to"
E17-2111,W14-3110,0,0.143765,"automatically generates a rating for the label, relative to the topic. Experiments show that this multimodal approach outperforms single-modality topic labelling systems. 1 Introduction LDA-style topic models (Blei et al., 2003) are a popular approach to document clustering, with the “topics” (in the form of multinominal distributions over words) and topic allocations per document (in the form of a multinomial distribution over the topics) providing a powerful document collection visualisation, gisting and navigational aid (Griffiths et al., 2007; Newman et al., 2010a; Chaney and Blei, 2012; Sievert and Shirley, 2014; Poursabzi-Sangdeh et al., 2016). Given its internal structure, an obvious way of presenting a topic t is as a ranked list of the highest-probability terms wi based on Pr(wi |t), often simply based on a fixed “cardinality” (i.e. number of topic words) such as 10. However, this has a number of disadvantages: (a) there is a cognitive load in forming an impression of what concept the topic represents from its topic words (Ale2 Related work Topic labelling methods usually involve two main steps: (1) the generation of candidate labels (e.g. text or images) for a given topic; and (2) the ranking of"
E17-2111,N16-1057,1,0.860234,"Missing"
E17-2111,P16-1217,0,0.0185589,"r a given topic; and (2) the ranking of candidate labels by relevance to the topic. Textual labels have been sourced from in a number of different ways, including noun chunks from a reference corpus (Mei et al., 2007), Wikipedia ar701 Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 701–706, c Valencia, Spain, April 3-7, 2017. 2017 Association for Computational Linguistics ticle titles (Lau et al., 2011; Aletras and Stevenson, 2014; Bhatia et al., 2016), or short text summaries (Cano Basave et al., 2014; Wan and Wang, 2016). Images are often selected from Wikipedia or the web based on querying with topic words (Aletras and Stevenson, 2013; Aletras and Mittal, 2017). Recent work on topic labelling has shown that text or image embeddings can improve candidate label generation and ranking (Bhatia et al., 2016; Aletras and Mittal, 2017). Bhatia et al. (2016) use word2vec (Mikolov et al., 2013) and doc2vec (Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most relevant textual labels for a topic are selected from Wikipedia article titles using the cosine si"
E17-2111,P11-1154,1,0.80732,"opic Labelling Ionut Sorodoc1 , Jey Han Lau1,2 , Nikolaos Aletras3 and Timothy Baldwin1 1 Computing and Information Systems, The University of Melbourne 2 IBM Research 3 Amazon.com {ionutsorodoc,jeyhan.lau,nikos.aletras}@gmail.com tb@ldwin.net Abstract tras et al., 2014; Aletras et al., 2017); (b) there is a potential bias in presenting the topic based on a fixed cardinality (Lau and Baldwin, 2016); and (c) it can be hard to interpret mixed or incoherent topics (Newman et al., 2010b). Automatic topic labelling methods have been proposed to assist with topic interpretation, e.g. based on text (Lau et al., 2011; Bhatia et al., 2016) or images (Aletras and Stevenson, 2013; Aletras and Mittal, 2017), with recent work showing that the optimal modality (i.e. text or image) for topic labelling varies across topics (Aletras and Mittal, 2017). The focus of this paper is the automatic rating of a textual or image label for a given topic. Our contributions are as follows: 1. we develop and release a novel topic labelling dataset with manually-scored image and text labels for a diverse set of topics; one particular point of divergence from other text–image datasets is that text and image labels are rated on a"
E17-2111,P14-2050,0,0.0115681,"(Le and Mikolov, 2014) to represent topics and candidate textual labels in the same latent semantic space. The most relevant textual labels for a topic are selected from Wikipedia article titles using the cosine similarity between the topic and article title embeddings. Finally, top labels are re-ranked in a supervised fashion using various features such as the PageRank score of the article in Wikipedia (Brin and Page, 1998), trigram letter ranking (Kou et al., 2015), topic word overlap, and word length of the label. Aletras and Mittal (2017) use pre-computed dependency-based word embeddings (Levy and Goldberg, 2014) to represent the topics and the caption of the images, as well as image embeddings using the output layer of VGG-net (Simonyan and Zisserman, 2014) pretrained on ImageNet (Deng et al., 2009). A concatenation of these three vectors is the input to a simple deep neural network with four hidden layers and a sigmoid output layer to predict the relevance score. Textual or visual modalities for labelling topics have been studied extensively, although independently from one another. Our work differs from the single-modality methods described above in that it uses a joint model to predict the continu"
E17-2111,P14-2101,0,\N,Missing
N13-1016,E09-1005,0,0.0161532,"s, i.e. SIFT features 2 https://developers.google.com/ apis-explorer/#s/customsearch/v1 3 http://en.wikipedia.org Ranking Candidate Images We rank images in the candidates set using graphbased algorithms. The graph is created by treating images as nodes and using similarity scores (textual or visual) between images to weight the edges. 3.3.1 PageRank PageRank (Page et al., 1999) is a graph-based algorithm for identifying important nodes in a graph that was originally developed for assigning importance to web pages. It has been used for a range of NLP tasks including word sense disambiguation (Agirre and Soroa, 2009) and keyword extraction (Mihalcea and Tarau, 2004). Let G = (V, E) be a graph with a set of vertices, V , denoting image candidates and a set of edges, E, denoting similarity scores between two images. For example, sim(Vi , Vj ) indicates the similarity between images Vi and Vj . The PageRank score (P r) over G for an image (Vi ) can be computed by the following equation: P r(Vi ) = d · X Vj ∈C(Vi ) sim(Vi , Vj ) P P r(Vj ) + (1 − d)v sim(Vj , Vk ) Vk ∈C(Vj ) (1) where C(Vi ) denotes the set of vertices which are connected to the vertex Vi . d is the damping factor which is set to the default"
N13-1016,W13-0102,1,0.780223,"t average human annotations). categories (e.g. SPORTS, POLITICS, COMPUTING ). Categories that have more that 80 articles associated with them are considered. These articles are collected to produce a corpus of approximately 60,000 articles generated from 1,461 categories. Documents in the two collections are tokenised and stop words removed. LDA was applied to learn 200 topics from NYT and 400 topics from WIKI. The gensim package6 was used to implement and compute LDA. The hyperparameters (α, β) were set to num of1 topics . Incoherent topics are filtered out by applying the method proposed by Aletras and Stevenson (2013). We randomly selected 100 topics from NYT and 200 topics from WIKI resulting in a data set of 300 topics. Candidate images for these topics were generated using the approach described in Section 3.1, producing a total of 6,000 candidate images (20 for 6 each topic). 4.2 Human judgements of the suitability of each image were obtained using an online crowdsourcing platform, Crowdflower7 . Annotators were provided with a topic (represented as a set of 10 keywords) and a candidate image. They were asked to judge how appropriate the image was as a representation of the main subject of the topic an"
N13-1016,D07-1109,0,0.0489585,"Missing"
N13-1016,W11-2503,0,0.0286949,"entified by applying graph centrality measures which assume that words that cooccurring in text are likely to refer to concepts that are close in the DBpedia graph. Our own work differs from the approaches described above since, to our knowledge, it is the first to propose labelling topics with images rather than text. Recent advances in computer vision has lead to the development of reliable techniques for exploiting information available in images (Datta et al., 2008; Szeliski, 2010) and these have been combined with NLP (Feng and Lapata, 2010a; Feng and Lapata, 2010b; Agrawal et al., 2011; Bruni et al., 2011). The closest work to our own is the text illustration techniques which have been proposed for story picturing (Joshi et al., 2006) and news articles illustration (Feng and Lapata, 2010b). The input to text illustration models is a textual document and a set of image candidates. The goal of the models is to associate the document with the correct image. Moreover, the problem of ranking images returned from a text query is related to, but different from, the one explored in our paper. Those approaches used queries that were much smaller (e.g. between one and three words) and more focussed than"
N13-1016,P10-1126,0,0.0423325,"e statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the problem of selecting images to illustrate automatically generated topics. Our approa"
N13-1016,N10-1125,0,0.0411802,"e statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the problem of selecting images to illustrate automatically generated topics. Our approa"
N13-1016,N09-1041,0,0.0470478,"nes and can provide images that are useful to represent a topic. 1 Introduction Topic models are statistical methods for summarising the content of a document collection using latent variables known as topics (Hofmann, 1999; Blei et al., 2003). Within a model, each topic is a multinomial distribution over words in the collection while documents are represented as distributions over topics. Topic modelling is now widely used in Natural Language Processing (NLP) and has been applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009), information retrieval (Wei and Croft, 2006), image labelling (Feng and Lapata, 2010a) and visualisation of document collections (Chaney and Blei, 2012). An alternative approach is to represent a topic using an illustrative image (or set of images). Images have the advantage that they can be understood quickly and are language independent. This is particularly important for applications in which the topics are used to provide an overview of a collection with many topics being shown simultaneously (Chaney and Blei, 2012; Gretarsson et al., 2012; Hinneburg et al., 2012). This paper explores the"
N13-1016,C10-2069,0,0.684296,"4DP, UK {n.aletras, m.stevenson}@dcs.shef.ac.uk Abstract Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). Topics generated automatically, e.g. using LDA, are now widely used in Computational Linguistics. Topics are normally represented as a set of keywords, often the n terms in a topic with the highest marginal probabilities. We introduce an alternative approach in which topics are represented using images. Candidate images for each topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image"
N13-1016,P11-1154,0,0.805906,", m.stevenson}@dcs.shef.ac.uk Abstract Topics are often represented by using the n terms with the highest marginal probabilities in the topic to generate a set of keywords. For example, wine, bottle, grape, flavour, dry. Interpreting such lists may not be straightforward, particularly since there may be no access to the source collection used to train the model. Therefore, researchers have recently begun developing automatic methods to generate meaningful and representative labels for topics. These techniques have focussed on the creation of textual labels (Mei et al., 2007; Lau et al., 2010; Lau et al., 2011). Topics generated automatically, e.g. using LDA, are now widely used in Computational Linguistics. Topics are normally represented as a set of keywords, often the n terms in a topic with the highest marginal probabilities. We introduce an alternative approach in which topics are represented using images. Candidate images for each topic are retrieved from the web by querying a search engine using the top n terms. The most suitable image is selected from this set using a graph-based algorithm which makes use of textual information from the metadata associated with each image and features extrac"
N13-1016,W04-3252,0,\N,Missing
P13-4026,W12-1014,1,0.797453,"ed. Consequently links to relevant articles in Wikipedia were added to each the meta-data of each artefact using Wikipedia Miner (Milne and Witten, 2008) to provide background information. In addition to the link, Wikipedia Miner returns a confidence value between 0 and 1 for each link based on the context of the item. The accuracy of the links added by Wikipedia Miner were evaluated using the meta-data associated with 21 randomly selected artefacts. Three annotators analysed the links added and found that a confidence value of 0.5 represented a good balance between accuracy and coverage. See Fernando and Stevenson (2012) for further details. Filtered 466,958 1,219,731 14,983 1,701,672 Table 1: Number of artefacts in Europeana collections before and after filtering 4 Data Processing A range of pre-preprocessing steps were carried out on these collections to provide additional information to support navigation in the PATHS system. 4.1 Artefact Similarity 4.3 We begin by computing the similarity between the various artefacts in the Europeana collections. This information is useful for navigation and recommendation but is not available in the Europeana collections since they are drawn from a diverse range of sour"
P13-4026,C12-1054,1,0.879666,"Missing"
P13-4026,W07-0907,0,0.022716,"this information is used within the system. 1 2 Related Work Heitzman et al. (1997) describe the ILEX system which acts as a guide through the jewellery collection of the National Museum of Scotland. The user explores the collection through a set of web pages which provide descriptions of each artefact that are personalised for each user. The system makes use of information about the artefacts the user has viewed to build up a model of their interests and uses this to customise the descriptions of each artefact and provide recommendations for further artefacts in which they may be interested. Grieser et al. (2007) also explore providing recommendations based on the artefacts a user has viewed so far. They make use of a range of techniques including language modelling, geospatial modelling and analysis of previous visitors’ behaviour to provide recommendations to visitors to the Melbourne Museum. Grieser et al. (2011) explore methods for determining the similarity between museum artefacts, commenting that this is useful for navigation through these collections and important for personalisation (Bowen and Filippini-Fantoni, 2004; O’Donnell et al., 2001), recommendation (Bohnert et al., 2009; Trant, 2009)"
P14-2103,N13-1016,1,0.775279,"Missing"
P14-2103,C10-2069,0,0.216961,"ent Court, 211 Portobello Sheffield, S1 4DP United Kingdom {n.aletras, m.stevenson}@dcs.shef.ac.uk Abstract could be labelled as E DUCATION and a suitable label for the topic shown above would be G LOBAL F INANCIAL C RISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms a"
P14-2103,P11-1154,0,0.217497,"ic shown above would be G LOBAL F INANCIAL C RISIS. Approaches that make use of alternative modalities, such as images (Aletras and Stevenson, 2013), have also been proposed. Mei et al. (2007) label topics using statistically significant bigrams identified in a reference collection. Magatti et al. (2009) introduced an approach for labelling topics that relied on two hierarchical knowledge resources labelled by humans, while Lau et al. (2010) proposed selecting the most representative word from a topic as its label. Hulpus et al. (2013) make use of structured data from DBpedia to label topics. Lau et al. (2011) proposed a method for automatically labelling topics using information from Wikipedia. A set of candidate labels is generated from Wikipedia article titles by querying using topic terms. Additional labels are then generated by chunk parsing the article titles to identify n-grams that represent Wikipedia articles as well. Outlier labels (less relevant to the topic) are identified and removed. Finally, the top-5 topic terms are added to the candidate set. The labels are ranked using Support Vector Regression (SVR) (Vapnik, 1998) and features extracted using word association measures (i.e. PMI,"
P14-2103,N13-3002,0,0.0235252,"d generate a graph from the words contained in the results. PageRank is then used to weigh the words in the graph and score the candidate labels. The state-of-the-art method for this task is supervised (Lau et al., 2011). Evaluation on a standard data set shows that the performance of our approach is consistently superior to previously reported methods. 1 Introduction Topic models (Hofmann, 1999; Blei et al., 2003) have proved to be a useful way to represent the content of document collections, e.g. (Chaney and Blei, 2012; Ganguly et al., 2013; Gretarsson et al., 2012; Hinneburg et al., 2012; Snyder et al., 2013). In these interfaces, topics need to be presented to users in an easily interpretable way. A common way to represent topics is as set of keywords generated from the n terms with the highest marginal probabilities. For example, a topic about the global financial crisis could be represented by its top 10 most probable terms: FINANCIAL , BANK , MARKET, GOVERNMENT, MORTGAGE , BAILOUT, BILLION , STREET, WALL , CRISIS . But interpreting such lists is not always straightforward, particularly since background knowledge may be required (Chang et al., 2009). Textual labels could assist with the interpr"
P14-2103,W04-3252,0,\N,Missing
P15-1169,D11-1120,0,0.738375,"Missing"
P15-1169,P13-1098,1,0.872331,"Missing"
P15-1169,E14-1043,1,0.897212,"Missing"
P15-1169,P13-1004,0,0.0129022,"ral embeddings. We apply spectral clustering on this matrix to obtain 30, 50, 100 and 200 word clusters. 5 In this section, we briefly overview Gaussian Process (GP) for classification, highlighting our motivation for using this method. GPs formulate a Bayesian non-parametric machine learning framework which defines a prior on functions (Rasmussen and Williams, 2006). The properties of the functions are given by a kernel which models the covariance in the response values as a function of its inputs. Although GPs form a powerful learning tool, they have only recently been used in NLP research (Cohn and Specia, 2013; Preot¸iuc-Pietro and Cohn, 2013) with classification applications limited to (Polajnar et al., 2011). Formally, GP methods aim to learn a function f : Rd → R drawn from a GP prior given the inputs x ∈ Rd : P Cw = x∈c NPMI(w, x) |c |− 1 , 4.2.3 Neural Embeddings (W2V-E) Recently, there has been a growing interest in neural language models, where the words are projected into a lower dimensional dense vector space via a hidden layer (Mikolov et al., 2013b). These models showed they can provide a better representation of words compared to traditional language models (Mikolov et al., 2013c) becau"
P15-1169,P14-1016,0,0.0813377,"Missing"
P15-1169,N13-1090,0,0.00346881,"e response values as a function of its inputs. Although GPs form a powerful learning tool, they have only recently been used in NLP research (Cohn and Specia, 2013; Preot¸iuc-Pietro and Cohn, 2013) with classification applications limited to (Polajnar et al., 2011). Formally, GP methods aim to learn a function f : Rd → R drawn from a GP prior given the inputs x ∈ Rd : P Cw = x∈c NPMI(w, x) |c |− 1 , 4.2.3 Neural Embeddings (W2V-E) Recently, there has been a growing interest in neural language models, where the words are projected into a lower dimensional dense vector space via a hidden layer (Mikolov et al., 2013b). These models showed they can provide a better representation of words compared to traditional language models (Mikolov et al., 2013c) because they capture syntactic information rather than just bag-of-context, handling non-linear transformations. In this low dimensional vector space, words with a small distance are considered semantically similar. We use the skipgram model with negative sampling (Mikolov et al., 2013a) to learn word embeddings on the Twitter reference corpus. In that case, the skip-gram model is factorising a word-context PMI matrix (Levy and Goldberg, 2014). We use a laye"
P15-1169,D13-1100,1,0.884751,"Missing"
P19-1424,E17-2041,0,0.148215,"iased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights"
P19-1424,P18-2041,1,0.816977,"tance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent a"
P19-1424,N19-1423,0,0.455709,"d by Aletras et al. (2016); (2) multi-label classification (type of violation, if any); (3) case importance detection. In all tasks, neural models outperform an SVM with bag-of-words (Aletras et al., 2016; Medvedeva et al., 2018), the only method tested in English legal judgment prediction so far. As a third contribution, we use an approach based on data anonymization to study, for the first time, whether the legal predictive models are biased towards demographic information or factual information relevant to human rights. Finally, as a side-product, we propose a hierarchical version of BERT (Devlin et al., 2019), which bypasses BERT ’s length limitation and leads to the best results. 2 ECHR Dataset ECHR hears allegations that a state has breached human rights provisions of the European Conven1 The dataset is submitted at https://archive. org/details/ECHR-ACL2019. 4317 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317–4323 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics tion of Human Rights.2 Our dataset contains approx. 11.5k cases from ECHR’s public database.3 For each case, the dataset provides a list of f"
P19-1424,D17-1289,0,0.282592,"., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights organizations and legal scholars can employ them to scrutinize the fairness of judicial decisions unveiling if they correlate with biases (Doshi-Velez and Kim, 2017; Binns et al., 2018). This paper contributes a new publicly availa"
P19-1424,N18-1100,0,0.0384572,"o the output layer using a sigmoid for binary violation, softmax for multi-label violation, or no activation for case importance regression. HAN: The Hierarchical Attention Network (Yang et al., 2016) is a state-of-the-art model for text classification. We use a slightly modified version where a BIGRU with self-attention reads the words of each fact, as in BIGRU - ATT, producing fact embeddings. A second-level BIGRU with selfattention reads the fact embeddings, producing a single case embedding that goes through a similar output layer as in BIGRU - ATT. LWAN: The Label-Wise Attention Network (Mullenbach et al., 2018) has been shown to be robust in multi-label classification. Instead of a single attention mechanism, LWAN employs L attentions, one for each possible label. This produces P L case embeddings (h(l) = i al,i hi ) per case, each one specialized to predict the corresponding label. Each of the case embeddings goes through a separate linear layer (L linear layers in total), each with a sigmoid, to decide if the corresponding label should be assigned. Since this is a multi-label model, we use it only in multi-label violation. BERT and HIER-BERT: BERT (Devlin et al., 2019) is a language model based on"
P19-1424,D08-1046,0,0.630188,"g strong baselines that surpass previous feature-based models in three tasks: (1) binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlo"
P19-1424,D14-1162,0,0.0852768,"em to BERT’s maximum length, which affects its performance. This also highlights an important limitation of BERT in processing long documents, a common characteristic in legal text processing. To surpass BERT’s maximum length limitation, we also propose a hierarchical version of BERT ( HIER - BERT ). Firstly BERT- BASE reads the words of each fact, producing fact embeddings. Then a self-attention mechanism reads fact embeddings, producing a single case embedding that goes through a similar output layer as in HAN. 5 Experiments 5.1 Experimental Setup Hyper-parameters: We use pre-trained GLOVE (Pennington et al., 2014) embeddings (d = 200) for all experiments. Hyper-parameters are tuned by random sampling 50 combinations and selecting the values with the best development loss in each task.6 Given the best hyper-parameters, we perform five runs for each model reporting mean scores and standard deviations. We use categorical cross-entropy loss for the classification tasks and mean absolute error for the regression task, Glorot initialization (Glorot and Bengio, 2010), Adam (Kingma and Ba, 2015) with default learning rate 0.001, and early stopping on the development loss. Baselines: A majority-class (MAJORITY)"
P19-1424,C18-1041,0,0.224557,"ce, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights organizations and legal scholars can employ them to scrutinize the fairness of judicial decisions unveiling if they correlate with biases (Doshi-Velez and Kim, 2017; Binns et al., 2018). This paper contributes a new publicly available English legal judgment prediction"
P19-1424,sulea-etal-2017-predicting,0,0.0773369,"Missing"
P19-1424,N19-1357,0,0.0316993,"ction and annotation of these attributes are manually crafted and dependent to the court. Zhong et al. (2018) decompose the problem of charge prediction into different subtasks that are tailored to the Chinese criminal court using multitask learning. 7 Limitations and Future Work The neural models we considered outperform previous feature-based models, but provide no justification for their predictions. Attention scores (Fig. 1) provide some indications of which parts of the texts affect the predictions most, but are far from being justifications that legal practitioners could trust; see also Jain and Wallace (2019). Providing valid justifications is an important priority for future work and an emerging topic in the NLP community.8 In this direction, we plan to expand the scope of this study by exploring the automated analysis of additional resources (e.g., relevant case law, dockets, prior judgments) that could be then utilized in a multi-input fashion to further improve performance and justify system decisions. We also plan to apply neural methods to data from other courts, e.g., the European Court of Justice, the US Supreme Court, and multiple languages, to gain a broader perspective of their potentia"
P19-1424,P12-1078,0,0.202689,"on; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the lik"
P19-1424,N16-1174,0,0.0436725,"Missing"
P19-1424,N18-1168,0,0.0677892,") binary violation classification; (2) multi-label classification; (3) case importance prediction. We also explore if models are biased towards demographic information via data anonymization. As a side-product, we propose a hierarchical version of BERT, which bypasses BERT’s length limitation. 1 Introduction Legal information is often represented in textual form (e.g., legal cases, contracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges"
P19-1424,D18-1390,0,0.229121,"ntracts, bills). Hence, legal text processing is a growing area in NLP with various applications such as legal topic classification (Nallapati and Manning, 2008; Chalkidis et al., 2019), court opinion generation (Ye et al., 2018) and analysis (Wang et al., 2012), legal information extraction (Chalkidis et al., 2018), and entity recognition (Cardellino et al., 2017; Chalkidis et al., 2017). Here, we focus on legal judgment prediction, where given a text describing the facts of a legal case, the goal is to predict the court’s outcome (Aletras et al., 2016; S¸ulea et al., 2017; Luo et al., 2017; Zhong et al., 2018; Hu et al., 2018). Such models may assist legal practitioners and citizens, while reducing legal costs and improving access to justice (Lawlor, 1963; Katz, 2012; Stevenson and Wagoner, 2015). Lawyers and judges can use them to estimate the likelihood of winning a case and come to more consistent and informed judgments, respectively. Human rights organizations and legal scholars can employ them to scrutinize the fairness of judicial decisions unveiling if they correlate with biases (Doshi-Velez and Kim, 2017; Binns et al., 2018). This paper contributes a new publicly available English legal ju"
P19-1424,W19-2209,1,\N,Missing
P19-1495,J08-4004,0,0.0319243,"Missing"
P19-1495,P07-1056,0,0.144709,"randomly sample an equal number of tweets addressed to each customer support handle for annotation. Using this method, we collected 1,971 tweets to which the customer support handles responded. Further, we have also manually grouped the customer support handles in several high-level domains based on their industry type and area of activity. We have done this to enable analyzing complaints by domain and assess transferability of classifiers across domains. In related work on sentiment analysis, reviews for products from four different domains were collected across domains in a similar fashion (Blitzer et al., 2007). All customer support handles grouped by category are presented in Table 2. We add to our data set randomly sampled tweets to ensure that there is a more representative and 2 https://developer.twitter.com/ diverse set of tweets for feature analysis and to ensure that the evaluation does not disproportionally contain complaints. We thus additionally sampled 1,478 tweets consisting of two groups of 739 tweets: the first group contains random tweets addressed to any other Twitter handle (at-replies) to match the initial sample, while the second group contains tweets not addressed to a Twitter ha"
P19-1495,P13-1025,0,0.0297088,"quent product or service the account usually receives complaints about (e.g., NikeSupport receives most complaints about the Nike Fitness Bands). Category Food & Beverage Apparel Retail Cars Services Software & Online Services Transport Electronics Other Total Complaints 95 141 124 67 207 189 139 174 96 1232 Not Complaints 35 117 75 25 130 103 109 112 33 739 Table 3: Number of tweets annotated as complaints across the nine domains. ing sentiment or emotion which have an overlap with complaints and complaint specific features which capture linguistic aspects typical of complaints (Meinl, 2013; Danescu-Niculescu-Mizil et al., 2013): Unigrams. We use the bag-of-words approach to represent each tweet as a TF-IDF weighted distribution over the vocabulary consisting of all words present in at least two tweets (2,641 words). LIWC. Traditional psychology studies use dictionary-based approaches to representing text. The most popular method is based on Linguistic Inquiry and Word Count (LIWC) (Pennebaker et al., 2001) consisting of 73 manually constructed lists of words (Pennebaker et al., 2015) including parts-of-speech, topical or stylistic categories. Each tweet is thus represented as a distribution over these categories. Wo"
P19-1495,P07-1033,0,0.0535955,"Missing"
P19-1495,R13-1026,0,0.0372747,"g vectors (Mikolov et al., 2013). The clusters help reduce the feature space and provide good interpretability.3 For brevity and clarity, we present experiments using 200 clusters as in (Preot¸iucPietro et al., 2015). We aggregated all the words in a tweet and represent each tweet as a distribution of the fraction of tokens belonging to each cluster. Part-of-Speech Tags. We analyze part-of-speech tag usage to quantify the syntactic patterns associated with complaints and to enhance the representation of unigrams. We part-of-speech tag all tweets using the Twitter model of the Stanford Tagger (Derczynski et al., 2013). In prediction experiments we supplement each unigram feature with their POS tag (e.g., I PRP, bought VBN). For feature analysis, we represent each tweet as a bag-of-words distribution over part-of-speech unigrams and bigrams in order to uncover regular syntactic patterns specific of complaints. Sentiment & Emotion Models. We use existing sentiment and emotion analysis models to study their relationship to complaint annotations and to measure their predictive power on our complaint data set. If the concepts of negative sentiment and complaint were to coincide, standard sentiment prediction mo"
P19-1495,P11-2102,0,0.187151,"Missing"
P19-1495,S17-2094,0,0.0314464,"Missing"
P19-1495,E14-1043,1,0.916385,"Missing"
P19-1495,P12-3005,0,0.0293436,"We thus additionally sampled 1,478 tweets consisting of two groups of 739 tweets: the first group contains random tweets addressed to any other Twitter handle (at-replies) to match the initial sample, while the second group contains tweets not addressed to a Twitter handle. As preprocessing, we anonymize all usernames present in the tweet and URLs and replace them with placeholder tokens. To extract the unigrams used as features, we use DLATK, which handles social media content and markup such as emoticons or hashtags (Schwartz et al., 2017). Tweets were filtered for English using langid.py (Lui and Baldwin, 2012) and retweets were excluded. 3.2 Annotation We create a binary annotation task for identifying if a tweet contains a complaint or not. Tweets are short and usually express a single thought. Therefore, we consider the entire tweet as a complaint if it contains at least one complaint speech act. For annotation, we adopt as the guideline a complaint definition similar to that from previous linguistic research (Olshtain and Weinbach, 1987; Cohen and Olshtain, 1993): “A complaint presents a state of affairs which breaches the writer’s favorable expectation”. Each tweet was labeled by two independen"
P19-1495,N13-1090,0,0.0100351,"utomatically generated word clusters. These clusters can be thought of as topics i.e., groups of words that are semantically and/or syntactically similar. The clusters help reduce the feature space and provide good interpretability (Lampos et al., 2014; Preot¸iuc-Pietro et al., 2015; Preot¸iuc-Pietro et al., 2015; Lampos et al., 2016; Aletras and Chamberlain, 2018). We follow Preot¸iuc-Pietro et al. (2015) to compute clusters using spectral clustering (Shi and Malik, 2000) applied to a word-word similarity matrix weighted with the cosine similarity of the corresponding word embedding vectors (Mikolov et al., 2013). The clusters help reduce the feature space and provide good interpretability.3 For brevity and clarity, we present experiments using 200 clusters as in (Preot¸iucPietro et al., 2015). We aggregated all the words in a tweet and represent each tweet as a distribution of the fraction of tokens belonging to each cluster. Part-of-Speech Tags. We analyze part-of-speech tag usage to quantify the syntactic patterns associated with complaints and to enhance the representation of unigrams. We part-of-speech tag all tweets using the Twitter model of the Stanford Tagger (Derczynski et al., 2013). In pre"
P19-1495,N18-1146,0,0.0253879,"r mediated communication, V´asquez (2011) performed an analysis of 100 negative reviews on TripAdvisor, which showed that complaints in this medium often co-occur with other speech acts including positive and negative remarks, frequently make explicit references to expectations not being met and directly demand a reparation or compensation. Meinl (2013) studied complaints in eBay reviews by annotating 200 reviews in English and German with the speech act sequence that makes up each complaint e.g., warning, annoyance (the annotations are not available publicly or after contacting the authors). Mikolov et al. (2018) analyze which financial complaints submitted to the Consumer Financial Protection Bureau will receive a timely response. Most recently, Yang et al. (2019) studied customer support dialogues and predicted if these complaints will be escalated with a government agency or made public on social media. To the best of our knowledge, the only previous work that tackles a concept defined as a complaint with computational methods is by Zhou and Ganesan (2016) which studies Yelp reviews. However, they define a complaint as a ‘sentence with negative connotation with supplemental information’. This defin"
P19-1495,D14-1162,0,0.0801696,"ment and complaint were to coincide, standard sentiment prediction models that have access to larger sets of training data should be very competitive on predicting complaints. We test the following models: • MPQA: We use the MPQA sentiment lexicon (Wiebe et al., 2005) to assign a positive and negative score to each tweet based on the ratio of tokens in a tweet which appear in the positive and negative MPQA lists respectively. These scores are used as features. • NRC: We use the word lexicon derived using 3 We have tried other alternatives to building clusters: using NPMI (Bouma, 2009), GloVe (Pennington et al., 2014) and LDA (Blei et al., 2003). 5011 crowd-sourcing from (Mohammad and Turney, 2010, 2013) for assigning to each tweet the proportion of tokens that have positive, negative and neutral sentiment, as well as one of eight emotions that include the six basic emotions of Ekman (Ekman, 1992) (anger, disgust, fear, joy, sadness and surprise) plus trust and anticipation. All scores are used as features in prediction in order to maximize their predictive power. • Volkova & Bachrach (V&B): We quantify positive, negative and neutral sentiment as well as the six Ekman emotions for each message using the mo"
P19-1495,S16-1003,0,0.0791331,"Missing"
P19-1495,P15-1169,1,0.91667,"Missing"
P19-1495,S18-1001,0,0.0613678,"Missing"
P19-1495,C18-1130,1,0.90615,"Missing"
P19-1495,P17-1068,1,0.927957,"Missing"
P19-1495,W10-0204,0,0.0449484,"ave access to larger sets of training data should be very competitive on predicting complaints. We test the following models: • MPQA: We use the MPQA sentiment lexicon (Wiebe et al., 2005) to assign a positive and negative score to each tweet based on the ratio of tokens in a tweet which appear in the positive and negative MPQA lists respectively. These scores are used as features. • NRC: We use the word lexicon derived using 3 We have tried other alternatives to building clusters: using NPMI (Bouma, 2009), GloVe (Pennington et al., 2014) and LDA (Blei et al., 2003). 5011 crowd-sourcing from (Mohammad and Turney, 2010, 2013) for assigning to each tweet the proportion of tokens that have positive, negative and neutral sentiment, as well as one of eight emotions that include the six basic emotions of Ekman (Ekman, 1992) (anger, disgust, fear, joy, sadness and surprise) plus trust and anticipation. All scores are used as features in prediction in order to maximize their predictive power. • Volkova & Bachrach (V&B): We quantify positive, negative and neutral sentiment as well as the six Ekman emotions for each message using the model made available in (Volkova and Bachrach, 2016) and use them as features in pr"
P19-1495,D17-2010,0,0.131081,"ensure that the evaluation does not disproportionally contain complaints. We thus additionally sampled 1,478 tweets consisting of two groups of 739 tweets: the first group contains random tweets addressed to any other Twitter handle (at-replies) to match the initial sample, while the second group contains tweets not addressed to a Twitter handle. As preprocessing, we anonymize all usernames present in the tweet and URLs and replace them with placeholder tokens. To extract the unigrams used as features, we use DLATK, which handles social media content and markup such as emoticons or hashtags (Schwartz et al., 2017). Tweets were filtered for English using langid.py (Lui and Baldwin, 2012) and retweets were excluded. 3.2 Annotation We create a binary annotation task for identifying if a tweet contains a complaint or not. Tweets are short and usually express a single thought. Therefore, we consider the entire tweet as a complaint if it contains at least one complaint speech act. For annotation, we adopt as the guideline a complaint definition similar to that from previous linguistic research (Olshtain and Weinbach, 1987; Cohen and Olshtain, 1993): “A complaint presents a state of affairs which breaches the"
P19-1495,D13-1170,0,0.00543479,"neutral sentiment as well as the six Ekman emotions for each message using the model made available in (Volkova and Bachrach, 2016) and use them as features in predicting complaints. The sentiment model is trained on a data set of 19,555 tweets that combine all previously annotated tweets across seven public data sets. • VADER: We use the outcome of the rule-based sentiment analysis model which has shown very good predictive performance on predicting sentiment in tweets (Gilbert and Hutto, 2014). • Stanford: We quantify sentiment using the Stanford sentiment prediction model as described in (Socher et al., 2013). Complaint Specific Features. The features in this category are inspired by linguistic aspects specific to complaints (Meinl, 2013): • Request. The illocutionary function of complaints is often that of requesting for a correction or reparation for the event that caused the breach of expectations (Olshtain and Weinbach, 1987). We explicitly predict if an utterance is a request using the model introduced in (Danescu-NiculescuMizil et al., 2013). • Intensifiers. In order to increase the facethreatening effect a complaint has on the complainee, intensifiers are usually used by the person expressi"
P19-1495,C16-1257,0,0.0586149,"Missing"
P19-1495,P16-1148,0,0.0906359,"2003). 5011 crowd-sourcing from (Mohammad and Turney, 2010, 2013) for assigning to each tweet the proportion of tokens that have positive, negative and neutral sentiment, as well as one of eight emotions that include the six basic emotions of Ekman (Ekman, 1992) (anger, disgust, fear, joy, sadness and surprise) plus trust and anticipation. All scores are used as features in prediction in order to maximize their predictive power. • Volkova & Bachrach (V&B): We quantify positive, negative and neutral sentiment as well as the six Ekman emotions for each message using the model made available in (Volkova and Bachrach, 2016) and use them as features in predicting complaints. The sentiment model is trained on a data set of 19,555 tweets that combine all previously annotated tweets across seven public data sets. • VADER: We use the outcome of the rule-based sentiment analysis model which has shown very good predictive performance on predicting sentiment in tweets (Gilbert and Hutto, 2014). • Stanford: We quantify sentiment using the Stanford sentiment prediction model as described in (Socher et al., 2013). Complaint Specific Features. The features in this category are inspired by linguistic aspects specific to comp"
P19-1495,N19-2008,0,0.0869676,"ccur with other speech acts including positive and negative remarks, frequently make explicit references to expectations not being met and directly demand a reparation or compensation. Meinl (2013) studied complaints in eBay reviews by annotating 200 reviews in English and German with the speech act sequence that makes up each complaint e.g., warning, annoyance (the annotations are not available publicly or after contacting the authors). Mikolov et al. (2018) analyze which financial complaints submitted to the Consumer Financial Protection Bureau will receive a timely response. Most recently, Yang et al. (2019) studied customer support dialogues and predicted if these complaints will be escalated with a government agency or made public on social media. To the best of our knowledge, the only previous work that tackles a concept defined as a complaint with computational methods is by Zhou and Ganesan (2016) which studies Yelp reviews. However, they define a complaint as a ‘sentence with negative connotation with supplemental information’. This definition is not aligned with previous research in linguistics (as presented above) and represents only a minor variation on sentiment analysis. They introduce"
P19-1495,P17-1039,0,0.0217627,"08) and politeness maxims (e.g., i must say). Finally, we directly predict the politeness score of the tweet using the model presented in (DanescuNiculescu-Mizil et al., 2013). • Temporal References. Temporal references are often used in complaints to stress how long a complainer has been waiting for a correction or reparation from the addressee or to provide context for their complaint (e.g., mentioning the date in which they have bought an item) (Meinl, 2013). We identify time expressions in tweets using SynTime, which achieved state-of-the-art results across on several benchmark data sets (Zhong et al., 2017). We represent temporal expressions both as days elapsed relative to the day of the post and in buckets of different granularities (one day, week, month, year). • Pronoun Types. Pronouns are used in complaints to reveal the personal involvement or opinion of the complainer and intensify or reduce the face-threat of the complaint based on the person or type of the pronoun (Claridge, 2007; Meinl, 2013). We split pronouns using dictionaries into: first person, second person, third person, demonstrative (e.g., this) and indefinite (e.g., everybody). 5 Linguistic Feature Analysis This section prese"
P19-1495,W16-0418,0,0.0220938,"h act sequence that makes up each complaint e.g., warning, annoyance (the annotations are not available publicly or after contacting the authors). Mikolov et al. (2018) analyze which financial complaints submitted to the Consumer Financial Protection Bureau will receive a timely response. Most recently, Yang et al. (2019) studied customer support dialogues and predicted if these complaints will be escalated with a government agency or made public on social media. To the best of our knowledge, the only previous work that tackles a concept defined as a complaint with computational methods is by Zhou and Ganesan (2016) which studies Yelp reviews. However, they define a complaint as a ‘sentence with negative connotation with supplemental information’. This definition is not aligned with previous research in linguistics (as presented above) and represents only a minor variation on sentiment analysis. They introduce a data set of complaints, unavailable at the time of this submission, and only perform a qualitative analysis, without building predictive models for identifying complaints. 3 Data To date, there is no available data set with annotated complaints as previously defined in linguistics (Olshtain and W"
P19-1495,S18-1005,0,0.0596852,"Missing"
P19-1495,P19-1272,1,0.831375,"Missing"
P19-1495,S17-2088,0,\N,Missing
S13-1018,E09-1005,1,0.849802,"se2 . • Subject and description: cosine similarity of TF.IDF vectors of respective fields. IDF values were calculated using a subset of Europeana items (the Culture Grid collection), available internally. These preliminary scores were im2 urlhttp://wordnetcode.princeton.edu/standofffiles/morphosemantic-links.xls 132 Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conference c and the Shared Task, pages 132–137, Atlanta, Georgia, June 13-14, 2013. 2013 Association for Computational Linguistics proved using TF.IDF based on Wikipedia, UKB (Agirre and Soroa, 2009) and a more informed time similarity measure. We describe each of these processes in turn. 2.1 TF.IDF A common approach to computing document similarity is to represent documents as Bag-Of-Words (BOW). Each BOW is a vector consisting of the words contained in the document, where each dimension corresponds to a word, and the weight is the frequency in the corresponding document. The similarity between two documents can be computed as the cosine of the angle between their vectors. This is the approached use above. This approach can be improved giving more weight to words which occur in only a fe"
S13-1018,agirre-etal-2010-exploring,1,0.860547,"idf2w qP 2 2 w∈a (tfw,a × idfw ) × w∈b (tfw,b × idfw ) P qP w∈a,b where tfw,x is the frequency of the term w in x ∈ {a, b} and idfw is the inverted document frequency of the word w measured in Wikipedia. We substituted the preliminary general similarity score by the obtained using the TF.IDF presented in this section. 2.2 UKB The semantic disambiguation UKB3 algorithm (Agirre and Soroa, 2009) applies personalized PageRank on a graph generated from the English WordNet (Fellbaum, 1998), or alternatively, from Wikipedia. This algorithm has proven to be very competitive in word similarity tasks (Agirre et al., 2010). To compute similarity using UKB we represent WordNet as a graph G = (V, E) as follows: graph nodes represent WordNet concepts (synsets) and 3 http://ixa2.si.ehu.es/ukb/ 133 dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges. Our method is provided with a pair of vectors of words and a graph-based representation of WordNet. We first compute the personalized PageRank over WordNet separately for each of the vector of words, producing a probability distribution over WordNet synsets. We"
S13-1018,P05-1045,0,0.00503141,"y, using the training data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and descri"
S13-1018,N03-1033,0,0.0199478,"data provided by the organisers with the previously defined similarity measures as features. We begin by describing our basic system in Section 2, followed by the machine learning system in 1 http://www.europeana.eu/ Basic system The items in this task are taken from Europeana. They cannot be redistributed, so we used the urls and scripts provided by the organizers to extract the corresponding metadata. We analysed the text in the metadata, performing lemmatization, PoS tagging, named entity recognition and classification (NERC) and date detection using Stanford CoreNLP (Finkel et al., 2005; Toutanova et al., 2003). A preliminary score for each similarity type was then calculated as follows: • General: cosine similarity of TF.IDF vectors of tokens, taken from all fields. • Author: cosine similarity of TF.IDF vectors of dc:Creator field. • People involved, time period and location: cosine similarity of TF.IDF vectors of location/date/people entities recognized by NERC in all fields. • Events: cosine similarity of TF.IDF vectors of event verbs and nouns. A list of verbs and nouns possibly denoting events was derived using the WordNet Morphosemantic Database2 . • Subject and description: cosine similarity"
S13-1018,L10-1000,0,\N,Missing
S13-1018,W12-1012,1,\N,Missing
S15-1003,E09-1005,0,0.0330812,"mber) will not be. The graph-based algorithm increases the weight of context features car.n.01 automobile ~ automobile auto ~ auto car ~ car machine ~ machine motorcar ~ motorcar + ~ car.n.01 Figure 1: In the Synset Distributional Model the vector representing a synset (white box) is computed as the centroid of its lemma vectors (grey boxes) that synsets share with neighbours and reduces those that are not shared. PageRank (Page et al., 1999) is a graph-based algorithm for identifying important nodes in a graph that has been applied to a range of NLP tasks including word sense disambiguation (Agirre and Soroa, 2009) and keyword extraction (Mihalcea and Tarau, 2004). Let G = (V, E) be a graph with a set of vertices, V , denoting synsets and a set of edges, E, denoting links between synsets in the WordNet hierarchy. The PageRank score (P r) over G for a synset (Vi ) can be computed by the following equation: P r(Vi ) = d · X Vj ∈I(Vi ) 1 P r(Vj ) + (1 − d)v O(Vj ) (1) where I(Vi ) denotes the in-degree of the vertex Vi and O(Vj ) is the out-degree of vertex Vj . d is the damping factor which is set to the default value of d = 0.85 (Page et al., 1999). In standard PageRank all elements of the vector v are t"
S15-1003,N09-1003,0,0.833161,"equation 1 to prefer certain nodes. The values in v effectively initialises the graph and assigning high values to nodes in v makes them more likely to be assigned a high PPR score. For each context feature c in C if c ∈ LM where LM contains all the lemma names of synsets in S, we apply PPR to assign importance to synsets. The score of each synset Sc in the personalisation vector 22 v, is set to |S1c |where |Sc |is the number of synsets that context feature i belongs. The personalisation value of all the other sysnets is set to 0. We apply PPR over WordNet for each context feature using UKB (Agirre et al., 2009) and obtain weights for each synset-context feature pair resulting to a new semantic space Hp , S × C, where vector elements are weighted by PageRank values. Figure 2 shows how the synset scores are computed by applying PPR over WordNet given the context feature car. Note that we use the context features of the distributional model D. 2.3 Latent Semantic Analysis Latent Semantic Analysis (LSA) (Deerwester et al., 1990; Landauer and Dumais, 1997) has been used to reduce the dimensionality of semantic spaces leading to improved performance. LSA applies Singular Value Decomposition (SVD) to a mat"
S15-1003,S12-1051,0,0.0273137,"the value that ˜ and maximises performance which is k = 700 for H ˜ p . For the joint spaces learned using k = 650 for H CCA, we also tune the number of the top l correlated features in RG. We set l ∈ {10, 20, ..., 650} and select the value that maximises performance which 24 is l = 250 for H ∗ and l = 40 for Hp∗ . 3.4 Evaluation Metric Performance is measured as the correlation between the similarity scores returned by each proposed method and the human judgements. This is the standard approach to evaluate word and text similarity tasks, e.g. (Budanitsky and Hirst, 2001; Agirre et al., 2009; Agirre et al., 2012). Our experiments use Spearman’s correlation coefficient. 3.5 Results Table 1 shows the Spearman’s correlation of similarity scores generated by each model and human judgements of similarity across various data sets by taking the maximum pairwise similarity score of two words’ synsets. The first row of the table shows the results obtained by the word distributional model of Baroni et al. (2014). The full hybrid models H and Hp perform consistently worse than the original distributional model D across data sets. The main reason is that a large number of synsets contain only one lemma name which"
S15-1003,P14-1023,0,0.328415,"ic space to represent words as vectors (Section 2.1). Then, we make use of the WordNet’s clusters of synonyms and hierarchy in combination with the standard distributional space to build hybrid models (Section 2.2) which are augmented using Latent Semantic Analysis (Section 2.3) and Canonical Correlation Analysis (Section 2.4). 2.1 Distributional Model We consider a semantic space D, as a word by context feature matrix, L × C. Vector representations consist of context features C in a reference corpus. We made use of pre-computed publicly available vectors1 optimised for word similarity tasks (Baroni et al., 2014). Word co-occurrence counts are extracted using a symmetric window of two words over a corpus of 2.8 billion tokens obtained by concatenating 1 http://clic.cimec.unitn.it/composes/ semantic-vectors.html 21 ukWaC, the English Wikipedia and the British National Corpus. Vectors are weighted using positive Pointwise Mutual Information and the set of context features consists of the top 300K most frequent words in the corpus. 2.2 Hybrid Models 2.2.1 Synset Distributional Model We assume that making use of information about the structure of WordNet can reduce noise introduced in vectors of D due to"
S15-1003,W11-2503,0,0.025799,"9; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines di"
S15-1003,P12-1015,0,0.0266854,"0.86 0.80 0.56 Table 1: Spearman’s correlation on various data sets. Maximum similarity between pairs of synsets. edness. First, we make use of WS-353 (Finkelstein et al., 2001) which contains 353 pairs of words annotated by humans. Furthermore, we make use of the similarity (WS-Sim) and relatedness (WS-Rel) pairs of words created by Agirre et al. (2009) from the original WS-353 data set. We also made use of the RG (Rubenstein and Goodenough, 1965) and MC (Miller and Charles, 1991) data sets which contain 65 and 30 pairs of nouns respectively. Finally, we make use of the larger MEN data set (Bruni et al., 2012) which contains 3,000 pairs of words that has been used as image tags. Annotations are obtained using croudsourcing. 3.3 Model Parameters The parameters we need to tune are the number of ˜ and H ˜ p , and the top components in LSA spaces, H ∗ ∗ CCA spaces, H and Hp . For the LSA spaces, we tune the number of the top k components in RG. We set k ∈ {50, 100, ..., 1000} and select the value that ˜ and maximises performance which is k = 700 for H ˜ p . For the joint spaces learned using k = 650 for H CCA, we also tune the number of the top l correlated features in RG. We set l ∈ {10, 20, ..., 650}"
S15-1003,D09-1003,0,0.0250988,"s such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (B"
S15-1003,D10-1113,0,0.0215044,"vious work tackled the problem through vector adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distribution"
S15-1003,D08-1094,0,0.0776876,"Missing"
S15-1003,P10-2017,0,0.0171648,"problem through vector adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representa"
S15-1003,E14-1049,0,0.0557514,"mensional variables x and y, CCA finds two projection vectors by maximising the correlations of the variables onto these projections. The function to be maximised is: ρ= p E[x2 ]E[y 2 ] 3 3.1 Joint Representation using CCA E[xy] The dimensionality of the projection vectors is lower or equal to the dimensionality of the original variables. The computation of CCA directly over H and Hp is computationally infeasible because of their high dimensionality (300K). We apply CCA over the re˜ and H ˜ p to obduced spaces learned using LSA, H tain two joint semantic spaces following a similar approach to Faruqui and Dyer (2014). These are the spaces H ∗ , resulting from the projection of the ˜ and Hp∗ , resulting Synset Distributional Model H, ˜ p. from the projection of the Synset Rank Model H (4) 23 Word Similarity Computing Similarity Since hybrid models represent words as synset vectors, similarity between two words can be computed following two ways. First, we compute similarity between two words as the maximum of their pairwise synset similarity. On the other hand, similarity can be computed as the average pairwise synset similarity using the synsets that the two words belong. Similarity is computed as the cos"
S15-1003,N15-1184,0,0.0378744,"on to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines distributional semantic models using relational information from various semantic lexicons, including WordNet, by making linked words in these lexicons to have similar vector representations. While our models are also based on using information from WordNet for refining vector representations, they are fundamentally different. They create synset vectors in an unsupervised fashion and more importantly can be used for sense tagging. 6 Conclusions This paper proposed hybrid models of lexical semantics that combine distributional and knowledge-based approaches and offer adva"
S15-1003,P14-1046,0,0.0205097,"to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines distributional semantic models using relational information from various semantic lexicons, including WordNet, by making linked words in these lexicons to have similar vector representations. While our models are also based on using information from WordNet for refining vector representatio"
S15-1003,J98-1001,0,0.396835,"Missing"
S15-1003,P14-1132,0,0.0604464,"Missing"
S15-1003,P08-1028,0,0.0509565,"eral features from them. This is not a problem for word similarity since there is no need to model the senses found in the lexicon. 5 Related Work Dealing with polysemy in distributional semantics is a fundamental issue since the various senses of a word type are conflated in a single vector. Previous work tackled the problem through vector adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applie"
S15-1003,S01-1005,0,0.109327,"Missing"
S15-1003,E14-1025,0,0.0258188,"tence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a"
S15-1003,N10-1013,0,0.0317742,"or adaptation, clustering and language models (Erk, 2012). Vector adaptation methods modify a traditional (i.e. polysemous) target word vector by applying pointwise operations such as addition or multiplication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporati"
S15-1003,P13-1056,0,0.0195276,"oon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 2011; Silberer et al., 2013; Lazaridou et al., 2014). Following a similar direction, Faruqui and 27 Dyer (2014) found that learning joint spaces from multilingual vector spaces using CCA improves the performance of standard monolingual vector spaces on semantic similarity. Fyshe et al. (2014) showed that integrating textual vector space models with brain activation data when people are reading words achieves better correlation to behavioural data than models of one modality. Our hybrid models are also closely related to a supervised method proposed by Faruqui et al. (2015). Their method refines distributional semantic m"
S15-1003,W04-0811,0,0.0637368,"Missing"
S15-1003,I11-1127,0,0.0376576,"Missing"
S15-1003,D11-1094,0,0.0481879,"Missing"
S15-1003,W10-2807,0,0.0206293,"plication to that and the surrounding words in a sentence (Mitchell and Lapata, 2008; Erk and Pad´o, 2008; Thater et al., 2011; Van de Cruys et al., 2011). Alternatively, clustering methods have been used to cluster together the different contexts a target word appears assuming that each cluster of contexts captures a different sense of the target word (Dinu and Lapata, 2010; Erk and Pado, 2010; Reisinger and Mooney, 2010). Language models have also been used to remove polysemy from word vectors by predicting words that could replace the target word given a context (Deschacht and Moens, 2009; Washtell, 2010; Moon and Erk, 2013). More recently, Polajnar and Clark (2014) applied context selection and normalisation to improve the quality of word vectors. Our hybrid models are related to the vector adaptation methods since we modify the synset vectors using its lemmas’ vectors to remove noise. Our work is also inspired by recent work on improving classic distributional vector representations of words by incorporating information from different modalities. For example, researchers have developed methods that make use of both visual and contextual information to improve word vectors (Bruni et al., 201"
S15-1003,W04-3252,0,\N,Missing
W12-1012,N09-1003,0,0.0900202,"Missing"
W12-1012,N10-1125,0,0.0994089,"ld help to identify them automatically. However, previous work on computing similarity in the CH domain has been limited and, in particular, has not made use of information from multiple types of media. For example, Grieser et al. (2011) computed similarity between exhibits in Melbourne Museum by applying a range of text similarity measures but did not make use of other media. Techniques for exploiting information from multimedia collections have been developed and are commonly applied to a wide range of problems such as Content-based Image Retrieval (Datta et al., 2008) and image annotation (Feng and Lapata, 2010). Introduction and Motivation In recent years a vast amount of Cultural Heritage (CH) artefacts have been digitised and made available on-line. For example, the Louvre and the British Museum provide information about exhibits on their web pages1 . In addition, information is also available via sites that aggregate CH information from multiple resources. A typical example is Europeana2 , a web-portal to collections from several European institutions that provides access to over 20 million items including paintings, films, books, archives and museum exhibits. However, online information about CH"
W12-1012,P06-2071,0,0.0617018,"Missing"
W13-0102,D07-1109,0,0.0121182,"l Information (PMI). Topic coherence is determined by measuring the distance between these vectors computed using a variety of metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task. 1 Introduction Topic modelling is a popular statistical method for (soft) clustering documents (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 1999). Latent Dirichlet Allocation (LDA) (Blei et al., 2003), one type of topic model, has been widely used in NLP and applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009) and generation of comparable corpora (Preiss, 2012). A variety of approaches has been proposed to evaluate the topics generated by these models. The first to be explored were extrinsic methods, measuring the performance achieved by a model in a specific task or using statistical methods. For example, topic models have been evaluated by measuring their accuracy for information retrieval (Wei and Croft, 2006). Statistical methods have also been applied to measure the predictive likelihood of a topic model in held-out documents by co"
W13-0102,N09-1041,0,0.0412623,"suring the distance between these vectors computed using a variety of metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task. 1 Introduction Topic modelling is a popular statistical method for (soft) clustering documents (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 1999). Latent Dirichlet Allocation (LDA) (Blei et al., 2003), one type of topic model, has been widely used in NLP and applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009) and generation of comparable corpora (Preiss, 2012). A variety of approaches has been proposed to evaluate the topics generated by these models. The first to be explored were extrinsic methods, measuring the performance achieved by a model in a specific task or using statistical methods. For example, topic models have been evaluated by measuring their accuracy for information retrieval (Wei and Croft, 2006). Statistical methods have also been applied to measure the predictive likelihood of a topic model in held-out documents by computing their perplexity. Wallach et al. (2009) gives a detaile"
W13-0102,islam-inkpen-2006-second,0,0.00952619,"wj )) (7) Finally, we introduce γ which is a parameter to assign more emphasis on context features with high PMI (or NPMI) values with a topic word. Vectors are weighted using P M I(wi , fj )γ or N P M I(wi , fj )γ where wi is a topic word and fj is a context feature. For all of our experiments we set γ = 2 which was found to produce the best results. 3.2.2 Reducing the Basis Including all co-occurring terms in the vectors leads to a high dimensional space. We also experimented with two approaches to reducing the number of terms to form a semantic space with smaller basis. Firstly, following Islam and Inkpen (2006), a Reduced Semantic Space is created by choosing the βwi most related context features for each topic word wi : 2 (log2 (m)) βwi = log(c(wi )) (8) δ where δ is a parameter for adjusting the number of features for each word and m is the size of the corpus. Varying the value of δ did not effect performance for values above 1. This parameter was set of 3 for the results reported here. In addition a frequency cut-off of 20 was also applied. In addition, a smaller semantic space was created by considering only topic words as context features, leading to n features for each topic word. This is ref"
W13-0102,P11-1154,0,0.230023,"Missing"
W13-0102,D11-1024,0,0.835817,"on of document collections (Chaney and Blei, 2012; Newman et al., 2010a), where automatically generated topics are used to provide an overview of the collection and the top-n words in each topic used to represent it. Chang et al. (2009) showed that humans find topics generated by models with high predictive likelihood to be less coherent than topics generated from others with lower predictive likelihood. Following Chang’s findings, recent work on evaluation of topic models has been focused on automatically measuring the coherence of generated topics by comparing them against human judgements (Mimno et al., 2011; Newman et al., 2010b). Newman et al. (2010b) define topic coherence as the average semantic relatedness between topic words and report the best correlation with humans using the Pointwise Mutual Information (PMI) between topic words in Wikipedia. 1: oil, louisiana, coast, gulf, orleans, spill, state, fisherman, fishing, seafood 2: north, kim, korea, korean, jong, south, il, official, party, son 3: model, wheel, engine, system, drive, front, vehicle, rear, speed, power 4: drink, alcohol, indonesia, drinking, indonesian, four, nokia, beverage, mcdonald, caffeine 5: privacy, andrews, elli, alex"
W13-0102,N10-1012,0,0.316027,"rics. However, these approaches do not provide any information about how interpretable the topics are to humans. Figure 1 shows some example topics generated by a topic model. The first three topics appear quite coherent, all the terms in each topic are associated with a common theme. On the other hand, it is difficult to identify a coherent theme connecting all of the words in topics 4 and 5. These topics are difficult to interpret and could be considered as “junk” topics. Interpretable topics are important in applications such as visualisation of document collections (Chaney and Blei, 2012; Newman et al., 2010a), where automatically generated topics are used to provide an overview of the collection and the top-n words in each topic used to represent it. Chang et al. (2009) showed that humans find topics generated by models with high predictive likelihood to be less coherent than topics generated from others with lower predictive likelihood. Following Chang’s findings, recent work on evaluation of topic models has been focused on automatically measuring the coherence of generated topics by comparing them against human judgements (Mimno et al., 2011; Newman et al., 2010b). Newman et al. (2010b) defin"
W13-0102,P03-1017,0,0.0616069,"Missing"
W13-0102,N12-1065,0,0.0214637,"metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task. 1 Introduction Topic modelling is a popular statistical method for (soft) clustering documents (Blei et al., 2003; Deerwester et al., 1990; Hofmann, 1999). Latent Dirichlet Allocation (LDA) (Blei et al., 2003), one type of topic model, has been widely used in NLP and applied to a range of tasks including word sense disambiguation (Boyd-Graber et al., 2007), multi-document summarisation (Haghighi and Vanderwende, 2009) and generation of comparable corpora (Preiss, 2012). A variety of approaches has been proposed to evaluate the topics generated by these models. The first to be explored were extrinsic methods, measuring the performance achieved by a model in a specific task or using statistical methods. For example, topic models have been evaluated by measuring their accuracy for information retrieval (Wei and Croft, 2006). Statistical methods have also been applied to measure the predictive likelihood of a topic model in held-out documents by computing their perplexity. Wallach et al. (2009) gives a detailed description of such statistical metrics. However,"
W19-0404,W13-0102,1,0.86446,"Missing"
W19-0404,N13-1016,1,0.867714,"Missing"
W19-0404,P14-2103,1,0.792236,"ble words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probable words. Words displayed in b"
W19-0404,N16-1057,0,0.0184346,"raditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible."
W19-0404,P11-1154,0,0.0285062,"pic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probabl"
W19-0404,E14-1056,0,0.0236024,"ial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank t"
W19-0404,D11-1024,0,0.0604857,"atent variables called topics. Topics are multinomial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus."
W19-0404,N10-1012,0,0.198147,"nts as a mixture of latent variables called topics. Topics are multinomial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in t"
W19-0404,P02-1040,0,0.10711,"ons are statistically significant for all three datasets (Pearson’s r varies between 0.81 and 0.90, p < 0.05). This suggests that the automated evaluation approach presented in Section 5 is a useful tool for assessing the effectiveness of methods for word re-ranking with the advantage that results can be obtained more rapidly than methods that require human judgments. However, human judgments are recommended when performance is similar and automated evaluation should not be relied upon to make fine-grained distinctions between approaches, as is common for some tasks (e.g. Machine Translation (Papineni et al., 2002)). 7 Conclusion We presented a study on word re-ranking methods designed to improve topic interpretability. Four methods were presented and assessed through two experiments. In the first experiment, participants on a crowdsourcing platform were asked to associate documents with related topics. In the second experiment, automated evaluation was based on a document retrieval task. Re-ranking the topic words was found to improve the interpretability of topics and therefore should be used as a post-processing step to improve topic representation. The most effective re-ranking schemes were those wh"
W19-0404,E17-2069,0,0.0136829,"rious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probable words. Words displayed in bold font are more general (less informative, e.g. with high document frequency) while the remaining words a"
W19-0404,W14-3110,0,0.180598,"pirical evaluation of the effectiveness of their approach. Other word re-ranking methods have also combined information about the overall probability of a word and its relative probability in one topic compared to others. Chuang et al. (2012) describe a word reranking method applied within a topic model visualisation system. Their approach combines information about the word’s overall probability within the corpus and its distinctiveness for a particular topic which is computed as the Kullback-Leibler divergence between the distribution of topics given the word and the distribution of topics. Sievert and Shirley (2014) also combine both types of information within a topic visualisation system. Bischof and Airoldi (2012) developed an approach for hierarchical topic models which balances information about the word frequency in a topic and the exclusivity of that word to that topic relative to a set of similar topics within the hierarchy. Others have proposed approaches that only take into account the relative probability of each word in a topic compared to the others. Song et al. (2009) introduced a word ranking method based on normalising the probability of a word in a topic with the sum of the probabilities"
W19-0404,Q17-1001,0,0.0214953,"combine information about the importance of words both within topics and their relative frequency in the entire corpus. In addition, close correlation between the results of the two evaluation approaches suggests that the automatic method proposed here could be used to evaluate re-ranking methods without the need for human judgements. 1 Introduction Probabilistic topic modelling (Blei, 2012) is a widely used approach in Natural Language Processing (Boyd-Graber et al., 2017) with applications to areas such as enhancing exploratory search interfaces (Chaney and Blei, 2012; Aletras et al., 2014; Smith et al., 2017; Aletras et al., 2017) and developing interpretable machine learning models (Paul, 2016). A topic model, e.g. Latent Dirichlet Allocation (LDA) (Blei et al., 2003) learns a low-dimensional representation of documents as a mixture of latent variables called topics. Topics are multinomial distributions over a predefined vocabulary of words. Traditionally, topics have been represented by lists of the topic’s n most probable words, however it is not always straightforward to interpret them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words"
W19-0404,E17-2111,1,0.783238,"et them due to noisy or domain specific data, spurious word co-occurrences and highly-frequent/low-informative words assigned with high probability (Chang et al., 2009). Improving the interpretability of topic models is an important area of research. A range of approaches have been developed including computing topic coherence (Newman et al., 2010; Mimno et al., 2011; Aletras and Stevenson, 2013a; Lau et al., 2014), determining optimal topic cardinality (Lau and Baldwin, 2016), labelling topics text and/or images (Lau et al., 2011; Aletras and Stevenson, 2014, 2013b; Aletras and Mittal, 2017; Sorodoc et al., 2017) and corpus pre-processing (Schofield et al., 2017). However, methods for re-ranking topic words to improve topic interpretability have not been systematically evaluated yet. We hypothesise that some words relevant to a particular topic have not been assigned with a high probability due to data sparseness or low frequency in the training corpus. Our goal is to identify these words and re-rank the list representing the topic to make it more comprehensible. Table 1 shows topics represented by the 30 most probable words. Words displayed in bold font are more general (less informative, e.g. with h"
W19-2209,D14-1181,0,0.00590848,"enbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results on several benchmark datasets, most notably: RCV 1 (Lewis et al., 2004), containing news articles; EUR - LEX (Mencia and Frnkranz, 2007), containing legal documents; Amazon-12K (McAuley and Leskovec, 2013), containing product descriptions; and Wiki-30K (Zubiaga, 2012), containing Wikipedia articles. Their proposed method outperformed both tree-based methods (e.g., FASTXML, (Prabhu and Varma, 2014)) and target-embedding methods (e.g., SLEEC (Bhatia et al., 2015), FASTTEXT (Bojanowski et al., 2016)). RNNs with self-attention have been employed in a wide variety o"
W19-2209,P19-1285,0,0.0161146,"TT (left) and BIGRU - LWAN (right). Gold labels (concepts) are shown at the top of each sub-figure, while the top 5 predicted labels are shown at the bottom. Correct predictions are shown in bold. BIGRU - LWAN’s label-wise attentions are depicted in different colors. ing heat-maps include only one color. 6 consider the structure (sections) of the documents. The best methods of this work rely on GRUs and thus are computationally expensive. The length of the documents further affects the training time of these methods. Hence, we plan to investigate the use of Transformers (Vaswani et al., 2017; Dai et al., 2019) and dilated CNNs (Kalchbrenner et al., 2017) as alternative document encoders. Conclusions and Future Work We compared various neural methods on a new legal XMTC dataset, EURLEX 57 K, also investigating few-shot and zero-shot learning. We showed that BIGRU - ATT is a strong baseline for this XMTC dataset, outperforming CNN - LWAN (Mullenbach et al., 2018), which was especially designed for XMTC , but that replacing the vanilla CNN of CNN LWAN by a BIGRU encoder ( BIGRU - LWAN ) leads to the best overall results, except for zero-shot labels. For the latter, the zero-shot version of CNN LWAN of"
W19-2209,N19-1423,0,0.0552829,"Missing"
W19-2209,P18-1128,0,0.0395065,"Missing"
W19-2209,N18-1100,0,0.319881,"onomics and Business, Greece ** Computer Science Department, University of Sheffield, UK [ihalk,fergadiotis,rulller,ion]@aueb.gr, n.aletras@sheffield.ac.uk Abstract Publications Office of the European Union. Although EUROVOC contains more than 7,000 concepts, most of them are rarely used in practice. Consequently, they are under-represented in EU RLEX 57 K , making the dataset also appropriate for few-shot and zero-shot learning. Experimenting on EURLEX 57 K, we explore the use of various RNN-based and CNN-based neural classifiers, including the state of the art LabelWise Attention Network of Mullenbach et al. (2018), called CNN - LWAN here. We show that both a simpler BIGRU with self-attention (Xu et al., 2015) and the Hierarchical Attention Network (HAN) of Yang et al. (2016) outperform CNN LWAN by a wide margin. Replacing the CNN encoder of CNN - LWAN with a BIGRU, which leads to a method we call BIGRU - LWAN, further improves performance. Similar findings are observed in the zero-shot setting where Z - BIGRU - LWAN outperforms Z - CNN - LWAN. We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR - LEX,"
W19-2209,P18-1031,0,0.0567814,"Missing"
W19-2209,D08-1046,0,0.645838,"ification (XMTC), is the task of tagging documents with relevant labels from an extremely large label set, typically containing thousands of labels (classes). Applications include building web directories (Partalas et al., 2015), labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), product categorization (McAuley and Leskovec, 2013), categorizing medical examinations (Mullenbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results on several benchmark datasets, most notably: RCV 1 (Lewis et al., 2004), containing news articles; EUR - LEX (Mencia and Frnkranz, 2007), containing legal documents; Amazo"
W19-2209,N19-1357,0,0.0614205,"Missing"
W19-2209,D14-1162,0,0.0824988,"Missing"
W19-2209,N18-1202,0,0.107142,"Missing"
W19-2209,P16-2034,0,0.0763691,"Missing"
W19-2209,N18-1189,0,0.108532,"labelwise attention. Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance. 1 Introduction Extreme multi-label text classification (XMTC), is the task of tagging documents with relevant labels from an extremely large label set, typically containing thousands of labels (classes). Applications include building web directories (Partalas et al., 2015), labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), product categorization (McAuley and Leskovec, 2013), categorizing medical examinations (Mullenbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results o"
W19-2209,D18-1352,0,0.314619,"labelwise attention. Replacing CNNs with BIGRUs in label-wise attention networks leads to the best overall performance. 1 Introduction Extreme multi-label text classification (XMTC), is the task of tagging documents with relevant labels from an extremely large label set, typically containing thousands of labels (classes). Applications include building web directories (Partalas et al., 2015), labeling scientific publications with concepts from ontologies (Tsatsaronis et al., 2015), product categorization (McAuley and Leskovec, 2013), categorizing medical examinations (Mullenbach et al., 2018; Rios and Kavuluru, 2018b), and indexing legal documents (Mencia and Frnkranz, 2007). We focus on legal text processing, an emerging NLP field with many applications (Nallapati and Manning, 2008; Aletras et al., 2016; Chalkidis et al., 2017), but limited publicly available resources. We release a new dataset, named EURLEX 57 K, including 57,000 English documents of EU legislation from the EUR - LEX portal. All documents have been tagged with concepts from the European Vocabulary (EUROVOC), maintained by the 2 Related Work Liu et al. (2017) proposed a CNN similar to that of Kim (2014) for XMTC. They reported results o"
W19-2209,N16-1174,0,0.809742,"cations Office of the European Union. Although EUROVOC contains more than 7,000 concepts, most of them are rarely used in practice. Consequently, they are under-represented in EU RLEX 57 K , making the dataset also appropriate for few-shot and zero-shot learning. Experimenting on EURLEX 57 K, we explore the use of various RNN-based and CNN-based neural classifiers, including the state of the art LabelWise Attention Network of Mullenbach et al. (2018), called CNN - LWAN here. We show that both a simpler BIGRU with self-attention (Xu et al., 2015) and the Hierarchical Attention Network (HAN) of Yang et al. (2016) outperform CNN LWAN by a wide margin. Replacing the CNN encoder of CNN - LWAN with a BIGRU, which leads to a method we call BIGRU - LWAN, further improves performance. Similar findings are observed in the zero-shot setting where Z - BIGRU - LWAN outperforms Z - CNN - LWAN. We consider the task of Extreme Multi-Label Text Classification (XMTC) in the legal domain. We release a new dataset of 57k legislative documents from EUR - LEX, the European Union’s public document database, annotated with concepts from EUROVOC, a multidisciplinary thesaurus. The dataset is substantially larger than previo"
