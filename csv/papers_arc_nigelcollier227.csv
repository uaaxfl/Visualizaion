2021.wnut-1.34,Integrating Transformers and Knowledge Graphs for {T}witter Stance Detection,2021,-1,-1,6,0,214,thomas clark,Proceedings of the Seventh Workshop on Noisy User-generated Text (W-NUT 2021),0,"Stance detection (SD) entails classifying the sentiment of a text towards a given target, and is a relevant sub-task for opinion mining and social media analysis. Recent works have explored knowledge infusion supplementing the linguistic competence and latent knowledge of large pre-trained language models with structured knowledge graphs (KGs), yet few works have applied such methods to the SD task. In this work, we first perform stance-relevant knowledge probing on Transformers-based pre-trained models in a zero-shot setting, showing these models{'} latent real-world knowledge about SD targets and their sensitivity to context. We then train and evaluate new knowledge-enriched stance detection models on two Twitter stance datasets, achieving state-of-the-art performance on both."
2021.wassa-1.19,Synthetic Examples Improve Cross-Target Generalization: A Study on Stance Detection on a {T}witter corpus.,2021,-1,-1,6,1,215,costanza conforti,"Proceedings of the Eleventh Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"Cross-target generalization is a known problem in stance detection (SD), where systems tend to perform poorly when exposed to targets unseen during training. Given that data annotation is expensive and time-consuming, finding ways to leverage abundant unlabeled in-domain data can offer great benefits. In this paper, we apply a weakly supervised framework to enhance cross-target generalization through synthetically annotated data. We focus on Twitter SD and show experimentally that integrating synthetic data is helpful for cross-target generalization, leading to significant improvements in performance, with gains in F1 scores ranging from +3.4 to +5.1."
2021.repl4nlp-1.5,Learning Sparse Sentence Encoding without Supervision: An Exploration of Sparsity in Variational Autoencoders,2021,-1,-1,4,1,2457,victor prokhorov,Proceedings of the 6th Workshop on Representation Learning for NLP (RepL4NLP-2021),0,"It has been long known that sparsity is an effective inductive bias for learning efficient representation of data in vectors with fixed dimensionality, and it has been explored in many areas of representation learning. Of particular interest to this work is the investigation of the sparsity within the VAE framework which has been explored a lot in the image domain, but has been lacking even a basic level of exploration in NLP. Additionally, NLP is also lagging behind in terms of learning sparse representations of large units of text e.g., sentences. We use the VAEs that induce sparse latent representations of large units of text to address the aforementioned shortcomings. First, we move in this direction by measuring the success of unsupervised state-of-the-art (SOTA) and other strong VAE-based sparsification baselines for text and propose a hierarchical sparse VAE model to address the stability issue of SOTA. Then, we look at the implications of sparsity on text classification across 3 datasets, and highlight a link between performance of sparse latent representations on downstream tasks and its ability to encode task-related information."
2021.naacl-main.334,Self-Alignment Pretraining for Biomedical Entity Representations,2021,-1,-1,5,1,216,fangyu liu,Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Despite the widespread success of self-supervised learning via masked language models (MLM), accurately capturing fine-grained semantic relationships in the biomedical domain remains a challenge. This is of paramount importance for entity-level tasks such as entity linking where the ability to model entity relations (especially synonymy) is pivotal. To address this challenge, we propose SapBERT, a pretraining scheme that self-aligns the representation space of biomedical entities. We design a scalable metric learning framework that can leverage UMLS, a massive collection of biomedical ontologies with 4M+ concepts. In contrast with previous pipeline-based hybrid systems, SapBERT offers an elegant one-model-for-all solution to the problem of medical entity linking (MEL), achieving a new state-of-the-art (SOTA) on six MEL benchmarking datasets. In the scientific domain, we achieve SOTA even without task-specific supervision. With substantial improvement over various domain-specific pretrained MLMs such as BioBERT, SciBERTand and PubMedBERT, our pretraining scheme proves to be both effective and robust."
2021.hackashop-1.1,Adversarial Training for News Stance Detection: Leveraging Signals from a Multi-Genre Corpus.,2021,-1,-1,7,1,215,costanza conforti,Proceedings of the EACL Hackashop on News Media Content Analysis and Automated Report Generation,0,"Cross-target generalization constitutes an important issue for news Stance Detection (SD). In this short paper, we investigate adversarial cross-genre SD, where knowledge from annotated user-generated data is leveraged to improve news SD on targets unseen during training. We implement a BERT-based adversarial network and show experimental performance improvements over a set of strong baselines. Given the abundance of user-generated data, which are considerably less expensive to retrieve and annotate than news articles, this constitutes a promising research direction."
2021.findings-emnlp.76,Plan-then-Generate: Controlled Data-to-Text Generation via Planning,2021,-1,-1,5,0,6588,yixuan su,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Recent developments in neural networks have led to the advance in data-to-text generation. However, the lack of ability of neural models to control the structure of generated output can be limiting in certain real-world applications. In this study, we propose a novel Plan-then-Generate (PlanGen) framework to improve the controllability of neural data-to-text models. Extensive experiments and analyses are conducted on two benchmark datasets, ToTTo and WebNLG. The results show that our model is able to control both the intra-sentence and inter-sentence structure of the generated output. Furthermore, empirical comparisons against previous state-of-the-art methods show that our model improves the generation quality as well as the output diversity as judged by human and automatic evaluations."
2021.findings-emnlp.77,Few-Shot Table-to-Text Generation with Prototype Memory,2021,-1,-1,4,0,6588,yixuan su,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"Neural table-to-text generation models have achieved remarkable progress on an array of tasks. However, due to the data-hungry nature of neural models, their performances strongly rely on large-scale training examples, limiting their applicability in real-world applications. To address this, we propose a new framework: Prototype-to-Generate (P2G), for table-to-text generation under the few-shot scenario. The proposed framework utilizes the retrieved prototypes, which are jointly selected by an IR system and a novel prototype selector to help the model bridging the structural gap between tables and texts. Experimental results on three benchmark datasets with three state-of-the-art models demonstrate that the proposed framework significantly improves the model performance across various evaluation metrics."
2021.findings-acl.50,"Keep the Primary, Rewrite the Secondary: A Two-Stage Approach for Paraphrase Generation",2021,-1,-1,5,0,6588,yixuan su,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.emnlp-main.109,"Fast, Effective, and Self-Supervised: Transforming Masked Language Models into Universal Lexical and Sentence Encoders",2021,-1,-1,4,1,216,fangyu liu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Previous work has indicated that pretrained Masked Language Models (MLMs) are not effective as universal lexical and sentence encoders off-the-shelf, i.e., without further task-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks using annotated task data. In this work, we demonstrate that it is possible to turn MLMs into effective lexical and sentence encoders even without any additional data, relying simply on self-supervision. We propose an extremely simple, fast, and effective contrastive learning technique, termed Mirror-BERT, which converts MLMs (e.g., BERT and RoBERTa) into such encoders in 20-30 seconds with no access to additional external knowledge. Mirror-BERT relies on identical and slightly modified string pairs as positive (i.e., synonymous) fine-tuning examples, and aims to maximise their similarity during {``}identity fine-tuning{''}. We report huge gains over off-the-shelf MLMs with Mirror-BERT both in lexical-level and in sentence-level tasks, across different domains and different languages. Notably, in sentence similarity (STS) and question-answer entailment (QNLI) tasks, our self-supervised Mirror-BERT model even matches the performance of the Sentence-BERT models from prior work which rely on annotated task data. Finally, we delve deeper into the inner workings of MLMs, and suggest some evidence on why this simple Mirror-BERT fine-tuning approach can yield effective universal lexical and sentence encoders."
2021.emnlp-main.383,Mixture-of-Partitions: Infusing Large Biomedical Knowledge Graphs into {BERT},2021,-1,-1,5,0,217,zaiqiao meng,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"Infusing factual knowledge into pre-trained models is fundamental for many knowledge-intensive tasks. In this paper, we proposed Mixture-of-Partitions (MoP), an infusion approach that can handle a very large knowledge graph (KG) by partitioning it into smaller sub-graphs and infusing their specific knowledge into various BERT models using lightweight adapters. To leverage the overall factual knowledge for a target task, these sub-graph adapters are further fine-tuned along with the underlying BERT through a mixture layer. We evaluate our MoP with three biomedical BERTs (SciBERT, BioBERT, PubmedBERT) on six downstream tasks (inc. NLI, QA, Classification), and the results show that our MoP consistently enhances the underlying BERTs in task performance, and achieves new SOTA performances on five evaluated datasets."
2021.emnlp-main.818,Visually Grounded Reasoning across Languages and Cultures,2021,-1,-1,5,1,216,fangyu liu,Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing,0,"The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for Multicultural Reasoning over Vision and Language (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems."
2021.eacl-main.18,Non-Autoregressive Text Generation with Pre-trained Language Models,2021,-1,-1,7,0,6588,yixuan su,Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume,0,"Non-autoregressive generation (NAG) has recently attracted great attention due to its fast inference speed. However, the generation quality of existing NAG models still lags behind their autoregressive counterparts. In this work, we show that BERT can be employed as the backbone of a NAG model for a greatly improved performance. Additionally, we devise two mechanisms to alleviate the two common problems of vanilla NAG models: the inflexibility of prefixed output length and the conditional independence of individual token predictions. To further strengthen the speed advantage of the proposed model, we propose a new decoding strategy, ratio-first, for applications where the output lengths can be approximately estimated beforehand. For a comprehensive evaluation, we test the proposed model on three text generation tasks, including text summarization, sentence compression and machine translation. Experimental results show that our model significantly outperforms existing non-autoregressive baselines and achieves competitive performance with many strong autoregressive models. In addition, we also conduct extensive analysis experiments to reveal the effect of each proposed component."
2021.conll-1.44,{M}irror{W}i{C}: On Eliciting Word-in-Context Representations from Pretrained Language Models,2021,-1,-1,3,0,9802,qianchu liu,Proceedings of the 25th Conference on Computational Natural Language Learning,0,"Recent work indicated that pretrained language models (PLMs) such as BERT and RoBERTa can be transformed into effective sentence and word encoders even via simple self-supervised techniques. Inspired by this line of work, in this paper we propose a fully unsupervised approach to improving word-in-context (WiC) representations in PLMs, achieved via a simple and efficient WiC-targeted fine-tuning procedure: MirrorWiC. The proposed method leverages only raw texts sampled from Wikipedia, assuming no sense-annotated data, and learns context-aware word representations within a standard contrastive learning setup. We experiment with a series of standard and comprehensive WiC benchmarks across multiple languages. Our proposed fully unsupervised MirrorWiC models obtain substantial gains over off-the-shelf PLMs across all monolingual, multilingual and cross-lingual setups. Moreover, on some standard WiC benchmarks, MirrorWiC is even on-par with supervised models fine-tuned with in-task data and sense labels."
2021.acl-short.72,Learning Domain-Specialised Representations for Cross-Lingual Biomedical Entity Linking,2021,-1,-1,4,1,216,fangyu liu,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Injecting external domain-specific knowledge (e.g., UMLS) into pretrained language models (LMs) advances their capability to handle specialised in-domain tasks such as biomedical entity linking (BEL). However, such abundant expert knowledge is available only for a handful of languages (e.g., English). In this work, by proposing a novel cross-lingual biomedical entity linking task (XL-BEL) and establishing a new XL-BEL benchmark spanning 10 typologically diverse languages, we first investigate the ability of standard knowledge-agnostic as well as knowledge-enhanced monolingual and multilingual LMs beyond the standard monolingual English BEL task. The scores indicate large gaps to English performance. We then address the challenge of transferring domain-specific knowledge in resource-rich languages to resource-poor ones. To this end, we propose and evaluate a series of cross-lingual transfer methods for the XL-BEL task, and demonstrate that general-domain bitext helps propagate the available English knowledge to languages with little to no in-domain data. Remarkably, we show that our proposed domain-specific transfer methods yield consistent gains across all target languages, sometimes up to 20 Precision@1 points, without any in-domain knowledge in the target language, and without any in-domain parallel data."
2021.acl-long.137,Dialogue Response Selection with Hierarchical Curriculum Learning,2021,-1,-1,8,0,6588,yixuan su,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"We study the learning of a matching model for dialogue response selection. Motivated by the recent finding that models trained with random negative samples are not ideal in real-world scenarios, we propose a hierarchical curriculum learning framework that trains the matching model in an {``}easy-to-difficult{''} scheme. Our learning framework consists of two complementary curricula: (1) corpus-level curriculum (CC); and (2) instance-level curriculum (IC). In CC, the model gradually increases its ability in finding the matching clues between the dialogue context and a response candidate. As for IC, it progressively strengthens the model{'}s ability in identifying the mismatching information between the dialogue context and a response candidate. Empirical studies on three benchmark datasets with three state-of-the-art matching models demonstrate that the proposed learning framework significantly improves the model performance across various evaluation metrics."
2020.findings-emnlp.365,{STANDER}: An Expert-Annotated Dataset for News Stance Detection and Evidence Retrieval,2020,-1,-1,6,1,215,costanza conforti,Findings of the Association for Computational Linguistics: EMNLP 2020,0,"We present a new challenging news dataset that targets both stance detection (SD) and fine-grained evidence retrieval (ER). With its 3,291 expert-annotated articles, the dataset constitutes a high-quality benchmark for future research in SD and multi-task learning. We provide a detailed description of the corpus collection methodology and carry out an extensive analysis on the sources of disagreement between annotators, observing a correlation between their disagreement and the diffusion of uncertainty around a target in the real world. Our experiments show that the dataset poses a strong challenge to recent state-of-the-art models. Notably, our dataset aligns with an existing Twitter SD dataset: their union thus addresses a key shortcoming of previous works, by providing the first dedicated resource to study multi-genre SD as well as the interplay of signals from social media and news sources in rumour verification."
2020.emnlp-main.253,{COMETA}: A Corpus for Medical Entity Linking in the Social Media,2020,-1,-1,4,1,4254,marco basaldella,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"Whilst there has been growing progress in Entity Linking (EL) for general language, existing datasets fail to address the complex nature of health terminology in layman{'}s language. Meanwhile, there is a growing need for applications that can understand the public{'}s voice in the health domain. To address this we introduce a new corpus called COMETA, consisting of 20k English biomedical entity mentions from Reddit expert-annotated with links to SNOMED CT, a widely-used medical knowledge graph. Our corpus satisfies a combination of desirable properties, from scale and coverage to diversity and quality, that to the best of our knowledge has not been met by any of the existing resources in the field. Through benchmark experiments on 20 EL baselines from string- to neural-based models we shed light on the ability of these systems to perform complex inference on entities and concepts under 2 challenging evaluation scenarios. Our experimental results on COMETA illustrate that no golden bullet exists and even the best mainstream techniques still have a significant performance gap to fill, while the best solution relies on combining different views of data."
2020.acl-main.157,Will-They-Won{'}t-They: A Very Large Dataset for Stance Detection on {T}witter,2020,34,0,6,1,215,costanza conforti,Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics,1,"We present a new challenging stance detection dataset, called Will-They-Won{'}t-They (WT--WT), which contains 51,284 tweets in English, making it by far the largest available dataset of the type. All the annotations are carried out by experts; therefore, the dataset constitutes a high-quality and reliable benchmark for future research in stance detection. Our experiments with a wide range of recent state-of-the-art stance detection systems show that the dataset poses a strong challenge to existing models in this domain."
N19-1196,Generating Knowledge Graph Paths from Textual Definitions using Sequence-to-Sequence Models,2019,0,0,3,1,2457,victor prokhorov,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"We present a novel method for mapping unrestricted text to knowledge graph entities by framing the task as a sequence-to-sequence problem. Specifically, given the encoded state of an input text, our decoder directly predicts paths in the knowledge graph, starting from the root and ending at the the target node following hypernym-hyponym relationships. In this way, and in contrast to other text-to-entity mapping systems, our model outputs hierarchically structured predictions that are fully interpretable in the context of the underlying ontology, in an end-to-end manner. We present a proof-of-concept experiment with encouraging results, comparable to those of state-of-the-art systems."
N19-1298,A Richer-but-Smarter Shortest Dependency Path with Attentive Augmentation for Relation Extraction,2019,0,1,4,0.882353,12196,duycat can,"Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",0,"To extract the relationship between two entities in a sentence, two common approaches are (1) using their shortest dependency path (SDP) and (2) using an attention model to capture a context-based representation of the sentence. Each approach suffers from its own disadvantage of either missing or redundant information. In this work, we propose a novel model that combines the advantages of these two approaches. This is based on the basic information in the SDP enhanced with information selected by several attention mechanisms with kernel filters, namely RbSP (Richer-but-Smarter SDP). To exploit the representation behind the RbSP structure effectively, we develop a combined deep neural model with a LSTM network on word sequences and a CNN on RbSP. Experimental results on the SemEval-2010 dataset demonstrate improved performance over competitive baselines. The data and source code are available at https://github.com/catcd/RbSP."
D19-6205,{B}io{R}eddit: Word Embeddings for User-Generated Biomedical {NLP},2019,0,1,2,1,4254,marco basaldella,Proceedings of the Tenth International Workshop on Health Text Mining and Information Analysis (LOUHI 2019),0,"Word embeddings, in their different shapes and iterations, have changed the natural language processing research landscape in the last years. The biomedical text processing field is no stranger to this revolution; however, scholars in the field largely trained their embeddings on scientific documents only, even when working on user-generated data. In this paper we show how training embeddings from a corpus collected from user-generated text from medical forums heavily influences the performance on downstream tasks, outperforming embeddings trained both on general purpose data or on scientific papers when applied on user-generated content."
D19-5612,On the Importance of the {K}ullback-{L}eibler Divergence Term in Variational Autoencoders for Text Generation,2019,0,2,5,1,2457,victor prokhorov,Proceedings of the 3rd Workshop on Neural Generation and Translation,0,"Variational Autoencoders (VAEs) are known to suffer from learning uninformative latent representation of the input due to issues such as approximated posterior collapse, or entanglement of the latent space. We impose an explicit constraint on the Kullback-Leibler (KL) divergence term inside the VAE objective function. While the explicit constraint naturally avoids posterior collapse, we use it to further understand the significance of the KL term in controlling the information transmitted through the VAE channel. Within this framework, we explore different properties of the estimated posterior distribution, and highlight the trade-off between the amount of information encoded in a latent code during training, and the generative capacity of the model."
W18-5507,Towards Automatic Fake News Detection: Cross-Level Stance Detection in News Articles,2018,-1,-1,3,1,215,costanza conforti,Proceedings of the First Workshop on Fact Extraction and {VER}ification ({FEVER}),0,"In this paper, we propose to adapt the four-staged pipeline proposed by Zubiaga et al. (2018) for the Rumor Verification task to the problem of Fake News Detection. We show that the recently released FNC-1 corpus covers two of its steps, namely the \textit{Tracking} and the \textit{Stance Detection} task. We identify asymmetry in length in the input to be a key characteristic of the latter step, when adapted to the framework of Fake News Detection, and propose to handle it as a specific type of \textit{Cross-Level Stance Detection}. Inspired by theories from the field of Journalism Studies, we implement and test two architectures to successfully model the internal structure of an article and its interactions with a claim."
P18-1119,Which {M}elbourne? Augmenting Geocoding with Maps,2018,0,8,3,0.740741,838,milan gritta,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"The purpose of text geolocation is to associate geographic information contained in a document with a set (or sets) of coordinates, either implicitly by using linguistic features and/or explicitly by using geographic metadata combined with heuristics. We introduce a geocoder (location mention disambiguator) that achieves state-of-the-art (SOTA) results on three diverse datasets by exploiting the implicit lexical clues. Moreover, we propose a new method for systematic encoding of geographic metadata to generate two distinct views of the same text. To that end, we introduce the Map Vector (MapVec), a sparse representation obtained by plotting prior geographic probabilities, derived from population figures, on a World Map. We then integrate the implicit (language) and explicit (map) features to significantly improve a range of metrics. We also introduce an open-source dataset for geoparsing of news events covering global disease outbreaks and epidemics to help future evaluation in geoparsing."
D18-1169,Card-660: {C}ambridge Rare Word Dataset - a Reliable Benchmark for Infrequent Word Representation Models,2018,30,3,4,0.95234,448,mohammad pilehvar,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Rare word representation has recently enjoyed a surge of interest, owing to the crucial role that effective handling of infrequent words can play in accurate semantic understanding. However, there is a paucity of reliable benchmarks for evaluation and comparison of these techniques. We show in this paper that the only existing benchmark (the Stanford Rare Word dataset) suffers from low-confidence annotations and limited vocabulary; hence, it does not constitute a solid comparison framework. In order to fill this evaluation gap, we propose Cambridge Rare word Dataset (Card-660), an expert-annotated word similarity dataset which provides a highly reliable, yet challenging, benchmark for rare word representation techniques. Through a set of experiments we show that even the best mainstream word embeddings, with millions of words in their vocabularies, are unable to achieve performances higher than 0.43 (Pearson correlation) on the dataset, compared to a human-level upperbound of 0.90. We release the dataset and the annotation materials at \url{https://pilehvar.github.io/card-660/}."
D18-1221,Mapping Text to Knowledge Graph Entities using Multi-Sense {LSTM}s,2018,0,5,3,0,20638,dimitri kartsaklis,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"This paper addresses the problem of mapping natural language text to knowledge base entities. The mapping process is approached as a composition of a phrase or a sentence into a point in a multi-dimensional entity space obtained from a knowledge graph. The compositional model is an LSTM equipped with a dynamic disambiguation mechanism on the input word embeddings (a Multi-Sense LSTM), addressing polysemy issues. Further, the knowledge base space is prepared by collecting random walks from a graph enhanced with textual features, which act as a set of semantic bridges between text and knowledge base entities. The ideas of this work are demonstrated on large-scale text-to-entity mapping and entity classification tasks, with state of the art results."
D18-1250,Large-scale Exploration of Neural Relation Classification Architectures,2018,0,4,6,1,12207,hoangquynh le,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"Experimental performance on the task of relation classification has generally improved using deep neural network architectures. One major drawback of reported studies is that individual models have been evaluated on a very narrow range of datasets, raising questions about the adaptability of the architectures, while making comparisons between approaches difficult. In this work, we present a systematic large-scale analysis of neural relation classification architectures on six benchmark datasets with widely varying characteristics. We propose a novel multi-channel LSTM model combined with a CNN that takes advantage of all currently popular linguistic and architectural features. Our {`}Man for All Seasons{'} approach achieves state-of-the-art performance on two datasets. More importantly, in our view, the model allowed us to obtain direct insights into the continued challenges faced by neural language models on this task."
S17-2002,{S}em{E}val-2017 Task 2: Multilingual and Cross-lingual Semantic Word Similarity,2017,50,47,3,0,5213,jose camachocollados,Proceedings of the 11th International Workshop on Semantic Evaluation ({S}em{E}val-2017),0,"This paper introduces a new task on Multilingual and Cross-lingual SemanticThis paper introduces a new task on Multilingual and Cross-lingual Semantic Word Similarity which measures the semantic similarity of word pairs within and across five languages: English, Farsi, German, Italian and Spanish. High quality datasets were manually curated for the five languages with high inter-annotator agreements (consistently in the 0.9 ballpark). These were used for semi-automatic construction of ten cross-lingual datasets. 17 teams participated in the task, submitting 24 systems in subtask 1 and 14 systems in subtask 2. Results show that systems that combine statistical knowledge from text corpora, in the form of word embeddings, and external knowledge from lexical resources are best performers in both subtasks. More information can be found on the task website: \url{http://alt.qcri.org/semeval2017/task2/}"
P17-1115,{V}ancouver Welcomes You! Minimalist Location Metonymy Resolution,2017,25,3,4,0.740741,838,milan gritta,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Named entities are frequently used in a metonymic manner. They serve as references to related entities such as people and organisations. Accurate identification and interpretation of metonymy can be directly beneficial to various NLP applications, such as Named Entity Recognition and Geographical Parsing. Until now, metonymy resolution (MR) methods mainly relied on parsers, taggers, dictionaries, external word lists and other handcrafted lexical resources. We show how a minimalist neural approach combined with a novel predicate window method can achieve competitive results on the SemEval 2007 task on Metonymy Resolution. Additionally, we contribute with a new Wikipedia-based MR dataset called RelocaR, which is tailored towards locations as well as improving previous deficiencies in annotation guidelines."
P17-1170,Towards a Seamless Integration of Word Senses into Downstream {NLP} Applications,2017,60,24,4,1,448,mohammad pilehvar,Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Lexical ambiguity can impede NLP systems from accurate understanding of semantics. Despite its potential benefits, the integration of sense-level information into NLP systems has remained understudied. By incorporating a novel disambiguation algorithm into a state-of-the-art classification model, we create a pipeline to integrate sense-level information into downstream NLP applications. We show that a simple disambiguation of the input text can lead to consistent performance improvement on multiple topic categorization and polarity detection datasets, particularly when the fine granularity of the underlying sense inventory is reduced and the document is sufficiently large. Our results also point to the need for sense representation research to focus more on in vivo evaluations which target the performance in downstream NLP applications rather than artificial benchmarks."
E17-2062,Inducing Embeddings for Rare and Unseen Words by Leveraging Lexical Resources,2017,20,7,2,1,448,mohammad pilehvar,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"We put forward an approach that exploits the knowledge encoded in lexical resources in order to induce representations for words that were not encountered frequently during training. Our approach provides an advantage over the past work in that it enables vocabulary expansion not only for morphological variations, but also for infrequent domain specific terms. We performed evaluations in different settings, showing that the technique can provide consistent improvements on multiple benchmarks across domains."
W16-6111,{NLP} and Online Health Reports: What do we say and what do we mean?,2016,0,0,1,1,219,nigel collier,Proceedings of the Seventh International Workshop on Health Text Mining and Information Analysis,0,None
W16-5102,Learning Orthographic Features in Bi-directional {LSTM} for Biomedical Named Entity Recognition,2016,32,7,2,1,3095,nut limsopatham,Proceedings of the Fifth Workshop on Building and Evaluating Resources for Biomedical Text Mining ({B}io{T}xt{M}2016),0,"End-to-end neural network models for named entity recognition (NER) have shown to achieve effective performances on general domain datasets (e.g. newswire), without requiring additional hand-crafted features. However, in biomedical domain, recent studies have shown that hand-engineered features (e.g. orthographic features) should be used to attain effective performance, due to the complexity of biomedical terminology (e.g. the use of acronyms and complex gene names). In this work, we propose a novel approach that allows a neural network model based on a long short-term memory (LSTM) to automatically learn orthographic features and incorporate them into a model for biomedical NER. Importantly, our bi-directional LSTM model learns and leverages orthographic features on an end-to-end basis. We evaluate our approach by comparing against existing neural network models for NER using three well-established biomedical datasets. Our experimental results show that the proposed approach consistently outperforms these strong baselines across all of the three datasets."
W16-3920,Bidirectional {LSTM} for Named Entity Recognition in {T}witter Messages,2016,19,30,2,1,3095,nut limsopatham,Proceedings of the 2nd Workshop on Noisy User-generated Text ({WNUT}),0,"In this paper, we present our approach for named entity recognition in Twitter messages that we used in our participation in the Named Entity Recognition in Twitter shared task at the COLING 2016 Workshop on Noisy User-generated text (WNUT). The main challenge that we aim to tackle in our participation is the short, noisy and colloquial nature of tweets, which makes named entity recognition in Twitter message a challenging task. In particular, we investigate an approach for dealing with this problem by enabling bidirectional long short-term memory (LSTM) to automatically learn orthographic features without requiring feature engineering. In comparison with other systems participating in the shared task, our system achieved the most effective performance on both the {`}segmentation and categorisation{'} and the {`}segmentation only{'} sub-tasks."
W16-2902,Improved Semantic Representation for Domain-Specific Entities,2016,21,4,2,1,448,mohammad pilehvar,Proceedings of the 15th Workshop on Biomedical Natural Language Processing,0,The authors gratefully acknowledge the supportn of the MRC grant No. MR/M025160/1 forn PheneBank.
W16-2918,Modelling the Combination of Generic and Target Domain Embeddings in a Convolutional Neural Network for Sentence Classification,2016,22,7,2,1,3095,nut limsopatham,Proceedings of the 15th Workshop on Biomedical Natural Language Processing,0,This is the final version of the article. It first appeared from the Association for Computational Linguistics via http://www.aclweb.org/anthology/W/W16/
P16-1096,Normalising Medical Concepts in Social Media Texts by Learning Semantic Representation,2016,34,30,2,1,3095,nut limsopatham,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Automatically recognising medical concepts mentioned in social media messages (e.g. tweets) enables several applications for enhancing health quality of people in a community, e.g. real-time monitoring of infectious diseases in population. However, the discrepancy between the type of language used in social media and medical ontologies poses a major challenge. Existing studies deal with this challenge by employing techniques, such as lexical term matching and statistical machine translation. In this work, we handle the medical concept normalisation at the semantic level. We investigate the use of neural networks to learn the transition between laymanxe2x80x99s language used in social media messages and formal medical language used in the descriptions of medical concepts in a standard ontology. We evaluate our approaches using three different datasets, where social media texts are extracted from Twitter messages and blog posts. Our experimental results show that our proposed approaches significantly and consistently outperform existing effective baselines, which achieved state-of-the-art performance on several medical concept normalisation tasks, by up to 44%."
D16-1174,De-Conflated Semantic Representations,2016,33,3,2,1,448,mohammad pilehvar,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"One major deficiency of most semantic representation techniques is that they usually model a word type as a single point in the semantic space, hence conflating all the meanings that the word can have. Addressing this issue by learning distinct representations for individual meanings of words has been the subject of several research studies in the past few years. However, the generated sense representations are either not linked to any sense inventory or are unreliable for infrequent word senses. We propose a technique that tackles these problems by de-conflating the representations of words based on the deep knowledge it derives from a semantic network. Our approach provides multiple advantages in comparison to the past work, including its high coverage and the ability to generate accurate representations even for infrequent word senses. We carry out evaluations on six datasets across two semantic similarity tasks and report state-of-the-art results on most of them."
D15-1194,Adapting Phrase-based Machine Translation to Normalise Medical Terms in Social Media Messages,2015,18,13,2,1,3095,nut limsopatham,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Previous studies have shown that health reports in social media, such as DailyStrength and Twitter, have potential for monitoring health conditions (e.g. adverse drug reactions, infectious diseases) in particular communities. However, in order for a machine to understand and make inferences on these health conditions, the ability to recognise when laymenxe2x80x99s terms refer to a particular medical concept (i.e. text normalisation) is required. To achieve this, we propose to adapt an existing phrase-based machine translation (MT) technique and a vector representation of words to map between a social media phrase and a medical concept. We evaluate our proposed approach using a collection of phrases from tweets related to adverse drug reactions. Our experimental results show that the combination of a phrase-based MT technique and the similarity between word vector representations outperforms the baselines that apply only either of them by up to 55%."
W14-1103,The impact of near domain transfer on biomedical named entity recognition,2014,-1,-1,1,1,219,nigel collier,Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi),0,None
E14-1059,Discriminating Rhetorical Analogies in Social Media,2014,26,2,3,0,26906,christoph lofi,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Analogies are considered to be one of the core concepts of human cognition and communication, and are very efficient at encoding complex information in a natural fashion. However, computational approaches towards largescale analysis of the semantics of analogies are hampered by the lack of suitable corpora with real-life example of analogies. In this paper we therefore propose a workflow for discriminating and extracting natural-language analogy statements from the Web, focusing on analogies between locations mined from travel reports, blogs, and the Social Web. For realizing this goal, we employ feature-rich supervised learning models which we extensively evaluate. We also showcase a crowd-supported workflow for building a suitable Gold dataset used for this purpose. The resulting system is able to successfully learn to identify analogies to a high degree of accuracy (F-Score 0.9) by using a high-dimensional subsequence feature space."
W13-2019,Exploring a Probabilistic {E}arley Parser for Event Composition in Biomedical Texts,2013,11,0,2,0,12203,maivu tran,Proceedings of the {B}io{NLP} Shared Task 2013 Workshop,0,"We describe a high precision system for extracting events of biomedical significance that was developed during the BioNLP shared task 2013 and tested on the Cancer Genetics data set. The system achieved an F-score on the development data of 73.67 but was ranked 5 th out of six with an F-score of 29.94 on the test data. However, precision was the second highest ranked on the task at 62.73. Analysis suggests the need to continue to improve our system for complex events particularly taking into account cross-domain differences in argument distributions."
W12-5503,An Experiment in Integrating Sentiment Features for Tech Stock Prediction in {T}witter,2012,-1,-1,4,0,42080,tien vu,Proceedings of the Workshop on Information Extraction and Entity Analytics on Social Media Data,0,None
C12-1040,A Hybrid Approach to Finding Phenotype Candidates in Genetic Texts,2012,39,8,1,1,219,nigel collier,Proceedings of {COLING} 2012,0,"Named entity recognition (NER) has been extensively studied for the names of genes and gene products but there are few proposed solutions for phenotypes. Phenotype terms are expected to play a key role in inferring gene function in complex heritable diseases but are intrinsically difficult to analyse due to their complex semantics and scale. In contrast to previous approaches we evaluate state-of-the-art techniques involving the fusion of machine learning on a rich feature set with evidence from extant domain knowledge-sources. The techniques are validated on two gold standard collections including a novel annotated collection of 112 abstracts derived from a systematic search of the Online Mendelian Inheritance of Man database for auto-immune diseases. Encouragingly the hybrid model outperforms a HMM, a CRF and a pure knowledge-based method to achieve an F1 of 77.07. Disagreement analysis points to further improvements on this emerging NE task. The annotated corpus and guidelines are available on request."
C12-1093,On-line Trend Analysis with Topic Models: {\\#}twitter Trends Detection Topic Model Online,2012,19,101,2,0,3097,jey lau,Proceedings of {COLING} 2012,0,"We present a novel topic modelling-based methodology to track emerging events in microblogs such as Twitter. Our topic model has an in-built update mechanism based on time slices and implements a dynamic vocabulary. We first show that the method is robust in detecting events using a range of datasets with injected novel events, and then demonstrate its application in identifying trending topics in Twitter."
C10-1025,An ontology-driven system for detecting global health events,2010,19,20,1,1,219,nigel collier,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,Text mining for global health surveillance is an emerging technology that is gaining increased attention from public health organisations and governments. The lack of multilingual resources such as WordNets specifically targeted at this task have so far been a major bottleneck. This paper reports on a major upgrade to the BioCaster Web monitoring system and its freely available multilingual ontology; improving its original design and extending its coverage of diseases from 70 to 336 in 12 languages.
W09-1318,Using Hedges to Enhance a Disease Outbreak Report Text Mining System,2009,5,8,3,0,31801,mike conway,Proceedings of the {B}io{NLP} 2009 Workshop,0,"Identifying serious infectious disease outbreaks in their early stages is an important task, both for national governments and international organizations like the World Health Organization. Text mining and information extraction systems can provide an important, low cost and timely early warning system in these circumstances by identifying the first signs of an outbreak automatically from online textual news. One interesting characteristic of disease outbreak reports --- which to the best of our knowledge has not been studied before --- is their use of speculative language (hedging) to describe uncertain situations. This paper describes two uses of hedging to enhance the BioCaster disease outbreak report text mining system."
I08-2140,Global Health Monitor - A Web-based System for Detecting and Mapping Infectious Diseases,2008,4,27,4,1,46437,son doan,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{II},0,"We present the Global Health Monitor, an online Web-based system for detecting and mapping infectious disease outbreaks that appear in news stories. The system analyzes English news stories from news feed providers, classifies them for topical relevance and plots them onto a Google map using geo-coding information, helping public health workers to monitor the spread of diseases in a geo-temporal context. The background knowledge for the system is contained in the BioCaster ontology (BCO) (Collier et al., 2007a) which includes both information on infectious diseases as well as geographical locations with their latitudes/longitudes. The system consists of four main stages: topic classification, named entity recognition (NER), disease/location detection and visualization. Evaluation of the system shows that it achieved high accuracy on a gold standard corpus. The system is now in practical use. Running on a clustercomputer, it monitors more than 1500 news feeds 24/7, updating the map every hour."
C08-1057,The Choice of Features for Classification of Verbs in Biomedical Texts,2008,20,17,3,0.405761,7440,anna korhonen,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"We conduct large-scale experiments to investigate optimal features for classification of verbs in biomedical texts. We introduce a range of feature sets and associated extraction techniques, and evaluate them thoroughly using a robust method new to the task: cost-based framework for pairwise clustering. Our best results compare favourably with earlier ones. Interestingly, they are obtained with sophisticated feature sets which include lexical and semantic information about selectional preferences of verbs. The latter are acquired automatically from corpus data using a fully unsupervised method."
W07-1003,The Role of Roles in Classifying Annotated Biomedical Text,2007,15,8,3,1,46437,son doan,"Biological, translational, and clinical language processing",0,"This paper investigates the roles of named entities (NE's) in annotated biomedical text classification. In the annotation schema of BioCaster, a text mining system for public health protection, important concepts that reflect information about infectious diseases were conceptually analyzed with a formal ontological methodology. Concepts were classified as Types, while others were identified as being Roles. Types are specified as NE classes and Roles are integrated into NEs as attributes. We focus on the Roles of NEs by extracting and using them in different ways as features in the classifier. Experimental results show that: 1) Roles for each NE greatly helped improve performance of the system, 2) combining information about NE classes with their Roles contribute significantly to the improvement of performance. We discuss in detail the effect of each Role on the accuracy of text classification."
P06-1044,Automatic Classification of Verbs in Biomedical Texts,2006,19,20,3,0.405761,7440,anna korhonen,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Lexical classes, when tailored to the application and domain in question, can provide an effective means to deal with a number of natural language processing (NLP) tasks. While manual construction of such classes is difficult, recent research shows that it is possible to automatically induce verb classes from cross-domain corpora with promising accuracy. We report a novel experiment where similar technology is applied to the important, challenging domain of biomedicine. We show that the resulting classification, acquired from a corpus of biomedical journal articles, is highly accurate and strongly domain-specific. It can be used to aid BIO-NLP directly or as useful material for investigating the syntax and semantics of verbs in biomedical texts."
W04-3253,Sentiment Analysis using Support Vector Machines with Diverse Information Sources,2004,16,481,2,0,51341,tony mullen,Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing,0,None
W04-1205,Zone Identification in Biology Articles as a Basis for Information Extraction,2004,12,35,2,0,51303,yoko mizuta,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,"Information extraction (IE) in the biomedical domain is now regarded as an essential technique for the dynamic management of factual information contained in archived journal articles and abstract collections. We aim to provide a technique serving as a basis for pinpointing and organizing factual information related to experimental results. In this paper, we enhance the idea proposed in (Mizuta and Collier, 2004); annotating articles in terms of rhetorical zones with shallow nesting. We give a qualitative analysis of the zone identification (ZI) process in biology articles. Specifically, we illustrate the linguistic and other features of each zone based on our investigation of articles selected from four major online journals. We also discuss controversial cases and nested zones, and ZI using multiple features. In doing so, we provide a stronger theoretical and practical support for our framework toward automatic ZI."
W04-1213,Introduction to the Bio-entity Recognition Task at {JNLPBA},2004,11,334,1,1,219,nigel collier,Proceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applications ({NLPBA}/{B}io{NLP}),0,We describe here the JNLPBA shared task of bio-entity recognition using an extended version of the GENIA version 3 named entity corpus of MEDLINE abstracts. We provide background information on the task and present a general discussion of the approaches taken by participating systems.
P04-3025,Incorporating topic information into semantic analysis models,2004,14,11,2,0,51341,tony mullen,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,This paper reports experiments in classifying texts based upon their favorability towards the subject of the text using a feature set enriched with topic information on a small dataset of music reviews hand-annotated for topic. The results of these experiments suggest ways in which incorporating topic information into such models may yield improvement over models which do not use topic information.
kawazoe-etal-2004-annotation,Annotation of Coreference Relations Among Linguistic Expressions and Images in Biological Articles,2004,8,0,3,1,39826,ai kawazoe,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper, we propose an annotation scheme which can be used not only for annotating coreference relations between linguistic expressions, but also those among linguistic expressions and images, in scientific texts such as biomedical articles. Images in biomedical domain often contain important information for analyses and diagnoses, and we consider that linking images to textual descriptions of their semantic contents in terms of coreference relations is useful for multimodal access to the information. We present our annotation scheme and the concept of a ``coreference pool,'' which plays a central role in the scheme. We also introduce a support tool for text annotation named Open Ontology Forge which we have already developed, and additional functions for the software to cover image annotations (ImageOF) which is now being developed."
mizuta-collier-2004-annotation,An Annotation Scheme for a Rhetorical Analysis of Biology Articles,2004,15,18,2,0,51303,yoko mizuta,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In information extraction from scientific texts, it is crucially important to identify the unique contribution of the research. The task is complicated by the large number of statements made in each article that pertain to results, including reference to previous work and technical details. Simple keyword searches are helpful for a content-based analysis but fail to tell new results from other ones. We aim to approach the problem from a rhetorical perspective and give a xe2x80x98zone analysisxe2x80x99 (ZA) of texts in light of Teufel, Carletta & Moens (1999). We analyze a text into xe2x80x98zonesxe2x80x99 with a shallow nesting based on the rhetorical status which each sequence of statements fit into and annotate the text correspondingly. Our current focus is on the molecular biology domain. In this paper, we propose an annotation scheme for ZA based on an empirical analysis of major online journals (EMBO, NAR, PNAS, and JCB), and illustrate how it works. Our scheme provides a way to differentiate the text in terms of the aspects of the authorxe2x80x99s own work (e.g. experimental procedure, findings, implications) and to identify a set of statements relating data and findings and therefore helps identify the authorxe2x80x99s new results and findings."
W03-1308,Bio-Medical Entity Extraction using Support Vector Machines,2003,28,10,2,0.666667,17433,koichi takeuchi,Proceedings of the {ACL} 2003 Workshop on Natural Language Processing in Biomedicine,0,"Support Vector Machines have achieved state of the art performance in several classification tasks. In this article we apply them to the identification and semantic annotation of scientific and technical terminology in the domain of molecular biology. This illustrates the extensibility of the traditional named entity task to special domains with extensive terminologies such as those in medicine and related disciplines. We illustrate SVM's capabilities using a sample of 100 journal abstracts texts taken from the {human, blood cell, transcription factor} domain of MEDLINE. Approximately 3400 terms are annotated and the model performs at about 74% F-score on cross-validation tests. A detailed analysis based on empirical evidence shows the contribution of various feature sets to performance."
W02-2029,Use of Support Vector Machines in Extended Named Entity Recognition,2002,18,110,2,0.666667,17433,koichi takeuchi,{COLING}-02: The 6th Conference on Natural Language Learning 2002 ({C}o{NLL}-2002),0,This paper explores the use of Support Vector Machines (SVMs) for an extended named entity task. We investigate the identification and classification of technical terms in the molecular biology domain and contrast this to results obtained for traditional NE recognition on the MUC-6 data set. Furthermore we compare the performance of the SVM model to a standard HMM bigram model. Results show that the SVM utilizing a rich feature set of a xc2xb13 context window and POS features (MUC-6 only) had a significant performance advantage on both the MUC-6 and molecular biology data sets.
collier-takeuchi-2002-pia,{PIA}-Core: Semantic Annotation through Example-based Learning,2002,8,10,1,1,219,nigel collier,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper summarizes the aims and scope of the PIA (Portable Information Access) projectxe2x80x99s PIA-Core system for automatic annotation of documents on the Semantic Web, i.e. the next generation World Wide Web. The focus of the project is to develop a portable information extraction system that can be easily adapted to new domains. PIA has its foundations on three resources: the PIA-Core information extraction module, application modules and PIA guidelines for ensuring consistent annotation. We are currently developing PIA-Core based on advanced machines learning methods to automatically annotate documents with terminology, names, temporal and quantity expressions etc. using examples of annotated documents."
collier-etal-2002-progress,Progress on Multi-lingual Named Entity Annotation Guidelines using {RDF} ({S}),2002,9,3,1,1,219,nigel collier,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper provides a discussion and concise summary of the PIA (Portable Information Access project) guidelines for annotators and tool developers for annotating what we call named entity xe2x80x98plusxe2x80x99 (NE) expressions such as individual names or technical terms that we want to distinguish for whatever reason from the rest of a text. In particular we consider how to annotate locally ambiguous syntactic and semantic structures. We provide notation that conforms to RDF(S) so that annotated documents can have their content accessed on the Semantic Web, i.e. the next generation World Wide Web. In this new framework named entities become instances of concepts in an explicit ontology, and the base text provides links to the annotation and ontology data files."
W00-1704,Building an Annotated Corpus in the Molecular-Biology Domain,2000,15,23,3,0,35318,yuka tateisi,Proceedings of the {COLING}-2000 Workshop on Semantic Annotation and Intelligent Content,0,"Corpus annotation is now a key topic for all areas of natural language processing (NLP) and information extraction (IE) which employ supervised learning. With the explosion of results in molecular-biology there is an increased need for IE to extract knowledge to support database building and to search intelligently for information in online journal collections. To support this we are building a corpus of annotated abstracts taken from National Library of Medicine's MEDLINE database. In this paper we report on this new corpus, its ontological basis, and our experience in designing the annotation scheme. Experimental results are shown for inter-annotator agreement and comments are made on methodological considerations."
W00-0904,Comparison between Tagged Corpora for the Named Entity Task,2000,19,25,2,0,51944,chikashi nobata,The Workshop on Comparing Corpora,0,"We present two measures for comparing corpora based on information theory statistics such as gain ratio as well as simple term-class frequency counts. We tested the predictions made by these measures about corpus difficulty in two domains --- news and molecular biology --- using the result of two well-used paradigms for NE, decision trees and HMMs and found that gain ratio was the more reliable predictor."
C00-1030,Extracting the Names of Genes and Gene Products with a Hidden {M}arkov Model,2000,17,217,1,1,219,nigel collier,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,We report the results of a study into the use of a linear interpolating hidden Markov model (HMM) for the task of extracting technical terminology from MEDLINE abstracts and texts in the molecular-biology domain. This is the first stage in a system that will extract event information for automatically updating biology databases. We trained the HMM entirely with bigrams based on lexical and character features in a relatively small corpus of 100 MEDLINE abstracts that were marked-up by domain experts with term classes such as proteins and DNA. Using cross-validation methods we achieved an F-score of 0.73 and we examine the contribution made by each part of the interpolation model to overcoming data sparseness.
E99-1043,The {GENIA} project: corpus-based knowledge acquisition and information extraction from genome research papers,1999,8,57,1,1,219,nigel collier,Ninth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present an outline of the genome information acquisition (GENIA) project for automatically extracting biochemical information from journal papers and abstracts. GENIA will be available over the Internet and is designed to aid in information extraction, retrieval and visualisation and to help reduce information overload on researchers. The vast repository of papers available online in databases such as MEDLINE is a natural environment in which to develop language engineering methods and tools and is an opportunity to show how language engineering can play a key role on the Internet."
P98-1041,Machine Translation vs. Dictionary Term Translation - a Comparison for {E}nglish-{J}apanese News Article Alignment,1998,7,18,1,1,219,nigel collier,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,Bilingual news article alignment methods based on multi-lingual information retrieval have been shown to be successful for the automatic production of so-called noisy-parallel corpora. In this paper we compare the use of machine translation (MT) to the commonly used dictionary term lookup (DTL) method for Reuter news article alignment in English and Japanese. The results show the trade-off between improved lexical disambiguation provided by machine translation and extended synonym choice provided by dictionary term lookup and indicate that MT is superior to DTL only at medium and low recall levels. At high recall levels DTL has superior precision.
P98-1042,An Experiment in Hybrid Dictionary and Statistical Sentence Alignment,1998,9,14,1,1,219,nigel collier,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"The task of aligning sentences in parallel corpora of two languages has been well studied using pure statistical or linguistic models. We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities. This paper seeks to further our knowledge of the alignment task by comparing the performance of the alignment models when used separately and together, i.e. as a hybrid system. Our results show that for our English-Japanese corpus of newspaper articles, the hybrid system using lexical matching and sentence length ratios outperforms the pure methods."
C98-1041,Machine Translation vs. Dictionary Term Translation - a Comparison for {E}nglish-{J}apanese News Article Alignment,1998,7,18,1,1,219,nigel collier,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,Bilingual news article alignment methods based on multi-lingual information retrieval have been shown to be successful for the automatic production of so-called noisy-parallel corpora. In this paper we compare the use of machine translation (MT) to the commonly used dictionary term lookup (DTL) method for Reuter news article alignment in English and Japanese. The results show the trade-off between improved lexical disambiguation provided by machine translation and extended synonym choice provided by dictionary term lookup and indicate that MT is superior to DTL only at medium and low recall levels. At high recall levels DTL has superior precision.
C98-1042,An Experiment in Hybrid Dictionary and Statistical Sentence Alignment,1998,9,14,1,1,219,nigel collier,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"The task of aligning sentences in parallel corpora of two languages has been well studied using pure statistical or linguistic models. We developed a linguistic method based on lexical matching with a bilingual dictionary and two statistical methods based on sentence length ratios and sentence offset probabilities. This paper seeks to further our knowledge of the alignment task by comparing the performance of the alignment models when used separately and together, i.e. as a hybrid system. Our results show that for our English-Japanese corpus of newspaper articles, the hybrid system using lexical matching and sentence length ratios outperforms the pure methods."
